stage: "sft"
do_train: true
model_name_or_path: "model/Qwen2.5-7B-Instruct/"
dataset: "SWE-Dev-train"
dataset_dir: "LLaMA-Factory/data"
template: "qwen"
finetuning_type: "lora"
output_dir: "./saves/Qwen2.5_7B_6e-4_2048_8_16/lora/sft"
overwrite_cache: true
overwrite_output_dir: true
cutoff_len: 32768
preprocessing_num_workers: 128
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 4
lr_scheduler_type: "cosine"
logging_steps: 1
warmup_steps: 410
save_steps: 150
eval_steps: 50
evaluation_strategy: "steps"
load_best_model_at_end: true
learning_rate: 0.0006
num_train_epochs: 4.0
disable_gradient_checkpointing: False
max_samples: 4096
val_size: 0.1
plot_loss: true
fp16: true
lora_dropout: 0.05
lora_rank: 8
lora_alpha: 16
deepspeed: "LLaMA-Factory/examples/deepspeed/ds_z3_offload_config.json"