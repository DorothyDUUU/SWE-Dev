# Multi-Agent System Inference
Here we integrate 10 Multi-Agent Systems inference in the [MASLab](https://github.com/MASWorks/MASLab) framework.

| No. | Methodology                                                                                     | Venue        | Role    | Topo.   | Tool | Generalization    |
|:---:|:------------------------------------------------------------------------------------------------|:------------:|:-------:|:-------:|:----:|:-----------------:|
| 1   | [Reflexion](https://arxiv.org/abs/2303.11366)                                                   | NeurIPS 2023 | Fixed   | Fixed   | No   | Yes               |
| 2   | [Self-Consistency](https://arxiv.org/abs/2203.11171)                                            | ICLR 2024    | Fixed   | Fixed   | No   | Yes               |
| 3   | [LLM Debate (Improving Factuality...)](https://arxiv.org/abs/2305.14325)                         | ICML 2024    | Fixed   | Fixed   | No   | Pre-defined Roles |
| 4   | [MAD (Multi-Agent Discussion Framework)](https://arxiv.org/abs/2402.18034)                       | EMNLP 2024   | Fixed   | Fixed   | No   | Pre-defined Roles |
| 5   | [Self-Refine](https://arxiv.org/abs/2303.17651)                                                  | NeurIPS 2024 | Fixed   | Fixed   | No   | Yes               |
| 6   | [AgentVerse](https://openreview.net/forum?id=qPrrV093o0)                                         | ICLR 2024    | Dynamic | Fixed   | No   | Yes               |
| 7   | [MetaGPT](https://openreview.net/forum?id=VtmBAGCN7o)                                           | ICLR 2024    | Fixed   | Fixed   | Yes  | Coding-Specific   |
| 8   | [ChatDev](https://arxiv.org/abs/2307.07924)                                                      | ACL 2024     | Fixed   | Fixed   | Yes  | Coding-Specific   |
| 9   | [MapCoder](https://arxiv.org/abs/2405.08586)                                                      | ACL 2024     | Fixed   | Fixed   | Yes  | Coding-Specific   |
| 10  | [EvoMAC (Evolving LLM-based Multi-Agent Systems)](https://arxiv.org/abs/2405.03340)                | ICLR 2025    | Dynamic | Dynamic | Yes  | Coding-Specific   |

## Inference
To run an inference using the `example-method` and metadata located in the `data/metadata` directory:
```bash
python run_inference.py --method example-method --metadata_folder data/metadata
```
The results of the inference will be stored in a structured directory under the `result` directory with the naming schema `<method>_result_<metadata_folder_name>`. The output includes both the original metadata and completed code aspects generated by the LLM.

### Command-line Arguments
- `--method`: Specifies the algorithmic strategy for code generation. The available choices are configured in the `methods` module.
- `--metadata_folder`: The directory containing JSON metadata files from which code completion tasks will be derived.

### Additional Options
- `--max_workers <number>`: (Default: 50) Set the number of threads for parallel processing.
- `--method_config_name <config_name>`: Use a pre-defined configuration preset for the selected method.
- `--model_name <model_name>`: (Default: "gpt-4o-mini") Choose the LLM for code completion tasks.
- `--model_api_config <config_path>`: Path to the model API configuration file.
- `--model_temperature <value>`: (Default: 0.5) Control the creativity level of the LLM (range: 0-1).
- `--model_max_tokens <num_tokens>`: (Default: 8192) Limit the tokens in the LLM's responses.
- `--model_timeout <seconds>`: (Default: 600) Maximum duration for each inference call.
- `--require_val`: Enable a validation step in the inference process.
- `--show_method_prints`: Output intermediate steps from the method classes during execution.


## Evaluation
```
python  run_eval.py
```