{
  "dir_path": "/app/scylla_driver",
  "package_name": "scylla_driver",
  "sample_name": "scylla_driver-test_parameter_binding",
  "src_dir": "cassandra/",
  "test_dir": "tests/",
  "test_file": "tests/unit/test_parameter_binding.py",
  "test_code": "# Copyright DataStax, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nfrom cassandra.encoder import Encoder\nfrom cassandra.protocol import ColumnMetadata\nfrom cassandra.query import (bind_params, ValueSequence, PreparedStatement,\n                             BoundStatement, UNSET_VALUE)\nfrom cassandra.cqltypes import Int32Type\nfrom cassandra.util import OrderedDict\n\n\nclass ParamBindingTest(unittest.TestCase):\n\n    def test_bind_sequence(self):\n        result = bind_params(\"%s %s %s\", (1, \"a\", 2.0), Encoder())\n        self.assertEqual(result, \"1 'a' 2.0\")\n\n    def test_bind_map(self):\n        result = bind_params(\"%(a)s %(b)s %(c)s\", dict(a=1, b=\"a\", c=2.0), Encoder())\n        self.assertEqual(result, \"1 'a' 2.0\")\n\n    def test_sequence_param(self):\n        result = bind_params(\"%s\", (ValueSequence((1, \"a\", 2.0)),), Encoder())\n        self.assertEqual(result, \"(1, 'a', 2.0)\")\n\n    def test_generator_param(self):\n        result = bind_params(\"%s\", ((i for i in range(3)),), Encoder())\n        self.assertEqual(result, \"[0, 1, 2]\")\n\n    def test_none_param(self):\n        result = bind_params(\"%s\", (None,), Encoder())\n        self.assertEqual(result, \"NULL\")\n\n    def test_list_collection(self):\n        result = bind_params(\"%s\", (['a', 'b', 'c'],), Encoder())\n        self.assertEqual(result, \"['a', 'b', 'c']\")\n\n    def test_set_collection(self):\n        result = bind_params(\"%s\", (set(['a', 'b']),), Encoder())\n        self.assertIn(result, (\"{'a', 'b'}\", \"{'b', 'a'}\"))\n\n    def test_map_collection(self):\n        vals = OrderedDict()\n        vals['a'] = 'a'\n        vals['b'] = 'b'\n        vals['c'] = 'c'\n        result = bind_params(\"%s\", (vals,), Encoder())\n        self.assertEqual(result, \"{'a': 'a', 'b': 'b', 'c': 'c'}\")\n\n    def test_quote_escaping(self):\n        result = bind_params(\"%s\", (\"\"\"'ef''ef\"ef\"\"ef'\"\"\",), Encoder())\n        self.assertEqual(result, \"\"\"'''ef''''ef\"ef\"\"ef'''\"\"\")\n\n    def test_float_precision(self):\n        f = 3.4028234663852886e+38\n        self.assertEqual(float(bind_params(\"%s\", (f,), Encoder())), f)\n\n\nclass BoundStatementTestV1(unittest.TestCase):\n\n    protocol_version = 1\n\n    @classmethod\n    def setUpClass(cls):\n        column_metadata = [ColumnMetadata('keyspace', 'cf', 'rk0', Int32Type),\n                           ColumnMetadata('keyspace', 'cf', 'rk1', Int32Type),\n                           ColumnMetadata('keyspace', 'cf', 'ck0', Int32Type),\n                           ColumnMetadata('keyspace', 'cf', 'v0', Int32Type)]\n        cls.prepared = PreparedStatement(column_metadata=column_metadata,\n                                         query_id=None,\n                                         routing_key_indexes=[1, 0],\n                                         query=None,\n                                         keyspace='keyspace',\n                                         protocol_version=cls.protocol_version, result_metadata=None,\n                                         result_metadata_id=None)\n        cls.bound = BoundStatement(prepared_statement=cls.prepared)\n\n    def test_invalid_argument_type(self):\n        values = (0, 0, 0, 'string not int')\n        try:\n            self.bound.bind(values)\n        except TypeError as e:\n            self.assertIn('v0', str(e))\n            self.assertIn('Int32Type', str(e))\n            self.assertIn('str', str(e))\n        else:\n            self.fail('Passed invalid type but exception was not thrown')\n\n        values = (['1', '2'], 0, 0, 0)\n\n        try:\n            self.bound.bind(values)\n        except TypeError as e:\n            self.assertIn('rk0', str(e))\n            self.assertIn('Int32Type', str(e))\n            self.assertIn('list', str(e))\n        else:\n            self.fail('Passed invalid type but exception was not thrown')\n\n    def test_inherit_fetch_size(self):\n        keyspace = 'keyspace1'\n        column_family = 'cf1'\n\n        column_metadata = [\n            ColumnMetadata(keyspace, column_family, 'foo1', Int32Type),\n            ColumnMetadata(keyspace, column_family, 'foo2', Int32Type)\n        ]\n\n        prepared_statement = PreparedStatement(column_metadata=column_metadata,\n                                               query_id=None,\n                                               routing_key_indexes=[],\n                                               query=None,\n                                               keyspace=keyspace,\n                                               protocol_version=self.protocol_version,\n                                               result_metadata=None,\n                                               result_metadata_id=None)\n        prepared_statement.fetch_size = 1234\n        bound_statement = BoundStatement(prepared_statement=prepared_statement)\n        self.assertEqual(1234, bound_statement.fetch_size)\n\n    def test_too_few_parameters_for_routing_key(self):\n        self.assertRaises(ValueError, self.prepared.bind, (1,))\n\n        bound = self.prepared.bind((1, 2))\n        self.assertEqual(bound.keyspace, 'keyspace')\n\n    def test_dict_missing_routing_key(self):\n        self.assertRaises(KeyError, self.bound.bind, {'rk0': 0, 'ck0': 0, 'v0': 0})\n        self.assertRaises(KeyError, self.bound.bind, {'rk1': 0, 'ck0': 0, 'v0': 0})\n\n    def test_missing_value(self):\n        self.assertRaises(KeyError, self.bound.bind, {'rk0': 0, 'rk1': 0, 'ck0': 0})\n\n    def test_extra_value(self):\n        self.bound.bind({'rk0': 0, 'rk1': 0, 'ck0': 0, 'v0': 0, 'should_not_be_here': 123})  # okay to have extra keys in dict\n        self.assertEqual(self.bound.values, [b'\\x00' * 4] * 4)  # four encoded zeros\n        self.assertRaises(ValueError, self.bound.bind, (0, 0, 0, 0, 123))\n\n    def test_values_none(self):\n        # should have values\n        self.assertRaises(ValueError, self.bound.bind, None)\n\n        # prepared statement with no values\n        prepared_statement = PreparedStatement(column_metadata=[],\n                                               query_id=None,\n                                               routing_key_indexes=[],\n                                               query=None,\n                                               keyspace='whatever',\n                                               protocol_version=self.protocol_version,\n                                               result_metadata=None,\n                                               result_metadata_id=None)\n        bound = prepared_statement.bind(None)\n        self.assertListEqual(bound.values, [])\n\n    def test_bind_none(self):\n        self.bound.bind({'rk0': 0, 'rk1': 0, 'ck0': 0, 'v0': None})\n        self.assertEqual(self.bound.values[-1], None)\n\n        old_values = self.bound.values\n        self.bound.bind((0, 0, 0, None))\n        self.assertIsNot(self.bound.values, old_values)\n        self.assertEqual(self.bound.values[-1], None)\n\n    def test_unset_value(self):\n        self.assertRaises(ValueError, self.bound.bind, {'rk0': 0, 'rk1': 0, 'ck0': 0, 'v0': UNSET_VALUE})\n        self.assertRaises(ValueError, self.bound.bind, (0, 0, 0, UNSET_VALUE))\n\n\nclass BoundStatementTestV2(BoundStatementTestV1):\n    protocol_version = 2\n\n\nclass BoundStatementTestV3(BoundStatementTestV1):\n    protocol_version = 3\n\n\nclass BoundStatementTestV4(BoundStatementTestV1):\n    protocol_version = 4\n\n    def test_dict_missing_routing_key(self):\n        # in v4 it implicitly binds UNSET_VALUE for missing items,\n        # UNSET_VALUE is ValueError for routing keys\n        self.assertRaises(ValueError, self.bound.bind, {'rk0': 0, 'ck0': 0, 'v0': 0})\n        self.assertRaises(ValueError, self.bound.bind, {'rk1': 0, 'ck0': 0, 'v0': 0})\n\n    def test_missing_value(self):\n        # in v4 missing values are UNSET_VALUE\n        self.bound.bind({'rk0': 0, 'rk1': 0, 'ck0': 0})\n        self.assertEqual(self.bound.values[-1], UNSET_VALUE)\n\n        old_values = self.bound.values\n        self.bound.bind((0, 0, 0))\n        self.assertIsNot(self.bound.values, old_values)\n        self.assertEqual(self.bound.values[-1], UNSET_VALUE)\n\n    def test_unset_value(self):\n        self.bound.bind({'rk0': 0, 'rk1': 0, 'ck0': 0, 'v0': UNSET_VALUE})\n        self.assertEqual(self.bound.values[-1], UNSET_VALUE)\n\n        self.bound.bind((0, 0, 0, UNSET_VALUE))\n        self.assertEqual(self.bound.values[-1], UNSET_VALUE)\n\n\nclass BoundStatementTestV5(BoundStatementTestV4):\n    protocol_version = 5\n",
  "GT_file_code": {
    "cassandra/query.py": "# Copyright DataStax, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nThis module holds classes for working with prepared statements and\nspecifying consistency levels and retry policies for individual\nqueries.\n\"\"\"\n\nfrom collections import namedtuple\nfrom datetime import datetime, timedelta, timezone\nimport re\nimport struct\nimport time\nimport warnings\n\nfrom cassandra import ConsistencyLevel, OperationTimedOut\nfrom cassandra.util import unix_time_from_uuid1, maybe_add_timeout_to_query\nfrom cassandra.encoder import Encoder\nimport cassandra.encoder\nfrom cassandra.policies import ColDesc\nfrom cassandra.protocol import _UNSET_VALUE\nfrom cassandra.util import OrderedDict, _sanitize_identifiers\n\nimport logging\nlog = logging.getLogger(__name__)\n\nUNSET_VALUE = _UNSET_VALUE\n\"\"\"\nSpecifies an unset value when binding a prepared statement.\n\nUnset values are ignored, allowing prepared statements to be used without specify\n\nSee https://issues.apache.org/jira/browse/CASSANDRA-7304 for further details on semantics.\n\n.. versionadded:: 2.6.0\n\nOnly valid when using native protocol v4+\n\"\"\"\n\nNON_ALPHA_REGEX = re.compile('[^a-zA-Z0-9]')\nSTART_BADCHAR_REGEX = re.compile('^[^a-zA-Z0-9]*')\nEND_BADCHAR_REGEX = re.compile('[^a-zA-Z0-9_]*$')\n\n_clean_name_cache = {}\n\n\ndef _clean_column_name(name):\n    try:\n        return _clean_name_cache[name]\n    except KeyError:\n        clean = NON_ALPHA_REGEX.sub(\"_\", START_BADCHAR_REGEX.sub(\"\", END_BADCHAR_REGEX.sub(\"\", name)))\n        _clean_name_cache[name] = clean\n        return clean\n\n\ndef tuple_factory(colnames, rows):\n    \"\"\"\n    Returns each row as a tuple\n\n    Example::\n\n        >>> from cassandra.query import tuple_factory\n        >>> session = cluster.connect('mykeyspace')\n        >>> session.row_factory = tuple_factory\n        >>> rows = session.execute(\"SELECT name, age FROM users LIMIT 1\")\n        >>> print(rows[0])\n        ('Bob', 42)\n\n    .. versionchanged:: 2.0.0\n        moved from ``cassandra.decoder`` to ``cassandra.query``\n    \"\"\"\n    return rows\n\nclass PseudoNamedTupleRow(object):\n    \"\"\"\n    Helper class for pseudo_named_tuple_factory. These objects provide an\n    __iter__ interface, as well as index- and attribute-based access to values,\n    but otherwise do not attempt to implement the full namedtuple or iterable\n    interface.\n    \"\"\"\n    def __init__(self, ordered_dict):\n        self._dict = ordered_dict\n        self._tuple = tuple(ordered_dict.values())\n\n    def __getattr__(self, name):\n        return self._dict[name]\n\n    def __getitem__(self, idx):\n        return self._tuple[idx]\n\n    def __iter__(self):\n        return iter(self._tuple)\n\n    def __repr__(self):\n        return '{t}({od})'.format(t=self.__class__.__name__,\n                                  od=self._dict)\n\n\ndef pseudo_namedtuple_factory(colnames, rows):\n    \"\"\"\n    Returns each row as a :class:`.PseudoNamedTupleRow`. This is the fallback\n    factory for cases where :meth:`.named_tuple_factory` fails to create rows.\n    \"\"\"\n    return [PseudoNamedTupleRow(od)\n            for od in ordered_dict_factory(colnames, rows)]\n\n\ndef named_tuple_factory(colnames, rows):\n    \"\"\"\n    Returns each row as a `namedtuple <https://docs.python.org/2/library/collections.html#collections.namedtuple>`_.\n    This is the default row factory.\n\n    Example::\n\n        >>> from cassandra.query import named_tuple_factory\n        >>> session = cluster.connect('mykeyspace')\n        >>> session.row_factory = named_tuple_factory\n        >>> rows = session.execute(\"SELECT name, age FROM users LIMIT 1\")\n        >>> user = rows[0]\n\n        >>> # you can access field by their name:\n        >>> print(\"name: %s, age: %d\" % (user.name, user.age))\n        name: Bob, age: 42\n\n        >>> # or you can access fields by their position (like a tuple)\n        >>> name, age = user\n        >>> print(\"name: %s, age: %d\" % (name, age))\n        name: Bob, age: 42\n        >>> name = user[0]\n        >>> age = user[1]\n        >>> print(\"name: %s, age: %d\" % (name, age))\n        name: Bob, age: 42\n\n    .. versionchanged:: 2.0.0\n        moved from ``cassandra.decoder`` to ``cassandra.query``\n    \"\"\"\n    clean_column_names = map(_clean_column_name, colnames)\n    try:\n        Row = namedtuple('Row', clean_column_names)\n    except SyntaxError:\n        warnings.warn(\n            \"Failed creating namedtuple for a result because there were too \"\n            \"many columns. This is due to a Python limitation that affects \"\n            \"namedtuple in Python 3.0-3.6 (see issue18896). The row will be \"\n            \"created with {substitute_factory_name}, which lacks some namedtuple \"\n            \"features and is slower. To avoid slower performance accessing \"\n            \"values on row objects, Upgrade to Python 3.7, or use a different \"\n            \"row factory. (column names: {colnames})\".format(\n                substitute_factory_name=pseudo_namedtuple_factory.__name__,\n                colnames=colnames\n            )\n        )\n        return pseudo_namedtuple_factory(colnames, rows)\n    except Exception:\n        clean_column_names = list(map(_clean_column_name, colnames))  # create list because py3 map object will be consumed by first attempt\n        log.warning(\"Failed creating named tuple for results with column names %s (cleaned: %s) \"\n                    \"(see Python 'namedtuple' documentation for details on name rules). \"\n                    \"Results will be returned with positional names. \"\n                    \"Avoid this by choosing different names, using SELECT \\\"<col name>\\\" AS aliases, \"\n                    \"or specifying a different row_factory on your Session\" %\n                    (colnames, clean_column_names))\n        Row = namedtuple('Row', _sanitize_identifiers(clean_column_names))\n\n    return [Row(*row) for row in rows]\n\n\ndef dict_factory(colnames, rows):\n    \"\"\"\n    Returns each row as a dict.\n\n    Example::\n\n        >>> from cassandra.query import dict_factory\n        >>> session = cluster.connect('mykeyspace')\n        >>> session.row_factory = dict_factory\n        >>> rows = session.execute(\"SELECT name, age FROM users LIMIT 1\")\n        >>> print(rows[0])\n        {u'age': 42, u'name': u'Bob'}\n\n    .. versionchanged:: 2.0.0\n        moved from ``cassandra.decoder`` to ``cassandra.query``\n    \"\"\"\n    return [dict(zip(colnames, row)) for row in rows]\n\n\ndef ordered_dict_factory(colnames, rows):\n    \"\"\"\n    Like :meth:`~cassandra.query.dict_factory`, but returns each row as an OrderedDict,\n    so the order of the columns is preserved.\n\n    .. versionchanged:: 2.0.0\n        moved from ``cassandra.decoder`` to ``cassandra.query``\n    \"\"\"\n    return [OrderedDict(zip(colnames, row)) for row in rows]\n\n\nFETCH_SIZE_UNSET = object()\n\n\nclass Statement(object):\n    \"\"\"\n    An abstract class representing a single query. There are three subclasses:\n    :class:`.SimpleStatement`, :class:`.BoundStatement`, and :class:`.BatchStatement`.\n    These can be passed to :meth:`.Session.execute()`.\n    \"\"\"\n\n    retry_policy = None\n    \"\"\"\n    An instance of a :class:`cassandra.policies.RetryPolicy` or one of its\n    subclasses.  This controls when a query will be retried and how it\n    will be retried.\n    \"\"\"\n\n    consistency_level = None\n    \"\"\"\n    The :class:`.ConsistencyLevel` to be used for this operation.  Defaults\n    to :const:`None`, which means that the default consistency level for\n    the Session this is executed in will be used.\n    \"\"\"\n\n    fetch_size = FETCH_SIZE_UNSET\n    \"\"\"\n    How many rows will be fetched at a time.  This overrides the default\n    of :attr:`.Session.default_fetch_size`\n\n    This only takes effect when protocol version 2 or higher is used.\n    See :attr:`.Cluster.protocol_version` for details.\n\n    .. versionadded:: 2.0.0\n    \"\"\"\n\n    keyspace = None\n    \"\"\"\n    The string name of the keyspace this query acts on. This is used when\n    :class:`~.TokenAwarePolicy` is configured in the profile load balancing policy.\n\n    It is set implicitly on :class:`.BoundStatement`, and :class:`.BatchStatement`,\n    but must be set explicitly on :class:`.SimpleStatement`.\n\n    .. versionadded:: 2.1.3\n    \"\"\"\n\n    table = None\n    \"\"\"\n    The string name of the table this query acts on. This is used when the tablet\n    feature is enabled and in the same time :class`~.TokenAwarePolicy` is configured\n    in the profile load balancing policy.\n    \"\"\"\n\n    custom_payload = None\n    \"\"\"\n    :ref:`custom_payload` to be passed to the server.\n\n    These are only allowed when using protocol version 4 or higher.\n\n    .. versionadded:: 2.6.0\n    \"\"\"\n\n    is_idempotent = False\n    \"\"\"\n    Flag indicating whether this statement is safe to run multiple times in speculative execution.\n    \"\"\"\n\n    _serial_consistency_level = None\n    _routing_key = None\n\n    def __init__(self, retry_policy=None, consistency_level=None, routing_key=None,\n                 serial_consistency_level=None, fetch_size=FETCH_SIZE_UNSET, keyspace=None, custom_payload=None,\n                 is_idempotent=False, table=None):\n        if retry_policy and not hasattr(retry_policy, 'on_read_timeout'):  # just checking one method to detect positional parameter errors\n            raise ValueError('retry_policy should implement cassandra.policies.RetryPolicy')\n        if retry_policy is not None:\n            self.retry_policy = retry_policy\n        if consistency_level is not None:\n            self.consistency_level = consistency_level\n        self._routing_key = routing_key\n        if serial_consistency_level is not None:\n            self.serial_consistency_level = serial_consistency_level\n        if fetch_size is not FETCH_SIZE_UNSET:\n            self.fetch_size = fetch_size\n        if keyspace is not None:\n            self.keyspace = keyspace\n        if table is not None:\n            self.table = table\n        if custom_payload is not None:\n            self.custom_payload = custom_payload\n        self.is_idempotent = is_idempotent\n\n    def _key_parts_packed(self, parts):\n        for p in parts:\n            l = len(p)\n            yield struct.pack(\">H%dsB\" % l, l, p, 0)\n\n    def _get_routing_key(self):\n        return self._routing_key\n\n    def _set_routing_key(self, key):\n        if isinstance(key, (list, tuple)):\n            if len(key) == 1:\n                self._routing_key = key[0]\n            else:\n                self._routing_key = b\"\".join(self._key_parts_packed(key))\n        else:\n            self._routing_key = key\n\n    def _del_routing_key(self):\n        self._routing_key = None\n\n    routing_key = property(\n        _get_routing_key,\n        _set_routing_key,\n        _del_routing_key,\n        \"\"\"\n        The :attr:`~.TableMetadata.partition_key` portion of the primary key,\n        which can be used to determine which nodes are replicas for the query.\n\n        If the partition key is a composite, a list or tuple must be passed in.\n        Each key component should be in its packed (binary) format, so all\n        components should be strings.\n        \"\"\")\n\n    def _get_serial_consistency_level(self):\n        return self._serial_consistency_level\n\n    def _set_serial_consistency_level(self, serial_consistency_level):\n        if (serial_consistency_level is not None and\n                not ConsistencyLevel.is_serial(serial_consistency_level)):\n            raise ValueError(\n                \"serial_consistency_level must be either ConsistencyLevel.SERIAL \"\n                \"or ConsistencyLevel.LOCAL_SERIAL\")\n        self._serial_consistency_level = serial_consistency_level\n\n    def _del_serial_consistency_level(self):\n        self._serial_consistency_level = None\n\n    serial_consistency_level = property(\n        _get_serial_consistency_level,\n        _set_serial_consistency_level,\n        _del_serial_consistency_level,\n        \"\"\"\n        The serial consistency level is only used by conditional updates\n        (``INSERT``, ``UPDATE`` and ``DELETE`` with an ``IF`` condition).  For\n        those, the ``serial_consistency_level`` defines the consistency level of\n        the serial phase (or \"paxos\" phase) while the normal\n        :attr:`~.consistency_level` defines the consistency for the \"learn\" phase,\n        i.e. what type of reads will be guaranteed to see the update right away.\n        For example, if a conditional write has a :attr:`~.consistency_level` of\n        :attr:`~.ConsistencyLevel.QUORUM` (and is successful), then a\n        :attr:`~.ConsistencyLevel.QUORUM` read is guaranteed to see that write.\n        But if the regular :attr:`~.consistency_level` of that write is\n        :attr:`~.ConsistencyLevel.ANY`, then only a read with a\n        :attr:`~.consistency_level` of :attr:`~.ConsistencyLevel.SERIAL` is\n        guaranteed to see it (even a read with consistency\n        :attr:`~.ConsistencyLevel.ALL` is not guaranteed to be enough).\n\n        The serial consistency can only be one of :attr:`~.ConsistencyLevel.SERIAL`\n        or :attr:`~.ConsistencyLevel.LOCAL_SERIAL`. While ``SERIAL`` guarantees full\n        linearizability (with other ``SERIAL`` updates), ``LOCAL_SERIAL`` only\n        guarantees it in the local data center.\n\n        The serial consistency level is ignored for any query that is not a\n        conditional update. Serial reads should use the regular\n        :attr:`consistency_level`.\n\n        Serial consistency levels may only be used against Cassandra 2.0+\n        and the :attr:`~.Cluster.protocol_version` must be set to 2 or higher.\n\n        See :doc:`/lwt` for a discussion on how to work with results returned from\n        conditional statements.\n\n        .. versionadded:: 2.0.0\n        \"\"\")\n\n\nclass SimpleStatement(Statement):\n    \"\"\"\n    A simple, un-prepared query.\n    \"\"\"\n\n    def __init__(self, query_string, retry_policy=None, consistency_level=None, routing_key=None,\n                 serial_consistency_level=None, fetch_size=FETCH_SIZE_UNSET, keyspace=None,\n                 custom_payload=None, is_idempotent=False):\n        \"\"\"\n        `query_string` should be a literal CQL statement with the exception\n        of parameter placeholders that will be filled through the\n        `parameters` argument of :meth:`.Session.execute()`.\n\n        See :class:`Statement` attributes for a description of the other parameters.\n        \"\"\"\n        Statement.__init__(self, retry_policy, consistency_level, routing_key,\n                           serial_consistency_level, fetch_size, keyspace, custom_payload, is_idempotent)\n        self._query_string = query_string\n\n    @property\n    def query_string(self):\n        return self._query_string\n\n    def __str__(self):\n        consistency = ConsistencyLevel.value_to_name.get(self.consistency_level, 'Not Set')\n        return (u'<SimpleStatement query=\"%s\", consistency=%s>' %\n                (self.query_string, consistency))\n    __repr__ = __str__\n\n\nclass PreparedStatement(object):\n    \"\"\"\n    A statement that has been prepared against at least one Cassandra node.\n    Instances of this class should not be created directly, but through\n    :meth:`.Session.prepare()`.\n\n    A :class:`.PreparedStatement` should be prepared only once. Re-preparing a statement\n    may affect performance (as the operation requires a network roundtrip).\n\n    |prepared_stmt_head|: Do not use ``*`` in prepared statements if you might\n    change the schema of the table being queried. The driver and server each\n    maintain a map between metadata for a schema and statements that were\n    prepared against that schema. When a user changes a schema, e.g. by adding\n    or removing a column, the server invalidates its mappings involving that\n    schema. However, there is currently no way to propagate that invalidation\n    to drivers. Thus, after a schema change, the driver will incorrectly\n    interpret the results of ``SELECT *`` queries prepared before the schema\n    change. This is currently being addressed in `CASSANDRA-10786\n    <https://issues.apache.org/jira/browse/CASSANDRA-10786>`_.\n\n    .. |prepared_stmt_head| raw:: html\n\n       <b>A note about <code>*</code> in prepared statements</b>\n    \"\"\"\n\n    column_metadata = None  #TODO: make this bind_metadata in next major\n    retry_policy = None\n    consistency_level = None\n    custom_payload = None\n    fetch_size = FETCH_SIZE_UNSET\n    keyspace = None  # change to prepared_keyspace in major release\n    protocol_version = None\n    query_id = None\n    query_string = None\n    result_metadata = None\n    result_metadata_id = None\n    column_encryption_policy = None\n    routing_key_indexes = None\n    _routing_key_index_set = None\n    serial_consistency_level = None  # TODO never used?\n\n    def __init__(self, column_metadata, query_id, routing_key_indexes, query,\n                 keyspace, protocol_version, result_metadata, result_metadata_id,\n                 column_encryption_policy=None):\n        self.column_metadata = column_metadata\n        self.query_id = query_id\n        self.routing_key_indexes = routing_key_indexes\n        self.query_string = query\n        self.keyspace = keyspace\n        self.protocol_version = protocol_version\n        self.result_metadata = result_metadata\n        self.result_metadata_id = result_metadata_id\n        self.column_encryption_policy = column_encryption_policy\n        self.is_idempotent = False\n\n    @classmethod\n    def from_message(cls, query_id, column_metadata, pk_indexes, cluster_metadata,\n                     query, prepared_keyspace, protocol_version, result_metadata,\n                     result_metadata_id, column_encryption_policy=None):\n        if not column_metadata:\n            return PreparedStatement(column_metadata, query_id, None,\n                                     query, prepared_keyspace, protocol_version, result_metadata,\n                                     result_metadata_id, column_encryption_policy)\n\n        if pk_indexes:\n            routing_key_indexes = pk_indexes\n        else:\n            routing_key_indexes = None\n\n            first_col = column_metadata[0]\n            ks_meta = cluster_metadata.keyspaces.get(first_col.keyspace_name)\n            if ks_meta:\n                table_meta = ks_meta.tables.get(first_col.table_name)\n                if table_meta:\n                    partition_key_columns = table_meta.partition_key\n\n                    # make a map of {column_name: index} for each column in the statement\n                    statement_indexes = dict((c.name, i) for i, c in enumerate(column_metadata))\n\n                    # a list of which indexes in the statement correspond to partition key items\n                    try:\n                        routing_key_indexes = [statement_indexes[c.name]\n                                               for c in partition_key_columns]\n                    except KeyError:  # we're missing a partition key component in the prepared\n                        pass          # statement; just leave routing_key_indexes as None\n\n        return PreparedStatement(column_metadata, query_id, routing_key_indexes,\n                                 query, prepared_keyspace, protocol_version, result_metadata,\n                                 result_metadata_id, column_encryption_policy)\n\n    def bind(self, values):\n        \"\"\"\n        Creates and returns a :class:`BoundStatement` instance using `values`.\n\n        See :meth:`BoundStatement.bind` for rules on input ``values``.\n        \"\"\"\n        return BoundStatement(self).bind(values)\n\n    def is_routing_key_index(self, i):\n        if self._routing_key_index_set is None:\n            self._routing_key_index_set = set(self.routing_key_indexes) if self.routing_key_indexes else set()\n        return i in self._routing_key_index_set\n\n    def __str__(self):\n        consistency = ConsistencyLevel.value_to_name.get(self.consistency_level, 'Not Set')\n        return (u'<PreparedStatement query=\"%s\", consistency=%s>' %\n                (self.query_string, consistency))\n    __repr__ = __str__\n\n\nclass BoundStatement(Statement):\n    \"\"\"\n    A prepared statement that has been bound to a particular set of values.\n    These may be created directly or through :meth:`.PreparedStatement.bind()`.\n    \"\"\"\n\n    prepared_statement = None\n    \"\"\"\n    The :class:`PreparedStatement` instance that this was created from.\n    \"\"\"\n\n    values = None\n    \"\"\"\n    The sequence of values that were bound to the prepared statement.\n    \"\"\"\n\n    def __init__(self, prepared_statement, retry_policy=None, consistency_level=None, routing_key=None,\n                 serial_consistency_level=None, fetch_size=FETCH_SIZE_UNSET, keyspace=None,\n                 custom_payload=None):\n        \"\"\"\n        `prepared_statement` should be an instance of :class:`PreparedStatement`.\n\n        See :class:`Statement` attributes for a description of the other parameters.\n        \"\"\"\n        self.prepared_statement = prepared_statement\n\n        self.retry_policy = prepared_statement.retry_policy\n        self.consistency_level = prepared_statement.consistency_level\n        self.serial_consistency_level = prepared_statement.serial_consistency_level\n        self.fetch_size = prepared_statement.fetch_size\n        self.custom_payload = prepared_statement.custom_payload\n        self.is_idempotent = prepared_statement.is_idempotent\n        self.values = []\n\n        meta = prepared_statement.column_metadata\n        if meta:\n            self.keyspace = meta[0].keyspace_name\n            self.table = meta[0].table_name\n\n        Statement.__init__(self, retry_policy, consistency_level, routing_key,\n                           serial_consistency_level, fetch_size, keyspace, custom_payload,\n                           prepared_statement.is_idempotent)\n\n    def bind(self, values):\n        \"\"\"\n        Binds a sequence of values for the prepared statement parameters\n        and returns this instance.  Note that `values` *must* be:\n\n        * a sequence, even if you are only binding one value, or\n        * a dict that relates 1-to-1 between dict keys and columns\n\n        .. versionchanged:: 2.6.0\n\n            :data:`~.UNSET_VALUE` was introduced. These can be bound as positional parameters\n            in a sequence, or by name in a dict. Additionally, when using protocol v4+:\n\n            * short sequences will be extended to match bind parameters with UNSET_VALUE\n            * names may be omitted from a dict with UNSET_VALUE implied.\n\n        .. versionchanged:: 3.0.0\n\n            method will not throw if extra keys are present in bound dict (PYTHON-178)\n        \"\"\"\n        if values is None:\n            values = ()\n        proto_version = self.prepared_statement.protocol_version\n        col_meta = self.prepared_statement.column_metadata\n        ce_policy = self.prepared_statement.column_encryption_policy\n\n        # special case for binding dicts\n        if isinstance(values, dict):\n            values_dict = values\n            values = []\n\n            # sort values accordingly\n            for col in col_meta:\n                try:\n                    values.append(values_dict[col.name])\n                except KeyError:\n                    if proto_version >= 4:\n                        values.append(UNSET_VALUE)\n                    else:\n                        raise KeyError(\n                            'Column name `%s` not found in bound dict.' %\n                            (col.name))\n\n        value_len = len(values)\n        col_meta_len = len(col_meta)\n\n        if value_len > col_meta_len:\n            raise ValueError(\n                \"Too many arguments provided to bind() (got %d, expected %d)\" %\n                (len(values), len(col_meta)))\n\n        # this is fail-fast for clarity pre-v4. When v4 can be assumed,\n        # the error will be better reported when UNSET_VALUE is implicitly added.\n        if proto_version < 4 and self.prepared_statement.routing_key_indexes and \\\n           value_len < len(self.prepared_statement.routing_key_indexes):\n            raise ValueError(\n                \"Too few arguments provided to bind() (got %d, required %d for routing key)\" %\n                (value_len, len(self.prepared_statement.routing_key_indexes)))\n\n        self.raw_values = values\n        self.values = []\n        for value, col_spec in zip(values, col_meta):\n            if value is None:\n                self.values.append(None)\n            elif value is UNSET_VALUE:\n                if proto_version >= 4:\n                    self._append_unset_value()\n                else:\n                    raise ValueError(\"Attempt to bind UNSET_VALUE while using unsuitable protocol version (%d < 4)\" % proto_version)\n            else:\n                try:\n                    col_desc = ColDesc(col_spec.keyspace_name, col_spec.table_name, col_spec.name)\n                    uses_ce = ce_policy and ce_policy.contains_column(col_desc)\n                    col_type = ce_policy.column_type(col_desc) if uses_ce else col_spec.type\n                    col_bytes = col_type.serialize(value, proto_version)\n                    if uses_ce:\n                        col_bytes = ce_policy.encrypt(col_desc, col_bytes)\n                    self.values.append(col_bytes)\n                except (TypeError, struct.error) as exc:\n                    actual_type = type(value)\n                    message = ('Received an argument of invalid type for column \"%s\". '\n                               'Expected: %s, Got: %s; (%s)' % (col_spec.name, col_spec.type, actual_type, exc))\n                    raise TypeError(message)\n\n        if proto_version >= 4:\n            diff = col_meta_len - len(self.values)\n            if diff:\n                for _ in range(diff):\n                    self._append_unset_value()\n\n        return self\n\n    def _append_unset_value(self):\n        next_index = len(self.values)\n        if self.prepared_statement.is_routing_key_index(next_index):\n            col_meta = self.prepared_statement.column_metadata[next_index]\n            raise ValueError(\"Cannot bind UNSET_VALUE as a part of the routing key '%s'\" % col_meta.name)\n        self.values.append(UNSET_VALUE)\n\n    @property\n    def routing_key(self):\n        if not self.prepared_statement.routing_key_indexes:\n            return None\n\n        if self._routing_key is not None:\n            return self._routing_key\n\n        routing_indexes = self.prepared_statement.routing_key_indexes\n        if len(routing_indexes) == 1:\n            self._routing_key = self.values[routing_indexes[0]]\n        else:\n            self._routing_key = b\"\".join(self._key_parts_packed(self.values[i] for i in routing_indexes))\n\n        return self._routing_key\n\n    def __str__(self):\n        consistency = ConsistencyLevel.value_to_name.get(self.consistency_level, 'Not Set')\n        return (u'<BoundStatement query=\"%s\", values=%s, consistency=%s>' %\n                (self.prepared_statement.query_string, self.raw_values, consistency))\n    __repr__ = __str__\n\n\nclass BatchType(object):\n    \"\"\"\n    A BatchType is used with :class:`.BatchStatement` instances to control\n    the atomicity of the batch operation.\n\n    .. versionadded:: 2.0.0\n    \"\"\"\n\n    LOGGED = None\n    \"\"\"\n    Atomic batch operation.\n    \"\"\"\n\n    UNLOGGED = None\n    \"\"\"\n    Non-atomic batch operation.\n    \"\"\"\n\n    COUNTER = None\n    \"\"\"\n    Batches of counter operations.\n    \"\"\"\n\n    def __init__(self, name, value):\n        self.name = name\n        self.value = value\n\n    def __str__(self):\n        return self.name\n\n    def __repr__(self):\n        return \"BatchType.%s\" % (self.name, )\n\n\nBatchType.LOGGED = BatchType(\"LOGGED\", 0)\nBatchType.UNLOGGED = BatchType(\"UNLOGGED\", 1)\nBatchType.COUNTER = BatchType(\"COUNTER\", 2)\n\n\nclass BatchStatement(Statement):\n    \"\"\"\n    A protocol-level batch of operations which are applied atomically\n    by default.\n\n    .. versionadded:: 2.0.0\n    \"\"\"\n\n    batch_type = None\n    \"\"\"\n    The :class:`.BatchType` for the batch operation.  Defaults to\n    :attr:`.BatchType.LOGGED`.\n    \"\"\"\n\n    serial_consistency_level = None\n    \"\"\"\n    The same as :attr:`.Statement.serial_consistency_level`, but is only\n    supported when using protocol version 3 or higher.\n    \"\"\"\n\n    _statements_and_parameters = None\n    _session = None\n\n    def __init__(self, batch_type=BatchType.LOGGED, retry_policy=None,\n                 consistency_level=None, serial_consistency_level=None,\n                 session=None, custom_payload=None):\n        \"\"\"\n        `batch_type` specifies The :class:`.BatchType` for the batch operation.\n        Defaults to :attr:`.BatchType.LOGGED`.\n\n        `retry_policy` should be a :class:`~.RetryPolicy` instance for\n        controlling retries on the operation.\n\n        `consistency_level` should be a :class:`~.ConsistencyLevel` value\n        to be used for all operations in the batch.\n\n        `custom_payload` is a :ref:`custom_payload` passed to the server.\n        Note: as Statement objects are added to the batch, this map is\n        updated with any values found in their custom payloads. These are\n        only allowed when using protocol version 4 or higher.\n\n        Example usage:\n\n        .. code-block:: python\n\n            insert_user = session.prepare(\"INSERT INTO users (name, age) VALUES (?, ?)\")\n            batch = BatchStatement(consistency_level=ConsistencyLevel.QUORUM)\n\n            for (name, age) in users_to_insert:\n                batch.add(insert_user, (name, age))\n\n            session.execute(batch)\n\n        You can also mix different types of operations within a batch:\n\n        .. code-block:: python\n\n            batch = BatchStatement()\n            batch.add(SimpleStatement(\"INSERT INTO users (name, age) VALUES (%s, %s)\"), (name, age))\n            batch.add(SimpleStatement(\"DELETE FROM pending_users WHERE name=%s\"), (name,))\n            session.execute(batch)\n\n        .. versionadded:: 2.0.0\n\n        .. versionchanged:: 2.1.0\n            Added `serial_consistency_level` as a parameter\n\n        .. versionchanged:: 2.6.0\n            Added `custom_payload` as a parameter\n        \"\"\"\n        self.batch_type = batch_type\n        self._statements_and_parameters = []\n        self._session = session\n        Statement.__init__(self, retry_policy=retry_policy, consistency_level=consistency_level,\n                           serial_consistency_level=serial_consistency_level, custom_payload=custom_payload)\n\n    def clear(self):\n        \"\"\"\n        This is a convenience method to clear a batch statement for reuse.\n\n        *Note:* it should not be used concurrently with uncompleted execution futures executing the same\n        ``BatchStatement``.\n        \"\"\"\n        del self._statements_and_parameters[:]\n        self.keyspace = None\n        self.routing_key = None\n        if self.custom_payload:\n            self.custom_payload.clear()\n\n    def add(self, statement, parameters=None):\n        \"\"\"\n        Adds a :class:`.Statement` and optional sequence of parameters\n        to be used with the statement to the batch.\n\n        Like with other statements, parameters must be a sequence, even\n        if there is only one item.\n        \"\"\"\n        if isinstance(statement, str):\n            if parameters:\n                encoder = Encoder() if self._session is None else self._session.encoder\n                statement = bind_params(statement, parameters, encoder)\n            self._add_statement_and_params(False, statement, ())\n        elif isinstance(statement, PreparedStatement):\n            query_id = statement.query_id\n            bound_statement = statement.bind(() if parameters is None else parameters)\n            self._update_state(bound_statement)\n            self._add_statement_and_params(True, query_id, bound_statement.values)\n        elif isinstance(statement, BoundStatement):\n            if parameters:\n                raise ValueError(\n                    \"Parameters cannot be passed with a BoundStatement \"\n                    \"to BatchStatement.add()\")\n            self._update_state(statement)\n            self._add_statement_and_params(True, statement.prepared_statement.query_id, statement.values)\n        else:\n            # it must be a SimpleStatement\n            query_string = statement.query_string\n            if parameters:\n                encoder = Encoder() if self._session is None else self._session.encoder\n                query_string = bind_params(query_string, parameters, encoder)\n            self._update_state(statement)\n            self._add_statement_and_params(False, query_string, ())\n        return self\n\n    def add_all(self, statements, parameters):\n        \"\"\"\n        Adds a sequence of :class:`.Statement` objects and a matching sequence\n        of parameters to the batch. Statement and parameter sequences must be of equal length or\n        one will be truncated. :const:`None` can be used in the parameters position where are needed.\n        \"\"\"\n        for statement, value in zip(statements, parameters):\n            self.add(statement, value)\n\n    def _add_statement_and_params(self, is_prepared, statement, parameters):\n        if len(self._statements_and_parameters) >= 0xFFFF:\n            raise ValueError(\"Batch statement cannot contain more than %d statements.\" % 0xFFFF)\n        self._statements_and_parameters.append((is_prepared, statement, parameters))\n\n    def _maybe_set_routing_attributes(self, statement):\n        if self.routing_key is None:\n            if statement.keyspace and statement.routing_key:\n                self.routing_key = statement.routing_key\n                self.keyspace = statement.keyspace\n\n    def _update_custom_payload(self, statement):\n        if statement.custom_payload:\n            if self.custom_payload is None:\n                self.custom_payload = {}\n            self.custom_payload.update(statement.custom_payload)\n\n    def _update_state(self, statement):\n        self._maybe_set_routing_attributes(statement)\n        self._update_custom_payload(statement)\n\n    def __len__(self):\n        return len(self._statements_and_parameters)\n\n    def __str__(self):\n        consistency = ConsistencyLevel.value_to_name.get(self.consistency_level, 'Not Set')\n        return (u'<BatchStatement type=%s, statements=%d, consistency=%s>' %\n                (self.batch_type, len(self), consistency))\n    __repr__ = __str__\n\n\nValueSequence = cassandra.encoder.ValueSequence\n\"\"\"\nA wrapper class that is used to specify that a sequence of values should\nbe treated as a CQL list of values instead of a single column collection when used\nas part of the `parameters` argument for :meth:`.Session.execute()`.\n\nThis is typically needed when supplying a list of keys to select.\nFor example::\n\n    >>> my_user_ids = ('alice', 'bob', 'charles')\n    >>> query = \"SELECT * FROM users WHERE user_id IN %s\"\n    >>> session.execute(query, parameters=[ValueSequence(my_user_ids)])\n\n\"\"\"\n\n\ndef bind_params(query, params, encoder):\n    if isinstance(params, dict):\n        return query % dict((k, encoder.cql_encode_all_types(v)) for k, v in params.items())\n    else:\n        return query % tuple(encoder.cql_encode_all_types(v) for v in params)\n\n\nclass TraceUnavailable(Exception):\n    \"\"\"\n    Raised when complete trace details cannot be fetched from Cassandra.\n    \"\"\"\n    pass\n\n\nclass QueryTrace(object):\n    \"\"\"\n    A trace of the duration and events that occurred when executing\n    an operation.\n    \"\"\"\n\n    trace_id = None\n    \"\"\"\n    :class:`uuid.UUID` unique identifier for this tracing session.  Matches\n    the ``session_id`` column in ``system_traces.sessions`` and\n    ``system_traces.events``.\n    \"\"\"\n\n    request_type = None\n    \"\"\"\n    A string that very generally describes the traced operation.\n    \"\"\"\n\n    duration = None\n    \"\"\"\n    A :class:`datetime.timedelta` measure of the duration of the query.\n    \"\"\"\n\n    client = None\n    \"\"\"\n    The IP address of the client that issued this request\n\n    This is only available when using Cassandra 2.2+\n    \"\"\"\n\n    coordinator = None\n    \"\"\"\n    The IP address of the host that acted as coordinator for this request.\n    \"\"\"\n\n    parameters = None\n    \"\"\"\n    A :class:`dict` of parameters for the traced operation, such as the\n    specific query string.\n    \"\"\"\n\n    started_at = None\n    \"\"\"\n    A UTC :class:`datetime.datetime` object describing when the operation\n    was started.\n    \"\"\"\n\n    events = None\n    \"\"\"\n    A chronologically sorted list of :class:`.TraceEvent` instances\n    representing the steps the traced operation went through.  This\n    corresponds to the rows in ``system_traces.events`` for this tracing\n    session.\n    \"\"\"\n\n    _session = None\n\n    _SELECT_SESSIONS_FORMAT = \"SELECT * FROM system_traces.sessions WHERE session_id = %s\"\n    _SELECT_EVENTS_FORMAT = \"SELECT * FROM system_traces.events WHERE session_id = %s\"\n    _BASE_RETRY_SLEEP = 0.003\n\n    def __init__(self, trace_id, session):\n        self.trace_id = trace_id\n        self._session = session\n\n    def populate(self, max_wait=2.0, wait_for_complete=True, query_cl=None):\n        \"\"\"\n        Retrieves the actual tracing details from Cassandra and populates the\n        attributes of this instance.  Because tracing details are stored\n        asynchronously by Cassandra, this may need to retry the session\n        detail fetch.  If the trace is still not available after `max_wait`\n        seconds, :exc:`.TraceUnavailable` will be raised; if `max_wait` is\n        :const:`None`, this will retry forever.\n\n        `wait_for_complete=False` bypasses the wait for duration to be populated.\n        This can be used to query events from partial sessions.\n\n        `query_cl` specifies a consistency level to use for polling the trace tables,\n        if it should be different than the session default.\n        \"\"\"\n        attempt = 0\n        start = time.time()\n        while True:\n            time_spent = time.time() - start\n            if max_wait is not None and time_spent >= max_wait:\n                raise TraceUnavailable(\n                    \"Trace information was not available within %f seconds. Consider raising Session.max_trace_wait.\" % (max_wait,))\n\n            log.debug(\"Attempting to fetch trace info for trace ID: %s\", self.trace_id)\n            metadata_request_timeout = self._session.cluster.control_connection and self._session.cluster.control_connection._metadata_request_timeout\n            session_results = self._execute(\n                SimpleStatement(maybe_add_timeout_to_query(self._SELECT_SESSIONS_FORMAT, metadata_request_timeout), consistency_level=query_cl), (self.trace_id,), time_spent, max_wait)\n\n            # PYTHON-730: There is race condition that the duration mutation is written before started_at the for fast queries\n            session_row = session_results.one() if session_results else None\n            is_complete = session_row is not None and session_row.duration is not None and session_row.started_at is not None\n            if not session_results or (wait_for_complete and not is_complete):\n                time.sleep(self._BASE_RETRY_SLEEP * (2 ** attempt))\n                attempt += 1\n                continue\n            if is_complete:\n                log.debug(\"Fetched trace info for trace ID: %s\", self.trace_id)\n            else:\n                log.debug(\"Fetching parital trace info for trace ID: %s\", self.trace_id)\n\n            self.request_type = session_row.request\n            self.duration = timedelta(microseconds=session_row.duration) if is_complete else None\n            self.started_at = session_row.started_at\n            self.coordinator = session_row.coordinator\n            self.parameters = session_row.parameters\n            # since C* 2.2\n            self.client = getattr(session_row, 'client', None)\n\n            log.debug(\"Attempting to fetch trace events for trace ID: %s\", self.trace_id)\n            time_spent = time.time() - start\n            event_results = self._execute(\n                SimpleStatement(maybe_add_timeout_to_query(self._SELECT_EVENTS_FORMAT, metadata_request_timeout),\n                                consistency_level=query_cl),\n                (self.trace_id,),\n                time_spent,\n                max_wait)\n            log.debug(\"Fetched trace events for trace ID: %s\", self.trace_id)\n            self.events = tuple(TraceEvent(r.activity, r.event_id, r.source, r.source_elapsed, r.thread)\n                                for r in event_results)\n            break\n\n    def _execute(self, query, parameters, time_spent, max_wait):\n        timeout = (max_wait - time_spent) if max_wait is not None else None\n        future = self._session._create_response_future(query, parameters, trace=False, custom_payload=None, timeout=timeout)\n        # in case the user switched the row factory, set it to namedtuple for this query\n        future.row_factory = named_tuple_factory\n        future.send_request()\n\n        try:\n            return future.result()\n        except OperationTimedOut:\n            raise TraceUnavailable(\"Trace information was not available within %f seconds\" % (max_wait,))\n\n    def __str__(self):\n        return \"%s [%s] coordinator: %s, started at: %s, duration: %s, parameters: %s\" \\\n               % (self.request_type, self.trace_id, self.coordinator, self.started_at,\n                  self.duration, self.parameters)\n\n\nclass TraceEvent(object):\n    \"\"\"\n    Representation of a single event within a query trace.\n    \"\"\"\n\n    description = None\n    \"\"\"\n    A brief description of the event.\n    \"\"\"\n\n    datetime = None\n    \"\"\"\n    A UTC :class:`datetime.datetime` marking when the event occurred.\n    \"\"\"\n\n    source = None\n    \"\"\"\n    The IP address of the node this event occurred on.\n    \"\"\"\n\n    source_elapsed = None\n    \"\"\"\n    A :class:`datetime.timedelta` measuring the amount of time until\n    this event occurred starting from when :attr:`.source` first\n    received the query.\n    \"\"\"\n\n    thread_name = None\n    \"\"\"\n    The name of the thread that this event occurred on.\n    \"\"\"\n\n    def __init__(self, description, timeuuid, source, source_elapsed, thread_name):\n        self.description = description\n        self.datetime = datetime.fromtimestamp(unix_time_from_uuid1(timeuuid), tz=timezone.utc)\n        self.source = source\n        if source_elapsed is not None:\n            self.source_elapsed = timedelta(microseconds=source_elapsed)\n        else:\n            self.source_elapsed = None\n        self.thread_name = thread_name\n\n    def __str__(self):\n        return \"%s on %s[%s] at %s\" % (self.description, self.source, self.thread_name, self.datetime)\n\n\n# TODO remove next major since we can target using the `host` attribute of session.execute\nclass HostTargetingStatement(object):\n    \"\"\"\n    Wraps any query statement and attaches a target host, making\n    it usable in a targeted LBP without modifying the user's statement.\n    \"\"\"\n    def __init__(self, inner_statement, target_host):\n            self.__class__ = type(inner_statement.__class__.__name__,\n                                  (self.__class__, inner_statement.__class__),\n                                  {})\n            self.__dict__ = inner_statement.__dict__\n            self.target_host = target_host\n",
    "cassandra/policies.py": "# Copyright DataStax, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport random\n\nfrom collections import namedtuple\nfrom functools import lru_cache\nfrom itertools import islice, cycle, groupby, repeat\nimport logging\nfrom random import randint, shuffle\nfrom threading import Lock\nimport socket\nimport warnings\n\nlog = logging.getLogger(__name__)\n\nfrom cassandra import WriteType as WT\n\n\n# This is done this way because WriteType was originally\n# defined here and in order not to break the API.\n# It may removed in the next mayor.\nWriteType = WT\n\nfrom cassandra import ConsistencyLevel, OperationTimedOut\n\nclass HostDistance(object):\n    \"\"\"\n    A measure of how \"distant\" a node is from the client, which\n    may influence how the load balancer distributes requests\n    and how many connections are opened to the node.\n    \"\"\"\n\n    IGNORED = -1\n    \"\"\"\n    A node with this distance should never be queried or have\n    connections opened to it.\n    \"\"\"\n\n    LOCAL_RACK = 0\n    \"\"\"\n    Nodes with ``LOCAL_RACK`` distance will be preferred for operations\n    under some load balancing policies (such as :class:`.RackAwareRoundRobinPolicy`)\n    and will have a greater number of connections opened against\n    them by default.\n\n    This distance is typically used for nodes within the same\n    datacenter and the same rack as the client.\n    \"\"\"\n\n    LOCAL = 1\n    \"\"\"\n    Nodes with ``LOCAL`` distance will be preferred for operations\n    under some load balancing policies (such as :class:`.DCAwareRoundRobinPolicy`)\n    and will have a greater number of connections opened against\n    them by default.\n\n    This distance is typically used for nodes within the same\n    datacenter as the client.\n    \"\"\"\n\n    REMOTE = 2\n    \"\"\"\n    Nodes with ``REMOTE`` distance will be treated as a last resort\n    by some load balancing policies (such as :class:`.DCAwareRoundRobinPolicy`\n    and :class:`.RackAwareRoundRobinPolicy`)and will have a smaller number of\n    connections opened against them by default.\n\n    This distance is typically used for nodes outside of the\n    datacenter that the client is running in.\n    \"\"\"\n\n\nclass HostStateListener(object):\n\n    def on_up(self, host):\n        \"\"\" Called when a node is marked up. \"\"\"\n        raise NotImplementedError()\n\n    def on_down(self, host):\n        \"\"\" Called when a node is marked down. \"\"\"\n        raise NotImplementedError()\n\n    def on_add(self, host):\n        \"\"\"\n        Called when a node is added to the cluster.  The newly added node\n        should be considered up.\n        \"\"\"\n        raise NotImplementedError()\n\n    def on_remove(self, host):\n        \"\"\" Called when a node is removed from the cluster. \"\"\"\n        raise NotImplementedError()\n\n\nclass LoadBalancingPolicy(HostStateListener):\n    \"\"\"\n    Load balancing policies are used to decide how to distribute\n    requests among all possible coordinator nodes in the cluster.\n\n    In particular, they may focus on querying \"near\" nodes (those\n    in a local datacenter) or on querying nodes who happen to\n    be replicas for the requested data.\n\n    You may also use subclasses of :class:`.LoadBalancingPolicy` for\n    custom behavior.\n\n    You should always use immutable collections (e.g., tuples or\n    frozensets) to store information about hosts to prevent accidental\n    modification. When there are changes to the hosts (e.g., a host is\n    down or up), the old collection should be replaced with a new one.\n    \"\"\"\n\n    _hosts_lock = None\n\n    def __init__(self):\n        self._hosts_lock = Lock()\n\n    def distance(self, host):\n        \"\"\"\n        Returns a measure of how remote a :class:`~.pool.Host` is in\n        terms of the :class:`.HostDistance` enums.\n        \"\"\"\n        raise NotImplementedError()\n\n    def populate(self, cluster, hosts):\n        \"\"\"\n        This method is called to initialize the load balancing\n        policy with a set of :class:`.Host` instances before its\n        first use.  The `cluster` parameter is an instance of\n        :class:`.Cluster`.\n        \"\"\"\n        raise NotImplementedError()\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        \"\"\"\n        Given a :class:`~.query.Statement` instance, return a iterable\n        of :class:`.Host` instances which should be queried in that\n        order.  A generator may work well for custom implementations\n        of this method.\n\n        Note that the `query` argument may be :const:`None` when preparing\n        statements.\n\n        `working_keyspace` should be the string name of the current keyspace,\n        as set through :meth:`.Session.set_keyspace()` or with a ``USE``\n        statement.\n        \"\"\"\n        raise NotImplementedError()\n\n    def check_supported(self):\n        \"\"\"\n        This will be called after the cluster Metadata has been initialized.\n        If the load balancing policy implementation cannot be supported for\n        some reason (such as a missing C extension), this is the point at\n        which it should raise an exception.\n        \"\"\"\n        pass\n\n\nclass RoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    A subclass of :class:`.LoadBalancingPolicy` which evenly\n    distributes queries across all nodes in the cluster,\n    regardless of what datacenter the nodes may be in.\n    \"\"\"\n    _live_hosts = frozenset(())\n    _position = 0\n\n    def populate(self, cluster, hosts):\n        self._live_hosts = frozenset(hosts)\n        if len(hosts) > 1:\n            self._position = randint(0, len(hosts) - 1)\n\n    def distance(self, host):\n        return HostDistance.LOCAL\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        # not thread-safe, but we don't care much about lost increments\n        # for the purposes of load balancing\n        pos = self._position\n        self._position += 1\n\n        hosts = self._live_hosts\n        length = len(hosts)\n        if length:\n            pos %= length\n            return islice(cycle(hosts), pos, pos + length)\n        else:\n            return []\n\n    def on_up(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.union((host, ))\n\n    def on_down(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.difference((host, ))\n\n    def on_add(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.union((host, ))\n\n    def on_remove(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.difference((host, ))\n\n\nclass DCAwareRoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    Similar to :class:`.RoundRobinPolicy`, but prefers hosts\n    in the local datacenter and only uses nodes in remote\n    datacenters as a last resort.\n    \"\"\"\n\n    local_dc = None\n    used_hosts_per_remote_dc = 0\n\n    def __init__(self, local_dc='', used_hosts_per_remote_dc=0):\n        \"\"\"\n        The `local_dc` parameter should be the name of the datacenter\n        (such as is reported by ``nodetool ring``) that should\n        be considered local. If not specified, the driver will choose\n        a local_dc based on the first host among :attr:`.Cluster.contact_points`\n        having a valid DC. If relying on this mechanism, all specified\n        contact points should be nodes in a single, local DC.\n\n        `used_hosts_per_remote_dc` controls how many nodes in\n        each remote datacenter will have connections opened\n        against them. In other words, `used_hosts_per_remote_dc` hosts\n        will be considered :attr:`~.HostDistance.REMOTE` and the\n        rest will be considered :attr:`~.HostDistance.IGNORED`.\n        By default, all remote hosts are ignored.\n        \"\"\"\n        self.local_dc = local_dc\n        self.used_hosts_per_remote_dc = used_hosts_per_remote_dc\n        self._dc_live_hosts = {}\n        self._position = 0\n        self._endpoints = []\n        LoadBalancingPolicy.__init__(self)\n\n    def _dc(self, host):\n        return host.datacenter or self.local_dc\n\n    def populate(self, cluster, hosts):\n        for dc, dc_hosts in groupby(hosts, lambda h: self._dc(h)):\n            self._dc_live_hosts[dc] = tuple(set(dc_hosts))\n\n        if not self.local_dc:\n            self._endpoints = [\n                endpoint\n                for endpoint in cluster.endpoints_resolved]\n\n        self._position = randint(0, len(hosts) - 1) if hosts else 0\n\n    def distance(self, host):\n        dc = self._dc(host)\n        if dc == self.local_dc:\n            return HostDistance.LOCAL\n\n        if not self.used_hosts_per_remote_dc:\n            return HostDistance.IGNORED\n        else:\n            dc_hosts = self._dc_live_hosts.get(dc)\n            if not dc_hosts:\n                return HostDistance.IGNORED\n\n            if host in list(dc_hosts)[:self.used_hosts_per_remote_dc]:\n                return HostDistance.REMOTE\n            else:\n                return HostDistance.IGNORED\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        # not thread-safe, but we don't care much about lost increments\n        # for the purposes of load balancing\n        pos = self._position\n        self._position += 1\n\n        local_live = self._dc_live_hosts.get(self.local_dc, ())\n        pos = (pos % len(local_live)) if local_live else 0\n        for host in islice(cycle(local_live), pos, pos + len(local_live)):\n            yield host\n\n        # the dict can change, so get candidate DCs iterating over keys of a copy\n        other_dcs = [dc for dc in self._dc_live_hosts.copy().keys() if dc != self.local_dc]\n        for dc in other_dcs:\n            remote_live = self._dc_live_hosts.get(dc, ())\n            for host in remote_live[:self.used_hosts_per_remote_dc]:\n                yield host\n\n    def on_up(self, host):\n        # not worrying about threads because this will happen during\n        # control connection startup/refresh\n        if not self.local_dc and host.datacenter:\n            if host.endpoint in self._endpoints:\n                self.local_dc = host.datacenter\n                log.info(\"Using datacenter '%s' for DCAwareRoundRobinPolicy (via host '%s'); \"\n                         \"if incorrect, please specify a local_dc to the constructor, \"\n                         \"or limit contact points to local cluster nodes\" %\n                         (self.local_dc, host.endpoint))\n                del self._endpoints\n\n        dc = self._dc(host)\n        with self._hosts_lock:\n            current_hosts = self._dc_live_hosts.get(dc, ())\n            if host not in current_hosts:\n                self._dc_live_hosts[dc] = current_hosts + (host, )\n\n    def on_down(self, host):\n        dc = self._dc(host)\n        with self._hosts_lock:\n            current_hosts = self._dc_live_hosts.get(dc, ())\n            if host in current_hosts:\n                hosts = tuple(h for h in current_hosts if h != host)\n                if hosts:\n                    self._dc_live_hosts[dc] = hosts\n                else:\n                    del self._dc_live_hosts[dc]\n\n    def on_add(self, host):\n        self.on_up(host)\n\n    def on_remove(self, host):\n        self.on_down(host)\n\nclass RackAwareRoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    Similar to :class:`.DCAwareRoundRobinPolicy`, but prefers hosts\n    in the local rack, before hosts in the local datacenter but a\n    different rack, before hosts in all other datercentres\n    \"\"\"\n\n    local_dc = None\n    local_rack = None\n    used_hosts_per_remote_dc = 0\n\n    def __init__(self, local_dc, local_rack, used_hosts_per_remote_dc=0):\n        \"\"\"\n        The `local_dc` and `local_rack` parameters should be the name of the\n        datacenter and rack (such as is reported by ``nodetool ring``) that\n        should be considered local.\n\n        `used_hosts_per_remote_dc` controls how many nodes in\n        each remote datacenter will have connections opened\n        against them. In other words, `used_hosts_per_remote_dc` hosts\n        will be considered :attr:`~.HostDistance.REMOTE` and the\n        rest will be considered :attr:`~.HostDistance.IGNORED`.\n        By default, all remote hosts are ignored.\n        \"\"\"\n        self.local_rack = local_rack\n        self.local_dc = local_dc\n        self.used_hosts_per_remote_dc = used_hosts_per_remote_dc\n        self._live_hosts = {}\n        self._dc_live_hosts = {}\n        self._endpoints = []\n        self._position = 0\n        LoadBalancingPolicy.__init__(self)\n\n    def _rack(self, host):\n        return host.rack or self.local_rack\n\n    def _dc(self, host):\n        return host.datacenter or self.local_dc\n\n    def populate(self, cluster, hosts):\n        for (dc, rack), rack_hosts in groupby(hosts, lambda host: (self._dc(host), self._rack(host))):\n            self._live_hosts[(dc, rack)] = tuple(set(rack_hosts))\n        for dc, dc_hosts in groupby(hosts, lambda host: self._dc(host)):\n            self._dc_live_hosts[dc] = tuple(set(dc_hosts))\n\n        self._position = randint(0, len(hosts) - 1) if hosts else 0\n\n    def distance(self, host):\n        rack = self._rack(host)\n        dc = self._dc(host)\n        if rack == self.local_rack and dc == self.local_dc:\n            return HostDistance.LOCAL_RACK\n\n        if dc == self.local_dc:\n            return HostDistance.LOCAL\n\n        if not self.used_hosts_per_remote_dc:\n            return HostDistance.IGNORED\n\n        dc_hosts = self._dc_live_hosts.get(dc, ())\n        if not dc_hosts:\n            return HostDistance.IGNORED\n        if host in dc_hosts and dc_hosts.index(host) < self.used_hosts_per_remote_dc:\n            return HostDistance.REMOTE\n        else:\n            return HostDistance.IGNORED\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        pos = self._position\n        self._position += 1\n\n        local_rack_live = self._live_hosts.get((self.local_dc, self.local_rack), ())\n        pos = (pos % len(local_rack_live)) if local_rack_live else 0\n        # Slice the cyclic iterator to start from pos and include the next len(local_live) elements\n        # This ensures we get exactly one full cycle starting from pos\n        for host in islice(cycle(local_rack_live), pos, pos + len(local_rack_live)):\n            yield host\n\n        local_live = [host for host in self._dc_live_hosts.get(self.local_dc, ()) if host.rack != self.local_rack]\n        pos = (pos % len(local_live)) if local_live else 0\n        for host in islice(cycle(local_live), pos, pos + len(local_live)):\n            yield host\n\n        # the dict can change, so get candidate DCs iterating over keys of a copy\n        for dc, remote_live in self._dc_live_hosts.copy().items():\n            if dc != self.local_dc:\n                for host in remote_live[:self.used_hosts_per_remote_dc]:\n                    yield host\n\n    def on_up(self, host):\n        dc = self._dc(host)\n        rack = self._rack(host)\n        with self._hosts_lock:\n            current_rack_hosts = self._live_hosts.get((dc, rack), ())\n            if host not in current_rack_hosts:\n                self._live_hosts[(dc, rack)] = current_rack_hosts + (host, )\n            current_dc_hosts = self._dc_live_hosts.get(dc, ())\n            if host not in current_dc_hosts:\n                self._dc_live_hosts[dc] = current_dc_hosts + (host, )\n\n    def on_down(self, host):\n        dc = self._dc(host)\n        rack = self._rack(host)\n        with self._hosts_lock:\n            current_rack_hosts = self._live_hosts.get((dc, rack), ())\n            if host in current_rack_hosts:\n                hosts = tuple(h for h in current_rack_hosts if h != host)\n                if hosts:\n                    self._live_hosts[(dc, rack)] = hosts\n                else:\n                    del self._live_hosts[(dc, rack)]\n            current_dc_hosts = self._dc_live_hosts.get(dc, ())\n            if host in current_dc_hosts:\n                hosts = tuple(h for h in current_dc_hosts if h != host)\n                if hosts:\n                    self._dc_live_hosts[dc] = hosts\n                else:\n                    del self._dc_live_hosts[dc]\n\n    def on_add(self, host):\n        self.on_up(host)\n\n    def on_remove(self, host):\n        self.on_down(host)\n\nclass TokenAwarePolicy(LoadBalancingPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` wrapper that adds token awareness to\n    a child policy.\n\n    This alters the child policy's behavior so that it first attempts to\n    send queries to :attr:`~.HostDistance.LOCAL` replicas (as determined\n    by the child policy) based on the :class:`.Statement`'s\n    :attr:`~.Statement.routing_key`. If :attr:`.shuffle_replicas` is\n    truthy, these replicas will be yielded in a random order. Once those\n    hosts are exhausted, the remaining hosts in the child policy's query\n    plan will be used in the order provided by the child policy.\n\n    If no :attr:`~.Statement.routing_key` is set on the query, the child\n    policy's query plan will be used as is.\n    \"\"\"\n\n    _child_policy = None\n    _cluster_metadata = None\n    _tablets_routing_v1 = False\n    shuffle_replicas = False\n    \"\"\"\n    Yield local replicas in a random order.\n    \"\"\"\n\n    def __init__(self, child_policy, shuffle_replicas=False):\n        self._child_policy = child_policy\n        self.shuffle_replicas = shuffle_replicas\n\n    def populate(self, cluster, hosts):\n        self._cluster_metadata = cluster.metadata\n        self._tablets_routing_v1 = cluster.control_connection._tablets_routing_v1\n        self._child_policy.populate(cluster, hosts)\n\n    def check_supported(self):\n        if not self._cluster_metadata.can_support_partitioner():\n            raise RuntimeError(\n                '%s cannot be used with the cluster partitioner (%s) because '\n                'the relevant C extension for this driver was not compiled. '\n                'See the installation instructions for details on building '\n                'and installing the C extensions.' %\n                (self.__class__.__name__, self._cluster_metadata.partitioner))\n\n    def distance(self, *args, **kwargs):\n        return self._child_policy.distance(*args, **kwargs)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        keyspace = query.keyspace if query and query.keyspace else working_keyspace\n\n        child = self._child_policy\n        if query is None or query.routing_key is None or keyspace is None:\n            for host in child.make_query_plan(keyspace, query):\n                yield host\n            return\n\n        replicas = []\n        if self._tablets_routing_v1:\n            tablet = self._cluster_metadata._tablets.get_tablet_for_key(\n                keyspace, query.table, self._cluster_metadata.token_map.token_class.from_key(query.routing_key))\n\n            if tablet is not None:\n                replicas_mapped = set(map(lambda r: r[0], tablet.replicas))\n                child_plan = child.make_query_plan(keyspace, query)\n\n                replicas = [host for host in child_plan if host.host_id in replicas_mapped]\n\n        if not replicas:\n            replicas = self._cluster_metadata.get_replicas(keyspace, query.routing_key)\n\n        if self.shuffle_replicas:\n            shuffle(replicas)\n\n        for replica in replicas:\n            if replica.is_up and child.distance(replica) in [HostDistance.LOCAL, HostDistance.LOCAL_RACK]:\n                yield replica\n\n        for host in child.make_query_plan(keyspace, query):\n            # skip if we've already listed this host\n            if host not in replicas or child.distance(host) == HostDistance.REMOTE:\n                yield host\n\n    def on_up(self, *args, **kwargs):\n        return self._child_policy.on_up(*args, **kwargs)\n\n    def on_down(self, *args, **kwargs):\n        return self._child_policy.on_down(*args, **kwargs)\n\n    def on_add(self, *args, **kwargs):\n        return self._child_policy.on_add(*args, **kwargs)\n\n    def on_remove(self, *args, **kwargs):\n        return self._child_policy.on_remove(*args, **kwargs)\n\n\nclass WhiteListRoundRobinPolicy(RoundRobinPolicy):\n    \"\"\"\n    A subclass of :class:`.RoundRobinPolicy` which evenly\n    distributes queries across all nodes in the cluster,\n    regardless of what datacenter the nodes may be in, but\n    only if that node exists in the list of allowed nodes\n\n    This policy is addresses the issue described in\n    https://datastax-oss.atlassian.net/browse/JAVA-145\n    Where connection errors occur when connection\n    attempts are made to private IP addresses remotely\n    \"\"\"\n\n    def __init__(self, hosts):\n        \"\"\"\n        The `hosts` parameter should be a sequence of hosts to permit\n        connections to.\n        \"\"\"\n        self._allowed_hosts = tuple(hosts)\n        self._allowed_hosts_resolved = []\n        for h in self._allowed_hosts:\n            unix_socket_path = getattr(h, \"_unix_socket_path\", None)\n            if unix_socket_path:\n                self._allowed_hosts_resolved.append(unix_socket_path)\n            else:\n                self._allowed_hosts_resolved.extend([endpoint[4][0]\n                                        for endpoint in socket.getaddrinfo(h, None, socket.AF_UNSPEC, socket.SOCK_STREAM)])\n\n        RoundRobinPolicy.__init__(self)\n\n    def populate(self, cluster, hosts):\n        self._live_hosts = frozenset(h for h in hosts if h.address in self._allowed_hosts_resolved)\n\n        if len(hosts) <= 1:\n            self._position = 0\n        else:\n            self._position = randint(0, len(hosts) - 1)\n\n    def distance(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            return HostDistance.LOCAL\n        else:\n            return HostDistance.IGNORED\n\n    def on_up(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            RoundRobinPolicy.on_up(self, host)\n\n    def on_add(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            RoundRobinPolicy.on_add(self, host)\n\n\nclass HostFilterPolicy(LoadBalancingPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` subclass configured with a child policy,\n    and a single-argument predicate. This policy defers to the child policy for\n    hosts where ``predicate(host)`` is truthy. Hosts for which\n    ``predicate(host)`` is falsy will be considered :attr:`.IGNORED`, and will\n    not be used in a query plan.\n\n    This can be used in the cases where you need a whitelist or blacklist\n    policy, e.g. to prepare for decommissioning nodes or for testing:\n\n    .. code-block:: python\n\n        def address_is_ignored(host):\n            return host.address in [ignored_address0, ignored_address1]\n\n        blacklist_filter_policy = HostFilterPolicy(\n            child_policy=RoundRobinPolicy(),\n            predicate=address_is_ignored\n        )\n\n        cluster = Cluster(\n            primary_host,\n            load_balancing_policy=blacklist_filter_policy,\n        )\n\n    See the note in the :meth:`.make_query_plan` documentation for a caveat on\n    how wrapping ordering polices (e.g. :class:`.RoundRobinPolicy`) may break\n    desirable properties of the wrapped policy.\n\n    Please note that whitelist and blacklist policies are not recommended for\n    general, day-to-day use. You probably want something like\n    :class:`.DCAwareRoundRobinPolicy`, which prefers a local DC but has\n    fallbacks, over a brute-force method like whitelisting or blacklisting.\n    \"\"\"\n\n    def __init__(self, child_policy, predicate):\n        \"\"\"\n        :param child_policy: an instantiated :class:`.LoadBalancingPolicy`\n                             that this one will defer to.\n        :param predicate: a one-parameter function that takes a :class:`.Host`.\n                          If it returns a falsy value, the :class:`.Host` will\n                          be :attr:`.IGNORED` and not returned in query plans.\n        \"\"\"\n        super(HostFilterPolicy, self).__init__()\n        self._child_policy = child_policy\n        self._predicate = predicate\n\n    def on_up(self, host, *args, **kwargs):\n        return self._child_policy.on_up(host, *args, **kwargs)\n\n    def on_down(self, host, *args, **kwargs):\n        return self._child_policy.on_down(host, *args, **kwargs)\n\n    def on_add(self, host, *args, **kwargs):\n        return self._child_policy.on_add(host, *args, **kwargs)\n\n    def on_remove(self, host, *args, **kwargs):\n        return self._child_policy.on_remove(host, *args, **kwargs)\n\n    @property\n    def predicate(self):\n        \"\"\"\n        A predicate, set on object initialization, that takes a :class:`.Host`\n        and returns a value. If the value is falsy, the :class:`.Host` is\n        :class:`~HostDistance.IGNORED`. If the value is truthy,\n        :class:`.HostFilterPolicy` defers to the child policy to determine the\n        host's distance.\n\n        This is a read-only value set in ``__init__``, implemented as a\n        ``property``.\n        \"\"\"\n        return self._predicate\n\n    def distance(self, host):\n        \"\"\"\n        Checks if ``predicate(host)``, then returns\n        :attr:`~HostDistance.IGNORED` if falsy, and defers to the child policy\n        otherwise.\n        \"\"\"\n        if self.predicate(host):\n            return self._child_policy.distance(host)\n        else:\n            return HostDistance.IGNORED\n\n    def populate(self, cluster, hosts):\n        self._child_policy.populate(cluster=cluster, hosts=hosts)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        \"\"\"\n        Defers to the child policy's\n        :meth:`.LoadBalancingPolicy.make_query_plan` and filters the results.\n\n        Note that this filtering may break desirable properties of the wrapped\n        policy in some cases. For instance, imagine if you configure this\n        policy to filter out ``host2``, and to wrap a round-robin policy that\n        rotates through three hosts in the order ``host1, host2, host3``,\n        ``host2, host3, host1``, ``host3, host1, host2``, repeating. This\n        policy will yield ``host1, host3``, ``host3, host1``, ``host3, host1``,\n        disproportionately favoring ``host3``.\n        \"\"\"\n        child_qp = self._child_policy.make_query_plan(\n            working_keyspace=working_keyspace, query=query\n        )\n        for host in child_qp:\n            if self.predicate(host):\n                yield host\n\n    def check_supported(self):\n        return self._child_policy.check_supported()\n\n\nclass ConvictionPolicy(object):\n    \"\"\"\n    A policy which decides when hosts should be considered down\n    based on the types of failures and the number of failures.\n\n    If custom behavior is needed, this class may be subclassed.\n    \"\"\"\n\n    def __init__(self, host):\n        \"\"\"\n        `host` is an instance of :class:`.Host`.\n        \"\"\"\n        self.host = host\n\n    def add_failure(self, connection_exc):\n        \"\"\"\n        Implementations should return :const:`True` if the host should be\n        convicted, :const:`False` otherwise.\n        \"\"\"\n        raise NotImplementedError()\n\n    def reset(self):\n        \"\"\"\n        Implementations should clear out any convictions or state regarding\n        the host.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass SimpleConvictionPolicy(ConvictionPolicy):\n    \"\"\"\n    The default implementation of :class:`ConvictionPolicy`,\n    which simply marks a host as down after the first failure\n    of any kind.\n    \"\"\"\n\n    def add_failure(self, connection_exc):\n        return not isinstance(connection_exc, OperationTimedOut)\n\n    def reset(self):\n        pass\n\n\nclass ReconnectionPolicy(object):\n    \"\"\"\n    This class and its subclasses govern how frequently an attempt is made\n    to reconnect to nodes that are marked as dead.\n\n    If custom behavior is needed, this class may be subclassed.\n    \"\"\"\n\n    def new_schedule(self):\n        \"\"\"\n        This should return a finite or infinite iterable of delays (each as a\n        floating point number of seconds) in-between each failed reconnection\n        attempt.  Note that if the iterable is finite, reconnection attempts\n        will cease once the iterable is exhausted.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass ConstantReconnectionPolicy(ReconnectionPolicy):\n    \"\"\"\n    A :class:`.ReconnectionPolicy` subclass which sleeps for a fixed delay\n    in-between each reconnection attempt.\n    \"\"\"\n\n    def __init__(self, delay, max_attempts=64):\n        \"\"\"\n        `delay` should be a floating point number of seconds to wait in-between\n        each attempt.\n\n        `max_attempts` should be a total number of attempts to be made before\n        giving up, or :const:`None` to continue reconnection attempts forever.\n        The default is 64.\n        \"\"\"\n        if delay < 0:\n            raise ValueError(\"delay must not be negative\")\n        if max_attempts is not None and max_attempts < 0:\n            raise ValueError(\"max_attempts must not be negative\")\n\n        self.delay = delay\n        self.max_attempts = max_attempts\n\n    def new_schedule(self):\n        if self.max_attempts:\n            return repeat(self.delay, self.max_attempts)\n        return repeat(self.delay)\n\n\nclass ExponentialReconnectionPolicy(ReconnectionPolicy):\n    \"\"\"\n    A :class:`.ReconnectionPolicy` subclass which exponentially increases\n    the length of the delay in-between each reconnection attempt up to\n    a set maximum delay.\n\n    A random amount of jitter (+/- 15%) will be added to the pure exponential\n    delay value to avoid the situations where many reconnection handlers are\n    trying to reconnect at exactly the same time.\n    \"\"\"\n\n    # TODO: max_attempts is 64 to preserve legacy default behavior\n    # consider changing to None in major release to prevent the policy\n    # giving up forever\n    def __init__(self, base_delay, max_delay, max_attempts=64):\n        \"\"\"\n        `base_delay` and `max_delay` should be in floating point units of\n        seconds.\n\n        `max_attempts` should be a total number of attempts to be made before\n        giving up, or :const:`None` to continue reconnection attempts forever.\n        The default is 64.\n        \"\"\"\n        if base_delay < 0 or max_delay < 0:\n            raise ValueError(\"Delays may not be negative\")\n\n        if max_delay < base_delay:\n            raise ValueError(\"Max delay must be greater than base delay\")\n\n        if max_attempts is not None and max_attempts < 0:\n            raise ValueError(\"max_attempts must not be negative\")\n\n        self.base_delay = base_delay\n        self.max_delay = max_delay\n        self.max_attempts = max_attempts\n\n    def new_schedule(self):\n        i, overflowed = 0, False\n        while self.max_attempts is None or i < self.max_attempts:\n            if overflowed:\n                yield self.max_delay\n            else:\n                try:\n                    yield self._add_jitter(min(self.base_delay * (2 ** i), self.max_delay))\n                except OverflowError:\n                    overflowed = True\n                    yield self.max_delay\n\n            i += 1\n\n    # Adds -+ 15% to the delay provided\n    def _add_jitter(self, value):\n        jitter = randint(85, 115)\n        delay = (jitter * value) / 100\n        return min(max(self.base_delay, delay), self.max_delay)\n\n\nclass RetryPolicy(object):\n    \"\"\"\n    A policy that describes whether to retry, rethrow, or ignore coordinator\n    timeout and unavailable failures. These are failures reported from the\n    server side. Timeouts are configured by\n    `settings in cassandra.yaml <https://github.com/apache/cassandra/blob/cassandra-2.1.4/conf/cassandra.yaml#L568-L584>`_.\n    Unavailable failures occur when the coordinator cannot achieve the consistency\n    level for a request. For further information see the method descriptions\n    below.\n\n    To specify a default retry policy, set the\n    :attr:`.Cluster.default_retry_policy` attribute to an instance of this\n    class or one of its subclasses.\n\n    To specify a retry policy per query, set the :attr:`.Statement.retry_policy`\n    attribute to an instance of this class or one of its subclasses.\n\n    If custom behavior is needed for retrying certain operations,\n    this class may be subclassed.\n    \"\"\"\n\n    RETRY = 0\n    \"\"\"\n    This should be returned from the below methods if the operation\n    should be retried on the same connection.\n    \"\"\"\n\n    RETHROW = 1\n    \"\"\"\n    This should be returned from the below methods if the failure\n    should be propagated and no more retries attempted.\n    \"\"\"\n\n    IGNORE = 2\n    \"\"\"\n    This should be returned from the below methods if the failure\n    should be ignored but no more retries should be attempted.\n    \"\"\"\n\n    RETRY_NEXT_HOST = 3\n    \"\"\"\n    This should be returned from the below methods if the operation\n    should be retried on another connection.\n    \"\"\"\n\n    def on_read_timeout(self, query, consistency, required_responses,\n                        received_responses, data_retrieved, retry_num):\n        \"\"\"\n        This is called when a read operation times out from the coordinator's\n        perspective (i.e. a replica did not respond to the coordinator in time).\n        It should return a tuple with two items: one of the class enums (such\n        as :attr:`.RETRY`) and a :class:`.ConsistencyLevel` to retry the\n        operation at or :const:`None` to keep the same consistency level.\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        The `required_responses` and `received_responses` parameters describe\n        how many replicas needed to respond to meet the requested consistency\n        level and how many actually did respond before the coordinator timed\n        out the request. `data_retrieved` is a boolean indicating whether\n        any of those responses contained data (as opposed to just a digest).\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, operations will be retried at most once, and only if\n        a sufficient number of replicas responded (with data digests).\n        \"\"\"\n        if retry_num != 0:\n            return self.RETHROW, None\n        elif received_responses >= required_responses and not data_retrieved:\n            return self.RETRY, consistency\n        else:\n            return self.RETHROW, None\n\n    def on_write_timeout(self, query, consistency, write_type,\n                         required_responses, received_responses, retry_num):\n        \"\"\"\n        This is called when a write operation times out from the coordinator's\n        perspective (i.e. a replica did not respond to the coordinator in time).\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `write_type` is one of the :class:`.WriteType` enums describing the\n        type of write operation.\n\n        The `required_responses` and `received_responses` parameters describe\n        how many replicas needed to acknowledge the write to meet the requested\n        consistency level and how many replicas actually did acknowledge the\n        write before the coordinator timed out the request.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, failed write operations will retried at most once, and\n        they will only be retried if the `write_type` was\n        :attr:`~.WriteType.BATCH_LOG`.\n        \"\"\"\n        if retry_num != 0:\n            return self.RETHROW, None\n        elif write_type == WriteType.BATCH_LOG:\n            return self.RETRY, consistency\n        else:\n            return self.RETHROW, None\n\n    def on_unavailable(self, query, consistency, required_replicas, alive_replicas, retry_num):\n        \"\"\"\n        This is called when the coordinator node determines that a read or\n        write operation cannot be successful because the number of live\n        replicas are too low to meet the requested :class:`.ConsistencyLevel`.\n        This means that the read or write operation was never forwarded to\n        any replicas.\n\n        `query` is the :class:`.Statement` that failed.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `required_replicas` is the number of replicas that would have needed to\n        acknowledge the operation to meet the requested consistency level.\n        `alive_replicas` is the number of replicas that the coordinator\n        considered alive at the time of the request.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, if this is the first retry, it triggers a retry on the next\n        host in the query plan with the same consistency level. If this is not the\n        first retry, no retries will be attempted and the error will be re-raised.\n        \"\"\"\n        return (self.RETRY_NEXT_HOST, None) if retry_num == 0 else (self.RETHROW, None)\n\n    def on_request_error(self, query, consistency, error, retry_num):\n        \"\"\"\n        This is called when an unexpected error happens. This can be in the\n        following situations:\n\n        * On a connection error\n        * On server errors: overloaded, isBootstrapping, serverError, etc.\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `error` the instance of the exception.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, it triggers a retry on the next host in the query plan\n        with the same consistency level.\n        \"\"\"\n        # TODO revisit this for the next major\n        # To preserve the same behavior than before, we don't take retry_num into account\n        return self.RETRY_NEXT_HOST, None\n\n\nclass FallthroughRetryPolicy(RetryPolicy):\n    \"\"\"\n    A retry policy that never retries and always propagates failures to\n    the application.\n    \"\"\"\n\n    def on_read_timeout(self, *args, **kwargs):\n        return self.RETHROW, None\n\n    def on_write_timeout(self, *args, **kwargs):\n        return self.RETHROW, None\n\n    def on_unavailable(self, *args, **kwargs):\n        return self.RETHROW, None\n\n    def on_request_error(self, *args, **kwargs):\n        return self.RETHROW, None\n\n\nclass DowngradingConsistencyRetryPolicy(RetryPolicy):\n    \"\"\"\n    *Deprecated:* This retry policy will be removed in the next major release.\n\n    A retry policy that sometimes retries with a lower consistency level than\n    the one initially requested.\n\n    **BEWARE**: This policy may retry queries using a lower consistency\n    level than the one initially requested. By doing so, it may break\n    consistency guarantees. In other words, if you use this retry policy,\n    there are cases (documented below) where a read at :attr:`~.QUORUM`\n    *may not* see a preceding write at :attr:`~.QUORUM`. Do not use this\n    policy unless you have understood the cases where this can happen and\n    are ok with that. It is also recommended to subclass this class so\n    that queries that required a consistency level downgrade can be\n    recorded (so that repairs can be made later, etc).\n\n    This policy implements the same retries as :class:`.RetryPolicy`,\n    but on top of that, it also retries in the following cases:\n\n    * On a read timeout: if the number of replicas that responded is\n      greater than one but lower than is required by the requested\n      consistency level, the operation is retried at a lower consistency\n      level.\n    * On a write timeout: if the operation is an :attr:`~.UNLOGGED_BATCH`\n      and at least one replica acknowledged the write, the operation is\n      retried at a lower consistency level.  Furthermore, for other\n      write types, if at least one replica acknowledged the write, the\n      timeout is ignored.\n    * On an unavailable exception: if at least one replica is alive, the\n      operation is retried at a lower consistency level.\n\n    The reasoning behind this retry policy is as follows: if, based\n    on the information the Cassandra coordinator node returns, retrying the\n    operation with the initially requested consistency has a chance to\n    succeed, do it. Otherwise, if based on that information we know the\n    initially requested consistency level cannot be achieved currently, then:\n\n    * For writes, ignore the exception (thus silently failing the\n      consistency requirement) if we know the write has been persisted on at\n      least one replica.\n    * For reads, try reading at a lower consistency level (thus silently\n      failing the consistency requirement).\n\n    In other words, this policy implements the idea that if the requested\n    consistency level cannot be achieved, the next best thing for writes is\n    to make sure the data is persisted, and that reading something is better\n    than reading nothing, even if there is a risk of reading stale data.\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(DowngradingConsistencyRetryPolicy, self).__init__(*args, **kwargs)\n        warnings.warn('DowngradingConsistencyRetryPolicy is deprecated '\n                      'and will be removed in the next major release.',\n                      DeprecationWarning)\n\n    def _pick_consistency(self, num_responses):\n        if num_responses >= 3:\n            return self.RETRY, ConsistencyLevel.THREE\n        elif num_responses >= 2:\n            return self.RETRY, ConsistencyLevel.TWO\n        elif num_responses >= 1:\n            return self.RETRY, ConsistencyLevel.ONE\n        else:\n            return self.RETHROW, None\n\n    def on_read_timeout(self, query, consistency, required_responses,\n                        received_responses, data_retrieved, retry_num):\n        if retry_num != 0:\n            return self.RETHROW, None\n        elif ConsistencyLevel.is_serial(consistency):\n            # Downgrading does not make sense for a CAS read query\n            return self.RETHROW, None\n        elif received_responses < required_responses:\n            return self._pick_consistency(received_responses)\n        elif not data_retrieved:\n            return self.RETRY, consistency\n        else:\n            return self.RETHROW, None\n\n    def on_write_timeout(self, query, consistency, write_type,\n                         required_responses, received_responses, retry_num):\n        if retry_num != 0:\n            return self.RETHROW, None\n\n        if write_type in (WriteType.SIMPLE, WriteType.BATCH, WriteType.COUNTER):\n            if received_responses > 0:\n                # persisted on at least one replica\n                return self.IGNORE, None\n            else:\n                return self.RETHROW, None\n        elif write_type == WriteType.UNLOGGED_BATCH:\n            return self._pick_consistency(received_responses)\n        elif write_type == WriteType.BATCH_LOG:\n            return self.RETRY, consistency\n\n        return self.RETHROW, None\n\n    def on_unavailable(self, query, consistency, required_replicas, alive_replicas, retry_num):\n        if retry_num != 0:\n            return self.RETHROW, None\n        elif ConsistencyLevel.is_serial(consistency):\n            # failed at the paxos phase of a LWT, retry on the next host\n            return self.RETRY_NEXT_HOST, None\n        else:\n            return self._pick_consistency(alive_replicas)\n\n\nclass ExponentialBackoffRetryPolicy(RetryPolicy):\n    \"\"\"\n    A policy that do retries with exponential backoff\n    \"\"\"\n\n    def __init__(self, max_num_retries: float, min_interval: float = 0.1, max_interval: float = 10.0,\n                 *args, **kwargs):\n        \"\"\"\n        `max_num_retries` counts how many times the operation would be retried,\n        `min_interval` is the initial time in seconds to wait before first retry\n        `max_interval` is the maximum time to wait between retries\n        \"\"\"\n        self.min_interval = min_interval\n        self.max_num_retries = max_num_retries\n        self.max_interval = max_interval\n        super(ExponentialBackoffRetryPolicy).__init__(*args, **kwargs)\n\n    def _calculate_backoff(self, attempt: int):\n        delay = min(self.max_interval, self.min_interval * 2 ** attempt)\n        # add some jitter\n        delay += random.random() * self.min_interval - (self.min_interval / 2)\n        return delay\n\n    def on_read_timeout(self, query, consistency, required_responses,\n                        received_responses, data_retrieved, retry_num):\n        if retry_num < self.max_num_retries and received_responses >= required_responses and not data_retrieved:\n            return self.RETRY, consistency, self._calculate_backoff(retry_num)\n        else:\n            return self.RETHROW, None, None\n\n    def on_write_timeout(self, query, consistency, write_type,\n                         required_responses, received_responses, retry_num):\n        if retry_num < self.max_num_retries and write_type == WriteType.BATCH_LOG:\n            return self.RETRY, consistency, self._calculate_backoff(retry_num)\n        else:\n            return self.RETHROW, None, None\n\n    def on_unavailable(self, query, consistency, required_replicas,\n                       alive_replicas, retry_num):\n        if retry_num < self.max_num_retries:\n            return self.RETRY_NEXT_HOST, None, self._calculate_backoff(retry_num)\n        else:\n            return self.RETHROW, None, None\n\n    def on_request_error(self, query, consistency, error, retry_num):\n        if retry_num < self.max_num_retries:\n            return self.RETRY_NEXT_HOST, None, self._calculate_backoff(retry_num)\n        else:\n            return self.RETHROW, None, None\n\n\nclass AddressTranslator(object):\n    \"\"\"\n    Interface for translating cluster-defined endpoints.\n\n    The driver discovers nodes using server metadata and topology change events. Normally,\n    the endpoint defined by the server is the right way to connect to a node. In some environments,\n    these addresses may not be reachable, or not preferred (public vs. private IPs in cloud environments,\n    suboptimal routing, etc). This interface allows for translating from server defined endpoints to\n    preferred addresses for driver connections.\n\n    *Note:* :attr:`~Cluster.contact_points` provided while creating the :class:`~.Cluster` instance are not\n    translated using this mechanism -- only addresses received from Cassandra nodes are.\n    \"\"\"\n    def translate(self, addr):\n        \"\"\"\n        Accepts the node ip address, and returns a translated address to be used connecting to this node.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass IdentityTranslator(AddressTranslator):\n    \"\"\"\n    Returns the endpoint with no translation\n    \"\"\"\n    def translate(self, addr):\n        return addr\n\n\nclass EC2MultiRegionTranslator(AddressTranslator):\n    \"\"\"\n    Resolves private ips of the hosts in the same datacenter as the client, and public ips of hosts in other datacenters.\n    \"\"\"\n    def translate(self, addr):\n        \"\"\"\n        Reverse DNS the public broadcast_address, then lookup that hostname to get the AWS-resolved IP, which\n        will point to the private IP address within the same datacenter.\n        \"\"\"\n        # get family of this address so we translate to the same\n        family = socket.getaddrinfo(addr, 0, socket.AF_UNSPEC, socket.SOCK_STREAM)[0][0]\n        host = socket.getfqdn(addr)\n        for a in socket.getaddrinfo(host, 0, family, socket.SOCK_STREAM):\n            try:\n                return a[4][0]\n            except Exception:\n                pass\n        return addr\n\n\nclass SpeculativeExecutionPolicy(object):\n    \"\"\"\n    Interface for specifying speculative execution plans\n    \"\"\"\n\n    def new_plan(self, keyspace, statement):\n        \"\"\"\n        Returns\n\n        :param keyspace:\n        :param statement:\n        :return:\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass SpeculativeExecutionPlan(object):\n    def next_execution(self, host):\n        raise NotImplementedError()\n\n\nclass NoSpeculativeExecutionPlan(SpeculativeExecutionPlan):\n    def next_execution(self, host):\n        return -1\n\n\nclass NoSpeculativeExecutionPolicy(SpeculativeExecutionPolicy):\n\n    def new_plan(self, keyspace, statement):\n        return NoSpeculativeExecutionPlan()\n\n\nclass ConstantSpeculativeExecutionPolicy(SpeculativeExecutionPolicy):\n    \"\"\"\n    A speculative execution policy that sends a new query every X seconds (**delay**) for a maximum of Y attempts (**max_attempts**).\n    \"\"\"\n\n    def __init__(self, delay, max_attempts):\n        self.delay = delay\n        self.max_attempts = max_attempts\n\n    class ConstantSpeculativeExecutionPlan(SpeculativeExecutionPlan):\n        def __init__(self, delay, max_attempts):\n            self.delay = delay\n            self.remaining = max_attempts\n\n        def next_execution(self, host):\n            if self.remaining > 0:\n                self.remaining -= 1\n                return self.delay\n            else:\n                return -1\n\n    def new_plan(self, keyspace, statement):\n        return self.ConstantSpeculativeExecutionPlan(self.delay, self.max_attempts)\n\n\nclass WrapperPolicy(LoadBalancingPolicy):\n\n    def __init__(self, child_policy):\n        self._child_policy = child_policy\n\n    def distance(self, *args, **kwargs):\n        return self._child_policy.distance(*args, **kwargs)\n\n    def populate(self, cluster, hosts):\n        self._child_policy.populate(cluster, hosts)\n\n    def on_up(self, *args, **kwargs):\n        return self._child_policy.on_up(*args, **kwargs)\n\n    def on_down(self, *args, **kwargs):\n        return self._child_policy.on_down(*args, **kwargs)\n\n    def on_add(self, *args, **kwargs):\n        return self._child_policy.on_add(*args, **kwargs)\n\n    def on_remove(self, *args, **kwargs):\n        return self._child_policy.on_remove(*args, **kwargs)\n\n\nclass DefaultLoadBalancingPolicy(WrapperPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` wrapper that adds the ability to target a specific host first.\n\n    If no host is set on the query, the child policy's query plan will be used as is.\n    \"\"\"\n\n    _cluster_metadata = None\n\n    def populate(self, cluster, hosts):\n        self._cluster_metadata = cluster.metadata\n        self._child_policy.populate(cluster, hosts)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        if query and query.keyspace:\n            keyspace = query.keyspace\n        else:\n            keyspace = working_keyspace\n\n        # TODO remove next major since execute(..., host=XXX) is now available\n        addr = getattr(query, 'target_host', None) if query else None\n        target_host = self._cluster_metadata.get_host(addr)\n\n        child = self._child_policy\n        if target_host and target_host.is_up:\n            yield target_host\n            for h in child.make_query_plan(keyspace, query):\n                if h != target_host:\n                    yield h\n        else:\n            for h in child.make_query_plan(keyspace, query):\n                yield h\n\n\n# TODO for backward compatibility, remove in next major\nclass DSELoadBalancingPolicy(DefaultLoadBalancingPolicy):\n    \"\"\"\n    *Deprecated:* This will be removed in the next major release,\n    consider using :class:`.DefaultLoadBalancingPolicy`.\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(DSELoadBalancingPolicy, self).__init__(*args, **kwargs)\n        warnings.warn(\"DSELoadBalancingPolicy will be removed in 4.0. Consider using \"\n                      \"DefaultLoadBalancingPolicy.\", DeprecationWarning)\n\n\nclass NeverRetryPolicy(RetryPolicy):\n    def _rethrow(self, *args, **kwargs):\n        return self.RETHROW, None\n\n    on_read_timeout = _rethrow\n    on_write_timeout = _rethrow\n    on_unavailable = _rethrow\n\n\nColDesc = namedtuple('ColDesc', ['ks', 'table', 'col'])\n\nclass ColumnEncryptionPolicy(object):\n    \"\"\"\n    A policy enabling (mostly) transparent encryption and decryption of data before it is\n    sent to the cluster.\n\n    Key materials and other configurations are specified on a per-column basis.  This policy can\n    then be used by driver structures which are aware of the underlying columns involved in their\n    work.  In practice this includes the following cases:\n\n    * Prepared statements - data for columns specified by the cluster's policy will be transparently\n      encrypted before they are sent\n    * Rows returned from any query - data for columns specified by the cluster's policy will be\n      transparently decrypted before they are returned to the user\n\n    To enable this functionality, create an instance of this class (or more likely a subclass)\n    before creating a cluster.  This policy should then be configured and supplied to the Cluster\n    at creation time via the :attr:`.Cluster.column_encryption_policy` attribute.\n    \"\"\"\n\n    def encrypt(self, coldesc, obj_bytes):\n        \"\"\"\n        Encrypt the specified bytes using the cryptography materials for the specified column.\n        Largely used internally, although this could also be used to encrypt values supplied\n        to non-prepared statements in a way that is consistent with this policy.\n        \"\"\"\n        raise NotImplementedError()\n\n    def decrypt(self, coldesc, encrypted_bytes):\n        \"\"\"\n        Decrypt the specified (encrypted) bytes using the cryptography materials for the\n        specified column.  Used internally; could be used externally as well but there's\n        not currently an obvious use case.\n        \"\"\"\n        raise NotImplementedError()\n\n    def add_column(self, coldesc, key):\n        \"\"\"\n        Provide cryptography materials to be used when encrypted and/or decrypting data\n        for the specified column.\n        \"\"\"\n        raise NotImplementedError()\n\n    def contains_column(self, coldesc):\n        \"\"\"\n        Predicate to determine if a specific column is supported by this policy.\n        Currently only used internally.\n        \"\"\"\n        raise NotImplementedError()\n\n    def encode_and_encrypt(self, coldesc, obj):\n        \"\"\"\n        Helper function to enable use of this policy on simple (i.e. non-prepared)\n        statements.\n        \"\"\"\n        raise NotImplementedError()\n",
    "cassandra/encoder.py": "# Copyright DataStax, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThese functions are used to convert Python objects into CQL strings.\nWhen non-prepared statements are executed, these encoder functions are\ncalled on each query parameter.\n\"\"\"\n\nimport logging\nlog = logging.getLogger(__name__)\n\nfrom binascii import hexlify\nimport calendar\nimport datetime\nimport math\nimport sys\nimport types\nfrom uuid import UUID\nimport ipaddress\n\nfrom cassandra.util import (OrderedDict, OrderedMap, OrderedMapSerializedKey,\n                            sortedset, Time, Date, Point, LineString, Polygon)\n\n\ndef cql_quote(term):\n    if isinstance(term, str):\n        return \"'%s'\" % str(term).replace(\"'\", \"''\")\n    else:\n        return str(term)\n\n\nclass ValueSequence(list):\n    pass\n\n\nclass Encoder(object):\n    \"\"\"\n    A container for mapping python types to CQL string literals when working\n    with non-prepared statements.  The type :attr:`~.Encoder.mapping` can be\n    directly customized by users.\n    \"\"\"\n\n    mapping = None\n    \"\"\"\n    A map of python types to encoder functions.\n    \"\"\"\n\n    def __init__(self):\n        self.mapping = {\n            float: self.cql_encode_float,\n            bytearray: self.cql_encode_bytes,\n            str: self.cql_encode_str,\n            int: self.cql_encode_object,\n            UUID: self.cql_encode_object,\n            datetime.datetime: self.cql_encode_datetime,\n            datetime.date: self.cql_encode_date,\n            datetime.time: self.cql_encode_time,\n            Date: self.cql_encode_date_ext,\n            Time: self.cql_encode_time,\n            dict: self.cql_encode_map_collection,\n            OrderedDict: self.cql_encode_map_collection,\n            OrderedMap: self.cql_encode_map_collection,\n            OrderedMapSerializedKey: self.cql_encode_map_collection,\n            list: self.cql_encode_list_collection,\n            tuple: self.cql_encode_list_collection,  # TODO: change to tuple in next major\n            set: self.cql_encode_set_collection,\n            sortedset: self.cql_encode_set_collection,\n            frozenset: self.cql_encode_set_collection,\n            types.GeneratorType: self.cql_encode_list_collection,\n            ValueSequence: self.cql_encode_sequence,\n            Point: self.cql_encode_str_quoted,\n            LineString: self.cql_encode_str_quoted,\n            Polygon: self.cql_encode_str_quoted\n        }\n\n        self.mapping.update({\n            memoryview: self.cql_encode_bytes,\n            bytes: self.cql_encode_bytes,\n            type(None): self.cql_encode_none,\n            ipaddress.IPv4Address: self.cql_encode_ipaddress,\n            ipaddress.IPv6Address: self.cql_encode_ipaddress\n        })\n\n    def cql_encode_none(self, val):\n        \"\"\"\n        Converts :const:`None` to the string 'NULL'.\n        \"\"\"\n        return 'NULL'\n\n    def cql_encode_unicode(self, val):\n        \"\"\"\n        Converts :class:`unicode` objects to UTF-8 encoded strings with quote escaping.\n        \"\"\"\n        return cql_quote(val.encode('utf-8'))\n\n    def cql_encode_str(self, val):\n        \"\"\"\n        Escapes quotes in :class:`str` objects.\n        \"\"\"\n        return cql_quote(val)\n\n    def cql_encode_str_quoted(self, val):\n        return \"'%s'\" % val\n\n    def cql_encode_bytes(self, val):\n        return (b'0x' + hexlify(val)).decode('utf-8')\n\n    def cql_encode_object(self, val):\n        \"\"\"\n        Default encoder for all objects that do not have a specific encoder function\n        registered. This function simply calls :meth:`str()` on the object.\n        \"\"\"\n        return str(val)\n\n    def cql_encode_float(self, val):\n        \"\"\"\n        Encode floats using repr to preserve precision\n        \"\"\"\n        if math.isinf(val):\n            return 'Infinity' if val > 0 else '-Infinity'\n        elif math.isnan(val):\n            return 'NaN'\n        else:\n            return repr(val)\n\n    def cql_encode_datetime(self, val):\n        \"\"\"\n        Converts a :class:`datetime.datetime` object to a (string) integer timestamp\n        with millisecond precision.\n        \"\"\"\n        timestamp = calendar.timegm(val.utctimetuple())\n        return str(int(timestamp * 1e3 + getattr(val, 'microsecond', 0) / 1e3))\n\n    def cql_encode_date(self, val):\n        \"\"\"\n        Converts a :class:`datetime.date` object to a string with format\n        ``YYYY-MM-DD``.\n        \"\"\"\n        return \"'%s'\" % val.strftime('%Y-%m-%d')\n\n    def cql_encode_time(self, val):\n        \"\"\"\n        Converts a :class:`cassandra.util.Time` object to a string with format\n        ``HH:MM:SS.mmmuuunnn``.\n        \"\"\"\n        return \"'%s'\" % val\n\n    def cql_encode_date_ext(self, val):\n        \"\"\"\n        Encodes a :class:`cassandra.util.Date` object as an integer\n        \"\"\"\n        # using the int form in case the Date exceeds datetime.[MIN|MAX]YEAR\n        return str(val.days_from_epoch + 2 ** 31)\n\n    def cql_encode_sequence(self, val):\n        \"\"\"\n        Converts a sequence to a string of the form ``(item1, item2, ...)``.  This\n        is suitable for ``IN`` value lists.\n        \"\"\"\n        return '(%s)' % ', '.join(self.mapping.get(type(v), self.cql_encode_object)(v)\n                                     for v in val)\n\n    cql_encode_tuple = cql_encode_sequence\n    \"\"\"\n    Converts a sequence to a string of the form ``(item1, item2, ...)``.  This\n    is suitable for ``tuple`` type columns.\n    \"\"\"\n\n    def cql_encode_map_collection(self, val):\n        \"\"\"\n        Converts a dict into a string of the form ``{key1: val1, key2: val2, ...}``.\n        This is suitable for ``map`` type columns.\n        \"\"\"\n        return '{%s}' % ', '.join('%s: %s' % (\n            self.mapping.get(type(k), self.cql_encode_object)(k),\n            self.mapping.get(type(v), self.cql_encode_object)(v)\n        ) for k, v in val.items())\n\n    def cql_encode_list_collection(self, val):\n        \"\"\"\n        Converts a sequence to a string of the form ``[item1, item2, ...]``.  This\n        is suitable for ``list`` type columns.\n        \"\"\"\n        return '[%s]' % ', '.join(self.mapping.get(type(v), self.cql_encode_object)(v) for v in val)\n\n    def cql_encode_set_collection(self, val):\n        \"\"\"\n        Converts a sequence to a string of the form ``{item1, item2, ...}``.  This\n        is suitable for ``set`` type columns.\n        \"\"\"\n        return '{%s}' % ', '.join(self.mapping.get(type(v), self.cql_encode_object)(v) for v in val)\n\n    def cql_encode_all_types(self, val, as_text_type=False):\n        \"\"\"\n        Converts any type into a CQL string, defaulting to ``cql_encode_object``\n        if :attr:`~Encoder.mapping` does not contain an entry for the type.\n        \"\"\"\n        encoded = self.mapping.get(type(val), self.cql_encode_object)(val)\n        if as_text_type and not isinstance(encoded, str):\n            return encoded.decode('utf-8')\n        return encoded\n\n    def cql_encode_ipaddress(self, val):\n        \"\"\"\n        Converts an ipaddress (IPV4Address, IPV6Address) to a CQL string. This\n        is suitable for ``inet`` type columns.\n        \"\"\"\n        return \"'%s'\" % val.compressed\n",
    "cassandra/__init__.py": "# Copyright DataStax, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom enum import Enum\nimport logging\n\n\nclass NullHandler(logging.Handler):\n\n    def emit(self, record):\n        pass\n\nlogging.getLogger('cassandra').addHandler(NullHandler())\n\n__version_info__ = (3, 28, 0)\n__version__ = '.'.join(map(str, __version_info__))\n\n\nclass ConsistencyLevel(object):\n    \"\"\"\n    Spcifies how many replicas must respond for an operation to be considered\n    a success.  By default, ``ONE`` is used for all operations.\n    \"\"\"\n\n    ANY = 0\n    \"\"\"\n    Only requires that one replica receives the write *or* the coordinator\n    stores a hint to replay later. Valid only for writes.\n    \"\"\"\n\n    ONE = 1\n    \"\"\"\n    Only one replica needs to respond to consider the operation a success\n    \"\"\"\n\n    TWO = 2\n    \"\"\"\n    Two replicas must respond to consider the operation a success\n    \"\"\"\n\n    THREE = 3\n    \"\"\"\n    Three replicas must respond to consider the operation a success\n    \"\"\"\n\n    QUORUM = 4\n    \"\"\"\n    ``ceil(RF/2) + 1`` replicas must respond to consider the operation a success\n    \"\"\"\n\n    ALL = 5\n    \"\"\"\n    All replicas must respond to consider the operation a success\n    \"\"\"\n\n    LOCAL_QUORUM = 6\n    \"\"\"\n    Requires a quorum of replicas in the local datacenter\n    \"\"\"\n\n    EACH_QUORUM = 7\n    \"\"\"\n    Requires a quorum of replicas in each datacenter\n    \"\"\"\n\n    SERIAL = 8\n    \"\"\"\n    For conditional inserts/updates that utilize Cassandra's lightweight\n    transactions, this requires consensus among all replicas for the\n    modified data.\n    \"\"\"\n\n    LOCAL_SERIAL = 9\n    \"\"\"\n    Like :attr:`~ConsistencyLevel.SERIAL`, but only requires consensus\n    among replicas in the local datacenter.\n    \"\"\"\n\n    LOCAL_ONE = 10\n    \"\"\"\n    Sends a request only to replicas in the local datacenter and waits for\n    one response.\n    \"\"\"\n\n    @staticmethod\n    def is_serial(cl):\n        return cl == ConsistencyLevel.SERIAL or cl == ConsistencyLevel.LOCAL_SERIAL\n\n\nConsistencyLevel.value_to_name = {\n    ConsistencyLevel.ANY: 'ANY',\n    ConsistencyLevel.ONE: 'ONE',\n    ConsistencyLevel.TWO: 'TWO',\n    ConsistencyLevel.THREE: 'THREE',\n    ConsistencyLevel.QUORUM: 'QUORUM',\n    ConsistencyLevel.ALL: 'ALL',\n    ConsistencyLevel.LOCAL_QUORUM: 'LOCAL_QUORUM',\n    ConsistencyLevel.EACH_QUORUM: 'EACH_QUORUM',\n    ConsistencyLevel.SERIAL: 'SERIAL',\n    ConsistencyLevel.LOCAL_SERIAL: 'LOCAL_SERIAL',\n    ConsistencyLevel.LOCAL_ONE: 'LOCAL_ONE'\n}\n\nConsistencyLevel.name_to_value = {\n    'ANY': ConsistencyLevel.ANY,\n    'ONE': ConsistencyLevel.ONE,\n    'TWO': ConsistencyLevel.TWO,\n    'THREE': ConsistencyLevel.THREE,\n    'QUORUM': ConsistencyLevel.QUORUM,\n    'ALL': ConsistencyLevel.ALL,\n    'LOCAL_QUORUM': ConsistencyLevel.LOCAL_QUORUM,\n    'EACH_QUORUM': ConsistencyLevel.EACH_QUORUM,\n    'SERIAL': ConsistencyLevel.SERIAL,\n    'LOCAL_SERIAL': ConsistencyLevel.LOCAL_SERIAL,\n    'LOCAL_ONE': ConsistencyLevel.LOCAL_ONE\n}\n\n\ndef consistency_value_to_name(value):\n    return ConsistencyLevel.value_to_name[value] if value is not None else \"Not Set\"\n\n\nclass ProtocolVersion(object):\n    \"\"\"\n    Defines native protocol versions supported by this driver.\n    \"\"\"\n    V1 = 1\n    \"\"\"\n    v1, supported in Cassandra 1.2-->2.2\n    \"\"\"\n\n    V2 = 2\n    \"\"\"\n    v2, supported in Cassandra 2.0-->2.2;\n    added support for lightweight transactions, batch operations, and automatic query paging.\n    \"\"\"\n\n    V3 = 3\n    \"\"\"\n    v3, supported in Cassandra 2.1-->3.x+;\n    added support for protocol-level client-side timestamps (see :attr:`.Session.use_client_timestamp`),\n    serial consistency levels for :class:`~.BatchStatement`, and an improved connection pool.\n    \"\"\"\n\n    V4 = 4\n    \"\"\"\n    v4, supported in Cassandra 2.2-->3.x+;\n    added a number of new types, server warnings, new failure messages, and custom payloads. Details in the\n    `project docs <https://github.com/apache/cassandra/blob/trunk/doc/native_protocol_v4.spec>`_\n    \"\"\"\n\n    V5 = 5\n    \"\"\"\n    v5, in beta from 3.x+. Finalised in 4.0-beta5\n    \"\"\"\n\n    V6 = 6\n    \"\"\"\n    v6, in beta from 4.0-beta5\n    \"\"\"\n\n    DSE_V1 = 0x41\n    \"\"\"\n    DSE private protocol v1, supported in DSE 5.1+\n    \"\"\"\n\n    DSE_V2 = 0x42\n    \"\"\"\n    DSE private protocol v2, supported in DSE 6.0+\n    \"\"\"\n\n    SUPPORTED_VERSIONS = (DSE_V2, DSE_V1, V6, V5, V4, V3, V2, V1)\n    \"\"\"\n    A tuple of all supported protocol versions\n    \"\"\"\n\n    BETA_VERSIONS = (V6,)\n    \"\"\"\n    A tuple of all beta protocol versions\n    \"\"\"\n\n    MIN_SUPPORTED = min(SUPPORTED_VERSIONS)\n    \"\"\"\n    Minimum protocol version supported by this driver.\n    \"\"\"\n\n    MAX_SUPPORTED = max(SUPPORTED_VERSIONS)\n    \"\"\"\n    Maximum protocol version supported by this driver.\n    \"\"\"\n\n    @classmethod\n    def get_lower_supported(cls, previous_version):\n        \"\"\"\n        Return the lower supported protocol version. Beta versions are omitted.\n        \"\"\"\n        try:\n            version = next(v for v in sorted(ProtocolVersion.SUPPORTED_VERSIONS, reverse=True) if\n                           v not in ProtocolVersion.BETA_VERSIONS and v < previous_version)\n        except StopIteration:\n            version = 0\n\n        return version\n\n    @classmethod\n    def uses_int_query_flags(cls, version):\n        return version >= cls.V5\n\n    @classmethod\n    def uses_prepare_flags(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def uses_prepared_metadata(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def uses_error_code_map(cls, version):\n        return version >= cls.V5\n\n    @classmethod\n    def uses_keyspace_flag(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def has_continuous_paging_support(cls, version):\n        return version >= cls.DSE_V1\n\n    @classmethod\n    def has_continuous_paging_next_pages(cls, version):\n        return version >= cls.DSE_V2\n\n    @classmethod\n    def has_checksumming_support(cls, version):\n        return cls.V5 <= version < cls.DSE_V1\n\n\nclass WriteType(object):\n    \"\"\"\n    For usage with :class:`.RetryPolicy`, this describe a type\n    of write operation.\n    \"\"\"\n\n    SIMPLE = 0\n    \"\"\"\n    A write to a single partition key. Such writes are guaranteed to be atomic\n    and isolated.\n    \"\"\"\n\n    BATCH = 1\n    \"\"\"\n    A write to multiple partition keys that used the distributed batch log to\n    ensure atomicity.\n    \"\"\"\n\n    UNLOGGED_BATCH = 2\n    \"\"\"\n    A write to multiple partition keys that did not use the distributed batch\n    log. Atomicity for such writes is not guaranteed.\n    \"\"\"\n\n    COUNTER = 3\n    \"\"\"\n    A counter write (for one or multiple partition keys). Such writes should\n    not be replayed in order to avoid overcount.\n    \"\"\"\n\n    BATCH_LOG = 4\n    \"\"\"\n    The initial write to the distributed batch log that Cassandra performs\n    internally before a BATCH write.\n    \"\"\"\n\n    CAS = 5\n    \"\"\"\n    A lighweight-transaction write, such as \"DELETE ... IF EXISTS\".\n    \"\"\"\n\n    VIEW = 6\n    \"\"\"\n    This WriteType is only seen in results for requests that were unable to\n    complete MV operations.\n    \"\"\"\n\n    CDC = 7\n    \"\"\"\n    This WriteType is only seen in results for requests that were unable to\n    complete CDC operations.\n    \"\"\"\n\n\nWriteType.name_to_value = {\n    'SIMPLE': WriteType.SIMPLE,\n    'BATCH': WriteType.BATCH,\n    'UNLOGGED_BATCH': WriteType.UNLOGGED_BATCH,\n    'COUNTER': WriteType.COUNTER,\n    'BATCH_LOG': WriteType.BATCH_LOG,\n    'CAS': WriteType.CAS,\n    'VIEW': WriteType.VIEW,\n    'CDC': WriteType.CDC\n}\n\n\nWriteType.value_to_name = {v: k for k, v in WriteType.name_to_value.items()}\n\n\nclass SchemaChangeType(object):\n    DROPPED = 'DROPPED'\n    CREATED = 'CREATED'\n    UPDATED = 'UPDATED'\n\n\nclass SchemaTargetType(object):\n    KEYSPACE = 'KEYSPACE'\n    TABLE = 'TABLE'\n    TYPE = 'TYPE'\n    FUNCTION = 'FUNCTION'\n    AGGREGATE = 'AGGREGATE'\n\n\nclass SignatureDescriptor(object):\n\n    def __init__(self, name, argument_types):\n        self.name = name\n        self.argument_types = argument_types\n\n    @property\n    def signature(self):\n        \"\"\"\n        function signature string in the form 'name([type0[,type1[...]]])'\n\n        can be used to uniquely identify overloaded function names within a keyspace\n        \"\"\"\n        return self.format_signature(self.name, self.argument_types)\n\n    @staticmethod\n    def format_signature(name, argument_types):\n        return \"%s(%s)\" % (name, ','.join(t for t in argument_types))\n\n    def __repr__(self):\n        return \"%s(%s, %s)\" % (self.__class__.__name__, self.name, self.argument_types)\n\n\nclass UserFunctionDescriptor(SignatureDescriptor):\n    \"\"\"\n    Describes a User function by name and argument signature\n    \"\"\"\n\n    name = None\n    \"\"\"\n    name of the function\n    \"\"\"\n\n    argument_types = None\n    \"\"\"\n    Ordered list of CQL argument type names comprising the type signature\n    \"\"\"\n\n\nclass UserAggregateDescriptor(SignatureDescriptor):\n    \"\"\"\n    Describes a User aggregate function by name and argument signature\n    \"\"\"\n\n    name = None\n    \"\"\"\n    name of the aggregate\n    \"\"\"\n\n    argument_types = None\n    \"\"\"\n    Ordered list of CQL argument type names comprising the type signature\n    \"\"\"\n\n\nclass DriverException(Exception):\n    \"\"\"\n    Base for all exceptions explicitly raised by the driver.\n    \"\"\"\n    pass\n\n\nclass RequestExecutionException(DriverException):\n    \"\"\"\n    Base for request execution exceptions returned from the server.\n    \"\"\"\n    pass\n\n\nclass Unavailable(RequestExecutionException):\n    \"\"\"\n    There were not enough live replicas to satisfy the requested consistency\n    level, so the coordinator node immediately failed the request without\n    forwarding it to any replicas.\n    \"\"\"\n\n    consistency = None\n    \"\"\" The requested :class:`ConsistencyLevel` \"\"\"\n\n    required_replicas = None\n    \"\"\" The number of replicas that needed to be live to complete the operation \"\"\"\n\n    alive_replicas = None\n    \"\"\" The number of replicas that were actually alive \"\"\"\n\n    def __init__(self, summary_message, consistency=None, required_replicas=None, alive_replicas=None):\n        self.consistency = consistency\n        self.required_replicas = required_replicas\n        self.alive_replicas = alive_replicas\n        Exception.__init__(self, summary_message + ' info=' +\n                           repr({'consistency': consistency_value_to_name(consistency),\n                                 'required_replicas': required_replicas,\n                                 'alive_replicas': alive_replicas}))\n\n\nclass Timeout(RequestExecutionException):\n    \"\"\"\n    Replicas failed to respond to the coordinator node before timing out.\n    \"\"\"\n\n    consistency = None\n    \"\"\" The requested :class:`ConsistencyLevel` \"\"\"\n\n    required_responses = None\n    \"\"\" The number of required replica responses \"\"\"\n\n    received_responses = None\n    \"\"\"\n    The number of replicas that responded before the coordinator timed out\n    the operation\n    \"\"\"\n\n    def __init__(self, summary_message, consistency=None, required_responses=None,\n                 received_responses=None, **kwargs):\n        self.consistency = consistency\n        self.required_responses = required_responses\n        self.received_responses = received_responses\n\n        if \"write_type\" in kwargs:\n            kwargs[\"write_type\"] = WriteType.value_to_name[kwargs[\"write_type\"]]\n\n        info = {'consistency': consistency_value_to_name(consistency),\n                'required_responses': required_responses,\n                'received_responses': received_responses}\n        info.update(kwargs)\n\n        Exception.__init__(self, summary_message + ' info=' + repr(info))\n\n\nclass ReadTimeout(Timeout):\n    \"\"\"\n    A subclass of :exc:`Timeout` for read operations.\n\n    This indicates that the replicas failed to respond to the coordinator\n    node before the configured timeout. This timeout is configured in\n    ``cassandra.yaml`` with the ``read_request_timeout_in_ms``\n    and ``range_request_timeout_in_ms`` options.\n    \"\"\"\n\n    data_retrieved = None\n    \"\"\"\n    A boolean indicating whether the requested data was retrieved\n    by the coordinator from any replicas before it timed out the\n    operation\n    \"\"\"\n\n    def __init__(self, message, data_retrieved=None, **kwargs):\n        Timeout.__init__(self, message, **kwargs)\n        self.data_retrieved = data_retrieved\n\n\nclass WriteTimeout(Timeout):\n    \"\"\"\n    A subclass of :exc:`Timeout` for write operations.\n\n    This indicates that the replicas failed to respond to the coordinator\n    node before the configured timeout. This timeout is configured in\n    ``cassandra.yaml`` with the ``write_request_timeout_in_ms``\n    option.\n    \"\"\"\n\n    write_type = None\n    \"\"\"\n    The type of write operation, enum on :class:`~cassandra.policies.WriteType`\n    \"\"\"\n\n    def __init__(self, message, write_type=None, **kwargs):\n        kwargs[\"write_type\"] = write_type\n        Timeout.__init__(self, message, **kwargs)\n        self.write_type = write_type\n\n\nclass CDCWriteFailure(RequestExecutionException):\n    \"\"\"\n    Hit limit on data in CDC folder, writes are rejected\n    \"\"\"\n    def __init__(self, message):\n        Exception.__init__(self, message)\n\n\nclass CoordinationFailure(RequestExecutionException):\n    \"\"\"\n    Replicas sent a failure to the coordinator.\n    \"\"\"\n\n    consistency = None\n    \"\"\" The requested :class:`ConsistencyLevel` \"\"\"\n\n    required_responses = None\n    \"\"\" The number of required replica responses \"\"\"\n\n    received_responses = None\n    \"\"\"\n    The number of replicas that responded before the coordinator timed out\n    the operation\n    \"\"\"\n\n    failures = None\n    \"\"\"\n    The number of replicas that sent a failure message\n    \"\"\"\n\n    error_code_map = None\n    \"\"\"\n    A map of inet addresses to error codes representing replicas that sent\n    a failure message.  Only set when `protocol_version` is 5 or higher.\n    \"\"\"\n\n    def __init__(self, summary_message, consistency=None, required_responses=None,\n                 received_responses=None, failures=None, error_code_map=None):\n        self.consistency = consistency\n        self.required_responses = required_responses\n        self.received_responses = received_responses\n        self.failures = failures\n        self.error_code_map = error_code_map\n\n        info_dict = {\n            'consistency': consistency_value_to_name(consistency),\n            'required_responses': required_responses,\n            'received_responses': received_responses,\n            'failures': failures\n        }\n\n        if error_code_map is not None:\n            # make error codes look like \"0x002a\"\n            formatted_map = dict((addr, '0x%04x' % err_code)\n                                 for (addr, err_code) in error_code_map.items())\n            info_dict['error_code_map'] = formatted_map\n\n        Exception.__init__(self, summary_message + ' info=' + repr(info_dict))\n\n\nclass ReadFailure(CoordinationFailure):\n    \"\"\"\n    A subclass of :exc:`CoordinationFailure` for read operations.\n\n    This indicates that the replicas sent a failure message to the coordinator.\n    \"\"\"\n\n    data_retrieved = None\n    \"\"\"\n    A boolean indicating whether the requested data was retrieved\n    by the coordinator from any replicas before it timed out the\n    operation\n    \"\"\"\n\n    def __init__(self, message, data_retrieved=None, **kwargs):\n        CoordinationFailure.__init__(self, message, **kwargs)\n        self.data_retrieved = data_retrieved\n\n\nclass WriteFailure(CoordinationFailure):\n    \"\"\"\n    A subclass of :exc:`CoordinationFailure` for write operations.\n\n    This indicates that the replicas sent a failure message to the coordinator.\n    \"\"\"\n\n    write_type = None\n    \"\"\"\n    The type of write operation, enum on :class:`~cassandra.policies.WriteType`\n    \"\"\"\n\n    def __init__(self, message, write_type=None, **kwargs):\n        CoordinationFailure.__init__(self, message, **kwargs)\n        self.write_type = write_type\n\n\nclass FunctionFailure(RequestExecutionException):\n    \"\"\"\n    User Defined Function failed during execution\n    \"\"\"\n\n    keyspace = None\n    \"\"\"\n    Keyspace of the function\n    \"\"\"\n\n    function = None\n    \"\"\"\n    Name of the function\n    \"\"\"\n\n    arg_types = None\n    \"\"\"\n    List of argument type names of the function\n    \"\"\"\n\n    def __init__(self, summary_message, keyspace, function, arg_types):\n        self.keyspace = keyspace\n        self.function = function\n        self.arg_types = arg_types\n        Exception.__init__(self, summary_message)\n\n\nclass RequestValidationException(DriverException):\n    \"\"\"\n    Server request validation failed\n    \"\"\"\n    pass\n\n\nclass ConfigurationException(RequestValidationException):\n    \"\"\"\n    Server indicated request errro due to current configuration\n    \"\"\"\n    pass\n\n\nclass AlreadyExists(ConfigurationException):\n    \"\"\"\n    An attempt was made to create a keyspace or table that already exists.\n    \"\"\"\n\n    keyspace = None\n    \"\"\"\n    The name of the keyspace that already exists, or, if an attempt was\n    made to create a new table, the keyspace that the table is in.\n    \"\"\"\n\n    table = None\n    \"\"\"\n    The name of the table that already exists, or, if an attempt was\n    make to create a keyspace, :const:`None`.\n    \"\"\"\n\n    def __init__(self, keyspace=None, table=None):\n        if table:\n            message = \"Table '%s.%s' already exists\" % (keyspace, table)\n        else:\n            message = \"Keyspace '%s' already exists\" % (keyspace,)\n\n        Exception.__init__(self, message)\n        self.keyspace = keyspace\n        self.table = table\n\n\nclass InvalidRequest(RequestValidationException):\n    \"\"\"\n    A query was made that was invalid for some reason, such as trying to set\n    the keyspace for a connection to a nonexistent keyspace.\n    \"\"\"\n    pass\n\n\nclass Unauthorized(RequestValidationException):\n    \"\"\"\n    The current user is not authorized to perform the requested operation.\n    \"\"\"\n    pass\n\n\nclass AuthenticationFailed(DriverException):\n    \"\"\"\n    Failed to authenticate.\n    \"\"\"\n    pass\n\n\nclass OperationTimedOut(DriverException):\n    \"\"\"\n    The operation took longer than the specified (client-side) timeout\n    to complete.  This is not an error generated by Cassandra, only\n    the driver.\n    \"\"\"\n\n    errors = None\n    \"\"\"\n    A dict of errors keyed by the :class:`~.Host` against which they occurred.\n    \"\"\"\n\n    last_host = None\n    \"\"\"\n    The last :class:`~.Host` this operation was attempted against.\n    \"\"\"\n\n    def __init__(self, errors=None, last_host=None):\n        self.errors = errors\n        self.last_host = last_host\n        message = \"errors=%s, last_host=%s\" % (self.errors, self.last_host)\n        Exception.__init__(self, message)\n\n\nclass UnsupportedOperation(DriverException):\n    \"\"\"\n    An attempt was made to use a feature that is not supported by the\n    selected protocol version.  See :attr:`Cluster.protocol_version`\n    for more details.\n    \"\"\"\n    pass\n\n\nclass UnresolvableContactPoints(DriverException):\n    \"\"\"\n    The driver was unable to resolve any provided hostnames.\n\n    Note that this is *not* raised when a :class:`.Cluster` is created with no\n    contact points, only when lookup fails for all hosts\n    \"\"\"\n    pass\n\n\nclass OperationType(Enum):\n    Read = 0\n    Write = 1\n\nclass RateLimitReached(ConfigurationException):\n    '''\n    Rate limit was exceeded for a partition affected by the request.\n    '''\n    op_type = None\n    rejected_by_coordinator = False\n\n    def __init__(self, op_type=None, rejected_by_coordinator=False):\n        self.op_type = op_type\n        self.rejected_by_coordinator = rejected_by_coordinator\n        message = f\"[request_error_rate_limit_reached OpType={op_type.name} RejectedByCoordinator={rejected_by_coordinator}]\"\n        Exception.__init__(self, message)\n"
  },
  "GT_src_dict": {
    "cassandra/query.py": {
      "PreparedStatement.__init__": {
        "code": "    def __init__(self, column_metadata, query_id, routing_key_indexes, query, keyspace, protocol_version, result_metadata, result_metadata_id, column_encryption_policy=None):\n        \"\"\"Initializes a `PreparedStatement` instance that represents a prepared Cassandra query with associated metadata.\n\nParameters:\n- `column_metadata`: Metadata about the columns in the prepared statement.\n- `query_id`: A unique identifier for the prepared query.\n- `routing_key_indexes`: Indices of the columns used for routing the query.\n- `query`: The CQL query string associated with the prepared statement.\n- `keyspace`: The keyspace this prepared statement operates on.\n- `protocol_version`: The version of the Cassandra protocol being used.\n- `result_metadata`: Metadata about the results returned by the prepared statement.\n- `result_metadata_id`: Identifier for the result metadata.\n- `column_encryption_policy`: Optional; specifies the policy for encrypting column values; defaults to `None`.\n\nAttributes:\n- `column_metadata`, `query_id`, `routing_key_indexes`, `query_string`, `keyspace`, `protocol_version`, `result_metadata`, and `result_metadata_id` are all set based on the provided parameters.\n- `column_encryption_policy` can be used to manage how column values are encrypted during queries.\n\nThis method is integral to the `PreparedStatement` class, allowing it to encapsulate the complexities of a prepared query while interfacing with other components of the Cassandra driver.\"\"\"\n        self.column_metadata = column_metadata\n        self.query_id = query_id\n        self.routing_key_indexes = routing_key_indexes\n        self.query_string = query\n        self.keyspace = keyspace\n        self.protocol_version = protocol_version\n        self.result_metadata = result_metadata\n        self.result_metadata_id = result_metadata_id\n        self.column_encryption_policy = column_encryption_policy\n        self.is_idempotent = False",
        "docstring": "Initializes a `PreparedStatement` instance that represents a prepared Cassandra query with associated metadata.\n\nParameters:\n- `column_metadata`: Metadata about the columns in the prepared statement.\n- `query_id`: A unique identifier for the prepared query.\n- `routing_key_indexes`: Indices of the columns used for routing the query.\n- `query`: The CQL query string associated with the prepared statement.\n- `keyspace`: The keyspace this prepared statement operates on.\n- `protocol_version`: The version of the Cassandra protocol being used.\n- `result_metadata`: Metadata about the results returned by the prepared statement.\n- `result_metadata_id`: Identifier for the result metadata.\n- `column_encryption_policy`: Optional; specifies the policy for encrypting column values; defaults to `None`.\n\nAttributes:\n- `column_metadata`, `query_id`, `routing_key_indexes`, `query_string`, `keyspace`, `protocol_version`, `result_metadata`, and `result_metadata_id` are all set based on the provided parameters.\n- `column_encryption_policy` can be used to manage how column values are encrypted during queries.\n\nThis method is integral to the `PreparedStatement` class, allowing it to encapsulate the complexities of a prepared query while interfacing with other components of the Cassandra driver.",
        "signature": "def __init__(self, column_metadata, query_id, routing_key_indexes, query, keyspace, protocol_version, result_metadata, result_metadata_id, column_encryption_policy=None):",
        "type": "Method",
        "class_signature": "class PreparedStatement(object):"
      },
      "PreparedStatement.bind": {
        "code": "    def bind(self, values):\n        \"\"\"Creates and returns a `BoundStatement` instance using the provided `values`. This method binds the specified values to the prepared statement's parameters, ensuring they are correctly serialized according to their types and the current protocol version.\n\nParameters:\n- `values`: A sequence (e.g., list, tuple) or a dictionary of values corresponding to the parameters in the prepared statement. If a dictionary is provided, the keys must match the column names. If some parameters are not included, they will be set to `UNSET_VALUE` (defined in the context as a constant indicating a value that should be ignored).\n\nReturns:\n- A `BoundStatement` instance, which can then be executed against the database.\n\nDependencies:\n- This method relies on the `BoundStatement` class and its `bind` method to bind the values to the prepared statement. If using protocol version 4 or higher, `UNSET_VALUE` allows for flexible binding without requiring all parameters.\"\"\"\n        '\\n        Creates and returns a :class:`BoundStatement` instance using `values`.\\n\\n        See :meth:`BoundStatement.bind` for rules on input ``values``.\\n        '\n        return BoundStatement(self).bind(values)",
        "docstring": "Creates and returns a `BoundStatement` instance using the provided `values`. This method binds the specified values to the prepared statement's parameters, ensuring they are correctly serialized according to their types and the current protocol version.\n\nParameters:\n- `values`: A sequence (e.g., list, tuple) or a dictionary of values corresponding to the parameters in the prepared statement. If a dictionary is provided, the keys must match the column names. If some parameters are not included, they will be set to `UNSET_VALUE` (defined in the context as a constant indicating a value that should be ignored).\n\nReturns:\n- A `BoundStatement` instance, which can then be executed against the database.\n\nDependencies:\n- This method relies on the `BoundStatement` class and its `bind` method to bind the values to the prepared statement. If using protocol version 4 or higher, `UNSET_VALUE` allows for flexible binding without requiring all parameters.",
        "signature": "def bind(self, values):",
        "type": "Method",
        "class_signature": "class PreparedStatement(object):"
      },
      "BoundStatement.__init__": {
        "code": "    def __init__(self, prepared_statement, retry_policy=None, consistency_level=None, routing_key=None, serial_consistency_level=None, fetch_size=FETCH_SIZE_UNSET, keyspace=None, custom_payload=None):\n        \"\"\"Initializes a BoundStatement instance that is bound to a specific set of values for a prepared statement.\n\nParameters:\n- prepared_statement (PreparedStatement): An instance of the PreparedStatement class that this BoundStatement is based on.\n- retry_policy (Optional[RetryPolicy]): An instance of a retry policy. If not provided, the policy from the prepared statement is used.\n- consistency_level (Optional[ConsistencyLevel]): Defines the consistency level for the operation. If not provided, the level from the prepared statement is used.\n- routing_key (Optional[Any]): A custom routing key to control which replicas process the query.\n- serial_consistency_level (Optional[ConsistencyLevel]): Specifies the serial consistency level, applicable only to certain operations. Defaults to the value in the prepared statement.\n- fetch_size (Optional[int]): Number of rows to fetch at a time, overriding the default fetch size. Defaults to FETCH_SIZE_UNSET.\n- keyspace (Optional[str]): The keyspace for the query. Defaults to the keyspace of the prepared_statement if available.\n- custom_payload (Optional[dict]): Custom payload data to send with the query. Defaults to the payload in the prepared statement.\n\nAttributes Initialized:\n- values (list): A list to hold the bound values for the prepared statement's parameters.\n- keyspace (str): Set to the keyspace of the first column of the prepared statement's metadata, if available.\n- table (str): Set to the table name of the first column of the prepared statement's metadata, if available.\n\nConstants:\n- FETCH_SIZE_UNSET: A sentinel value indicating that no fetch size has been explicitly set. This allows the default fetch size to be used when initializing the BoundStatement.\n\nThis constructor also calls the __init__ method of its superclass, Statement, to inherit common properties and behaviors.\"\"\"\n        '\\n        `prepared_statement` should be an instance of :class:`PreparedStatement`.\\n\\n        See :class:`Statement` attributes for a description of the other parameters.\\n        '\n        self.prepared_statement = prepared_statement\n        self.retry_policy = prepared_statement.retry_policy\n        self.consistency_level = prepared_statement.consistency_level\n        self.serial_consistency_level = prepared_statement.serial_consistency_level\n        self.fetch_size = prepared_statement.fetch_size\n        self.custom_payload = prepared_statement.custom_payload\n        self.is_idempotent = prepared_statement.is_idempotent\n        self.values = []\n        meta = prepared_statement.column_metadata\n        if meta:\n            self.keyspace = meta[0].keyspace_name\n            self.table = meta[0].table_name\n        Statement.__init__(self, retry_policy, consistency_level, routing_key, serial_consistency_level, fetch_size, keyspace, custom_payload, prepared_statement.is_idempotent)",
        "docstring": "Initializes a BoundStatement instance that is bound to a specific set of values for a prepared statement.\n\nParameters:\n- prepared_statement (PreparedStatement): An instance of the PreparedStatement class that this BoundStatement is based on.\n- retry_policy (Optional[RetryPolicy]): An instance of a retry policy. If not provided, the policy from the prepared statement is used.\n- consistency_level (Optional[ConsistencyLevel]): Defines the consistency level for the operation. If not provided, the level from the prepared statement is used.\n- routing_key (Optional[Any]): A custom routing key to control which replicas process the query.\n- serial_consistency_level (Optional[ConsistencyLevel]): Specifies the serial consistency level, applicable only to certain operations. Defaults to the value in the prepared statement.\n- fetch_size (Optional[int]): Number of rows to fetch at a time, overriding the default fetch size. Defaults to FETCH_SIZE_UNSET.\n- keyspace (Optional[str]): The keyspace for the query. Defaults to the keyspace of the prepared_statement if available.\n- custom_payload (Optional[dict]): Custom payload data to send with the query. Defaults to the payload in the prepared statement.\n\nAttributes Initialized:\n- values (list): A list to hold the bound values for the prepared statement's parameters.\n- keyspace (str): Set to the keyspace of the first column of the prepared statement's metadata, if available.\n- table (str): Set to the table name of the first column of the prepared statement's metadata, if available.\n\nConstants:\n- FETCH_SIZE_UNSET: A sentinel value indicating that no fetch size has been explicitly set. This allows the default fetch size to be used when initializing the BoundStatement.\n\nThis constructor also calls the __init__ method of its superclass, Statement, to inherit common properties and behaviors.",
        "signature": "def __init__(self, prepared_statement, retry_policy=None, consistency_level=None, routing_key=None, serial_consistency_level=None, fetch_size=FETCH_SIZE_UNSET, keyspace=None, custom_payload=None):",
        "type": "Method",
        "class_signature": "class BoundStatement(Statement):"
      },
      "BoundStatement.bind": {
        "code": "    def bind(self, values):\n        \"\"\"Binds a sequence of values to the parameters of the prepared statement and returns the current instance of `BoundStatement`.\n\nThe `values` parameter can be either a sequence or a dictionary that matches the column names to their corresponding values. When using protocol version 4 or higher, any missing values will be replaced with `UNSET_VALUE`, which allows for flexibility in parameter binding.\n\nParameters:\n- `values`: A sequence (e.g., list or tuple) of values to bind to the prepared statement parameters, or a dictionary that relates keys (column names) to their respective values.\n\nReturn:\n- Returns the current instance of `BoundStatement` with the values bound to the prepared statement.\n\nRaises:\n- `ValueError`: If the number of provided values exceeds the expected number of parameters or if routing key parameters are insufficient.\n- `KeyError`: If using a dictionary, and a column name is not found in the bound dictionary while using a protocol version less than 4.\n- `TypeError`: If an argument of an incompatible type is provided for any of the prepared statement's columns.\n\nConstants:\n- `UNSET_VALUE`: Defined as part of the `cassandra.encoder` module, this constant represents an unset value when binding parameters. It is relevant for protocol version 4 or higher and allows binding sequences with fewer values than there are placeholders in the prepared statement. The `UNSET_VALUE` helps in managing optional parameters without explicitly providing them.\"\"\"\n        '\\n        Binds a sequence of values for the prepared statement parameters\\n        and returns this instance.  Note that `values` *must* be:\\n\\n        * a sequence, even if you are only binding one value, or\\n        * a dict that relates 1-to-1 between dict keys and columns\\n\\n        .. versionchanged:: 2.6.0\\n\\n            :data:`~.UNSET_VALUE` was introduced. These can be bound as positional parameters\\n            in a sequence, or by name in a dict. Additionally, when using protocol v4+:\\n\\n            * short sequences will be extended to match bind parameters with UNSET_VALUE\\n            * names may be omitted from a dict with UNSET_VALUE implied.\\n\\n        .. versionchanged:: 3.0.0\\n\\n            method will not throw if extra keys are present in bound dict (PYTHON-178)\\n        '\n        if values is None:\n            values = ()\n        proto_version = self.prepared_statement.protocol_version\n        col_meta = self.prepared_statement.column_metadata\n        ce_policy = self.prepared_statement.column_encryption_policy\n        if isinstance(values, dict):\n            values_dict = values\n            values = []\n            for col in col_meta:\n                try:\n                    values.append(values_dict[col.name])\n                except KeyError:\n                    if proto_version >= 4:\n                        values.append(UNSET_VALUE)\n                    else:\n                        raise KeyError('Column name `%s` not found in bound dict.' % col.name)\n        value_len = len(values)\n        col_meta_len = len(col_meta)\n        if value_len > col_meta_len:\n            raise ValueError('Too many arguments provided to bind() (got %d, expected %d)' % (len(values), len(col_meta)))\n        if proto_version < 4 and self.prepared_statement.routing_key_indexes and (value_len < len(self.prepared_statement.routing_key_indexes)):\n            raise ValueError('Too few arguments provided to bind() (got %d, required %d for routing key)' % (value_len, len(self.prepared_statement.routing_key_indexes)))\n        self.raw_values = values\n        self.values = []\n        for value, col_spec in zip(values, col_meta):\n            if value is None:\n                self.values.append(None)\n            elif value is UNSET_VALUE:\n                if proto_version >= 4:\n                    self._append_unset_value()\n                else:\n                    raise ValueError('Attempt to bind UNSET_VALUE while using unsuitable protocol version (%d < 4)' % proto_version)\n            else:\n                try:\n                    col_desc = ColDesc(col_spec.keyspace_name, col_spec.table_name, col_spec.name)\n                    uses_ce = ce_policy and ce_policy.contains_column(col_desc)\n                    col_type = ce_policy.column_type(col_desc) if uses_ce else col_spec.type\n                    col_bytes = col_type.serialize(value, proto_version)\n                    if uses_ce:\n                        col_bytes = ce_policy.encrypt(col_desc, col_bytes)\n                    self.values.append(col_bytes)\n                except (TypeError, struct.error) as exc:\n                    actual_type = type(value)\n                    message = 'Received an argument of invalid type for column \"%s\". Expected: %s, Got: %s; (%s)' % (col_spec.name, col_spec.type, actual_type, exc)\n                    raise TypeError(message)\n        if proto_version >= 4:\n            diff = col_meta_len - len(self.values)\n            if diff:\n                for _ in range(diff):\n                    self._append_unset_value()\n        return self",
        "docstring": "Binds a sequence of values to the parameters of the prepared statement and returns the current instance of `BoundStatement`.\n\nThe `values` parameter can be either a sequence or a dictionary that matches the column names to their corresponding values. When using protocol version 4 or higher, any missing values will be replaced with `UNSET_VALUE`, which allows for flexibility in parameter binding.\n\nParameters:\n- `values`: A sequence (e.g., list or tuple) of values to bind to the prepared statement parameters, or a dictionary that relates keys (column names) to their respective values.\n\nReturn:\n- Returns the current instance of `BoundStatement` with the values bound to the prepared statement.\n\nRaises:\n- `ValueError`: If the number of provided values exceeds the expected number of parameters or if routing key parameters are insufficient.\n- `KeyError`: If using a dictionary, and a column name is not found in the bound dictionary while using a protocol version less than 4.\n- `TypeError`: If an argument of an incompatible type is provided for any of the prepared statement's columns.\n\nConstants:\n- `UNSET_VALUE`: Defined as part of the `cassandra.encoder` module, this constant represents an unset value when binding parameters. It is relevant for protocol version 4 or higher and allows binding sequences with fewer values than there are placeholders in the prepared statement. The `UNSET_VALUE` helps in managing optional parameters without explicitly providing them.",
        "signature": "def bind(self, values):",
        "type": "Method",
        "class_signature": "class BoundStatement(Statement):"
      },
      "bind_params": {
        "code": "def bind_params(query, params, encoder):\n    \"\"\"Bind a CQL query string with parameters, encoding the parameters using the provided encoder.\n\nParameters:\n- query (str): A CQL statement containing placeholders for parameters, which will be formatted with the given `params`.\n- params (dict or iterable): The values to bind to the query. If a dictionary is provided, its keys must match the placeholders in the query. If an iterable (like a list or tuple) is provided, it will be treated as a sequence of positional parameters.\n- encoder (cassandra.encoder.Encoder): An instance of the Encoder class responsible for encoding CQL types for proper compatibility with Cassandra.\n\nReturns:\n- str: The CQL query string formatted with the encoded parameter values.\n\nImportant Notes:\n- The `encoder.cql_encode_all_types` method is used to ensure that parameters are correctly serialized according to their CQL types before formatting. This allows for seamless integration with the Cassandra database.\"\"\"\n    if isinstance(params, dict):\n        return query % dict(((k, encoder.cql_encode_all_types(v)) for k, v in params.items()))\n    else:\n        return query % tuple((encoder.cql_encode_all_types(v) for v in params))",
        "docstring": "Bind a CQL query string with parameters, encoding the parameters using the provided encoder.\n\nParameters:\n- query (str): A CQL statement containing placeholders for parameters, which will be formatted with the given `params`.\n- params (dict or iterable): The values to bind to the query. If a dictionary is provided, its keys must match the placeholders in the query. If an iterable (like a list or tuple) is provided, it will be treated as a sequence of positional parameters.\n- encoder (cassandra.encoder.Encoder): An instance of the Encoder class responsible for encoding CQL types for proper compatibility with Cassandra.\n\nReturns:\n- str: The CQL query string formatted with the encoded parameter values.\n\nImportant Notes:\n- The `encoder.cql_encode_all_types` method is used to ensure that parameters are correctly serialized according to their CQL types before formatting. This allows for seamless integration with the Cassandra database.",
        "signature": "def bind_params(query, params, encoder):",
        "type": "Function",
        "class_signature": null
      }
    },
    "cassandra/policies.py": {},
    "cassandra/encoder.py": {
      "Encoder.__init__": {
        "code": "    def __init__(self):\n        \"\"\"Initializes the Encoder instance, setting up a mapping of Python types to their corresponding CQL string encoding functions. This mapping allows the Encoder to convert various data types (such as `float`, `str`, `dict`, `list`, and specialized types like `UUID`, `datetime`, and `ipaddress`) into appropriate CQL representations when non-prepared statements are executed.\n\nAttributes:\n- `self.mapping`: A dictionary that maps Python types to specific encoding methods defined within the Encoder class. This mapping can be customized by the user to support additional types or change existing behaviors.\n\nThis initialization is essential for the successful encoding of complex data types by supporting multiple collections and structures, allowing seamless integration with Cassandra's Query Language (CQL).\"\"\"\n        self.mapping = {float: self.cql_encode_float, bytearray: self.cql_encode_bytes, str: self.cql_encode_str, int: self.cql_encode_object, UUID: self.cql_encode_object, datetime.datetime: self.cql_encode_datetime, datetime.date: self.cql_encode_date, datetime.time: self.cql_encode_time, Date: self.cql_encode_date_ext, Time: self.cql_encode_time, dict: self.cql_encode_map_collection, OrderedDict: self.cql_encode_map_collection, OrderedMap: self.cql_encode_map_collection, OrderedMapSerializedKey: self.cql_encode_map_collection, list: self.cql_encode_list_collection, tuple: self.cql_encode_list_collection, set: self.cql_encode_set_collection, sortedset: self.cql_encode_set_collection, frozenset: self.cql_encode_set_collection, types.GeneratorType: self.cql_encode_list_collection, ValueSequence: self.cql_encode_sequence, Point: self.cql_encode_str_quoted, LineString: self.cql_encode_str_quoted, Polygon: self.cql_encode_str_quoted}\n        self.mapping.update({memoryview: self.cql_encode_bytes, bytes: self.cql_encode_bytes, type(None): self.cql_encode_none, ipaddress.IPv4Address: self.cql_encode_ipaddress, ipaddress.IPv6Address: self.cql_encode_ipaddress})",
        "docstring": "Initializes the Encoder instance, setting up a mapping of Python types to their corresponding CQL string encoding functions. This mapping allows the Encoder to convert various data types (such as `float`, `str`, `dict`, `list`, and specialized types like `UUID`, `datetime`, and `ipaddress`) into appropriate CQL representations when non-prepared statements are executed.\n\nAttributes:\n- `self.mapping`: A dictionary that maps Python types to specific encoding methods defined within the Encoder class. This mapping can be customized by the user to support additional types or change existing behaviors.\n\nThis initialization is essential for the successful encoding of complex data types by supporting multiple collections and structures, allowing seamless integration with Cassandra's Query Language (CQL).",
        "signature": "def __init__(self):",
        "type": "Method",
        "class_signature": "class Encoder(object):"
      }
    },
    "cassandra/__init__.py": {}
  },
  "dependency_dict": {
    "cassandra/query.py:bind_params": {
      "cassandra/encoder.py": {
        "Encoder.cql_encode_all_types": {
          "code": "    def cql_encode_all_types(self, val, as_text_type=False):\n        \"\"\"\n        Converts any type into a CQL string, defaulting to ``cql_encode_object``\n        if :attr:`~Encoder.mapping` does not contain an entry for the type.\n        \"\"\"\n        encoded = self.mapping.get(type(val), self.cql_encode_object)(val)\n        if as_text_type and (not isinstance(encoded, str)):\n            return encoded.decode('utf-8')\n        return encoded",
          "docstring": "Converts any type into a CQL string, defaulting to ``cql_encode_object``\nif :attr:`~Encoder.mapping` does not contain an entry for the type.",
          "signature": "def cql_encode_all_types(self, val, as_text_type=False):",
          "type": "Method",
          "class_signature": "class Encoder(object):"
        }
      }
    },
    "cassandra/query.py:BoundStatement:__init__": {
      "cassandra/query.py": {
        "Statement.__init__": {
          "code": "    def __init__(self, retry_policy=None, consistency_level=None, routing_key=None, serial_consistency_level=None, fetch_size=FETCH_SIZE_UNSET, keyspace=None, custom_payload=None, is_idempotent=False, table=None):\n        if retry_policy and (not hasattr(retry_policy, 'on_read_timeout')):\n            raise ValueError('retry_policy should implement cassandra.policies.RetryPolicy')\n        if retry_policy is not None:\n            self.retry_policy = retry_policy\n        if consistency_level is not None:\n            self.consistency_level = consistency_level\n        self._routing_key = routing_key\n        if serial_consistency_level is not None:\n            self.serial_consistency_level = serial_consistency_level\n        if fetch_size is not FETCH_SIZE_UNSET:\n            self.fetch_size = fetch_size\n        if keyspace is not None:\n            self.keyspace = keyspace\n        if table is not None:\n            self.table = table\n        if custom_payload is not None:\n            self.custom_payload = custom_payload\n        self.is_idempotent = is_idempotent",
          "docstring": "",
          "signature": "def __init__(self, retry_policy=None, consistency_level=None, routing_key=None, serial_consistency_level=None, fetch_size=FETCH_SIZE_UNSET, keyspace=None, custom_payload=None, is_idempotent=False, table=None):",
          "type": "Method",
          "class_signature": "class Statement(object):"
        },
        "Statement._set_serial_consistency_level": {
          "code": "    def _set_serial_consistency_level(self, serial_consistency_level):\n        if serial_consistency_level is not None and (not ConsistencyLevel.is_serial(serial_consistency_level)):\n            raise ValueError('serial_consistency_level must be either ConsistencyLevel.SERIAL or ConsistencyLevel.LOCAL_SERIAL')\n        self._serial_consistency_level = serial_consistency_level",
          "docstring": "",
          "signature": "def _set_serial_consistency_level(self, serial_consistency_level):",
          "type": "Method",
          "class_signature": "class Statement(object):"
        }
      }
    },
    "cassandra/query.py:BoundStatement:bind": {
      "cassandra/cqltypes.py": {
        "Int32Type.serialize": {
          "code": "    def serialize(byts, protocol_version):\n        return int32_pack(byts)",
          "docstring": "",
          "signature": "def serialize(byts, protocol_version):",
          "type": "Method",
          "class_signature": "class Int32Type(_CassandraType):"
        }
      },
      "cassandra/query.py": {
        "BoundStatement._append_unset_value": {
          "code": "    def _append_unset_value(self):\n        next_index = len(self.values)\n        if self.prepared_statement.is_routing_key_index(next_index):\n            col_meta = self.prepared_statement.column_metadata[next_index]\n            raise ValueError(\"Cannot bind UNSET_VALUE as a part of the routing key '%s'\" % col_meta.name)\n        self.values.append(UNSET_VALUE)",
          "docstring": "",
          "signature": "def _append_unset_value(self):",
          "type": "Method",
          "class_signature": "class BoundStatement(Statement):"
        }
      }
    },
    "cassandra/query.py:PreparedStatement:bind": {}
  },
  "call_tree": {
    "tests/unit/test_parameter_binding.py:ParamBindingTest:test_bind_map": {
      "cassandra/encoder.py:Encoder:__init__": {},
      "cassandra/query.py:bind_params": {
        "cassandra/encoder.py:Encoder:cql_encode_all_types": {
          "cassandra/encoder.py:Encoder:cql_encode_object": {},
          "cassandra/encoder.py:Encoder:cql_encode_str": {
            "cassandra/encoder.py:cql_quote": {}
          },
          "cassandra/encoder.py:Encoder:cql_encode_float": {}
        }
      }
    },
    "tests/unit/test_parameter_binding.py:ParamBindingTest:test_bind_sequence": {
      "cassandra/encoder.py:Encoder:__init__": {},
      "cassandra/query.py:bind_params": {
        "cassandra/encoder.py:Encoder:cql_encode_all_types": {
          "cassandra/encoder.py:Encoder:cql_encode_object": {},
          "cassandra/encoder.py:Encoder:cql_encode_str": {
            "cassandra/encoder.py:cql_quote": {}
          },
          "cassandra/encoder.py:Encoder:cql_encode_float": {}
        }
      }
    },
    "tests/unit/test_parameter_binding.py:ParamBindingTest:test_float_precision": {
      "cassandra/encoder.py:Encoder:__init__": {},
      "cassandra/query.py:bind_params": {
        "cassandra/encoder.py:Encoder:cql_encode_all_types": {
          "cassandra/encoder.py:Encoder:cql_encode_float": {}
        }
      }
    },
    "tests/unit/test_parameter_binding.py:ParamBindingTest:test_generator_param": {
      "cassandra/encoder.py:Encoder:__init__": {},
      "cassandra/query.py:bind_params": {
        "cassandra/encoder.py:Encoder:cql_encode_all_types": {
          "cassandra/encoder.py:Encoder:cql_encode_list_collection": {
            "cassandra/encoder.py:Encoder:cql_encode_object": {}
          }
        }
      }
    },
    "tests/unit/test_parameter_binding.py:ParamBindingTest:test_list_collection": {
      "cassandra/encoder.py:Encoder:__init__": {},
      "cassandra/query.py:bind_params": {
        "cassandra/encoder.py:Encoder:cql_encode_all_types": {
          "cassandra/encoder.py:Encoder:cql_encode_list_collection": {
            "cassandra/encoder.py:Encoder:cql_encode_str": {
              "cassandra/encoder.py:cql_quote": {}
            }
          }
        }
      }
    },
    "tests/unit/test_parameter_binding.py:ParamBindingTest:test_map_collection": {
      "cassandra/encoder.py:Encoder:__init__": {},
      "cassandra/query.py:bind_params": {
        "cassandra/encoder.py:Encoder:cql_encode_all_types": {
          "cassandra/encoder.py:Encoder:cql_encode_map_collection": {
            "cassandra/encoder.py:Encoder:cql_encode_str": {
              "cassandra/encoder.py:cql_quote": {}
            }
          }
        }
      }
    },
    "tests/unit/test_parameter_binding.py:ParamBindingTest:test_none_param": {
      "cassandra/encoder.py:Encoder:__init__": {},
      "cassandra/query.py:bind_params": {
        "cassandra/encoder.py:Encoder:cql_encode_all_types": {
          "cassandra/encoder.py:Encoder:cql_encode_none": {}
        }
      }
    },
    "tests/unit/test_parameter_binding.py:ParamBindingTest:test_quote_escaping": {
      "cassandra/encoder.py:Encoder:__init__": {},
      "cassandra/query.py:bind_params": {
        "cassandra/encoder.py:Encoder:cql_encode_all_types": {
          "cassandra/encoder.py:Encoder:cql_encode_str": {
            "cassandra/encoder.py:cql_quote": {}
          }
        }
      }
    },
    "tests/unit/test_parameter_binding.py:ParamBindingTest:test_sequence_param": {
      "cassandra/encoder.py:Encoder:__init__": {},
      "cassandra/query.py:bind_params": {
        "cassandra/encoder.py:Encoder:cql_encode_all_types": {
          "cassandra/encoder.py:Encoder:cql_encode_sequence": {
            "cassandra/encoder.py:Encoder:cql_encode_object": {},
            "cassandra/encoder.py:Encoder:cql_encode_str": {
              "cassandra/encoder.py:cql_quote": {}
            },
            "cassandra/encoder.py:Encoder:cql_encode_float": {}
          }
        }
      }
    },
    "tests/unit/test_parameter_binding.py:ParamBindingTest:test_set_collection": {
      "cassandra/encoder.py:Encoder:__init__": {},
      "cassandra/query.py:bind_params": {
        "cassandra/encoder.py:Encoder:cql_encode_all_types": {
          "cassandra/encoder.py:Encoder:cql_encode_set_collection": {
            "cassandra/encoder.py:Encoder:cql_encode_str": {
              "cassandra/encoder.py:cql_quote": {}
            }
          }
        }
      }
    },
    "tests/unit/test_parameter_binding.py:BoundStatementTestV1:setUpClass": {
      "cassandra/query.py:PreparedStatement:__init__": {},
      "cassandra/query.py:BoundStatement:__init__": {
        "cassandra/query.py:Statement:_set_serial_consistency_level": {},
        "cassandra/query.py:Statement:__init__": {}
      }
    },
    "tests/unit/test_parameter_binding.py:BoundStatementTestV1:test_bind_none": {
      "cassandra/query.py:BoundStatement:bind": {
        "cassandra/cqltypes.py:Int32Type:serialize": {}
      }
    },
    "tests/unit/test_parameter_binding.py:BoundStatementTestV1:test_dict_missing_routing_key": {
      "cassandra/query.py:BoundStatement:bind": {}
    },
    "tests/unit/test_parameter_binding.py:BoundStatementTestV1:test_extra_value": {
      "cassandra/query.py:BoundStatement:bind": {
        "cassandra/cqltypes.py:Int32Type:serialize": {}
      }
    },
    "tests/unit/test_parameter_binding.py:BoundStatementTestV1:test_inherit_fetch_size": {
      "cassandra/query.py:PreparedStatement:__init__": {},
      "cassandra/query.py:BoundStatement:__init__": {
        "cassandra/query.py:Statement:_set_serial_consistency_level": {},
        "cassandra/query.py:Statement:__init__": {}
      }
    },
    "tests/unit/test_parameter_binding.py:BoundStatementTestV1:test_invalid_argument_type": {
      "cassandra/query.py:BoundStatement:bind": {
        "cassandra/cqltypes.py:Int32Type:serialize": {}
      }
    },
    "tests/unit/test_parameter_binding.py:BoundStatementTestV1:test_missing_value": {
      "cassandra/query.py:BoundStatement:bind": {}
    },
    "tests/unit/test_parameter_binding.py:BoundStatementTestV1:test_too_few_parameters_for_routing_key": {
      "cassandra/query.py:PreparedStatement:bind": {
        "cassandra/query.py:BoundStatement:__init__": {
          "cassandra/query.py:Statement:_set_serial_consistency_level": {},
          "cassandra/query.py:Statement:__init__": {}
        },
        "cassandra/query.py:BoundStatement:bind": {
          "cassandra/cqltypes.py:Int32Type:serialize": {},
          "cassandra/query.py:BoundStatement:_append_unset_value": {
            "cassandra/query.py:PreparedStatement:is_routing_key_index": {}
          }
        }
      }
    },
    "tests/unit/test_parameter_binding.py:BoundStatementTestV1:test_unset_value": {
      "cassandra/query.py:BoundStatement:bind": {
        "cassandra/cqltypes.py:Int32Type:serialize": {}
      }
    },
    "tests/unit/test_parameter_binding.py:BoundStatementTestV1:test_values_none": {
      "cassandra/query.py:BoundStatement:bind": {
        "cassandra/query.py:BoundStatement:_append_unset_value": {
          "cassandra/query.py:PreparedStatement:is_routing_key_index": {}
        }
      },
      "cassandra/query.py:PreparedStatement:__init__": {},
      "cassandra/query.py:PreparedStatement:bind": {
        "cassandra/query.py:BoundStatement:__init__": {
          "cassandra/query.py:Statement:_set_serial_consistency_level": {},
          "cassandra/query.py:Statement:__init__": {}
        },
        "cassandra/query.py:BoundStatement:bind": {}
      }
    },
    "tests/unit/test_parameter_binding.py:BoundStatementTestV4:test_dict_missing_routing_key": {
      "cassandra/query.py:BoundStatement:bind": {
        "cassandra/cqltypes.py:Int32Type:serialize": {},
        "cassandra/query.py:BoundStatement:_append_unset_value": {
          "cassandra/query.py:PreparedStatement:is_routing_key_index": {}
        }
      }
    },
    "tests/unit/test_parameter_binding.py:BoundStatementTestV4:test_missing_value": {
      "cassandra/query.py:BoundStatement:bind": {
        "cassandra/cqltypes.py:Int32Type:serialize": {},
        "cassandra/query.py:BoundStatement:_append_unset_value": {
          "cassandra/query.py:PreparedStatement:is_routing_key_index": {}
        }
      }
    },
    "tests/unit/test_parameter_binding.py:BoundStatementTestV4:test_unset_value": {
      "cassandra/query.py:BoundStatement:bind": {
        "cassandra/cqltypes.py:Int32Type:serialize": {},
        "cassandra/query.py:BoundStatement:_append_unset_value": {
          "cassandra/query.py:PreparedStatement:is_routing_key_index": {}
        }
      }
    },
    "/mnt/sfs_turbo/yaxindu/tmp/scylla_driver-image-test_parameter_binding/scylla_driver-test_parameter_binding/tests/unit/test_policies.py:TestRackOrDCAwareRoundRobinPolicy:test_with_remotes": {
      "cassandra/policies.py:DCAwareRoundRobinPolicy:DCAwareRoundRobinPolicy": {},
      "cassandra/policies.py:RackAwareRoundRobinPolicy:RackAwareRoundRobinPolicy": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/scylla_driver-image-test_parameter_binding/scylla_driver-test_parameter_binding/tests/unit/test_policies.py:TestRackOrDCAwareRoundRobinPolicy:test_get_distance": {
      "cassandra/policies.py:DCAwareRoundRobinPolicy:DCAwareRoundRobinPolicy": {},
      "cassandra/policies.py:RackAwareRoundRobinPolicy:RackAwareRoundRobinPolicy": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/scylla_driver-image-test_parameter_binding/scylla_driver-test_parameter_binding/tests/integration/long/test_loadbalancingpolicies.py:LoadBalancingPolicyTests:test_token_aware_is_used_by_default": {
      "cassandra/policies.py:TokenAwarePolicy:TokenAwarePolicy": {},
      "cassandra/policies.py:DCAwareRoundRobinPolicy:DCAwareRoundRobinPolicy": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/scylla_driver-image-test_parameter_binding/scylla_driver-test_parameter_binding/tests/integration/advanced/graph/test_graph.py:GraphTimeoutTests:test_server_timeout_less_then_request": {
      "cassandra/__init__.py:InvalidRequest:InvalidRequest": {},
      "cassandra/__init__.py:OperationTimedOut:OperationTimedOut": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/scylla_driver-image-test_parameter_binding/scylla_driver-test_parameter_binding/tests/integration/advanced/graph/test_graph.py:GraphProfileTests:test_graph_profile": {
      "cassandra/__init__.py:InvalidRequest:InvalidRequest": {},
      "cassandra/__init__.py:OperationTimedOut:OperationTimedOut": {}
    }
  },
  "PRD": "# PROJECT NAME: scylla_driver-test_parameter_binding\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 cassandra/\n    \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 InvalidRequest.InvalidRequest\n    \u2502   \u2514\u2500\u2500 OperationTimedOut.OperationTimedOut\n    \u251c\u2500\u2500 encoder.py\n    \u2502   \u2514\u2500\u2500 Encoder.__init__\n    \u251c\u2500\u2500 policies.py\n    \u2502   \u251c\u2500\u2500 DCAwareRoundRobinPolicy.DCAwareRoundRobinPolicy\n    \u2502   \u251c\u2500\u2500 RackAwareRoundRobinPolicy.RackAwareRoundRobinPolicy\n    \u2502   \u2514\u2500\u2500 TokenAwarePolicy.TokenAwarePolicy\n    \u2514\u2500\u2500 query.py\n        \u251c\u2500\u2500 BoundStatement.__init__\n        \u251c\u2500\u2500 BoundStatement.bind\n        \u251c\u2500\u2500 PreparedStatement.__init__\n        \u251c\u2500\u2500 PreparedStatement.bind\n        \u2514\u2500\u2500 bind_params\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThe module facilitates robust parameter binding and validation for interactions with Apache Cassandra, ensuring accurate and efficient preparation and execution of CQL (Cassandra Query Language) statements. It provides capabilities for dynamic binding of various data types and collections (e.g., integers, strings, maps, sequences) into query statements while enforcing type consistency against defined metadata. Additionally, it supports handling edge cases such as missing, null, or unset values and can accommodate protocol versions with varying behaviors for these scenarios. This module addresses the critical need for reliable and consistent query parameterization, simplifying development workflows for Apache Cassandra users and ensuring seamless integration of complex query logic into applications.\n\n## FILE 1: cassandra/query.py\n\n- CLASS METHOD: BoundStatement.bind\n  - CLASS SIGNATURE: class BoundStatement(Statement):\n  - SIGNATURE: def bind(self, values):\n  - DOCSTRING: \n```python\n\"\"\"\nBinds a sequence of values to the parameters of the prepared statement and returns the current instance of `BoundStatement`.\n\nThe `values` parameter can be either a sequence or a dictionary that matches the column names to their corresponding values. When using protocol version 4 or higher, any missing values will be replaced with `UNSET_VALUE`, which allows for flexibility in parameter binding.\n\nParameters:\n- `values`: A sequence (e.g., list or tuple) of values to bind to the prepared statement parameters, or a dictionary that relates keys (column names) to their respective values.\n\nReturn:\n- Returns the current instance of `BoundStatement` with the values bound to the prepared statement.\n\nRaises:\n- `ValueError`: If the number of provided values exceeds the expected number of parameters or if routing key parameters are insufficient.\n- `KeyError`: If using a dictionary, and a column name is not found in the bound dictionary while using a protocol version less than 4.\n- `TypeError`: If an argument of an incompatible type is provided for any of the prepared statement's columns.\n\nConstants:\n- `UNSET_VALUE`: Defined as part of the `cassandra.encoder` module, this constant represents an unset value when binding parameters. It is relevant for protocol version 4 or higher and allows binding sequences with fewer values than there are placeholders in the prepared statement. The `UNSET_VALUE` helps in managing optional parameters without explicitly providing them.\n\"\"\"\n```\n\n- CLASS METHOD: PreparedStatement.bind\n  - CLASS SIGNATURE: class PreparedStatement(object):\n  - SIGNATURE: def bind(self, values):\n  - DOCSTRING: \n```python\n\"\"\"\nCreates and returns a `BoundStatement` instance using the provided `values`. This method binds the specified values to the prepared statement's parameters, ensuring they are correctly serialized according to their types and the current protocol version.\n\nParameters:\n- `values`: A sequence (e.g., list, tuple) or a dictionary of values corresponding to the parameters in the prepared statement. If a dictionary is provided, the keys must match the column names. If some parameters are not included, they will be set to `UNSET_VALUE` (defined in the context as a constant indicating a value that should be ignored).\n\nReturns:\n- A `BoundStatement` instance, which can then be executed against the database.\n\nDependencies:\n- This method relies on the `BoundStatement` class and its `bind` method to bind the values to the prepared statement. If using protocol version 4 or higher, `UNSET_VALUE` allows for flexible binding without requiring all parameters.\n\"\"\"\n```\n\n- FUNCTION NAME: bind_params\n  - SIGNATURE: def bind_params(query, params, encoder):\n  - DOCSTRING: \n```python\n\"\"\"\nBind a CQL query string with parameters, encoding the parameters using the provided encoder.\n\nParameters:\n- query (str): A CQL statement containing placeholders for parameters, which will be formatted with the given `params`.\n- params (dict or iterable): The values to bind to the query. If a dictionary is provided, its keys must match the placeholders in the query. If an iterable (like a list or tuple) is provided, it will be treated as a sequence of positional parameters.\n- encoder (cassandra.encoder.Encoder): An instance of the Encoder class responsible for encoding CQL types for proper compatibility with Cassandra.\n\nReturns:\n- str: The CQL query string formatted with the encoded parameter values.\n\nImportant Notes:\n- The `encoder.cql_encode_all_types` method is used to ensure that parameters are correctly serialized according to their CQL types before formatting. This allows for seamless integration with the Cassandra database.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - cassandra/encoder.py:Encoder:cql_encode_all_types\n\n- CLASS METHOD: PreparedStatement.__init__\n  - CLASS SIGNATURE: class PreparedStatement(object):\n  - SIGNATURE: def __init__(self, column_metadata, query_id, routing_key_indexes, query, keyspace, protocol_version, result_metadata, result_metadata_id, column_encryption_policy=None):\n  - DOCSTRING: \n```python\n\"\"\"\nInitializes a `PreparedStatement` instance that represents a prepared Cassandra query with associated metadata.\n\nParameters:\n- `column_metadata`: Metadata about the columns in the prepared statement.\n- `query_id`: A unique identifier for the prepared query.\n- `routing_key_indexes`: Indices of the columns used for routing the query.\n- `query`: The CQL query string associated with the prepared statement.\n- `keyspace`: The keyspace this prepared statement operates on.\n- `protocol_version`: The version of the Cassandra protocol being used.\n- `result_metadata`: Metadata about the results returned by the prepared statement.\n- `result_metadata_id`: Identifier for the result metadata.\n- `column_encryption_policy`: Optional; specifies the policy for encrypting column values; defaults to `None`.\n\nAttributes:\n- `column_metadata`, `query_id`, `routing_key_indexes`, `query_string`, `keyspace`, `protocol_version`, `result_metadata`, and `result_metadata_id` are all set based on the provided parameters.\n- `column_encryption_policy` can be used to manage how column values are encrypted during queries.\n\nThis method is integral to the `PreparedStatement` class, allowing it to encapsulate the complexities of a prepared query while interfacing with other components of the Cassandra driver.\n\"\"\"\n```\n\n- CLASS METHOD: BoundStatement.__init__\n  - CLASS SIGNATURE: class BoundStatement(Statement):\n  - SIGNATURE: def __init__(self, prepared_statement, retry_policy=None, consistency_level=None, routing_key=None, serial_consistency_level=None, fetch_size=FETCH_SIZE_UNSET, keyspace=None, custom_payload=None):\n  - DOCSTRING: \n```python\n\"\"\"\nInitializes a BoundStatement instance that is bound to a specific set of values for a prepared statement.\n\nParameters:\n- prepared_statement (PreparedStatement): An instance of the PreparedStatement class that this BoundStatement is based on.\n- retry_policy (Optional[RetryPolicy]): An instance of a retry policy. If not provided, the policy from the prepared statement is used.\n- consistency_level (Optional[ConsistencyLevel]): Defines the consistency level for the operation. If not provided, the level from the prepared statement is used.\n- routing_key (Optional[Any]): A custom routing key to control which replicas process the query.\n- serial_consistency_level (Optional[ConsistencyLevel]): Specifies the serial consistency level, applicable only to certain operations. Defaults to the value in the prepared statement.\n- fetch_size (Optional[int]): Number of rows to fetch at a time, overriding the default fetch size. Defaults to FETCH_SIZE_UNSET.\n- keyspace (Optional[str]): The keyspace for the query. Defaults to the keyspace of the prepared_statement if available.\n- custom_payload (Optional[dict]): Custom payload data to send with the query. Defaults to the payload in the prepared statement.\n\nAttributes Initialized:\n- values (list): A list to hold the bound values for the prepared statement's parameters.\n- keyspace (str): Set to the keyspace of the first column of the prepared statement's metadata, if available.\n- table (str): Set to the table name of the first column of the prepared statement's metadata, if available.\n\nConstants:\n- FETCH_SIZE_UNSET: A sentinel value indicating that no fetch size has been explicitly set. This allows the default fetch size to be used when initializing the BoundStatement.\n\nThis constructor also calls the __init__ method of its superclass, Statement, to inherit common properties and behaviors.\n\"\"\"\n```\n\n## FILE 2: cassandra/policies.py\n\n## FILE 3: cassandra/encoder.py\n\n- CLASS METHOD: Encoder.__init__\n  - CLASS SIGNATURE: class Encoder(object):\n  - SIGNATURE: def __init__(self):\n  - DOCSTRING: \n```python\n\"\"\"\nInitializes the Encoder instance, setting up a mapping of Python types to their corresponding CQL string encoding functions. This mapping allows the Encoder to convert various data types (such as `float`, `str`, `dict`, `list`, and specialized types like `UUID`, `datetime`, and `ipaddress`) into appropriate CQL representations when non-prepared statements are executed.\n\nAttributes:\n- `self.mapping`: A dictionary that maps Python types to specific encoding methods defined within the Encoder class. This mapping can be customized by the user to support additional types or change existing behaviors.\n\nThis initialization is essential for the successful encoding of complex data types by supporting multiple collections and structures, allowing seamless integration with Cassandra's Query Language (CQL).\n\"\"\"\n```\n\n## FILE 4: cassandra/__init__.py\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "cassandra/query.py": "\"\"\"\nThis module holds classes for working with prepared statements and\nspecifying consistency levels and retry policies for individual\nqueries.\n\"\"\"\nfrom collections import namedtuple\nfrom datetime import datetime, timedelta, timezone\nimport re\nimport struct\nimport time\nimport warnings\nfrom cassandra import ConsistencyLevel, OperationTimedOut\nfrom cassandra.util import unix_time_from_uuid1, maybe_add_timeout_to_query\nfrom cassandra.encoder import Encoder\nimport cassandra.encoder\nfrom cassandra.policies import ColDesc\nfrom cassandra.protocol import _UNSET_VALUE\nfrom cassandra.util import OrderedDict, _sanitize_identifiers\nimport logging\nlog = logging.getLogger(__name__)\nUNSET_VALUE = _UNSET_VALUE\n'\\nSpecifies an unset value when binding a prepared statement.\\n\\nUnset values are ignored, allowing prepared statements to be used without specify\\n\\nSee https://issues.apache.org/jira/browse/CASSANDRA-7304 for further details on semantics.\\n\\n.. versionadded:: 2.6.0\\n\\nOnly valid when using native protocol v4+\\n'\nNON_ALPHA_REGEX = re.compile('[^a-zA-Z0-9]')\nSTART_BADCHAR_REGEX = re.compile('^[^a-zA-Z0-9]*')\nEND_BADCHAR_REGEX = re.compile('[^a-zA-Z0-9_]*$')\n_clean_name_cache = {}\n\ndef _clean_column_name(name):\n    try:\n        return _clean_name_cache[name]\n    except KeyError:\n        clean = NON_ALPHA_REGEX.sub('_', START_BADCHAR_REGEX.sub('', END_BADCHAR_REGEX.sub('', name)))\n        _clean_name_cache[name] = clean\n        return clean\n\ndef tuple_factory(colnames, rows):\n    \"\"\"\n    Returns each row as a tuple\n\n    Example::\n\n        >>> from cassandra.query import tuple_factory\n        >>> session = cluster.connect('mykeyspace')\n        >>> session.row_factory = tuple_factory\n        >>> rows = session.execute(\"SELECT name, age FROM users LIMIT 1\")\n        >>> print(rows[0])\n        ('Bob', 42)\n\n    .. versionchanged:: 2.0.0\n        moved from ``cassandra.decoder`` to ``cassandra.query``\n    \"\"\"\n    return rows\n\nclass PseudoNamedTupleRow(object):\n    \"\"\"\n    Helper class for pseudo_named_tuple_factory. These objects provide an\n    __iter__ interface, as well as index- and attribute-based access to values,\n    but otherwise do not attempt to implement the full namedtuple or iterable\n    interface.\n    \"\"\"\n\n    def __init__(self, ordered_dict):\n        self._dict = ordered_dict\n        self._tuple = tuple(ordered_dict.values())\n\n    def __getattr__(self, name):\n        return self._dict[name]\n\n    def __getitem__(self, idx):\n        return self._tuple[idx]\n\n    def __iter__(self):\n        return iter(self._tuple)\n\n    def __repr__(self):\n        return '{t}({od})'.format(t=self.__class__.__name__, od=self._dict)\n\ndef pseudo_namedtuple_factory(colnames, rows):\n    \"\"\"\n    Returns each row as a :class:`.PseudoNamedTupleRow`. This is the fallback\n    factory for cases where :meth:`.named_tuple_factory` fails to create rows.\n    \"\"\"\n    return [PseudoNamedTupleRow(od) for od in ordered_dict_factory(colnames, rows)]\n\ndef named_tuple_factory(colnames, rows):\n    \"\"\"\n    Returns each row as a `namedtuple <https://docs.python.org/2/library/collections.html#collections.namedtuple>`_.\n    This is the default row factory.\n\n    Example::\n\n        >>> from cassandra.query import named_tuple_factory\n        >>> session = cluster.connect('mykeyspace')\n        >>> session.row_factory = named_tuple_factory\n        >>> rows = session.execute(\"SELECT name, age FROM users LIMIT 1\")\n        >>> user = rows[0]\n\n        >>> # you can access field by their name:\n        >>> print(\"name: %s, age: %d\" % (user.name, user.age))\n        name: Bob, age: 42\n\n        >>> # or you can access fields by their position (like a tuple)\n        >>> name, age = user\n        >>> print(\"name: %s, age: %d\" % (name, age))\n        name: Bob, age: 42\n        >>> name = user[0]\n        >>> age = user[1]\n        >>> print(\"name: %s, age: %d\" % (name, age))\n        name: Bob, age: 42\n\n    .. versionchanged:: 2.0.0\n        moved from ``cassandra.decoder`` to ``cassandra.query``\n    \"\"\"\n    clean_column_names = map(_clean_column_name, colnames)\n    try:\n        Row = namedtuple('Row', clean_column_names)\n    except SyntaxError:\n        warnings.warn('Failed creating namedtuple for a result because there were too many columns. This is due to a Python limitation that affects namedtuple in Python 3.0-3.6 (see issue18896). The row will be created with {substitute_factory_name}, which lacks some namedtuple features and is slower. To avoid slower performance accessing values on row objects, Upgrade to Python 3.7, or use a different row factory. (column names: {colnames})'.format(substitute_factory_name=pseudo_namedtuple_factory.__name__, colnames=colnames))\n        return pseudo_namedtuple_factory(colnames, rows)\n    except Exception:\n        clean_column_names = list(map(_clean_column_name, colnames))\n        log.warning('Failed creating named tuple for results with column names %s (cleaned: %s) (see Python \\'namedtuple\\' documentation for details on name rules). Results will be returned with positional names. Avoid this by choosing different names, using SELECT \"<col name>\" AS aliases, or specifying a different row_factory on your Session' % (colnames, clean_column_names))\n        Row = namedtuple('Row', _sanitize_identifiers(clean_column_names))\n    return [Row(*row) for row in rows]\n\ndef dict_factory(colnames, rows):\n    \"\"\"\n    Returns each row as a dict.\n\n    Example::\n\n        >>> from cassandra.query import dict_factory\n        >>> session = cluster.connect('mykeyspace')\n        >>> session.row_factory = dict_factory\n        >>> rows = session.execute(\"SELECT name, age FROM users LIMIT 1\")\n        >>> print(rows[0])\n        {u'age': 42, u'name': u'Bob'}\n\n    .. versionchanged:: 2.0.0\n        moved from ``cassandra.decoder`` to ``cassandra.query``\n    \"\"\"\n    return [dict(zip(colnames, row)) for row in rows]\n\ndef ordered_dict_factory(colnames, rows):\n    \"\"\"\n    Like :meth:`~cassandra.query.dict_factory`, but returns each row as an OrderedDict,\n    so the order of the columns is preserved.\n\n    .. versionchanged:: 2.0.0\n        moved from ``cassandra.decoder`` to ``cassandra.query``\n    \"\"\"\n    return [OrderedDict(zip(colnames, row)) for row in rows]\nFETCH_SIZE_UNSET = object()\n\nclass Statement(object):\n    \"\"\"\n    An abstract class representing a single query. There are three subclasses:\n    :class:`.SimpleStatement`, :class:`.BoundStatement`, and :class:`.BatchStatement`.\n    These can be passed to :meth:`.Session.execute()`.\n    \"\"\"\n    retry_policy = None\n    '\\n    An instance of a :class:`cassandra.policies.RetryPolicy` or one of its\\n    subclasses.  This controls when a query will be retried and how it\\n    will be retried.\\n    '\n    consistency_level = None\n    '\\n    The :class:`.ConsistencyLevel` to be used for this operation.  Defaults\\n    to :const:`None`, which means that the default consistency level for\\n    the Session this is executed in will be used.\\n    '\n    fetch_size = FETCH_SIZE_UNSET\n    '\\n    How many rows will be fetched at a time.  This overrides the default\\n    of :attr:`.Session.default_fetch_size`\\n\\n    This only takes effect when protocol version 2 or higher is used.\\n    See :attr:`.Cluster.protocol_version` for details.\\n\\n    .. versionadded:: 2.0.0\\n    '\n    keyspace = None\n    '\\n    The string name of the keyspace this query acts on. This is used when\\n    :class:`~.TokenAwarePolicy` is configured in the profile load balancing policy.\\n\\n    It is set implicitly on :class:`.BoundStatement`, and :class:`.BatchStatement`,\\n    but must be set explicitly on :class:`.SimpleStatement`.\\n\\n    .. versionadded:: 2.1.3\\n    '\n    table = None\n    '\\n    The string name of the table this query acts on. This is used when the tablet\\n    feature is enabled and in the same time :class`~.TokenAwarePolicy` is configured\\n    in the profile load balancing policy.\\n    '\n    custom_payload = None\n    '\\n    :ref:`custom_payload` to be passed to the server.\\n\\n    These are only allowed when using protocol version 4 or higher.\\n\\n    .. versionadded:: 2.6.0\\n    '\n    is_idempotent = False\n    '\\n    Flag indicating whether this statement is safe to run multiple times in speculative execution.\\n    '\n    _serial_consistency_level = None\n    _routing_key = None\n\n    def __init__(self, retry_policy=None, consistency_level=None, routing_key=None, serial_consistency_level=None, fetch_size=FETCH_SIZE_UNSET, keyspace=None, custom_payload=None, is_idempotent=False, table=None):\n        if retry_policy and (not hasattr(retry_policy, 'on_read_timeout')):\n            raise ValueError('retry_policy should implement cassandra.policies.RetryPolicy')\n        if retry_policy is not None:\n            self.retry_policy = retry_policy\n        if consistency_level is not None:\n            self.consistency_level = consistency_level\n        self._routing_key = routing_key\n        if serial_consistency_level is not None:\n            self.serial_consistency_level = serial_consistency_level\n        if fetch_size is not FETCH_SIZE_UNSET:\n            self.fetch_size = fetch_size\n        if keyspace is not None:\n            self.keyspace = keyspace\n        if table is not None:\n            self.table = table\n        if custom_payload is not None:\n            self.custom_payload = custom_payload\n        self.is_idempotent = is_idempotent\n\n    def _key_parts_packed(self, parts):\n        for p in parts:\n            l = len(p)\n            yield struct.pack('>H%dsB' % l, l, p, 0)\n\n    def _get_routing_key(self):\n        return self._routing_key\n\n    def _set_routing_key(self, key):\n        if isinstance(key, (list, tuple)):\n            if len(key) == 1:\n                self._routing_key = key[0]\n            else:\n                self._routing_key = b''.join(self._key_parts_packed(key))\n        else:\n            self._routing_key = key\n\n    def _del_routing_key(self):\n        self._routing_key = None\n    routing_key = property(_get_routing_key, _set_routing_key, _del_routing_key, '\\n        The :attr:`~.TableMetadata.partition_key` portion of the primary key,\\n        which can be used to determine which nodes are replicas for the query.\\n\\n        If the partition key is a composite, a list or tuple must be passed in.\\n        Each key component should be in its packed (binary) format, so all\\n        components should be strings.\\n        ')\n\n    def _get_serial_consistency_level(self):\n        return self._serial_consistency_level\n\n    def _set_serial_consistency_level(self, serial_consistency_level):\n        if serial_consistency_level is not None and (not ConsistencyLevel.is_serial(serial_consistency_level)):\n            raise ValueError('serial_consistency_level must be either ConsistencyLevel.SERIAL or ConsistencyLevel.LOCAL_SERIAL')\n        self._serial_consistency_level = serial_consistency_level\n\n    def _del_serial_consistency_level(self):\n        self._serial_consistency_level = None\n    serial_consistency_level = property(_get_serial_consistency_level, _set_serial_consistency_level, _del_serial_consistency_level, '\\n        The serial consistency level is only used by conditional updates\\n        (``INSERT``, ``UPDATE`` and ``DELETE`` with an ``IF`` condition).  For\\n        those, the ``serial_consistency_level`` defines the consistency level of\\n        the serial phase (or \"paxos\" phase) while the normal\\n        :attr:`~.consistency_level` defines the consistency for the \"learn\" phase,\\n        i.e. what type of reads will be guaranteed to see the update right away.\\n        For example, if a conditional write has a :attr:`~.consistency_level` of\\n        :attr:`~.ConsistencyLevel.QUORUM` (and is successful), then a\\n        :attr:`~.ConsistencyLevel.QUORUM` read is guaranteed to see that write.\\n        But if the regular :attr:`~.consistency_level` of that write is\\n        :attr:`~.ConsistencyLevel.ANY`, then only a read with a\\n        :attr:`~.consistency_level` of :attr:`~.ConsistencyLevel.SERIAL` is\\n        guaranteed to see it (even a read with consistency\\n        :attr:`~.ConsistencyLevel.ALL` is not guaranteed to be enough).\\n\\n        The serial consistency can only be one of :attr:`~.ConsistencyLevel.SERIAL`\\n        or :attr:`~.ConsistencyLevel.LOCAL_SERIAL`. While ``SERIAL`` guarantees full\\n        linearizability (with other ``SERIAL`` updates), ``LOCAL_SERIAL`` only\\n        guarantees it in the local data center.\\n\\n        The serial consistency level is ignored for any query that is not a\\n        conditional update. Serial reads should use the regular\\n        :attr:`consistency_level`.\\n\\n        Serial consistency levels may only be used against Cassandra 2.0+\\n        and the :attr:`~.Cluster.protocol_version` must be set to 2 or higher.\\n\\n        See :doc:`/lwt` for a discussion on how to work with results returned from\\n        conditional statements.\\n\\n        .. versionadded:: 2.0.0\\n        ')\n\nclass SimpleStatement(Statement):\n    \"\"\"\n    A simple, un-prepared query.\n    \"\"\"\n\n    def __init__(self, query_string, retry_policy=None, consistency_level=None, routing_key=None, serial_consistency_level=None, fetch_size=FETCH_SIZE_UNSET, keyspace=None, custom_payload=None, is_idempotent=False):\n        \"\"\"\n        `query_string` should be a literal CQL statement with the exception\n        of parameter placeholders that will be filled through the\n        `parameters` argument of :meth:`.Session.execute()`.\n\n        See :class:`Statement` attributes for a description of the other parameters.\n        \"\"\"\n        Statement.__init__(self, retry_policy, consistency_level, routing_key, serial_consistency_level, fetch_size, keyspace, custom_payload, is_idempotent)\n        self._query_string = query_string\n\n    @property\n    def query_string(self):\n        return self._query_string\n\n    def __str__(self):\n        consistency = ConsistencyLevel.value_to_name.get(self.consistency_level, 'Not Set')\n        return u'<SimpleStatement query=\"%s\", consistency=%s>' % (self.query_string, consistency)\n    __repr__ = __str__\n\nclass PreparedStatement(object):\n    \"\"\"\n    A statement that has been prepared against at least one Cassandra node.\n    Instances of this class should not be created directly, but through\n    :meth:`.Session.prepare()`.\n\n    A :class:`.PreparedStatement` should be prepared only once. Re-preparing a statement\n    may affect performance (as the operation requires a network roundtrip).\n\n    |prepared_stmt_head|: Do not use ``*`` in prepared statements if you might\n    change the schema of the table being queried. The driver and server each\n    maintain a map between metadata for a schema and statements that were\n    prepared against that schema. When a user changes a schema, e.g. by adding\n    or removing a column, the server invalidates its mappings involving that\n    schema. However, there is currently no way to propagate that invalidation\n    to drivers. Thus, after a schema change, the driver will incorrectly\n    interpret the results of ``SELECT *`` queries prepared before the schema\n    change. This is currently being addressed in `CASSANDRA-10786\n    <https://issues.apache.org/jira/browse/CASSANDRA-10786>`_.\n\n    .. |prepared_stmt_head| raw:: html\n\n       <b>A note about <code>*</code> in prepared statements</b>\n    \"\"\"\n    column_metadata = None\n    retry_policy = None\n    consistency_level = None\n    custom_payload = None\n    fetch_size = FETCH_SIZE_UNSET\n    keyspace = None\n    protocol_version = None\n    query_id = None\n    query_string = None\n    result_metadata = None\n    result_metadata_id = None\n    column_encryption_policy = None\n    routing_key_indexes = None\n    _routing_key_index_set = None\n    serial_consistency_level = None\n\n    @classmethod\n    def from_message(cls, query_id, column_metadata, pk_indexes, cluster_metadata, query, prepared_keyspace, protocol_version, result_metadata, result_metadata_id, column_encryption_policy=None):\n        if not column_metadata:\n            return PreparedStatement(column_metadata, query_id, None, query, prepared_keyspace, protocol_version, result_metadata, result_metadata_id, column_encryption_policy)\n        if pk_indexes:\n            routing_key_indexes = pk_indexes\n        else:\n            routing_key_indexes = None\n            first_col = column_metadata[0]\n            ks_meta = cluster_metadata.keyspaces.get(first_col.keyspace_name)\n            if ks_meta:\n                table_meta = ks_meta.tables.get(first_col.table_name)\n                if table_meta:\n                    partition_key_columns = table_meta.partition_key\n                    statement_indexes = dict(((c.name, i) for i, c in enumerate(column_metadata)))\n                    try:\n                        routing_key_indexes = [statement_indexes[c.name] for c in partition_key_columns]\n                    except KeyError:\n                        pass\n        return PreparedStatement(column_metadata, query_id, routing_key_indexes, query, prepared_keyspace, protocol_version, result_metadata, result_metadata_id, column_encryption_policy)\n\n    def is_routing_key_index(self, i):\n        if self._routing_key_index_set is None:\n            self._routing_key_index_set = set(self.routing_key_indexes) if self.routing_key_indexes else set()\n        return i in self._routing_key_index_set\n\n    def __str__(self):\n        consistency = ConsistencyLevel.value_to_name.get(self.consistency_level, 'Not Set')\n        return u'<PreparedStatement query=\"%s\", consistency=%s>' % (self.query_string, consistency)\n    __repr__ = __str__\n\nclass BoundStatement(Statement):\n    \"\"\"\n    A prepared statement that has been bound to a particular set of values.\n    These may be created directly or through :meth:`.PreparedStatement.bind()`.\n    \"\"\"\n    prepared_statement = None\n    '\\n    The :class:`PreparedStatement` instance that this was created from.\\n    '\n    values = None\n    '\\n    The sequence of values that were bound to the prepared statement.\\n    '\n\n    def _append_unset_value(self):\n        next_index = len(self.values)\n        if self.prepared_statement.is_routing_key_index(next_index):\n            col_meta = self.prepared_statement.column_metadata[next_index]\n            raise ValueError(\"Cannot bind UNSET_VALUE as a part of the routing key '%s'\" % col_meta.name)\n        self.values.append(UNSET_VALUE)\n\n    @property\n    def routing_key(self):\n        if not self.prepared_statement.routing_key_indexes:\n            return None\n        if self._routing_key is not None:\n            return self._routing_key\n        routing_indexes = self.prepared_statement.routing_key_indexes\n        if len(routing_indexes) == 1:\n            self._routing_key = self.values[routing_indexes[0]]\n        else:\n            self._routing_key = b''.join(self._key_parts_packed((self.values[i] for i in routing_indexes)))\n        return self._routing_key\n\n    def __str__(self):\n        consistency = ConsistencyLevel.value_to_name.get(self.consistency_level, 'Not Set')\n        return u'<BoundStatement query=\"%s\", values=%s, consistency=%s>' % (self.prepared_statement.query_string, self.raw_values, consistency)\n    __repr__ = __str__\n\nclass BatchType(object):\n    \"\"\"\n    A BatchType is used with :class:`.BatchStatement` instances to control\n    the atomicity of the batch operation.\n\n    .. versionadded:: 2.0.0\n    \"\"\"\n    LOGGED = None\n    '\\n    Atomic batch operation.\\n    '\n    UNLOGGED = None\n    '\\n    Non-atomic batch operation.\\n    '\n    COUNTER = None\n    '\\n    Batches of counter operations.\\n    '\n\n    def __init__(self, name, value):\n        self.name = name\n        self.value = value\n\n    def __str__(self):\n        return self.name\n\n    def __repr__(self):\n        return 'BatchType.%s' % (self.name,)\nBatchType.LOGGED = BatchType('LOGGED', 0)\nBatchType.UNLOGGED = BatchType('UNLOGGED', 1)\nBatchType.COUNTER = BatchType('COUNTER', 2)\n\nclass BatchStatement(Statement):\n    \"\"\"\n    A protocol-level batch of operations which are applied atomically\n    by default.\n\n    .. versionadded:: 2.0.0\n    \"\"\"\n    batch_type = None\n    '\\n    The :class:`.BatchType` for the batch operation.  Defaults to\\n    :attr:`.BatchType.LOGGED`.\\n    '\n    serial_consistency_level = None\n    '\\n    The same as :attr:`.Statement.serial_consistency_level`, but is only\\n    supported when using protocol version 3 or higher.\\n    '\n    _statements_and_parameters = None\n    _session = None\n\n    def __init__(self, batch_type=BatchType.LOGGED, retry_policy=None, consistency_level=None, serial_consistency_level=None, session=None, custom_payload=None):\n        \"\"\"\n        `batch_type` specifies The :class:`.BatchType` for the batch operation.\n        Defaults to :attr:`.BatchType.LOGGED`.\n\n        `retry_policy` should be a :class:`~.RetryPolicy` instance for\n        controlling retries on the operation.\n\n        `consistency_level` should be a :class:`~.ConsistencyLevel` value\n        to be used for all operations in the batch.\n\n        `custom_payload` is a :ref:`custom_payload` passed to the server.\n        Note: as Statement objects are added to the batch, this map is\n        updated with any values found in their custom payloads. These are\n        only allowed when using protocol version 4 or higher.\n\n        Example usage:\n\n        .. code-block:: python\n\n            insert_user = session.prepare(\"INSERT INTO users (name, age) VALUES (?, ?)\")\n            batch = BatchStatement(consistency_level=ConsistencyLevel.QUORUM)\n\n            for (name, age) in users_to_insert:\n                batch.add(insert_user, (name, age))\n\n            session.execute(batch)\n\n        You can also mix different types of operations within a batch:\n\n        .. code-block:: python\n\n            batch = BatchStatement()\n            batch.add(SimpleStatement(\"INSERT INTO users (name, age) VALUES (%s, %s)\"), (name, age))\n            batch.add(SimpleStatement(\"DELETE FROM pending_users WHERE name=%s\"), (name,))\n            session.execute(batch)\n\n        .. versionadded:: 2.0.0\n\n        .. versionchanged:: 2.1.0\n            Added `serial_consistency_level` as a parameter\n\n        .. versionchanged:: 2.6.0\n            Added `custom_payload` as a parameter\n        \"\"\"\n        self.batch_type = batch_type\n        self._statements_and_parameters = []\n        self._session = session\n        Statement.__init__(self, retry_policy=retry_policy, consistency_level=consistency_level, serial_consistency_level=serial_consistency_level, custom_payload=custom_payload)\n\n    def clear(self):\n        \"\"\"\n        This is a convenience method to clear a batch statement for reuse.\n\n        *Note:* it should not be used concurrently with uncompleted execution futures executing the same\n        ``BatchStatement``.\n        \"\"\"\n        del self._statements_and_parameters[:]\n        self.keyspace = None\n        self.routing_key = None\n        if self.custom_payload:\n            self.custom_payload.clear()\n\n    def add(self, statement, parameters=None):\n        \"\"\"\n        Adds a :class:`.Statement` and optional sequence of parameters\n        to be used with the statement to the batch.\n\n        Like with other statements, parameters must be a sequence, even\n        if there is only one item.\n        \"\"\"\n        if isinstance(statement, str):\n            if parameters:\n                encoder = Encoder() if self._session is None else self._session.encoder\n                statement = bind_params(statement, parameters, encoder)\n            self._add_statement_and_params(False, statement, ())\n        elif isinstance(statement, PreparedStatement):\n            query_id = statement.query_id\n            bound_statement = statement.bind(() if parameters is None else parameters)\n            self._update_state(bound_statement)\n            self._add_statement_and_params(True, query_id, bound_statement.values)\n        elif isinstance(statement, BoundStatement):\n            if parameters:\n                raise ValueError('Parameters cannot be passed with a BoundStatement to BatchStatement.add()')\n            self._update_state(statement)\n            self._add_statement_and_params(True, statement.prepared_statement.query_id, statement.values)\n        else:\n            query_string = statement.query_string\n            if parameters:\n                encoder = Encoder() if self._session is None else self._session.encoder\n                query_string = bind_params(query_string, parameters, encoder)\n            self._update_state(statement)\n            self._add_statement_and_params(False, query_string, ())\n        return self\n\n    def add_all(self, statements, parameters):\n        \"\"\"\n        Adds a sequence of :class:`.Statement` objects and a matching sequence\n        of parameters to the batch. Statement and parameter sequences must be of equal length or\n        one will be truncated. :const:`None` can be used in the parameters position where are needed.\n        \"\"\"\n        for statement, value in zip(statements, parameters):\n            self.add(statement, value)\n\n    def _add_statement_and_params(self, is_prepared, statement, parameters):\n        if len(self._statements_and_parameters) >= 65535:\n            raise ValueError('Batch statement cannot contain more than %d statements.' % 65535)\n        self._statements_and_parameters.append((is_prepared, statement, parameters))\n\n    def _maybe_set_routing_attributes(self, statement):\n        if self.routing_key is None:\n            if statement.keyspace and statement.routing_key:\n                self.routing_key = statement.routing_key\n                self.keyspace = statement.keyspace\n\n    def _update_custom_payload(self, statement):\n        if statement.custom_payload:\n            if self.custom_payload is None:\n                self.custom_payload = {}\n            self.custom_payload.update(statement.custom_payload)\n\n    def _update_state(self, statement):\n        self._maybe_set_routing_attributes(statement)\n        self._update_custom_payload(statement)\n\n    def __len__(self):\n        return len(self._statements_and_parameters)\n\n    def __str__(self):\n        consistency = ConsistencyLevel.value_to_name.get(self.consistency_level, 'Not Set')\n        return u'<BatchStatement type=%s, statements=%d, consistency=%s>' % (self.batch_type, len(self), consistency)\n    __repr__ = __str__\nValueSequence = cassandra.encoder.ValueSequence\n'\\nA wrapper class that is used to specify that a sequence of values should\\nbe treated as a CQL list of values instead of a single column collection when used\\nas part of the `parameters` argument for :meth:`.Session.execute()`.\\n\\nThis is typically needed when supplying a list of keys to select.\\nFor example::\\n\\n    >>> my_user_ids = (\\'alice\\', \\'bob\\', \\'charles\\')\\n    >>> query = \"SELECT * FROM users WHERE user_id IN %s\"\\n    >>> session.execute(query, parameters=[ValueSequence(my_user_ids)])\\n\\n'\n\nclass TraceUnavailable(Exception):\n    \"\"\"\n    Raised when complete trace details cannot be fetched from Cassandra.\n    \"\"\"\n    pass\n\nclass QueryTrace(object):\n    \"\"\"\n    A trace of the duration and events that occurred when executing\n    an operation.\n    \"\"\"\n    trace_id = None\n    '\\n    :class:`uuid.UUID` unique identifier for this tracing session.  Matches\\n    the ``session_id`` column in ``system_traces.sessions`` and\\n    ``system_traces.events``.\\n    '\n    request_type = None\n    '\\n    A string that very generally describes the traced operation.\\n    '\n    duration = None\n    '\\n    A :class:`datetime.timedelta` measure of the duration of the query.\\n    '\n    client = None\n    '\\n    The IP address of the client that issued this request\\n\\n    This is only available when using Cassandra 2.2+\\n    '\n    coordinator = None\n    '\\n    The IP address of the host that acted as coordinator for this request.\\n    '\n    parameters = None\n    '\\n    A :class:`dict` of parameters for the traced operation, such as the\\n    specific query string.\\n    '\n    started_at = None\n    '\\n    A UTC :class:`datetime.datetime` object describing when the operation\\n    was started.\\n    '\n    events = None\n    '\\n    A chronologically sorted list of :class:`.TraceEvent` instances\\n    representing the steps the traced operation went through.  This\\n    corresponds to the rows in ``system_traces.events`` for this tracing\\n    session.\\n    '\n    _session = None\n    _SELECT_SESSIONS_FORMAT = 'SELECT * FROM system_traces.sessions WHERE session_id = %s'\n    _SELECT_EVENTS_FORMAT = 'SELECT * FROM system_traces.events WHERE session_id = %s'\n    _BASE_RETRY_SLEEP = 0.003\n\n    def __init__(self, trace_id, session):\n        self.trace_id = trace_id\n        self._session = session\n\n    def populate(self, max_wait=2.0, wait_for_complete=True, query_cl=None):\n        \"\"\"\n        Retrieves the actual tracing details from Cassandra and populates the\n        attributes of this instance.  Because tracing details are stored\n        asynchronously by Cassandra, this may need to retry the session\n        detail fetch.  If the trace is still not available after `max_wait`\n        seconds, :exc:`.TraceUnavailable` will be raised; if `max_wait` is\n        :const:`None`, this will retry forever.\n\n        `wait_for_complete=False` bypasses the wait for duration to be populated.\n        This can be used to query events from partial sessions.\n\n        `query_cl` specifies a consistency level to use for polling the trace tables,\n        if it should be different than the session default.\n        \"\"\"\n        attempt = 0\n        start = time.time()\n        while True:\n            time_spent = time.time() - start\n            if max_wait is not None and time_spent >= max_wait:\n                raise TraceUnavailable('Trace information was not available within %f seconds. Consider raising Session.max_trace_wait.' % (max_wait,))\n            log.debug('Attempting to fetch trace info for trace ID: %s', self.trace_id)\n            metadata_request_timeout = self._session.cluster.control_connection and self._session.cluster.control_connection._metadata_request_timeout\n            session_results = self._execute(SimpleStatement(maybe_add_timeout_to_query(self._SELECT_SESSIONS_FORMAT, metadata_request_timeout), consistency_level=query_cl), (self.trace_id,), time_spent, max_wait)\n            session_row = session_results.one() if session_results else None\n            is_complete = session_row is not None and session_row.duration is not None and (session_row.started_at is not None)\n            if not session_results or (wait_for_complete and (not is_complete)):\n                time.sleep(self._BASE_RETRY_SLEEP * 2 ** attempt)\n                attempt += 1\n                continue\n            if is_complete:\n                log.debug('Fetched trace info for trace ID: %s', self.trace_id)\n            else:\n                log.debug('Fetching parital trace info for trace ID: %s', self.trace_id)\n            self.request_type = session_row.request\n            self.duration = timedelta(microseconds=session_row.duration) if is_complete else None\n            self.started_at = session_row.started_at\n            self.coordinator = session_row.coordinator\n            self.parameters = session_row.parameters\n            self.client = getattr(session_row, 'client', None)\n            log.debug('Attempting to fetch trace events for trace ID: %s', self.trace_id)\n            time_spent = time.time() - start\n            event_results = self._execute(SimpleStatement(maybe_add_timeout_to_query(self._SELECT_EVENTS_FORMAT, metadata_request_timeout), consistency_level=query_cl), (self.trace_id,), time_spent, max_wait)\n            log.debug('Fetched trace events for trace ID: %s', self.trace_id)\n            self.events = tuple((TraceEvent(r.activity, r.event_id, r.source, r.source_elapsed, r.thread) for r in event_results))\n            break\n\n    def _execute(self, query, parameters, time_spent, max_wait):\n        timeout = max_wait - time_spent if max_wait is not None else None\n        future = self._session._create_response_future(query, parameters, trace=False, custom_payload=None, timeout=timeout)\n        future.row_factory = named_tuple_factory\n        future.send_request()\n        try:\n            return future.result()\n        except OperationTimedOut:\n            raise TraceUnavailable('Trace information was not available within %f seconds' % (max_wait,))\n\n    def __str__(self):\n        return '%s [%s] coordinator: %s, started at: %s, duration: %s, parameters: %s' % (self.request_type, self.trace_id, self.coordinator, self.started_at, self.duration, self.parameters)\n\nclass TraceEvent(object):\n    \"\"\"\n    Representation of a single event within a query trace.\n    \"\"\"\n    description = None\n    '\\n    A brief description of the event.\\n    '\n    datetime = None\n    '\\n    A UTC :class:`datetime.datetime` marking when the event occurred.\\n    '\n    source = None\n    '\\n    The IP address of the node this event occurred on.\\n    '\n    source_elapsed = None\n    '\\n    A :class:`datetime.timedelta` measuring the amount of time until\\n    this event occurred starting from when :attr:`.source` first\\n    received the query.\\n    '\n    thread_name = None\n    '\\n    The name of the thread that this event occurred on.\\n    '\n\n    def __init__(self, description, timeuuid, source, source_elapsed, thread_name):\n        self.description = description\n        self.datetime = datetime.fromtimestamp(unix_time_from_uuid1(timeuuid), tz=timezone.utc)\n        self.source = source\n        if source_elapsed is not None:\n            self.source_elapsed = timedelta(microseconds=source_elapsed)\n        else:\n            self.source_elapsed = None\n        self.thread_name = thread_name\n\n    def __str__(self):\n        return '%s on %s[%s] at %s' % (self.description, self.source, self.thread_name, self.datetime)\n\nclass HostTargetingStatement(object):\n    \"\"\"\n    Wraps any query statement and attaches a target host, making\n    it usable in a targeted LBP without modifying the user's statement.\n    \"\"\"\n\n    def __init__(self, inner_statement, target_host):\n        self.__class__ = type(inner_statement.__class__.__name__, (self.__class__, inner_statement.__class__), {})\n        self.__dict__ = inner_statement.__dict__\n        self.target_host = target_host",
    "cassandra/policies.py": "import random\nfrom collections import namedtuple\nfrom functools import lru_cache\nfrom itertools import islice, cycle, groupby, repeat\nimport logging\nfrom random import randint, shuffle\nfrom threading import Lock\nimport socket\nimport warnings\nlog = logging.getLogger(__name__)\nfrom cassandra import WriteType as WT\nWriteType = WT\nfrom cassandra import ConsistencyLevel, OperationTimedOut\n\nclass HostDistance(object):\n    \"\"\"\n    A measure of how \"distant\" a node is from the client, which\n    may influence how the load balancer distributes requests\n    and how many connections are opened to the node.\n    \"\"\"\n    IGNORED = -1\n    '\\n    A node with this distance should never be queried or have\\n    connections opened to it.\\n    '\n    LOCAL_RACK = 0\n    '\\n    Nodes with ``LOCAL_RACK`` distance will be preferred for operations\\n    under some load balancing policies (such as :class:`.RackAwareRoundRobinPolicy`)\\n    and will have a greater number of connections opened against\\n    them by default.\\n\\n    This distance is typically used for nodes within the same\\n    datacenter and the same rack as the client.\\n    '\n    LOCAL = 1\n    '\\n    Nodes with ``LOCAL`` distance will be preferred for operations\\n    under some load balancing policies (such as :class:`.DCAwareRoundRobinPolicy`)\\n    and will have a greater number of connections opened against\\n    them by default.\\n\\n    This distance is typically used for nodes within the same\\n    datacenter as the client.\\n    '\n    REMOTE = 2\n    '\\n    Nodes with ``REMOTE`` distance will be treated as a last resort\\n    by some load balancing policies (such as :class:`.DCAwareRoundRobinPolicy`\\n    and :class:`.RackAwareRoundRobinPolicy`)and will have a smaller number of\\n    connections opened against them by default.\\n\\n    This distance is typically used for nodes outside of the\\n    datacenter that the client is running in.\\n    '\n\nclass HostStateListener(object):\n\n    def on_up(self, host):\n        \"\"\" Called when a node is marked up. \"\"\"\n        raise NotImplementedError()\n\n    def on_down(self, host):\n        \"\"\" Called when a node is marked down. \"\"\"\n        raise NotImplementedError()\n\n    def on_add(self, host):\n        \"\"\"\n        Called when a node is added to the cluster.  The newly added node\n        should be considered up.\n        \"\"\"\n        raise NotImplementedError()\n\n    def on_remove(self, host):\n        \"\"\" Called when a node is removed from the cluster. \"\"\"\n        raise NotImplementedError()\n\nclass LoadBalancingPolicy(HostStateListener):\n    \"\"\"\n    Load balancing policies are used to decide how to distribute\n    requests among all possible coordinator nodes in the cluster.\n\n    In particular, they may focus on querying \"near\" nodes (those\n    in a local datacenter) or on querying nodes who happen to\n    be replicas for the requested data.\n\n    You may also use subclasses of :class:`.LoadBalancingPolicy` for\n    custom behavior.\n\n    You should always use immutable collections (e.g., tuples or\n    frozensets) to store information about hosts to prevent accidental\n    modification. When there are changes to the hosts (e.g., a host is\n    down or up), the old collection should be replaced with a new one.\n    \"\"\"\n    _hosts_lock = None\n\n    def __init__(self):\n        self._hosts_lock = Lock()\n\n    def distance(self, host):\n        \"\"\"\n        Returns a measure of how remote a :class:`~.pool.Host` is in\n        terms of the :class:`.HostDistance` enums.\n        \"\"\"\n        raise NotImplementedError()\n\n    def populate(self, cluster, hosts):\n        \"\"\"\n        This method is called to initialize the load balancing\n        policy with a set of :class:`.Host` instances before its\n        first use.  The `cluster` parameter is an instance of\n        :class:`.Cluster`.\n        \"\"\"\n        raise NotImplementedError()\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        \"\"\"\n        Given a :class:`~.query.Statement` instance, return a iterable\n        of :class:`.Host` instances which should be queried in that\n        order.  A generator may work well for custom implementations\n        of this method.\n\n        Note that the `query` argument may be :const:`None` when preparing\n        statements.\n\n        `working_keyspace` should be the string name of the current keyspace,\n        as set through :meth:`.Session.set_keyspace()` or with a ``USE``\n        statement.\n        \"\"\"\n        raise NotImplementedError()\n\n    def check_supported(self):\n        \"\"\"\n        This will be called after the cluster Metadata has been initialized.\n        If the load balancing policy implementation cannot be supported for\n        some reason (such as a missing C extension), this is the point at\n        which it should raise an exception.\n        \"\"\"\n        pass\n\nclass RoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    A subclass of :class:`.LoadBalancingPolicy` which evenly\n    distributes queries across all nodes in the cluster,\n    regardless of what datacenter the nodes may be in.\n    \"\"\"\n    _live_hosts = frozenset(())\n    _position = 0\n\n    def populate(self, cluster, hosts):\n        self._live_hosts = frozenset(hosts)\n        if len(hosts) > 1:\n            self._position = randint(0, len(hosts) - 1)\n\n    def distance(self, host):\n        return HostDistance.LOCAL\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        pos = self._position\n        self._position += 1\n        hosts = self._live_hosts\n        length = len(hosts)\n        if length:\n            pos %= length\n            return islice(cycle(hosts), pos, pos + length)\n        else:\n            return []\n\n    def on_up(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.union((host,))\n\n    def on_down(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.difference((host,))\n\n    def on_add(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.union((host,))\n\n    def on_remove(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.difference((host,))\n\nclass DCAwareRoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    Similar to :class:`.RoundRobinPolicy`, but prefers hosts\n    in the local datacenter and only uses nodes in remote\n    datacenters as a last resort.\n    \"\"\"\n    local_dc = None\n    used_hosts_per_remote_dc = 0\n\n    def __init__(self, local_dc='', used_hosts_per_remote_dc=0):\n        \"\"\"\n        The `local_dc` parameter should be the name of the datacenter\n        (such as is reported by ``nodetool ring``) that should\n        be considered local. If not specified, the driver will choose\n        a local_dc based on the first host among :attr:`.Cluster.contact_points`\n        having a valid DC. If relying on this mechanism, all specified\n        contact points should be nodes in a single, local DC.\n\n        `used_hosts_per_remote_dc` controls how many nodes in\n        each remote datacenter will have connections opened\n        against them. In other words, `used_hosts_per_remote_dc` hosts\n        will be considered :attr:`~.HostDistance.REMOTE` and the\n        rest will be considered :attr:`~.HostDistance.IGNORED`.\n        By default, all remote hosts are ignored.\n        \"\"\"\n        self.local_dc = local_dc\n        self.used_hosts_per_remote_dc = used_hosts_per_remote_dc\n        self._dc_live_hosts = {}\n        self._position = 0\n        self._endpoints = []\n        LoadBalancingPolicy.__init__(self)\n\n    def _dc(self, host):\n        return host.datacenter or self.local_dc\n\n    def populate(self, cluster, hosts):\n        for dc, dc_hosts in groupby(hosts, lambda h: self._dc(h)):\n            self._dc_live_hosts[dc] = tuple(set(dc_hosts))\n        if not self.local_dc:\n            self._endpoints = [endpoint for endpoint in cluster.endpoints_resolved]\n        self._position = randint(0, len(hosts) - 1) if hosts else 0\n\n    def distance(self, host):\n        dc = self._dc(host)\n        if dc == self.local_dc:\n            return HostDistance.LOCAL\n        if not self.used_hosts_per_remote_dc:\n            return HostDistance.IGNORED\n        else:\n            dc_hosts = self._dc_live_hosts.get(dc)\n            if not dc_hosts:\n                return HostDistance.IGNORED\n            if host in list(dc_hosts)[:self.used_hosts_per_remote_dc]:\n                return HostDistance.REMOTE\n            else:\n                return HostDistance.IGNORED\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        pos = self._position\n        self._position += 1\n        local_live = self._dc_live_hosts.get(self.local_dc, ())\n        pos = pos % len(local_live) if local_live else 0\n        for host in islice(cycle(local_live), pos, pos + len(local_live)):\n            yield host\n        other_dcs = [dc for dc in self._dc_live_hosts.copy().keys() if dc != self.local_dc]\n        for dc in other_dcs:\n            remote_live = self._dc_live_hosts.get(dc, ())\n            for host in remote_live[:self.used_hosts_per_remote_dc]:\n                yield host\n\n    def on_up(self, host):\n        if not self.local_dc and host.datacenter:\n            if host.endpoint in self._endpoints:\n                self.local_dc = host.datacenter\n                log.info(\"Using datacenter '%s' for DCAwareRoundRobinPolicy (via host '%s'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\" % (self.local_dc, host.endpoint))\n                del self._endpoints\n        dc = self._dc(host)\n        with self._hosts_lock:\n            current_hosts = self._dc_live_hosts.get(dc, ())\n            if host not in current_hosts:\n                self._dc_live_hosts[dc] = current_hosts + (host,)\n\n    def on_down(self, host):\n        dc = self._dc(host)\n        with self._hosts_lock:\n            current_hosts = self._dc_live_hosts.get(dc, ())\n            if host in current_hosts:\n                hosts = tuple((h for h in current_hosts if h != host))\n                if hosts:\n                    self._dc_live_hosts[dc] = hosts\n                else:\n                    del self._dc_live_hosts[dc]\n\n    def on_add(self, host):\n        self.on_up(host)\n\n    def on_remove(self, host):\n        self.on_down(host)\n\nclass RackAwareRoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    Similar to :class:`.DCAwareRoundRobinPolicy`, but prefers hosts\n    in the local rack, before hosts in the local datacenter but a\n    different rack, before hosts in all other datercentres\n    \"\"\"\n    local_dc = None\n    local_rack = None\n    used_hosts_per_remote_dc = 0\n\n    def __init__(self, local_dc, local_rack, used_hosts_per_remote_dc=0):\n        \"\"\"\n        The `local_dc` and `local_rack` parameters should be the name of the\n        datacenter and rack (such as is reported by ``nodetool ring``) that\n        should be considered local.\n\n        `used_hosts_per_remote_dc` controls how many nodes in\n        each remote datacenter will have connections opened\n        against them. In other words, `used_hosts_per_remote_dc` hosts\n        will be considered :attr:`~.HostDistance.REMOTE` and the\n        rest will be considered :attr:`~.HostDistance.IGNORED`.\n        By default, all remote hosts are ignored.\n        \"\"\"\n        self.local_rack = local_rack\n        self.local_dc = local_dc\n        self.used_hosts_per_remote_dc = used_hosts_per_remote_dc\n        self._live_hosts = {}\n        self._dc_live_hosts = {}\n        self._endpoints = []\n        self._position = 0\n        LoadBalancingPolicy.__init__(self)\n\n    def _rack(self, host):\n        return host.rack or self.local_rack\n\n    def _dc(self, host):\n        return host.datacenter or self.local_dc\n\n    def populate(self, cluster, hosts):\n        for (dc, rack), rack_hosts in groupby(hosts, lambda host: (self._dc(host), self._rack(host))):\n            self._live_hosts[dc, rack] = tuple(set(rack_hosts))\n        for dc, dc_hosts in groupby(hosts, lambda host: self._dc(host)):\n            self._dc_live_hosts[dc] = tuple(set(dc_hosts))\n        self._position = randint(0, len(hosts) - 1) if hosts else 0\n\n    def distance(self, host):\n        rack = self._rack(host)\n        dc = self._dc(host)\n        if rack == self.local_rack and dc == self.local_dc:\n            return HostDistance.LOCAL_RACK\n        if dc == self.local_dc:\n            return HostDistance.LOCAL\n        if not self.used_hosts_per_remote_dc:\n            return HostDistance.IGNORED\n        dc_hosts = self._dc_live_hosts.get(dc, ())\n        if not dc_hosts:\n            return HostDistance.IGNORED\n        if host in dc_hosts and dc_hosts.index(host) < self.used_hosts_per_remote_dc:\n            return HostDistance.REMOTE\n        else:\n            return HostDistance.IGNORED\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        pos = self._position\n        self._position += 1\n        local_rack_live = self._live_hosts.get((self.local_dc, self.local_rack), ())\n        pos = pos % len(local_rack_live) if local_rack_live else 0\n        for host in islice(cycle(local_rack_live), pos, pos + len(local_rack_live)):\n            yield host\n        local_live = [host for host in self._dc_live_hosts.get(self.local_dc, ()) if host.rack != self.local_rack]\n        pos = pos % len(local_live) if local_live else 0\n        for host in islice(cycle(local_live), pos, pos + len(local_live)):\n            yield host\n        for dc, remote_live in self._dc_live_hosts.copy().items():\n            if dc != self.local_dc:\n                for host in remote_live[:self.used_hosts_per_remote_dc]:\n                    yield host\n\n    def on_up(self, host):\n        dc = self._dc(host)\n        rack = self._rack(host)\n        with self._hosts_lock:\n            current_rack_hosts = self._live_hosts.get((dc, rack), ())\n            if host not in current_rack_hosts:\n                self._live_hosts[dc, rack] = current_rack_hosts + (host,)\n            current_dc_hosts = self._dc_live_hosts.get(dc, ())\n            if host not in current_dc_hosts:\n                self._dc_live_hosts[dc] = current_dc_hosts + (host,)\n\n    def on_down(self, host):\n        dc = self._dc(host)\n        rack = self._rack(host)\n        with self._hosts_lock:\n            current_rack_hosts = self._live_hosts.get((dc, rack), ())\n            if host in current_rack_hosts:\n                hosts = tuple((h for h in current_rack_hosts if h != host))\n                if hosts:\n                    self._live_hosts[dc, rack] = hosts\n                else:\n                    del self._live_hosts[dc, rack]\n            current_dc_hosts = self._dc_live_hosts.get(dc, ())\n            if host in current_dc_hosts:\n                hosts = tuple((h for h in current_dc_hosts if h != host))\n                if hosts:\n                    self._dc_live_hosts[dc] = hosts\n                else:\n                    del self._dc_live_hosts[dc]\n\n    def on_add(self, host):\n        self.on_up(host)\n\n    def on_remove(self, host):\n        self.on_down(host)\n\nclass TokenAwarePolicy(LoadBalancingPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` wrapper that adds token awareness to\n    a child policy.\n\n    This alters the child policy's behavior so that it first attempts to\n    send queries to :attr:`~.HostDistance.LOCAL` replicas (as determined\n    by the child policy) based on the :class:`.Statement`'s\n    :attr:`~.Statement.routing_key`. If :attr:`.shuffle_replicas` is\n    truthy, these replicas will be yielded in a random order. Once those\n    hosts are exhausted, the remaining hosts in the child policy's query\n    plan will be used in the order provided by the child policy.\n\n    If no :attr:`~.Statement.routing_key` is set on the query, the child\n    policy's query plan will be used as is.\n    \"\"\"\n    _child_policy = None\n    _cluster_metadata = None\n    _tablets_routing_v1 = False\n    shuffle_replicas = False\n    '\\n    Yield local replicas in a random order.\\n    '\n\n    def __init__(self, child_policy, shuffle_replicas=False):\n        self._child_policy = child_policy\n        self.shuffle_replicas = shuffle_replicas\n\n    def populate(self, cluster, hosts):\n        self._cluster_metadata = cluster.metadata\n        self._tablets_routing_v1 = cluster.control_connection._tablets_routing_v1\n        self._child_policy.populate(cluster, hosts)\n\n    def check_supported(self):\n        if not self._cluster_metadata.can_support_partitioner():\n            raise RuntimeError('%s cannot be used with the cluster partitioner (%s) because the relevant C extension for this driver was not compiled. See the installation instructions for details on building and installing the C extensions.' % (self.__class__.__name__, self._cluster_metadata.partitioner))\n\n    def distance(self, *args, **kwargs):\n        return self._child_policy.distance(*args, **kwargs)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        keyspace = query.keyspace if query and query.keyspace else working_keyspace\n        child = self._child_policy\n        if query is None or query.routing_key is None or keyspace is None:\n            for host in child.make_query_plan(keyspace, query):\n                yield host\n            return\n        replicas = []\n        if self._tablets_routing_v1:\n            tablet = self._cluster_metadata._tablets.get_tablet_for_key(keyspace, query.table, self._cluster_metadata.token_map.token_class.from_key(query.routing_key))\n            if tablet is not None:\n                replicas_mapped = set(map(lambda r: r[0], tablet.replicas))\n                child_plan = child.make_query_plan(keyspace, query)\n                replicas = [host for host in child_plan if host.host_id in replicas_mapped]\n        if not replicas:\n            replicas = self._cluster_metadata.get_replicas(keyspace, query.routing_key)\n        if self.shuffle_replicas:\n            shuffle(replicas)\n        for replica in replicas:\n            if replica.is_up and child.distance(replica) in [HostDistance.LOCAL, HostDistance.LOCAL_RACK]:\n                yield replica\n        for host in child.make_query_plan(keyspace, query):\n            if host not in replicas or child.distance(host) == HostDistance.REMOTE:\n                yield host\n\n    def on_up(self, *args, **kwargs):\n        return self._child_policy.on_up(*args, **kwargs)\n\n    def on_down(self, *args, **kwargs):\n        return self._child_policy.on_down(*args, **kwargs)\n\n    def on_add(self, *args, **kwargs):\n        return self._child_policy.on_add(*args, **kwargs)\n\n    def on_remove(self, *args, **kwargs):\n        return self._child_policy.on_remove(*args, **kwargs)\n\nclass WhiteListRoundRobinPolicy(RoundRobinPolicy):\n    \"\"\"\n    A subclass of :class:`.RoundRobinPolicy` which evenly\n    distributes queries across all nodes in the cluster,\n    regardless of what datacenter the nodes may be in, but\n    only if that node exists in the list of allowed nodes\n\n    This policy is addresses the issue described in\n    https://datastax-oss.atlassian.net/browse/JAVA-145\n    Where connection errors occur when connection\n    attempts are made to private IP addresses remotely\n    \"\"\"\n\n    def __init__(self, hosts):\n        \"\"\"\n        The `hosts` parameter should be a sequence of hosts to permit\n        connections to.\n        \"\"\"\n        self._allowed_hosts = tuple(hosts)\n        self._allowed_hosts_resolved = []\n        for h in self._allowed_hosts:\n            unix_socket_path = getattr(h, '_unix_socket_path', None)\n            if unix_socket_path:\n                self._allowed_hosts_resolved.append(unix_socket_path)\n            else:\n                self._allowed_hosts_resolved.extend([endpoint[4][0] for endpoint in socket.getaddrinfo(h, None, socket.AF_UNSPEC, socket.SOCK_STREAM)])\n        RoundRobinPolicy.__init__(self)\n\n    def populate(self, cluster, hosts):\n        self._live_hosts = frozenset((h for h in hosts if h.address in self._allowed_hosts_resolved))\n        if len(hosts) <= 1:\n            self._position = 0\n        else:\n            self._position = randint(0, len(hosts) - 1)\n\n    def distance(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            return HostDistance.LOCAL\n        else:\n            return HostDistance.IGNORED\n\n    def on_up(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            RoundRobinPolicy.on_up(self, host)\n\n    def on_add(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            RoundRobinPolicy.on_add(self, host)\n\nclass HostFilterPolicy(LoadBalancingPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` subclass configured with a child policy,\n    and a single-argument predicate. This policy defers to the child policy for\n    hosts where ``predicate(host)`` is truthy. Hosts for which\n    ``predicate(host)`` is falsy will be considered :attr:`.IGNORED`, and will\n    not be used in a query plan.\n\n    This can be used in the cases where you need a whitelist or blacklist\n    policy, e.g. to prepare for decommissioning nodes or for testing:\n\n    .. code-block:: python\n\n        def address_is_ignored(host):\n            return host.address in [ignored_address0, ignored_address1]\n\n        blacklist_filter_policy = HostFilterPolicy(\n            child_policy=RoundRobinPolicy(),\n            predicate=address_is_ignored\n        )\n\n        cluster = Cluster(\n            primary_host,\n            load_balancing_policy=blacklist_filter_policy,\n        )\n\n    See the note in the :meth:`.make_query_plan` documentation for a caveat on\n    how wrapping ordering polices (e.g. :class:`.RoundRobinPolicy`) may break\n    desirable properties of the wrapped policy.\n\n    Please note that whitelist and blacklist policies are not recommended for\n    general, day-to-day use. You probably want something like\n    :class:`.DCAwareRoundRobinPolicy`, which prefers a local DC but has\n    fallbacks, over a brute-force method like whitelisting or blacklisting.\n    \"\"\"\n\n    def __init__(self, child_policy, predicate):\n        \"\"\"\n        :param child_policy: an instantiated :class:`.LoadBalancingPolicy`\n                             that this one will defer to.\n        :param predicate: a one-parameter function that takes a :class:`.Host`.\n                          If it returns a falsy value, the :class:`.Host` will\n                          be :attr:`.IGNORED` and not returned in query plans.\n        \"\"\"\n        super(HostFilterPolicy, self).__init__()\n        self._child_policy = child_policy\n        self._predicate = predicate\n\n    def on_up(self, host, *args, **kwargs):\n        return self._child_policy.on_up(host, *args, **kwargs)\n\n    def on_down(self, host, *args, **kwargs):\n        return self._child_policy.on_down(host, *args, **kwargs)\n\n    def on_add(self, host, *args, **kwargs):\n        return self._child_policy.on_add(host, *args, **kwargs)\n\n    def on_remove(self, host, *args, **kwargs):\n        return self._child_policy.on_remove(host, *args, **kwargs)\n\n    @property\n    def predicate(self):\n        \"\"\"\n        A predicate, set on object initialization, that takes a :class:`.Host`\n        and returns a value. If the value is falsy, the :class:`.Host` is\n        :class:`~HostDistance.IGNORED`. If the value is truthy,\n        :class:`.HostFilterPolicy` defers to the child policy to determine the\n        host's distance.\n\n        This is a read-only value set in ``__init__``, implemented as a\n        ``property``.\n        \"\"\"\n        return self._predicate\n\n    def distance(self, host):\n        \"\"\"\n        Checks if ``predicate(host)``, then returns\n        :attr:`~HostDistance.IGNORED` if falsy, and defers to the child policy\n        otherwise.\n        \"\"\"\n        if self.predicate(host):\n            return self._child_policy.distance(host)\n        else:\n            return HostDistance.IGNORED\n\n    def populate(self, cluster, hosts):\n        self._child_policy.populate(cluster=cluster, hosts=hosts)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        \"\"\"\n        Defers to the child policy's\n        :meth:`.LoadBalancingPolicy.make_query_plan` and filters the results.\n\n        Note that this filtering may break desirable properties of the wrapped\n        policy in some cases. For instance, imagine if you configure this\n        policy to filter out ``host2``, and to wrap a round-robin policy that\n        rotates through three hosts in the order ``host1, host2, host3``,\n        ``host2, host3, host1``, ``host3, host1, host2``, repeating. This\n        policy will yield ``host1, host3``, ``host3, host1``, ``host3, host1``,\n        disproportionately favoring ``host3``.\n        \"\"\"\n        child_qp = self._child_policy.make_query_plan(working_keyspace=working_keyspace, query=query)\n        for host in child_qp:\n            if self.predicate(host):\n                yield host\n\n    def check_supported(self):\n        return self._child_policy.check_supported()\n\nclass ConvictionPolicy(object):\n    \"\"\"\n    A policy which decides when hosts should be considered down\n    based on the types of failures and the number of failures.\n\n    If custom behavior is needed, this class may be subclassed.\n    \"\"\"\n\n    def __init__(self, host):\n        \"\"\"\n        `host` is an instance of :class:`.Host`.\n        \"\"\"\n        self.host = host\n\n    def add_failure(self, connection_exc):\n        \"\"\"\n        Implementations should return :const:`True` if the host should be\n        convicted, :const:`False` otherwise.\n        \"\"\"\n        raise NotImplementedError()\n\n    def reset(self):\n        \"\"\"\n        Implementations should clear out any convictions or state regarding\n        the host.\n        \"\"\"\n        raise NotImplementedError()\n\nclass SimpleConvictionPolicy(ConvictionPolicy):\n    \"\"\"\n    The default implementation of :class:`ConvictionPolicy`,\n    which simply marks a host as down after the first failure\n    of any kind.\n    \"\"\"\n\n    def add_failure(self, connection_exc):\n        return not isinstance(connection_exc, OperationTimedOut)\n\n    def reset(self):\n        pass\n\nclass ReconnectionPolicy(object):\n    \"\"\"\n    This class and its subclasses govern how frequently an attempt is made\n    to reconnect to nodes that are marked as dead.\n\n    If custom behavior is needed, this class may be subclassed.\n    \"\"\"\n\n    def new_schedule(self):\n        \"\"\"\n        This should return a finite or infinite iterable of delays (each as a\n        floating point number of seconds) in-between each failed reconnection\n        attempt.  Note that if the iterable is finite, reconnection attempts\n        will cease once the iterable is exhausted.\n        \"\"\"\n        raise NotImplementedError()\n\nclass ConstantReconnectionPolicy(ReconnectionPolicy):\n    \"\"\"\n    A :class:`.ReconnectionPolicy` subclass which sleeps for a fixed delay\n    in-between each reconnection attempt.\n    \"\"\"\n\n    def __init__(self, delay, max_attempts=64):\n        \"\"\"\n        `delay` should be a floating point number of seconds to wait in-between\n        each attempt.\n\n        `max_attempts` should be a total number of attempts to be made before\n        giving up, or :const:`None` to continue reconnection attempts forever.\n        The default is 64.\n        \"\"\"\n        if delay < 0:\n            raise ValueError('delay must not be negative')\n        if max_attempts is not None and max_attempts < 0:\n            raise ValueError('max_attempts must not be negative')\n        self.delay = delay\n        self.max_attempts = max_attempts\n\n    def new_schedule(self):\n        if self.max_attempts:\n            return repeat(self.delay, self.max_attempts)\n        return repeat(self.delay)\n\nclass ExponentialReconnectionPolicy(ReconnectionPolicy):\n    \"\"\"\n    A :class:`.ReconnectionPolicy` subclass which exponentially increases\n    the length of the delay in-between each reconnection attempt up to\n    a set maximum delay.\n\n    A random amount of jitter (+/- 15%) will be added to the pure exponential\n    delay value to avoid the situations where many reconnection handlers are\n    trying to reconnect at exactly the same time.\n    \"\"\"\n\n    def __init__(self, base_delay, max_delay, max_attempts=64):\n        \"\"\"\n        `base_delay` and `max_delay` should be in floating point units of\n        seconds.\n\n        `max_attempts` should be a total number of attempts to be made before\n        giving up, or :const:`None` to continue reconnection attempts forever.\n        The default is 64.\n        \"\"\"\n        if base_delay < 0 or max_delay < 0:\n            raise ValueError('Delays may not be negative')\n        if max_delay < base_delay:\n            raise ValueError('Max delay must be greater than base delay')\n        if max_attempts is not None and max_attempts < 0:\n            raise ValueError('max_attempts must not be negative')\n        self.base_delay = base_delay\n        self.max_delay = max_delay\n        self.max_attempts = max_attempts\n\n    def new_schedule(self):\n        i, overflowed = (0, False)\n        while self.max_attempts is None or i < self.max_attempts:\n            if overflowed:\n                yield self.max_delay\n            else:\n                try:\n                    yield self._add_jitter(min(self.base_delay * 2 ** i, self.max_delay))\n                except OverflowError:\n                    overflowed = True\n                    yield self.max_delay\n            i += 1\n\n    def _add_jitter(self, value):\n        jitter = randint(85, 115)\n        delay = jitter * value / 100\n        return min(max(self.base_delay, delay), self.max_delay)\n\nclass RetryPolicy(object):\n    \"\"\"\n    A policy that describes whether to retry, rethrow, or ignore coordinator\n    timeout and unavailable failures. These are failures reported from the\n    server side. Timeouts are configured by\n    `settings in cassandra.yaml <https://github.com/apache/cassandra/blob/cassandra-2.1.4/conf/cassandra.yaml#L568-L584>`_.\n    Unavailable failures occur when the coordinator cannot achieve the consistency\n    level for a request. For further information see the method descriptions\n    below.\n\n    To specify a default retry policy, set the\n    :attr:`.Cluster.default_retry_policy` attribute to an instance of this\n    class or one of its subclasses.\n\n    To specify a retry policy per query, set the :attr:`.Statement.retry_policy`\n    attribute to an instance of this class or one of its subclasses.\n\n    If custom behavior is needed for retrying certain operations,\n    this class may be subclassed.\n    \"\"\"\n    RETRY = 0\n    '\\n    This should be returned from the below methods if the operation\\n    should be retried on the same connection.\\n    '\n    RETHROW = 1\n    '\\n    This should be returned from the below methods if the failure\\n    should be propagated and no more retries attempted.\\n    '\n    IGNORE = 2\n    '\\n    This should be returned from the below methods if the failure\\n    should be ignored but no more retries should be attempted.\\n    '\n    RETRY_NEXT_HOST = 3\n    '\\n    This should be returned from the below methods if the operation\\n    should be retried on another connection.\\n    '\n\n    def on_read_timeout(self, query, consistency, required_responses, received_responses, data_retrieved, retry_num):\n        \"\"\"\n        This is called when a read operation times out from the coordinator's\n        perspective (i.e. a replica did not respond to the coordinator in time).\n        It should return a tuple with two items: one of the class enums (such\n        as :attr:`.RETRY`) and a :class:`.ConsistencyLevel` to retry the\n        operation at or :const:`None` to keep the same consistency level.\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        The `required_responses` and `received_responses` parameters describe\n        how many replicas needed to respond to meet the requested consistency\n        level and how many actually did respond before the coordinator timed\n        out the request. `data_retrieved` is a boolean indicating whether\n        any of those responses contained data (as opposed to just a digest).\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, operations will be retried at most once, and only if\n        a sufficient number of replicas responded (with data digests).\n        \"\"\"\n        if retry_num != 0:\n            return (self.RETHROW, None)\n        elif received_responses >= required_responses and (not data_retrieved):\n            return (self.RETRY, consistency)\n        else:\n            return (self.RETHROW, None)\n\n    def on_write_timeout(self, query, consistency, write_type, required_responses, received_responses, retry_num):\n        \"\"\"\n        This is called when a write operation times out from the coordinator's\n        perspective (i.e. a replica did not respond to the coordinator in time).\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `write_type` is one of the :class:`.WriteType` enums describing the\n        type of write operation.\n\n        The `required_responses` and `received_responses` parameters describe\n        how many replicas needed to acknowledge the write to meet the requested\n        consistency level and how many replicas actually did acknowledge the\n        write before the coordinator timed out the request.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, failed write operations will retried at most once, and\n        they will only be retried if the `write_type` was\n        :attr:`~.WriteType.BATCH_LOG`.\n        \"\"\"\n        if retry_num != 0:\n            return (self.RETHROW, None)\n        elif write_type == WriteType.BATCH_LOG:\n            return (self.RETRY, consistency)\n        else:\n            return (self.RETHROW, None)\n\n    def on_unavailable(self, query, consistency, required_replicas, alive_replicas, retry_num):\n        \"\"\"\n        This is called when the coordinator node determines that a read or\n        write operation cannot be successful because the number of live\n        replicas are too low to meet the requested :class:`.ConsistencyLevel`.\n        This means that the read or write operation was never forwarded to\n        any replicas.\n\n        `query` is the :class:`.Statement` that failed.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `required_replicas` is the number of replicas that would have needed to\n        acknowledge the operation to meet the requested consistency level.\n        `alive_replicas` is the number of replicas that the coordinator\n        considered alive at the time of the request.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, if this is the first retry, it triggers a retry on the next\n        host in the query plan with the same consistency level. If this is not the\n        first retry, no retries will be attempted and the error will be re-raised.\n        \"\"\"\n        return (self.RETRY_NEXT_HOST, None) if retry_num == 0 else (self.RETHROW, None)\n\n    def on_request_error(self, query, consistency, error, retry_num):\n        \"\"\"\n        This is called when an unexpected error happens. This can be in the\n        following situations:\n\n        * On a connection error\n        * On server errors: overloaded, isBootstrapping, serverError, etc.\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `error` the instance of the exception.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, it triggers a retry on the next host in the query plan\n        with the same consistency level.\n        \"\"\"\n        return (self.RETRY_NEXT_HOST, None)\n\nclass FallthroughRetryPolicy(RetryPolicy):\n    \"\"\"\n    A retry policy that never retries and always propagates failures to\n    the application.\n    \"\"\"\n\n    def on_read_timeout(self, *args, **kwargs):\n        return (self.RETHROW, None)\n\n    def on_write_timeout(self, *args, **kwargs):\n        return (self.RETHROW, None)\n\n    def on_unavailable(self, *args, **kwargs):\n        return (self.RETHROW, None)\n\n    def on_request_error(self, *args, **kwargs):\n        return (self.RETHROW, None)\n\nclass DowngradingConsistencyRetryPolicy(RetryPolicy):\n    \"\"\"\n    *Deprecated:* This retry policy will be removed in the next major release.\n\n    A retry policy that sometimes retries with a lower consistency level than\n    the one initially requested.\n\n    **BEWARE**: This policy may retry queries using a lower consistency\n    level than the one initially requested. By doing so, it may break\n    consistency guarantees. In other words, if you use this retry policy,\n    there are cases (documented below) where a read at :attr:`~.QUORUM`\n    *may not* see a preceding write at :attr:`~.QUORUM`. Do not use this\n    policy unless you have understood the cases where this can happen and\n    are ok with that. It is also recommended to subclass this class so\n    that queries that required a consistency level downgrade can be\n    recorded (so that repairs can be made later, etc).\n\n    This policy implements the same retries as :class:`.RetryPolicy`,\n    but on top of that, it also retries in the following cases:\n\n    * On a read timeout: if the number of replicas that responded is\n      greater than one but lower than is required by the requested\n      consistency level, the operation is retried at a lower consistency\n      level.\n    * On a write timeout: if the operation is an :attr:`~.UNLOGGED_BATCH`\n      and at least one replica acknowledged the write, the operation is\n      retried at a lower consistency level.  Furthermore, for other\n      write types, if at least one replica acknowledged the write, the\n      timeout is ignored.\n    * On an unavailable exception: if at least one replica is alive, the\n      operation is retried at a lower consistency level.\n\n    The reasoning behind this retry policy is as follows: if, based\n    on the information the Cassandra coordinator node returns, retrying the\n    operation with the initially requested consistency has a chance to\n    succeed, do it. Otherwise, if based on that information we know the\n    initially requested consistency level cannot be achieved currently, then:\n\n    * For writes, ignore the exception (thus silently failing the\n      consistency requirement) if we know the write has been persisted on at\n      least one replica.\n    * For reads, try reading at a lower consistency level (thus silently\n      failing the consistency requirement).\n\n    In other words, this policy implements the idea that if the requested\n    consistency level cannot be achieved, the next best thing for writes is\n    to make sure the data is persisted, and that reading something is better\n    than reading nothing, even if there is a risk of reading stale data.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(DowngradingConsistencyRetryPolicy, self).__init__(*args, **kwargs)\n        warnings.warn('DowngradingConsistencyRetryPolicy is deprecated and will be removed in the next major release.', DeprecationWarning)\n\n    def _pick_consistency(self, num_responses):\n        if num_responses >= 3:\n            return (self.RETRY, ConsistencyLevel.THREE)\n        elif num_responses >= 2:\n            return (self.RETRY, ConsistencyLevel.TWO)\n        elif num_responses >= 1:\n            return (self.RETRY, ConsistencyLevel.ONE)\n        else:\n            return (self.RETHROW, None)\n\n    def on_read_timeout(self, query, consistency, required_responses, received_responses, data_retrieved, retry_num):\n        if retry_num != 0:\n            return (self.RETHROW, None)\n        elif ConsistencyLevel.is_serial(consistency):\n            return (self.RETHROW, None)\n        elif received_responses < required_responses:\n            return self._pick_consistency(received_responses)\n        elif not data_retrieved:\n            return (self.RETRY, consistency)\n        else:\n            return (self.RETHROW, None)\n\n    def on_write_timeout(self, query, consistency, write_type, required_responses, received_responses, retry_num):\n        if retry_num != 0:\n            return (self.RETHROW, None)\n        if write_type in (WriteType.SIMPLE, WriteType.BATCH, WriteType.COUNTER):\n            if received_responses > 0:\n                return (self.IGNORE, None)\n            else:\n                return (self.RETHROW, None)\n        elif write_type == WriteType.UNLOGGED_BATCH:\n            return self._pick_consistency(received_responses)\n        elif write_type == WriteType.BATCH_LOG:\n            return (self.RETRY, consistency)\n        return (self.RETHROW, None)\n\n    def on_unavailable(self, query, consistency, required_replicas, alive_replicas, retry_num):\n        if retry_num != 0:\n            return (self.RETHROW, None)\n        elif ConsistencyLevel.is_serial(consistency):\n            return (self.RETRY_NEXT_HOST, None)\n        else:\n            return self._pick_consistency(alive_replicas)\n\nclass ExponentialBackoffRetryPolicy(RetryPolicy):\n    \"\"\"\n    A policy that do retries with exponential backoff\n    \"\"\"\n\n    def __init__(self, max_num_retries: float, min_interval: float=0.1, max_interval: float=10.0, *args, **kwargs):\n        \"\"\"\n        `max_num_retries` counts how many times the operation would be retried,\n        `min_interval` is the initial time in seconds to wait before first retry\n        `max_interval` is the maximum time to wait between retries\n        \"\"\"\n        self.min_interval = min_interval\n        self.max_num_retries = max_num_retries\n        self.max_interval = max_interval\n        super(ExponentialBackoffRetryPolicy).__init__(*args, **kwargs)\n\n    def _calculate_backoff(self, attempt: int):\n        delay = min(self.max_interval, self.min_interval * 2 ** attempt)\n        delay += random.random() * self.min_interval - self.min_interval / 2\n        return delay\n\n    def on_read_timeout(self, query, consistency, required_responses, received_responses, data_retrieved, retry_num):\n        if retry_num < self.max_num_retries and received_responses >= required_responses and (not data_retrieved):\n            return (self.RETRY, consistency, self._calculate_backoff(retry_num))\n        else:\n            return (self.RETHROW, None, None)\n\n    def on_write_timeout(self, query, consistency, write_type, required_responses, received_responses, retry_num):\n        if retry_num < self.max_num_retries and write_type == WriteType.BATCH_LOG:\n            return (self.RETRY, consistency, self._calculate_backoff(retry_num))\n        else:\n            return (self.RETHROW, None, None)\n\n    def on_unavailable(self, query, consistency, required_replicas, alive_replicas, retry_num):\n        if retry_num < self.max_num_retries:\n            return (self.RETRY_NEXT_HOST, None, self._calculate_backoff(retry_num))\n        else:\n            return (self.RETHROW, None, None)\n\n    def on_request_error(self, query, consistency, error, retry_num):\n        if retry_num < self.max_num_retries:\n            return (self.RETRY_NEXT_HOST, None, self._calculate_backoff(retry_num))\n        else:\n            return (self.RETHROW, None, None)\n\nclass AddressTranslator(object):\n    \"\"\"\n    Interface for translating cluster-defined endpoints.\n\n    The driver discovers nodes using server metadata and topology change events. Normally,\n    the endpoint defined by the server is the right way to connect to a node. In some environments,\n    these addresses may not be reachable, or not preferred (public vs. private IPs in cloud environments,\n    suboptimal routing, etc). This interface allows for translating from server defined endpoints to\n    preferred addresses for driver connections.\n\n    *Note:* :attr:`~Cluster.contact_points` provided while creating the :class:`~.Cluster` instance are not\n    translated using this mechanism -- only addresses received from Cassandra nodes are.\n    \"\"\"\n\n    def translate(self, addr):\n        \"\"\"\n        Accepts the node ip address, and returns a translated address to be used connecting to this node.\n        \"\"\"\n        raise NotImplementedError()\n\nclass IdentityTranslator(AddressTranslator):\n    \"\"\"\n    Returns the endpoint with no translation\n    \"\"\"\n\n    def translate(self, addr):\n        return addr\n\nclass EC2MultiRegionTranslator(AddressTranslator):\n    \"\"\"\n    Resolves private ips of the hosts in the same datacenter as the client, and public ips of hosts in other datacenters.\n    \"\"\"\n\n    def translate(self, addr):\n        \"\"\"\n        Reverse DNS the public broadcast_address, then lookup that hostname to get the AWS-resolved IP, which\n        will point to the private IP address within the same datacenter.\n        \"\"\"\n        family = socket.getaddrinfo(addr, 0, socket.AF_UNSPEC, socket.SOCK_STREAM)[0][0]\n        host = socket.getfqdn(addr)\n        for a in socket.getaddrinfo(host, 0, family, socket.SOCK_STREAM):\n            try:\n                return a[4][0]\n            except Exception:\n                pass\n        return addr\n\nclass SpeculativeExecutionPolicy(object):\n    \"\"\"\n    Interface for specifying speculative execution plans\n    \"\"\"\n\n    def new_plan(self, keyspace, statement):\n        \"\"\"\n        Returns\n\n        :param keyspace:\n        :param statement:\n        :return:\n        \"\"\"\n        raise NotImplementedError()\n\nclass SpeculativeExecutionPlan(object):\n\n    def next_execution(self, host):\n        raise NotImplementedError()\n\nclass NoSpeculativeExecutionPlan(SpeculativeExecutionPlan):\n\n    def next_execution(self, host):\n        return -1\n\nclass NoSpeculativeExecutionPolicy(SpeculativeExecutionPolicy):\n\n    def new_plan(self, keyspace, statement):\n        return NoSpeculativeExecutionPlan()\n\nclass ConstantSpeculativeExecutionPolicy(SpeculativeExecutionPolicy):\n    \"\"\"\n    A speculative execution policy that sends a new query every X seconds (**delay**) for a maximum of Y attempts (**max_attempts**).\n    \"\"\"\n\n    def __init__(self, delay, max_attempts):\n        self.delay = delay\n        self.max_attempts = max_attempts\n\n    class ConstantSpeculativeExecutionPlan(SpeculativeExecutionPlan):\n\n        def __init__(self, delay, max_attempts):\n            self.delay = delay\n            self.remaining = max_attempts\n\n        def next_execution(self, host):\n            if self.remaining > 0:\n                self.remaining -= 1\n                return self.delay\n            else:\n                return -1\n\n    def new_plan(self, keyspace, statement):\n        return self.ConstantSpeculativeExecutionPlan(self.delay, self.max_attempts)\n\nclass WrapperPolicy(LoadBalancingPolicy):\n\n    def __init__(self, child_policy):\n        self._child_policy = child_policy\n\n    def distance(self, *args, **kwargs):\n        return self._child_policy.distance(*args, **kwargs)\n\n    def populate(self, cluster, hosts):\n        self._child_policy.populate(cluster, hosts)\n\n    def on_up(self, *args, **kwargs):\n        return self._child_policy.on_up(*args, **kwargs)\n\n    def on_down(self, *args, **kwargs):\n        return self._child_policy.on_down(*args, **kwargs)\n\n    def on_add(self, *args, **kwargs):\n        return self._child_policy.on_add(*args, **kwargs)\n\n    def on_remove(self, *args, **kwargs):\n        return self._child_policy.on_remove(*args, **kwargs)\n\nclass DefaultLoadBalancingPolicy(WrapperPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` wrapper that adds the ability to target a specific host first.\n\n    If no host is set on the query, the child policy's query plan will be used as is.\n    \"\"\"\n    _cluster_metadata = None\n\n    def populate(self, cluster, hosts):\n        self._cluster_metadata = cluster.metadata\n        self._child_policy.populate(cluster, hosts)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        if query and query.keyspace:\n            keyspace = query.keyspace\n        else:\n            keyspace = working_keyspace\n        addr = getattr(query, 'target_host', None) if query else None\n        target_host = self._cluster_metadata.get_host(addr)\n        child = self._child_policy\n        if target_host and target_host.is_up:\n            yield target_host\n            for h in child.make_query_plan(keyspace, query):\n                if h != target_host:\n                    yield h\n        else:\n            for h in child.make_query_plan(keyspace, query):\n                yield h\n\nclass DSELoadBalancingPolicy(DefaultLoadBalancingPolicy):\n    \"\"\"\n    *Deprecated:* This will be removed in the next major release,\n    consider using :class:`.DefaultLoadBalancingPolicy`.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(DSELoadBalancingPolicy, self).__init__(*args, **kwargs)\n        warnings.warn('DSELoadBalancingPolicy will be removed in 4.0. Consider using DefaultLoadBalancingPolicy.', DeprecationWarning)\n\nclass NeverRetryPolicy(RetryPolicy):\n\n    def _rethrow(self, *args, **kwargs):\n        return (self.RETHROW, None)\n    on_read_timeout = _rethrow\n    on_write_timeout = _rethrow\n    on_unavailable = _rethrow\nColDesc = namedtuple('ColDesc', ['ks', 'table', 'col'])\n\nclass ColumnEncryptionPolicy(object):\n    \"\"\"\n    A policy enabling (mostly) transparent encryption and decryption of data before it is\n    sent to the cluster.\n\n    Key materials and other configurations are specified on a per-column basis.  This policy can\n    then be used by driver structures which are aware of the underlying columns involved in their\n    work.  In practice this includes the following cases:\n\n    * Prepared statements - data for columns specified by the cluster's policy will be transparently\n      encrypted before they are sent\n    * Rows returned from any query - data for columns specified by the cluster's policy will be\n      transparently decrypted before they are returned to the user\n\n    To enable this functionality, create an instance of this class (or more likely a subclass)\n    before creating a cluster.  This policy should then be configured and supplied to the Cluster\n    at creation time via the :attr:`.Cluster.column_encryption_policy` attribute.\n    \"\"\"\n\n    def encrypt(self, coldesc, obj_bytes):\n        \"\"\"\n        Encrypt the specified bytes using the cryptography materials for the specified column.\n        Largely used internally, although this could also be used to encrypt values supplied\n        to non-prepared statements in a way that is consistent with this policy.\n        \"\"\"\n        raise NotImplementedError()\n\n    def decrypt(self, coldesc, encrypted_bytes):\n        \"\"\"\n        Decrypt the specified (encrypted) bytes using the cryptography materials for the\n        specified column.  Used internally; could be used externally as well but there's\n        not currently an obvious use case.\n        \"\"\"\n        raise NotImplementedError()\n\n    def add_column(self, coldesc, key):\n        \"\"\"\n        Provide cryptography materials to be used when encrypted and/or decrypting data\n        for the specified column.\n        \"\"\"\n        raise NotImplementedError()\n\n    def contains_column(self, coldesc):\n        \"\"\"\n        Predicate to determine if a specific column is supported by this policy.\n        Currently only used internally.\n        \"\"\"\n        raise NotImplementedError()\n\n    def encode_and_encrypt(self, coldesc, obj):\n        \"\"\"\n        Helper function to enable use of this policy on simple (i.e. non-prepared)\n        statements.\n        \"\"\"\n        raise NotImplementedError()",
    "cassandra/encoder.py": "\"\"\"\nThese functions are used to convert Python objects into CQL strings.\nWhen non-prepared statements are executed, these encoder functions are\ncalled on each query parameter.\n\"\"\"\nimport logging\nlog = logging.getLogger(__name__)\nfrom binascii import hexlify\nimport calendar\nimport datetime\nimport math\nimport sys\nimport types\nfrom uuid import UUID\nimport ipaddress\nfrom cassandra.util import OrderedDict, OrderedMap, OrderedMapSerializedKey, sortedset, Time, Date, Point, LineString, Polygon\n\ndef cql_quote(term):\n    if isinstance(term, str):\n        return \"'%s'\" % str(term).replace(\"'\", \"''\")\n    else:\n        return str(term)\n\nclass ValueSequence(list):\n    pass\n\nclass Encoder(object):\n    \"\"\"\n    A container for mapping python types to CQL string literals when working\n    with non-prepared statements.  The type :attr:`~.Encoder.mapping` can be\n    directly customized by users.\n    \"\"\"\n    mapping = None\n    '\\n    A map of python types to encoder functions.\\n    '\n\n    def cql_encode_none(self, val):\n        \"\"\"\n        Converts :const:`None` to the string 'NULL'.\n        \"\"\"\n        return 'NULL'\n\n    def cql_encode_unicode(self, val):\n        \"\"\"\n        Converts :class:`unicode` objects to UTF-8 encoded strings with quote escaping.\n        \"\"\"\n        return cql_quote(val.encode('utf-8'))\n\n    def cql_encode_str(self, val):\n        \"\"\"\n        Escapes quotes in :class:`str` objects.\n        \"\"\"\n        return cql_quote(val)\n\n    def cql_encode_str_quoted(self, val):\n        return \"'%s'\" % val\n\n    def cql_encode_bytes(self, val):\n        return (b'0x' + hexlify(val)).decode('utf-8')\n\n    def cql_encode_object(self, val):\n        \"\"\"\n        Default encoder for all objects that do not have a specific encoder function\n        registered. This function simply calls :meth:`str()` on the object.\n        \"\"\"\n        return str(val)\n\n    def cql_encode_float(self, val):\n        \"\"\"\n        Encode floats using repr to preserve precision\n        \"\"\"\n        if math.isinf(val):\n            return 'Infinity' if val > 0 else '-Infinity'\n        elif math.isnan(val):\n            return 'NaN'\n        else:\n            return repr(val)\n\n    def cql_encode_datetime(self, val):\n        \"\"\"\n        Converts a :class:`datetime.datetime` object to a (string) integer timestamp\n        with millisecond precision.\n        \"\"\"\n        timestamp = calendar.timegm(val.utctimetuple())\n        return str(int(timestamp * 1000.0 + getattr(val, 'microsecond', 0) / 1000.0))\n\n    def cql_encode_date(self, val):\n        \"\"\"\n        Converts a :class:`datetime.date` object to a string with format\n        ``YYYY-MM-DD``.\n        \"\"\"\n        return \"'%s'\" % val.strftime('%Y-%m-%d')\n\n    def cql_encode_time(self, val):\n        \"\"\"\n        Converts a :class:`cassandra.util.Time` object to a string with format\n        ``HH:MM:SS.mmmuuunnn``.\n        \"\"\"\n        return \"'%s'\" % val\n\n    def cql_encode_date_ext(self, val):\n        \"\"\"\n        Encodes a :class:`cassandra.util.Date` object as an integer\n        \"\"\"\n        return str(val.days_from_epoch + 2 ** 31)\n\n    def cql_encode_sequence(self, val):\n        \"\"\"\n        Converts a sequence to a string of the form ``(item1, item2, ...)``.  This\n        is suitable for ``IN`` value lists.\n        \"\"\"\n        return '(%s)' % ', '.join((self.mapping.get(type(v), self.cql_encode_object)(v) for v in val))\n    cql_encode_tuple = cql_encode_sequence\n    '\\n    Converts a sequence to a string of the form ``(item1, item2, ...)``.  This\\n    is suitable for ``tuple`` type columns.\\n    '\n\n    def cql_encode_map_collection(self, val):\n        \"\"\"\n        Converts a dict into a string of the form ``{key1: val1, key2: val2, ...}``.\n        This is suitable for ``map`` type columns.\n        \"\"\"\n        return '{%s}' % ', '.join(('%s: %s' % (self.mapping.get(type(k), self.cql_encode_object)(k), self.mapping.get(type(v), self.cql_encode_object)(v)) for k, v in val.items()))\n\n    def cql_encode_list_collection(self, val):\n        \"\"\"\n        Converts a sequence to a string of the form ``[item1, item2, ...]``.  This\n        is suitable for ``list`` type columns.\n        \"\"\"\n        return '[%s]' % ', '.join((self.mapping.get(type(v), self.cql_encode_object)(v) for v in val))\n\n    def cql_encode_set_collection(self, val):\n        \"\"\"\n        Converts a sequence to a string of the form ``{item1, item2, ...}``.  This\n        is suitable for ``set`` type columns.\n        \"\"\"\n        return '{%s}' % ', '.join((self.mapping.get(type(v), self.cql_encode_object)(v) for v in val))\n\n    def cql_encode_all_types(self, val, as_text_type=False):\n        \"\"\"\n        Converts any type into a CQL string, defaulting to ``cql_encode_object``\n        if :attr:`~Encoder.mapping` does not contain an entry for the type.\n        \"\"\"\n        encoded = self.mapping.get(type(val), self.cql_encode_object)(val)\n        if as_text_type and (not isinstance(encoded, str)):\n            return encoded.decode('utf-8')\n        return encoded\n\n    def cql_encode_ipaddress(self, val):\n        \"\"\"\n        Converts an ipaddress (IPV4Address, IPV6Address) to a CQL string. This\n        is suitable for ``inet`` type columns.\n        \"\"\"\n        return \"'%s'\" % val.compressed",
    "cassandra/__init__.py": "from enum import Enum\nimport logging\n\nclass NullHandler(logging.Handler):\n\n    def emit(self, record):\n        pass\nlogging.getLogger('cassandra').addHandler(NullHandler())\n__version_info__ = (3, 28, 0)\n__version__ = '.'.join(map(str, __version_info__))\n\nclass ConsistencyLevel(object):\n    \"\"\"\n    Spcifies how many replicas must respond for an operation to be considered\n    a success.  By default, ``ONE`` is used for all operations.\n    \"\"\"\n    ANY = 0\n    '\\n    Only requires that one replica receives the write *or* the coordinator\\n    stores a hint to replay later. Valid only for writes.\\n    '\n    ONE = 1\n    '\\n    Only one replica needs to respond to consider the operation a success\\n    '\n    TWO = 2\n    '\\n    Two replicas must respond to consider the operation a success\\n    '\n    THREE = 3\n    '\\n    Three replicas must respond to consider the operation a success\\n    '\n    QUORUM = 4\n    '\\n    ``ceil(RF/2) + 1`` replicas must respond to consider the operation a success\\n    '\n    ALL = 5\n    '\\n    All replicas must respond to consider the operation a success\\n    '\n    LOCAL_QUORUM = 6\n    '\\n    Requires a quorum of replicas in the local datacenter\\n    '\n    EACH_QUORUM = 7\n    '\\n    Requires a quorum of replicas in each datacenter\\n    '\n    SERIAL = 8\n    \"\\n    For conditional inserts/updates that utilize Cassandra's lightweight\\n    transactions, this requires consensus among all replicas for the\\n    modified data.\\n    \"\n    LOCAL_SERIAL = 9\n    '\\n    Like :attr:`~ConsistencyLevel.SERIAL`, but only requires consensus\\n    among replicas in the local datacenter.\\n    '\n    LOCAL_ONE = 10\n    '\\n    Sends a request only to replicas in the local datacenter and waits for\\n    one response.\\n    '\n\n    @staticmethod\n    def is_serial(cl):\n        return cl == ConsistencyLevel.SERIAL or cl == ConsistencyLevel.LOCAL_SERIAL\nConsistencyLevel.value_to_name = {ConsistencyLevel.ANY: 'ANY', ConsistencyLevel.ONE: 'ONE', ConsistencyLevel.TWO: 'TWO', ConsistencyLevel.THREE: 'THREE', ConsistencyLevel.QUORUM: 'QUORUM', ConsistencyLevel.ALL: 'ALL', ConsistencyLevel.LOCAL_QUORUM: 'LOCAL_QUORUM', ConsistencyLevel.EACH_QUORUM: 'EACH_QUORUM', ConsistencyLevel.SERIAL: 'SERIAL', ConsistencyLevel.LOCAL_SERIAL: 'LOCAL_SERIAL', ConsistencyLevel.LOCAL_ONE: 'LOCAL_ONE'}\nConsistencyLevel.name_to_value = {'ANY': ConsistencyLevel.ANY, 'ONE': ConsistencyLevel.ONE, 'TWO': ConsistencyLevel.TWO, 'THREE': ConsistencyLevel.THREE, 'QUORUM': ConsistencyLevel.QUORUM, 'ALL': ConsistencyLevel.ALL, 'LOCAL_QUORUM': ConsistencyLevel.LOCAL_QUORUM, 'EACH_QUORUM': ConsistencyLevel.EACH_QUORUM, 'SERIAL': ConsistencyLevel.SERIAL, 'LOCAL_SERIAL': ConsistencyLevel.LOCAL_SERIAL, 'LOCAL_ONE': ConsistencyLevel.LOCAL_ONE}\n\ndef consistency_value_to_name(value):\n    return ConsistencyLevel.value_to_name[value] if value is not None else 'Not Set'\n\nclass ProtocolVersion(object):\n    \"\"\"\n    Defines native protocol versions supported by this driver.\n    \"\"\"\n    V1 = 1\n    '\\n    v1, supported in Cassandra 1.2-->2.2\\n    '\n    V2 = 2\n    '\\n    v2, supported in Cassandra 2.0-->2.2;\\n    added support for lightweight transactions, batch operations, and automatic query paging.\\n    '\n    V3 = 3\n    '\\n    v3, supported in Cassandra 2.1-->3.x+;\\n    added support for protocol-level client-side timestamps (see :attr:`.Session.use_client_timestamp`),\\n    serial consistency levels for :class:`~.BatchStatement`, and an improved connection pool.\\n    '\n    V4 = 4\n    '\\n    v4, supported in Cassandra 2.2-->3.x+;\\n    added a number of new types, server warnings, new failure messages, and custom payloads. Details in the\\n    `project docs <https://github.com/apache/cassandra/blob/trunk/doc/native_protocol_v4.spec>`_\\n    '\n    V5 = 5\n    '\\n    v5, in beta from 3.x+. Finalised in 4.0-beta5\\n    '\n    V6 = 6\n    '\\n    v6, in beta from 4.0-beta5\\n    '\n    DSE_V1 = 65\n    '\\n    DSE private protocol v1, supported in DSE 5.1+\\n    '\n    DSE_V2 = 66\n    '\\n    DSE private protocol v2, supported in DSE 6.0+\\n    '\n    SUPPORTED_VERSIONS = (DSE_V2, DSE_V1, V6, V5, V4, V3, V2, V1)\n    '\\n    A tuple of all supported protocol versions\\n    '\n    BETA_VERSIONS = (V6,)\n    '\\n    A tuple of all beta protocol versions\\n    '\n    MIN_SUPPORTED = min(SUPPORTED_VERSIONS)\n    '\\n    Minimum protocol version supported by this driver.\\n    '\n    MAX_SUPPORTED = max(SUPPORTED_VERSIONS)\n    '\\n    Maximum protocol version supported by this driver.\\n    '\n\n    @classmethod\n    def get_lower_supported(cls, previous_version):\n        \"\"\"\n        Return the lower supported protocol version. Beta versions are omitted.\n        \"\"\"\n        try:\n            version = next((v for v in sorted(ProtocolVersion.SUPPORTED_VERSIONS, reverse=True) if v not in ProtocolVersion.BETA_VERSIONS and v < previous_version))\n        except StopIteration:\n            version = 0\n        return version\n\n    @classmethod\n    def uses_int_query_flags(cls, version):\n        return version >= cls.V5\n\n    @classmethod\n    def uses_prepare_flags(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def uses_prepared_metadata(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def uses_error_code_map(cls, version):\n        return version >= cls.V5\n\n    @classmethod\n    def uses_keyspace_flag(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def has_continuous_paging_support(cls, version):\n        return version >= cls.DSE_V1\n\n    @classmethod\n    def has_continuous_paging_next_pages(cls, version):\n        return version >= cls.DSE_V2\n\n    @classmethod\n    def has_checksumming_support(cls, version):\n        return cls.V5 <= version < cls.DSE_V1\n\nclass WriteType(object):\n    \"\"\"\n    For usage with :class:`.RetryPolicy`, this describe a type\n    of write operation.\n    \"\"\"\n    SIMPLE = 0\n    '\\n    A write to a single partition key. Such writes are guaranteed to be atomic\\n    and isolated.\\n    '\n    BATCH = 1\n    '\\n    A write to multiple partition keys that used the distributed batch log to\\n    ensure atomicity.\\n    '\n    UNLOGGED_BATCH = 2\n    '\\n    A write to multiple partition keys that did not use the distributed batch\\n    log. Atomicity for such writes is not guaranteed.\\n    '\n    COUNTER = 3\n    '\\n    A counter write (for one or multiple partition keys). Such writes should\\n    not be replayed in order to avoid overcount.\\n    '\n    BATCH_LOG = 4\n    '\\n    The initial write to the distributed batch log that Cassandra performs\\n    internally before a BATCH write.\\n    '\n    CAS = 5\n    '\\n    A lighweight-transaction write, such as \"DELETE ... IF EXISTS\".\\n    '\n    VIEW = 6\n    '\\n    This WriteType is only seen in results for requests that were unable to\\n    complete MV operations.\\n    '\n    CDC = 7\n    '\\n    This WriteType is only seen in results for requests that were unable to\\n    complete CDC operations.\\n    '\nWriteType.name_to_value = {'SIMPLE': WriteType.SIMPLE, 'BATCH': WriteType.BATCH, 'UNLOGGED_BATCH': WriteType.UNLOGGED_BATCH, 'COUNTER': WriteType.COUNTER, 'BATCH_LOG': WriteType.BATCH_LOG, 'CAS': WriteType.CAS, 'VIEW': WriteType.VIEW, 'CDC': WriteType.CDC}\nWriteType.value_to_name = {v: k for k, v in WriteType.name_to_value.items()}\n\nclass SchemaChangeType(object):\n    DROPPED = 'DROPPED'\n    CREATED = 'CREATED'\n    UPDATED = 'UPDATED'\n\nclass SchemaTargetType(object):\n    KEYSPACE = 'KEYSPACE'\n    TABLE = 'TABLE'\n    TYPE = 'TYPE'\n    FUNCTION = 'FUNCTION'\n    AGGREGATE = 'AGGREGATE'\n\nclass SignatureDescriptor(object):\n\n    def __init__(self, name, argument_types):\n        self.name = name\n        self.argument_types = argument_types\n\n    @property\n    def signature(self):\n        \"\"\"\n        function signature string in the form 'name([type0[,type1[...]]])'\n\n        can be used to uniquely identify overloaded function names within a keyspace\n        \"\"\"\n        return self.format_signature(self.name, self.argument_types)\n\n    @staticmethod\n    def format_signature(name, argument_types):\n        return '%s(%s)' % (name, ','.join((t for t in argument_types)))\n\n    def __repr__(self):\n        return '%s(%s, %s)' % (self.__class__.__name__, self.name, self.argument_types)\n\nclass UserFunctionDescriptor(SignatureDescriptor):\n    \"\"\"\n    Describes a User function by name and argument signature\n    \"\"\"\n    name = None\n    '\\n    name of the function\\n    '\n    argument_types = None\n    '\\n    Ordered list of CQL argument type names comprising the type signature\\n    '\n\nclass UserAggregateDescriptor(SignatureDescriptor):\n    \"\"\"\n    Describes a User aggregate function by name and argument signature\n    \"\"\"\n    name = None\n    '\\n    name of the aggregate\\n    '\n    argument_types = None\n    '\\n    Ordered list of CQL argument type names comprising the type signature\\n    '\n\nclass DriverException(Exception):\n    \"\"\"\n    Base for all exceptions explicitly raised by the driver.\n    \"\"\"\n    pass\n\nclass RequestExecutionException(DriverException):\n    \"\"\"\n    Base for request execution exceptions returned from the server.\n    \"\"\"\n    pass\n\nclass Unavailable(RequestExecutionException):\n    \"\"\"\n    There were not enough live replicas to satisfy the requested consistency\n    level, so the coordinator node immediately failed the request without\n    forwarding it to any replicas.\n    \"\"\"\n    consistency = None\n    ' The requested :class:`ConsistencyLevel` '\n    required_replicas = None\n    ' The number of replicas that needed to be live to complete the operation '\n    alive_replicas = None\n    ' The number of replicas that were actually alive '\n\n    def __init__(self, summary_message, consistency=None, required_replicas=None, alive_replicas=None):\n        self.consistency = consistency\n        self.required_replicas = required_replicas\n        self.alive_replicas = alive_replicas\n        Exception.__init__(self, summary_message + ' info=' + repr({'consistency': consistency_value_to_name(consistency), 'required_replicas': required_replicas, 'alive_replicas': alive_replicas}))\n\nclass Timeout(RequestExecutionException):\n    \"\"\"\n    Replicas failed to respond to the coordinator node before timing out.\n    \"\"\"\n    consistency = None\n    ' The requested :class:`ConsistencyLevel` '\n    required_responses = None\n    ' The number of required replica responses '\n    received_responses = None\n    '\\n    The number of replicas that responded before the coordinator timed out\\n    the operation\\n    '\n\n    def __init__(self, summary_message, consistency=None, required_responses=None, received_responses=None, **kwargs):\n        self.consistency = consistency\n        self.required_responses = required_responses\n        self.received_responses = received_responses\n        if 'write_type' in kwargs:\n            kwargs['write_type'] = WriteType.value_to_name[kwargs['write_type']]\n        info = {'consistency': consistency_value_to_name(consistency), 'required_responses': required_responses, 'received_responses': received_responses}\n        info.update(kwargs)\n        Exception.__init__(self, summary_message + ' info=' + repr(info))\n\nclass ReadTimeout(Timeout):\n    \"\"\"\n    A subclass of :exc:`Timeout` for read operations.\n\n    This indicates that the replicas failed to respond to the coordinator\n    node before the configured timeout. This timeout is configured in\n    ``cassandra.yaml`` with the ``read_request_timeout_in_ms``\n    and ``range_request_timeout_in_ms`` options.\n    \"\"\"\n    data_retrieved = None\n    '\\n    A boolean indicating whether the requested data was retrieved\\n    by the coordinator from any replicas before it timed out the\\n    operation\\n    '\n\n    def __init__(self, message, data_retrieved=None, **kwargs):\n        Timeout.__init__(self, message, **kwargs)\n        self.data_retrieved = data_retrieved\n\nclass WriteTimeout(Timeout):\n    \"\"\"\n    A subclass of :exc:`Timeout` for write operations.\n\n    This indicates that the replicas failed to respond to the coordinator\n    node before the configured timeout. This timeout is configured in\n    ``cassandra.yaml`` with the ``write_request_timeout_in_ms``\n    option.\n    \"\"\"\n    write_type = None\n    '\\n    The type of write operation, enum on :class:`~cassandra.policies.WriteType`\\n    '\n\n    def __init__(self, message, write_type=None, **kwargs):\n        kwargs['write_type'] = write_type\n        Timeout.__init__(self, message, **kwargs)\n        self.write_type = write_type\n\nclass CDCWriteFailure(RequestExecutionException):\n    \"\"\"\n    Hit limit on data in CDC folder, writes are rejected\n    \"\"\"\n\n    def __init__(self, message):\n        Exception.__init__(self, message)\n\nclass CoordinationFailure(RequestExecutionException):\n    \"\"\"\n    Replicas sent a failure to the coordinator.\n    \"\"\"\n    consistency = None\n    ' The requested :class:`ConsistencyLevel` '\n    required_responses = None\n    ' The number of required replica responses '\n    received_responses = None\n    '\\n    The number of replicas that responded before the coordinator timed out\\n    the operation\\n    '\n    failures = None\n    '\\n    The number of replicas that sent a failure message\\n    '\n    error_code_map = None\n    '\\n    A map of inet addresses to error codes representing replicas that sent\\n    a failure message.  Only set when `protocol_version` is 5 or higher.\\n    '\n\n    def __init__(self, summary_message, consistency=None, required_responses=None, received_responses=None, failures=None, error_code_map=None):\n        self.consistency = consistency\n        self.required_responses = required_responses\n        self.received_responses = received_responses\n        self.failures = failures\n        self.error_code_map = error_code_map\n        info_dict = {'consistency': consistency_value_to_name(consistency), 'required_responses': required_responses, 'received_responses': received_responses, 'failures': failures}\n        if error_code_map is not None:\n            formatted_map = dict(((addr, '0x%04x' % err_code) for addr, err_code in error_code_map.items()))\n            info_dict['error_code_map'] = formatted_map\n        Exception.__init__(self, summary_message + ' info=' + repr(info_dict))\n\nclass ReadFailure(CoordinationFailure):\n    \"\"\"\n    A subclass of :exc:`CoordinationFailure` for read operations.\n\n    This indicates that the replicas sent a failure message to the coordinator.\n    \"\"\"\n    data_retrieved = None\n    '\\n    A boolean indicating whether the requested data was retrieved\\n    by the coordinator from any replicas before it timed out the\\n    operation\\n    '\n\n    def __init__(self, message, data_retrieved=None, **kwargs):\n        CoordinationFailure.__init__(self, message, **kwargs)\n        self.data_retrieved = data_retrieved\n\nclass WriteFailure(CoordinationFailure):\n    \"\"\"\n    A subclass of :exc:`CoordinationFailure` for write operations.\n\n    This indicates that the replicas sent a failure message to the coordinator.\n    \"\"\"\n    write_type = None\n    '\\n    The type of write operation, enum on :class:`~cassandra.policies.WriteType`\\n    '\n\n    def __init__(self, message, write_type=None, **kwargs):\n        CoordinationFailure.__init__(self, message, **kwargs)\n        self.write_type = write_type\n\nclass FunctionFailure(RequestExecutionException):\n    \"\"\"\n    User Defined Function failed during execution\n    \"\"\"\n    keyspace = None\n    '\\n    Keyspace of the function\\n    '\n    function = None\n    '\\n    Name of the function\\n    '\n    arg_types = None\n    '\\n    List of argument type names of the function\\n    '\n\n    def __init__(self, summary_message, keyspace, function, arg_types):\n        self.keyspace = keyspace\n        self.function = function\n        self.arg_types = arg_types\n        Exception.__init__(self, summary_message)\n\nclass RequestValidationException(DriverException):\n    \"\"\"\n    Server request validation failed\n    \"\"\"\n    pass\n\nclass ConfigurationException(RequestValidationException):\n    \"\"\"\n    Server indicated request errro due to current configuration\n    \"\"\"\n    pass\n\nclass AlreadyExists(ConfigurationException):\n    \"\"\"\n    An attempt was made to create a keyspace or table that already exists.\n    \"\"\"\n    keyspace = None\n    '\\n    The name of the keyspace that already exists, or, if an attempt was\\n    made to create a new table, the keyspace that the table is in.\\n    '\n    table = None\n    '\\n    The name of the table that already exists, or, if an attempt was\\n    make to create a keyspace, :const:`None`.\\n    '\n\n    def __init__(self, keyspace=None, table=None):\n        if table:\n            message = \"Table '%s.%s' already exists\" % (keyspace, table)\n        else:\n            message = \"Keyspace '%s' already exists\" % (keyspace,)\n        Exception.__init__(self, message)\n        self.keyspace = keyspace\n        self.table = table\n\nclass InvalidRequest(RequestValidationException):\n    \"\"\"\n    A query was made that was invalid for some reason, such as trying to set\n    the keyspace for a connection to a nonexistent keyspace.\n    \"\"\"\n    pass\n\nclass Unauthorized(RequestValidationException):\n    \"\"\"\n    The current user is not authorized to perform the requested operation.\n    \"\"\"\n    pass\n\nclass AuthenticationFailed(DriverException):\n    \"\"\"\n    Failed to authenticate.\n    \"\"\"\n    pass\n\nclass OperationTimedOut(DriverException):\n    \"\"\"\n    The operation took longer than the specified (client-side) timeout\n    to complete.  This is not an error generated by Cassandra, only\n    the driver.\n    \"\"\"\n    errors = None\n    '\\n    A dict of errors keyed by the :class:`~.Host` against which they occurred.\\n    '\n    last_host = None\n    '\\n    The last :class:`~.Host` this operation was attempted against.\\n    '\n\n    def __init__(self, errors=None, last_host=None):\n        self.errors = errors\n        self.last_host = last_host\n        message = 'errors=%s, last_host=%s' % (self.errors, self.last_host)\n        Exception.__init__(self, message)\n\nclass UnsupportedOperation(DriverException):\n    \"\"\"\n    An attempt was made to use a feature that is not supported by the\n    selected protocol version.  See :attr:`Cluster.protocol_version`\n    for more details.\n    \"\"\"\n    pass\n\nclass UnresolvableContactPoints(DriverException):\n    \"\"\"\n    The driver was unable to resolve any provided hostnames.\n\n    Note that this is *not* raised when a :class:`.Cluster` is created with no\n    contact points, only when lookup fails for all hosts\n    \"\"\"\n    pass\n\nclass OperationType(Enum):\n    Read = 0\n    Write = 1\n\nclass RateLimitReached(ConfigurationException):\n    \"\"\"\n    Rate limit was exceeded for a partition affected by the request.\n    \"\"\"\n    op_type = None\n    rejected_by_coordinator = False\n\n    def __init__(self, op_type=None, rejected_by_coordinator=False):\n        self.op_type = op_type\n        self.rejected_by_coordinator = rejected_by_coordinator\n        message = f'[request_error_rate_limit_reached OpType={op_type.name} RejectedByCoordinator={rejected_by_coordinator}]'\n        Exception.__init__(self, message)"
  }
}