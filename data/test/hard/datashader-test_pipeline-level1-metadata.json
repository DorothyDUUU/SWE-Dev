{
  "dir_path": "/app/datashader",
  "package_name": "datashader",
  "sample_name": "datashader-test_pipeline",
  "src_dir": "datashader/",
  "test_dir": "datashader/tests/",
  "test_file": "datashader/tests/test_pipeline.py",
  "test_code": "from __future__ import annotations\nimport numpy as np\nimport pandas as pd\nimport pytest\nimport datashader as ds\nimport datashader.transfer_functions as tf\n\n\ndf = pd.DataFrame({'x': np.array([0.] * 10 + [1] * 10),\n                   'y': np.array([0.] * 5 + [1] * 5 + [0] * 5 + [1] * 5),\n                   'f64': np.arange(20, dtype='f8')})\ndf.loc['f64', 2] = np.nan\n\ncvs = ds.Canvas(plot_width=2, plot_height=2, x_range=(0, 1), y_range=(0, 1))\ncvs10 = ds.Canvas(plot_width=10, plot_height=10, x_range=(0, 1), y_range=(0, 1))\n\n\ndef test_pipeline():\n    pipeline = ds.Pipeline(df, ds.Point('x', 'y'))\n    img = pipeline((0, 1), (0, 1), 2, 2)\n    agg = cvs.points(df, 'x', 'y', ds.count())\n    assert img.equals(tf.shade(agg))\n\n    def color_fn(agg):\n        return tf.shade(agg, 'pink', 'red')\n    pipeline.color_fn = color_fn\n    img = pipeline((0, 1), (0, 1), 2, 2)\n    assert img.equals(color_fn(agg))\n\n    def transform_fn(agg):\n        return agg + 1\n    pipeline.transform_fn = transform_fn\n    img = pipeline((0, 1), (0, 1), 2, 2)\n    assert img.equals(color_fn(transform_fn(agg)))\n\n    pipeline = ds.Pipeline(df, ds.Point('x', 'y'), ds.sum('f64'))\n    img = pipeline((0, 1), (0, 1), 2, 2)\n    agg = cvs.points(df, 'x', 'y', ds.sum('f64'))\n    assert img.equals(tf.shade(agg))\n\n\n@pytest.mark.parametrize(\"line_width\", [0.0, 0.5, 1.0, 2.0])\ndef test_pipeline_antialias(line_width):\n    glyph = ds.glyphs.LineAxis0('x', 'y')\n\n    glyph.set_line_width(line_width=line_width)\n    assert glyph._line_width == line_width\n    assert glyph.antialiased == (line_width > 0)\n\n    pipeline = ds.Pipeline(df, glyph)\n    img = pipeline(width=cvs10.plot_width, height=cvs10.plot_height,\n                   x_range=cvs10.x_range, y_range=cvs10.y_range)\n    agg = cvs10.line(df, 'x', 'y', agg=ds.reductions.count(), line_width=line_width)\n    assert img.equals(tf.dynspread(tf.shade(agg)))\n",
  "GT_file_code": {
    "datashader/core.py": "from __future__ import annotations\n\nfrom numbers import Number\nfrom math import log10\nimport warnings\nimport contextlib\n\nimport numpy as np\nimport pandas as pd\nfrom packaging.version import Version\nfrom xarray import DataArray, Dataset\n\nfrom .utils import Dispatcher, ngjit, calc_res, calc_bbox, orient_array, \\\n    dshape_from_xarray_dataset\nfrom .utils import get_indices, dshape_from_pandas, dshape_from_dask\nfrom .utils import Expr # noqa (API import)\nfrom .resampling import resample_2d, resample_2d_distributed\nfrom . import reductions as rd\n\ntry:\n    import dask.dataframe as dd\n    import dask.array as da\nexcept ImportError:\n    dd, da = None, None\n\ntry:\n    import cudf\nexcept Exception:\n    cudf = None\n\ntry:\n    import dask_cudf\nexcept Exception:\n    dask_cudf = None\n\ntry:\n    import spatialpandas\nexcept Exception:\n    spatialpandas = None\n\nclass Axis:\n    \"\"\"Interface for implementing axis transformations.\n\n    Instances hold implementations of transformations to and from axis space.\n    The default implementation is equivalent to:\n\n    >>> def forward_transform(data_x):\n    ...     scale * mapper(data_x) + t\n    >>> def inverse_transform(axis_x):\n    ...     inverse_mapper((axis_x - t)/s)\n\n    Where ``mapper`` and ``inverse_mapper`` are elementwise functions mapping\n    to and from axis-space respectively, and ``scale`` and ``transform`` are\n    parameters describing a linear scale and translate transformation, computed\n    by the ``compute_scale_and_translate`` method.\n    \"\"\"\n\n    def compute_scale_and_translate(self, range, n):\n        \"\"\"Compute the scale and translate parameters for a linear transformation\n        ``output = s * input + t``, mapping from data space to axis space.\n\n        Parameters\n        ----------\n        range : tuple\n            A tuple representing the range ``[min, max]`` along the axis, in\n            data space. Both min and max are inclusive.\n        n : int\n            The number of bins along the axis.\n\n        Returns\n        -------\n        s, t : floats\n        \"\"\"\n        start, end = map(self.mapper, range)\n        s = n/(end - start)\n        t = -start * s\n        return s, t\n\n    def compute_index(self, st, n):\n        \"\"\"Compute a 1D array representing the axis index.\n\n        Parameters\n        ----------\n        st : tuple\n            A tuple of ``(scale, translate)`` parameters.\n        n : int\n            The number of bins along the dimension.\n\n        Returns\n        -------\n        index : ndarray\n        \"\"\"\n        px = np.arange(n)+0.5\n        s, t = st\n        return self.inverse_mapper((px - t)/s)\n\n    def mapper(val):\n        \"\"\"A mapping from data space to axis space\"\"\"\n        raise NotImplementedError\n\n    def inverse_mapper(val):\n        \"\"\"A mapping from axis space to data space\"\"\"\n        raise NotImplementedError\n\n    def validate(self, range):\n        \"\"\"Given a range (low,high), raise an error if the range is invalid for this axis\"\"\"\n        pass\n\n\nclass LinearAxis(Axis):\n    \"\"\"A linear Axis\"\"\"\n    @staticmethod\n    @ngjit\n    def mapper(val):\n        return val\n\n    @staticmethod\n    @ngjit\n    def inverse_mapper(val):\n        return val\n\n\nclass LogAxis(Axis):\n    \"\"\"A base-10 logarithmic Axis\"\"\"\n    @staticmethod\n    @ngjit\n    def mapper(val):\n        return log10(float(val))\n\n    @staticmethod\n    @ngjit\n    def inverse_mapper(val):\n        y = 10 # temporary workaround for https://github.com/numba/numba/issues/3135 (numba 0.39.0)\n        return y**val\n\n    def validate(self, range):\n        if range is None:\n            # Nothing to check if no range\n            return\n        if range[0] <= 0 or range[1] <= 0:\n            raise ValueError('Range values must be >0 for logarithmic axes')\n\n\n_axis_lookup = {'linear': LinearAxis(), 'log': LogAxis()}\n\n\ndef validate_xy_or_geometry(glyph, x, y, geometry):\n    if (geometry is None and (x is None or y is None) or\n            geometry is not None and (x is not None or y is not None)):\n        raise ValueError(\"\"\"\n{glyph} coordinates may be specified by providing both the x and y arguments, or by\nproviding the geometry argument. Received:\n    x: {x}\n    y: {y}\n    geometry: {geometry}\n\"\"\".format(glyph=glyph, x=repr(x), y=repr(y), geometry=repr(geometry)))\n\n\nclass Canvas:\n    \"\"\"An abstract canvas representing the space in which to bin.\n\n    Parameters\n    ----------\n    plot_width, plot_height : int, optional\n        Width and height of the output aggregate in pixels.\n    x_range, y_range : tuple, optional\n        A tuple representing the bounds inclusive space ``[min, max]`` along\n        the axis.\n    x_axis_type, y_axis_type : str, optional\n        The type of the axis. Valid options are ``'linear'`` [default], and\n        ``'log'``.\n    \"\"\"\n    def __init__(self, plot_width=600, plot_height=600,\n                 x_range=None, y_range=None,\n                 x_axis_type='linear', y_axis_type='linear'):\n        self.plot_width = plot_width\n        self.plot_height = plot_height\n        self.x_range = None if x_range is None else tuple(x_range)\n        self.y_range = None if y_range is None else tuple(y_range)\n        self.x_axis = _axis_lookup[x_axis_type]\n        self.y_axis = _axis_lookup[y_axis_type]\n\n    def points(self, source, x=None, y=None, agg=None, geometry=None):\n        \"\"\"Compute a reduction by pixel, mapping data to pixels as points.\n\n        Parameters\n        ----------\n        source : pandas.DataFrame, dask.DataFrame, or xarray.DataArray/Dataset\n            The input datasource.\n        x, y : str\n            Column names for the x and y coordinates of each point. If provided,\n            the geometry argument may not also be provided.\n        agg : Reduction, optional\n            Reduction to compute. Default is ``count()``.\n        geometry: str\n            Column name of a PointsArray of the coordinates of each point. If provided,\n            the x and y arguments may not also be provided.\n        \"\"\"\n        from .glyphs import Point, MultiPointGeometry\n        from .reductions import count as count_rdn\n\n        validate_xy_or_geometry('Point', x, y, geometry)\n\n        if agg is None:\n            agg = count_rdn()\n\n        if geometry is None:\n            glyph = Point(x, y)\n        else:\n            if spatialpandas and isinstance(source, spatialpandas.dask.DaskGeoDataFrame):\n                # Downselect partitions to those that may contain points in viewport\n                x_range = self.x_range if self.x_range is not None else (None, None)\n                y_range = self.y_range if self.y_range is not None else (None, None)\n                source = source.cx_partitions[slice(*x_range), slice(*y_range)]\n                glyph = MultiPointGeometry(geometry)\n            elif spatialpandas and isinstance(source, spatialpandas.GeoDataFrame):\n                glyph = MultiPointGeometry(geometry)\n            elif (geopandas_source := self._source_from_geopandas(source)) is not None:\n                source = geopandas_source\n                from datashader.glyphs.points import MultiPointGeoPandas\n                glyph = MultiPointGeoPandas(geometry)\n            else:\n                raise ValueError(\n                    \"source must be an instance of spatialpandas.GeoDataFrame, \"\n                    \"spatialpandas.dask.DaskGeoDataFrame, geopandas.GeoDataFrame, or \"\n                    \"dask_geopandas.GeoDataFrame. Received objects of type {typ}\".format(\n                        typ=type(source)))\n\n        return bypixel(source, self, glyph, agg)\n\n    def line(self, source, x=None, y=None, agg=None, axis=0, geometry=None,\n             line_width=0, antialias=False):\n        \"\"\"Compute a reduction by pixel, mapping data to pixels as one or\n        more lines.\n\n        For aggregates that take in extra fields, the interpolated bins will\n        receive the fields from the previous point. In pseudocode:\n\n        >>> for i in range(len(rows) - 1):    # doctest: +SKIP\n        ...     row0 = rows[i]\n        ...     row1 = rows[i + 1]\n        ...     for xi, yi in interpolate(row0.x, row0.y, row1.x, row1.y):\n        ...         add_to_aggregate(xi, yi, row0)\n\n        Parameters\n        ----------\n        source : pandas.DataFrame, dask.DataFrame, or xarray.DataArray/Dataset\n            The input datasource.\n        x, y : str or number or list or tuple or np.ndarray\n            Specification of the x and y coordinates of each vertex\n            * str or number: Column labels in source\n            * list or tuple: List or tuple of column labels in source\n            * np.ndarray: When axis=1, a literal array of the\n              coordinates to be used for every row\n        agg : Reduction, optional\n            Reduction to compute. Default is ``any()``.\n        axis : 0 or 1, default 0\n            Axis in source to draw lines along\n            * 0: Draw lines using data from the specified columns across\n                 all rows in source\n            * 1: Draw one line per row in source using data from the\n                 specified columns\n        geometry : str\n            Column name of a LinesArray of the coordinates of each line. If provided,\n            the x and y arguments may not also be provided.\n        line_width : number, optional\n            Width of the line to draw, in pixels. If zero, the\n            default, lines are drawn using a simple algorithm with a\n            blocky single-pixel width based on whether the line passes\n            through each pixel or does not. If greater than one, lines\n            are drawn with the specified width using a slower and\n            more complex antialiasing algorithm with fractional values\n            along each edge, so that lines have a more uniform visual\n            appearance across all angles. Line widths between 0 and 1\n            effectively use a line_width of 1 pixel but with a\n            proportionate reduction in the strength of each pixel,\n            approximating the visual appearance of a subpixel line\n            width.\n        antialias : bool, optional\n            This option is kept for backward compatibility only.\n            ``True`` is equivalent to ``line_width=1`` and\n            ``False`` (the default) to ``line_width=0``. Do not specify\n            both ``antialias`` and ``line_width`` in the same call as a\n            ``ValueError`` will be raised if they disagree.\n\n        Examples\n        --------\n        Define a canvas and a pandas DataFrame with 6 rows\n        >>> import pandas as pd  # doctest: +SKIP\n        ... import numpy as np\n        ... import datashader as ds\n        ... from datashader import Canvas\n        ... import datashader.transfer_functions as tf\n        ... cvs = Canvas()\n        ... df = pd.DataFrame({\n        ...    'A1': [1, 1.5, 2, 2.5, 3, 4],\n        ...    'A2': [1.5, 2, 3, 3.2, 4, 5],\n        ...    'B1': [10, 12, 11, 14, 13, 15],\n        ...    'B2': [11, 9, 10, 7, 8, 12],\n        ... }, dtype='float64')\n\n        Aggregate one line across all rows, with coordinates df.A1 by df.B1\n        >>> agg = cvs.line(df, x='A1', y='B1', axis=0)  # doctest: +SKIP\n        ... tf.spread(tf.shade(agg))\n\n        Aggregate two lines across all rows. The first with coordinates\n        df.A1 by df.B1 and the second with coordinates df.A2 by df.B2\n        >>> agg = cvs.line(df, x=['A1', 'A2'], y=['B1', 'B2'], axis=0)  # doctest: +SKIP\n        ... tf.spread(tf.shade(agg))\n\n        Aggregate two lines across all rows where the lines share the same\n        x coordinates. The first line will have coordinates df.A1 by df.B1\n        and the second will have coordinates df.A1 by df.B2\n        >>> agg = cvs.line(df, x='A1', y=['B1', 'B2'], axis=0)  # doctest: +SKIP\n        ... tf.spread(tf.shade(agg))\n\n        Aggregate 6 length-2 lines, one per row, where the ith line has\n        coordinates [df.A1[i], df.A2[i]] by [df.B1[i], df.B2[i]]\n        >>> agg = cvs.line(df, x=['A1', 'A2'], y=['B1', 'B2'], axis=1)  # doctest: +SKIP\n        ... tf.spread(tf.shade(agg))\n\n        Aggregate 6 length-4 lines, one per row, where the x coordinates\n        of every line are [0, 1, 2, 3] and the y coordinates of the ith line\n        are [df.A1[i], df.A2[i], df.B1[i], df.B2[i]].\n        >>> agg = cvs.line(df,  # doctest: +SKIP\n        ...                x=np.arange(4),\n        ...                y=['A1', 'A2', 'B1', 'B2'],\n        ...                axis=1)\n        ... tf.spread(tf.shade(agg))\n\n        Aggregate RaggedArrays of variable length lines, one per row\n        (requires pandas >= 0.24.0)\n        >>> df_ragged = pd.DataFrame({  # doctest: +SKIP\n        ...    'A1': pd.array([\n        ...        [1, 1.5], [2, 2.5, 3], [1.5, 2, 3, 4], [3.2, 4, 5]],\n        ...        dtype='Ragged[float32]'),\n        ...    'B1': pd.array([\n        ...        [10, 12], [11, 14, 13], [10, 7, 9, 10], [7, 8, 12]],\n        ...        dtype='Ragged[float32]'),\n        ...    'group': pd.Categorical([0, 1, 2, 1])\n        ... })\n        ...\n        ... agg = cvs.line(df_ragged, x='A1', y='B1', axis=1)\n        ... tf.spread(tf.shade(agg))\n\n        Aggregate RaggedArrays of variable length lines by group column,\n        one per row (requires pandas >= 0.24.0)\n        >>> agg = cvs.line(df_ragged, x='A1', y='B1',  # doctest: +SKIP\n        ...                agg=ds.count_cat('group'), axis=1)\n        ... tf.spread(tf.shade(agg))\n        \"\"\"\n        from .glyphs import (LineAxis0, LinesAxis1, LinesAxis1XConstant,\n                             LinesAxis1YConstant, LineAxis0Multi,\n                             LinesAxis1Ragged, LineAxis1Geometry, LinesXarrayCommonX)\n\n        validate_xy_or_geometry('Line', x, y, geometry)\n\n        if agg is None:\n            agg = rd.any()\n\n        if line_width is None:\n            line_width = 0\n\n        # Check and convert antialias kwarg to line_width.\n        if antialias and line_width != 0:\n            raise ValueError(\n                \"Do not specify values for both the line_width and \\n\"\n                \"antialias keyword arguments; use line_width instead.\")\n        if antialias:\n            line_width = 1.0\n\n        if geometry is not None:\n            if spatialpandas and isinstance(source, spatialpandas.dask.DaskGeoDataFrame):\n                # Downselect partitions to those that may contain lines in viewport\n                x_range = self.x_range if self.x_range is not None else (None, None)\n                y_range = self.y_range if self.y_range is not None else (None, None)\n                source = source.cx_partitions[slice(*x_range), slice(*y_range)]\n                glyph = LineAxis1Geometry(geometry)\n            elif spatialpandas and isinstance(source, spatialpandas.GeoDataFrame):\n                glyph = LineAxis1Geometry(geometry)\n            elif (geopandas_source := self._source_from_geopandas(source)) is not None:\n                source = geopandas_source\n                from datashader.glyphs.line import LineAxis1GeoPandas\n                glyph = LineAxis1GeoPandas(geometry)\n            else:\n                raise ValueError(\n                    \"source must be an instance of spatialpandas.GeoDataFrame, \"\n                    \"spatialpandas.dask.DaskGeoDataFrame, geopandas.GeoDataFrame, or \"\n                    \"dask_geopandas.GeoDataFrame. Received objects of type {typ}\".format(\n                        typ=type(source)))\n\n        elif isinstance(source, Dataset) and isinstance(x, str) and isinstance(y, str):\n            x_arr = source[x]\n            y_arr = source[y]\n            if x_arr.ndim != 1:\n                raise ValueError(f\"x array must have 1 dimension not {x_arr.ndim}\")\n\n            if y_arr.ndim != 2:\n                raise ValueError(f\"y array must have 2 dimensions not {y_arr.ndim}\")\n            if x not in y_arr.dims:\n                raise ValueError(\"x must be one of the coordinate dimensions of y\")\n\n            y_coord_dims = list(y_arr.coords.dims)\n            x_dim_index = y_coord_dims.index(x)\n            glyph = LinesXarrayCommonX(x, y, x_dim_index)\n        else:\n            # Broadcast column specifications to handle cases where\n            # x is a list and y is a string or vice versa\n            orig_x, orig_y = x, y\n            x, y = _broadcast_column_specifications(x, y)\n\n            if axis == 0:\n                if (isinstance(x, (Number, str)) and\n                        isinstance(y, (Number, str))):\n                    glyph = LineAxis0(x, y)\n                elif (isinstance(x, (list, tuple)) and\n                        isinstance(y, (list, tuple))):\n                    glyph = LineAxis0Multi(tuple(x), tuple(y))\n                else:\n                    raise ValueError(\"\"\"\nInvalid combination of x and y arguments to Canvas.line when axis=0.\n    Received:\n        x: {x}\n        y: {y}\nSee docstring for more information on valid usage\"\"\".format(\n                        x=repr(orig_x), y=repr(orig_y)))\n\n            elif axis == 1:\n                if isinstance(x, (list, tuple)) and isinstance(y, (list, tuple)):\n                    glyph = LinesAxis1(tuple(x), tuple(y))\n                elif (isinstance(x, np.ndarray) and\n                      isinstance(y,  (list, tuple))):\n                    glyph = LinesAxis1XConstant(x, tuple(y))\n                elif (isinstance(x, (list, tuple)) and\n                      isinstance(y, np.ndarray)):\n                    glyph = LinesAxis1YConstant(tuple(x), y)\n                elif (isinstance(x, (Number, str)) and\n                        isinstance(y, (Number, str))):\n                    glyph = LinesAxis1Ragged(x, y)\n                else:\n                    raise ValueError(\"\"\"\nInvalid combination of x and y arguments to Canvas.line when axis=1.\n    Received:\n        x: {x}\n        y: {y}\nSee docstring for more information on valid usage\"\"\".format(\n                        x=repr(orig_x), y=repr(orig_y)))\n\n            else:\n                raise ValueError(\"\"\"\nThe axis argument to Canvas.line must be 0 or 1\n    Received: {axis}\"\"\".format(axis=axis))\n\n        if (line_width > 0 and ((cudf and isinstance(source, cudf.DataFrame)) or\n                               (dask_cudf and isinstance(source, dask_cudf.DataFrame)))):\n            warnings.warn(\n                \"Antialiased lines are not supported for CUDA-backed sources, \"\n                \"so reverting to line_width=0\")\n            line_width = 0\n\n        glyph.set_line_width(line_width)\n\n        if glyph.antialiased:\n            # This is required to identify and report use of reductions that do\n            # not yet support antialiasing.\n            non_cat_agg = agg\n            if isinstance(non_cat_agg, rd.by):\n                non_cat_agg = non_cat_agg.reduction\n\n            if not isinstance(non_cat_agg, (\n                rd.any, rd.count, rd.max, rd.min, rd.sum, rd.summary, rd._sum_zero,\n                rd._first_or_last, rd.mean, rd.max_n, rd.min_n, rd._first_n_or_last_n,\n                rd._max_or_min_row_index, rd._max_n_or_min_n_row_index, rd.where,\n            )):\n                raise NotImplementedError(\n                    f\"{type(non_cat_agg)} reduction not implemented for antialiased lines\")\n\n        return bypixel(source, self, glyph, agg, antialias=glyph.antialiased)\n\n    def area(self, source, x, y, agg=None, axis=0, y_stack=None):\n        \"\"\"Compute a reduction by pixel, mapping data to pixels as a filled\n        area region\n\n        Parameters\n        ----------\n        source : pandas.DataFrame, dask.DataFrame, or xarray.DataArray/Dataset\n            The input datasource.\n        x, y : str or number or list or tuple or np.ndarray\n            Specification of the x and y coordinates of each vertex of the\n            line defining the starting edge of the area region.\n            * str or number: Column labels in source\n            * list or tuple: List or tuple of column labels in source\n            * np.ndarray: When axis=1, a literal array of the\n              coordinates to be used for every row\n        agg : Reduction, optional\n            Reduction to compute. Default is ``count()``.\n        axis : 0 or 1, default 0\n            Axis in source to draw lines along\n            * 0: Draw area regions using data from the specified columns\n                 across all rows in source\n            * 1: Draw one area region per row in source using data from the\n                 specified columns\n        y_stack: str or number or list or tuple or np.ndarray or None\n            Specification of the y coordinates of each vertex of the line\n            defining the ending edge of the area region, where the x\n            coordinate is given by the x argument described above.\n\n            If y_stack is None, then the area region is filled to the y=0 line\n\n            If y_stack is not None, then the form of y_stack must match the\n            form of y.\n\n        Examples\n        --------\n        Define a canvas and a pandas DataFrame with 6 rows\n        >>> import pandas as pd  # doctest: +SKIP\n        ... import numpy as np\n        ... import datashader as ds\n        ... from datashader import Canvas\n        ... import datashader.transfer_functions as tf\n        ... cvs = Canvas()\n        ... df = pd.DataFrame({\n        ...    'A1': [1, 1.5, 2, 2.5, 3, 4],\n        ...    'A2': [1.6, 2.1, 2.9, 3.2, 4.2, 5],\n        ...    'B1': [10, 12, 11, 14, 13, 15],\n        ...    'B2': [11, 9, 10, 7, 8, 12],\n        ... }, dtype='float64')\n\n        Aggregate one area region across all rows, that starts with\n        coordinates df.A1 by df.B1 and is filled to the y=0 line\n        >>> agg = cvs.area(df, x='A1', y='B1',  # doctest: +SKIP\n        ...                agg=ds.count(), axis=0)\n        ... tf.shade(agg)\n\n        Aggregate one area region across all rows, that starts with\n        coordinates df.A1 by df.B1 and is filled to the line with coordinates\n        df.A1 by df.B2\n        >>> agg = cvs.area(df, x='A1', y='B1', y_stack='B2', # doctest: +SKIP\n        ...                agg=ds.count(), axis=0)\n        ... tf.shade(agg)\n\n        Aggregate two area regions across all rows. The first starting\n        with coordinates df.A1 by df.B1 and the second with coordinates\n        df.A2 by df.B2. Both regions are filled to the y=0 line\n        >>> agg = cvs.area(df, x=['A1', 'A2'], y=['B1', 'B2'],  # doctest: +SKIP\n                           agg=ds.count(), axis=0)\n        ... tf.shade(agg)\n\n        Aggregate two area regions across all rows where the regions share the\n        same x coordinates. The first region will start with coordinates\n        df.A1 by df.B1 and the second will start with coordinates\n        df.A1 by df.B2. Both regions are filled to the y=0 line\n        >>> agg = cvs.area(df, x='A1', y=['B1', 'B2'], agg=ds.count(), axis=0)  # doctest: +SKIP\n        ... tf.shade(agg)\n\n        Aggregate 6 length-2 area regions, one per row, where the ith region\n        starts with coordinates [df.A1[i], df.A2[i]] by [df.B1[i], df.B2[i]]\n        and is filled to the y=0 line\n        >>> agg = cvs.area(df, x=['A1', 'A2'], y=['B1', 'B2'],  # doctest: +SKIP\n                           agg=ds.count(), axis=1)\n        ... tf.shade(agg)\n\n        Aggregate 6 length-4 area regions, one per row, where the\n        starting x coordinates of every region are [0, 1, 2, 3] and\n        the starting y coordinates of the ith region are\n        [df.A1[i], df.A2[i], df.B1[i], df.B2[i]].  All regions are filled to\n        the y=0 line\n        >>> agg = cvs.area(df,  # doctest: +SKIP\n        ...                x=np.arange(4),\n        ...                y=['A1', 'A2', 'B1', 'B2'],\n        ...                agg=ds.count(),\n        ...                axis=1)\n        ... tf.shade(agg)\n\n        Aggregate RaggedArrays of variable length area regions, one per row.\n        The starting coordinates of the ith region are df_ragged.A1 by\n        df_ragged.B1 and the regions are filled to the y=0 line.\n        (requires pandas >= 0.24.0)\n        >>> df_ragged = pd.DataFrame({  # doctest: +SKIP\n        ...    'A1': pd.array([\n        ...        [1, 1.5], [2, 2.5, 3], [1.5, 2, 3, 4], [3.2, 4, 5]],\n        ...        dtype='Ragged[float32]'),\n        ...    'B1': pd.array([\n        ...        [10, 12], [11, 14, 13], [10, 7, 9, 10], [7, 8, 12]],\n        ...        dtype='Ragged[float32]'),\n        ...    'B2': pd.array([\n        ...        [6, 10], [9, 10, 18], [9, 5, 6, 8], [4, 5, 11]],\n        ...        dtype='Ragged[float32]'),\n        ...    'group': pd.Categorical([0, 1, 2, 1])\n        ... })\n        ...\n        ... agg = cvs.area(df_ragged, x='A1', y='B1', agg=ds.count(), axis=1)\n        ... tf.shade(agg)\n\n        Instead of filling regions to the y=0 line, fill to the line with\n        coordinates df_ragged.A1 by df_ragged.B2\n        >>> agg = cvs.area(df_ragged, x='A1', y='B1', y_stack='B2', # doctest: +SKIP\n        ...                agg=ds.count(), axis=1)\n        ... tf.shade(agg)\n\n        (requires pandas >= 0.24.0)\n        \"\"\"\n        from .glyphs import (\n            AreaToZeroAxis0, AreaToLineAxis0,\n            AreaToZeroAxis0Multi, AreaToLineAxis0Multi,\n            AreaToZeroAxis1, AreaToLineAxis1,\n            AreaToZeroAxis1XConstant, AreaToLineAxis1XConstant,\n            AreaToZeroAxis1YConstant, AreaToLineAxis1YConstant,\n            AreaToZeroAxis1Ragged, AreaToLineAxis1Ragged,\n        )\n        from .reductions import any as any_rdn\n        if agg is None:\n            agg = any_rdn()\n\n        # Broadcast column specifications to handle cases where\n        # x is a list and y is a string or vice versa\n        orig_x, orig_y, orig_y_stack = x, y, y_stack\n        x, y, y_stack = _broadcast_column_specifications(x, y, y_stack)\n\n        if axis == 0:\n            if y_stack is None:\n                if (isinstance(x, (Number, str)) and\n                        isinstance(y, (Number, str))):\n                    glyph = AreaToZeroAxis0(x, y)\n                elif (isinstance(x, (list, tuple)) and\n                      isinstance(y, (list, tuple))):\n                    glyph = AreaToZeroAxis0Multi(tuple(x), tuple(y))\n                else:\n                    raise ValueError(\"\"\"\nInvalid combination of x and y arguments to Canvas.area when axis=0.\n    Received:\n        x: {x}\n        y: {y}\nSee docstring for more information on valid usage\"\"\".format(\n                        x=repr(x), y=repr(y)))\n            else:\n                # y_stack is not None\n                if (isinstance(x, (Number, str)) and\n                        isinstance(y, (Number, str)) and\n                        isinstance(y_stack, (Number, str))):\n\n                    glyph = AreaToLineAxis0(x, y, y_stack)\n                elif (isinstance(x, (list, tuple)) and\n                      isinstance(y, (list, tuple)) and\n                      isinstance(y_stack, (list, tuple))):\n                    glyph = AreaToLineAxis0Multi(\n                        tuple(x), tuple(y), tuple(y_stack))\n                else:\n                    raise ValueError(\"\"\"\nInvalid combination of x, y, and y_stack arguments to Canvas.area when axis=0.\n    Received:\n        x: {x}\n        y: {y}\n        y_stack: {y_stack}\nSee docstring for more information on valid usage\"\"\".format(\n                        x=repr(orig_x),\n                        y=repr(orig_y),\n                        y_stack=repr(orig_y_stack)))\n\n        elif axis == 1:\n            if y_stack is None:\n                if (isinstance(x, (list, tuple)) and\n                        isinstance(y, (list, tuple))):\n                    glyph = AreaToZeroAxis1(tuple(x), tuple(y))\n                elif (isinstance(x, np.ndarray) and\n                      isinstance(y, (list, tuple))):\n                    glyph = AreaToZeroAxis1XConstant(x, tuple(y))\n                elif (isinstance(x, (list, tuple)) and\n                      isinstance(y, np.ndarray)):\n                    glyph = AreaToZeroAxis1YConstant(tuple(x), y)\n                elif (isinstance(x, (Number, str)) and\n                      isinstance(y, (Number, str))):\n                    glyph = AreaToZeroAxis1Ragged(x, y)\n                else:\n                    raise ValueError(\"\"\"\nInvalid combination of x and y arguments to Canvas.area when axis=1.\n    Received:\n        x: {x}\n        y: {y}\nSee docstring for more information on valid usage\"\"\".format(\n                        x=repr(x), y=repr(y)))\n            else:\n                if (isinstance(x, (list, tuple)) and\n                        isinstance(y, (list, tuple)) and\n                        isinstance(y_stack, (list, tuple))):\n                    glyph = AreaToLineAxis1(\n                        tuple(x), tuple(y), tuple(y_stack))\n                elif (isinstance(x, np.ndarray) and\n                      isinstance(y, (list, tuple)) and\n                      isinstance(y_stack, (list, tuple))):\n                    glyph = AreaToLineAxis1XConstant(\n                        x, tuple(y), tuple(y_stack))\n                elif (isinstance(x, (list, tuple)) and\n                      isinstance(y, np.ndarray) and\n                      isinstance(y_stack, np.ndarray)):\n                    glyph = AreaToLineAxis1YConstant(tuple(x), y, y_stack)\n                elif (isinstance(x, (Number, str)) and\n                      isinstance(y, (Number, str)) and\n                      isinstance(y_stack, (Number, str))):\n                    glyph = AreaToLineAxis1Ragged(x, y, y_stack)\n                else:\n                    raise ValueError(\"\"\"\nInvalid combination of x, y, and y_stack arguments to Canvas.area when axis=1.\n    Received:\n        x: {x}\n        y: {y}\n        y_stack: {y_stack}\nSee docstring for more information on valid usage\"\"\".format(\n                        x=repr(orig_x),\n                        y=repr(orig_y),\n                        y_stack=repr(orig_y_stack)))\n        else:\n            raise ValueError(\"\"\"\nThe axis argument to Canvas.area must be 0 or 1\n    Received: {axis}\"\"\".format(axis=axis))\n\n        return bypixel(source, self, glyph, agg)\n\n    def polygons(self, source, geometry, agg=None):\n        \"\"\"Compute a reduction by pixel, mapping data to pixels as one or\n        more filled polygons.\n\n        Parameters\n        ----------\n        source : xarray.DataArray or Dataset\n            The input datasource.\n        geometry : str\n            Column name of a PolygonsArray of the coordinates of each line.\n        agg : Reduction, optional\n            Reduction to compute. Default is ``any()``.\n\n        Returns\n        -------\n        data : xarray.DataArray\n\n        Examples\n        --------\n        >>> import datashader as ds  # doctest: +SKIP\n        ... import datashader.transfer_functions as tf\n        ... from spatialpandas.geometry import PolygonArray\n        ... from spatialpandas import GeoDataFrame\n        ... import pandas as pd\n        ...\n        ... polygons = PolygonArray([\n        ...     # First Element\n        ...     [[0, 0, 1, 0, 2, 2, -1, 4, 0, 0],  # Filled quadrilateral (CCW order)\n        ...      [0.5, 1,  1, 2,  1.5, 1.5,  0.5, 1],     # Triangular hole (CW order)\n        ...      [0, 2, 0, 2.5, 0.5, 2.5, 0.5, 2, 0, 2],  # Rectangular hole (CW order)\n        ...      [2.5, 3, 3.5, 3, 3.5, 4, 2.5, 3],  # Filled triangle\n        ...     ],\n        ...\n        ...     # Second Element\n        ...     [[3, 0, 3, 2, 4, 2, 4, 0, 3, 0],  # Filled rectangle (CCW order)\n        ...      # Rectangular hole (CW order)\n        ...      [3.25, 0.25, 3.75, 0.25, 3.75, 1.75, 3.25, 1.75, 3.25, 0.25],\n        ...     ]\n        ... ])\n        ...\n        ... df = GeoDataFrame({'polygons': polygons, 'v': range(len(polygons))})\n        ...\n        ... cvs = ds.Canvas()\n        ... agg = cvs.polygons(df, geometry='polygons', agg=ds.sum('v'))\n        ... tf.shade(agg)\n        \"\"\"\n        from .glyphs import PolygonGeom\n        from .reductions import any as any_rdn\n\n        if spatialpandas and isinstance(source, spatialpandas.dask.DaskGeoDataFrame):\n            # Downselect partitions to those that may contain polygons in viewport\n            x_range = self.x_range if self.x_range is not None else (None, None)\n            y_range = self.y_range if self.y_range is not None else (None, None)\n            source = source.cx_partitions[slice(*x_range), slice(*y_range)]\n            glyph = PolygonGeom(geometry)\n        elif spatialpandas and isinstance(source, spatialpandas.GeoDataFrame):\n            glyph = PolygonGeom(geometry)\n        elif (geopandas_source := self._source_from_geopandas(source)) is not None:\n            source = geopandas_source\n            from .glyphs.polygon import GeopandasPolygonGeom\n            glyph = GeopandasPolygonGeom(geometry)\n        else:\n            raise ValueError(\n                \"source must be an instance of spatialpandas.GeoDataFrame, \"\n                \"spatialpandas.dask.DaskGeoDataFrame, geopandas.GeoDataFrame or \"\n                f\"dask_geopandas.GeoDataFrame, not {type(source)}\")\n\n        if agg is None:\n            agg = any_rdn()\n        return bypixel(source, self, glyph, agg)\n\n    def quadmesh(self, source, x=None, y=None, agg=None):\n        \"\"\"Samples a recti- or curvi-linear quadmesh by canvas size and bounds.\n\n        Parameters\n        ----------\n        source : xarray.DataArray or Dataset\n            The input datasource.\n        x, y : str\n            Column names for the x and y coordinates of each point.\n        agg : Reduction, optional\n            Reduction to compute. Default is ``mean()``. Note that agg is ignored when\n            upsampling.\n\n        Returns\n        -------\n        data : xarray.DataArray\n        \"\"\"\n        from .glyphs import QuadMeshRaster, QuadMeshRectilinear, QuadMeshCurvilinear\n\n        # Determine reduction operation\n        from .reductions import mean as mean_rnd\n\n        if isinstance(source, Dataset):\n            if agg is None or agg.column is None:\n                name = list(source.data_vars)[0]\n            else:\n                name = agg.column\n            # Keep as dataset so that source[agg.column] works\n            source = source[[name]]\n        elif isinstance(source, DataArray):\n            # Make dataset so that source[agg.column] works\n            name = source.name\n            source = source.to_dataset()\n        else:\n            raise ValueError(\"Invalid input type\")\n\n        if agg is None:\n            agg = mean_rnd(name)\n\n        if x is None and y is None:\n            y, x = source[name].dims\n        elif not x or not y:\n            raise ValueError(\"Either specify both x and y coordinates\"\n                             \"or allow them to be inferred.\")\n        yarr, xarr = source[y], source[x]\n\n        if (yarr.ndim > 1 or xarr.ndim > 1) and xarr.dims != yarr.dims:\n            raise ValueError(\"Ensure that x- and y-coordinate arrays \"\n                             \"share the same dimensions. x-coordinates \"\n                             \"are indexed by %s dims while \"\n                             \"y-coordinates are indexed by %s dims.\" %\n                             (xarr.dims, yarr.dims))\n\n        if (name is not None\n                and agg.column is not None\n                and agg.column != name):\n            raise ValueError('DataArray name %r does not match '\n                             'supplied reduction %s.' %\n                             (source.name, agg))\n\n        if xarr.ndim == 1:\n            xaxis_linear = self.x_axis is _axis_lookup[\"linear\"]\n            yaxis_linear = self.y_axis is _axis_lookup[\"linear\"]\n            even_yspacing = np.allclose(\n                yarr, np.linspace(yarr[0].data, yarr[-1].data, len(yarr))\n            )\n            even_xspacing = np.allclose(\n                xarr, np.linspace(xarr[0].data, xarr[-1].data, len(xarr))\n            )\n\n            if xaxis_linear and yaxis_linear and even_xspacing and even_yspacing:\n                # Source is a raster, where all x and y coordinates are evenly spaced\n                glyph = QuadMeshRaster(x, y, name)\n                upsample_width, upsample_height = glyph.is_upsample(\n                        source, x, y, name, self.x_range, self.y_range,\n                        self.plot_width, self.plot_height\n                )\n                if upsample_width and upsample_height:\n                    # Override aggregate with more efficient one for upsampling\n                    agg = rd._upsample(name)\n                    return bypixel(source, self, glyph, agg)\n                elif not upsample_width and not upsample_height:\n                    # Downsample both width and height\n                    return bypixel(source, self, glyph, agg)\n                else:\n                    # Mix of upsampling and downsampling\n                    # Use general rectilinear quadmesh implementation\n                    glyph = QuadMeshRectilinear(x, y, name)\n                    return bypixel(source, self, glyph, agg)\n            else:\n                # Source is a general rectilinear quadmesh\n                glyph = QuadMeshRectilinear(x, y, name)\n                return bypixel(source, self, glyph, agg)\n        elif xarr.ndim == 2:\n            glyph = QuadMeshCurvilinear(x, y, name)\n            return bypixel(source, self, glyph, agg)\n        else:\n            raise ValueError(\"\"\"\\\nx- and y-coordinate arrays must have 1 or 2 dimensions.\n    Received arrays with dimensions: {dims}\"\"\".format(\n                dims=list(xarr.dims)))\n\n    # TODO re 'untested', below: Consider replacing with e.g. a 3x3\n    # array in the call to Canvas (plot_height=3,plot_width=3), then\n    # show the output as a numpy array that has a compact\n    # representation\n    def trimesh(self, vertices, simplices, mesh=None, agg=None, interp=True, interpolate=None):\n        \"\"\"Compute a reduction by pixel, mapping data to pixels as a triangle.\n\n        >>> import datashader as ds\n        >>> verts = pd.DataFrame({'x': [0, 5, 10],\n        ...                         'y': [0, 10, 0],\n        ...                         'weight': [1, 5, 3]},\n        ...                        columns=['x', 'y', 'weight'])\n        >>> tris = pd.DataFrame({'v0': [2], 'v1': [0], 'v2': [1]},\n        ...                       columns=['v0', 'v1', 'v2'])\n        >>> cvs = ds.Canvas(x_range=(verts.x.min(), verts.x.max()),\n        ...                 y_range=(verts.y.min(), verts.y.max()))\n        >>> untested = cvs.trimesh(verts, tris)\n\n        Parameters\n        ----------\n        vertices : pandas.DataFrame, dask.DataFrame\n            The input datasource for triangle vertex coordinates. These can be\n            interpreted as the x/y coordinates of the vertices, with optional\n            weights for value interpolation. Columns should be ordered\n            corresponding to 'x', 'y', followed by zero or more (optional)\n            columns containing vertex values. The rows need not be ordered.\n            The column data types must be floating point or integer.\n        simplices : pandas.DataFrame, dask.DataFrame\n            The input datasource for triangle (simplex) definitions. These can\n            be interpreted as rows of ``vertices``, aka positions in the\n            ``vertices`` index. Columns should be ordered corresponding to\n            'vertex0', 'vertex1', and 'vertex2'. Order of the vertices can be\n            clockwise or counter-clockwise; it does not matter as long as the\n            data is consistent for all simplices in the dataframe. The\n            rows need not be ordered.  The data type for the first\n            three columns in the dataframe must be integer.\n        agg : Reduction, optional\n            Reduction to compute. Default is ``mean()``.\n        mesh : pandas.DataFrame, optional\n            An ordered triangle mesh in tabular form, used for optimization\n            purposes. This dataframe is expected to have come from\n            ``datashader.utils.mesh()``. If this argument is not None, the first\n            two arguments are ignored.\n        interpolate : str, optional default=linear\n            Method to use for interpolation between specified values. ``nearest``\n            means to use a single value for the whole triangle, and ``linear``\n            means to do bilinear interpolation of the pixels within each\n            triangle (a weighted average of the vertex values). For\n            backwards compatibility, also accepts ``interp=True`` for ``linear``\n            and ``interp=False`` for ``nearest``.\n        \"\"\"\n        from .glyphs import Triangles\n        from .reductions import mean as mean_rdn\n        from .utils import mesh as create_mesh\n\n        source = mesh\n\n        # 'interp' argument is deprecated as of datashader=0.6.4\n        if interpolate is not None:\n            if interpolate == 'linear':\n                interp = True\n            elif interpolate == 'nearest':\n                interp = False\n            else:\n                raise ValueError('Invalid interpolate method: options include {}'.format(\n                    ['linear','nearest']))\n\n        # Validation is done inside the [pd]d_mesh utility functions\n        if source is None:\n            source = create_mesh(vertices, simplices)\n\n        verts_have_weights = len(vertices.columns) > 2\n        if verts_have_weights:\n            weight_col = vertices.columns[2]\n        else:\n            weight_col = simplices.columns[3]\n\n        if agg is None:\n            agg = mean_rdn(weight_col)\n        elif agg.column is None:\n            agg.column = weight_col\n\n        cols = source.columns\n        x, y, weights = cols[0], cols[1], cols[2:]\n\n        return bypixel(source, self, Triangles(x, y, weights, weight_type=verts_have_weights,\n                                               interp=interp), agg)\n\n    def raster(self,\n               source,\n               layer=None,\n               upsample_method='linear',    # Deprecated as of datashader=0.6.4\n               downsample_method=rd.mean(), # Deprecated as of datashader=0.6.4\n               nan_value=None,\n               agg=None,\n               interpolate=None,\n               chunksize=None,\n               max_mem=None):\n        \"\"\"Sample a raster dataset by canvas size and bounds.\n\n        Handles 2D or 3D xarray DataArrays, assuming that the last two\n        array dimensions are the y- and x-axis that are to be\n        resampled. If a 3D array is supplied a layer may be specified\n        to resample to select the layer along the first dimension to\n        resample.\n\n        Missing values (those having the value indicated by the\n        \"nodata\" attribute of the raster) are replaced with `NaN` if\n        floats, and 0 if int.\n\n        Also supports resampling out-of-core DataArrays backed by dask\n        Arrays. By default it will try to maintain the same chunksize\n        in the output array but a custom chunksize may be provided.\n        If there are memory constraints they may be defined using the\n        max_mem parameter, which determines how large the chunks in\n        memory may be.\n\n        Parameters\n        ----------\n        source : xarray.DataArray or xr.Dataset\n            2D or 3D labelled array (if Dataset, the agg reduction must\n            define the data variable).\n        layer : float\n            For a 3D array, value along the z dimension : optional default=None\n        ds_method : str (optional)\n            Grid cell aggregation method for a possible downsampling.\n        us_method : str (optional)\n            Grid cell interpolation method for a possible upsampling.\n        nan_value : int or float, optional\n            Optional nan_value which will be masked out when applying\n            the resampling.\n        agg : Reduction, optional default=mean()\n            Resampling mode when downsampling raster. The supported\n            options include: first, last, mean, mode, var, std, min,\n            The agg can be specified as either a string name or as a\n            reduction function, but note that the function object will\n            be used only to extract the agg type (mean, max, etc.) and\n            the optional column name; the hardcoded raster code\n            supports only a fixed set of reductions and ignores the\n            actual code of the provided agg.\n        interpolate : str, optional  default=linear\n            Resampling mode when upsampling raster.\n            options include: nearest, linear.\n        chunksize : tuple(int, int) (optional)\n            Size of the output chunks. By default this the chunk size is\n            inherited from the *src* array.\n        max_mem : int (optional)\n            The maximum number of bytes that should be loaded into memory\n            during the regridding operation.\n\n        Returns\n        -------\n        data : xarray.Dataset\n        \"\"\"\n        # For backwards compatibility\n        if agg         is None:\n            agg=downsample_method\n        if interpolate is None:\n            interpolate=upsample_method\n\n        upsample_methods = ['nearest','linear']\n\n        downsample_methods = {'first':'first', rd.first:'first',\n                              'last':'last',   rd.last:'last',\n                              'mode':'mode',   rd.mode:'mode',\n                              'mean':'mean',   rd.mean:'mean',\n                              'var':'var',     rd.var:'var',\n                              'std':'std',     rd.std:'std',\n                              'min':'min',     rd.min:'min',\n                              'max':'max',     rd.max:'max'}\n\n        if interpolate not in upsample_methods:\n            raise ValueError('Invalid interpolate method: options include {}'.format(\n                upsample_methods))\n\n        if not isinstance(source, (DataArray, Dataset)):\n            raise ValueError('Expected xarray DataArray or Dataset as '\n                             'the data source, found %s.'\n                             % type(source).__name__)\n\n        column = None\n        if isinstance(agg, rd.Reduction):\n            agg, column = type(agg), agg.column\n            if (isinstance(source, DataArray) and column is not None\n                and source.name != column):\n                agg_repr = '%s(%r)' % (agg.__name__, column)\n                raise ValueError('DataArray name %r does not match '\n                                 'supplied reduction %s.' %\n                                 (source.name, agg_repr))\n\n        if isinstance(source, Dataset):\n            data_vars = list(source.data_vars)\n            if column is None:\n                raise ValueError('When supplying a Dataset the agg reduction '\n                                 'must specify the variable to aggregate. '\n                                 'Available data_vars include: %r.' % data_vars)\n            elif column not in source.data_vars:\n                raise KeyError('Supplied reduction column %r not found '\n                               'in Dataset, expected one of the following '\n                               'data variables: %r.' % (column, data_vars))\n            source = source[column]\n\n        if agg not in downsample_methods.keys():\n            raise ValueError('Invalid aggregation method: options include {}'.format(\n                list(downsample_methods.keys())))\n        ds_method = downsample_methods[agg]\n\n        if source.ndim not in [2, 3]:\n            raise ValueError('Raster aggregation expects a 2D or 3D '\n                             'DataArray, found %s dimensions' % source.ndim)\n\n        res = calc_res(source)\n        ydim, xdim = source.dims[-2:]\n        xvals, yvals = source[xdim].values, source[ydim].values\n        left, bottom, right, top = calc_bbox(xvals, yvals, res)\n        if layer is not None:\n            source=source.sel(**{source.dims[0]: layer})\n        array = orient_array(source, res)\n\n        if nan_value is not None:\n            mask = array==nan_value\n            array = np.ma.masked_array(array, mask=mask, fill_value=nan_value)\n            fill_value = nan_value\n        elif np.issubdtype(source.dtype, np.integer):\n            fill_value = 0\n        else:\n            fill_value = np.nan\n\n        if self.x_range is None:\n            self.x_range = (left,right)\n        if self.y_range is None:\n            self.y_range = (bottom,top)\n\n        # window coordinates\n        xmin = max(self.x_range[0], left)\n        ymin = max(self.y_range[0], bottom)\n        xmax = min(self.x_range[1], right)\n        ymax = min(self.y_range[1], top)\n\n        width_ratio = min((xmax - xmin) / (self.x_range[1] - self.x_range[0]), 1)\n        height_ratio = min((ymax - ymin) / (self.y_range[1] - self.y_range[0]), 1)\n\n        if np.isclose(width_ratio, 0) or np.isclose(height_ratio, 0):\n            raise ValueError('Canvas x_range or y_range values do not match closely enough '\n                             'with the data source to be able to accurately rasterize. '\n                             'Please provide ranges that are more accurate.')\n\n        w = max(int(round(self.plot_width * width_ratio)), 1)\n        h = max(int(round(self.plot_height * height_ratio)), 1)\n        cmin, cmax = get_indices(xmin, xmax, xvals, res[0])\n        rmin, rmax = get_indices(ymin, ymax, yvals, res[1])\n\n        kwargs = dict(w=w, h=h, ds_method=ds_method,\n                      us_method=interpolate, fill_value=fill_value)\n        if array.ndim == 2:\n            source_window = array[rmin:rmax+1, cmin:cmax+1]\n            if ds_method in ['var', 'std']:\n                source_window = source_window.astype('f')\n            if da and isinstance(source_window, da.Array):\n                data = resample_2d_distributed(\n                    source_window, chunksize=chunksize, max_mem=max_mem,\n                    **kwargs)\n            else:\n                data = resample_2d(source_window, **kwargs)\n            layers = 1\n        else:\n            source_window = array[:, rmin:rmax+1, cmin:cmax+1]\n            if ds_method in ['var', 'std']:\n                source_window = source_window.astype('f')\n            arrays = []\n            for arr in source_window:\n                if da and isinstance(arr, da.Array):\n                    arr = resample_2d_distributed(\n                        arr, chunksize=chunksize, max_mem=max_mem,\n                        **kwargs)\n                else:\n                    arr = resample_2d(arr, **kwargs)\n                arrays.append(arr)\n            data = np.dstack(arrays)\n            layers = len(arrays)\n\n        if w != self.plot_width or h != self.plot_height:\n            num_height = self.plot_height - h\n            num_width = self.plot_width - w\n\n            lpad = xmin - self.x_range[0]\n            rpad = self.x_range[1] - xmax\n            lpct = lpad / (lpad + rpad) if lpad + rpad > 0 else 0\n            left = max(int(np.ceil(num_width * lpct)), 0)\n            right = max(num_width - left, 0)\n            lshape, rshape = (self.plot_height, left), (self.plot_height, right)\n            if layers > 1:\n                lshape, rshape = lshape + (layers,), rshape + (layers,)\n            left_pad = np.full(lshape, fill_value, source_window.dtype)\n            right_pad = np.full(rshape, fill_value, source_window.dtype)\n\n            tpad = ymin - self.y_range[0]\n            bpad = self.y_range[1] - ymax\n            tpct = tpad / (tpad + bpad) if tpad + bpad > 0 else 0\n            top = max(int(np.ceil(num_height * tpct)), 0)\n            bottom = max(num_height - top, 0)\n            tshape, bshape = (top, w), (bottom, w)\n            if layers > 1:\n                tshape, bshape = tshape + (layers,), bshape + (layers,)\n            top_pad = np.full(tshape, fill_value, source_window.dtype)\n            bottom_pad = np.full(bshape, fill_value, source_window.dtype)\n\n            concat = da.concatenate if da and isinstance(data, da.Array) else np.concatenate\n            arrays = (top_pad, data) if top_pad.shape[0] > 0 else (data,)\n            if bottom_pad.shape[0] > 0:\n                arrays += (bottom_pad,)\n            data = concat(arrays, axis=0) if len(arrays) > 1 else arrays[0]\n            arrays = (left_pad, data) if left_pad.shape[1] > 0 else (data,)\n            if right_pad.shape[1] > 0:\n                arrays += (right_pad,)\n            data = concat(arrays, axis=1) if len(arrays) > 1 else arrays[0]\n\n        # Reorient array to original orientation\n        if res[1] > 0:\n            data = data[::-1]\n        if res[0] < 0:\n            data = data[:, ::-1]\n\n        # Compute DataArray metadata\n\n        # To avoid floating point representation error,\n        # do not recompute x coords if same x_range and same plot_width,\n        # do not recompute y coords if same y_range and same plot_height\n        close_x = np.allclose([left, right], self.x_range) and np.size(xvals) == self.plot_width\n        close_y = np.allclose([bottom, top], self.y_range) and np.size(yvals) == self.plot_height\n\n        if close_x:\n            xs = xvals\n        else:\n            x_st = self.x_axis.compute_scale_and_translate(self.x_range, self.plot_width)\n            xs = self.x_axis.compute_index(x_st, self.plot_width)\n            if res[0] < 0:\n                xs = xs[::-1]\n\n        if close_y:\n            ys = yvals\n        else:\n            y_st = self.y_axis.compute_scale_and_translate(self.y_range, self.plot_height)\n            ys = self.y_axis.compute_index(y_st, self.plot_height)\n            if res[1] > 0:\n                ys = ys[::-1]\n\n        coords = {xdim: xs, ydim: ys}\n        dims = [ydim, xdim]\n        attrs = dict(res=res[0], x_range=self.x_range, y_range=self.y_range)\n\n        # Find nodata value if available in any of the common conventional locations\n        # See https://corteva.github.io/rioxarray/stable/getting_started/nodata_management.html\n        # and https://github.com/holoviz/datashader/issues/990\n        for a in ['_FillValue', 'missing_value', 'fill_value', 'nodata', 'NODATA']:\n            if a in source.attrs:\n                attrs['nodata'] = source.attrs[a]\n                break\n        if 'nodata' not in attrs:\n            try:\n                attrs['nodata'] = source.attrs['nodatavals'][0]\n            except Exception:\n                pass\n\n        # Handle DataArray with layers\n        if data.ndim == 3:\n            data = data.transpose([2, 0, 1])\n            layer_dim = source.dims[0]\n            coords[layer_dim] = source.coords[layer_dim]\n            dims = [layer_dim]+dims\n        return DataArray(data, coords=coords, dims=dims, attrs=attrs)\n\n    def validate_ranges(self, x_range, y_range):\n        self.x_axis.validate(x_range)\n        self.y_axis.validate(y_range)\n\n    def validate_size(self, width, height):\n        if width <= 0 or height <= 0:\n            raise ValueError(\"Invalid size: plot_width and plot_height must be bigger than 0\")\n\n    def validate(self):\n        \"\"\"Check that parameter settings are valid for this object\"\"\"\n        self.validate_ranges(self.x_range, self.y_range)\n        self.validate_size(self.plot_width, self.plot_height)\n\n    def _source_from_geopandas(self, source):\n        \"\"\"\n        Check if the specified source is a geopandas or dask-geopandas GeoDataFrame.\n        If so, spatially filter the source and return it.\n        If not, return None.\n        \"\"\"\n        dfs = []\n        with contextlib.suppress(ImportError):\n            import geopandas\n            dfs.append(geopandas.GeoDataFrame)\n\n        with contextlib.suppress(ImportError):\n            import dask_geopandas\n            if Version(dask_geopandas.__version__) >= Version(\"0.4.0\"):\n                from dask_geopandas.core import GeoDataFrame as gdf1\n                dfs.append(gdf1)\n\n                # See https://github.com/geopandas/dask-geopandas/issues/311\n                with contextlib.suppress(TypeError):\n                    from dask_geopandas.expr import GeoDataFrame as gdf2\n                    dfs.append(gdf2)\n            else:\n                dfs.append(dask_geopandas.GeoDataFrame)\n\n        if isinstance(source, tuple(dfs)):\n            # Explicit shapely version check as cannot continue unless shapely >= 2\n            from shapely import __version__ as shapely_version\n            if Version(shapely_version) < Version('2.0.0'):\n                raise ImportError(\"Use of GeoPandas in Datashader requires Shapely >= 2.0.0\")\n\n            if isinstance(source, geopandas.GeoDataFrame):\n                x_range = self.x_range if self.x_range is not None else (-np.inf, np.inf)\n                y_range = self.y_range if self.y_range is not None else (-np.inf, np.inf)\n                from shapely import box\n                query = source.sindex.query(box(x_range[0], y_range[0], x_range[1], y_range[1]))\n                source = source.iloc[query]\n            else:\n                x_range = self.x_range if self.x_range is not None else (None, None)\n                y_range = self.y_range if self.y_range is not None else (None, None)\n                source = source.cx[slice(*x_range), slice(*y_range)]\n            return source\n        else:\n            return None\n\n\ndef bypixel(source, canvas, glyph, agg, *, antialias=False):\n    \"\"\"Compute an aggregate grouped by pixel sized bins.\n\n    Aggregate input data ``source`` into a grid with shape and axis matching\n    ``canvas``, mapping data to bins by ``glyph``, and aggregating by reduction\n    ``agg``.\n\n    Parameters\n    ----------\n    source : pandas.DataFrame, dask.DataFrame\n        Input datasource\n    canvas : Canvas\n    glyph : Glyph\n    agg : Reduction\n    \"\"\"\n    source, dshape = _bypixel_sanitise(source, glyph, agg)\n\n    schema = dshape.measure\n    glyph.validate(schema)\n    agg.validate(schema)\n    canvas.validate()\n\n    # All-NaN objects (e.g. chunks of arrays with no data) are valid in Datashader\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n        return bypixel.pipeline(source, schema, canvas, glyph, agg, antialias=antialias)\n\n\ndef _bypixel_sanitise(source, glyph, agg):\n    # Convert 1D xarray DataArrays and DataSets into Dask DataFrames\n    if isinstance(source, DataArray) and source.ndim == 1:\n        if not source.name:\n            source.name = 'value'\n        source = source.reset_coords()\n    if isinstance(source, Dataset) and len(source.dims) == 1:\n        columns = list(source.coords.keys()) + list(source.data_vars.keys())\n        cols_to_keep = _cols_to_keep(columns, glyph, agg)\n        source = source.drop_vars([col for col in columns if col not in cols_to_keep])\n        if dd:\n            source = source.to_dask_dataframe()\n        else:\n            source = source.to_dataframe()\n\n    if (isinstance(source, pd.DataFrame) or\n            (cudf and isinstance(source, cudf.DataFrame))):\n        # Avoid datashape.Categorical instantiation bottleneck\n        # by only retaining the necessary columns:\n        # https://github.com/bokeh/datashader/issues/396\n        # Preserve column ordering without duplicates\n\n        cols_to_keep = _cols_to_keep(source.columns, glyph, agg)\n        if len(cols_to_keep) < len(source.columns):\n            # If _sindex is set, ensure it is not dropped\n            # https://github.com/holoviz/datashader/issues/1121\n            sindex = None\n            from .glyphs.polygon import PolygonGeom\n            if isinstance(glyph, PolygonGeom):\n                sindex = getattr(source[glyph.geometry].array, \"_sindex\", None)\n            source = source[cols_to_keep]\n            if (sindex is not None and\n                    getattr(source[glyph.geometry].array, \"_sindex\", None) is None):\n                source[glyph.geometry].array._sindex = sindex\n        dshape = dshape_from_pandas(source)\n    elif dd and isinstance(source, dd.DataFrame):\n        dshape, source = dshape_from_dask(source)\n    elif isinstance(source, Dataset):\n        # Multi-dimensional Dataset\n        dshape = dshape_from_xarray_dataset(source)\n    else:\n        raise ValueError(\"source must be a pandas or dask DataFrame\")\n\n    return source, dshape\n\n\ndef _cols_to_keep(columns, glyph, agg):\n    \"\"\"\n    Return which columns from the supplied data source are kept as they are\n    needed by the specified agg. Excludes any SpecialColumn.\n    \"\"\"\n    cols_to_keep = dict({col: False for col in columns})\n    for col in glyph.required_columns():\n        cols_to_keep[col] = True\n\n    def recurse(cols_to_keep, agg):\n        if hasattr(agg, 'values'):\n            for subagg in agg.values:\n                recurse(cols_to_keep, subagg)\n        elif hasattr(agg, 'columns'):\n            for column in agg.columns:\n                if column not in (None, rd.SpecialColumn.RowIndex):\n                    cols_to_keep[column] = True\n        elif agg.column not in (None, rd.SpecialColumn.RowIndex):\n            cols_to_keep[agg.column] = True\n\n    recurse(cols_to_keep, agg)\n\n    return [col for col, keepit in cols_to_keep.items() if keepit]\n\n\ndef _broadcast_column_specifications(*args):\n    lengths = {len(a) for a in args if isinstance(a, (list, tuple))}\n    if len(lengths) != 1:\n        # None of the inputs are lists/tuples, return them as is\n        return args\n    else:\n        n = lengths.pop()\n        return tuple(\n            (arg,) * n if isinstance(arg, (Number, str)) else arg\n            for arg in args\n        )\n\n\nbypixel.pipeline = Dispatcher()\n",
    "datashader/glyphs/points.py": "from __future__ import annotations\nfrom packaging.version import Version\nimport numpy as np\nfrom toolz import memoize\n\nfrom datashader.glyphs.glyph import Glyph\nfrom datashader.utils import isreal, ngjit\n\nfrom numba import cuda\n\ntry:\n    import cudf\n    from ..transfer_functions._cuda_utils import cuda_args\nexcept Exception:\n    cudf = None\n    cuda_args = None\n\ntry:\n    from geopandas.array import GeometryDtype as gpd_GeometryDtype\nexcept ImportError:\n    gpd_GeometryDtype = type(None)\n\ntry:\n    import spatialpandas\nexcept Exception:\n    spatialpandas = None\n\n\ndef values(s):\n    if isinstance(s, cudf.Series):\n        if Version(cudf.__version__) >= Version(\"22.02\"):\n            return s.to_cupy(na_value=np.nan)\n        else:\n            return s.to_gpu_array(fillna=np.nan)\n\n    else:\n        return s.values\n\n\nclass _GeometryLike(Glyph):\n    def __init__(self, geometry):\n        self.geometry = geometry\n        self._cached_bounds = None\n\n    @property\n    def ndims(self):\n        return 1\n\n    @property\n    def inputs(self):\n        return (self.geometry,)\n\n    @property\n    def geom_dtypes(self):\n        if spatialpandas:\n            from spatialpandas.geometry import GeometryDtype\n            return (GeometryDtype,)\n        else:\n            return ()  # Empty tuple\n\n    def validate(self, in_dshape):\n        if not isinstance(in_dshape[str(self.geometry)], self.geom_dtypes):\n            raise ValueError(\n                '{col} must be an array with one of the following types: {typs}'.format(\n                    col=self.geometry,\n                    typs=', '.join(typ.__name__ for typ in self.geom_dtypes)\n                ))\n\n    @property\n    def x_label(self):\n        return 'x'\n\n    @property\n    def y_label(self):\n        return 'y'\n\n    def required_columns(self):\n        return [self.geometry]\n\n    def compute_x_bounds(self, df):\n        col = df[self.geometry]\n        if isinstance(col.dtype, gpd_GeometryDtype):\n            # geopandas\n            if self._cached_bounds is None:\n                self._cached_bounds = col.total_bounds\n            bounds = self._cached_bounds[::2]\n        else:\n            # spatialpandas\n            bounds = col.array.total_bounds_x\n        return self.maybe_expand_bounds(bounds)\n\n    def compute_y_bounds(self, df):\n        col = df[self.geometry]\n        if isinstance(col.dtype, gpd_GeometryDtype):\n            # geopandas\n            if self._cached_bounds is None:\n                self._cached_bounds = col.total_bounds\n            bounds = self._cached_bounds[1::2]\n        else:\n            # spatialpandas\n            bounds = col.array.total_bounds_y\n        return self.maybe_expand_bounds(bounds)\n\n    @memoize\n    def compute_bounds_dask(self, ddf):\n        total_bounds = ddf[self.geometry].total_bounds\n        x_extents = (total_bounds[0], total_bounds[2])\n        y_extents = (total_bounds[1], total_bounds[3])\n\n        return (self.maybe_expand_bounds(x_extents),\n                self.maybe_expand_bounds(y_extents))\n\n\nclass _PointLike(Glyph):\n    \"\"\"Shared methods between Point and Line\"\"\"\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    @property\n    def ndims(self):\n        return 1\n\n    @property\n    def inputs(self):\n        return (self.x, self.y)\n\n    def validate(self, in_dshape):\n        if not isreal(in_dshape.measure[str(self.x)]):\n            raise ValueError('x must be real')\n        elif not isreal(in_dshape.measure[str(self.y)]):\n            raise ValueError('y must be real')\n\n    @property\n    def x_label(self):\n        return self.x\n\n    @property\n    def y_label(self):\n        return self.y\n\n    def required_columns(self):\n        return [self.x, self.y]\n\n    def compute_x_bounds(self, df):\n        bounds = self._compute_bounds(df[self.x])\n        return self.maybe_expand_bounds(bounds)\n\n    def compute_y_bounds(self, df):\n        bounds = self._compute_bounds(df[self.y])\n        return self.maybe_expand_bounds(bounds)\n\n    @memoize\n    def compute_bounds_dask(self, ddf):\n\n        r = ddf.map_partitions(lambda df: np.array([[\n            np.nanmin(df[self.x].values).item(),\n            np.nanmax(df[self.x].values).item(),\n            np.nanmin(df[self.y].values).item(),\n            np.nanmax(df[self.y].values).item()]]\n        )).compute()\n\n        x_extents = np.nanmin(r[:, 0]), np.nanmax(r[:, 1])\n        y_extents = np.nanmin(r[:, 2]), np.nanmax(r[:, 3])\n\n        return (self.maybe_expand_bounds(x_extents),\n                self.maybe_expand_bounds(y_extents))\n\n\nclass Point(_PointLike):\n    \"\"\"A point, with center at ``x`` and ``y``.\n\n    Points map each record to a single bin.\n    Points falling exactly on the upper bounds are treated as a special case,\n    mapping into the previous bin rather than being cropped off.\n\n    Parameters\n    ----------\n    x, y : str\n        Column names for the x and y coordinates of each point.\n    \"\"\"\n    @memoize\n    def _build_extend(self, x_mapper, y_mapper, info, append, _antialias_stage_2,\n                      _antialias_stage_2_funcs):\n        x_name = self.x\n        y_name = self.y\n\n        @ngjit\n        @self.expand_aggs_and_cols(append)\n        def _perform_extend_points(i, sx, tx, sy, ty, xmin, xmax,\n                                   ymin, ymax, xs, ys, xxmax, yymax,\n                                   *aggs_and_cols):\n            x = xs[i]\n            y = ys[i]\n\n            # points outside bounds are dropped; remainder\n            # are mapped onto pixels\n            if (xmin <= x <= xmax) and (ymin <= y <= ymax):\n                xx = int(x_mapper(x) * sx + tx)\n                yy = int(y_mapper(y) * sy + ty)\n\n                xi, yi = (xxmax-1 if xx >= xxmax else xx,\n                          yymax-1 if yy >= yymax else yy)\n                append(i, xi, yi, *aggs_and_cols)\n\n        @ngjit\n        @self.expand_aggs_and_cols(append)\n        def extend_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys,\n                       xxmax, yymax, *aggs_and_cols):\n            for i in range(xs.shape[0]):\n                _perform_extend_points(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                                       xs, ys, xxmax, yymax, *aggs_and_cols)\n\n        @cuda.jit\n        @self.expand_aggs_and_cols(append)\n        def extend_cuda(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys,\n                        xxmax, yymax, *aggs_and_cols):\n            i = cuda.grid(1)\n            if i < xs.shape[0]:\n                _perform_extend_points(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                                       xs, ys, xxmax, yymax, *aggs_and_cols)\n\n        def extend(aggs, df, vt, bounds):\n            yymax, xxmax = aggs[0].shape[:2]\n            aggs_and_cols = aggs + info(df, aggs[0].shape[:2])\n            sx, tx, sy, ty = vt\n            xmin, xmax, ymin, ymax = bounds\n\n            if cudf and isinstance(df, cudf.DataFrame):\n                xs = values(df[x_name])\n                ys = values(df[y_name])\n                do_extend = extend_cuda[cuda_args(xs.shape[0])]\n            else:\n                xs = df[x_name].values\n                ys = df[y_name].values\n                do_extend = extend_cpu\n\n            do_extend(\n                sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, xxmax, yymax, *aggs_and_cols\n            )\n\n        return extend\n\n\nclass MultiPointGeoPandas(_GeometryLike):\n    # geopandas must be available if a GeoPandasPointGeometry object is created.\n    @property\n    def geom_dtypes(self):\n        from geopandas.array import GeometryDtype\n        return (GeometryDtype,)\n\n    @memoize\n    def _build_extend(\n        self, x_mapper, y_mapper, info, append, _antialias_stage_2, _antialias_stage_2_funcs,\n    ):\n        # Lazy import shapely. Cannot get here if geopandas and shapely are not available.\n        import shapely\n\n        geometry_name = self.geometry\n\n        @ngjit\n        @self.expand_aggs_and_cols(append)\n        def _perform_extend_points(\n            i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, values, *aggs_and_cols\n        ):\n            x = values[j]\n            y = values[j + 1]\n            # points outside bounds are dropped; remainder\n            # are mapped onto pixels\n            if (xmin <= x <= xmax) and (ymin <= y <= ymax):\n                xx = int(x_mapper(x) * sx + tx)\n                yy = int(y_mapper(y) * sy + ty)\n                xi, yi = (xx - 1 if x == xmax else xx,\n                          yy - 1 if y == ymax else yy)\n\n                append(i, xi, yi, *aggs_and_cols)\n\n        def extend(aggs, df, vt, bounds):\n            aggs_and_cols = aggs + info(df, aggs[0].shape[:2])\n            sx, tx, sy, ty = vt\n            xmin, xmax, ymin, ymax = bounds\n            geometry = df[geometry_name].array\n\n            ragged = shapely.to_ragged_array(geometry)\n            geometry_type = ragged[0]\n\n            if geometry_type not in (shapely.GeometryType.MULTIPOINT, shapely.GeometryType.POINT):\n                raise ValueError(\n                    \"Canvas.points supports GeoPandas geometry types of POINT and MULTIPOINT, \"\n                    f\"not {repr(geometry_type)}\")\n\n            coords = ragged[1].ravel()  # No offsets required if POINT not MULTIPOINT\n            if geometry_type == shapely.GeometryType.POINT:\n                extend_point_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax, coords, *aggs_and_cols)\n            else:\n                offsets = ragged[2][0]\n                extend_multipoint_cpu(\n                    sx, tx, sy, ty, xmin, xmax, ymin, ymax, coords, offsets, *aggs_and_cols)\n\n        @ngjit\n        @self.expand_aggs_and_cols(append)\n        def extend_multipoint_cpu(\n            sx, tx, sy, ty, xmin, xmax, ymin, ymax, values, offsets, *aggs_and_cols,\n        ):\n            for i in range(len(offsets) - 1):\n                start = offsets[i]\n                stop = offsets[i+1]\n                for j in range(start, stop):\n                    _perform_extend_points(\n                        i, 2*j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, values, *aggs_and_cols,\n                    )\n\n        @ngjit\n        @self.expand_aggs_and_cols(append)\n        def extend_point_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax, values, *aggs_and_cols):\n            n = len(values) // 2\n            for i in range(n):\n                _perform_extend_points(\n                    i, 2*i, sx, tx, sy, ty, xmin, xmax, ymin, ymax, values, *aggs_and_cols,\n                )\n\n        return extend\n\n\nclass MultiPointGeometry(_GeometryLike):\n    # spatialpandas must be available if a MultiPointGeometry object is created.\n\n    @property\n    def geom_dtypes(self):\n        from spatialpandas.geometry import PointDtype, MultiPointDtype\n        return PointDtype, MultiPointDtype\n\n    @memoize\n    def _build_extend(self, x_mapper, y_mapper, info, append, _antialias_stage_2,\n                      _antialias_stage_2_funcs):\n        geometry_name = self.geometry\n\n        @ngjit\n        @self.expand_aggs_and_cols(append)\n        def _perform_extend_points(\n                i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, values, *aggs_and_cols\n        ):\n            x = values[j]\n            y = values[j + 1]\n            # points outside bounds are dropped; remainder\n            # are mapped onto pixels\n            if (xmin <= x <= xmax) and (ymin <= y <= ymax):\n                xx = int(x_mapper(x) * sx + tx)\n                yy = int(y_mapper(y) * sy + ty)\n                xi, yi = (xx - 1 if x == xmax else xx,\n                          yy - 1 if y == ymax else yy)\n\n                append(i, xi, yi, *aggs_and_cols)\n\n        @ngjit\n        @self.expand_aggs_and_cols(append)\n        def extend_point_cpu(\n                sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                values, missing, eligible_inds, *aggs_and_cols\n        ):\n            for i in eligible_inds:\n                if missing[i] is True:\n                    continue\n                _perform_extend_points(\n                    i, 2 * i, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                    values, *aggs_and_cols\n                )\n\n        @ngjit\n        @self.expand_aggs_and_cols(append)\n        def extend_multipoint_cpu(\n                sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                values, missing, offsets, eligible_inds, *aggs_and_cols\n        ):\n            for i in eligible_inds:\n                if missing[i] is True:\n                    continue\n                start = offsets[i]\n                stop = offsets[i + 1]\n                for j in range(start, stop, 2):\n                    _perform_extend_points(\n                        i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                        values, *aggs_and_cols\n                    )\n\n        def extend(aggs, df, vt, bounds):\n            from spatialpandas.geometry import PointArray\n\n            aggs_and_cols = aggs + info(df, aggs[0].shape[:2])\n            sx, tx, sy, ty = vt\n            xmin, xmax, ymin, ymax = bounds\n\n            geometry = df[geometry_name].array\n\n            if geometry._sindex is not None:\n                # Compute indices of potentially intersecting polygons using\n                # geometry's R-tree if there is one\n                eligible_inds = geometry.sindex.intersects((xmin, ymin, xmax, ymax))\n            else:\n                # Otherwise, process all indices\n                eligible_inds = np.arange(0, len(geometry), dtype='uint32')\n\n            missing = geometry.isna()\n\n            if isinstance(geometry, PointArray):\n                values = geometry.flat_values\n                extend_point_cpu(\n                    sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                    values, missing, eligible_inds, *aggs_and_cols\n                )\n            else:\n                values = geometry.buffer_values\n                offsets = geometry.buffer_offsets[0]\n\n                extend_multipoint_cpu(\n                    sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                    values, missing, offsets, eligible_inds, *aggs_and_cols\n                )\n\n        return extend\n",
    "datashader/pipeline.py": "from __future__ import annotations\n\nfrom toolz import identity\n\nfrom . import transfer_functions as tf\nfrom . import reductions\nfrom . import core\n\n\nclass Pipeline:\n    \"\"\"A datashading pipeline callback.\n\n    Given a declarative specification, creates a callable with the following\n    signature:\n\n    ``callback(x_range, y_range, width, height)``\n\n    where ``x_range`` and ``y_range`` form the bounding box on the viewport,\n    and ``width`` and ``height`` specify the output image dimensions.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame, dask.DataFrame\n    glyph : Glyph\n        The glyph to bin by.\n    agg : Reduction, optional\n        The reduction to compute per-pixel. Default is ``count()``.\n    transform_fn : callable, optional\n        A callable that takes the computed aggregate as an argument, and\n        returns another aggregate. This can be used to do preprocessing before\n        passing to the ``color_fn`` function.\n    color_fn : callable, optional\n        A callable that takes the output of ``tranform_fn``, and returns an\n        ``Image`` object. Default is ``shade``.\n    spread_fn : callable, optional\n        A callable that takes the output of ``color_fn``, and returns another\n        ``Image`` object. Default is ``dynspread``.\n    height_scale: float, optional\n        Factor by which to scale the provided height\n    width_scale: float, optional\n        Factor by which to scale the provided width\n    \"\"\"\n    def __init__(self, df, glyph, agg=reductions.count(),\n                 transform_fn=identity, color_fn=tf.shade, spread_fn=tf.dynspread,\n                 width_scale=1.0, height_scale=1.0):\n        self.df = df\n        self.glyph = glyph\n        self.agg = agg\n        self.transform_fn = transform_fn\n        self.color_fn = color_fn\n        self.spread_fn = spread_fn\n        self.width_scale = width_scale\n        self.height_scale = height_scale\n\n    def __call__(self, x_range=None, y_range=None, width=600, height=600):\n        \"\"\"Compute an image from the specified pipeline.\n\n        Parameters\n        ----------\n        x_range, y_range : tuple, optional\n            The bounding box on the viewport, specified as tuples of\n            ``(min, max)``\n        width, height : int, optional\n            The shape of the image\n        \"\"\"\n        canvas = core.Canvas(plot_width=int(width*self.width_scale),\n                             plot_height=int(height*self.height_scale),\n                             x_range=x_range, y_range=y_range)\n        bins = core.bypixel(self.df, canvas, self.glyph, self.agg,\n                            antialias=self.glyph.antialiased)\n        img = self.color_fn(self.transform_fn(bins))\n        return self.spread_fn(img)\n",
    "datashader/transfer_functions/__init__.py": "from __future__ import annotations\n\nfrom collections.abc import Iterator\n\nfrom io import BytesIO\n\nimport warnings\n\nimport numpy as np\nimport numba as nb\nimport toolz as tz\nimport xarray as xr\nfrom PIL.Image import fromarray\n\nfrom datashader.colors import rgb, Sets1to3\nfrom datashader.utils import nansum_missing, ngjit\n\ntry:\n    import dask.array as da\nexcept ImportError:\n    da = None\n\ntry:\n    import cupy\nexcept Exception:\n    cupy = None\n\n__all__ = ['Image', 'stack', 'shade', 'set_background', 'spread', 'dynspread']\n\n\nclass Image(xr.DataArray):\n    __slots__ = ()\n    __array_priority__ = 70\n    border=1\n\n    def to_pil(self, origin='lower'):\n        data = self.data\n        if cupy:\n            data = cupy.asnumpy(data)\n        arr = np.flipud(data) if origin == 'lower' else data\n        return fromarray(arr, 'RGBA')\n\n    def to_bytesio(self, format='png', origin='lower'):\n        fp = BytesIO()\n        self.to_pil(origin).save(fp, format)\n        fp.seek(0)\n        return fp\n\n    def _repr_png_(self):\n        \"\"\"Supports rich PNG display in a Jupyter notebook\"\"\"\n        return self.to_pil()._repr_png_()\n\n    def _repr_html_(self):\n        \"\"\"Supports rich HTML display in a Jupyter notebook\"\"\"\n        # imported here to avoid depending on these packages unless actually used\n        from io import BytesIO\n        from base64 import b64encode\n\n        b = BytesIO()\n        self.to_pil().save(b, format='png')\n\n        h = \"\"\"<img style=\"margin: auto; border:\"\"\" + str(self.border) + \"\"\"px solid\" \"\"\" + \\\n            \"\"\"src='data:image/png;base64,{0}'/>\"\"\".\\\n                format(b64encode(b.getvalue()).decode('utf-8'))\n        return h\n\n\n\nclass Images:\n    \"\"\"\n    A list of HTML-representable objects to display in a table.\n    Primarily intended for Image objects, but could be anything\n    that has _repr_html_.\n    \"\"\"\n\n    def __init__(self, *images):\n        \"\"\"Makes an HTML table from a list of HTML-representable arguments.\"\"\"\n        for i in images:\n            assert hasattr(i,\"_repr_html_\")\n        self.images = images\n        self.num_cols = None\n\n    def cols(self,n):\n        \"\"\"\n        Set the number of columns to use in the HTML table.\n        Returns self for convenience.\n        \"\"\"\n        self.num_cols=n\n        return self\n\n    def _repr_html_(self):\n        \"\"\"Supports rich display in a Jupyter notebook, using an HTML table\"\"\"\n        htmls = []\n        col=0\n        tr=\"\"\"<tr style=\"background-color:white\">\"\"\"\n        for i in self.images:\n            label=i.name if hasattr(i,\"name\") and i.name is not None else \"\"\n\n            htmls.append(\"\"\"<td style=\"text-align: center\"><b>\"\"\" + label +\n                         \"\"\"</b><br><br>{0}</td>\"\"\".format(i._repr_html_()))\n            col+=1\n            if self.num_cols is not None and col>=self.num_cols:\n                col=0\n                htmls.append(\"</tr>\"+tr)\n\n        return \"\"\"<table style=\"width:100%; text-align: center\"><tbody>\"\"\"+ tr +\\\n               \"\".join(htmls) + \"\"\"</tr></tbody></table>\"\"\"\n\n\n\ndef stack(*imgs, **kwargs):\n    \"\"\"Combine images together, overlaying later images onto earlier ones.\n\n    Parameters\n    ----------\n    imgs : iterable of Image\n        The images to combine.\n    how : str, optional\n        The compositing operator to combine pixels. Default is `'over'`.\n    \"\"\"\n    from datashader.composite import composite_op_lookup\n\n    if not imgs:\n        raise ValueError(\"No images passed in\")\n    shapes = []\n    for i in imgs:\n        if not isinstance(i, Image):\n            raise TypeError(\"Expected `Image`, got: `{0}`\".format(type(i)))\n        elif not shapes:\n            shapes.append(i.shape)\n        elif shapes and i.shape not in shapes:\n            raise ValueError(\"The stacked images must have the same shape.\")\n\n    name = kwargs.get('name', None)\n    op = composite_op_lookup[kwargs.get('how', 'over')]\n    if len(imgs) == 1:\n        return imgs[0]\n    imgs = xr.align(*imgs, copy=False, join='outer')\n    with np.errstate(divide='ignore', invalid='ignore'):\n        out = tz.reduce(tz.flip(op), [i.data for i in imgs])\n    return Image(out, coords=imgs[0].coords, dims=imgs[0].dims, name=name)\n\n\ndef eq_hist(data, mask=None, nbins=256*256):\n    \"\"\"Compute the numpy array after histogram equalization.\n\n    For use in `shade`.\n\n    Parameters\n    ----------\n    data : ndarray\n    mask : ndarray, optional\n       Boolean array of missing points. Where True, the output will be `NaN`.\n    nbins : int, optional\n        Maximum number of bins to use. If data is of type boolean or integer\n        this will determine when to switch from exact unique value counts to\n        a binned histogram.\n\n    Returns\n    -------\n    ndarray or tuple(ndarray, int)\n        Returns the array when mask isn't set, otherwise returns the\n        array and the computed number of discrete levels.\n\n    Notes\n    -----\n    This function is adapted from the implementation in scikit-image [1]_.\n\n    References\n    ----------\n    .. [1] http://scikit-image.org/docs/stable/api/skimage.exposure.html#equalize-hist\n    \"\"\"\n    if cupy and isinstance(data, cupy.ndarray):\n        from._cuda_utils import interp\n        array_module = cupy\n    elif not isinstance(data, np.ndarray):\n        raise TypeError(\"data must be an ndarray\")\n    else:\n        interp = np.interp\n        array_module = np\n\n    if mask is not None and array_module.all(mask):\n        # Issue #1166, return early with array of all nans if all of data is masked out.\n        return array_module.full_like(data, np.nan), 0\n\n    data2 = data if mask is None else data[~mask]\n\n    # Run more accurate value counting if data is of boolean or integer type\n    # and unique value array is smaller than nbins.\n    if data2.dtype == bool or (array_module.issubdtype(data2.dtype, array_module.integer) and\n                               array_module.ptp(data2) < nbins):\n        values, counts = array_module.unique(data2, return_counts=True)\n        vmin, vmax = values[0].item(), values[-1].item()  # Convert from arrays to scalars.\n        interval = vmax-vmin\n        bin_centers = array_module.arange(vmin, vmax+1)\n        hist = array_module.zeros(interval+1, dtype='uint64')\n        hist[values-vmin] = counts\n        discrete_levels = len(values)\n    else:\n        hist, bin_edges = array_module.histogram(data2, bins=nbins)\n        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n        keep_mask = (hist > 0)\n        discrete_levels = array_module.count_nonzero(keep_mask)\n        if discrete_levels != len(hist):\n            # Remove empty histogram bins.\n            hist = hist[keep_mask]\n            bin_centers = bin_centers[keep_mask]\n    cdf = hist.cumsum()\n    cdf = cdf / float(cdf[-1])\n    out = interp(data, bin_centers, cdf).reshape(data.shape)\n    return out if mask is None else array_module.where(mask, array_module.nan, out), discrete_levels\n\n\n\n\n_interpolate_lookup = {'log': lambda d, m: np.log1p(np.where(m, np.nan, d)),\n                       'cbrt': lambda d, m: np.where(m, np.nan, d)**(1/3.),\n                       'linear': lambda d, m: np.where(m, np.nan, d),\n                       'eq_hist': eq_hist}\n\n\ndef _normalize_interpolate_how(how):\n    if callable(how):\n        return how\n    elif how in _interpolate_lookup:\n        return _interpolate_lookup[how]\n    raise ValueError(\"Unknown interpolation method: {0}\".format(how))\n\n\ndef _rescale_discrete_levels(discrete_levels, span):\n    if discrete_levels is None:\n        raise ValueError(\"interpolator did not return a valid discrete_levels\")\n\n    # Straight line y = mx + c through (2, 1.5) and (100, 1) where\n    # x is number of discrete_levels and y is lower span limit.\n    m = -0.5/98.0  # (y[1] - y[0]) / (x[1] - x[0])\n    c = 1.5 - 2*m  # y[0] - m*x[0]\n    multiple = m*discrete_levels + c\n\n    if multiple > 1:\n        lower_span = max(span[1] - multiple*(span[1] - span[0]), 0)\n        span = (lower_span, 1)\n\n    return span\n\n\ndef _interpolate(agg, cmap, how, alpha, span, min_alpha, name, rescale_discrete_levels):\n    if cupy and isinstance(agg.data, cupy.ndarray):\n        from ._cuda_utils import masked_clip_2d, interp\n    else:\n        from ._cpu_utils import masked_clip_2d\n        interp = np.interp\n\n    if agg.ndim != 2:\n        raise ValueError(\"agg must be 2D\")\n    interpolater = _normalize_interpolate_how(how)\n\n    data = agg.data\n    if da and isinstance(data, da.Array):\n        data = data.compute()\n    else:\n        data = data.copy()\n\n    # Compute mask\n    if np.issubdtype(data.dtype, np.bool_):\n        mask = ~data\n        data = data.astype(np.int8)\n    else:\n        if data.dtype.kind == 'u':\n            mask = data == 0\n        else:\n            mask = np.isnan(data)\n\n    # Handle case where everything is masked out\n    if mask.all():\n        return Image(np.zeros(shape=agg.data.shape,\n                              dtype=np.uint32), coords=agg.coords,\n                     dims=agg.dims, attrs=agg.attrs, name=name)\n\n    # Handle offset / clip\n    if span is None:\n        offset = np.nanmin(data[~mask])\n    else:\n        offset = np.array(span, dtype=data.dtype)[0]\n        masked_clip_2d(data, mask, *span)\n\n    # If log/cbrt, could case to float64 right away\n    # If linear, can keep current type\n    data -= offset\n\n    with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n        # Transform data (log, eq_hist, etc.)\n        data = interpolater(data, mask)\n        discrete_levels = None\n        if isinstance(data, (list, tuple)):\n            data, discrete_levels = data\n\n        # Transform span\n        if span is None:\n            masked_data = np.where(~mask, data, np.nan)\n            span = np.nanmin(masked_data), np.nanmax(masked_data)\n\n            if rescale_discrete_levels and discrete_levels is not None:  # Only valid for eq_hist\n                span = _rescale_discrete_levels(discrete_levels, span)\n        else:\n            if how == 'eq_hist':\n                # For eq_hist to work with span, we'd need to compute the histogram\n                # only on the specified span's range.\n                raise ValueError(\"span is not (yet) valid to use with eq_hist\")\n\n            span = interpolater([0, span[1] - span[0]], 0)\n\n    if isinstance(cmap, Iterator):\n        cmap = list(cmap)\n    if isinstance(cmap, tuple) and isinstance(cmap[0], str):\n        cmap = list(cmap)  # Tuple of hex values or color names, as produced by Bokeh\n    if isinstance(cmap, list):\n        rspan, gspan, bspan = np.array(list(zip(*map(rgb, cmap))))\n        span = np.linspace(span[0], span[1], len(cmap))\n        r = np.nan_to_num(interp(data, span, rspan, left=255), copy=False).astype(np.uint8)\n        g = np.nan_to_num(interp(data, span, gspan, left=255), copy=False).astype(np.uint8)\n        b = np.nan_to_num(interp(data, span, bspan, left=255), copy=False).astype(np.uint8)\n        a = np.where(np.isnan(data), 0, alpha).astype(np.uint8)\n        rgba = np.dstack([r, g, b, a])\n    elif isinstance(cmap, str) or isinstance(cmap, tuple):\n        color = rgb(cmap)\n        aspan = np.arange(min_alpha, alpha+1)\n        span = np.linspace(span[0], span[1], len(aspan))\n        r = np.full(data.shape, color[0], dtype=np.uint8)\n        g = np.full(data.shape, color[1], dtype=np.uint8)\n        b = np.full(data.shape, color[2], dtype=np.uint8)\n        a = np.nan_to_num(interp(data, span, aspan, left=0, right=255), copy=False).astype(np.uint8)\n        rgba = np.dstack([r, g, b, a])\n    elif callable(cmap):\n        # Assume callable is matplotlib colormap\n        scaled_data = (data - span[0])/(span[1] - span[0])\n        if cupy and isinstance(scaled_data, cupy.ndarray):\n            # Convert cupy array to numpy before passing to matplotlib colormap\n            scaled_data = cupy.asnumpy(scaled_data)\n\n        rgba = cmap(scaled_data, bytes=True)\n        rgba[:, :, 3] = np.where(np.isnan(scaled_data), 0, alpha).astype(np.uint8)\n    else:\n        raise TypeError(\"Expected `cmap` of `matplotlib.colors.Colormap`, \"\n                        \"`list`, `str`, or `tuple`; got: '{0}'\".format(type(cmap)))\n\n    img = rgba.view(np.uint32).reshape(data.shape)\n\n    if cupy and isinstance(img, cupy.ndarray):\n        # Convert cupy array to numpy for final image\n        img = cupy.asnumpy(img)\n\n    return Image(img, coords=agg.coords, dims=agg.dims, name=name)\n\n\ndef _colorize(agg, color_key, how, alpha, span, min_alpha, name, color_baseline,\n              rescale_discrete_levels):\n    if cupy and isinstance(agg.data, cupy.ndarray):\n        array = cupy.array\n    else:\n        array = np.array\n\n    if not agg.ndim == 3:\n        raise ValueError(\"agg must be 3D\")\n\n    cats = agg.indexes[agg.dims[-1]]\n    if not len(cats): # No categories and therefore no data; return an empty image\n        return Image(np.zeros(agg.shape[0:2], dtype=np.uint32), dims=agg.dims[:-1],\n                     coords=dict([\n                         (agg.dims[1], agg.coords[agg.dims[1]]),\n                         (agg.dims[0], agg.coords[agg.dims[0]]) ]), name=name)\n\n    if color_key is None:\n        raise ValueError(\"Color key must be provided, with at least as many \" +\n                         \"colors as there are categorical fields\")\n    if not isinstance(color_key, dict):\n        color_key = dict(zip(cats, color_key))\n    if len(color_key) < len(cats):\n        raise ValueError(f\"Insufficient colors provided ({len(color_key)}) for the categorical \"\n                         f\"fields available ({len(cats)})\")\n\n    colors = [rgb(color_key[c]) for c in cats]\n    rs, gs, bs = map(array, zip(*colors))\n\n    # Reorient array (transposing the category dimension first)\n    agg_t = agg.transpose(*((agg.dims[-1],)+agg.dims[:2]))\n    data = agg_t.data.transpose([1, 2, 0])\n    if da and isinstance(data, da.Array):\n        data = data.compute()\n    color_data = data.copy()\n\n    # subtract color_baseline if needed\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', r'All-NaN slice encountered')\n        baseline = np.nanmin(color_data) if color_baseline is None else color_baseline\n    with np.errstate(invalid='ignore'):\n        if baseline > 0:\n            color_data -= baseline\n        elif baseline < 0:\n            color_data += -baseline\n        if color_data.dtype.kind != 'u' and color_baseline is not None:\n            color_data[color_data<0]=0\n\n    color_total = nansum_missing(color_data, axis=2)\n    # dot does not handle nans, so replace with zeros\n    color_data[np.isnan(data)] = 0\n\n    # zero-count pixels will be 0/0, but it's safe to ignore that when dividing\n    with np.errstate(divide='ignore', invalid='ignore'):\n        r = (color_data.dot(rs)/color_total).astype(np.uint8)\n        g = (color_data.dot(gs)/color_total).astype(np.uint8)\n        b = (color_data.dot(bs)/color_total).astype(np.uint8)\n\n    # special case -- to give an appropriate color when min_alpha != 0 and data=0,\n    # take avg color of all non-nan categories\n    color_mask = ~np.isnan(data)\n    cmask_sum = np.sum(color_mask, axis=2)\n\n    with np.errstate(divide='ignore', invalid='ignore'):\n        r2 = (color_mask.dot(rs)/cmask_sum).astype(np.uint8)\n        g2 = (color_mask.dot(gs)/cmask_sum).astype(np.uint8)\n        b2 = (color_mask.dot(bs)/cmask_sum).astype(np.uint8)\n\n    missing_colors = np.sum(color_data, axis=2) == 0\n    r = np.where(missing_colors, r2, r)\n    g = np.where(missing_colors, g2, g)\n    b = np.where(missing_colors, b2, b)\n\n    total = nansum_missing(data, axis=2)\n    mask = np.isnan(total)\n    a = _interpolate_alpha(data, total, mask, how, alpha, span, min_alpha, rescale_discrete_levels)\n\n    values = np.dstack([r, g, b, a]).view(np.uint32).reshape(a.shape)\n    if cupy and isinstance(values, cupy.ndarray):\n        # Convert cupy array to numpy for final image\n        values = cupy.asnumpy(values)\n\n    return Image(values,\n                 dims=agg.dims[:-1],\n                 coords=dict([\n                     (agg.dims[1], agg.coords[agg.dims[1]]),\n                     (agg.dims[0], agg.coords[agg.dims[0]]),\n                 ]),\n                 name=name)\n\n\ndef _interpolate_alpha(data, total, mask, how, alpha, span, min_alpha, rescale_discrete_levels):\n\n    if cupy and isinstance(data, cupy.ndarray):\n        from ._cuda_utils import interp, masked_clip_2d\n        array_module = cupy\n    else:\n        from ._cpu_utils import masked_clip_2d\n        interp = np.interp\n        array_module = np\n\n    # if span is provided, use it, otherwise produce a span based off the\n    # min/max of the data\n    if span is None:\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', r'All-NaN slice encountered')\n            offset = np.nanmin(total)\n        if total.dtype.kind == 'u' and offset == 0:\n            mask = mask | (total == 0)\n            # If at least one element is not masked, use the minimum as the offset\n            # otherwise the offset remains at zero\n            if not np.all(mask):\n                offset = total[total > 0].min()\n            total = np.where(~mask, total, np.nan)\n\n        a_scaled = _normalize_interpolate_how(how)(total - offset, mask)\n        discrete_levels = None\n        if isinstance(a_scaled, (list, tuple)):\n            a_scaled, discrete_levels = a_scaled\n\n        # All-NaN objects (e.g. chunks of arrays with no data) are valid in Datashader\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n            norm_span = [np.nanmin(a_scaled).item(), np.nanmax(a_scaled).item()]\n\n        if rescale_discrete_levels and discrete_levels is not None:  # Only valid for how='eq_hist'\n            norm_span = _rescale_discrete_levels(discrete_levels, norm_span)\n\n    else:\n        if how == 'eq_hist':\n            # For eq_hist to work with span, we'll need to compute the histogram\n            # only on the specified span's range.\n            raise ValueError(\"span is not (yet) valid to use with eq_hist\")\n        # even in fixed-span mode cells with 0 should remain fully transparent\n        # i.e. a 0 will be fully transparent, but any non-zero number will\n        # be clipped to the span range and have min-alpha applied\n        offset = np.array(span, dtype=data.dtype)[0]\n        if total.dtype.kind == 'u' and np.nanmin(total) == 0:\n            mask = mask | (total <= 0)\n            total = np.where(~mask, total, np.nan)\n        masked_clip_2d(total, mask, *span)\n\n        a_scaled = _normalize_interpolate_how(how)(total - offset, mask)\n        if isinstance(a_scaled, (list, tuple)):\n            a_scaled = a_scaled[0]  # Ignore discrete_levels\n\n        norm_span = _normalize_interpolate_how(how)([0, span[1] - span[0]], 0)\n        if isinstance(norm_span, (list, tuple)):\n            norm_span = norm_span[0]  # Ignore discrete_levels\n\n    # Issue 1178. Convert norm_span from 2-tuple to numpy/cupy array.\n    # array_module.hstack() tolerates tuple of one float and one cupy array,\n    # whereas array_module.array() does not.\n    norm_span = array_module.hstack(norm_span)\n\n    # Interpolate the alpha values\n    a_float = interp(a_scaled, norm_span, array_module.array([min_alpha, alpha]), left=0, right=255)\n    a = np.nan_to_num(a_float, copy=False).astype(np.uint8)\n    return a\n\n\ndef _apply_discrete_colorkey(agg, color_key, alpha, name, color_baseline):\n    # use the same approach as 3D case\n\n    if cupy and isinstance(agg.data, cupy.ndarray):\n        module = cupy\n        array = cupy.array\n    else:\n        module = np\n        array = np.array\n\n    if not agg.ndim == 2:\n        raise ValueError(\"agg must be 2D\")\n\n    # validate color_key\n    if (color_key is None) or (not isinstance(color_key, dict)):\n        raise ValueError(\"Color key must be provided as a dictionary\")\n\n    agg_data = agg.data\n    if da and isinstance(agg_data, da.Array):\n        agg_data = agg_data.compute()\n\n    cats = color_key.keys()\n    colors = [rgb(color_key[c]) for c in cats]\n    rs, gs, bs = map(array, zip(*colors))\n\n    data = module.empty_like(agg_data) * module.nan\n\n    r = module.zeros_like(data, dtype=module.uint8)\n    g = module.zeros_like(data, dtype=module.uint8)\n    b = module.zeros_like(data, dtype=module.uint8)\n\n    r2 = module.zeros_like(data, dtype=module.uint8)\n    g2 = module.zeros_like(data, dtype=module.uint8)\n    b2 = module.zeros_like(data, dtype=module.uint8)\n\n    for i, c in enumerate(cats):\n        value_mask = agg_data == c\n        data[value_mask] = 1\n        r2[value_mask] = rs[i]\n        g2[value_mask] = gs[i]\n        b2[value_mask] = bs[i]\n\n    color_data = data.copy()\n\n    # subtract color_baseline if needed\n    baseline = module.nanmin(color_data) if color_baseline is None else color_baseline\n    with np.errstate(invalid='ignore'):\n        if baseline > 0:\n            color_data -= baseline\n        elif baseline < 0:\n            color_data += -baseline\n        if color_data.dtype.kind != 'u' and color_baseline is not None:\n            color_data[color_data < 0] = 0\n\n    color_data[module.isnan(data)] = 0\n    if not color_data.any():\n        r[:] = r2\n        g[:] = g2\n        b[:] = b2\n\n    missing_colors = color_data == 0\n    r = module.where(missing_colors, r2, r)\n    g = module.where(missing_colors, g2, g)\n    b = module.where(missing_colors, b2, b)\n\n    # alpha channel\n    a = np.where(np.isnan(data), 0, alpha).astype(np.uint8)\n\n    values = module.dstack([r, g, b, a]).view(module.uint32).reshape(a.shape)\n\n    if cupy and isinstance(agg.data, cupy.ndarray):\n        # Convert cupy array to numpy for final image\n        values = cupy.asnumpy(values)\n\n    return Image(values,\n                 dims=agg.dims,\n                 coords=agg.coords,\n                 name=name\n                 )\n\n\ndef shade(agg, cmap=[\"lightblue\", \"darkblue\"], color_key=Sets1to3,\n          how='eq_hist', alpha=255, min_alpha=40, span=None, name=None,\n          color_baseline=None, rescale_discrete_levels=False):\n    \"\"\"Convert a DataArray to an image by choosing an RGBA pixel color for each value.\n\n    Requires a DataArray with a single data dimension, here called the\n    \"value\", indexed using either 2D or 3D coordinates.\n\n    For a DataArray with 2D coordinates, the RGB channels are computed\n    from the values by interpolated lookup into the given colormap\n    ``cmap``.  The A channel is then set to the given fixed ``alpha``\n    value for all non-zero values, and to zero for all zero values.\n    A dictionary ``color_key`` that specifies categories (values in ``agg``)\n    and corresponding colors can be provided to support discrete coloring\n    2D aggregates, i.e aggregates with a single category per pixel,\n    with no mixing. The A channel is set the given ``alpha`` value for all\n    pixels in the categories specified in ``color_key``, and to zero otherwise.\n\n    DataArrays with 3D coordinates are expected to contain values\n    distributed over different categories that are indexed by the\n    additional coordinate. Such an array would reduce to the\n    2D-coordinate case if collapsed across the categories (e.g. if one\n    did ``aggc.sum(dim='cat')`` for a categorical dimension ``cat``).\n    The RGB channels for the uncollapsed, 3D case are mixed from separate\n    values over all categories. They are computed by averaging the colors\n    in the provided ``color_key`` (with one color per category),\n    weighted by the array's value for that category.\n    The A channel is then computed from the array's total value\n    collapsed across all categories at that location, ranging from the\n    specified ``min_alpha`` to the maximum alpha value (255).\n\n    Parameters\n    ----------\n    agg : DataArray\n    cmap : list of colors or matplotlib.colors.Colormap, optional\n        The colormap to use for 2D agg arrays. Can be either a list of\n        colors (specified either by name, RGBA hexcode, or as a tuple\n        of ``(red, green, blue)`` values.), or a matplotlib colormap\n        object.  Default is ``[\"lightblue\", \"darkblue\"]``.\n    color_key : dict or iterable\n        The colors to use for a categorical agg array. In 3D case, it can be\n        either a ``dict`` mapping from field name to colors, or an\n        iterable of colors in the same order as the record fields,\n        and including at least that many distinct colors. In 2D case,\n        ``color_key`` must be a ``dict`` where all keys are categories,\n        and values are corresponding colors. Number of categories does not\n        necessarily equal to the number of unique values in the agg DataArray.\n    how : str or callable, optional\n        The interpolation method to use, for the ``cmap`` of a 2D\n        DataArray or the alpha channel of a 3D DataArray. Valid\n        strings are 'eq_hist' [default], 'cbrt' (cube root), 'log'\n        (logarithmic), and 'linear'. Callables take 2 arguments - a\n        2-dimensional array of magnitudes at each pixel, and a boolean\n        mask array indicating missingness. They should return a numeric\n        array of the same shape, with ``NaN`` values where the mask was\n        True.\n    alpha : int, optional\n        Value between 0 - 255 representing the alpha value to use for\n        colormapped pixels that contain data (i.e. non-NaN values).\n        Also used as the maximum alpha value when alpha is indicating\n        data value, such as for single colors or categorical plots.\n        Regardless of this value, ``NaN`` values are set to be fully\n        transparent when doing colormapping.\n    min_alpha : float, optional\n        The minimum alpha value to use for non-empty pixels when\n        alpha is indicating data value, in [0, 255].  Use a higher value\n        to avoid undersaturation, i.e. poorly visible low-value datapoints,\n        at the expense of the overall dynamic range. Note that ``min_alpha``\n        will not take any effect when doing discrete categorical coloring\n        for 2D case as the aggregate can have only a single value to denote\n        the category.\n    span : list of min-max range, optional\n        Min and max data values to use for 2D colormapping,\n        and 3D alpha interpolation, when wishing to override autoranging.\n    name : string name, optional\n        Optional string name to give to the Image object to return,\n        to label results for display.\n    color_baseline : float or None\n        Baseline for calculating how categorical data mixes to\n        determine the color of a pixel. The color for each category is\n        weighted by how far that category's value is above this\n        baseline value, out of the total sum across all categories'\n        values. A value of zero is appropriate for counts and for\n        other physical quantities for which zero is a meaningful\n        reference; each category then contributes to the final color\n        in proportion to how much each category contributes to the\n        final sum.  However, if values can be negative or if they are\n        on an interval scale where values e.g. twice as far from zero\n        are not twice as high (such as temperature in Fahrenheit), then\n        you will need to provide a suitable baseline value for use in\n        calculating color mixing.  A value of None (the default) means\n        to take the minimum across the entire aggregate array, which\n        is safe but may not weight the colors as you expect; any\n        categories with values near this baseline will contribute\n        almost nothing to the final color. As a special case, if the\n        only data present in a pixel is at the baseline level, the\n        color will be an evenly weighted average of all such\n        categories with data (to avoid the color being undefined in\n        this case).\n    rescale_discrete_levels : boolean, optional\n        If ``how='eq_hist`` and there are only a few discrete values,\n        then ``rescale_discrete_levels=True`` decreases the lower\n        limit of the autoranged span so that the values are rendering\n        towards the (more visible) top of the ``cmap`` range, thus\n        avoiding washout of the lower values.  Has no effect if\n        ``how!=`eq_hist``. Default is False.\n    \"\"\"\n    if not isinstance(agg, xr.DataArray):\n        raise TypeError(\"agg must be instance of DataArray\")\n    name = agg.name if name is None else name\n\n    if not ((0 <= min_alpha <= 255) and (0 <= alpha <= 255)):\n        raise ValueError(f\"min_alpha ({min_alpha}) and alpha ({alpha}) must be between 0 and 255\")\n\n    if rescale_discrete_levels and how != 'eq_hist':\n        rescale_discrete_levels = False\n\n    if agg.ndim == 2:\n        if color_key is not None and isinstance(color_key, dict):\n            return _apply_discrete_colorkey(\n                agg, color_key, alpha, name, color_baseline\n            )\n        else:\n            return _interpolate(agg, cmap, how, alpha, span, min_alpha, name,\n                                rescale_discrete_levels)\n    elif agg.ndim == 3:\n        return _colorize(agg, color_key, how, alpha, span, min_alpha, name, color_baseline,\n                         rescale_discrete_levels)\n    else:\n        raise ValueError(\"agg must use 2D or 3D coordinates\")\n\n\ndef set_background(img, color=None, name=None):\n    \"\"\"Return a new image, with the background set to `color`.\n\n    Parameters\n    ----------\n    img : Image\n    color : color name or tuple, optional\n        The background color. Can be specified either by name, hexcode, or as a\n        tuple of ``(red, green, blue)`` values.\n    \"\"\"\n    from datashader.composite import over\n\n    if not isinstance(img, Image):\n        raise TypeError(\"Expected `Image`, got: `{0}`\".format(type(img)))\n    name = img.name if name is None else name\n    if color is None:\n        return img\n    background = np.uint8(rgb(color) + (255,)).view('uint32')[0]\n    data = over(img.data, background)\n    return Image(data, coords=img.coords, dims=img.dims, name=name)\n\n\ndef spread(img, px=1, shape='circle', how=None, mask=None, name=None):\n    \"\"\"Spread pixels in an image.\n\n    Spreading expands each pixel a certain number of pixels on all sides\n    according to a given shape, merging pixels using a specified compositing\n    operator. This can be useful to make sparse plots more visible.\n\n    Parameters\n    ----------\n    img : Image or other DataArray\n    px : int, optional\n        Number of pixels to spread on all sides\n    shape : str, optional\n        The shape to spread by. Options are 'circle' [default] or 'square'.\n    how : str, optional\n        The name of the compositing operator to use when combining\n        pixels. Default of None uses 'over' operator for Image objects\n        and 'add' operator otherwise.\n    mask : ndarray, shape (M, M), optional\n        The mask to spread over. If provided, this mask is used instead of\n        generating one based on `px` and `shape`. Must be a square array\n        with odd dimensions. Pixels are spread from the center of the mask to\n        locations where the mask is True.\n    name : string name, optional\n        Optional string name to give to the Image object to return,\n        to label results for display.\n    \"\"\"\n    if not isinstance(img, xr.DataArray):\n        raise TypeError(\"Expected `xr.DataArray`, got: `{0}`\".format(type(img)))\n    is_image = isinstance(img, Image)\n    name = img.name if name is None else name\n    if mask is None:\n        if not isinstance(px, int) or px < 0:\n            raise ValueError(\"``px`` must be an integer >= 0\")\n        if px == 0:\n            return img\n        mask = _mask_lookup[shape](px)\n    elif not (isinstance(mask, np.ndarray) and mask.ndim == 2 and\n              mask.shape[0] == mask.shape[1] and mask.shape[0] % 2 == 1):\n        raise ValueError(\"mask must be a square 2 dimensional ndarray with \"\n                         \"odd dimensions.\")\n        mask = mask if mask.dtype == 'bool' else mask.astype('bool')\n    if how is None:\n        how = 'over' if is_image else 'add'\n\n    w = mask.shape[0]\n    extra = w // 2\n    M, N = img.shape[:2]\n    padded_shape = (M + 2*extra, N + 2*extra)\n    float_type = img.dtype in [np.float32, np.float64]\n    fill_value = np.nan if float_type else 0\n    if cupy and isinstance(img.data, cupy.ndarray):\n        # Convert img.data to numpy array before passing to nb.jit kernels\n        img.data = cupy.asnumpy(img.data)\n\n    if is_image:\n        kernel = _build_spread_kernel(how, is_image)\n    elif float_type:\n        kernel = _build_float_kernel(how, w)\n    else:\n        kernel = _build_int_kernel(how, w, img.dtype == np.uint32)\n\n    def apply_kernel(layer):\n        buf = np.full(padded_shape, fill_value, dtype=layer.dtype)\n        kernel(layer.data, mask, buf)\n        return buf[extra:-extra, extra:-extra].copy()\n\n    if len(img.shape)==2:\n        out = apply_kernel(img)\n    else:\n        out = np.dstack([apply_kernel(img[:,:,category])\n                        for category in range(img.shape[2])])\n\n    return img.__class__(out, dims=img.dims, coords=img.coords, name=name)\n\n\n@tz.memoize\ndef _build_int_kernel(how, mask_size, ignore_zeros):\n    \"\"\"Build a spreading kernel for a given composite operator\"\"\"\n    from datashader.composite import composite_op_lookup, validate_operator\n\n    validate_operator(how, is_image=False)\n    op = composite_op_lookup[how + \"_arr\"]\n    @ngjit\n    def stencilled(arr, mask, out):\n        M, N = arr.shape\n        for y in range(M):\n            for x in range(N):\n                el = arr[y, x]\n                for i in range(mask_size):\n                    for j in range(mask_size):\n                        if mask[i, j]:\n                            if ignore_zeros and el==0:\n                                result = out[i + y, j + x]\n                            elif ignore_zeros and out[i + y, j + x]==0:\n                                result = el\n                            else:\n                                result = op(el, out[i + y, j + x])\n                            out[i + y, j + x] = result\n    return stencilled\n\n\n@tz.memoize\ndef _build_float_kernel(how, mask_size):\n    \"\"\"Build a spreading kernel for a given composite operator\"\"\"\n    from datashader.composite import composite_op_lookup, validate_operator\n\n    validate_operator(how, is_image=False)\n    op = composite_op_lookup[how + \"_arr\"]\n    @ngjit\n    def stencilled(arr, mask, out):\n        M, N = arr.shape\n        for y in range(M):\n            for x in range(N):\n                el = arr[y, x]\n                for i in range(mask_size):\n                    for j in range(mask_size):\n                        if mask[i, j]:\n                            if np.isnan(el):\n                                result = out[i + y, j + x]\n                            elif np.isnan(out[i + y, j + x]):\n                                result = el\n                            else:\n                                result = op(el, out[i + y, j + x])\n                            out[i + y, j + x] = result\n    return stencilled\n\n\n@tz.memoize\ndef _build_spread_kernel(how, is_image):\n    \"\"\"Build a spreading kernel for a given composite operator\"\"\"\n    from datashader.composite import composite_op_lookup, validate_operator\n\n    validate_operator(how, is_image=True)\n    op = composite_op_lookup[how + (\"\" if is_image else \"_arr\")]\n\n    @ngjit\n    def kernel(arr, mask, out):\n        M, N = arr.shape\n        w = mask.shape[0]\n        for y in range(M):\n            for x in range(N):\n                el = arr[y, x]\n                # Skip if data is transparent\n                process_image = is_image and ((int(el) >> 24) & 255) # Transparent pixel\n                process_array = (not is_image) and (not np.isnan(el))\n                if process_image or process_array:\n                    for i in range(w):\n                        for j in range(w):\n                            # Skip if mask is False at this value\n                            if mask[i, j]:\n                                if el==0:\n                                    result = out[i + y, j + x]\n                                if out[i + y, j + x]==0:\n                                    result = el\n                                else:\n                                    result = op(el, out[i + y, j + x])\n                                out[i + y, j + x] = result\n    return kernel\n\n\ndef _square_mask(px):\n    \"\"\"Produce a square mask with sides of length ``2 * px + 1``\"\"\"\n    px = int(px)\n    w = 2 * px + 1\n    return np.ones((w, w), dtype='bool')\n\n\ndef _circle_mask(r):\n    \"\"\"Produce a circular mask with a diameter of ``2 * r + 1``\"\"\"\n    x = np.arange(-r, r + 1, dtype='i4')\n    return np.where(np.sqrt(x**2 + x[:, None]**2) <= r+0.5, True, False)\n\n\n_mask_lookup = {'square': _square_mask,\n                'circle': _circle_mask}\n\n\ndef dynspread(img, threshold=0.5, max_px=3, shape='circle', how=None, name=None):\n    \"\"\"Spread pixels in an image dynamically based on the image density.\n\n    Spreading expands each pixel a certain number of pixels on all sides\n    according to a given shape, merging pixels using a specified compositing\n    operator. This can be useful to make sparse plots more visible. Dynamic\n    spreading determines how many pixels to spread based on a density\n    heuristic.  Spreading starts at 1 pixel, and stops when the fraction\n    of adjacent non-empty pixels reaches the specified threshold, or\n    the max_px is reached, whichever comes first.\n\n    Parameters\n    ----------\n    img : Image\n    threshold : float, optional\n        A tuning parameter in [0, 1], with higher values giving more\n        spreading.\n    max_px : int, optional\n        Maximum number of pixels to spread on all sides.\n    shape : str, optional\n        The shape to spread by. Options are 'circle' [default] or 'square'.\n    how : str, optional\n        The name of the compositing operator to use when combining\n        pixels. Default of None uses 'over' operator for Image objects\n        and 'add' operator otherwise.\n    \"\"\"\n    is_image = isinstance(img, Image)\n    if not 0 <= threshold <= 1:\n        raise ValueError(\"threshold must be in [0, 1]\")\n    if not isinstance(max_px, int) or max_px < 0:\n        raise ValueError(\"max_px must be >= 0\")\n    # Simple linear search. Not super efficient, but max_px is usually small.\n    float_type = img.dtype in [np.float32, np.float64]\n    if cupy and isinstance(img.data, cupy.ndarray):\n        # Convert img.data to numpy array before passing to nb.jit kernels\n        img.data = cupy.asnumpy(img.data)\n\n    px_=0\n    for px in range(1, max_px + 1):\n        px_=px\n        if is_image:\n            density = _rgb_density(img.data, px*2)\n        elif len(img.shape) == 2:\n            density = _array_density(img.data, float_type, px*2)\n        else:\n            masked = np.logical_not(np.isnan(img)) if float_type else (img != 0)\n            flat_mask = np.sum(masked, axis=2, dtype='uint32')\n            density = _array_density(flat_mask.data, False, px*2)\n        if density > threshold:\n            px_=px_-1\n            break\n\n    if px_>=1:\n        return spread(img, px_, shape=shape, how=how, name=name)\n    else:\n        return img\n\n\n@nb.jit(nopython=True, nogil=True, cache=True)\ndef _array_density(arr, float_type, px=1):\n    \"\"\"Compute a density heuristic of an array.\n\n    The density is a number in [0, 1], and indicates the normalized mean number\n    of non-empty pixels that have neighbors in the given px radius.\n    \"\"\"\n    M, N = arr.shape\n    cnt = has_neighbors = 0\n    for y in range(0, M):\n        for x in range(0, N):\n            el = arr[y, x]\n            if (float_type and not np.isnan(el)) or (not float_type and el!=0):\n                cnt += 1\n                neighbors = 0\n                for i in     range(max(0, y - px), min(y + px + 1, M)):\n                    for j in range(max(0, x - px), min(x + px + 1, N)):\n                        if ((float_type and not np.isnan(arr[i, j])) or\n                            (not float_type and arr[i, j] != 0)):\n                            neighbors += 1\n                if neighbors>1: # (excludes self)\n                    has_neighbors += 1\n    return has_neighbors/cnt if cnt else np.inf\n\n\n@nb.jit(nopython=True, nogil=True, cache=True)\ndef _rgb_density(arr, px=1):\n    \"\"\"Compute a density heuristic of an image.\n\n    The density is a number in [0, 1], and indicates the normalized mean number\n    of non-empty pixels that have neighbors in the given px radius.\n    \"\"\"\n    M, N = arr.shape\n    cnt = has_neighbors = 0\n    for y in range(0, M):\n        for x in range(0, N):\n            if (arr[y, x] >> 24) & 255:\n                cnt += 1\n                neighbors = 0\n                for i in     range(max(0, y - px), min(y + px + 1, M)):\n                    for j in range(max(0, x - px), min(x + px + 1, N)):\n                        if (arr[i, j] >> 24) & 255:\n                            neighbors += 1\n                if neighbors>1: # (excludes self)\n                    has_neighbors += 1\n    return has_neighbors/cnt if cnt else np.inf\n",
    "datashader/reductions.py": "from __future__ import annotations\nimport copy\nfrom enum import Enum\nfrom packaging.version import Version\nimport numpy as np\nfrom datashader.datashape import dshape, isnumeric, Record, Option\nfrom datashader.datashape import coretypes as ct\nfrom toolz import concat, unique\nimport xarray as xr\n\nfrom datashader.antialias import AntialiasCombination, AntialiasStage2\nfrom datashader.utils import isminus1, isnull\nfrom numba import cuda as nb_cuda\n\ntry:\n    from datashader.transfer_functions._cuda_utils import (\n        cuda_atomic_nanmin, cuda_atomic_nanmax, cuda_args, cuda_row_min_in_place,\n        cuda_nanmax_n_in_place_4d, cuda_nanmax_n_in_place_3d,\n        cuda_nanmin_n_in_place_4d, cuda_nanmin_n_in_place_3d,\n        cuda_row_max_n_in_place_4d, cuda_row_max_n_in_place_3d,\n        cuda_row_min_n_in_place_4d, cuda_row_min_n_in_place_3d, cuda_shift_and_insert,\n    )\nexcept ImportError:\n    (cuda_atomic_nanmin, cuda_atomic_nanmax, cuda_args, cuda_row_min_in_place,\n        cuda_nanmax_n_in_place_4d, cuda_nanmax_n_in_place_3d,\n        cuda_nanmin_n_in_place_4d, cuda_nanmin_n_in_place_3d,\n        cuda_row_max_n_in_place_4d, cuda_row_max_n_in_place_3d,\n        cuda_row_min_n_in_place_4d, cuda_row_min_n_in_place_3d, cuda_shift_and_insert,\n    ) = None, None, None, None, None, None, None, None, None, None, None, None, None\n\ntry:\n    import cudf\n    import cupy as cp\nexcept Exception:\n    cudf = cp = None\n\nfrom .utils import (\n    Expr, ngjit, nansum_missing, nanmax_in_place, nansum_in_place, row_min_in_place,\n    nanmax_n_in_place_4d, nanmax_n_in_place_3d, nanmin_n_in_place_4d, nanmin_n_in_place_3d,\n    row_max_n_in_place_4d, row_max_n_in_place_3d, row_min_n_in_place_4d, row_min_n_in_place_3d,\n    shift_and_insert,\n)\n\n\nclass SpecialColumn(Enum):\n    \"\"\"\n    Internally datashader identifies the columns required by the user's\n    Reductions and extracts them from the supplied source (e.g. DataFrame) to\n    pass through the dynamically-generated append function in compiler.py and\n    end up as arguments to the Reduction._append* functions. Each column is\n    a string name or a SpecialColumn. A column of None is used in Reduction\n    classes to denote that no column is required.\n    \"\"\"\n    RowIndex = 1\n\n\nclass UsesCudaMutex(Enum):\n    \"\"\"\n    Enum that encapsulates the need for a Reduction to use a CUDA mutex to\n    operate correctly on a GPU. Possible values:\n\n    No: the Reduction append_cuda function is atomic and no mutex is required.\n    Local: Reduction append_cuda needs wrapping in a mutex.\n    Global: the overall compiled append function needs wrapping in a mutex.\n    \"\"\"\n    No = 0\n    Local = 1\n    Global = 2\n\n\nclass Preprocess(Expr):\n    \"\"\"Base clase for preprocessing steps.\"\"\"\n    def __init__(self, column: str | SpecialColumn | None):\n        self.column = column\n\n    @property\n    def inputs(self):\n        return (self.column,)\n\n    @property\n    def nan_check_column(self):\n        return None\n\n\nclass extract(Preprocess):\n    \"\"\"Extract a column from a dataframe as a numpy array of values.\"\"\"\n    def apply(self, df, cuda):\n        if self.column is SpecialColumn.RowIndex:\n            attr_name = \"_datashader_row_offset\"\n            if isinstance(df, xr.Dataset):\n                row_offset = df.attrs[attr_name]\n                row_length = df.attrs[\"_datashader_row_length\"]\n            else:\n                attrs = getattr(df, \"attrs\", None)\n                row_offset = getattr(attrs or df, attr_name, 0)\n                row_length = len(df)\n\n        if cudf and isinstance(df, cudf.DataFrame):\n            if self.column is SpecialColumn.RowIndex:\n                return cp.arange(row_offset, row_offset + row_length, dtype=np.int64)\n\n            if df[self.column].dtype.kind == 'f':\n                nullval = np.nan\n            else:\n                nullval = 0\n            if Version(cudf.__version__) >= Version(\"22.02\"):\n                return df[self.column].to_cupy(na_value=nullval)\n            return cp.array(df[self.column].to_gpu_array(fillna=nullval))\n        elif self.column is SpecialColumn.RowIndex:\n            if cuda:\n                return cp.arange(row_offset, row_offset + row_length, dtype=np.int64)\n            else:\n                return np.arange(row_offset, row_offset + row_length, dtype=np.int64)\n        elif isinstance(df, xr.Dataset):\n            if cuda and not isinstance(df[self.column].data, cp.ndarray):\n                return cp.asarray(df[self.column])\n            else:\n                return df[self.column].data\n        else:\n            return df[self.column].values\n\n\nclass CategoryPreprocess(Preprocess):\n    \"\"\"Base class for categorizing preprocessors.\"\"\"\n    @property\n    def cat_column(self):\n        \"\"\"Returns name of categorized column\"\"\"\n        return self.column\n\n    def categories(self, input_dshape):\n        \"\"\"Returns list of categories corresponding to input shape\"\"\"\n        raise NotImplementedError(\"categories not implemented\")\n\n    def validate(self, in_dshape):\n        \"\"\"Validates input shape\"\"\"\n        raise NotImplementedError(\"validate not implemented\")\n\n    def apply(self, df, cuda):\n        \"\"\"Applies preprocessor to DataFrame and returns array\"\"\"\n        raise NotImplementedError(\"apply not implemented\")\n\n\nclass category_codes(CategoryPreprocess):\n    \"\"\"\n    Extract just the category codes from a categorical column.\n\n    To create a new type of categorizer, derive a subclass from this\n    class or one of its subclasses, implementing ``__init__``,\n    ``_hashable_inputs``, ``categories``, ``validate``, and ``apply``.\n\n    See the implementation of ``category_modulo`` in ``reductions.py``\n    for an example.\n    \"\"\"\n    def categories(self, input_dshape):\n        return input_dshape.measure[self.column].categories\n\n    def validate(self, in_dshape):\n        if self.column not in in_dshape.dict:\n            raise ValueError(\"specified column not found\")\n        if not isinstance(in_dshape.measure[self.column], ct.Categorical):\n            raise ValueError(\"input must be categorical\")\n\n    def apply(self, df, cuda):\n        if cudf and isinstance(df, cudf.DataFrame):\n            if Version(cudf.__version__) >= Version(\"22.02\"):\n                return df[self.column].cat.codes.to_cupy()\n            return df[self.column].cat.codes.to_gpu_array()\n        else:\n            return df[self.column].cat.codes.values\n\nclass category_modulo(category_codes):\n    \"\"\"\n    A variation on category_codes that assigns categories using an integer column, modulo a base.\n    Category is computed as (column_value - offset)%modulo.\n    \"\"\"\n\n    # couldn't find anything in the datashape docs about how to check if a CType is an integer, so\n    # just define a big set\n    IntegerTypes = {ct.bool_, ct.uint8, ct.uint16, ct.uint32, ct.uint64, ct.int8, ct.int16,\n                    ct.int32, ct.int64}\n\n    def __init__(self, column, modulo, offset=0):\n        super().__init__(column)\n        self.offset = offset\n        self.modulo = modulo\n\n    def _hashable_inputs(self):\n        return super()._hashable_inputs() + (self.offset, self.modulo)\n\n    def categories(self, in_dshape):\n        return list(range(self.modulo))\n\n    def validate(self, in_dshape):\n        if self.column not in in_dshape.dict:\n            raise ValueError(\"specified column not found\")\n        if in_dshape.measure[self.column] not in self.IntegerTypes:\n            raise ValueError(\"input must be an integer column\")\n\n    def apply(self, df, cuda):\n        result = (df[self.column] - self.offset) % self.modulo\n        if cudf and isinstance(df, cudf.Series):\n            if Version(cudf.__version__) >= Version(\"22.02\"):\n                return result.to_cupy()\n            return result.to_gpu_array()\n        else:\n            return result.values\n\nclass category_binning(category_modulo):\n    \"\"\"\n    A variation on category_codes that assigns categories by binning a continuous-valued column.\n    The number of categories returned is always nbins+1.\n    The last category (nbin) is for NaNs in the data column, as well as for values under/over the\n    binned interval (when include_under or include_over is False).\n\n    Parameters\n    ----------\n    column:   column to use\n    lower:    lower bound of first bin\n    upper:    upper bound of last bin\n    nbins:     number of bins\n    include_under: if True, values below bin 0 are assigned to category 0\n    include_over:  if True, values above the last bin (nbins-1) are assigned to category nbin-1\n    \"\"\"\n\n    def __init__(self, column, lower, upper, nbins, include_under=True, include_over=True):\n        super().__init__(column, nbins + 1)  # +1 category for NaNs and clipped values\n        self.bin0 = lower\n        self.binsize = (upper - lower) / float(nbins)\n        self.nbins = nbins\n        self.bin_under = 0 if include_under else nbins\n        self.bin_over  = nbins-1 if include_over else nbins\n\n    def _hashable_inputs(self):\n        return super()._hashable_inputs() + (self.bin0, self.binsize, self.bin_under, self.bin_over)\n\n    def validate(self, in_dshape):\n        if self.column not in in_dshape.dict:\n            raise ValueError(\"specified column not found\")\n\n    def apply(self, df, cuda):\n        if cudf and isinstance(df, cudf.DataFrame):\n            if Version(cudf.__version__) >= Version(\"22.02\"):\n                values = df[self.column].to_cupy(na_value=cp.nan)\n            else:\n                values = cp.array(df[self.column].to_gpu_array(fillna=True))\n            nan_values = cp.isnan(values)\n        else:\n            values = df[self.column].to_numpy()\n            nan_values = np.isnan(values)\n\n        index_float = (values - self.bin0) / self.binsize\n        # NaN values are corrected below, so set them to zero to avoid warnings when\n        # converting from float to int.\n        index_float[nan_values] = 0\n        index = index_float.astype(int)\n        index[index < 0] = self.bin_under\n        index[index >= self.nbins] = self.bin_over\n        index[nan_values] = self.nbins\n        return index\n\n\nclass category_values(CategoryPreprocess):\n    \"\"\"Extract a category and a value column from a dataframe as (2,N) numpy array of values.\"\"\"\n    def __init__(self, categorizer, value_column):\n        super().__init__(value_column)\n        self.categorizer = categorizer\n\n    @property\n    def inputs(self):\n        return (self.categorizer.column, self.column)\n\n    @property\n    def cat_column(self):\n        \"\"\"Returns name of categorized column\"\"\"\n        return self.categorizer.column\n\n    def categories(self, input_dshape):\n        return self.categorizer.categories\n\n    def validate(self, in_dshape):\n        return self.categorizer.validate(in_dshape)\n\n    def apply(self, df, cuda):\n        a = self.categorizer.apply(df, cuda)\n        if cudf and isinstance(df, cudf.DataFrame):\n            import cupy\n            if self.column == SpecialColumn.RowIndex:\n                nullval = -1\n            elif df[self.column].dtype.kind == 'f':\n                nullval = np.nan\n            else:\n                nullval = 0\n            a = cupy.asarray(a)\n            if self.column == SpecialColumn.RowIndex:\n                b = extract(SpecialColumn.RowIndex).apply(df, cuda)\n            elif Version(cudf.__version__) >= Version(\"22.02\"):\n                b = df[self.column].to_cupy(na_value=nullval)\n            else:\n                b = cupy.asarray(df[self.column].fillna(nullval))\n            return cupy.stack((a, b), axis=-1)\n        else:\n            if self.column == SpecialColumn.RowIndex:\n                b = extract(SpecialColumn.RowIndex).apply(df, cuda)\n            else:\n                b = df[self.column].values\n            return np.stack((a, b), axis=-1)\n\n\nclass Reduction(Expr):\n    \"\"\"Base class for per-bin reductions.\"\"\"\n    def __init__(self, column: str | SpecialColumn | None=None):\n        self.column = column\n        self._nan_check_column = None\n\n    @property\n    def nan_check_column(self):\n        if self._nan_check_column is not None:\n            return extract(self._nan_check_column)\n        else:\n            return None\n\n    def uses_cuda_mutex(self) -> UsesCudaMutex:\n        \"\"\"Return ``True`` if this Reduction needs to use a CUDA mutex to\n        ensure that it is threadsafe across CUDA threads.\n\n        If the CUDA append functions are all atomic (i.e. using functions from\n        the numba.cuda.atomic module) then this is ``False``, otherwise it is\n        ``True``.\n        \"\"\"\n        return UsesCudaMutex.No\n\n    def uses_row_index(self, cuda, partitioned):\n        \"\"\"Return ``True`` if this Reduction uses a row index virtual column.\n\n        For some reductions the order of the rows of supplied data is\n        important. These include ``first`` and ``last`` reductions as well as\n        ``where`` reductions that return a row index. In some situations the\n        order is intrinsic such as ``first`` reductions that are processed\n        sequentially (i.e. on a CPU without using Dask) and no extra column is\n        required. But in situations of parallel processing (using a GPU or\n        Dask) extra information is needed that is provided by a row index\n        virtual column.\n\n        Returning ``True`` from this function will cause a row index column to\n        be created and passed to the ``append`` functions in the usual manner.\n        \"\"\"\n        return False\n\n    def validate(self, in_dshape):\n        if self.column == SpecialColumn.RowIndex:\n            return\n        if self.column not in in_dshape.dict:\n            raise ValueError(\"specified column not found\")\n        if not isnumeric(in_dshape.measure[self.column]):\n            raise ValueError(\"input must be numeric\")\n\n    @property\n    def inputs(self):\n        return (extract(self.column),)\n\n    def is_categorical(self):\n        \"\"\"Return ``True`` if this is or contains a categorical reduction.\"\"\"\n        return False\n\n    def is_where(self):\n        \"\"\"Return ``True`` if this is a ``where`` reduction or directly wraps\n        a where reduction.\"\"\"\n        return False\n\n    def _antialias_requires_2_stages(self):\n        # Return True if this Reduction must be processed with 2 stages,\n        # False if it doesn't matter.\n        # Overridden in derived classes as appropriate.\n        return False\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        # Only called if using antialiased lines. Overridden in derived classes.\n        # Returns a tuple containing an item for each constituent reduction.\n        # Each item is (AntialiasCombination, zero_value)).\n        raise NotImplementedError(f\"{type(self)}._antialias_stage_2 is not defined\")\n\n    def _build_bases(self, cuda, partitioned):\n        return (self,)\n\n    def _build_combine_temps(self, cuda, partitioned):\n        # Temporaries (i.e. not returned to user) that are reductions, the\n        # aggs of which are passed to the combine() function but not the\n        # append() functions, as opposed to _build_temps() which are passed\n        # to both append() and combine().\n        return ()\n\n    def _build_temps(self, cuda=False):\n        # Temporaries (i.e. not returned to user) that are reductions, the\n        # aggs of which are passed to both append() and combine() functions.\n        return ()\n\n    def _build_create(self, required_dshape):\n        fields = getattr(required_dshape.measure, \"fields\", None)\n        if fields is not None and len(required_dshape.measure.fields) > 0:\n            # If more than one field then they all have the same dtype so can just take the first.\n            first_field = required_dshape.measure.fields[0]\n            required_dshape = dshape(first_field[1])\n\n        if isinstance(required_dshape, Option):\n            required_dshape = dshape(required_dshape.ty)\n\n        if required_dshape == dshape(ct.bool_):\n            return self._create_bool\n        elif required_dshape == dshape(ct.float32):\n            return self._create_float32_nan\n        elif required_dshape == dshape(ct.float64):\n            return self._create_float64_nan\n        elif required_dshape == dshape(ct.int64):\n            return self._create_int64\n        elif required_dshape == dshape(ct.uint32):\n            return self._create_uint32\n        else:\n            raise NotImplementedError(f\"Unexpected dshape {dshape}\")\n\n    def _build_append(self, dshape, schema, cuda, antialias, self_intersect):\n        if cuda:\n            if antialias and self.column is None:\n                return self._append_no_field_antialias_cuda\n            elif antialias:\n                return self._append_antialias_cuda\n            elif self.column is None:\n                return self._append_no_field_cuda\n            else:\n                return self._append_cuda\n        else:\n            if antialias and self.column is None:\n                return self._append_no_field_antialias\n            elif antialias:\n                return self._append_antialias\n            elif self.column is None:\n                return self._append_no_field\n            else:\n                return self._append\n\n    def _build_combine(self, dshape, antialias, cuda, partitioned, categorical = False):\n        return self._combine\n\n    def _build_finalize(self, dshape):\n        return self._finalize\n\n    @staticmethod\n    def _create_bool(shape, array_module):\n        return array_module.zeros(shape, dtype='bool')\n\n    @staticmethod\n    def _create_float32_nan(shape, array_module):\n        return array_module.full(shape, array_module.nan, dtype='f4')\n\n    @staticmethod\n    def _create_float64_nan(shape, array_module):\n        return array_module.full(shape, array_module.nan, dtype='f8')\n\n    @staticmethod\n    def _create_float64_empty(shape, array_module):\n        return array_module.empty(shape, dtype='f8')\n\n    @staticmethod\n    def _create_float64_zero(shape, array_module):\n        return array_module.zeros(shape, dtype='f8')\n\n    @staticmethod\n    def _create_int64(shape, array_module):\n        return array_module.full(shape, -1, dtype='i8')\n\n    @staticmethod\n    def _create_uint32(shape, array_module):\n        return array_module.zeros(shape, dtype='u4')\n\n\nclass OptionalFieldReduction(Reduction):\n    \"\"\"Base class for things like ``count`` or ``any`` for which the field is optional\"\"\"\n    def __init__(self, column=None):\n        super().__init__(column)\n\n    @property\n    def inputs(self):\n        return (extract(self.column),) if self.column is not None else ()\n\n    def validate(self, in_dshape):\n        if self.column is not None:\n            super().validate(in_dshape)\n\n    @staticmethod\n    def _finalize(bases, cuda=False, **kwargs):\n        return xr.DataArray(bases[0], **kwargs)\n\n\nclass SelfIntersectingOptionalFieldReduction(OptionalFieldReduction):\n    \"\"\"\n    Base class for optional field reductions for which self-intersecting\n    geometry may or may not be desirable.\n    Ignored if not using antialiasing.\n    \"\"\"\n    def __init__(self, column=None, self_intersect=True):\n        super().__init__(column)\n        self.self_intersect = self_intersect\n\n    def _antialias_requires_2_stages(self):\n        return not self.self_intersect\n\n    def _build_append(self, dshape, schema, cuda, antialias, self_intersect):\n        if antialias and not self_intersect:\n            # append functions specific to antialiased lines without self_intersect\n            if cuda:\n                if self.column is None:\n                    return self._append_no_field_antialias_cuda_not_self_intersect\n                else:\n                    return self._append_antialias_cuda_not_self_intersect\n            else:\n                if self.column is None:\n                    return self._append_no_field_antialias_not_self_intersect\n                else:\n                    return self._append_antialias_not_self_intersect\n\n        # Fall back to base class implementation\n        return super()._build_append(dshape, schema, cuda, antialias, self_intersect)\n\n    def _hashable_inputs(self):\n        # Reductions with different self_intersect attributes much have different hashes otherwise\n        # toolz.memoize will treat them as the same to give incorrect results.\n        return super()._hashable_inputs() + (self.self_intersect,)\n\n\nclass count(SelfIntersectingOptionalFieldReduction):\n    \"\"\"Count elements in each bin, returning the result as a uint32, or a\n    float32 if using antialiasing.\n\n    Parameters\n    ----------\n    column : str, optional\n        If provided, only counts elements in ``column`` that are not ``NaN``.\n        Otherwise, counts every element.\n    \"\"\"\n    def out_dshape(self, in_dshape, antialias, cuda, partitioned):\n        return dshape(ct.float32) if antialias else dshape(ct.uint32)\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        if self_intersect:\n            return (AntialiasStage2(AntialiasCombination.SUM_1AGG, array_module.nan),)\n        else:\n            return (AntialiasStage2(AntialiasCombination.SUM_2AGG, array_module.nan),)\n\n    # CPU append functions\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if not isnull(field):\n            agg[y, x] += 1\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        if not isnull(field):\n            if isnull(agg[y, x]):\n                agg[y, x] = aa_factor - prev_aa_factor\n            else:\n                agg[y, x] += aa_factor - prev_aa_factor\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias_not_self_intersect(x, y, agg, field, aa_factor, prev_aa_factor):\n        if not isnull(field):\n            if isnull(agg[y, x]) or aa_factor > agg[y, x]:\n                agg[y, x] = aa_factor\n                return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_no_field(x, y, agg):\n        agg[y, x] += 1\n        return 0\n\n    @staticmethod\n    @ngjit\n    def _append_no_field_antialias(x, y, agg, aa_factor, prev_aa_factor):\n        if isnull(agg[y, x]):\n            agg[y, x] = aa_factor - prev_aa_factor\n        else:\n            agg[y, x] += aa_factor - prev_aa_factor\n        return 0\n\n    @staticmethod\n    @ngjit\n    def _append_no_field_antialias_not_self_intersect(x, y, agg, aa_factor, prev_aa_factor):\n        if isnull(agg[y, x]) or aa_factor > agg[y, x]:\n            agg[y, x] = aa_factor\n            return 0\n        return -1\n\n    # GPU append functions\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_antialias_cuda(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field*aa_factor\n        if not isnull(value):\n            old = cuda_atomic_nanmax(agg, (y, x), value)\n            if isnull(old) or old < value:\n                return 0\n        return -1\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_no_field_antialias_cuda_not_self_intersect(x, y, agg, aa_factor, prev_aa_factor):\n        if not isnull(aa_factor):\n            old = cuda_atomic_nanmax(agg, (y, x), aa_factor)\n            if isnull(old) or old < aa_factor:\n                return 0\n        return -1\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_cuda(x, y, agg, field):\n        if not isnull(field):\n            nb_cuda.atomic.add(agg, (y, x), 1)\n            return 0\n        return -1\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_no_field_antialias_cuda(x, y, agg, aa_factor, prev_aa_factor):\n        if not isnull(aa_factor):\n            old = cuda_atomic_nanmax(agg, (y, x), aa_factor)\n            if isnull(old) or old < aa_factor:\n                return 0\n        return -1\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_no_field_cuda(x, y, agg):\n        nb_cuda.atomic.add(agg, (y, x), 1)\n        return 0\n\n    def _build_combine(self, dshape, antialias, cuda, partitioned, categorical = False):\n        if antialias:\n            return self._combine_antialias\n        else:\n            return self._combine\n\n    @staticmethod\n    def _combine(aggs):\n        return aggs.sum(axis=0, dtype='u4')\n\n    @staticmethod\n    def _combine_antialias(aggs):\n        ret = aggs[0]\n        for i in range(1, len(aggs)):\n            nansum_in_place(ret, aggs[i])\n        return ret\n\n\nclass _count_ignore_antialiasing(count):\n    \"\"\"Count reduction but ignores antialiasing. Used by mean reduction.\n    \"\"\"\n    def out_dshape(self, in_dshape, antialias, cuda, partitioned):\n        return dshape(ct.uint32)\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        if self_intersect:\n            return (AntialiasStage2(AntialiasCombination.SUM_1AGG, 0),)\n        else:\n            return (AntialiasStage2(AntialiasCombination.SUM_2AGG, 0),)\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        if not isnull(field) and prev_aa_factor == 0.0:\n            agg[y, x] += 1\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias_not_self_intersect(x, y, agg, field, aa_factor, prev_aa_factor):\n        if not isnull(field) and prev_aa_factor == 0.0:\n            agg[y, x] += 1\n            return 0\n        return -1\n\n\nclass by(Reduction):\n    \"\"\"Apply the provided reduction separately per category.\n\n    Parameters\n    ----------\n    cats: str or CategoryPreprocess instance\n        Name of column to aggregate over, or a categorizer object that returns categories.\n        Resulting aggregate has an outer dimension axis along the categories present.\n    reduction : Reduction\n        Per-category reduction function.\n    \"\"\"\n    def __init__(self, cat_column, reduction=count()):\n        super().__init__()\n\n        # set basic categorizer\n        if isinstance(cat_column, CategoryPreprocess):\n            self.categorizer = cat_column\n        elif isinstance(cat_column, str):\n            self.categorizer = category_codes(cat_column)\n        else:\n            raise TypeError(\"first argument must be a column name or a CategoryPreprocess instance\")\n\n        self.column = self.categorizer.column # for backwards compatibility with count_cat\n\n        self.columns = (self.categorizer.column,)\n        if (columns := getattr(reduction, 'columns', None)) is not None:\n            # Must reverse columns (from where reduction) so that val_column property\n            # is the column that is returned to the user.\n            self.columns += columns[::-1]\n        else:\n            self.columns += (getattr(reduction, 'column', None),)\n\n        self.reduction = reduction\n        # if a value column is supplied, set category_values preprocessor\n        if self.val_column is not None:\n            self.preprocess = category_values(self.categorizer, self.val_column)\n        else:\n            self.preprocess = self.categorizer\n\n    def __hash__(self):\n        return hash((type(self), self._hashable_inputs(), self.categorizer._hashable_inputs(),\n                     self.reduction))\n\n    def _build_temps(self, cuda=False):\n        return tuple(by(self.categorizer, tmp) for tmp in self.reduction._build_temps(cuda))\n\n    @property\n    def cat_column(self):\n        return self.columns[0]\n\n    @property\n    def val_column(self):\n        return self.columns[1]\n\n    def validate(self, in_dshape):\n        self.preprocess.validate(in_dshape)\n        self.reduction.validate(in_dshape)\n\n    def out_dshape(self, input_dshape, antialias, cuda, partitioned):\n        cats = self.categorizer.categories(input_dshape)\n        red_shape = self.reduction.out_dshape(input_dshape, antialias, cuda, partitioned)\n        return dshape(Record([(c, red_shape) for c in cats]))\n\n    @property\n    def inputs(self):\n        return (self.preprocess,)\n\n    def is_categorical(self):\n        return True\n\n    def is_where(self):\n        return self.reduction.is_where()\n\n    @property\n    def nan_check_column(self):\n        return self.reduction.nan_check_column\n\n    def uses_cuda_mutex(self) -> UsesCudaMutex:\n        return self.reduction.uses_cuda_mutex()\n\n    def uses_row_index(self, cuda, partitioned):\n        return self.reduction.uses_row_index(cuda, partitioned)\n\n    def _antialias_requires_2_stages(self):\n        return self.reduction._antialias_requires_2_stages()\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        ret = self.reduction._antialias_stage_2(self_intersect, array_module)\n        return (AntialiasStage2(combination=ret[0].combination,\n                                zero=ret[0].zero,\n                                n_reduction=ret[0].n_reduction,\n                                categorical=True),)\n\n    def _build_create(self, required_dshape):\n        n_cats = len(required_dshape.measure.fields)\n        return lambda shape, array_module: self.reduction._build_create(\n            required_dshape)(shape + (n_cats,), array_module)\n\n    def _build_bases(self, cuda, partitioned):\n        bases = self.reduction._build_bases(cuda, partitioned)\n        if len(bases) == 1 and bases[0] is self:\n            return bases\n        return tuple(by(self.categorizer, base) for base in bases)\n\n    def _build_append(self, dshape, schema, cuda, antialias, self_intersect):\n        return self.reduction._build_append(dshape, schema, cuda, antialias, self_intersect)\n\n    def _build_combine(self, dshape, antialias, cuda, partitioned, categorical = False):\n        return self.reduction._build_combine(dshape, antialias, cuda, partitioned, True)\n\n    def _build_combine_temps(self, cuda, partitioned):\n        return self.reduction._build_combine_temps(cuda, partitioned)\n\n    def _build_finalize(self, dshape):\n        cats = list(self.categorizer.categories(dshape))\n\n        def finalize(bases, cuda=False, **kwargs):\n            # Return a modified copy of kwargs. Cannot modify supplied kwargs as it\n            # may be used by multiple reductions, e.g. if a summary reduction.\n            kwargs = copy.deepcopy(kwargs)\n            kwargs['dims'] += [self.cat_column]\n            kwargs['coords'][self.cat_column] = cats\n            return self.reduction._build_finalize(dshape)(bases, cuda=cuda, **kwargs)\n\n        return finalize\n\nclass any(OptionalFieldReduction):\n    \"\"\"Whether any elements in ``column`` map to each bin.\n\n    Parameters\n    ----------\n    column : str, optional\n        If provided, any elements in ``column`` that are ``NaN`` are skipped.\n    \"\"\"\n    def out_dshape(self, in_dshape, antialias, cuda, partitioned):\n        return dshape(ct.float32) if antialias else dshape(ct.bool_)\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        return (AntialiasStage2(AntialiasCombination.MAX, array_module.nan),)\n\n    # CPU append functions\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if not isnull(field):\n            agg[y, x] = True\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        if not isnull(field):\n            if isnull(agg[y, x]) or aa_factor > agg[y, x]:\n                agg[y, x] = aa_factor\n                return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_no_field(x, y, agg):\n        agg[y, x] = True\n        return 0\n\n    @staticmethod\n    @ngjit\n    def _append_no_field_antialias(x, y, agg, aa_factor, prev_aa_factor):\n        if isnull(agg[y, x]) or aa_factor > agg[y, x]:\n            agg[y, x] = aa_factor\n            return 0\n        return -1\n\n    # GPU append functions\n    _append_cuda =_append\n    _append_no_field_cuda = _append_no_field\n\n    def _build_combine(self, dshape, antialias, cuda, partitioned, categorical = False):\n        if antialias:\n            return self._combine_antialias\n        else:\n            return self._combine\n\n    @staticmethod\n    def _combine(aggs):\n        return aggs.sum(axis=0, dtype='bool')\n\n    @staticmethod\n    def _combine_antialias(aggs):\n        ret = aggs[0]\n        for i in range(1, len(aggs)):\n            nanmax_in_place(ret, aggs[i])\n        return ret\n\n\nclass _upsample(Reduction):\n    \"\"\"\"Special internal class used for upsampling\"\"\"\n    def out_dshape(self, in_dshape, antialias, cuda, partitioned):\n        return dshape(Option(ct.float64))\n\n    @staticmethod\n    def _finalize(bases, cuda=False, **kwargs):\n        return xr.DataArray(bases[0], **kwargs)\n\n    @property\n    def inputs(self):\n        return (extract(self.column),)\n\n    def _build_create(self, required_dshape):\n        # Use uninitialized memory, the upsample function must explicitly set unused\n        # values to nan\n        return self._create_float64_empty\n\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        # not called, the upsample function must set agg directly\n        pass\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_cuda(x, y, agg, field):\n        # not called, the upsample function must set agg directly\n        pass\n\n    @staticmethod\n    def _combine(aggs):\n        return np.nanmax(aggs, axis=0)\n\n\nclass FloatingReduction(Reduction):\n    \"\"\"Base classes for reductions that always have floating-point dtype.\"\"\"\n    def out_dshape(self, in_dshape, antialias, cuda, partitioned):\n        return dshape(Option(ct.float64))\n\n    @staticmethod\n    def _finalize(bases, cuda=False, **kwargs):\n        return xr.DataArray(bases[0], **kwargs)\n\n\nclass _sum_zero(FloatingReduction):\n    \"\"\"Sum of all elements in ``column``.\n\n    Parameters\n    ----------\n    column : str\n        Name of the column to aggregate over. Column data type must be numeric.\n    \"\"\"\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        if self_intersect:\n            return (AntialiasStage2(AntialiasCombination.SUM_1AGG, 0),)\n        else:\n            return (AntialiasStage2(AntialiasCombination.SUM_2AGG, 0),)\n\n    def _build_create(self, required_dshape):\n        return self._create_float64_zero\n\n    # CPU append functions.\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if not isnull(field):\n            # agg[y, x] cannot be null as initialised to zero.\n            agg[y, x] += field\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field*(aa_factor - prev_aa_factor)\n        if not isnull(value):\n            # agg[y, x] cannot be null as initialised to zero.\n            agg[y, x] += value\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias_not_self_intersect(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field*aa_factor\n        if not isnull(value) and value > agg[y, x]:\n            # agg[y, x] cannot be null as initialised to zero.\n            agg[y, x] = value\n            return 0\n        return -1\n\n    # GPU append functions\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_cuda(x, y, agg, field):\n        if not isnull(field):\n            nb_cuda.atomic.add(agg, (y, x), field)\n            return 0\n        return -1\n\n    @staticmethod\n    def _combine(aggs):\n        return aggs.sum(axis=0, dtype='f8')\n\n\nclass SelfIntersectingFloatingReduction(FloatingReduction):\n    \"\"\"\n    Base class for floating reductions for which self-intersecting geometry\n    may or may not be desirable.\n    Ignored if not using antialiasing.\n    \"\"\"\n    def __init__(self, column=None, self_intersect=True):\n        super().__init__(column)\n        self.self_intersect = self_intersect\n\n    def _antialias_requires_2_stages(self):\n        return not self.self_intersect\n\n    def _build_append(self, dshape, schema, cuda, antialias, self_intersect):\n        if antialias and not self_intersect:\n            if cuda:\n                raise NotImplementedError(\"SelfIntersectingOptionalFieldReduction\")\n            else:\n                if self.column is None:\n                    return self._append_no_field_antialias_not_self_intersect\n                else:\n                    return self._append_antialias_not_self_intersect\n\n        return super()._build_append(dshape, schema, cuda, antialias, self_intersect)\n\n    def _hashable_inputs(self):\n        # Reductions with different self_intersect attributes much have different hashes otherwise\n        # toolz.memoize will treat them as the same to give incorrect results.\n        return super()._hashable_inputs() + (self.self_intersect,)\n\n\nclass sum(SelfIntersectingFloatingReduction):\n    \"\"\"Sum of all elements in ``column``.\n\n    Elements of resulting aggregate are nan if they are not updated.\n\n    Parameters\n    ----------\n    column : str\n        Name of the column to aggregate over. Column data type must be numeric.\n        ``NaN`` values in the column are skipped.\n    \"\"\"\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        if self_intersect:\n            return (AntialiasStage2(AntialiasCombination.SUM_1AGG, array_module.nan),)\n        else:\n            return (AntialiasStage2(AntialiasCombination.SUM_2AGG, array_module.nan),)\n\n    def _build_bases(self, cuda, partitioned):\n        if cuda:\n            return (_sum_zero(self.column), any(self.column))\n        else:\n            return (self,)\n\n    # CPU append functions\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if not isnull(field):\n            if isnull(agg[y, x]):\n                agg[y, x] = field\n            else:\n                agg[y, x] += field\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field*(aa_factor - prev_aa_factor)\n        if not isnull(value):\n            if isnull(agg[y, x]):\n                agg[y, x] = value\n            else:\n                agg[y, x] += value\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias_not_self_intersect(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field*aa_factor\n        if not isnull(value):\n            if isnull(agg[y, x]) or value > agg[y, x]:\n                agg[y, x] = value\n                return 0\n        return -1\n\n    @staticmethod\n    def _combine(aggs):\n        return nansum_missing(aggs, axis=0)\n\n    @staticmethod\n    def _finalize(bases, cuda=False, **kwargs):\n        if cuda:\n            sums, anys = bases\n            x = np.where(anys, sums, np.nan)\n            return xr.DataArray(x, **kwargs)\n        else:\n            return xr.DataArray(bases[0], **kwargs)\n\n\nclass m2(FloatingReduction):\n    \"\"\"Sum of square differences from the mean of all elements in ``column``.\n\n    Intermediate value for computing ``var`` and ``std``, not intended to be\n    used on its own.\n\n    Parameters\n    ----------\n    column : str\n        Name of the column to aggregate over. Column data type must be numeric.\n        ``NaN`` values in the column are skipped.\n    \"\"\"\n    def uses_cuda_mutex(self) -> UsesCudaMutex:\n        return UsesCudaMutex.Global\n\n    def _build_append(self, dshape, schema, cuda, antialias, self_intersect):\n        return super()._build_append(dshape, schema, cuda, antialias, self_intersect)\n\n    def _build_create(self, required_dshape):\n        return self._create_float64_zero\n\n    def _build_temps(self, cuda=False):\n        return (_sum_zero(self.column), count(self.column))\n\n    # CPU append functions\n    @staticmethod\n    @ngjit\n    def _append(x, y, m2, field, sum, count):\n        # sum & count are the results of sum[y, x], count[y, x] before being\n        # updated by field\n        if not isnull(field):\n            if count > 0:\n                u1 = np.float64(sum) / count\n                u = np.float64(sum + field) / (count + 1)\n                m2[y, x] += (field - u1) * (field - u)\n                return 0\n        return -1\n\n    # GPU append functions\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_cuda(x, y, m2, field, sum, count):\n        # sum & count are the results of sum[y, x], count[y, x] before being\n        # updated by field\n        if not isnull(field):\n            if count > 0:\n                u1 = np.float64(sum) / count\n                u = np.float64(sum + field) / (count + 1)\n                m2[y, x] += (field - u1) * (field - u)\n                return 0\n        return -1\n\n    @staticmethod\n    def _combine(Ms, sums, ns):\n        with np.errstate(divide='ignore', invalid='ignore'):\n            mu = np.nansum(sums, axis=0) / ns.sum(axis=0)\n            return np.nansum(Ms + ns*(sums/ns - mu)**2, axis=0)\n\n\nclass min(FloatingReduction):\n    \"\"\"Minimum value of all elements in ``column``.\n\n    Parameters\n    ----------\n    column : str\n        Name of the column to aggregate over. Column data type must be numeric.\n        ``NaN`` values in the column are skipped.\n    \"\"\"\n    def _antialias_requires_2_stages(self):\n        return True\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        return (AntialiasStage2(AntialiasCombination.MIN, array_module.nan),)\n\n    # CPU append functions\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if not isnull(field) and (isnull(agg[y, x]) or agg[y, x] > field):\n            agg[y, x] = field\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field*aa_factor\n        if not isnull(value) and (isnull(agg[y, x]) or value > agg[y, x]):\n            agg[y, x] = value\n            return 0\n        return -1\n\n    # GPU append functions\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_cuda(x, y, agg, field):\n        if not isnull(field):\n            old = cuda_atomic_nanmin(agg, (y, x), field)\n            if isnull(old) or old > field:\n                return 0\n        return -1\n\n    @staticmethod\n    def _combine(aggs):\n        return np.nanmin(aggs, axis=0)\n\n\nclass max(FloatingReduction):\n    \"\"\"Maximum value of all elements in ``column``.\n\n    Parameters\n    ----------\n    column : str\n        Name of the column to aggregate over. Column data type must be numeric.\n        ``NaN`` values in the column are skipped.\n    \"\"\"\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        return (AntialiasStage2(AntialiasCombination.MAX, array_module.nan),)\n\n    # CPU append functions\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if not isnull(field) and (isnull(agg[y, x]) or agg[y, x] < field):\n            agg[y, x] = field\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field*aa_factor\n        if not isnull(value) and (isnull(agg[y, x]) or value > agg[y, x]):\n            agg[y, x] = value\n            return 0\n        return -1\n\n    # GPU append functions\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_antialias_cuda(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field*aa_factor\n        if not isnull(value):\n            old = cuda_atomic_nanmax(agg, (y, x), value)\n            if isnull(old) or old < value:\n                return 0\n        return -1\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_cuda(x, y, agg, field):\n        if not isnull(field):\n            old = cuda_atomic_nanmax(agg, (y, x), field)\n            if isnull(old) or old < field:\n                return 0\n        return -1\n\n    @staticmethod\n    def _combine(aggs):\n        return np.nanmax(aggs, axis=0)\n\n\nclass count_cat(by):\n    \"\"\"Count of all elements in ``column``, grouped by category.\n    Alias for `by(...,count())`, for backwards compatibility.\n\n    Parameters\n    ----------\n    column : str\n        Name of the column to aggregate over. Column data type must be\n        categorical. Resulting aggregate has a outer dimension axis along the\n        categories present.\n    \"\"\"\n    def __init__(self, column):\n        super().__init__(column, count())\n\n\nclass mean(Reduction):\n    \"\"\"Mean of all elements in ``column``.\n\n    Parameters\n    ----------\n    column : str\n        Name of the column to aggregate over. Column data type must be numeric.\n        ``NaN`` values in the column are skipped.\n    \"\"\"\n    def _build_bases(self, cuda, partitioned):\n        return (_sum_zero(self.column), _count_ignore_antialiasing(self.column))\n\n    @staticmethod\n    def _finalize(bases, cuda=False, **kwargs):\n        sums, counts = bases\n        with np.errstate(divide='ignore', invalid='ignore'):\n            x = np.where(counts > 0, sums/counts, np.nan)\n        return xr.DataArray(x, **kwargs)\n\n\nclass var(Reduction):\n    \"\"\"Variance of all elements in ``column``.\n\n    Parameters\n    ----------\n    column : str\n        Name of the column to aggregate over. Column data type must be numeric.\n        ``NaN`` values in the column are skipped.\n    \"\"\"\n    def _build_bases(self, cuda, partitioned):\n        return (_sum_zero(self.column), count(self.column), m2(self.column))\n\n    @staticmethod\n    def _finalize(bases, cuda=False, **kwargs):\n        sums, counts, m2s = bases\n        with np.errstate(divide='ignore', invalid='ignore'):\n            x = np.where(counts > 0, m2s / counts, np.nan)\n        return xr.DataArray(x, **kwargs)\n\n\nclass std(Reduction):\n    \"\"\"Standard Deviation of all elements in ``column``.\n\n    Parameters\n    ----------\n    column : str\n        Name of the column to aggregate over. Column data type must be numeric.\n        ``NaN`` values in the column are skipped.\n    \"\"\"\n    def _build_bases(self, cuda, partitioned):\n        return (_sum_zero(self.column), count(self.column), m2(self.column))\n\n    @staticmethod\n    def _finalize(bases, cuda=False, **kwargs):\n        sums, counts, m2s = bases\n        with np.errstate(divide='ignore', invalid='ignore'):\n            x = np.where(counts > 0, np.sqrt(m2s / counts), np.nan)\n        return xr.DataArray(x, **kwargs)\n\n\nclass _first_or_last(Reduction):\n    \"\"\"Abstract base class of first and last reductions.\n    \"\"\"\n    def out_dshape(self, in_dshape, antialias, cuda, partitioned):\n        return dshape(ct.float64)\n\n    def uses_row_index(self, cuda, partitioned):\n        return cuda or partitioned\n\n    def _antialias_requires_2_stages(self):\n        return True\n\n    def _build_bases(self, cuda, partitioned):\n        if self.uses_row_index(cuda, partitioned):\n            row_index_selector = self._create_row_index_selector()\n            wrapper = where(selector=row_index_selector, lookup_column=self.column)\n            wrapper._nan_check_column = self.column\n            # where reduction is always preceded by its selector reduction\n            return row_index_selector._build_bases(cuda, partitioned) + (wrapper,)\n        else:\n            return super()._build_bases(cuda, partitioned)\n\n    @staticmethod\n    def _combine(aggs):\n        # Dask combine is handled by a where reduction using a row index.\n        # Hence this can only ever be called if npartitions == 1 in which case len(aggs) == 1.\n        if len(aggs) > 1:\n            raise RuntimeError(\"_combine should never be called with more than one agg\")\n        return aggs[0]\n\n    def _create_row_index_selector(self):\n        pass\n\n    @staticmethod\n    def _finalize(bases, cuda=False, **kwargs):\n        # Note returning the last of the bases which is correct regardless of whether\n        # this is a simple reduction (with a single base) or a compound where reduction\n        # (with 2 bases, the second of which is the where reduction).\n        return xr.DataArray(bases[-1], **kwargs)\n\n\nclass first(_first_or_last):\n    \"\"\"First value encountered in ``column``.\n\n    Useful for categorical data where an actual value must always be returned,\n    not an average or other numerical calculation.\n\n    Currently only supported for rasters, externally to this class.\n\n    Parameters\n    ----------\n    column : str\n        Name of the column to aggregate over. If the data type is floating point,\n        ``NaN`` values in the column are skipped.\n    \"\"\"\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        return (AntialiasStage2(AntialiasCombination.FIRST, array_module.nan),)\n\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if not isnull(field) and isnull(agg[y, x]):\n            agg[y, x] = field\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field*aa_factor\n        if not isnull(value) and (isnull(agg[y, x]) or value > agg[y, x]):\n            agg[y, x] = value\n            return 0\n        return -1\n\n    def _create_row_index_selector(self):\n        return _min_row_index()\n\n\nclass last(_first_or_last):\n    \"\"\"Last value encountered in ``column``.\n\n    Useful for categorical data where an actual value must always be returned,\n    not an average or other numerical calculation.\n\n    Currently only supported for rasters, externally to this class.\n\n    Parameters\n    ----------\n    column : str\n        Name of the column to aggregate over. If the data type is floating point,\n        ``NaN`` values in the column are skipped.\n    \"\"\"\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        return (AntialiasStage2(AntialiasCombination.LAST, array_module.nan),)\n\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if not isnull(field):\n            agg[y, x] = field\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field*aa_factor\n        if not isnull(value) and (isnull(agg[y, x]) or value > agg[y, x]):\n            agg[y, x] = value\n            return 0\n        return -1\n\n    def _create_row_index_selector(self):\n        return _max_row_index()\n\n\nclass FloatingNReduction(OptionalFieldReduction):\n    def __init__(self, column=None, n=1):\n        super().__init__(column)\n        self.n = n if n >= 1 else 1\n\n    def out_dshape(self, in_dshape, antialias, cuda, partitioned):\n        return dshape(ct.float64)\n\n    def _add_finalize_kwargs(self, **kwargs):\n        # Add the new dimension and coordinate.\n        n_name = \"n\"\n        n_values = np.arange(self.n)\n\n        # Return a modified copy of kwargs. Cannot modify supplied kwargs as it\n        # may be used by multiple reductions, e.g. if a summary reduction.\n        kwargs = copy.deepcopy(kwargs)\n        kwargs['dims'] += [n_name]\n        kwargs['coords'][n_name] = n_values\n        return kwargs\n\n    def _build_create(self, required_dshape):\n        return lambda shape, array_module: super(FloatingNReduction, self)._build_create(\n            required_dshape)(shape + (self.n,), array_module)\n\n    def _build_finalize(self, dshape):\n        def finalize(bases, cuda=False, **kwargs):\n            kwargs = self._add_finalize_kwargs(**kwargs)\n            return self._finalize(bases, cuda=cuda, **kwargs)\n\n        return finalize\n\n    def _hashable_inputs(self):\n        return super()._hashable_inputs() + (self.n,)\n\n\nclass _first_n_or_last_n(FloatingNReduction):\n    \"\"\"Abstract base class of first_n and last_n reductions.\n    \"\"\"\n    def uses_row_index(self, cuda, partitioned):\n        return cuda or partitioned\n\n    def _antialias_requires_2_stages(self):\n        return True\n\n    def _build_bases(self, cuda, partitioned):\n        if self.uses_row_index(cuda, partitioned):\n            row_index_selector = self._create_row_index_selector()\n            wrapper = where(selector=row_index_selector, lookup_column=self.column)\n            wrapper._nan_check_column = self.column\n            # where reduction is always preceded by its selector reduction\n            return row_index_selector._build_bases(cuda, partitioned) + (wrapper,)\n        else:\n            return super()._build_bases(cuda, partitioned)\n\n    @staticmethod\n    def _combine(aggs):\n        # Dask combine is handled by a where reduction using a row index.\n        # Hence this can only ever be called if npartitions == 1 in which case len(aggs) == 1.\n        if len(aggs) > 1:\n            raise RuntimeError(\"_combine should never be called with more than one agg\")\n        return aggs[0]\n\n    def _create_row_index_selector(self):\n        pass\n\n    @staticmethod\n    def _finalize(bases, cuda=False, **kwargs):\n        # Note returning the last of the bases which is correct regardless of whether\n        # this is a simple reduction (with a single base) or a compound where reduction\n        # (with 2 bases, the second of which is the where reduction).\n        return xr.DataArray(bases[-1], **kwargs)\n\n\nclass first_n(_first_n_or_last_n):\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        return (AntialiasStage2(AntialiasCombination.FIRST, array_module.nan, n_reduction=True),)\n\n    # CPU append functions\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if not isnull(field):\n            # Check final value first for quick abort.\n            n = agg.shape[2]\n            if not isnull(agg[y, x, n-1]):\n                return -1\n\n            # Linear walk along stored values.\n            # Could do binary search instead but not expecting n to be large.\n            for i in range(n):\n                if isnull(agg[y, x, i]):\n                    #\u00a0Nothing to shift.\n                    agg[y, x, i] = field\n                    return i\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field*aa_factor\n        if not isnull(value):\n            # Check final value first for quick abort.\n            n = agg.shape[2]\n            if not isnull(agg[y, x, n-1]):\n                return -1\n\n            # Linear walk along stored values.\n            # Could do binary search instead but not expecting n to be large.\n            for i in range(n):\n                if isnull(agg[y, x, i]):\n                    #\u00a0Nothing to shift.\n                    agg[y, x, i] = value\n                    return i\n        return -1\n\n    def _create_row_index_selector(self):\n        return _min_n_row_index(n=self.n)\n\n\nclass last_n(_first_n_or_last_n):\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        return (AntialiasStage2(AntialiasCombination.LAST, array_module.nan, n_reduction=True),)\n\n    # CPU append functions\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if not isnull(field):\n            #\u00a0Always inserts at front of agg's third dimension.\n            shift_and_insert(agg[y, x], field, 0)\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field*aa_factor\n        if not isnull(value):\n            #\u00a0Always inserts at front of agg's third dimension.\n            shift_and_insert(agg[y, x], value, 0)\n            return 0\n        return -1\n\n    def _create_row_index_selector(self):\n        return _max_n_row_index(n=self.n)\n\n\nclass max_n(FloatingNReduction):\n    def uses_cuda_mutex(self) -> UsesCudaMutex:\n        return UsesCudaMutex.Local\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        return (AntialiasStage2(AntialiasCombination.MAX, array_module.nan, n_reduction=True),)\n\n    # CPU append functions\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if not isnull(field):\n            # Linear walk along stored values.\n            # Could do binary search instead but not expecting n to be large.\n            n = agg.shape[2]\n            for i in range(n):\n                if isnull(agg[y, x, i]) or field > agg[y, x, i]:\n                    shift_and_insert(agg[y, x], field, i)\n                    return i\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field*aa_factor\n        if not isnull(value):\n            # Linear walk along stored values.\n            # Could do binary search instead but not expecting n to be large.\n            n = agg.shape[2]\n            for i in range(n):\n                if isnull(agg[y, x, i]) or value > agg[y, x, i]:\n                    shift_and_insert(agg[y, x], value, i)\n                    return i\n        return -1\n\n    # GPU append functions\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_cuda(x, y, agg, field):\n        if not isnull(field):\n            # Linear walk along stored values.\n            # Could do binary search instead but not expecting n to be large.\n            n = agg.shape[2]\n            for i in range(n):\n                if isnull(agg[y, x, i]) or field > agg[y, x, i]:\n                    cuda_shift_and_insert(agg[y, x], field, i)\n                    return i\n        return -1\n\n    def _build_combine(self, dshape, antialias, cuda, partitioned, categorical = False):\n        if cuda:\n            return self._combine_cuda\n        else:\n            return self._combine\n\n    @staticmethod\n    def _combine(aggs):\n        ret = aggs[0]\n        for i in range(1, len(aggs)):\n            if ret.ndim == 3:  #\u00a0ndim is either 3 (ny, nx, n) or 4 (ny, nx, ncat, n)\n                nanmax_n_in_place_3d(aggs[0], aggs[i])\n            else:\n                nanmax_n_in_place_4d(aggs[0], aggs[i])\n        return ret\n\n    @staticmethod\n    def _combine_cuda(aggs):\n        ret = aggs[0]\n        kernel_args = cuda_args(ret.shape[:-1])\n        for i in range(1, len(aggs)):\n            if ret.ndim == 3:  #\u00a0ndim is either 3 (ny, nx, n) or 4 (ny, nx, ncat, n)\n                cuda_nanmax_n_in_place_3d[kernel_args](aggs[0], aggs[i])\n            else:\n                cuda_nanmax_n_in_place_4d[kernel_args](aggs[0], aggs[i])\n        return ret\n\n\nclass min_n(FloatingNReduction):\n    def uses_cuda_mutex(self) -> UsesCudaMutex:\n        return UsesCudaMutex.Local\n\n    def _antialias_requires_2_stages(self):\n        return True\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        return (AntialiasStage2(AntialiasCombination.MIN, array_module.nan, n_reduction=True),)\n\n    # CPU append functions\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if not isnull(field):\n            # Linear walk along stored values.\n            # Could do binary search instead but not expecting n to be large.\n            n = agg.shape[2]\n            for i in range(n):\n                if isnull(agg[y, x, i]) or field < agg[y, x, i]:\n                    shift_and_insert(agg[y, x], field, i)\n                    return i\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field*aa_factor\n        if not isnull(value):\n            # Linear walk along stored values.\n            # Could do binary search instead but not expecting n to be large.\n            n = agg.shape[2]\n            for i in range(n):\n                if isnull(agg[y, x, i]) or value < agg[y, x, i]:\n                    shift_and_insert(agg[y, x], value, i)\n                    return i\n        return -1\n\n    # GPU append functions\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_cuda(x, y, agg, field):\n        if not isnull(field):\n            # Linear walk along stored values.\n            # Could do binary search instead but not expecting n to be large.\n            n = agg.shape[2]\n            for i in range(n):\n                if isnull(agg[y, x, i]) or field < agg[y, x, i]:\n                    cuda_shift_and_insert(agg[y, x], field, i)\n                    return i\n        return -1\n\n    def _build_combine(self, dshape, antialias, cuda, partitioned, categorical = False):\n        if cuda:\n            return self._combine_cuda\n        else:\n            return self._combine\n\n    @staticmethod\n    def _combine(aggs):\n        ret = aggs[0]\n        for i in range(1, len(aggs)):\n            if ret.ndim == 3:  #\u00a0ndim is either 3 (ny, nx, n) or 4 (ny, nx, ncat, n)\n                nanmin_n_in_place_3d(aggs[0], aggs[i])\n            else:\n                nanmin_n_in_place_4d(aggs[0], aggs[i])\n        return ret\n\n    @staticmethod\n    def _combine_cuda(aggs):\n        ret = aggs[0]\n        kernel_args = cuda_args(ret.shape[:-1])\n        for i in range(1, len(aggs)):\n            if ret.ndim == 3:  #\u00a0ndim is either 3 (ny, nx, n) or 4 (ny, nx, ncat, n)\n                cuda_nanmin_n_in_place_3d[kernel_args](aggs[0], aggs[i])\n            else:\n                cuda_nanmin_n_in_place_4d[kernel_args](aggs[0], aggs[i])\n        return ret\n\n\nclass mode(Reduction):\n    \"\"\"Mode (most common value) of all the values encountered in ``column``.\n\n    Useful for categorical data where an actual value must always be returned,\n    not an average or other numerical calculation.\n\n    Currently only supported for rasters, externally to this class.\n    Implementing it for other glyph types would be difficult due to potentially\n    unbounded data storage requirements to store indefinite point or line\n    data per pixel.\n\n    Parameters\n    ----------\n    column : str\n        Name of the column to aggregate over. If the data type is floating point,\n        ``NaN`` values in the column are skipped.\n    \"\"\"\n    def out_dshape(self, in_dshape, antialias, cuda, partitioned):\n        return dshape(Option(ct.float64))\n\n    @staticmethod\n    def _append(x, y, agg):\n        raise NotImplementedError(\"mode is currently implemented only for rasters\")\n\n    @staticmethod\n    def _combine(aggs):\n        raise NotImplementedError(\"mode is currently implemented only for rasters\")\n\n    @staticmethod\n    def _finalize(bases, **kwargs):\n        raise NotImplementedError(\"mode is currently implemented only for rasters\")\n\n\nclass where(FloatingReduction):\n    \"\"\"\n    Returns values from a ``lookup_column`` corresponding to a ``selector``\n    reduction that is applied to some other column.\n\n    If ``lookup_column`` is ``None`` then it uses the index of the row in the\n    DataFrame instead of a named column. This is returned as an int64\n    aggregation with -1 used to denote no value.\n\n    Examples\n    --------\n    >>> canvas.line(df, 'x', 'y', agg=ds.where(ds.max(\"value\"), \"other\"))  # doctest: +SKIP\n\n    This returns the values of the \"other\" column that correspond to the\n    maximum of the \"value\" column in each bin.\n\n    Parameters\n    ----------\n    selector: Reduction\n        Reduction used to select the values of the ``lookup_column`` which are\n        returned by this ``where`` reduction.\n\n    lookup_column : str | None\n        Column containing values that are returned from this ``where``\n        reduction, or ``None`` to return row indexes instead.\n    \"\"\"\n    def __init__(self, selector: Reduction, lookup_column: str | None=None):\n        if not isinstance(selector, (first, first_n, last, last_n, max, max_n, min, min_n,\n                                     _max_or_min_row_index, _max_n_or_min_n_row_index)):\n            raise TypeError(\n                \"selector can only be a first, first_n, last, last_n, \"\n                \"max, max_n, min or min_n reduction\")\n        if lookup_column is None:\n            lookup_column = SpecialColumn.RowIndex\n        super().__init__(lookup_column)\n        self.selector = selector\n        # List of all column names that this reduction uses.\n        self.columns = (selector.column, lookup_column)\n\n    def __hash__(self):\n        return hash((type(self), self._hashable_inputs(), self.selector))\n\n    def is_where(self):\n        return True\n\n    def out_dshape(self, input_dshape, antialias, cuda, partitioned):\n        if self.column == SpecialColumn.RowIndex:\n            return dshape(ct.int64)\n        else:\n            return dshape(ct.float64)\n\n    def uses_cuda_mutex(self) -> UsesCudaMutex:\n        return UsesCudaMutex.Local\n\n    def uses_row_index(self, cuda, partitioned):\n        return (self.column == SpecialColumn.RowIndex or\n                self.selector.uses_row_index(cuda, partitioned))\n\n    def validate(self, in_dshape):\n        if self.column != SpecialColumn.RowIndex:\n            super().validate(in_dshape)\n        self.selector.validate(in_dshape)\n        if self.column != SpecialColumn.RowIndex and self.column == self.selector.column:\n            raise ValueError(\"where and its contained reduction cannot use the same column\")\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        ret = self.selector._antialias_stage_2(self_intersect, array_module)\n        if self.column == SpecialColumn.RowIndex:\n            # Override antialiased zero value when returning integer row index.\n            ret = (AntialiasStage2(combination=ret[0].combination,\n                                   zero=-1,\n                                   n_reduction=ret[0].n_reduction),)\n        return ret\n\n    # CPU append functions\n    #\u00a0All where._append* functions have an extra argument which is the update index.\n    # For 3D aggs like max_n, this is the index of insertion in the final dimension,\n    # and the previous values from this index upwards are shifted along to make room\n    # for the new value.\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field, update_index):\n        if agg.ndim > 2:\n            shift_and_insert(agg[y, x], field, update_index)\n        else:\n            agg[y, x] = field\n        return update_index\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor, update_index):\n        # Ignore aa_factor.\n        if agg.ndim > 2:\n            shift_and_insert(agg[y, x], field, update_index)\n        else:\n            agg[y, x] = field\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_antialias_cuda(x, y, agg, field, aa_factor, prev_aa_factor, update_index):\n        # Ignore aa_factor\n        if agg.ndim > 2:\n            cuda_shift_and_insert(agg[y, x], field, update_index)\n        else:\n            agg[y, x] = field\n        return update_index\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_cuda(x, y, agg, field, update_index):\n        if agg.ndim > 2:\n            cuda_shift_and_insert(agg[y, x], field, update_index)\n        else:\n            agg[y, x] = field\n        return update_index\n\n    def _build_append(self, dshape, schema, cuda, antialias, self_intersect):\n        # If self.column is SpecialColumn.RowIndex then append function is passed a\n        # 'field' argument which is the row index.\n        if cuda:\n            if antialias:\n                return self._append_antialias_cuda\n            else:\n                return self._append_cuda\n        else:\n            if antialias:\n                return self._append_antialias\n            else:\n                return self._append\n\n    def _build_bases(self, cuda, partitioned):\n        selector = self.selector\n        if isinstance(selector, (_first_or_last, _first_n_or_last_n)) and \\\n                selector.uses_row_index(cuda, partitioned):\n            # Need to swap out the selector with an equivalent row index selector\n            row_index_selector = selector._create_row_index_selector()\n            if self.column == SpecialColumn.RowIndex:\n                # If selector uses a row index and this where returns the same row index,\n                # can just swap out this where reduction with the row_index_selector.\n                row_index_selector._nan_check_column = self.selector.column\n                return row_index_selector._build_bases(cuda, partitioned)\n            else:\n                new_where = where(row_index_selector, self.column)\n                new_where._nan_check_column = self.selector.column\n                return row_index_selector._build_bases(cuda, partitioned) + \\\n                    new_where._build_bases(cuda, partitioned)\n        else:\n            return selector._build_bases(cuda, partitioned) + \\\n                super()._build_bases(cuda, partitioned)\n\n    def _combine_callback(self, cuda, partitioned, categorical):\n        #\u00a0Used by:\n        # 1) where._build_combine()) below, the usual mechanism for combining aggs from\n        #    different dask partitions.\n        # 2) make_antialias_stage_2_functions() in compiler.py to perform stage 2 combine\n        #    of antialiased aggs.\n        selector = self.selector\n        is_n_reduction = isinstance(selector, FloatingNReduction)\n        if cuda:\n            append = selector._append_cuda\n        else:\n            append = selector._append\n\n        # If the selector uses a row_index then selector_aggs will be int64 with -1\n        # representing missing data. Otherwise missing data is NaN.\n        invalid = isminus1 if self.selector.uses_row_index(cuda, partitioned) else isnull\n\n        @ngjit\n        def combine_cpu_2d(aggs, selector_aggs):\n            ny, nx = aggs[0].shape\n            for y in range(ny):\n                for x in range(nx):\n                    value = selector_aggs[1][y, x]\n                    if not invalid(value) and append(x, y, selector_aggs[0], value) >= 0:\n                        aggs[0][y, x] = aggs[1][y, x]\n\n        @ngjit\n        def combine_cpu_3d(aggs, selector_aggs):\n            ny, nx, ncat = aggs[0].shape\n            for y in range(ny):\n                for x in range(nx):\n                    for cat in range(ncat):\n                        value = selector_aggs[1][y, x, cat]\n                        if not invalid(value) and append(x, y, selector_aggs[0][:, :, cat],\n                                                         value) >= 0:\n                            aggs[0][y, x, cat] = aggs[1][y, x, cat]\n\n        @ngjit\n        def combine_cpu_n_3d(aggs, selector_aggs):\n            ny, nx, n = aggs[0].shape\n            for y in range(ny):\n                for x in range(nx):\n                    for i in range(n):\n                        value = selector_aggs[1][y, x, i]\n                        if invalid(value):\n                            break\n                        update_index = append(x, y, selector_aggs[0], value)\n                        if update_index < 0:\n                            break\n                        shift_and_insert(aggs[0][y, x], aggs[1][y, x, i], update_index)\n\n        @ngjit\n        def combine_cpu_n_4d(aggs, selector_aggs):\n            ny, nx, ncat, n = aggs[0].shape\n            for y in range(ny):\n                for x in range(nx):\n                    for cat in range(ncat):\n                        for i in range(n):\n                            value = selector_aggs[1][y, x, cat, i]\n                            if invalid(value):\n                                break\n                            update_index = append(x, y, selector_aggs[0][:, :, cat, :], value)\n                            if update_index < 0:\n                                break\n                            shift_and_insert(aggs[0][y, x, cat], aggs[1][y, x, cat, i],\n                                             update_index)\n\n        @nb_cuda.jit\n        def combine_cuda_2d(aggs, selector_aggs):\n            ny, nx = aggs[0].shape\n            x, y = nb_cuda.grid(2)\n            if x < nx and y < ny:\n                value = selector_aggs[1][y, x]\n                if not invalid(value) and append(x, y, selector_aggs[0], value) >= 0:\n                    aggs[0][y, x] = aggs[1][y, x]\n\n        @nb_cuda.jit\n        def combine_cuda_3d(aggs, selector_aggs):\n            ny, nx, ncat = aggs[0].shape\n            x, y, cat = nb_cuda.grid(3)\n            if x < nx and y < ny and cat < ncat:\n                value = selector_aggs[1][y, x, cat]\n                if not invalid(value) and append(x, y, selector_aggs[0][:, :, cat], value) >= 0:\n                    aggs[0][y, x, cat] = aggs[1][y, x, cat]\n\n        @nb_cuda.jit\n        def combine_cuda_n_3d(aggs, selector_aggs):\n            ny, nx, n = aggs[0].shape\n            x, y = nb_cuda.grid(2)\n            if x < nx and y < ny:\n                for i in range(n):\n                    value = selector_aggs[1][y, x, i]\n                    if invalid(value):\n                        break\n                    update_index = append(x, y, selector_aggs[0], value)\n                    if update_index < 0:\n                        break\n                    cuda_shift_and_insert(aggs[0][y, x], aggs[1][y, x, i], update_index)\n\n        @nb_cuda.jit\n        def combine_cuda_n_4d(aggs, selector_aggs):\n            ny, nx, ncat, n = aggs[0].shape\n            x, y, cat = nb_cuda.grid(3)\n            if x < nx and y < ny and cat < ncat:\n                for i in range(n):\n                    value = selector_aggs[1][y, x, cat, i]\n                    if invalid(value):\n                        break\n                    update_index = append(x, y, selector_aggs[0][:, :, cat, :], value)\n                    if update_index < 0:\n                        break\n                    cuda_shift_and_insert(aggs[0][y, x, cat], aggs[1][y, x, cat, i], update_index)\n\n        if is_n_reduction:\n            #\u00a0ndim is either 3 (ny, nx, n) or 4 (ny, nx, ncat, n)\n            if cuda:\n                return combine_cuda_n_4d if categorical else combine_cuda_n_3d\n            else:\n                return combine_cpu_n_4d if categorical else combine_cpu_n_3d\n        else:\n            #\u00a0ndim is either 2 (ny, nx) or 3 (ny, nx, ncat)\n            if cuda:\n                return combine_cuda_3d if categorical else combine_cuda_2d\n            else:\n                return combine_cpu_3d if categorical else combine_cpu_2d\n\n    def _build_combine(self, dshape, antialias, cuda, partitioned, categorical = False):\n        combine = self._combine_callback(cuda, partitioned, categorical)\n\n        def wrapped_combine(aggs, selector_aggs):\n            if len(aggs) == 1:\n                pass\n            elif cuda:\n                assert len(aggs) == 2\n                is_n_reduction = isinstance(self.selector, FloatingNReduction)\n                shape = aggs[0].shape[:-1] if is_n_reduction else aggs[0].shape\n                combine[cuda_args(shape)](aggs, selector_aggs)\n            else:\n                for i in range(1, len(aggs)):\n                    combine((aggs[0], aggs[i]), (selector_aggs[0], selector_aggs[i]))\n\n            return aggs[0], selector_aggs[0]\n\n        return wrapped_combine\n\n    def _build_combine_temps(self, cuda, partitioned):\n        return (self.selector,)\n\n    def _build_create(self, required_dshape):\n        # Return a function that when called with a shape creates an agg array\n        # of the required type (numpy/cupy) and dtype.\n        if isinstance(self.selector, FloatingNReduction):\n            # This specialisation isn't ideal but Reduction classes do not\n            # store information about the required extra dimension.\n            return lambda shape, array_module: super(where, self)._build_create(\n                required_dshape)(shape + (self.selector.n,), array_module)\n        else:\n            return super()._build_create(required_dshape)\n\n    def _build_finalize(self, dshape):\n        if isinstance(self.selector, FloatingNReduction):\n            add_finalize_kwargs = self.selector._add_finalize_kwargs\n        else:\n            add_finalize_kwargs = None\n\n        def finalize(bases, cuda=False, **kwargs):\n            if add_finalize_kwargs is not None:\n                kwargs = add_finalize_kwargs(**kwargs)\n\n            return xr.DataArray(bases[-1], **kwargs)\n\n        return finalize\n\n\nclass summary(Expr):\n    \"\"\"A collection of named reductions.\n\n    Computes all aggregates simultaneously, output is stored as a\n    ``xarray.Dataset``.\n\n    Examples\n    --------\n    A reduction for computing the mean of column \"a\", and the sum of column \"b\"\n    for each bin, all in a single pass.\n\n    >>> import datashader as ds\n    >>> red = ds.summary(mean_a=ds.mean('a'), sum_b=ds.sum('b'))\n\n    Notes\n    -----\n    A single pass of the source dataset using antialiased lines can either be\n    performed using a single-stage aggregation (e.g. ``self_intersect=True``)\n    or two stages (``self_intersect=False``). If a ``summary`` contains a\n    ``count`` or ``sum`` reduction with ``self_intersect=False``, or any of\n    ``first``, ``last`` or ``min``, then the antialiased line pass will be\n    performed in two stages.\n    \"\"\"\n    def __init__(self, **kwargs):\n        ks, vs = zip(*sorted(kwargs.items()))\n        self.keys = ks\n        self.values = vs\n\n    def __hash__(self):\n        return hash((type(self), tuple(self.keys), tuple(self.values)))\n\n    def is_categorical(self):\n        for v in self.values:\n            if v.is_categorical():\n                return True\n        return False\n\n    def uses_row_index(self, cuda, partitioned):\n        for v in self.values:\n            if v.uses_row_index(cuda, partitioned):\n                return True\n        return False\n\n    def validate(self, input_dshape):\n        for v in self.values:\n            v.validate(input_dshape)\n\n        # Check that any included FloatingNReductions have the same n values.\n        n_values = []\n        for v in self.values:\n            if isinstance(v, where):\n                v = v.selector\n            if isinstance(v, FloatingNReduction):\n                n_values.append(v.n)\n        if len(np.unique(n_values)) > 1:\n            raise ValueError(\n                \"Using multiple FloatingNReductions with different n values is not supported\")\n\n    @property\n    def inputs(self):\n        return tuple(unique(concat(v.inputs for v in self.values)))\n\n\nclass _max_or_min_row_index(OptionalFieldReduction):\n    \"\"\"Abstract base class of max and min row_index reductions.\n    \"\"\"\n    def __init__(self):\n        super().__init__(column=SpecialColumn.RowIndex)\n\n    def out_dshape(self, in_dshape, antialias, cuda, partitioned):\n        return dshape(ct.int64)\n\n    def uses_row_index(self, cuda, partitioned):\n        return True\n\n\nclass _max_row_index(_max_or_min_row_index):\n    \"\"\"Max reduction operating on row index.\n\n    This is a private class as it is not intended to be used explicitly in\n    user code. It is primarily purpose is to support the use of ``last``\n    reductions using dask and/or CUDA.\n    \"\"\"\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        return (AntialiasStage2(AntialiasCombination.MAX, -1),)\n\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        #\u00a0field is int64 row index\n        if field > agg[y, x]:\n            agg[y, x] = field\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        #\u00a0field is int64 row index\n        # Ignore aa_factor\n        if field > agg[y, x]:\n            agg[y, x] = field\n            return 0\n        return -1\n\n    # GPU append functions\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_cuda(x, y, agg, field):\n        #\u00a0field is int64 row index\n        if field != -1:\n            old = nb_cuda.atomic.max(agg, (y, x), field)\n            if old < field:\n                return 0\n        return -1\n\n    @staticmethod\n    def _combine(aggs):\n        # Maximum ignoring -1 values\n        # Works for CPU and GPU\n        ret = aggs[0]\n        for i in range(1, len(aggs)):\n            # Works with numpy or cupy arrays\n            np.maximum(ret, aggs[i], out=ret)\n        return ret\n\n\nclass _min_row_index(_max_or_min_row_index):\n    \"\"\"Min reduction operating on row index.\n\n    This is a private class as it is not intended to be used explicitly in\n    user code. It is primarily purpose is to support the use of ``first``\n    reductions using dask and/or CUDA.\n    \"\"\"\n    def _antialias_requires_2_stages(self):\n        return True\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        return (AntialiasStage2(AntialiasCombination.MIN, -1),)\n\n    def uses_cuda_mutex(self) -> UsesCudaMutex:\n        return UsesCudaMutex.Local\n\n    # CPU append functions\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        # field is int64 row index\n        if field != -1 and (agg[y, x] == -1 or field < agg[y, x]):\n            agg[y, x] = field\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        # field is int64 row index\n        # Ignore aa_factor\n        if field != -1 and (agg[y, x] == -1 or field < agg[y, x]):\n            agg[y, x] = field\n            return 0\n        return -1\n\n    # GPU append functions\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_cuda(x, y, agg, field):\n        # field is int64 row index\n        # Always uses cuda mutex so this does not need to be atomic\n        if field != -1 and (agg[y, x] == -1 or field < agg[y, x]):\n            agg[y, x] = field\n            return 0\n        return -1\n\n    def _build_combine(self, dshape, antialias, cuda, partitioned, categorical = False):\n        if cuda:\n            return self._combine_cuda\n        else:\n            return self._combine\n\n    @staticmethod\n    def _combine(aggs):\n        # Minimum ignoring -1 values\n        ret = aggs[0]\n        for i in range(1, len(aggs)):\n            # Can take 2d (ny, nx) or 3d (ny, nx, ncat) arrays.\n            row_min_in_place(ret, aggs[i])\n        return ret\n\n    @staticmethod\n    def _combine_cuda(aggs):\n        ret = aggs[0]\n        if len(aggs) > 1:\n            if ret.ndim == 2:  #\u00a0ndim is either 2 (ny, nx) or 3 (ny, nx, ncat)\n                # 3d view of each agg\n                aggs = [cp.expand_dims(agg, 2) for agg in aggs]\n            kernel_args = cuda_args(ret.shape[:3])\n            for i in range(1, len(aggs)):\n                cuda_row_min_in_place[kernel_args](aggs[0], aggs[i])\n        return ret\n\n\nclass _max_n_or_min_n_row_index(FloatingNReduction):\n    \"\"\"Abstract base class of max_n and min_n row_index reductions.\n    \"\"\"\n    def __init__(self, n=1):\n        super().__init__(column=SpecialColumn.RowIndex)\n        self.n = n if n >= 1 else 1\n\n    def out_dshape(self, in_dshape, antialias, cuda, partitioned):\n        return dshape(ct.int64)\n\n    def uses_cuda_mutex(self) -> UsesCudaMutex:\n        return UsesCudaMutex.Local\n\n    def uses_row_index(self, cuda, partitioned):\n        return True\n\n    def _build_combine(self, dshape, antialias, cuda, partitioned, categorical = False):\n        if cuda:\n            return self._combine_cuda\n        else:\n            return self._combine\n\n\nclass _max_n_row_index(_max_n_or_min_n_row_index):\n    \"\"\"Max_n reduction operating on row index.\n\n    This is a private class as it is not intended to be used explicitly in\n    user code. It is primarily purpose is to support the use of ``last_n``\n    reductions using dask and/or CUDA.\n    \"\"\"\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        return (AntialiasStage2(AntialiasCombination.MAX, -1, n_reduction=True),)\n\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        # field is int64 row index\n        if field != -1:\n            # Linear walk along stored values.\n            # Could do binary search instead but not expecting n to be large.\n            n = agg.shape[2]\n            for i in range(n):\n                if agg[y, x, i] == -1 or field > agg[y, x, i]:\n                    shift_and_insert(agg[y, x], field, i)\n                    return i\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        # field is int64 row index\n        # Ignoring aa_factor\n        if field != -1:\n            # Linear walk along stored values.\n            # Could do binary search instead but not expecting n to be large.\n            n = agg.shape[2]\n            for i in range(n):\n                if agg[y, x, i] == -1 or field > agg[y, x, i]:\n                    # Bump previous values along to make room for new value.\n                    for j in range(n-1, i, -1):\n                        agg[y, x, j] = agg[y, x, j-1]\n                    agg[y, x, i] = field\n                    return i\n        return -1\n\n    # GPU append functions\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_cuda(x, y, agg, field):\n        #\u00a0field is int64 row index\n        # Always uses cuda mutex so this does not need to be atomic\n        if field != -1:\n            # Linear walk along stored values.\n            # Could do binary search instead but not expecting n to be large.\n            n = agg.shape[2]\n            for i in range(n):\n                if agg[y, x, i] == -1 or field > agg[y, x, i]:\n                    cuda_shift_and_insert(agg[y, x], field, i)\n                    return i\n        return -1\n\n    @staticmethod\n    def _combine(aggs):\n        ret = aggs[0]\n        if len(aggs) > 1:\n            if ret.ndim == 3:  #\u00a0ndim is either 3 (ny, nx, n) or 4 (ny, nx, ncat, n)\n                row_max_n_in_place_3d(aggs[0], aggs[1])\n            else:\n                row_max_n_in_place_4d(aggs[0], aggs[1])\n        return ret\n\n    @staticmethod\n    def _combine_cuda(aggs):\n        ret = aggs[0]\n        if len(aggs) > 1:\n            kernel_args = cuda_args(ret.shape[:-1])\n            if ret.ndim == 3:  #\u00a0ndim is either 3 (ny, nx, n) or 4 (ny, nx, ncat, n)\n                cuda_row_max_n_in_place_3d[kernel_args](aggs[0], aggs[1])\n            else:\n                cuda_row_max_n_in_place_4d[kernel_args](aggs[0], aggs[1])\n        return ret\n\n\nclass _min_n_row_index(_max_n_or_min_n_row_index):\n    \"\"\"Min_n reduction operating on row index.\n\n    This is a private class as it is not intended to be used explicitly in\n    user code. It is primarily purpose is to support the use of ``first_n``\n    reductions using dask and/or CUDA.\n    \"\"\"\n    def _antialias_requires_2_stages(self):\n        return True\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        return (AntialiasStage2(AntialiasCombination.MIN, -1, n_reduction=True),)\n\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        # field is int64 row index\n        if field != -1:\n            # Linear walk along stored values.\n            # Could do binary search instead but not expecting n to be large.\n            n = agg.shape[2]\n            for i in range(n):\n                if agg[y, x, i] == -1 or field < agg[y, x, i]:\n                    shift_and_insert(agg[y, x], field, i)\n                    return i\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        # field is int64 row index\n        # Ignoring aa_factor\n        if field != -1:\n            # Linear walk along stored values.\n            # Could do binary search instead but not expecting n to be large.\n            n = agg.shape[2]\n            for i in range(n):\n                if agg[y, x, i] == -1 or field < agg[y, x, i]:\n                    shift_and_insert(agg[y, x], field, i)\n                    return i\n        return -1\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_cuda(x, y, agg, field):\n        #\u00a0field is int64 row index\n        # Always uses cuda mutex so this does not need to be atomic\n        if field != -1:\n            # Linear walk along stored values.\n            # Could do binary search instead but not expecting n to be large.\n            n = agg.shape[2]\n            for i in range(n):\n                if agg[y, x, i] == -1 or field < agg[y, x, i]:\n                    cuda_shift_and_insert(agg[y, x], field, i)\n                    return i\n        return -1\n\n    @staticmethod\n    def _combine(aggs):\n        ret = aggs[0]\n        if len(aggs) > 1:\n            if ret.ndim == 3:  #\u00a0ndim is either 3 (ny, nx, n) or 4 (ny, nx, ncat, n)\n                row_min_n_in_place_3d(aggs[0], aggs[1])\n            else:\n                row_min_n_in_place_4d(aggs[0], aggs[1])\n        return ret\n\n    @staticmethod\n    def _combine_cuda(aggs):\n        ret = aggs[0]\n        if len(aggs) > 1:\n            kernel_args = cuda_args(ret.shape[:-1])\n            if ret.ndim == 3:  #\u00a0ndim is either 3 (ny, nx, n) or 4 (ny, nx, ncat, n)\n                cuda_row_min_n_in_place_3d[kernel_args](aggs[0], aggs[1])\n            else:\n                cuda_row_min_n_in_place_4d[kernel_args](aggs[0], aggs[1])\n\n        return ret\n\n\n__all__ = list(set([_k for _k,_v in locals().items()\n                    if isinstance(_v,type) and (issubclass(_v,Reduction) or _v is summary)\n                    and _v not in [Reduction, OptionalFieldReduction,\n                                   FloatingReduction, m2]])) + \\\n                    ['category_modulo', 'category_binning']\n",
    "datashader/glyphs/line.py": "from __future__ import annotations\nimport math\nimport numpy as np\nfrom toolz import memoize\n\nfrom datashader.antialias import two_stage_agg\nfrom datashader.glyphs.points import _PointLike, _GeometryLike\nfrom datashader.utils import isnull, isreal, ngjit\nfrom numba import cuda\nimport numba.types as nb_types\n\n\ntry:\n    import cudf\n    import cupy as cp\n    from ..transfer_functions._cuda_utils import cuda_args\nexcept ImportError:\n    cudf = None\n    cp = None\n    cuda_args = None\n\ntry:\n    import spatialpandas\nexcept Exception:\n    spatialpandas = None\n\n\nclass _AntiAliasedLine:\n    \"\"\" Methods common to all lines. \"\"\"\n    _line_width = 0  # Use antialiasing if > 0.\n\n    def set_line_width(self, line_width):\n        self._line_width = line_width\n        if hasattr(self, \"antialiased\"):\n            self.antialiased = (line_width > 0)\n\n    def _build_extend(self, x_mapper, y_mapper, info, append, antialias_stage_2,\n                      antialias_stage_2_funcs):\n        return self._internal_build_extend(\n                x_mapper, y_mapper, info, append, self._line_width, antialias_stage_2,\n                antialias_stage_2_funcs)\n\n\ndef _line_internal_build_extend(\n    x_mapper, y_mapper, append, line_width, antialias_stage_2, antialias_stage_2_funcs,\n    expand_aggs_and_cols,\n):\n    antialias = line_width > 0\n    map_onto_pixel = _build_map_onto_pixel_for_line(x_mapper, y_mapper, antialias)\n    overwrite, use_2_stage_agg = two_stage_agg(antialias_stage_2)\n    if not use_2_stage_agg:\n        antialias_stage_2_funcs = None\n    draw_segment = _build_draw_segment(\n        append, map_onto_pixel, expand_aggs_and_cols, line_width, overwrite,\n    )\n    return draw_segment, antialias_stage_2_funcs\n\n\nclass LineAxis0(_PointLike, _AntiAliasedLine):\n    \"\"\"A line, with vertices defined by ``x`` and ``y``.\n\n    Parameters\n    ----------\n    x, y : str\n        Column names for the x and y coordinates of each vertex.\n    \"\"\"\n    @memoize\n    def _internal_build_extend(\n            self, x_mapper, y_mapper, info, append, line_width, antialias_stage_2,\n            antialias_stage_2_funcs):\n        expand_aggs_and_cols = self.expand_aggs_and_cols(append)\n        draw_segment, antialias_stage_2_funcs = _line_internal_build_extend(\n            x_mapper, y_mapper, append, line_width, antialias_stage_2, antialias_stage_2_funcs,\n            expand_aggs_and_cols,\n        )\n        extend_cpu, extend_cuda = _build_extend_line_axis0(\n            draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs,\n        )\n\n        x_name = self.x\n        y_name = self.y\n\n        def extend(aggs, df, vt, bounds, plot_start=True):\n            sx, tx, sy, ty = vt\n            xmin, xmax, ymin, ymax = bounds\n            aggs_and_cols = aggs + info(df, aggs[0].shape[:2])\n\n            if cudf and isinstance(df, cudf.DataFrame):\n                xs = self.to_cupy_array(df, x_name)\n                ys = self.to_cupy_array(df, y_name)\n                do_extend = extend_cuda[cuda_args(xs.shape)]\n            else:\n                xs = df.loc[:, x_name].to_numpy()\n                ys = df.loc[:, y_name].to_numpy()\n                do_extend = extend_cpu\n\n            # line may be clipped, then mapped to pixels\n            do_extend(\n                sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                xs, ys, plot_start, antialias_stage_2, *aggs_and_cols\n            )\n\n        return extend\n\n\nclass LineAxis0Multi(_PointLike, _AntiAliasedLine):\n    \"\"\"\n    \"\"\"\n\n    def validate(self, in_dshape):\n        if not all([isreal(in_dshape.measure[str(xcol)]) for xcol in self.x]):\n            raise ValueError('x columns must be real')\n        elif not all([isreal(in_dshape.measure[str(ycol)]) for ycol in self.y]):\n            raise ValueError('y columns must be real')\n\n        if len(self.x) != len(self.y):\n            raise ValueError(\n                f'x and y coordinate lengths do not match: {len(self.x)} != {len(self.y)}')\n\n    @property\n    def x_label(self):\n        return 'x'\n\n    @property\n    def y_label(self):\n        return 'y'\n\n    def required_columns(self):\n        return self.x + self.y\n\n    def compute_x_bounds(self, df):\n        bounds_list = [self._compute_bounds(df[x])\n                       for x in self.x]\n        mins, maxes = zip(*bounds_list)\n        return self.maybe_expand_bounds((min(mins), max(maxes)))\n\n    def compute_y_bounds(self, df):\n        bounds_list = [self._compute_bounds(df[y])\n                       for y in self.y]\n        mins, maxes = zip(*bounds_list)\n        return self.maybe_expand_bounds((min(mins), max(maxes)))\n\n    @memoize\n    def compute_bounds_dask(self, ddf):\n\n        r = ddf.map_partitions(lambda df: np.array([[\n            np.nanmin([np.nanmin(df[c].values).item() for c in self.x]),\n            np.nanmax([np.nanmax(df[c].values).item() for c in self.x]),\n            np.nanmin([np.nanmin(df[c].values).item() for c in self.y]),\n            np.nanmax([np.nanmax(df[c].values).item() for c in self.y])]]\n        )).compute()\n\n        x_extents = np.nanmin(r[:, 0]), np.nanmax(r[:, 1])\n        y_extents = np.nanmin(r[:, 2]), np.nanmax(r[:, 3])\n\n        return (self.maybe_expand_bounds(x_extents),\n                self.maybe_expand_bounds(y_extents))\n\n    @memoize\n    def _internal_build_extend(\n            self, x_mapper, y_mapper, info, append, line_width, antialias_stage_2,\n            antialias_stage_2_funcs):\n        expand_aggs_and_cols = self.expand_aggs_and_cols(append)\n        draw_segment, antialias_stage_2_funcs = _line_internal_build_extend(\n            x_mapper, y_mapper, append, line_width, antialias_stage_2, antialias_stage_2_funcs,\n            expand_aggs_and_cols,\n        )\n        extend_cpu, extend_cuda = _build_extend_line_axis0_multi(\n            draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs,\n        )\n\n        x_names = self.x\n        y_names = self.y\n\n        def extend(aggs, df, vt, bounds, plot_start=True):\n            sx, tx, sy, ty = vt\n            xmin, xmax, ymin, ymax = bounds\n            aggs_and_cols = aggs + info(df, aggs[0].shape[:2])\n\n            if cudf and isinstance(df, cudf.DataFrame):\n                xs = self.to_cupy_array(df, x_names)\n                ys = self.to_cupy_array(df, y_names)\n                do_extend = extend_cuda[cuda_args(xs.shape)]\n            else:\n                xs = df.loc[:, list(x_names)].to_numpy()\n                ys = df.loc[:, list(y_names)].to_numpy()\n                do_extend = extend_cpu\n\n            # line may be clipped, then mapped to pixels\n            do_extend(\n                sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                xs, ys, plot_start, antialias_stage_2, *aggs_and_cols,\n            )\n\n        return extend\n\n\nclass LinesAxis1(_PointLike, _AntiAliasedLine):\n    \"\"\"A collection of lines (on line per row) with vertices defined\n    by the lists of columns in ``x`` and ``y``\n\n    Parameters\n    ----------\n    x, y : list\n        Lists of column names for the x and y coordinates\n    \"\"\"\n\n    def validate(self, in_dshape):\n        if not all([isreal(in_dshape.measure[str(xcol)])\n                    for xcol in self.x]):\n            raise ValueError('x columns must be real')\n        elif not all([isreal(in_dshape.measure[str(ycol)])\n                      for ycol in self.y]):\n            raise ValueError('y columns must be real')\n\n        unique_x_measures = set(in_dshape.measure[str(xcol)]\n                                for xcol in self.x)\n        if len(unique_x_measures) > 1:\n            raise ValueError('x columns must have the same data type')\n\n        unique_y_measures = set(in_dshape.measure[str(ycol)]\n                                for ycol in self.y)\n        if len(unique_y_measures) > 1:\n            raise ValueError('y columns must have the same data type')\n\n        if len(self.x) != len(self.y):\n            raise ValueError(\n                f'x and y coordinate lengths do not match: {len(self.x)} != {len(self.y)}')\n\n    def required_columns(self):\n        return self.x + self.y\n\n    @property\n    def x_label(self):\n        return 'x'\n\n    @property\n    def y_label(self):\n        return 'y'\n\n    def compute_x_bounds(self, df):\n        xs = tuple(df[xlabel] for xlabel in self.x)\n\n        bounds_list = [self._compute_bounds(xcol) for xcol in xs]\n        mins, maxes = zip(*bounds_list)\n\n        return self.maybe_expand_bounds((min(mins), max(maxes)))\n\n    def compute_y_bounds(self, df):\n        ys = tuple(df[ylabel] for ylabel in self.y)\n\n        bounds_list = [self._compute_bounds(ycol) for ycol in ys]\n        mins, maxes = zip(*bounds_list)\n\n        return self.maybe_expand_bounds((min(mins), max(maxes)))\n\n    @memoize\n    def compute_bounds_dask(self, ddf):\n\n        r = ddf.map_partitions(lambda df: np.array([[\n            np.nanmin([np.nanmin(df[c].values).item() for c in self.x]),\n            np.nanmax([np.nanmax(df[c].values).item() for c in self.x]),\n            np.nanmin([np.nanmin(df[c].values).item() for c in self.y]),\n            np.nanmax([np.nanmax(df[c].values).item() for c in self.y])]]\n        )).compute()\n\n        x_extents = np.nanmin(r[:, 0]), np.nanmax(r[:, 1])\n        y_extents = np.nanmin(r[:, 2]), np.nanmax(r[:, 3])\n\n        return (self.maybe_expand_bounds(x_extents),\n                self.maybe_expand_bounds(y_extents))\n\n    @memoize\n    def _internal_build_extend(\n            self, x_mapper, y_mapper, info, append, line_width, antialias_stage_2,\n            antialias_stage_2_funcs):\n        expand_aggs_and_cols = self.expand_aggs_and_cols(append)\n        draw_segment, antialias_stage_2_funcs = _line_internal_build_extend(\n            x_mapper, y_mapper, append, line_width, antialias_stage_2, antialias_stage_2_funcs,\n            expand_aggs_and_cols,\n        )\n        extend_cpu, extend_cuda = _build_extend_line_axis1_none_constant(\n            draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs,\n        )\n        x_names = self.x\n        y_names = self.y\n\n        def extend(aggs, df, vt, bounds, plot_start=True):\n            sx, tx, sy, ty = vt\n            xmin, xmax, ymin, ymax = bounds\n            aggs_and_cols = aggs + info(df, aggs[0].shape[:2])\n\n            if cudf and isinstance(df, cudf.DataFrame):\n                xs = self.to_cupy_array(df, x_names)\n                ys = self.to_cupy_array(df, y_names)\n                do_extend = extend_cuda[cuda_args(xs.shape)]\n            else:\n                xs = df.loc[:, list(x_names)].to_numpy()\n                ys = df.loc[:, list(y_names)].to_numpy()\n                do_extend = extend_cpu\n\n            do_extend(\n                sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, *aggs_and_cols\n            )\n\n        return extend\n\n\nclass LinesAxis1XConstant(LinesAxis1):\n    \"\"\"\n    \"\"\"\n    def validate(self, in_dshape):\n        if not all([isreal(in_dshape.measure[str(ycol)]) for ycol in self.y]):\n            raise ValueError('y columns must be real')\n\n        unique_y_measures = set(in_dshape.measure[str(ycol)]\n                                for ycol in self.y)\n        if len(unique_y_measures) > 1:\n            raise ValueError('y columns must have the same data type')\n\n        if len(self.x) != len(self.y):\n            raise ValueError(\n                f'x and y coordinate lengths do not match: {len(self.x)} != {len(self.y)}')\n\n    def required_columns(self):\n        return self.y\n\n    def compute_x_bounds(self, *args):\n        x_min = np.nanmin(self.x)\n        x_max = np.nanmax(self.x)\n        return self.maybe_expand_bounds((x_min, x_max))\n\n    @memoize\n    def compute_bounds_dask(self, ddf):\n\n        r = ddf.map_partitions(lambda df: np.array([[\n            np.nanmin([np.nanmin(df[c].values).item() for c in self.y]),\n            np.nanmax([np.nanmax(df[c].values).item() for c in self.y])]]\n        )).compute()\n\n        y_extents = np.nanmin(r[:, 0]), np.nanmax(r[:, 1])\n\n        return (self.compute_x_bounds(),\n                self.maybe_expand_bounds(y_extents))\n\n    @memoize\n    def _internal_build_extend(\n            self, x_mapper, y_mapper, info, append, line_width, antialias_stage_2,\n            antialias_stage_2_funcs):\n        expand_aggs_and_cols = self.expand_aggs_and_cols(append)\n        draw_segment, antialias_stage_2_funcs = _line_internal_build_extend(\n            x_mapper, y_mapper, append, line_width, antialias_stage_2, antialias_stage_2_funcs,\n            expand_aggs_and_cols,\n        )\n        extend_cpu, extend_cuda = _build_extend_line_axis1_x_constant(\n            draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs,\n        )\n\n        x_values = self.x\n        y_names = self.y\n\n        def extend(aggs, df, vt, bounds, plot_start=True):\n            sx, tx, sy, ty = vt\n            xmin, xmax, ymin, ymax = bounds\n            aggs_and_cols = aggs + info(df, aggs[0].shape[:2])\n\n            if cudf and isinstance(df, cudf.DataFrame):\n                xs = cp.asarray(x_values)\n                ys = self.to_cupy_array(df, y_names)\n                do_extend = extend_cuda[cuda_args(ys.shape)]\n            else:\n                xs = x_values\n                ys = df.loc[:, list(y_names)].to_numpy()\n                do_extend = extend_cpu\n\n            do_extend(\n                sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, *aggs_and_cols\n            )\n\n        return extend\n\n\nclass LinesAxis1YConstant(LinesAxis1):\n    \"\"\"\n    \"\"\"\n    def validate(self, in_dshape):\n        if not all([isreal(in_dshape.measure[str(xcol)]) for xcol in self.x]):\n            raise ValueError('x columns must be real')\n\n        unique_x_measures = set(in_dshape.measure[str(xcol)]\n                                for xcol in self.x)\n        if len(unique_x_measures) > 1:\n            raise ValueError('x columns must have the same data type')\n\n        if len(self.x) != len(self.y):\n            raise ValueError(\n                f'x and y coordinate lengths do not match: {len(self.x)} != {len(self.y)}')\n\n    def required_columns(self):\n        return self.x\n\n    def compute_y_bounds(self, *args):\n        y_min = np.nanmin(self.y)\n        y_max = np.nanmax(self.y)\n        return self.maybe_expand_bounds((y_min, y_max))\n\n    @memoize\n    def compute_bounds_dask(self, ddf):\n\n        r = ddf.map_partitions(lambda df: np.array([[\n            np.nanmin([np.nanmin(df[c].values).item() for c in self.x]),\n            np.nanmax([np.nanmax(df[c].values).item() for c in self.x])]]\n        )).compute()\n\n        x_extents = np.nanmin(r[:, 0]), np.nanmax(r[:, 1])\n\n        return (self.maybe_expand_bounds(x_extents),\n                self.compute_y_bounds())\n\n    @memoize\n    def _internal_build_extend(\n            self, x_mapper, y_mapper, info, append, line_width, antialias_stage_2,\n            antialias_stage_2_funcs):\n        expand_aggs_and_cols = self.expand_aggs_and_cols(append)\n        draw_segment, antialias_stage_2_funcs = _line_internal_build_extend(\n            x_mapper, y_mapper, append, line_width, antialias_stage_2, antialias_stage_2_funcs,\n            expand_aggs_and_cols,\n        )\n        extend_cpu, extend_cuda = _build_extend_line_axis1_y_constant(\n            draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs,\n        )\n\n        x_names = self.x\n        y_values = self.y\n\n        def extend(aggs, df, vt, bounds, plot_start=True):\n            sx, tx, sy, ty = vt\n            xmin, xmax, ymin, ymax = bounds\n            aggs_and_cols = aggs + info(df, aggs[0].shape[:2])\n\n            if cudf and isinstance(df, cudf.DataFrame):\n                xs = self.to_cupy_array(df, x_names)\n                ys = cp.asarray(y_values)\n                do_extend = extend_cuda[cuda_args(xs.shape)]\n            else:\n                xs = df.loc[:, list(x_names)].to_numpy()\n                ys = y_values\n                do_extend = extend_cpu\n\n            do_extend(\n                sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, *aggs_and_cols\n            )\n\n        return extend\n\n\nclass LinesAxis1Ragged(_PointLike, _AntiAliasedLine):\n    def validate(self, in_dshape):\n        try:\n            from datashader.datatypes import RaggedDtype\n        except ImportError:\n            RaggedDtype = type(None)\n\n        if not isinstance(in_dshape[str(self.x)], RaggedDtype):\n            raise ValueError('x must be a RaggedArray')\n        elif not isinstance(in_dshape[str(self.y)], RaggedDtype):\n            raise ValueError('y must be a RaggedArray')\n\n    def required_columns(self):\n        return (self.x,) + (self.y,)\n\n    def compute_x_bounds(self, df):\n        bounds = self._compute_bounds(df[self.x].array.flat_array)\n        return self.maybe_expand_bounds(bounds)\n\n    def compute_y_bounds(self, df):\n        bounds = self._compute_bounds(df[self.y].array.flat_array)\n        return self.maybe_expand_bounds(bounds)\n\n    @memoize\n    def compute_bounds_dask(self, ddf):\n\n        r = ddf.map_partitions(lambda df: np.array([[\n            np.nanmin(df[self.x].array.flat_array).item(),\n            np.nanmax(df[self.x].array.flat_array).item(),\n            np.nanmin(df[self.y].array.flat_array).item(),\n            np.nanmax(df[self.y].array.flat_array).item()]]\n        )).compute()\n\n        x_extents = np.nanmin(r[:, 0]), np.nanmax(r[:, 1])\n        y_extents = np.nanmin(r[:, 2]), np.nanmax(r[:, 3])\n\n        return (self.maybe_expand_bounds(x_extents),\n                self.maybe_expand_bounds(y_extents))\n\n    @memoize\n    def _internal_build_extend(\n            self, x_mapper, y_mapper, info, append, line_width, antialias_stage_2,\n            antialias_stage_2_funcs):\n        expand_aggs_and_cols = self.expand_aggs_and_cols(append)\n        draw_segment, antialias_stage_2_funcs = _line_internal_build_extend(\n            x_mapper, y_mapper, append, line_width, antialias_stage_2, antialias_stage_2_funcs,\n            expand_aggs_and_cols,\n        )\n        extend_cpu = _build_extend_line_axis1_ragged(\n            draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs,\n        )\n        x_name = self.x\n        y_name = self.y\n        def extend(aggs, df, vt, bounds, plot_start=True):\n            sx, tx, sy, ty = vt\n            xmin, xmax, ymin, ymax = bounds\n\n            xs = df[x_name].array\n            ys = df[y_name].array\n\n            aggs_and_cols = aggs + info(df, aggs[0].shape[:2])\n            # line may be clipped, then mapped to pixels\n            extend_cpu(\n                sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, *aggs_and_cols\n            )\n\n        return extend\n\n\nclass LineAxis1Geometry(_GeometryLike, _AntiAliasedLine):\n    # spatialpandas must be available if a LineAxis1Geometry object is created.\n\n    @property\n    def geom_dtypes(self):\n        from spatialpandas.geometry import (\n            LineDtype, MultiLineDtype, RingDtype, PolygonDtype,\n            MultiPolygonDtype\n        )\n        return (LineDtype, MultiLineDtype, RingDtype,\n                PolygonDtype, MultiPolygonDtype)\n\n    @memoize\n    def _internal_build_extend(\n            self, x_mapper, y_mapper, info, append, line_width, antialias_stage_2,\n            antialias_stage_2_funcs):\n        from spatialpandas.geometry import (\n            PolygonArray, MultiPolygonArray, RingArray\n        )\n        expand_aggs_and_cols = self.expand_aggs_and_cols(append)\n        draw_segment, antialias_stage_2_funcs = _line_internal_build_extend(\n            x_mapper, y_mapper, append, line_width, antialias_stage_2, antialias_stage_2_funcs,\n            expand_aggs_and_cols,\n        )\n        perform_extend_cpu = _build_extend_line_axis1_geometry(\n            draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs,\n        )\n        geometry_name = self.geometry\n\n        def extend(aggs, df, vt, bounds, plot_start=True):\n            sx, tx, sy, ty = vt\n            xmin, xmax, ymin, ymax = bounds\n            aggs_and_cols = aggs + info(df, aggs[0].shape[:2])\n            geom_array = df[geometry_name].array\n\n            # Use type to decide whether geometry represents a closed .\n            # We skip for closed geometries so as not to double count the first/last\n            # pixel\n            if isinstance(geom_array, (PolygonArray, MultiPolygonArray)):\n                # Convert polygon array to multi line of boundary\n                geom_array = geom_array.boundary\n                closed_rings = True\n            elif isinstance(geom_array, RingArray):\n                closed_rings = True\n            else:\n                closed_rings = False\n\n            perform_extend_cpu(\n                sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                geom_array, closed_rings, antialias_stage_2, *aggs_and_cols\n            )\n\n        return extend\n\n\nclass LineAxis1GeoPandas(_GeometryLike, _AntiAliasedLine):\n    # geopandas must be available for a GeoPandasLine to be created.\n    @property\n    def geom_dtypes(self):\n        from geopandas.array import GeometryDtype\n        return (GeometryDtype,)\n\n    @memoize\n    def _internal_build_extend(\n        self, x_mapper, y_mapper, info, append, line_width, antialias_stage_2,\n        antialias_stage_2_funcs,\n    ):\n        expand_aggs_and_cols = self.expand_aggs_and_cols(append)\n        draw_segment, antialias_stage_2_funcs = _line_internal_build_extend(\n            x_mapper, y_mapper, append, line_width, antialias_stage_2, antialias_stage_2_funcs,\n            expand_aggs_and_cols,\n        )\n        perform_extend_cpu = _build_extend_line_axis1_geopandas(\n            draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs,\n        )\n        geometry_name = self.geometry\n\n        def extend(aggs, df, vt, bounds, plot_start=True):\n            sx, tx, sy, ty = vt\n            xmin, xmax, ymin, ymax = bounds\n            aggs_and_cols = aggs + info(df, aggs[0].shape[:2])\n            geom_array = df[geometry_name].array\n\n            perform_extend_cpu(\n                sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                geom_array, antialias_stage_2, *aggs_and_cols,\n            )\n\n        return extend\n\n\nclass LinesXarrayCommonX(LinesAxis1):\n    def __init__(self, x, y, x_dim_index: int):\n        super().__init__(x, y)\n        self.x_dim_index = x_dim_index\n\n    def __hash__(self):\n        # This ensures that @memoize below caches different functions for different x_dim_index.\n        return hash((type(self), self.x_dim_index))\n\n    def compute_x_bounds(self, dataset):\n        bounds = self._compute_bounds(dataset[self.x])\n        return self.maybe_expand_bounds(bounds)\n\n    def compute_y_bounds(self, dataset):\n        bounds = self._compute_bounds(dataset[self.y])\n        return self.maybe_expand_bounds(bounds)\n\n    def compute_bounds_dask(self, xr_ds):\n        return self.compute_x_bounds(xr_ds), self.compute_y_bounds(xr_ds)\n\n    def validate(self, in_dshape):\n        if not isreal(in_dshape.measure[str(self.x)]):\n            raise ValueError('x column must be real')\n\n        if not isreal(in_dshape.measure[str(self.y)]):\n            raise ValueError('y column must be real')\n\n    @memoize\n    def _internal_build_extend(\n        self, x_mapper, y_mapper, info, append, line_width, antialias_stage_2,\n        antialias_stage_2_funcs,\n    ):\n        expand_aggs_and_cols = self.expand_aggs_and_cols(append)\n        draw_segment, antialias_stage_2_funcs = _line_internal_build_extend(\n            x_mapper, y_mapper, append, line_width, antialias_stage_2, antialias_stage_2_funcs,\n            expand_aggs_and_cols,\n        )\n        swap_dims = self.x_dim_index == 0\n        extend_cpu, extend_cuda = _build_extend_line_axis1_x_constant(\n            draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs, swap_dims,\n        )\n\n        x_name = self.x\n        y_name = self.y\n\n        def extend(aggs, df, vt, bounds, plot_start=True):\n            sx, tx, sy, ty = vt\n            xmin, xmax, ymin, ymax = bounds\n            aggs_and_cols = aggs + info(df, aggs[0].shape[:2])\n\n            if cudf and isinstance(df, cudf.DataFrame):\n                xs = cp.asarray(df[x_name])\n                ys = cp.asarray(df[y_name])\n                do_extend = extend_cuda[cuda_args(ys.shape)]\n            elif cp and isinstance(df[y_name].data, cp.ndarray):\n                xs = cp.asarray(df[x_name])\n                ys = df[y_name].data\n                shape = ys.shape[::-1] if swap_dims else ys.shape\n                do_extend = extend_cuda[cuda_args(shape)]\n            else:\n                xs = df[x_name].to_numpy()\n                ys = df[y_name].to_numpy()\n                do_extend = extend_cpu\n\n            do_extend(\n                sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, *aggs_and_cols\n            )\n\n        return extend\n\n\ndef _build_map_onto_pixel_for_line(x_mapper, y_mapper, want_antialias=False):\n    @ngjit\n    def map_onto_pixel_snap(sx, tx, sy, ty, xmin, xmax, ymin, ymax, x, y):\n        \"\"\"Map points onto pixel grid.\n\n        Points falling on upper bound are mapped into previous bin.\n\n        If the line has been clipped, x and y will have been\n        computed to lie on the bounds; we compare point and bounds\n        in integer space to avoid fp error. In contrast, with\n        auto-ranging, a point on the bounds will be the same\n        floating point number as the bound, so comparison in fp\n        representation of continuous space or in integer space\n        doesn't change anything.\n        \"\"\"\n        xx = int(x_mapper(x) * sx + tx)\n        yy = int(y_mapper(y) * sy + ty)\n\n        # Note that sx and tx were designed so that\n        # x_mapper(xmax) * sx + tx equals the width of the canvas in pixels\n        #\n        # Likewise, sy and ty were designed so that\n        # y_mapper(ymax) * sy + ty equals the height of the canvas in pixels\n        #\n        # We round these results to integers (rather than casting to integers\n        # with the int constructor) to handle cases where floating-point\n        # precision errors results in a value just under the integer number\n        # of pixels.\n        xxmax = round(x_mapper(xmax) * sx + tx)\n        yymax = round(y_mapper(ymax) * sy + ty)\n\n        return (xx - 1 if xx == xxmax else xx,\n                yy - 1 if yy == yymax else yy)\n\n    @ngjit\n    def map_onto_pixel_no_snap(sx, tx, sy, ty, xmin, xmax, ymin, ymax, x, y):\n        xx = x_mapper(x)*sx + tx - 0.5\n        yy = y_mapper(y)*sy + ty - 0.5\n        return xx, yy\n\n    if want_antialias:\n        return map_onto_pixel_no_snap\n    else:\n        return map_onto_pixel_snap\n\n\n@ngjit\ndef _liang_barsky(xmin, xmax, ymin, ymax, x0, x1, y0, y1, skip):\n    \"\"\" An implementation of the Liang-Barsky line clipping algorithm.\n\n    https://en.wikipedia.org/wiki/Liang%E2%80%93Barsky_algorithm\n\n    \"\"\"\n    # Check if line is fully outside viewport\n    if x0 < xmin and x1 < xmin:\n        skip = True\n    elif x0 > xmax and x1 > xmax:\n        skip = True\n    elif y0 < ymin and y1 < ymin:\n        skip = True\n    elif y0 > ymax and y1 > ymax:\n        skip = True\n\n    t0, t1 = 0, 1\n    dx1 = x1 - x0\n    t0, t1, accept = _clipt(-dx1, x0 - xmin, t0, t1)\n    if not accept:\n        skip = True\n    t0, t1, accept = _clipt(dx1, xmax - x0, t0, t1)\n    if not accept:\n        skip = True\n    dy1 = y1 - y0\n    t0, t1, accept = _clipt(-dy1, y0 - ymin, t0, t1)\n    if not accept:\n        skip = True\n    t0, t1, accept = _clipt(dy1, ymax - y0, t0, t1)\n    if not accept:\n        skip = True\n    if t1 < 1:\n        clipped_end = True\n        x1 = x0 + t1 * dx1\n        y1 = y0 + t1 * dy1\n    else:\n        clipped_end = False\n    if t0 > 0:\n        # If x0 is clipped, we need to plot the new start\n        clipped_start = True\n        x0 = x0 + t0 * dx1\n        y0 = y0 + t0 * dy1\n    else:\n        clipped_start = False\n\n    return x0, x1, y0, y1, skip, clipped_start, clipped_end\n\n\n@ngjit\ndef _clipt(p, q, t0, t1):\n    accept = True\n    if p < 0 and q < 0:\n        r = q / p\n        if r > t1:\n            accept = False\n        elif r > t0:\n            t0 = r\n    elif p > 0 and q < p:\n        r = q / p\n        if r < t0:\n            accept = False\n        elif r < t1:\n            t1 = r\n    elif q < 0:\n        accept = False\n    return t0, t1, accept\n\n\n@ngjit\ndef _clamp(x, low, high):\n    # Clamp ``x`` in the range ``low`` to ``high``.\n    return max(low, min(x, high))\n\n\n@ngjit\ndef _linearstep(edge0, edge1, x):\n    t = _clamp((x - edge0) / (edge1 - edge0), 0.0, 1.0)\n    return t\n\n\n@ngjit\ndef _x_intercept(y, cx0, cy0, cx1, cy1):\n    # Return x value of intercept between line at constant y and line\n    # between corner points.\n    if cy0 == cy1:\n        # Line is horizontal, return the \"upper\", i.e. right-hand, end of it.\n        return cx1\n    frac = (y - cy0) / (cy1 - cy0)  # In range 0..1\n    return cx0 + frac*(cx1 - cx0)\n\n\ndef _build_full_antialias(expand_aggs_and_cols):\n    \"\"\"Specialize antialiased line drawing algorithm for a given append/axis combination\"\"\"\n    @ngjit\n    @expand_aggs_and_cols\n    def _full_antialias(line_width, overwrite, i, x0, x1, y0, y1,\n                        segment_start, segment_end, xm, ym, append,\n                        nx, ny, buffer, *aggs_and_cols):\n        \"\"\"Draw an antialiased line segment.\n\n        If overwrite=True can overwrite each pixel multiple times because\n        using max for the overwriting.  If False can only write each pixel\n        once per segment and its previous segment.\n        Argument xm, ym are only valid if overwrite and segment_start are False.\n        \"\"\"\n        if x0 == x1 and y0 == y1:\n            return\n\n        # Scan occurs in y-direction. But wish to scan in the shortest direction,\n        # so if |x0-x1| < |y0-y1| then flip (x,y) coords for maths and flip back\n        # again before setting pixels.\n        flip_xy = abs(x0-x1) < abs(y0-y1)\n        if flip_xy:\n            x0, y0 = y0, x0\n            x1, y1 = y1, x1\n            xm, ym = ym, xm\n\n        scale = 1.0\n\n        # line_width less than 1 is rendered as 1 but with lower intensity.\n        if line_width < 1.0:\n            scale *= line_width\n            line_width = 1.0\n\n        aa = 1.0\n        halfwidth = 0.5*(line_width + aa)\n\n        # Want y0 <= y1, so switch vertical direction if this is not so.\n        flip_order = y1 < y0 or (y1 == y0 and x1 < x0)\n\n        # Start (x0, y0), end (y0, y1)\n        #       c1 +-------------+ c2          along    | right\n        # (x0, y0) | o         o | (x1, y1)    vector   | vector\n        #       c0 +-------------+ c3          ---->    v\n\n        alongx = float(x1 - x0)\n        alongy = float(y1 - y0)  # Always +ve\n        length = math.sqrt(alongx**2 + alongy**2)\n        alongx /= length\n        alongy /= length\n\n        rightx = alongy\n        righty = -alongx\n\n        # 4 corners, x and y.  Uses buffer, which must have length 8.  Order of coords is\n        # (x0, x1, x2, x3, y0, y1, y2, y3).  Each CPU/GPU thread has its own local buffer\n        # so there is no cross-talk.  Contents of buffer are written and read within the\n        # lifetime of this function, so it doesn't matter what they are before this\n        # function is called or after it returns.\n        if flip_order:\n            buffer[0] = x1 - halfwidth*( rightx - alongx)\n            buffer[1] = x1 - halfwidth*(-rightx - alongx)\n            buffer[2] = x0 - halfwidth*(-rightx + alongx)\n            buffer[3] = x0 - halfwidth*( rightx + alongx)\n            buffer[4] = y1 - halfwidth*( righty - alongy)\n            buffer[5] = y1 - halfwidth*(-righty - alongy)\n            buffer[6] = y0 - halfwidth*(-righty + alongy)\n            buffer[7] = y0 - halfwidth*( righty + alongy)\n        else:\n            buffer[0] = x0 + halfwidth*( rightx - alongx)\n            buffer[1] = x0 + halfwidth*(-rightx - alongx)\n            buffer[2] = x1 + halfwidth*(-rightx + alongx)\n            buffer[3] = x1 + halfwidth*( rightx + alongx)\n            buffer[4] = y0 + halfwidth*( righty - alongy)\n            buffer[5] = y0 + halfwidth*(-righty - alongy)\n            buffer[6] = y1 + halfwidth*(-righty + alongy)\n            buffer[7] = y1 + halfwidth*( righty + alongy)\n\n        xmax = nx-1\n        ymax = ny-1\n        if flip_xy:\n            xmax, ymax = ymax, xmax\n\n        # Index of lowest-y point.\n        if flip_order:\n            lowindex = 0 if x0 > x1 else 1\n        else:\n            lowindex = 0 if x1 > x0 else 1\n\n        if not overwrite and not segment_start:\n            prev_alongx = x0 - xm\n            prev_alongy = y0 - ym\n            prev_length = math.sqrt(prev_alongx**2 + prev_alongy**2)\n            if prev_length > 0.0:\n                prev_alongx /= prev_length\n                prev_alongy /= prev_length\n                prev_rightx = prev_alongy\n                prev_righty = -prev_alongx\n            else:\n                overwrite = True\n\n        # y limits of scan.\n        ystart = _clamp(math.ceil(buffer[4 + lowindex]), 0, ymax)\n        yend = _clamp(math.floor(buffer[4 + (lowindex+2) % 4]), 0, ymax)\n        # Need to know which edges are to left and right; both will change.\n        ll = lowindex  # Index of lower point of left edge.\n        lu = (ll + 1) % 4  # Index of upper point of left edge.\n        rl = lowindex  # Index of lower point of right edge.\n        ru = (rl + 3) % 4  # Index of upper point of right edge.\n        for y in range(ystart, yend+1):\n            if ll == lowindex and y > buffer[4 + lu]:\n                ll = lu\n                lu = (ll + 1) % 4\n            if rl == lowindex and y > buffer[4 + ru]:\n                rl = ru\n                ru = (rl + 3) % 4\n            # Find x limits of scan at this y.\n            xleft = _clamp(math.ceil(_x_intercept(\n                y, buffer[ll], buffer[4+ll], buffer[lu], buffer[4+lu])), 0, xmax)\n            xright = _clamp(math.floor(_x_intercept(\n                y, buffer[rl], buffer[4+rl], buffer[ru], buffer[4+ru])), 0, xmax)\n            for x in range(xleft, xright+1):\n                along = (x-x0)*alongx + (y-y0)*alongy  # dot product\n                prev_correction = False\n                if along < 0.0:\n                    # Before start of segment\n                    if overwrite or segment_start or (x-x0)*prev_alongx + (y-y0)*prev_alongy > 0.0:\n                        distance = math.sqrt((x-x0)**2 + (y-y0)**2)  # round join/end cap\n                    else:\n                        continue\n                elif along > length:\n                    # After end of segment\n                    if overwrite or segment_end:\n                        distance = math.sqrt((x-x1)**2 + (y-y1)**2)  # round join/end cap\n                    else:\n                        continue\n                else:\n                    # Within segment\n                    distance = abs((x-x0)*rightx + (y-y0)*righty)\n                    if not overwrite and not segment_start and \\\n                            -prev_length <= (x-x0)*prev_alongx + (y-y0)*prev_alongy <= 0.0 and \\\n                            abs((x-x0)*prev_rightx + (y-y0)*prev_righty) <= halfwidth:\n                        prev_correction = True\n                value = 1.0 - _linearstep(0.5*(line_width - aa), halfwidth, distance)\n                value *= scale\n                prev_value = 0.0\n                if prev_correction:\n                    # Already set pixel from previous segment, need to correct it\n                    prev_distance = abs((x-x0)*prev_rightx + (y-y0)*prev_righty)\n                    prev_value = 1.0 - _linearstep(0.5*(line_width - aa), halfwidth, prev_distance)\n                    prev_value *= scale\n                    if value <= prev_value:\n                        # Have already used a larger value (alpha) for this pixel.\n                        value = 0.0\n                if value > 0.0:\n                    xx, yy = (y, x) if flip_xy else (x, y)\n                    append(i, xx, yy, value, prev_value, *aggs_and_cols)\n\n    return _full_antialias\n\n\ndef _build_bresenham(expand_aggs_and_cols):\n    \"\"\"Specialize a bresenham kernel for a given append/axis combination\"\"\"\n    @ngjit\n    @expand_aggs_and_cols\n    def _bresenham(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax, segment_start,\n                   x0, x1, y0, y1, clipped, append, *aggs_and_cols):\n        \"\"\"Draw a line segment using Bresenham's algorithm\n        This method plots a line segment with integer coordinates onto a pixel\n        grid.\n        \"\"\"\n        dx = x1 - x0\n        ix = (dx > 0) - (dx < 0)\n        dx = abs(dx) * 2\n\n        dy = y1 - y0\n        iy = (dy > 0) - (dy < 0)\n        dy = abs(dy) * 2\n\n        # If vertices weren't clipped and are concurrent in integer space,\n        # call append and return, so that the second vertex won't be hit below.\n        if not clipped and not (dx | dy):\n            append(i, x0, y0, *aggs_and_cols)\n            return\n\n        if segment_start:\n            append(i, x0, y0, *aggs_and_cols)\n\n        if dx >= dy:\n            error = 2 * dy - dx\n            while x0 != x1:\n                if error >= 0 and (error or ix > 0):\n                    error -= 2 * dx\n                    y0 += iy\n                error += 2 * dy\n                x0 += ix\n                append(i, x0, y0, *aggs_and_cols)\n        else:\n            error = 2 * dx - dy\n            while y0 != y1:\n                if error >= 0 and (error or iy > 0):\n                    error -= 2 * dy\n                    x0 += ix\n                error += 2 * dx\n                y0 += iy\n                append(i, x0, y0, *aggs_and_cols)\n    return _bresenham\n\ndef _build_draw_segment(append, map_onto_pixel, expand_aggs_and_cols, line_width, overwrite):\n    \"\"\"Specialize a line plotting kernel for a given append/axis combination\"\"\"\n\n    if line_width > 0.0:\n        _bresenham = None\n        _full_antialias = _build_full_antialias(expand_aggs_and_cols)\n    else:\n        _bresenham = _build_bresenham(expand_aggs_and_cols)\n        _full_antialias = None\n\n    @ngjit\n    @expand_aggs_and_cols\n    def draw_segment(\n            i, sx, tx, sy, ty, xmin, xmax, ymin, ymax, segment_start, segment_end,\n            x0, x1, y0, y1, xm, ym, buffer, *aggs_and_cols\n    ):\n        # xm, ym are only valid if segment_start is True.\n\n        # buffer is a length-8 float64 array if antialiasing is to be used,\n        # or None otherwise. It is allocated in the appropriate extend_cpu or\n        # extend_cuda function so that it is of the correct type (numpy or\n        # cupy) and that there is one per CPU/GPU thread.\n\n        # NOTE: The slightly bizarre variable versioning herein for variables\n        # x0, y0, y0, y1 is to deal with Numba not having SSA form prior to\n        # version 0.49.0. The result of lack of SSA is that the type inference\n        # algorithms would widen types that are multiply defined as would be the\n        # case in code such as `x, y = function(x, y)` if the function returned\n        # a wider type for x, y then the input x, y.\n        skip = False\n\n        # If any of the coordinates are NaN, there's a discontinuity.\n        # Skip the entire segment.\n        if isnull(x0) or isnull(y0) or isnull(x1) or isnull(y1):\n            skip = True\n        # Use Liang-Barsky to clip the segment to a bounding box\n        x0_1, x1_1, y0_1, y1_1, skip, clipped_start, clipped_end = \\\n            _liang_barsky(xmin, xmax, ymin, ymax, x0, x1, y0, y1, skip)\n\n        if not skip:\n            clipped = clipped_start or clipped_end\n            segment_start = segment_start or clipped_start\n            x0_2, y0_2 = map_onto_pixel(\n                sx, tx, sy, ty, xmin, xmax, ymin, ymax, x0_1, y0_1\n            )\n            x1_2, y1_2 = map_onto_pixel(\n                sx, tx, sy, ty, xmin, xmax, ymin, ymax, x1_1, y1_1\n            )\n            if line_width > 0.0:\n                if segment_start:\n                    xm_2 = ym_2 = 0.0\n                else:\n                    xm_2, ym_2 = map_onto_pixel(\n                        sx, tx, sy, ty, xmin, xmax, ymin, ymax, xm, ym)\n                nx = round((xmax - xmin)*sx)\n                ny = round((ymax - ymin)*sy)\n                _full_antialias(line_width, overwrite, i, x0_2, x1_2, y0_2, y1_2,\n                                segment_start, segment_end, xm_2, ym_2, append,\n                                nx, ny, buffer, *aggs_and_cols)\n            else:\n                _bresenham(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                           segment_start, x0_2, x1_2, y0_2, y1_2,\n                           clipped, append, *aggs_and_cols)\n\n    return draw_segment\n\ndef _build_extend_line_axis0(draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs):\n    use_2_stage_agg = antialias_stage_2_funcs is not None\n\n    @ngjit\n    @expand_aggs_and_cols\n    def perform_extend_line(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                            plot_start, xs, ys, buffer, *aggs_and_cols):\n        x0 = xs[i]\n        y0 = ys[i]\n        x1 = xs[i + 1]\n        y1 = ys[i + 1]\n        segment_start = (plot_start if i == 0 else\n                         (isnull(xs[i - 1]) or isnull(ys[i - 1])))\n\n        segment_end = (i == len(xs)-2) or isnull(xs[i+2]) or isnull(ys[i+2])\n\n        if segment_start or use_2_stage_agg:\n            xm = 0.0\n            ym = 0.0\n        else:\n            xm = xs[i-1]\n            ym = ys[i-1]\n\n        draw_segment(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                     segment_start, segment_end, x0, x1, y0, y1,\n                     xm, ym, buffer, *aggs_and_cols)\n\n    @ngjit\n    @expand_aggs_and_cols\n    def extend_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                   xs, ys, plot_start, antialias_stage_2, *aggs_and_cols):\n        \"\"\"Aggregate along a line formed by ``xs`` and ``ys``\"\"\"\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n        nrows = xs.shape[0]\n        for i in range(nrows - 1):\n            perform_extend_line(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                                plot_start, xs, ys, buffer, *aggs_and_cols)\n\n    @cuda.jit\n    @expand_aggs_and_cols\n    def extend_cuda(sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                    xs, ys, plot_start, antialias_stage_2, *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = cuda.local.array(8, nb_types.float64) if antialias else None\n        i = cuda.grid(1)\n        if i < xs.shape[0] - 1:\n            perform_extend_line(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                                plot_start, xs, ys, buffer, *aggs_and_cols)\n\n    return extend_cpu, extend_cuda\n\n\ndef _build_extend_line_axis0_multi(draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs):\n    if antialias_stage_2_funcs is not None:\n        aa_stage_2_accumulate, aa_stage_2_clear, aa_stage_2_copy_back = antialias_stage_2_funcs\n    use_2_stage_agg = antialias_stage_2_funcs is not None\n\n    @ngjit\n    @expand_aggs_and_cols\n    def perform_extend_line(i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                            plot_start, xs, ys, buffer, *aggs_and_cols):\n        x0 = xs[i, j]\n        y0 = ys[i, j]\n        x1 = xs[i + 1, j]\n        y1 = ys[i + 1, j]\n        segment_start = (plot_start if i == 0 else\n                         (isnull(xs[i - 1, j]) or isnull(ys[i - 1, j])))\n\n        segment_end = (i == len(xs)-2) or isnull(xs[i+2, j]) or isnull(ys[i+2, j])\n\n        if segment_start or use_2_stage_agg:\n            xm = 0.0\n            ym = 0.0\n        else:\n            xm = xs[i-1, j]\n            ym = ys[i-1, j]\n\n        draw_segment(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                     segment_start, segment_end, x0, x1, y0, y1,\n                     xm, ym, buffer, *aggs_and_cols)\n\n    @ngjit\n    @expand_aggs_and_cols\n    def extend_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys,\n                   plot_start, antialias_stage_2, *aggs_and_cols):\n        \"\"\"Aggregate along a line formed by ``xs`` and ``ys``\"\"\"\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n        nrows, ncols = xs.shape\n\n        for j in range(ncols):\n            for i in range(nrows - 1):\n                perform_extend_line(i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                                    plot_start, xs, ys, buffer, *aggs_and_cols)\n\n    def extend_cpu_antialias_2agg(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys,\n                                  plot_start, antialias_stage_2, *aggs_and_cols):\n        \"\"\"Aggregate along a line formed by ``xs`` and ``ys``\"\"\"\n        n_aggs = len(antialias_stage_2[0])\n        aggs_and_accums = tuple((agg, agg.copy()) for agg in aggs_and_cols[:n_aggs])\n        cpu_antialias_2agg_impl(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys,\n                                plot_start, antialias_stage_2, aggs_and_accums, *aggs_and_cols)\n\n    @ngjit\n    @expand_aggs_and_cols\n    def cpu_antialias_2agg_impl(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys,\n                                plot_start, antialias_stage_2, aggs_and_accums, *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n\n        nrows, ncols = xs.shape\n        for j in range(ncols):\n            for i in range(nrows - 1):\n                perform_extend_line(i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                                    plot_start, xs, ys, buffer, *aggs_and_cols)\n\n            if ncols == 1:\n                return\n\n            aa_stage_2_accumulate(aggs_and_accums, j==0)\n\n            if j < ncols - 1:\n                aa_stage_2_clear(aggs_and_accums)\n\n        aa_stage_2_copy_back(aggs_and_accums)\n\n\n    @cuda.jit\n    @expand_aggs_and_cols\n    def extend_cuda(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys,\n                    plot_start, antialias_stage_2, *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = cuda.local.array(8, nb_types.float64) if antialias else None\n        i, j = cuda.grid(2)\n        if i < xs.shape[0] - 1 and j < xs.shape[1]:\n            perform_extend_line(i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                                plot_start, xs, ys, buffer, *aggs_and_cols)\n\n    if use_2_stage_agg:\n        return extend_cpu_antialias_2agg, extend_cuda\n    else:\n        return extend_cpu, extend_cuda\n\n\ndef _build_extend_line_axis1_none_constant(draw_segment, expand_aggs_and_cols,\n                                           antialias_stage_2_funcs):\n    if antialias_stage_2_funcs is not None:\n        aa_stage_2_accumulate, aa_stage_2_clear, aa_stage_2_copy_back = antialias_stage_2_funcs\n    use_2_stage_agg = antialias_stage_2_funcs is not None\n\n    @ngjit\n    @expand_aggs_and_cols\n    def perform_extend_line(\n            i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n            xs, ys, buffer, *aggs_and_cols\n    ):\n        x0 = xs[i, j]\n        y0 = ys[i, j]\n        x1 = xs[i, j + 1]\n        y1 = ys[i, j + 1]\n        segment_start = (\n                (j == 0) or isnull(xs[i, j - 1]) or isnull(ys[i, j - 1])\n        )\n\n        segment_end = (j == xs.shape[1]-2) or isnull(xs[i, j+2]) or isnull(ys[i, j+2])\n\n        if segment_start or use_2_stage_agg:\n            xm = 0.0\n            ym = 0.0\n        else:\n            xm = xs[i, j-1]\n            ym = ys[i, j-1]\n\n        draw_segment(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                     segment_start, segment_end, x0, x1, y0, y1,\n                     xm, ym, buffer, *aggs_and_cols)\n\n    @ngjit\n    @expand_aggs_and_cols\n    def extend_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2,\n                   *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n        ncols = xs.shape[1]\n        for i in range(xs.shape[0]):\n            for j in range(ncols - 1):\n                perform_extend_line(\n                    i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                    xs, ys, buffer, *aggs_and_cols\n                )\n\n    def extend_cpu_antialias_2agg(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys,\n                                  antialias_stage_2, *aggs_and_cols):\n        n_aggs = len(antialias_stage_2[0])\n        aggs_and_accums = tuple((agg, agg.copy()) for agg in aggs_and_cols[:n_aggs])\n        cpu_antialias_2agg_impl(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys,\n                                antialias_stage_2, aggs_and_accums, *aggs_and_cols)\n\n    @ngjit\n    @expand_aggs_and_cols\n    def cpu_antialias_2agg_impl(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys,\n                                antialias_stage_2, aggs_and_accums, *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n\n        ncols = xs.shape[1]\n        for i in range(xs.shape[0]):\n            for j in range(ncols - 1):\n                perform_extend_line(i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                                    xs, ys, buffer, *aggs_and_cols)\n\n            if xs.shape[0] == 1:\n                return\n\n            aa_stage_2_accumulate(aggs_and_accums, i==0)\n\n            if i < xs.shape[0] - 1:\n                aa_stage_2_clear(aggs_and_accums)\n\n        aa_stage_2_copy_back(aggs_and_accums)\n\n    @cuda.jit\n    @expand_aggs_and_cols\n    def extend_cuda(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2,\n                    *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = cuda.local.array(8, nb_types.float64) if antialias else None\n        i, j = cuda.grid(2)\n        if i < xs.shape[0] and j < xs.shape[1] - 1:\n            perform_extend_line(\n                i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys,\n                buffer, *aggs_and_cols\n            )\n\n    if use_2_stage_agg:\n        return extend_cpu_antialias_2agg, extend_cuda\n    else:\n        return extend_cpu, extend_cuda\n\n\ndef _build_extend_line_axis1_x_constant(draw_segment, expand_aggs_and_cols,\n                                        antialias_stage_2_funcs, swap_dims: bool = False):\n    if antialias_stage_2_funcs is not None:\n        aa_stage_2_accumulate, aa_stage_2_clear, aa_stage_2_copy_back = antialias_stage_2_funcs\n    use_2_stage_agg = antialias_stage_2_funcs is not None\n\n    @ngjit\n    @expand_aggs_and_cols\n    def perform_extend_line(\n            i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, buffer, *aggs_and_cols\n    ):\n        x0 = xs[j]\n        x1 = xs[j + 1]\n        if swap_dims:\n            y0 = ys[j, i]\n            y1 = ys[j + 1, i]\n            segment_start = (j == 0) or isnull(xs[j - 1]) or isnull(ys[j - 1, i])\n            segment_end = (j == len(xs)-2) or isnull(xs[j+2]) or isnull(ys[j+2, i])\n        else:\n            y0 = ys[i, j]\n            y1 = ys[i, j + 1]\n            segment_start = (j == 0) or isnull(xs[j - 1]) or isnull(ys[i, j - 1])\n            segment_end = (j == len(xs)-2) or isnull(xs[j+2]) or isnull(ys[i, j+2])\n\n        if segment_start or use_2_stage_agg:\n            xm = 0.0\n            ym = 0.0\n        else:\n            xm = xs[j-1]\n            ym = ys[j-1, i] if swap_dims else ys[i, j-1]\n\n        draw_segment(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                     segment_start, segment_end, x0, x1, y0, y1,\n                     xm, ym, buffer, *aggs_and_cols)\n\n    @ngjit\n    @expand_aggs_and_cols\n    def extend_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2,\n                   *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n        ncols, nrows = ys.shape if swap_dims else ys.shape[::-1]\n        for i in range(nrows):\n            for j in range(ncols - 1):\n                perform_extend_line(\n                    i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, buffer, *aggs_and_cols\n                )\n\n    def extend_cpu_antialias_2agg(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys,\n                                  antialias_stage_2, *aggs_and_cols):\n        n_aggs = len(antialias_stage_2[0])\n        aggs_and_accums = tuple((agg, agg.copy()) for agg in aggs_and_cols[:n_aggs])\n        cpu_antialias_2agg_impl(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys,\n                                antialias_stage_2, aggs_and_accums, *aggs_and_cols)\n\n    @ngjit\n    @expand_aggs_and_cols\n    def cpu_antialias_2agg_impl(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys,\n                                antialias_stage_2, aggs_and_accums, *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n\n        ncols = ys.shape[1]\n        for i in range(ys.shape[0]):\n            for j in range(ncols - 1):\n                perform_extend_line(\n                    i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys,\n                    buffer, *aggs_and_cols\n                )\n\n            if ys.shape[0] == 1:\n                return\n\n            aa_stage_2_accumulate(aggs_and_accums, i==0)\n\n            if i < ys.shape[0] - 1:\n                aa_stage_2_clear(aggs_and_accums)\n\n        aa_stage_2_copy_back(aggs_and_accums)\n\n    @cuda.jit\n    @expand_aggs_and_cols\n    def extend_cuda(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2,\n                    *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = cuda.local.array(8, nb_types.float64) if antialias else None\n        i, j = cuda.grid(2)\n        ncols, nrows = ys.shape if swap_dims else ys.shape[::-1]\n        if i < nrows and j < ncols - 1:\n            perform_extend_line(\n                i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, buffer, *aggs_and_cols\n            )\n\n    if use_2_stage_agg:\n        return extend_cpu_antialias_2agg, extend_cuda\n    else:\n        return extend_cpu, extend_cuda\n\n\ndef _build_extend_line_axis1_y_constant(draw_segment, expand_aggs_and_cols,\n                                        antialias_stage_2_funcs):\n    if antialias_stage_2_funcs is not None:\n        aa_stage_2_accumulate, aa_stage_2_clear, aa_stage_2_copy_back = antialias_stage_2_funcs\n    use_2_stage_agg = antialias_stage_2_funcs is not None\n\n    @ngjit\n    @expand_aggs_and_cols\n    def perform_extend_line(\n            i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, buffer, *aggs_and_cols\n    ):\n        x0 = xs[i, j]\n        y0 = ys[j]\n        x1 = xs[i, j + 1]\n        y1 = ys[j + 1]\n\n        segment_start = (\n                (j == 0) or isnull(xs[i, j - 1]) or isnull(ys[j - 1])\n        )\n\n        segment_end = (j == len(ys)-2) or isnull(xs[i, j+2]) or isnull(ys[j+2])\n\n        if segment_start or use_2_stage_agg:\n            xm = 0.0\n            ym = 0.0\n        else:\n            xm = xs[i, j-1]\n            ym = ys[j-1]\n\n        draw_segment(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                     segment_start, segment_end, x0, x1, y0, y1,\n                     xm, ym, buffer, *aggs_and_cols)\n\n    @ngjit\n    @expand_aggs_and_cols\n    def extend_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2,\n                   *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n        ncols = xs.shape[1]\n        for i in range(xs.shape[0]):\n            for j in range(ncols - 1):\n                perform_extend_line(\n                    i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                    xs, ys, buffer, *aggs_and_cols\n                )\n\n    def extend_cpu_antialias_2agg(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys,\n                                  antialias_stage_2, *aggs_and_cols):\n        n_aggs = len(antialias_stage_2[0])\n        aggs_and_accums = tuple((agg, agg.copy()) for agg in aggs_and_cols[:n_aggs])\n        cpu_antialias_2agg_impl(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys,\n                                antialias_stage_2, aggs_and_accums, *aggs_and_cols)\n\n    @ngjit\n    @expand_aggs_and_cols\n    def cpu_antialias_2agg_impl(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys,\n                                antialias_stage_2, aggs_and_accums, *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n\n        ncols = xs.shape[1]\n        for i in range(xs.shape[0]):\n\n            for j in range(ncols - 1):\n                perform_extend_line(\n                    i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                    xs, ys, buffer, *aggs_and_cols\n                )\n\n            if xs.shape[0] == 1:\n                return\n\n            aa_stage_2_accumulate(aggs_and_accums, i==0)\n\n            if i < xs.shape[0] - 1:\n                aa_stage_2_clear(aggs_and_accums)\n\n        aa_stage_2_copy_back(aggs_and_accums)\n\n    @cuda.jit\n    @expand_aggs_and_cols\n    def extend_cuda(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2,\n                    *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = cuda.local.array(8, nb_types.float64) if antialias else None\n        i, j = cuda.grid(2)\n        if i < xs.shape[0] and j < xs.shape[1] - 1:\n            perform_extend_line(\n                i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                xs, ys, buffer, *aggs_and_cols\n            )\n\n    if use_2_stage_agg:\n        return extend_cpu_antialias_2agg, extend_cuda\n    else:\n        return extend_cpu, extend_cuda\n\n\ndef _build_extend_line_axis1_ragged(draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs):\n    if antialias_stage_2_funcs is not None:\n        aa_stage_2_accumulate, aa_stage_2_clear, aa_stage_2_copy_back = antialias_stage_2_funcs\n    use_2_stage_agg = antialias_stage_2_funcs is not None\n\n    def extend_cpu(\n            sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, *aggs_and_cols\n    ):\n        x_start_i = xs.start_indices\n        x_flat = xs.flat_array\n\n        y_start_i = ys.start_indices\n        y_flat = ys.flat_array\n\n        extend_cpu_numba(\n            sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n            x_start_i, x_flat, y_start_i, y_flat, antialias_stage_2, *aggs_and_cols\n        )\n\n    @ngjit\n    @expand_aggs_and_cols\n    def extend_cpu_numba(\n            sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n            x_start_i, x_flat, y_start_i, y_flat, antialias_stage_2, *aggs_and_cols\n    ):\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n        nrows = len(x_start_i)\n        x_flat_len = len(x_flat)\n        y_flat_len = len(y_flat)\n\n        for i in range(nrows):\n            # Get x index range\n            x_start_index = x_start_i[i]\n            x_stop_index = (x_start_i[i + 1]\n                            if i < nrows - 1\n                            else x_flat_len)\n\n            # Get y index range\n            y_start_index = y_start_i[i]\n            y_stop_index = (y_start_i[i + 1]\n                            if i < nrows - 1\n                            else y_flat_len)\n\n            # Find line segment length as shorter of the two segments\n            segment_len = min(x_stop_index - x_start_index,\n                              y_stop_index - y_start_index)\n\n            for j in range(segment_len - 1):\n\n                x0 = x_flat[x_start_index + j]\n                y0 = y_flat[y_start_index + j]\n                x1 = x_flat[x_start_index + j + 1]\n                y1 = y_flat[y_start_index + j + 1]\n\n                segment_start = (\n                        (j == 0) or\n                        isnull(x_flat[x_start_index + j - 1]) or\n                        isnull(y_flat[y_start_index + j - 1])\n                )\n\n                segment_end = (\n                        (j == segment_len-2) or\n                        isnull(x_flat[x_start_index + j + 2]) or\n                        isnull(y_flat[y_start_index + j + 2])\n                )\n\n                if segment_start or use_2_stage_agg:\n                    xm = 0.0\n                    ym = 0.0\n                else:\n                    xm = x_flat[x_start_index + j - 1]\n                    ym = y_flat[y_start_index + j - 1]\n\n                draw_segment(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                             segment_start, segment_end, x0, x1, y0, y1,\n                             xm, ym, buffer, *aggs_and_cols)\n\n    def extend_cpu_antialias_2agg(\n            sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, *aggs_and_cols\n    ):\n        x_start_i = xs.start_indices\n        x_flat = xs.flat_array\n\n        y_start_i = ys.start_indices\n        y_flat = ys.flat_array\n\n        n_aggs = len(antialias_stage_2[0])\n        aggs_and_accums = tuple((agg, agg.copy()) for agg in aggs_and_cols[:n_aggs])\n\n        extend_cpu_numba_antialias_2agg(\n            sx, tx, sy, ty, xmin, xmax, ymin, ymax, x_start_i, x_flat,\n            y_start_i, y_flat, antialias_stage_2, aggs_and_accums, *aggs_and_cols\n        )\n\n    @ngjit\n    @expand_aggs_and_cols\n    def extend_cpu_numba_antialias_2agg(\n            sx, tx, sy, ty, xmin, xmax, ymin, ymax, x_start_i, x_flat,\n            y_start_i, y_flat, antialias_stage_2, aggs_and_accums, *aggs_and_cols\n    ):\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n\n        nrows = len(x_start_i)\n        x_flat_len = len(x_flat)\n        y_flat_len = len(y_flat)\n\n        for i in range(nrows):\n            # Get x index range\n            x_start_index = x_start_i[i]\n            x_stop_index = (x_start_i[i + 1]\n                            if i < nrows - 1\n                            else x_flat_len)\n\n            # Get y index range\n            y_start_index = y_start_i[i]\n            y_stop_index = (y_start_i[i + 1]\n                            if i < nrows - 1\n                            else y_flat_len)\n\n            # Find line segment length as shorter of the two segments\n            segment_len = min(x_stop_index - x_start_index,\n                              y_stop_index - y_start_index)\n\n            for j in range(segment_len - 1):\n\n                x0 = x_flat[x_start_index + j]\n                y0 = y_flat[y_start_index + j]\n                x1 = x_flat[x_start_index + j + 1]\n                y1 = y_flat[y_start_index + j + 1]\n\n                segment_start = (\n                        (j == 0) or\n                        isnull(x_flat[x_start_index + j - 1]) or\n                        isnull(y_flat[y_start_index + j - 1])\n                )\n\n                segment_end = (\n                        (j == segment_len-2) or\n                        isnull(x_flat[x_start_index + j + 2]) or\n                        isnull(y_flat[y_start_index + j + 2])\n                )\n\n                draw_segment(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                             segment_start, segment_end, x0, x1, y0, y1,\n                             0.0, 0.0, buffer, *aggs_and_cols)\n\n            if nrows == 1:\n                return\n\n            aa_stage_2_accumulate(aggs_and_accums, i==0)\n\n            if i < nrows - 1:\n                aa_stage_2_clear(aggs_and_accums)\n\n        aa_stage_2_copy_back(aggs_and_accums)\n\n    if use_2_stage_agg:\n        return extend_cpu_antialias_2agg\n    else:\n        return extend_cpu\n\n\ndef _build_extend_line_axis1_geometry(draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs):\n    if antialias_stage_2_funcs is not None:\n        aa_stage_2_accumulate, aa_stage_2_clear, aa_stage_2_copy_back = antialias_stage_2_funcs\n    use_2_stage_agg = antialias_stage_2_funcs is not None\n\n    def extend_cpu(\n            sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n            geometry, closed_rings, antialias_stage_2, *aggs_and_cols\n    ):\n        values = geometry.buffer_values\n        missing = geometry.isna()\n        offsets = geometry.buffer_offsets\n\n        if len(offsets) == 2:\n            # MultiLineArray\n            offsets0, offsets1 = offsets\n        else:\n            # LineArray\n            offsets1 = offsets[0]\n            offsets0 = np.arange(len(offsets1))\n\n        if geometry._sindex is not None:\n            # Compute indices of potentially intersecting polygons using\n            # geometry's R-tree if there is one\n            eligible_inds = geometry.sindex.intersects((xmin, ymin, xmax, ymax))\n        else:\n            # Otherwise, process all indices\n            eligible_inds = np.arange(0, len(geometry), dtype='uint32')\n\n        extend_cpu_numba(\n            sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n            values, missing, offsets0, offsets1, eligible_inds,\n            closed_rings, antialias_stage_2, *aggs_and_cols\n        )\n\n    @ngjit\n    @expand_aggs_and_cols\n    def extend_cpu_numba(\n            sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n            values, missing, offsets0, offsets1, eligible_inds,\n            closed_rings, antialias_stage_2, *aggs_and_cols\n    ):\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n        for i in eligible_inds:\n            if missing[i]:\n                continue\n\n            start0 = offsets0[i]\n            stop0 = offsets0[i + 1]\n\n            for j in range(start0, stop0):\n                start1 = offsets1[j]\n                stop1 = offsets1[j + 1]\n\n                for k in range(start1, stop1 - 2, 2):\n                    x0 = values[k]\n                    if not np.isfinite(x0):\n                        continue\n\n                    y0 = values[k + 1]\n                    if not np.isfinite(y0):\n                        continue\n\n                    x1 = values[k + 2]\n                    if not np.isfinite(x1):\n                        continue\n\n                    y1 = values[k + 3]\n                    if not np.isfinite(y1):\n                        continue\n\n                    segment_start = (\n                            (k == start1 and not closed_rings) or\n                            (k > start1 and\n                             (not np.isfinite(values[k - 2]) or not np.isfinite(values[k - 1])))\n                    )\n\n                    segment_end = (\n                            (not closed_rings and k == stop1-4) or\n                            (k < stop1-4 and\n                             (not np.isfinite(values[k + 4]) or not np.isfinite(values[k + 5])))\n                    )\n\n                    if segment_start or use_2_stage_agg:\n                        xm = 0.0\n                        ym = 0.0\n                    elif k == start1 and closed_rings:\n                        xm = values[stop1-4]\n                        ym = values[stop1-3]\n                    else:\n                        xm = values[k-2]\n                        ym = values[k-1]\n\n                    draw_segment(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                                 segment_start, segment_end, x0, x1, y0, y1,\n                                 xm, ym, buffer, *aggs_and_cols)\n\n    def extend_cpu_antialias_2agg(\n            sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n            geometry, closed_rings, antialias_stage_2, *aggs_and_cols\n    ):\n        values = geometry.buffer_values\n        missing = geometry.isna()\n        offsets = geometry.buffer_offsets\n\n        if len(offsets) == 2:\n            # MultiLineArray\n            offsets0, offsets1 = offsets\n        else:\n            # LineArray\n            offsets1 = offsets[0]\n            offsets0 = np.arange(len(offsets1))\n\n        if geometry._sindex is not None:\n            # Compute indices of potentially intersecting polygons using\n            # geometry's R-tree if there is one\n            eligible_inds = geometry.sindex.intersects((xmin, ymin, xmax, ymax))\n        else:\n            # Otherwise, process all indices\n            eligible_inds = np.arange(0, len(geometry), dtype='uint32')\n\n        n_aggs = len(antialias_stage_2[0])\n        aggs_and_accums = tuple((agg, agg.copy()) for agg in aggs_and_cols[:n_aggs])\n\n        extend_cpu_numba_antialias_2agg(\n            sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n            values, missing, offsets0, offsets1, eligible_inds,\n            closed_rings, antialias_stage_2, aggs_and_accums, *aggs_and_cols\n        )\n\n    @ngjit\n    @expand_aggs_and_cols\n    def extend_cpu_numba_antialias_2agg(\n            sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n            values, missing, offsets0, offsets1, eligible_inds,\n            closed_rings, antialias_stage_2, aggs_and_accums, *aggs_and_cols\n    ):\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n\n        first_pass = True\n        for i in eligible_inds:\n            if missing[i]:\n                continue\n\n            start0 = offsets0[i]\n            stop0 = offsets0[i + 1]\n\n            for j in range(start0, stop0):\n                start1 = offsets1[j]\n                stop1 = offsets1[j + 1]\n\n                for k in range(start1, stop1 - 2, 2):\n                    x0 = values[k]\n                    if not np.isfinite(x0):\n                        continue\n\n                    y0 = values[k + 1]\n                    if not np.isfinite(y0):\n                        continue\n\n                    x1 = values[k + 2]\n                    if not np.isfinite(x1):\n                        continue\n\n                    y1 = values[k + 3]\n                    if not np.isfinite(y1):\n                        continue\n\n                    segment_start = (\n                            (k == start1 and not closed_rings) or\n                            (k > start1 and\n                             (not np.isfinite(values[k - 2]) or not np.isfinite(values[k - 1])))\n                    )\n\n                    segment_end = (\n                            (not closed_rings and k == stop1-4) or\n                            (k < stop1-4 and\n                             (not np.isfinite(values[k + 4]) or not np.isfinite(values[k + 5])))\n                    )\n\n                    draw_segment(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                                 segment_start, segment_end, x0, x1, y0, y1,\n                                 0.0, 0.0, buffer, *aggs_and_cols)\n\n            aa_stage_2_accumulate(aggs_and_accums, first_pass)\n            first_pass = False\n            aa_stage_2_clear(aggs_and_accums)\n\n        aa_stage_2_copy_back(aggs_and_accums)\n\n    if use_2_stage_agg:\n        return extend_cpu_antialias_2agg\n    else:\n        return extend_cpu\n\n\ndef _build_extend_line_axis1_geopandas(draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs):\n    if antialias_stage_2_funcs is not None:\n        aa_stage_2_accumulate, aa_stage_2_clear, aa_stage_2_copy_back = antialias_stage_2_funcs\n    use_2_stage_agg = antialias_stage_2_funcs is not None\n\n    # Lazy import shapely. Cannot get here if geopandas and shapely are not available.\n    import shapely\n\n    def _process_geometry(geometry):\n        ragged = shapely.to_ragged_array(geometry)\n        geometry_type = ragged[0]\n\n        if geometry_type not in (\n            shapely.GeometryType.LINESTRING, shapely.GeometryType.MULTILINESTRING,\n            shapely.GeometryType.MULTIPOLYGON, shapely.GeometryType.POLYGON,\n        ):\n            raise ValueError(\n                \"Canvas.line supports GeoPandas geometry types of LINESTRING, MULTILINESTRING, \"\n                f\"MULTIPOLYGON and POLYGON, not {repr(geometry_type)}\")\n\n        coords = ragged[1].ravel()\n\n        # Use type to decide whether geometry represents closed line loops or open list strips.\n        # Skip the last point for closed geometries so as not to double count the first/last point.\n        if geometry_type == shapely.GeometryType.LINESTRING:\n            offsets = ragged[2][0]\n            outer_offsets = np.arange(len(offsets))\n            closed_rings = False\n        elif geometry_type == shapely.GeometryType.MULTILINESTRING:\n            offsets, outer_offsets = ragged[2]\n            closed_rings = False\n        elif geometry_type == shapely.GeometryType.MULTIPOLYGON:\n            offsets, temp_offsets, outer_offsets = ragged[2]\n            outer_offsets = temp_offsets[outer_offsets]\n            closed_rings = True\n        else:  # geometry_type == shapely.GeometryType.POLYGON:\n            offsets, outer_offsets = ragged[2]\n            closed_rings = True\n\n        return coords, offsets, outer_offsets, closed_rings\n\n    def extend_cpu(\n        sx, tx, sy, ty, xmin, xmax, ymin, ymax, geometry, antialias_stage_2, *aggs_and_cols\n    ):\n        coords, offsets, outer_offsets, closed_rings = _process_geometry(geometry)\n        extend_cpu_numba(\n            sx, tx, sy, ty, xmin, xmax, ymin, ymax, coords, offsets, outer_offsets, closed_rings,\n            antialias_stage_2, *aggs_and_cols)\n\n    @ngjit\n    @expand_aggs_and_cols\n    def extend_cpu_numba(\n            sx, tx, sy, ty, xmin, xmax, ymin, ymax, values, offsets, outer_offsets,\n            closed_rings, antialias_stage_2, *aggs_and_cols\n    ):\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n        n_multilines = len(outer_offsets) - 1\n        for i in range(n_multilines):\n            start0 = outer_offsets[i]\n            stop0 = outer_offsets[i + 1]\n\n            for j in range(start0, stop0):\n                start1 = offsets[j]\n                stop1 = offsets[j + 1]\n\n                for k in range(2*start1, 2*stop1 - 2, 2):\n                    x0 = values[k]\n                    y0 = values[k + 1]\n                    x1 = values[k + 2]\n                    y1 = values[k + 3]\n                    if not (np.isfinite(x0) and np.isfinite(y0) and\n                            np.isfinite(x1) and np.isfinite(y1)):\n                        continue\n\n                    segment_start = (\n                            (k == start1 and not closed_rings) or\n                            (k > start1 and\n                             (not np.isfinite(values[k - 2]) or not np.isfinite(values[k - 1])))\n                    )\n\n                    segment_end = (\n                            (not closed_rings and k == stop1-4) or\n                            (k < stop1-4 and\n                             (not np.isfinite(values[k + 4]) or not np.isfinite(values[k + 5])))\n                    )\n\n                    if segment_start or use_2_stage_agg:\n                        xm = 0.0\n                        ym = 0.0\n                    elif k == start1 and closed_rings:\n                        xm = values[stop1-4]\n                        ym = values[stop1-3]\n                    else:\n                        xm = values[k-2]\n                        ym = values[k-1]\n\n                    draw_segment(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                                 segment_start, segment_end, x0, x1, y0, y1,\n                                 xm, ym, buffer, *aggs_and_cols)\n\n    def extend_cpu_antialias_2agg(\n        sx, tx, sy, ty, xmin, xmax, ymin, ymax, geometry, antialias_stage_2, *aggs_and_cols\n    ):\n        coords, offsets, outer_offsets, closed_rings = _process_geometry(geometry)\n        n_aggs = len(antialias_stage_2[0])\n        aggs_and_accums = tuple((agg, agg.copy()) for agg in aggs_and_cols[:n_aggs])\n\n        extend_cpu_numba_antialias_2agg(\n            sx, tx, sy, ty, xmin, xmax, ymin, ymax, coords, offsets, outer_offsets, closed_rings,\n            antialias_stage_2, aggs_and_accums, *aggs_and_cols)\n\n    @ngjit\n    @expand_aggs_and_cols\n    def extend_cpu_numba_antialias_2agg(\n        sx, tx, sy, ty, xmin, xmax, ymin, ymax, values, offsets, outer_offsets,\n        closed_rings, antialias_stage_2, aggs_and_accums, *aggs_and_cols\n    ):\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n        n_multilines = len(outer_offsets) - 1\n        first_pass = True\n        for i in range(n_multilines):\n            start0 = outer_offsets[i]\n            stop0 = outer_offsets[i + 1]\n\n            for j in range(start0, stop0):\n                start1 = offsets[j]\n                stop1 = offsets[j + 1]\n\n                for k in range(2*start1, 2*stop1 - 2, 2):\n                    x0 = values[k]\n                    y0 = values[k + 1]\n                    x1 = values[k + 2]\n                    y1 = values[k + 3]\n                    if not (np.isfinite(x0) and np.isfinite(y0) and\n                            np.isfinite(x1) and np.isfinite(y1)):\n                        continue\n\n                    segment_start = (\n                            (k == start1 and not closed_rings) or\n                            (k > start1 and\n                             (not np.isfinite(values[k - 2]) or not np.isfinite(values[k - 1])))\n                    )\n\n                    segment_end = (\n                            (not closed_rings and k == stop1-4) or\n                            (k < stop1-4 and\n                             (not np.isfinite(values[k + 4]) or not np.isfinite(values[k + 5])))\n                    )\n\n                    draw_segment(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax,\n                                 segment_start, segment_end, x0, x1, y0, y1,\n                                 #xm, ym, buffer, *aggs_and_cols)\n                                 0.0, 0.0, buffer, *aggs_and_cols)\n\n            aa_stage_2_accumulate(aggs_and_accums, first_pass)\n            first_pass = False\n            aa_stage_2_clear(aggs_and_accums)\n\n        aa_stage_2_copy_back(aggs_and_accums)\n\n    if use_2_stage_agg:\n        return extend_cpu_antialias_2agg\n    else:\n        return extend_cpu\n"
  },
  "GT_src_dict": {
    "datashader/core.py": {
      "Canvas.points": {
        "code": "    def points(self, source, x=None, y=None, agg=None, geometry=None):\n        \"\"\"Compute a reduction by pixel, mapping data to pixels as points.\n\nParameters\n----------\nsource : pandas.DataFrame, dask.DataFrame, or xarray.DataArray/Dataset\n    The input datasource containing the data to be rendered as points.\nx, y : str, optional\n    Column names for the x and y coordinates of each point. At least one of these must be provided unless `geometry` is used.\nagg : Reduction, optional\n    Reduction to compute from the data; default is `count()`, which counts the number of points in each pixel.\ngeometry : str, optional\n    Column name containing a PointsArray of the coordinates of each point. If provided, `x` and `y` must not be specified.\n\nReturns\n-------\nAggregate results mapped to pixels, as defined by the specified parameters. This aggregate is computed using the `bypixel` function, which handles the data normalization and reduction.\n\nNotes\n-----\n- The function relies on `validate_xy_or_geometry` to ensure that the input parameters are valid, meaning that either both `x` and `y` or `geometry` must be specified.\n- If `geometry` is used, the method may utilize `MultiPointGeometry` from the `.glyphs` module when processing `spatialpandas` or `geopandas` data sources.\n- By default, if `agg` is not provided, a count reduction is used to determine the number of occurrences per pixel.\n\nDependencies\n------------\n- The function imports `Point` and `MultiPointGeometry` from the `.glyphs` module to create appropriate geometrical representations of the data points.\n- It utilizes `count` from the `.reductions` module to provide a default aggregation method.\n- The code also integrates with spatial data libraries such as `spatialpandas` and `geopandas` to handle geographical dataframes and potentially filter data based on viewport partitions.\"\"\"\n        'Compute a reduction by pixel, mapping data to pixels as points.\\n\\n        Parameters\\n        ----------\\n        source : pandas.DataFrame, dask.DataFrame, or xarray.DataArray/Dataset\\n            The input datasource.\\n        x, y : str\\n            Column names for the x and y coordinates of each point. If provided,\\n            the geometry argument may not also be provided.\\n        agg : Reduction, optional\\n            Reduction to compute. Default is ``count()``.\\n        geometry: str\\n            Column name of a PointsArray of the coordinates of each point. If provided,\\n            the x and y arguments may not also be provided.\\n        '\n        from .glyphs import Point, MultiPointGeometry\n        from .reductions import count as count_rdn\n        validate_xy_or_geometry('Point', x, y, geometry)\n        if agg is None:\n            agg = count_rdn()\n        if geometry is None:\n            glyph = Point(x, y)\n        elif spatialpandas and isinstance(source, spatialpandas.dask.DaskGeoDataFrame):\n            x_range = self.x_range if self.x_range is not None else (None, None)\n            y_range = self.y_range if self.y_range is not None else (None, None)\n            source = source.cx_partitions[slice(*x_range), slice(*y_range)]\n            glyph = MultiPointGeometry(geometry)\n        elif spatialpandas and isinstance(source, spatialpandas.GeoDataFrame):\n            glyph = MultiPointGeometry(geometry)\n        elif (geopandas_source := self._source_from_geopandas(source)) is not None:\n            source = geopandas_source\n            from datashader.glyphs.points import MultiPointGeoPandas\n            glyph = MultiPointGeoPandas(geometry)\n        else:\n            raise ValueError('source must be an instance of spatialpandas.GeoDataFrame, spatialpandas.dask.DaskGeoDataFrame, geopandas.GeoDataFrame, or dask_geopandas.GeoDataFrame. Received objects of type {typ}'.format(typ=type(source)))\n        return bypixel(source, self, glyph, agg)",
        "docstring": "Compute a reduction by pixel, mapping data to pixels as points.\n\nParameters\n----------\nsource : pandas.DataFrame, dask.DataFrame, or xarray.DataArray/Dataset\n    The input datasource containing the data to be rendered as points.\nx, y : str, optional\n    Column names for the x and y coordinates of each point. At least one of these must be provided unless `geometry` is used.\nagg : Reduction, optional\n    Reduction to compute from the data; default is `count()`, which counts the number of points in each pixel.\ngeometry : str, optional\n    Column name containing a PointsArray of the coordinates of each point. If provided, `x` and `y` must not be specified.\n\nReturns\n-------\nAggregate results mapped to pixels, as defined by the specified parameters. This aggregate is computed using the `bypixel` function, which handles the data normalization and reduction.\n\nNotes\n-----\n- The function relies on `validate_xy_or_geometry` to ensure that the input parameters are valid, meaning that either both `x` and `y` or `geometry` must be specified.\n- If `geometry` is used, the method may utilize `MultiPointGeometry` from the `.glyphs` module when processing `spatialpandas` or `geopandas` data sources.\n- By default, if `agg` is not provided, a count reduction is used to determine the number of occurrences per pixel.\n\nDependencies\n------------\n- The function imports `Point` and `MultiPointGeometry` from the `.glyphs` module to create appropriate geometrical representations of the data points.\n- It utilizes `count` from the `.reductions` module to provide a default aggregation method.\n- The code also integrates with spatial data libraries such as `spatialpandas` and `geopandas` to handle geographical dataframes and potentially filter data based on viewport partitions.",
        "signature": "def points(self, source, x=None, y=None, agg=None, geometry=None):",
        "type": "Method",
        "class_signature": "class Canvas:"
      },
      "Canvas.line": {
        "code": "    def line(self, source, x=None, y=None, agg=None, axis=0, geometry=None, line_width=0, antialias=False):\n        \"\"\"Compute a reduction by pixel, representing data as one or more lines on a canvas.\n\nParameters\n----------\nsource : pandas.DataFrame, dask.DataFrame, or xarray.DataArray/Dataset\n    The input data source containing the coordinates.\nx, y : str or number or list or tuple or np.ndarray\n    Specifications for the x and y coordinates of each vertex.\nagg : Reduction, optional\n    Reduction operation to compute, defaulting to `any()`.\naxis : 0 or 1, default 0\n    Axis along which to draw lines, where 0 uses all rows and 1 draws one line per row.\ngeometry : str, optional\n    Name of a LinesArray in the source that defines the line coordinates, if provided x and y may not be.\nline_width : number, optional\n    Specifies the width of the line in pixels. A value of 0 results in a single-pixel width.\nantialias : bool, optional\n    Enables antialiasing for smoother lines; interpreted as `True` for line_width=1 and `False` for line_width=0. Cannot be specified with line_width.\n\nReturns\n-------\nAggregated data representing the lines drawn across the canvas.\n\nNotes\n-----\nThis method interacts with the private method `_source_from_geopandas` to validate and convert source data if it originates from GeoPandas or Dask GeoDataFrames. The line drawing logic utilizes glyphs (from `.glyphs` module) to determine how to render the lines based on the provided coordinates and aggregation operation. \n\nThe function also makes use of constants defined in `reductions` (imported as `rd`), particularly for defining default behavior and supported aggregation types. Furthermore, it checks compatibility with CUDA-backed sources pertaining to line width and antialiasing capabilities.\"\"\"\n        \"Compute a reduction by pixel, mapping data to pixels as one or\\n        more lines.\\n\\n        For aggregates that take in extra fields, the interpolated bins will\\n        receive the fields from the previous point. In pseudocode:\\n\\n        >>> for i in range(len(rows) - 1):    # doctest: +SKIP\\n        ...     row0 = rows[i]\\n        ...     row1 = rows[i + 1]\\n        ...     for xi, yi in interpolate(row0.x, row0.y, row1.x, row1.y):\\n        ...         add_to_aggregate(xi, yi, row0)\\n\\n        Parameters\\n        ----------\\n        source : pandas.DataFrame, dask.DataFrame, or xarray.DataArray/Dataset\\n            The input datasource.\\n        x, y : str or number or list or tuple or np.ndarray\\n            Specification of the x and y coordinates of each vertex\\n            * str or number: Column labels in source\\n            * list or tuple: List or tuple of column labels in source\\n            * np.ndarray: When axis=1, a literal array of the\\n              coordinates to be used for every row\\n        agg : Reduction, optional\\n            Reduction to compute. Default is ``any()``.\\n        axis : 0 or 1, default 0\\n            Axis in source to draw lines along\\n            * 0: Draw lines using data from the specified columns across\\n                 all rows in source\\n            * 1: Draw one line per row in source using data from the\\n                 specified columns\\n        geometry : str\\n            Column name of a LinesArray of the coordinates of each line. If provided,\\n            the x and y arguments may not also be provided.\\n        line_width : number, optional\\n            Width of the line to draw, in pixels. If zero, the\\n            default, lines are drawn using a simple algorithm with a\\n            blocky single-pixel width based on whether the line passes\\n            through each pixel or does not. If greater than one, lines\\n            are drawn with the specified width using a slower and\\n            more complex antialiasing algorithm with fractional values\\n            along each edge, so that lines have a more uniform visual\\n            appearance across all angles. Line widths between 0 and 1\\n            effectively use a line_width of 1 pixel but with a\\n            proportionate reduction in the strength of each pixel,\\n            approximating the visual appearance of a subpixel line\\n            width.\\n        antialias : bool, optional\\n            This option is kept for backward compatibility only.\\n            ``True`` is equivalent to ``line_width=1`` and\\n            ``False`` (the default) to ``line_width=0``. Do not specify\\n            both ``antialias`` and ``line_width`` in the same call as a\\n            ``ValueError`` will be raised if they disagree.\\n\\n        Examples\\n        --------\\n        Define a canvas and a pandas DataFrame with 6 rows\\n        >>> import pandas as pd  # doctest: +SKIP\\n        ... import numpy as np\\n        ... import datashader as ds\\n        ... from datashader import Canvas\\n        ... import datashader.transfer_functions as tf\\n        ... cvs = Canvas()\\n        ... df = pd.DataFrame({\\n        ...    'A1': [1, 1.5, 2, 2.5, 3, 4],\\n        ...    'A2': [1.5, 2, 3, 3.2, 4, 5],\\n        ...    'B1': [10, 12, 11, 14, 13, 15],\\n        ...    'B2': [11, 9, 10, 7, 8, 12],\\n        ... }, dtype='float64')\\n\\n        Aggregate one line across all rows, with coordinates df.A1 by df.B1\\n        >>> agg = cvs.line(df, x='A1', y='B1', axis=0)  # doctest: +SKIP\\n        ... tf.spread(tf.shade(agg))\\n\\n        Aggregate two lines across all rows. The first with coordinates\\n        df.A1 by df.B1 and the second with coordinates df.A2 by df.B2\\n        >>> agg = cvs.line(df, x=['A1', 'A2'], y=['B1', 'B2'], axis=0)  # doctest: +SKIP\\n        ... tf.spread(tf.shade(agg))\\n\\n        Aggregate two lines across all rows where the lines share the same\\n        x coordinates. The first line will have coordinates df.A1 by df.B1\\n        and the second will have coordinates df.A1 by df.B2\\n        >>> agg = cvs.line(df, x='A1', y=['B1', 'B2'], axis=0)  # doctest: +SKIP\\n        ... tf.spread(tf.shade(agg))\\n\\n        Aggregate 6 length-2 lines, one per row, where the ith line has\\n        coordinates [df.A1[i], df.A2[i]] by [df.B1[i], df.B2[i]]\\n        >>> agg = cvs.line(df, x=['A1', 'A2'], y=['B1', 'B2'], axis=1)  # doctest: +SKIP\\n        ... tf.spread(tf.shade(agg))\\n\\n        Aggregate 6 length-4 lines, one per row, where the x coordinates\\n        of every line are [0, 1, 2, 3] and the y coordinates of the ith line\\n        are [df.A1[i], df.A2[i], df.B1[i], df.B2[i]].\\n        >>> agg = cvs.line(df,  # doctest: +SKIP\\n        ...                x=np.arange(4),\\n        ...                y=['A1', 'A2', 'B1', 'B2'],\\n        ...                axis=1)\\n        ... tf.spread(tf.shade(agg))\\n\\n        Aggregate RaggedArrays of variable length lines, one per row\\n        (requires pandas >= 0.24.0)\\n        >>> df_ragged = pd.DataFrame({  # doctest: +SKIP\\n        ...    'A1': pd.array([\\n        ...        [1, 1.5], [2, 2.5, 3], [1.5, 2, 3, 4], [3.2, 4, 5]],\\n        ...        dtype='Ragged[float32]'),\\n        ...    'B1': pd.array([\\n        ...        [10, 12], [11, 14, 13], [10, 7, 9, 10], [7, 8, 12]],\\n        ...        dtype='Ragged[float32]'),\\n        ...    'group': pd.Categorical([0, 1, 2, 1])\\n        ... })\\n        ...\\n        ... agg = cvs.line(df_ragged, x='A1', y='B1', axis=1)\\n        ... tf.spread(tf.shade(agg))\\n\\n        Aggregate RaggedArrays of variable length lines by group column,\\n        one per row (requires pandas >= 0.24.0)\\n        >>> agg = cvs.line(df_ragged, x='A1', y='B1',  # doctest: +SKIP\\n        ...                agg=ds.count_cat('group'), axis=1)\\n        ... tf.spread(tf.shade(agg))\\n        \"\n        from .glyphs import LineAxis0, LinesAxis1, LinesAxis1XConstant, LinesAxis1YConstant, LineAxis0Multi, LinesAxis1Ragged, LineAxis1Geometry, LinesXarrayCommonX\n        validate_xy_or_geometry('Line', x, y, geometry)\n        if agg is None:\n            agg = rd.any()\n        if line_width is None:\n            line_width = 0\n        if antialias and line_width != 0:\n            raise ValueError('Do not specify values for both the line_width and \\nantialias keyword arguments; use line_width instead.')\n        if antialias:\n            line_width = 1.0\n        if geometry is not None:\n            if spatialpandas and isinstance(source, spatialpandas.dask.DaskGeoDataFrame):\n                x_range = self.x_range if self.x_range is not None else (None, None)\n                y_range = self.y_range if self.y_range is not None else (None, None)\n                source = source.cx_partitions[slice(*x_range), slice(*y_range)]\n                glyph = LineAxis1Geometry(geometry)\n            elif spatialpandas and isinstance(source, spatialpandas.GeoDataFrame):\n                glyph = LineAxis1Geometry(geometry)\n            elif (geopandas_source := self._source_from_geopandas(source)) is not None:\n                source = geopandas_source\n                from datashader.glyphs.line import LineAxis1GeoPandas\n                glyph = LineAxis1GeoPandas(geometry)\n            else:\n                raise ValueError('source must be an instance of spatialpandas.GeoDataFrame, spatialpandas.dask.DaskGeoDataFrame, geopandas.GeoDataFrame, or dask_geopandas.GeoDataFrame. Received objects of type {typ}'.format(typ=type(source)))\n        elif isinstance(source, Dataset) and isinstance(x, str) and isinstance(y, str):\n            x_arr = source[x]\n            y_arr = source[y]\n            if x_arr.ndim != 1:\n                raise ValueError(f'x array must have 1 dimension not {x_arr.ndim}')\n            if y_arr.ndim != 2:\n                raise ValueError(f'y array must have 2 dimensions not {y_arr.ndim}')\n            if x not in y_arr.dims:\n                raise ValueError('x must be one of the coordinate dimensions of y')\n            y_coord_dims = list(y_arr.coords.dims)\n            x_dim_index = y_coord_dims.index(x)\n            glyph = LinesXarrayCommonX(x, y, x_dim_index)\n        else:\n            orig_x, orig_y = (x, y)\n            x, y = _broadcast_column_specifications(x, y)\n            if axis == 0:\n                if isinstance(x, (Number, str)) and isinstance(y, (Number, str)):\n                    glyph = LineAxis0(x, y)\n                elif isinstance(x, (list, tuple)) and isinstance(y, (list, tuple)):\n                    glyph = LineAxis0Multi(tuple(x), tuple(y))\n                else:\n                    raise ValueError('\\nInvalid combination of x and y arguments to Canvas.line when axis=0.\\n    Received:\\n        x: {x}\\n        y: {y}\\nSee docstring for more information on valid usage'.format(x=repr(orig_x), y=repr(orig_y)))\n            elif axis == 1:\n                if isinstance(x, (list, tuple)) and isinstance(y, (list, tuple)):\n                    glyph = LinesAxis1(tuple(x), tuple(y))\n                elif isinstance(x, np.ndarray) and isinstance(y, (list, tuple)):\n                    glyph = LinesAxis1XConstant(x, tuple(y))\n                elif isinstance(x, (list, tuple)) and isinstance(y, np.ndarray):\n                    glyph = LinesAxis1YConstant(tuple(x), y)\n                elif isinstance(x, (Number, str)) and isinstance(y, (Number, str)):\n                    glyph = LinesAxis1Ragged(x, y)\n                else:\n                    raise ValueError('\\nInvalid combination of x and y arguments to Canvas.line when axis=1.\\n    Received:\\n        x: {x}\\n        y: {y}\\nSee docstring for more information on valid usage'.format(x=repr(orig_x), y=repr(orig_y)))\n            else:\n                raise ValueError('\\nThe axis argument to Canvas.line must be 0 or 1\\n    Received: {axis}'.format(axis=axis))\n        if line_width > 0 and (cudf and isinstance(source, cudf.DataFrame) or (dask_cudf and isinstance(source, dask_cudf.DataFrame))):\n            warnings.warn('Antialiased lines are not supported for CUDA-backed sources, so reverting to line_width=0')\n            line_width = 0\n        glyph.set_line_width(line_width)\n        if glyph.antialiased:\n            non_cat_agg = agg\n            if isinstance(non_cat_agg, rd.by):\n                non_cat_agg = non_cat_agg.reduction\n            if not isinstance(non_cat_agg, (rd.any, rd.count, rd.max, rd.min, rd.sum, rd.summary, rd._sum_zero, rd._first_or_last, rd.mean, rd.max_n, rd.min_n, rd._first_n_or_last_n, rd._max_or_min_row_index, rd._max_n_or_min_n_row_index, rd.where)):\n                raise NotImplementedError(f'{type(non_cat_agg)} reduction not implemented for antialiased lines')\n        return bypixel(source, self, glyph, agg, antialias=glyph.antialiased)",
        "docstring": "Compute a reduction by pixel, representing data as one or more lines on a canvas.\n\nParameters\n----------\nsource : pandas.DataFrame, dask.DataFrame, or xarray.DataArray/Dataset\n    The input data source containing the coordinates.\nx, y : str or number or list or tuple or np.ndarray\n    Specifications for the x and y coordinates of each vertex.\nagg : Reduction, optional\n    Reduction operation to compute, defaulting to `any()`.\naxis : 0 or 1, default 0\n    Axis along which to draw lines, where 0 uses all rows and 1 draws one line per row.\ngeometry : str, optional\n    Name of a LinesArray in the source that defines the line coordinates, if provided x and y may not be.\nline_width : number, optional\n    Specifies the width of the line in pixels. A value of 0 results in a single-pixel width.\nantialias : bool, optional\n    Enables antialiasing for smoother lines; interpreted as `True` for line_width=1 and `False` for line_width=0. Cannot be specified with line_width.\n\nReturns\n-------\nAggregated data representing the lines drawn across the canvas.\n\nNotes\n-----\nThis method interacts with the private method `_source_from_geopandas` to validate and convert source data if it originates from GeoPandas or Dask GeoDataFrames. The line drawing logic utilizes glyphs (from `.glyphs` module) to determine how to render the lines based on the provided coordinates and aggregation operation. \n\nThe function also makes use of constants defined in `reductions` (imported as `rd`), particularly for defining default behavior and supported aggregation types. Furthermore, it checks compatibility with CUDA-backed sources pertaining to line width and antialiasing capabilities.",
        "signature": "def line(self, source, x=None, y=None, agg=None, axis=0, geometry=None, line_width=0, antialias=False):",
        "type": "Method",
        "class_signature": "class Canvas:"
      }
    },
    "datashader/glyphs/points.py": {
      "_PointLike.__init__": {
        "code": "    def __init__(self, x, y):\n        \"\"\"Initialize a _PointLike glyph instance representing a point with specified coordinates.\n\nParameters\n----------\nx : str\n    The column name representing the x coordinate of each point.\ny : str\n    The column name representing the y coordinate of each point.\n\nAttributes\n----------\nself.x : str\n    Stores the x coordinate column name.\nself.y : str\n    Stores the y coordinate column name.\n\nThe _PointLike class serves as a base for creating point-like visualizations in data structures where point coordinates are involved. It provides validation and boundary computation functionalities using the specified x and y coordinates.\"\"\"\n        self.x = x\n        self.y = y",
        "docstring": "Initialize a _PointLike glyph instance representing a point with specified coordinates.\n\nParameters\n----------\nx : str\n    The column name representing the x coordinate of each point.\ny : str\n    The column name representing the y coordinate of each point.\n\nAttributes\n----------\nself.x : str\n    Stores the x coordinate column name.\nself.y : str\n    Stores the y coordinate column name.\n\nThe _PointLike class serves as a base for creating point-like visualizations in data structures where point coordinates are involved. It provides validation and boundary computation functionalities using the specified x and y coordinates.",
        "signature": "def __init__(self, x, y):",
        "type": "Method",
        "class_signature": "class _PointLike(Glyph):"
      }
    },
    "datashader/pipeline.py": {
      "Pipeline.__init__": {
        "code": "    def __init__(self, df, glyph, agg=reductions.count(), transform_fn=identity, color_fn=tf.shade, spread_fn=tf.dynspread, width_scale=1.0, height_scale=1.0):\n        \"\"\"Initialize a Pipeline instance for creating a datashading pipeline callback.\n\nParameters\n----------\ndf : pandas.DataFrame, dask.DataFrame\n    The data source to be visualized.\nglyph : Glyph\n    The type of glyph used for visualization.\nagg : Reduction, optional\n    The aggregation function to compute per-pixel. Defaults to ``count()`` from the reductions module.\ntransform_fn : callable, optional\n    A preprocessing function that transforms the computed aggregate before passing it to the color function. Defaults to the identity function from the toolz library.\ncolor_fn : callable, optional\n    A function to generate an Image from the transformed aggregate. Defaults to the ``shade`` function from the transfer_functions module.\nspread_fn : callable, optional\n    A function for post-processing the Image. Defaults to the ``dynspread`` function from the transfer_functions module.\nwidth_scale : float, optional\n    A scaling factor for the image width. Defaults to 1.0.\nheight_scale : float, optional\n    A scaling factor for the image height. Defaults to 1.0.\n\nAttributes\n----------\nself.df : pandas.DataFrame or dask.DataFrame\n    Stores the input data.\nself.glyph : Glyph\n    Stores the glyph type.\nself.agg : Reduction\n    Stores the aggregation function.\nself.transform_fn : callable\n    Stores the transformation function.\nself.color_fn : callable\n    Stores the color function.\nself.spread_fn : callable\n    Stores the spreading function.\nself.width_scale : float\n    Stores the width scaling factor.\nself.height_scale : float\n    Stores the height scaling factor.\"\"\"\n        self.df = df\n        self.glyph = glyph\n        self.agg = agg\n        self.transform_fn = transform_fn\n        self.color_fn = color_fn\n        self.spread_fn = spread_fn\n        self.width_scale = width_scale\n        self.height_scale = height_scale",
        "docstring": "Initialize a Pipeline instance for creating a datashading pipeline callback.\n\nParameters\n----------\ndf : pandas.DataFrame, dask.DataFrame\n    The data source to be visualized.\nglyph : Glyph\n    The type of glyph used for visualization.\nagg : Reduction, optional\n    The aggregation function to compute per-pixel. Defaults to ``count()`` from the reductions module.\ntransform_fn : callable, optional\n    A preprocessing function that transforms the computed aggregate before passing it to the color function. Defaults to the identity function from the toolz library.\ncolor_fn : callable, optional\n    A function to generate an Image from the transformed aggregate. Defaults to the ``shade`` function from the transfer_functions module.\nspread_fn : callable, optional\n    A function for post-processing the Image. Defaults to the ``dynspread`` function from the transfer_functions module.\nwidth_scale : float, optional\n    A scaling factor for the image width. Defaults to 1.0.\nheight_scale : float, optional\n    A scaling factor for the image height. Defaults to 1.0.\n\nAttributes\n----------\nself.df : pandas.DataFrame or dask.DataFrame\n    Stores the input data.\nself.glyph : Glyph\n    Stores the glyph type.\nself.agg : Reduction\n    Stores the aggregation function.\nself.transform_fn : callable\n    Stores the transformation function.\nself.color_fn : callable\n    Stores the color function.\nself.spread_fn : callable\n    Stores the spreading function.\nself.width_scale : float\n    Stores the width scaling factor.\nself.height_scale : float\n    Stores the height scaling factor.",
        "signature": "def __init__(self, df, glyph, agg=reductions.count(), transform_fn=identity, color_fn=tf.shade, spread_fn=tf.dynspread, width_scale=1.0, height_scale=1.0):",
        "type": "Method",
        "class_signature": "class Pipeline:"
      },
      "Pipeline.__call__": {
        "code": "    def __call__(self, x_range=None, y_range=None, width=600, height=600):\n        \"\"\"Compute an image from the specified data pipeline, which visualizes data from a Pandas or Dask DataFrame.\n\nParameters\n----------\nx_range : tuple, optional\n    The bounding box on the x-axis of the viewport, specified as a tuple of (min, max).\ny_range : tuple, optional\n    The bounding box on the y-axis of the viewport, specified as a tuple of (min, max).\nwidth : int, optional\n    The width of the output image (default is 600).\nheight : int, optional\n    The height of the output image (default is 600).\n\nReturns\n-------\nImage\n    An image generated from the data according to the specified pipeline, which includes \n    binning, transformation, coloring, and spreading.\n\nNotes\n-----\nThe method utilizes the `core.Canvas`, `core.bypixel`, and the provided scaling factors (`width_scale`, `height_scale`) to determine the dimensions of the canvas and the resultant image. The aggregation function (`agg`), transformation function (`transform_fn`), coloring function (`color_fn`), and spreading function (`spread_fn`) are applied sequentially to derive the final image output. The method is designed to work with the `Pipeline` class, which encapsulates the process of visualizing data with customizable options.\"\"\"\n        'Compute an image from the specified pipeline.\\n\\n        Parameters\\n        ----------\\n        x_range, y_range : tuple, optional\\n            The bounding box on the viewport, specified as tuples of\\n            ``(min, max)``\\n        width, height : int, optional\\n            The shape of the image\\n        '\n        canvas = core.Canvas(plot_width=int(width * self.width_scale), plot_height=int(height * self.height_scale), x_range=x_range, y_range=y_range)\n        bins = core.bypixel(self.df, canvas, self.glyph, self.agg, antialias=self.glyph.antialiased)\n        img = self.color_fn(self.transform_fn(bins))\n        return self.spread_fn(img)",
        "docstring": "Compute an image from the specified data pipeline, which visualizes data from a Pandas or Dask DataFrame.\n\nParameters\n----------\nx_range : tuple, optional\n    The bounding box on the x-axis of the viewport, specified as a tuple of (min, max).\ny_range : tuple, optional\n    The bounding box on the y-axis of the viewport, specified as a tuple of (min, max).\nwidth : int, optional\n    The width of the output image (default is 600).\nheight : int, optional\n    The height of the output image (default is 600).\n\nReturns\n-------\nImage\n    An image generated from the data according to the specified pipeline, which includes \n    binning, transformation, coloring, and spreading.\n\nNotes\n-----\nThe method utilizes the `core.Canvas`, `core.bypixel`, and the provided scaling factors (`width_scale`, `height_scale`) to determine the dimensions of the canvas and the resultant image. The aggregation function (`agg`), transformation function (`transform_fn`), coloring function (`color_fn`), and spreading function (`spread_fn`) are applied sequentially to derive the final image output. The method is designed to work with the `Pipeline` class, which encapsulates the process of visualizing data with customizable options.",
        "signature": "def __call__(self, x_range=None, y_range=None, width=600, height=600):",
        "type": "Method",
        "class_signature": "class Pipeline:"
      }
    },
    "datashader/transfer_functions/__init__.py": {
      "shade": {
        "code": "def shade(agg, cmap=['lightblue', 'darkblue'], color_key=Sets1to3, how='eq_hist', alpha=255, min_alpha=40, span=None, name=None, color_baseline=None, rescale_discrete_levels=False):\n    \"\"\"Convert a `DataArray` to an image by assigning RGBA pixel colors to each value, based on specified colormap and alpha settings. This function supports both 2D and 3D coordinate `DataArrays`, enabling color mapping for continuous values and discrete categories.\n\nParameters\n----------\nagg : xr.DataArray\n    Input data array to convert into an image. Must be either 2D or 3D.\ncmap : list of colors or matplotlib.colors.Colormap, optional\n    Colormap for coloring 2D aggregates; can be a list of colors or a matplotlib colormap. Default is `[\"lightblue\", \"darkblue\"]`.\ncolor_key : dict or iterable, optional\n    Specifies colors for discrete categories in 2D case or categorical data in 3D case.\nhow : str or callable, optional\n    Interpolation method for colormapping. Options include 'eq_hist' (default), 'cbrt', 'log', and 'linear'.\nalpha : int, optional\n    Alpha value (0-255) for non-NaN data pixels.\nmin_alpha : float, optional\n    Minimum alpha value for data pixels (0-255). Default is 40.\nspan : list of min-max range, optional\n    Overrides default autoranging for 2D colormapping and 3D alpha interpolation.\nname : string, optional\n    Optional name for the resulting Image object.\ncolor_baseline : float or None, optional\n    Baseline for color mixing in categorical data; defaults to None, which uses the minimum aggregate value.\nrescale_discrete_levels : boolean, optional\n    If True, adjusts span for better visibility in case of few discrete values. Default is False.\n\nReturns\n-------\nImage\n    An image representation of the input data, encoded as an Image object.\n\nRaises\n------\nTypeError\n    If `agg` is not an instance of `DataArray`.\nValueError\n    If `min_alpha` and `alpha` are not in the range 0-255, or if `agg` does not have 2D or 3D coordinates.\n\nNotes\n-----\nUtilizes helper functions `_interpolate`, `_colorize`, and `_apply_discrete_colorkey` from the same module for handling different types of data arrays. Color definitions may rely on `Sets1to3` imported from `datashader.colors`. Ensure all dependencies such as `matplotlib` for colormaps are available.\"\"\"\n    'Convert a DataArray to an image by choosing an RGBA pixel color for each value.\\n\\n    Requires a DataArray with a single data dimension, here called the\\n    \"value\", indexed using either 2D or 3D coordinates.\\n\\n    For a DataArray with 2D coordinates, the RGB channels are computed\\n    from the values by interpolated lookup into the given colormap\\n    ``cmap``.  The A channel is then set to the given fixed ``alpha``\\n    value for all non-zero values, and to zero for all zero values.\\n    A dictionary ``color_key`` that specifies categories (values in ``agg``)\\n    and corresponding colors can be provided to support discrete coloring\\n    2D aggregates, i.e aggregates with a single category per pixel,\\n    with no mixing. The A channel is set the given ``alpha`` value for all\\n    pixels in the categories specified in ``color_key``, and to zero otherwise.\\n\\n    DataArrays with 3D coordinates are expected to contain values\\n    distributed over different categories that are indexed by the\\n    additional coordinate. Such an array would reduce to the\\n    2D-coordinate case if collapsed across the categories (e.g. if one\\n    did ``aggc.sum(dim=\\'cat\\')`` for a categorical dimension ``cat``).\\n    The RGB channels for the uncollapsed, 3D case are mixed from separate\\n    values over all categories. They are computed by averaging the colors\\n    in the provided ``color_key`` (with one color per category),\\n    weighted by the array\\'s value for that category.\\n    The A channel is then computed from the array\\'s total value\\n    collapsed across all categories at that location, ranging from the\\n    specified ``min_alpha`` to the maximum alpha value (255).\\n\\n    Parameters\\n    ----------\\n    agg : DataArray\\n    cmap : list of colors or matplotlib.colors.Colormap, optional\\n        The colormap to use for 2D agg arrays. Can be either a list of\\n        colors (specified either by name, RGBA hexcode, or as a tuple\\n        of ``(red, green, blue)`` values.), or a matplotlib colormap\\n        object.  Default is ``[\"lightblue\", \"darkblue\"]``.\\n    color_key : dict or iterable\\n        The colors to use for a categorical agg array. In 3D case, it can be\\n        either a ``dict`` mapping from field name to colors, or an\\n        iterable of colors in the same order as the record fields,\\n        and including at least that many distinct colors. In 2D case,\\n        ``color_key`` must be a ``dict`` where all keys are categories,\\n        and values are corresponding colors. Number of categories does not\\n        necessarily equal to the number of unique values in the agg DataArray.\\n    how : str or callable, optional\\n        The interpolation method to use, for the ``cmap`` of a 2D\\n        DataArray or the alpha channel of a 3D DataArray. Valid\\n        strings are \\'eq_hist\\' [default], \\'cbrt\\' (cube root), \\'log\\'\\n        (logarithmic), and \\'linear\\'. Callables take 2 arguments - a\\n        2-dimensional array of magnitudes at each pixel, and a boolean\\n        mask array indicating missingness. They should return a numeric\\n        array of the same shape, with ``NaN`` values where the mask was\\n        True.\\n    alpha : int, optional\\n        Value between 0 - 255 representing the alpha value to use for\\n        colormapped pixels that contain data (i.e. non-NaN values).\\n        Also used as the maximum alpha value when alpha is indicating\\n        data value, such as for single colors or categorical plots.\\n        Regardless of this value, ``NaN`` values are set to be fully\\n        transparent when doing colormapping.\\n    min_alpha : float, optional\\n        The minimum alpha value to use for non-empty pixels when\\n        alpha is indicating data value, in [0, 255].  Use a higher value\\n        to avoid undersaturation, i.e. poorly visible low-value datapoints,\\n        at the expense of the overall dynamic range. Note that ``min_alpha``\\n        will not take any effect when doing discrete categorical coloring\\n        for 2D case as the aggregate can have only a single value to denote\\n        the category.\\n    span : list of min-max range, optional\\n        Min and max data values to use for 2D colormapping,\\n        and 3D alpha interpolation, when wishing to override autoranging.\\n    name : string name, optional\\n        Optional string name to give to the Image object to return,\\n        to label results for display.\\n    color_baseline : float or None\\n        Baseline for calculating how categorical data mixes to\\n        determine the color of a pixel. The color for each category is\\n        weighted by how far that category\\'s value is above this\\n        baseline value, out of the total sum across all categories\\'\\n        values. A value of zero is appropriate for counts and for\\n        other physical quantities for which zero is a meaningful\\n        reference; each category then contributes to the final color\\n        in proportion to how much each category contributes to the\\n        final sum.  However, if values can be negative or if they are\\n        on an interval scale where values e.g. twice as far from zero\\n        are not twice as high (such as temperature in Fahrenheit), then\\n        you will need to provide a suitable baseline value for use in\\n        calculating color mixing.  A value of None (the default) means\\n        to take the minimum across the entire aggregate array, which\\n        is safe but may not weight the colors as you expect; any\\n        categories with values near this baseline will contribute\\n        almost nothing to the final color. As a special case, if the\\n        only data present in a pixel is at the baseline level, the\\n        color will be an evenly weighted average of all such\\n        categories with data (to avoid the color being undefined in\\n        this case).\\n    rescale_discrete_levels : boolean, optional\\n        If ``how=\\'eq_hist`` and there are only a few discrete values,\\n        then ``rescale_discrete_levels=True`` decreases the lower\\n        limit of the autoranged span so that the values are rendering\\n        towards the (more visible) top of the ``cmap`` range, thus\\n        avoiding washout of the lower values.  Has no effect if\\n        ``how!=`eq_hist``. Default is False.\\n    '\n    if not isinstance(agg, xr.DataArray):\n        raise TypeError('agg must be instance of DataArray')\n    name = agg.name if name is None else name\n    if not (0 <= min_alpha <= 255 and 0 <= alpha <= 255):\n        raise ValueError(f'min_alpha ({min_alpha}) and alpha ({alpha}) must be between 0 and 255')\n    if rescale_discrete_levels and how != 'eq_hist':\n        rescale_discrete_levels = False\n    if agg.ndim == 2:\n        if color_key is not None and isinstance(color_key, dict):\n            return _apply_discrete_colorkey(agg, color_key, alpha, name, color_baseline)\n        else:\n            return _interpolate(agg, cmap, how, alpha, span, min_alpha, name, rescale_discrete_levels)\n    elif agg.ndim == 3:\n        return _colorize(agg, color_key, how, alpha, span, min_alpha, name, color_baseline, rescale_discrete_levels)\n    else:\n        raise ValueError('agg must use 2D or 3D coordinates')",
        "docstring": "Convert a `DataArray` to an image by assigning RGBA pixel colors to each value, based on specified colormap and alpha settings. This function supports both 2D and 3D coordinate `DataArrays`, enabling color mapping for continuous values and discrete categories.\n\nParameters\n----------\nagg : xr.DataArray\n    Input data array to convert into an image. Must be either 2D or 3D.\ncmap : list of colors or matplotlib.colors.Colormap, optional\n    Colormap for coloring 2D aggregates; can be a list of colors or a matplotlib colormap. Default is `[\"lightblue\", \"darkblue\"]`.\ncolor_key : dict or iterable, optional\n    Specifies colors for discrete categories in 2D case or categorical data in 3D case.\nhow : str or callable, optional\n    Interpolation method for colormapping. Options include 'eq_hist' (default), 'cbrt', 'log', and 'linear'.\nalpha : int, optional\n    Alpha value (0-255) for non-NaN data pixels.\nmin_alpha : float, optional\n    Minimum alpha value for data pixels (0-255). Default is 40.\nspan : list of min-max range, optional\n    Overrides default autoranging for 2D colormapping and 3D alpha interpolation.\nname : string, optional\n    Optional name for the resulting Image object.\ncolor_baseline : float or None, optional\n    Baseline for color mixing in categorical data; defaults to None, which uses the minimum aggregate value.\nrescale_discrete_levels : boolean, optional\n    If True, adjusts span for better visibility in case of few discrete values. Default is False.\n\nReturns\n-------\nImage\n    An image representation of the input data, encoded as an Image object.\n\nRaises\n------\nTypeError\n    If `agg` is not an instance of `DataArray`.\nValueError\n    If `min_alpha` and `alpha` are not in the range 0-255, or if `agg` does not have 2D or 3D coordinates.\n\nNotes\n-----\nUtilizes helper functions `_interpolate`, `_colorize`, and `_apply_discrete_colorkey` from the same module for handling different types of data arrays. Color definitions may rely on `Sets1to3` imported from `datashader.colors`. Ensure all dependencies such as `matplotlib` for colormaps are available.",
        "signature": "def shade(agg, cmap=['lightblue', 'darkblue'], color_key=Sets1to3, how='eq_hist', alpha=255, min_alpha=40, span=None, name=None, color_baseline=None, rescale_discrete_levels=False):",
        "type": "Function",
        "class_signature": null
      },
      "dynspread": {
        "code": "def dynspread(img, threshold=0.5, max_px=3, shape='circle', how=None, name=None):\n    \"\"\"Spread pixels in an image dynamically based on the image density, allowing for sparse plots to become more visible. The spread initiates at one pixel and continues until either the specified threshold of adjacent non-empty pixels is reached or the maximum allowed spread (`max_px`) is enforced.\n\nParameters\n----------\nimg : Image\n    The input image that is subject to pixel spreading.\nthreshold : float, optional\n    A value in the range [0, 1] that determines the spreading effect; higher values yield more extensive spreading.\nmax_px : int, optional\n    The maximum number of pixels to spread on each side, with a default of 3.\nshape : str, optional\n    The geometric shape used for spreading, defaulting to 'circle'; can also be 'square'.\nhow : str, optional\n    The compositing operator to combine pixels during spreading. Defaults to 'over' for Image objects and 'add' for others.\nname : string, optional\n    An optional name for the resulting Image object.\n\nReturns\n-------\nImage\n    The modified image with pixels spread according to the specified parameters.\n\nRaises\n------\nValueError\n    If the threshold is not within the range [0, 1] or if `max_px` is not a non-negative integer.\n\nNotes\n-----\nThe function leverages helper functions like `_array_density` and `_rgb_density` to compute the density of non-empty pixels in determining how far to spread. It also checks if the image data is in a cupy array, converting it to a numpy array as necessary. This ensures compatibility across different array types.\"\"\"\n    \"Spread pixels in an image dynamically based on the image density.\\n\\n    Spreading expands each pixel a certain number of pixels on all sides\\n    according to a given shape, merging pixels using a specified compositing\\n    operator. This can be useful to make sparse plots more visible. Dynamic\\n    spreading determines how many pixels to spread based on a density\\n    heuristic.  Spreading starts at 1 pixel, and stops when the fraction\\n    of adjacent non-empty pixels reaches the specified threshold, or\\n    the max_px is reached, whichever comes first.\\n\\n    Parameters\\n    ----------\\n    img : Image\\n    threshold : float, optional\\n        A tuning parameter in [0, 1], with higher values giving more\\n        spreading.\\n    max_px : int, optional\\n        Maximum number of pixels to spread on all sides.\\n    shape : str, optional\\n        The shape to spread by. Options are 'circle' [default] or 'square'.\\n    how : str, optional\\n        The name of the compositing operator to use when combining\\n        pixels. Default of None uses 'over' operator for Image objects\\n        and 'add' operator otherwise.\\n    \"\n    is_image = isinstance(img, Image)\n    if not 0 <= threshold <= 1:\n        raise ValueError('threshold must be in [0, 1]')\n    if not isinstance(max_px, int) or max_px < 0:\n        raise ValueError('max_px must be >= 0')\n    float_type = img.dtype in [np.float32, np.float64]\n    if cupy and isinstance(img.data, cupy.ndarray):\n        img.data = cupy.asnumpy(img.data)\n    px_ = 0\n    for px in range(1, max_px + 1):\n        px_ = px\n        if is_image:\n            density = _rgb_density(img.data, px * 2)\n        elif len(img.shape) == 2:\n            density = _array_density(img.data, float_type, px * 2)\n        else:\n            masked = np.logical_not(np.isnan(img)) if float_type else img != 0\n            flat_mask = np.sum(masked, axis=2, dtype='uint32')\n            density = _array_density(flat_mask.data, False, px * 2)\n        if density > threshold:\n            px_ = px_ - 1\n            break\n    if px_ >= 1:\n        return spread(img, px_, shape=shape, how=how, name=name)\n    else:\n        return img",
        "docstring": "Spread pixels in an image dynamically based on the image density, allowing for sparse plots to become more visible. The spread initiates at one pixel and continues until either the specified threshold of adjacent non-empty pixels is reached or the maximum allowed spread (`max_px`) is enforced.\n\nParameters\n----------\nimg : Image\n    The input image that is subject to pixel spreading.\nthreshold : float, optional\n    A value in the range [0, 1] that determines the spreading effect; higher values yield more extensive spreading.\nmax_px : int, optional\n    The maximum number of pixels to spread on each side, with a default of 3.\nshape : str, optional\n    The geometric shape used for spreading, defaulting to 'circle'; can also be 'square'.\nhow : str, optional\n    The compositing operator to combine pixels during spreading. Defaults to 'over' for Image objects and 'add' for others.\nname : string, optional\n    An optional name for the resulting Image object.\n\nReturns\n-------\nImage\n    The modified image with pixels spread according to the specified parameters.\n\nRaises\n------\nValueError\n    If the threshold is not within the range [0, 1] or if `max_px` is not a non-negative integer.\n\nNotes\n-----\nThe function leverages helper functions like `_array_density` and `_rgb_density` to compute the density of non-empty pixels in determining how far to spread. It also checks if the image data is in a cupy array, converting it to a numpy array as necessary. This ensures compatibility across different array types.",
        "signature": "def dynspread(img, threshold=0.5, max_px=3, shape='circle', how=None, name=None):",
        "type": "Function",
        "class_signature": null
      }
    },
    "datashader/reductions.py": {
      "SelfIntersectingOptionalFieldReduction.__init__": {
        "code": "    def __init__(self, column=None, self_intersect=True):\n        \"\"\"Initialize a SelfIntersectingOptionalFieldReduction instance.\n\n    This class extends OptionalFieldReduction to support reductions where \n    self-intersecting geometry may or may not be desirable when using antialiasing. \n    It combines the concept of optional fields, allowing a reduction without a specific field, \n    while also factoring in the self-intersection of geometries in the calculation.\n\n    Parameters\n    ----------\n    column : str or SpecialColumn, optional\n        The name of the column to perform the reduction on. If None, the reduction will \n        not specify a column. The SpecialColumn enumeration is used to represent specific \n        functionalities like RowIndex.\n        \n    self_intersect : bool, default=True\n        A flag indicating whether self-intersection of geometries should be considered during \n        the reduction. If set to True, self-intersection is accounted for in the calculation,\n        affecting the way reductions are computed, especially in antialiased rendering.\n\n    Attributes\n    ----------\n    column : str or SpecialColumn\n        Stores the name or identifier of the column associated with the reduction.\n\n    self_intersect : bool\n        Indicates if self-intersection needs to be considered during the reduction process.\n\n    This initialization is important for setting up the appropriate reduction strategy \n    based on whether the reduction is expected to handle self-intersecting geometries, \n    which can alter performance and the accuracy of results in graphical computations.\"\"\"\n        super().__init__(column)\n        self.self_intersect = self_intersect",
        "docstring": "Initialize a SelfIntersectingOptionalFieldReduction instance.\n\nThis class extends OptionalFieldReduction to support reductions where \nself-intersecting geometry may or may not be desirable when using antialiasing. \nIt combines the concept of optional fields, allowing a reduction without a specific field, \nwhile also factoring in the self-intersection of geometries in the calculation.\n\nParameters\n----------\ncolumn : str or SpecialColumn, optional\n    The name of the column to perform the reduction on. If None, the reduction will \n    not specify a column. The SpecialColumn enumeration is used to represent specific \n    functionalities like RowIndex.\n    \nself_intersect : bool, default=True\n    A flag indicating whether self-intersection of geometries should be considered during \n    the reduction. If set to True, self-intersection is accounted for in the calculation,\n    affecting the way reductions are computed, especially in antialiased rendering.\n\nAttributes\n----------\ncolumn : str or SpecialColumn\n    Stores the name or identifier of the column associated with the reduction.\n\nself_intersect : bool\n    Indicates if self-intersection needs to be considered during the reduction process.\n\nThis initialization is important for setting up the appropriate reduction strategy \nbased on whether the reduction is expected to handle self-intersecting geometries, \nwhich can alter performance and the accuracy of results in graphical computations.",
        "signature": "def __init__(self, column=None, self_intersect=True):",
        "type": "Method",
        "class_signature": "class SelfIntersectingOptionalFieldReduction(OptionalFieldReduction):"
      },
      "SelfIntersectingFloatingReduction.__init__": {
        "code": "    def __init__(self, column=None, self_intersect=True):\n        \"\"\"Initializes a SelfIntersectingFloatingReduction instance which is a specialized reduction class for modified floating-point reductions where self-intersecting geometry may or may not be desired.\n\n    Parameters\n    ----------\n    column : str, optional\n        The column name to aggregate over. Defaults to None.\n    self_intersect : bool, optional\n        A flag indicating whether self-intersecting geometry is allowed. Defaults to True.\n\n    Attributes\n    ----------\n    self_intersect : bool\n        Stores the value of the self_intersect parameter, controlling whether the reduction considers self-intersecting geometries during computations.\n\n    This class inherits from FloatingReduction and is primarily used in scenarios where fair aggregation behavior is required when dealing with complex geometries. The `self_intersect` attribute influences whether certain optimizations or processing techniques are applied during the reduction operations.\"\"\"\n        super().__init__(column)\n        self.self_intersect = self_intersect",
        "docstring": "Initializes a SelfIntersectingFloatingReduction instance which is a specialized reduction class for modified floating-point reductions where self-intersecting geometry may or may not be desired.\n\nParameters\n----------\ncolumn : str, optional\n    The column name to aggregate over. Defaults to None.\nself_intersect : bool, optional\n    A flag indicating whether self-intersecting geometry is allowed. Defaults to True.\n\nAttributes\n----------\nself_intersect : bool\n    Stores the value of the self_intersect parameter, controlling whether the reduction considers self-intersecting geometries during computations.\n\nThis class inherits from FloatingReduction and is primarily used in scenarios where fair aggregation behavior is required when dealing with complex geometries. The `self_intersect` attribute influences whether certain optimizations or processing techniques are applied during the reduction operations.",
        "signature": "def __init__(self, column=None, self_intersect=True):",
        "type": "Method",
        "class_signature": "class SelfIntersectingFloatingReduction(FloatingReduction):"
      }
    },
    "datashader/glyphs/line.py": {
      "_AntiAliasedLine.set_line_width": {
        "code": "    def set_line_width(self, line_width):\n        \"\"\"Set the line width for antialiased line rendering.\n\nParameters\n----------\nline_width : float\n    The width of the line to be rendered. A line width greater than zero enables antialiasing.\n    \nReturns\n-------\nNone\n\nSide Effects\n------------\nIf the instance has an attribute `antialiased`, its value is set to `True` if `line_width` is greater than zero, indicating that antialiasing will be applied to the line rendering.\n\nConstants\n---------\nThe attribute `_line_width` is an integer initialized to 0, which stores the width of the line. This attribute is essential for determining the rendering style (antialiased or not) based on the specified line width.\"\"\"\n        self._line_width = line_width\n        if hasattr(self, 'antialiased'):\n            self.antialiased = line_width > 0",
        "docstring": "Set the line width for antialiased line rendering.\n\nParameters\n----------\nline_width : float\n    The width of the line to be rendered. A line width greater than zero enables antialiasing.\n    \nReturns\n-------\nNone\n\nSide Effects\n------------\nIf the instance has an attribute `antialiased`, its value is set to `True` if `line_width` is greater than zero, indicating that antialiasing will be applied to the line rendering.\n\nConstants\n---------\nThe attribute `_line_width` is an integer initialized to 0, which stores the width of the line. This attribute is essential for determining the rendering style (antialiased or not) based on the specified line width.",
        "signature": "def set_line_width(self, line_width):",
        "type": "Method",
        "class_signature": "class _AntiAliasedLine:"
      }
    }
  },
  "dependency_dict": {
    "datashader/pipeline.py:Pipeline:__call__": {
      "datashader/core.py": {
        "Canvas.__init__": {
          "code": "    def __init__(self, plot_width=600, plot_height=600, x_range=None, y_range=None, x_axis_type='linear', y_axis_type='linear'):\n        self.plot_width = plot_width\n        self.plot_height = plot_height\n        self.x_range = None if x_range is None else tuple(x_range)\n        self.y_range = None if y_range is None else tuple(y_range)\n        self.x_axis = _axis_lookup[x_axis_type]\n        self.y_axis = _axis_lookup[y_axis_type]",
          "docstring": "",
          "signature": "def __init__(self, plot_width=600, plot_height=600, x_range=None, y_range=None, x_axis_type='linear', y_axis_type='linear'):",
          "type": "Method",
          "class_signature": "class Canvas:"
        },
        "bypixel": {
          "code": "def bypixel(source, canvas, glyph, agg, *, antialias=False):\n    \"\"\"Compute an aggregate grouped by pixel sized bins.\n\n    Aggregate input data ``source`` into a grid with shape and axis matching\n    ``canvas``, mapping data to bins by ``glyph``, and aggregating by reduction\n    ``agg``.\n\n    Parameters\n    ----------\n    source : pandas.DataFrame, dask.DataFrame\n        Input datasource\n    canvas : Canvas\n    glyph : Glyph\n    agg : Reduction\n    \"\"\"\n    source, dshape = _bypixel_sanitise(source, glyph, agg)\n    schema = dshape.measure\n    glyph.validate(schema)\n    agg.validate(schema)\n    canvas.validate()\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', 'All-NaN (slice|axis) encountered')\n        return bypixel.pipeline(source, schema, canvas, glyph, agg, antialias=antialias)",
          "docstring": "Compute an aggregate grouped by pixel sized bins.\n\nAggregate input data ``source`` into a grid with shape and axis matching\n``canvas``, mapping data to bins by ``glyph``, and aggregating by reduction\n``agg``.\n\nParameters\n----------\nsource : pandas.DataFrame, dask.DataFrame\n    Input datasource\ncanvas : Canvas\nglyph : Glyph\nagg : Reduction",
          "signature": "def bypixel(source, canvas, glyph, agg, *, antialias=False):",
          "type": "Function",
          "class_signature": null
        }
      },
      "datashader/tests/test_pipeline.py": {
        "color_fn": {
          "code": "    def color_fn(agg):\n        return tf.shade(agg, 'pink', 'red')",
          "docstring": "",
          "signature": "def color_fn(agg):",
          "type": "Function",
          "class_signature": null
        },
        "transform_fn": {
          "code": "    def transform_fn(agg):\n        return agg + 1",
          "docstring": "",
          "signature": "def transform_fn(agg):",
          "type": "Function",
          "class_signature": null
        }
      }
    },
    "datashader/reductions.py:SelfIntersectingOptionalFieldReduction:__init__": {
      "datashader/reductions.py": {
        "OptionalFieldReduction.__init__": {
          "code": "    def __init__(self, column=None):\n        super().__init__(column)",
          "docstring": "",
          "signature": "def __init__(self, column=None):",
          "type": "Method",
          "class_signature": "class OptionalFieldReduction(Reduction):"
        }
      }
    },
    "datashader/core.py:Canvas:points": {
      "datashader/core.py": {
        "validate_xy_or_geometry": {
          "code": "def validate_xy_or_geometry(glyph, x, y, geometry):\n    if geometry is None and (x is None or y is None) or (geometry is not None and (x is not None or y is not None)):\n        raise ValueError('\\n{glyph} coordinates may be specified by providing both the x and y arguments, or by\\nproviding the geometry argument. Received:\\n    x: {x}\\n    y: {y}\\n    geometry: {geometry}\\n'.format(glyph=glyph, x=repr(x), y=repr(y), geometry=repr(geometry)))",
          "docstring": "",
          "signature": "def validate_xy_or_geometry(glyph, x, y, geometry):",
          "type": "Function",
          "class_signature": null
        },
        "bypixel": {
          "code": "def bypixel(source, canvas, glyph, agg, *, antialias=False):\n    \"\"\"Compute an aggregate grouped by pixel sized bins.\n\n    Aggregate input data ``source`` into a grid with shape and axis matching\n    ``canvas``, mapping data to bins by ``glyph``, and aggregating by reduction\n    ``agg``.\n\n    Parameters\n    ----------\n    source : pandas.DataFrame, dask.DataFrame\n        Input datasource\n    canvas : Canvas\n    glyph : Glyph\n    agg : Reduction\n    \"\"\"\n    source, dshape = _bypixel_sanitise(source, glyph, agg)\n    schema = dshape.measure\n    glyph.validate(schema)\n    agg.validate(schema)\n    canvas.validate()\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', 'All-NaN (slice|axis) encountered')\n        return bypixel.pipeline(source, schema, canvas, glyph, agg, antialias=antialias)",
          "docstring": "Compute an aggregate grouped by pixel sized bins.\n\nAggregate input data ``source`` into a grid with shape and axis matching\n``canvas``, mapping data to bins by ``glyph``, and aggregating by reduction\n``agg``.\n\nParameters\n----------\nsource : pandas.DataFrame, dask.DataFrame\n    Input datasource\ncanvas : Canvas\nglyph : Glyph\nagg : Reduction",
          "signature": "def bypixel(source, canvas, glyph, agg, *, antialias=False):",
          "type": "Function",
          "class_signature": null
        }
      }
    },
    "datashader/transfer_functions/__init__.py:shade": {
      "datashader/transfer_functions/__init__.py": {
        "_interpolate": {
          "code": "def _interpolate(agg, cmap, how, alpha, span, min_alpha, name, rescale_discrete_levels):\n    if cupy and isinstance(agg.data, cupy.ndarray):\n        from ._cuda_utils import masked_clip_2d, interp\n    else:\n        from ._cpu_utils import masked_clip_2d\n        interp = np.interp\n    if agg.ndim != 2:\n        raise ValueError('agg must be 2D')\n    interpolater = _normalize_interpolate_how(how)\n    data = agg.data\n    if da and isinstance(data, da.Array):\n        data = data.compute()\n    else:\n        data = data.copy()\n    if np.issubdtype(data.dtype, np.bool_):\n        mask = ~data\n        data = data.astype(np.int8)\n    elif data.dtype.kind == 'u':\n        mask = data == 0\n    else:\n        mask = np.isnan(data)\n    if mask.all():\n        return Image(np.zeros(shape=agg.data.shape, dtype=np.uint32), coords=agg.coords, dims=agg.dims, attrs=agg.attrs, name=name)\n    if span is None:\n        offset = np.nanmin(data[~mask])\n    else:\n        offset = np.array(span, dtype=data.dtype)[0]\n        masked_clip_2d(data, mask, *span)\n    data -= offset\n    with np.errstate(invalid='ignore', divide='ignore'):\n        data = interpolater(data, mask)\n        discrete_levels = None\n        if isinstance(data, (list, tuple)):\n            data, discrete_levels = data\n        if span is None:\n            masked_data = np.where(~mask, data, np.nan)\n            span = (np.nanmin(masked_data), np.nanmax(masked_data))\n            if rescale_discrete_levels and discrete_levels is not None:\n                span = _rescale_discrete_levels(discrete_levels, span)\n        else:\n            if how == 'eq_hist':\n                raise ValueError('span is not (yet) valid to use with eq_hist')\n            span = interpolater([0, span[1] - span[0]], 0)\n    if isinstance(cmap, Iterator):\n        cmap = list(cmap)\n    if isinstance(cmap, tuple) and isinstance(cmap[0], str):\n        cmap = list(cmap)\n    if isinstance(cmap, list):\n        rspan, gspan, bspan = np.array(list(zip(*map(rgb, cmap))))\n        span = np.linspace(span[0], span[1], len(cmap))\n        r = np.nan_to_num(interp(data, span, rspan, left=255), copy=False).astype(np.uint8)\n        g = np.nan_to_num(interp(data, span, gspan, left=255), copy=False).astype(np.uint8)\n        b = np.nan_to_num(interp(data, span, bspan, left=255), copy=False).astype(np.uint8)\n        a = np.where(np.isnan(data), 0, alpha).astype(np.uint8)\n        rgba = np.dstack([r, g, b, a])\n    elif isinstance(cmap, str) or isinstance(cmap, tuple):\n        color = rgb(cmap)\n        aspan = np.arange(min_alpha, alpha + 1)\n        span = np.linspace(span[0], span[1], len(aspan))\n        r = np.full(data.shape, color[0], dtype=np.uint8)\n        g = np.full(data.shape, color[1], dtype=np.uint8)\n        b = np.full(data.shape, color[2], dtype=np.uint8)\n        a = np.nan_to_num(interp(data, span, aspan, left=0, right=255), copy=False).astype(np.uint8)\n        rgba = np.dstack([r, g, b, a])\n    elif callable(cmap):\n        scaled_data = (data - span[0]) / (span[1] - span[0])\n        if cupy and isinstance(scaled_data, cupy.ndarray):\n            scaled_data = cupy.asnumpy(scaled_data)\n        rgba = cmap(scaled_data, bytes=True)\n        rgba[:, :, 3] = np.where(np.isnan(scaled_data), 0, alpha).astype(np.uint8)\n    else:\n        raise TypeError(\"Expected `cmap` of `matplotlib.colors.Colormap`, `list`, `str`, or `tuple`; got: '{0}'\".format(type(cmap)))\n    img = rgba.view(np.uint32).reshape(data.shape)\n    if cupy and isinstance(img, cupy.ndarray):\n        img = cupy.asnumpy(img)\n    return Image(img, coords=agg.coords, dims=agg.dims, name=name)",
          "docstring": "",
          "signature": "def _interpolate(agg, cmap, how, alpha, span, min_alpha, name, rescale_discrete_levels):",
          "type": "Function",
          "class_signature": null
        }
      }
    },
    "datashader/reductions.py:SelfIntersectingFloatingReduction:__init__": {
      "datashader/reductions.py": {
        "Reduction.__init__": {
          "code": "    def __init__(self, column: str | SpecialColumn | None=None):\n        self.column = column\n        self._nan_check_column = None",
          "docstring": "",
          "signature": "def __init__(self, column: str | SpecialColumn | None=None):",
          "type": "Method",
          "class_signature": "class Reduction(Expr):"
        }
      }
    },
    "datashader/core.py:Canvas:line": {
      "datashader/core.py": {
        "validate_xy_or_geometry": {
          "code": "def validate_xy_or_geometry(glyph, x, y, geometry):\n    if geometry is None and (x is None or y is None) or (geometry is not None and (x is not None or y is not None)):\n        raise ValueError('\\n{glyph} coordinates may be specified by providing both the x and y arguments, or by\\nproviding the geometry argument. Received:\\n    x: {x}\\n    y: {y}\\n    geometry: {geometry}\\n'.format(glyph=glyph, x=repr(x), y=repr(y), geometry=repr(geometry)))",
          "docstring": "",
          "signature": "def validate_xy_or_geometry(glyph, x, y, geometry):",
          "type": "Function",
          "class_signature": null
        },
        "bypixel": {
          "code": "def bypixel(source, canvas, glyph, agg, *, antialias=False):\n    \"\"\"Compute an aggregate grouped by pixel sized bins.\n\n    Aggregate input data ``source`` into a grid with shape and axis matching\n    ``canvas``, mapping data to bins by ``glyph``, and aggregating by reduction\n    ``agg``.\n\n    Parameters\n    ----------\n    source : pandas.DataFrame, dask.DataFrame\n        Input datasource\n    canvas : Canvas\n    glyph : Glyph\n    agg : Reduction\n    \"\"\"\n    source, dshape = _bypixel_sanitise(source, glyph, agg)\n    schema = dshape.measure\n    glyph.validate(schema)\n    agg.validate(schema)\n    canvas.validate()\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', 'All-NaN (slice|axis) encountered')\n        return bypixel.pipeline(source, schema, canvas, glyph, agg, antialias=antialias)",
          "docstring": "Compute an aggregate grouped by pixel sized bins.\n\nAggregate input data ``source`` into a grid with shape and axis matching\n``canvas``, mapping data to bins by ``glyph``, and aggregating by reduction\n``agg``.\n\nParameters\n----------\nsource : pandas.DataFrame, dask.DataFrame\n    Input datasource\ncanvas : Canvas\nglyph : Glyph\nagg : Reduction",
          "signature": "def bypixel(source, canvas, glyph, agg, *, antialias=False):",
          "type": "Function",
          "class_signature": null
        },
        "_broadcast_column_specifications": {
          "code": "def _broadcast_column_specifications(*args):\n    lengths = {len(a) for a in args if isinstance(a, (list, tuple))}\n    if len(lengths) != 1:\n        return args\n    else:\n        n = lengths.pop()\n        return tuple(((arg,) * n if isinstance(arg, (Number, str)) else arg for arg in args))",
          "docstring": "",
          "signature": "def _broadcast_column_specifications(*args):",
          "type": "Function",
          "class_signature": null
        }
      }
    }
  },
  "call_tree": {
    "datashader/tests/test_pipeline.py:test_pipeline": {
      "datashader/glyphs/points.py:_PointLike:__init__": {},
      "datashader/pipeline.py:Pipeline:__init__": {},
      "datashader/pipeline.py:Pipeline:__call__": {
        "datashader/core.py:Canvas:__init__": {},
        "datashader/core.py:bypixel": {
          "datashader/core.py:_bypixel_sanitise": {
            "datashader/core.py:_cols_to_keep": {
              "datashader/glyphs/points.py:_PointLike:required_columns": {},
              "datashader/core.py:recurse": {}
            },
            "datashader/utils.py:dshape_from_pandas": {
              "datashader/utils.py:dshape_from_pandas_helper": {
                "datashader/datashape/coretypes.py:CType:from_numpy_dtype": {
                  "datashader/datashape/coretypes.py:Type:lookup_type": {}
                },
                "datashader/datashape/coretypes.py:Mono:__eq__": {
                  "datashader/datashape/coretypes.py:Mono:shape": {},
                  "datashader/datashape/coretypes.py:Mono:measure": {},
                  "datashader/datashape/coretypes.py:Mono:info": {
                    "datashader/datashape/coretypes.py:Mono:parameters": {
                      "datashader/datashape/coretypes.py:Mono:_slotted": {}
                    }
                  }
                }
              },
              "datashader/datashape/coretypes.py:Record:__init__": {
                "datashader/datashape/coretypes.py:_launder": {}
              },
              "datashader/datashape/coretypes.py:Mono:__rmul__": {
                "datashader/datashape/coretypes.py:Fixed:__init__": {},
                "datashader/datashape/coretypes.py:DataShape:__init__": {
                  "datashader/datashape/coretypes.py:_launder": {}
                }
              }
            }
          },
          "datashader/datashape/coretypes.py:DataShape:measure": {
            "datashader/datashape/coretypes.py:Mono:parameters": {
              "datashader/datashape/coretypes.py:Mono:_slotted": {}
            }
          },
          "datashader/glyphs/points.py:_PointLike:validate": {
            "datashader/datashape/coretypes.py:Mono:measure": {},
            "datashader/datashape/coretypes.py:Record:__getitem__": {
              "datashader/datashape/coretypes.py:Record:dict": {
                "datashader/datashape/coretypes.py:Record:fields": {}
              }
            },
            "datashader/utils.py:isreal": {
              "datashader/datashape/predicates.py:launder": {},
              "datashader/datashape/typesets.py:TypeSet:__contains__": {
                "datashader/datashape/typesets.py:TypeSet:_set": {
                  "datashader/datashape/coretypes.py:Mono:__hash__": {}
                },
                "datashader/datashape/coretypes.py:Mono:__hash__": {}
              }
            }
          },
          "datashader/reductions.py:OptionalFieldReduction:validate": {},
          "datashader/core.py:Canvas:validate": {
            "datashader/core.py:Canvas:validate_ranges": {
              "datashader/core.py:Axis:validate": {}
            },
            "datashader/core.py:Canvas:validate_size": {}
          },
          "datashader/utils.py:Dispatcher:__call__": {
            "datashader/data_libraries/pandas.py:pandas_pipeline": {
              "datashader/utils.py:Dispatcher:__call__": {
                "[ignored_or_cut_off]": "..."
              }
            }
          },
          "datashader/reductions.py:Reduction:validate": {
            "datashader/datashape/coretypes.py:Record:dict": {
              "datashader/datashape/coretypes.py:Record:fields": {}
            },
            "datashader/datashape/coretypes.py:Mono:measure": {},
            "datashader/datashape/coretypes.py:Record:__getitem__": {
              "datashader/datashape/coretypes.py:Record:dict": {
                "datashader/datashape/coretypes.py:Record:fields": {}
              }
            },
            "datashader/datashape/predicates.py:isnumeric": {
              "datashader/datashape/predicates.py:launder": {},
              "datashader/datashape/coretypes.py:to_numpy_dtype": {
                "datashader/datashape/coretypes.py:Mono:measure": {},
                "datashader/datashape/coretypes.py:to_numpy": {
                  "datashader/datashape/coretypes.py:CType:to_numpy_dtype": {}
                }
              }
            }
          }
        },
        "datashader/transfer_functions/__init__.py:shade": {
          "datashader/transfer_functions/__init__.py:_interpolate": {
            "datashader/transfer_functions/__init__.py:_normalize_interpolate_how": {},
            "datashader/transfer_functions/__init__.py:eq_hist": {},
            "datashader/colors.py:rgb": {
              "datashader/colors.py:hex_to_rgb": {}
            }
          }
        },
        "datashader/transfer_functions/__init__.py:dynspread": {},
        "datashader/tests/test_pipeline.py:color_fn": {
          "datashader/transfer_functions/__init__.py:shade": {
            "datashader/transfer_functions/__init__.py:_interpolate": {
              "datashader/transfer_functions/__init__.py:_normalize_interpolate_how": {},
              "datashader/transfer_functions/__init__.py:eq_hist": {},
              "datashader/colors.py:rgb": {
                "datashader/colors.py:hex_to_rgb": {}
              }
            }
          }
        },
        "datashader/tests/test_pipeline.py:transform_fn": {}
      },
      "datashader/reductions.py:SelfIntersectingOptionalFieldReduction:__init__": {
        "datashader/reductions.py:OptionalFieldReduction:__init__": {
          "datashader/reductions.py:Reduction:__init__": {}
        }
      },
      "datashader/core.py:Canvas:points": {
        "datashader/core.py:validate_xy_or_geometry": {},
        "datashader/glyphs/points.py:_PointLike:__init__": {},
        "datashader/core.py:bypixel": {
          "datashader/core.py:_bypixel_sanitise": {
            "datashader/core.py:_cols_to_keep": {
              "datashader/glyphs/points.py:_PointLike:required_columns": {},
              "datashader/core.py:recurse": {}
            },
            "datashader/utils.py:dshape_from_pandas": {
              "datashader/utils.py:dshape_from_pandas_helper": {
                "datashader/datashape/coretypes.py:CType:from_numpy_dtype": {
                  "datashader/datashape/coretypes.py:Type:lookup_type": {}
                },
                "datashader/datashape/coretypes.py:Mono:__eq__": {
                  "datashader/datashape/coretypes.py:Mono:shape": {},
                  "datashader/datashape/coretypes.py:Mono:measure": {},
                  "datashader/datashape/coretypes.py:Mono:info": {
                    "datashader/datashape/coretypes.py:Mono:parameters": {
                      "datashader/datashape/coretypes.py:Mono:_slotted": {}
                    }
                  }
                }
              },
              "datashader/datashape/coretypes.py:Record:__init__": {
                "datashader/datashape/coretypes.py:_launder": {}
              },
              "datashader/datashape/coretypes.py:Mono:__rmul__": {
                "datashader/datashape/coretypes.py:Fixed:__init__": {},
                "datashader/datashape/coretypes.py:DataShape:__init__": {
                  "datashader/datashape/coretypes.py:_launder": {}
                }
              }
            }
          },
          "datashader/datashape/coretypes.py:DataShape:measure": {
            "datashader/datashape/coretypes.py:Mono:parameters": {
              "datashader/datashape/coretypes.py:Mono:_slotted": {}
            }
          },
          "datashader/glyphs/points.py:_PointLike:validate": {
            "datashader/datashape/coretypes.py:Mono:measure": {},
            "datashader/datashape/coretypes.py:Record:__getitem__": {
              "datashader/datashape/coretypes.py:Record:dict": {
                "datashader/datashape/coretypes.py:Record:fields": {}
              }
            },
            "datashader/utils.py:isreal": {
              "datashader/datashape/predicates.py:launder": {},
              "datashader/datashape/typesets.py:TypeSet:__contains__": {
                "datashader/datashape/typesets.py:TypeSet:_set": {
                  "datashader/datashape/coretypes.py:Mono:__hash__": {}
                },
                "datashader/datashape/coretypes.py:Mono:__hash__": {}
              }
            }
          },
          "datashader/reductions.py:OptionalFieldReduction:validate": {},
          "datashader/core.py:Canvas:validate": {
            "datashader/core.py:Canvas:validate_ranges": {
              "datashader/core.py:Axis:validate": {}
            },
            "datashader/core.py:Canvas:validate_size": {}
          },
          "datashader/utils.py:Dispatcher:__call__": {
            "datashader/data_libraries/pandas.py:pandas_pipeline": {
              "datashader/utils.py:Dispatcher:__call__": {
                "[ignored_or_cut_off]": "..."
              }
            }
          },
          "datashader/reductions.py:Reduction:validate": {
            "datashader/datashape/coretypes.py:Record:dict": {
              "datashader/datashape/coretypes.py:Record:fields": {}
            },
            "datashader/datashape/coretypes.py:Mono:measure": {},
            "datashader/datashape/coretypes.py:Record:__getitem__": {
              "datashader/datashape/coretypes.py:Record:dict": {
                "datashader/datashape/coretypes.py:Record:fields": {}
              }
            },
            "datashader/datashape/predicates.py:isnumeric": {
              "datashader/datashape/predicates.py:launder": {},
              "datashader/datashape/coretypes.py:to_numpy_dtype": {
                "datashader/datashape/coretypes.py:Mono:measure": {},
                "datashader/datashape/coretypes.py:to_numpy": {
                  "datashader/datashape/coretypes.py:CType:to_numpy_dtype": {}
                }
              }
            }
          }
        }
      },
      "datashader/transfer_functions/__init__.py:shade": {
        "datashader/transfer_functions/__init__.py:_interpolate": {
          "datashader/transfer_functions/__init__.py:_normalize_interpolate_how": {},
          "datashader/transfer_functions/__init__.py:eq_hist": {},
          "datashader/colors.py:rgb": {
            "datashader/colors.py:hex_to_rgb": {}
          }
        }
      },
      "datashader/tests/test_pipeline.py:color_fn": {
        "datashader/transfer_functions/__init__.py:shade": {
          "datashader/transfer_functions/__init__.py:_interpolate": {
            "datashader/transfer_functions/__init__.py:_normalize_interpolate_how": {},
            "datashader/transfer_functions/__init__.py:eq_hist": {},
            "datashader/colors.py:rgb": {
              "datashader/colors.py:hex_to_rgb": {}
            }
          }
        }
      },
      "datashader/tests/test_pipeline.py:transform_fn": {},
      "datashader/reductions.py:SelfIntersectingFloatingReduction:__init__": {
        "datashader/reductions.py:Reduction:__init__": {}
      }
    },
    "datashader/tests/test_pipeline.py:test_pipeline_antialias": {
      "datashader/glyphs/points.py:_PointLike:__init__": {},
      "datashader/glyphs/line.py:_AntiAliasedLine:set_line_width": {},
      "datashader/pipeline.py:Pipeline:__init__": {},
      "datashader/pipeline.py:Pipeline:__call__": {
        "datashader/core.py:Canvas:__init__": {},
        "datashader/core.py:bypixel": {
          "datashader/core.py:_bypixel_sanitise": {
            "datashader/core.py:_cols_to_keep": {
              "datashader/glyphs/points.py:_PointLike:required_columns": {},
              "datashader/core.py:recurse": {}
            },
            "datashader/utils.py:dshape_from_pandas": {
              "datashader/utils.py:dshape_from_pandas_helper": {
                "datashader/datashape/coretypes.py:CType:from_numpy_dtype": {
                  "datashader/datashape/coretypes.py:Type:lookup_type": {}
                },
                "datashader/datashape/coretypes.py:Mono:__eq__": {
                  "datashader/datashape/coretypes.py:Mono:shape": {},
                  "datashader/datashape/coretypes.py:Mono:measure": {},
                  "datashader/datashape/coretypes.py:Mono:info": {
                    "datashader/datashape/coretypes.py:Mono:parameters": {
                      "datashader/datashape/coretypes.py:Mono:_slotted": {}
                    }
                  }
                }
              },
              "datashader/datashape/coretypes.py:Record:__init__": {
                "datashader/datashape/coretypes.py:_launder": {}
              },
              "datashader/datashape/coretypes.py:Mono:__rmul__": {
                "datashader/datashape/coretypes.py:Fixed:__init__": {},
                "datashader/datashape/coretypes.py:DataShape:__init__": {
                  "datashader/datashape/coretypes.py:_launder": {}
                }
              }
            }
          },
          "datashader/datashape/coretypes.py:DataShape:measure": {
            "datashader/datashape/coretypes.py:Mono:parameters": {
              "datashader/datashape/coretypes.py:Mono:_slotted": {}
            }
          },
          "datashader/glyphs/points.py:_PointLike:validate": {
            "datashader/datashape/coretypes.py:Mono:measure": {},
            "datashader/datashape/coretypes.py:Record:__getitem__": {
              "datashader/datashape/coretypes.py:Record:dict": {
                "datashader/datashape/coretypes.py:Record:fields": {}
              }
            },
            "datashader/utils.py:isreal": {
              "datashader/datashape/predicates.py:launder": {},
              "datashader/datashape/typesets.py:TypeSet:__contains__": {
                "datashader/datashape/typesets.py:TypeSet:_set": {
                  "datashader/datashape/coretypes.py:Mono:__hash__": {}
                },
                "datashader/datashape/coretypes.py:Mono:__hash__": {}
              }
            }
          },
          "datashader/reductions.py:OptionalFieldReduction:validate": {},
          "datashader/core.py:Canvas:validate": {
            "datashader/core.py:Canvas:validate_ranges": {
              "datashader/core.py:Axis:validate": {}
            },
            "datashader/core.py:Canvas:validate_size": {}
          },
          "datashader/utils.py:Dispatcher:__call__": {
            "datashader/data_libraries/pandas.py:pandas_pipeline": {
              "datashader/utils.py:Dispatcher:__call__": {
                "[ignored_or_cut_off]": "..."
              }
            }
          }
        },
        "datashader/transfer_functions/__init__.py:shade": {
          "datashader/transfer_functions/__init__.py:_interpolate": {
            "datashader/transfer_functions/__init__.py:_normalize_interpolate_how": {},
            "datashader/transfer_functions/__init__.py:eq_hist": {},
            "datashader/colors.py:rgb": {
              "datashader/colors.py:hex_to_rgb": {}
            }
          }
        },
        "datashader/transfer_functions/__init__.py:dynspread": {}
      },
      "datashader/reductions.py:SelfIntersectingOptionalFieldReduction:__init__": {
        "datashader/reductions.py:OptionalFieldReduction:__init__": {
          "datashader/reductions.py:Reduction:__init__": {}
        }
      },
      "datashader/core.py:Canvas:line": {
        "datashader/core.py:validate_xy_or_geometry": {},
        "datashader/core.py:_broadcast_column_specifications": {},
        "datashader/glyphs/points.py:_PointLike:__init__": {},
        "datashader/glyphs/line.py:_AntiAliasedLine:set_line_width": {},
        "datashader/core.py:bypixel": {
          "datashader/core.py:_bypixel_sanitise": {
            "datashader/core.py:_cols_to_keep": {
              "datashader/glyphs/points.py:_PointLike:required_columns": {},
              "datashader/core.py:recurse": {}
            },
            "datashader/utils.py:dshape_from_pandas": {
              "datashader/utils.py:dshape_from_pandas_helper": {
                "datashader/datashape/coretypes.py:CType:from_numpy_dtype": {
                  "datashader/datashape/coretypes.py:Type:lookup_type": {}
                },
                "datashader/datashape/coretypes.py:Mono:__eq__": {
                  "datashader/datashape/coretypes.py:Mono:shape": {},
                  "datashader/datashape/coretypes.py:Mono:measure": {},
                  "datashader/datashape/coretypes.py:Mono:info": {
                    "datashader/datashape/coretypes.py:Mono:parameters": {
                      "datashader/datashape/coretypes.py:Mono:_slotted": {}
                    }
                  }
                }
              },
              "datashader/datashape/coretypes.py:Record:__init__": {
                "datashader/datashape/coretypes.py:_launder": {}
              },
              "datashader/datashape/coretypes.py:Mono:__rmul__": {
                "datashader/datashape/coretypes.py:Fixed:__init__": {},
                "datashader/datashape/coretypes.py:DataShape:__init__": {
                  "datashader/datashape/coretypes.py:_launder": {}
                }
              }
            }
          },
          "datashader/datashape/coretypes.py:DataShape:measure": {
            "datashader/datashape/coretypes.py:Mono:parameters": {
              "datashader/datashape/coretypes.py:Mono:_slotted": {}
            }
          },
          "datashader/glyphs/points.py:_PointLike:validate": {
            "datashader/datashape/coretypes.py:Mono:measure": {},
            "datashader/datashape/coretypes.py:Record:__getitem__": {
              "datashader/datashape/coretypes.py:Record:dict": {
                "datashader/datashape/coretypes.py:Record:fields": {}
              }
            },
            "datashader/utils.py:isreal": {
              "datashader/datashape/predicates.py:launder": {},
              "datashader/datashape/typesets.py:TypeSet:__contains__": {
                "datashader/datashape/typesets.py:TypeSet:_set": {
                  "datashader/datashape/coretypes.py:Mono:__hash__": {}
                },
                "datashader/datashape/coretypes.py:Mono:__hash__": {}
              }
            }
          },
          "datashader/reductions.py:OptionalFieldReduction:validate": {},
          "datashader/core.py:Canvas:validate": {
            "datashader/core.py:Canvas:validate_ranges": {
              "datashader/core.py:Axis:validate": {}
            },
            "datashader/core.py:Canvas:validate_size": {}
          },
          "datashader/utils.py:Dispatcher:__call__": {
            "datashader/data_libraries/pandas.py:pandas_pipeline": {
              "datashader/utils.py:Dispatcher:__call__": {
                "[ignored_or_cut_off]": "..."
              }
            }
          }
        }
      },
      "datashader/transfer_functions/__init__.py:shade": {
        "datashader/transfer_functions/__init__.py:_interpolate": {
          "datashader/transfer_functions/__init__.py:_normalize_interpolate_how": {},
          "datashader/transfer_functions/__init__.py:eq_hist": {},
          "datashader/colors.py:rgb": {
            "datashader/colors.py:hex_to_rgb": {}
          }
        }
      },
      "datashader/transfer_functions/__init__.py:dynspread": {}
    }
  },
  "PRD": "# PROJECT NAME: datashader-test_pipeline\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 datashader/\n    \u251c\u2500\u2500 core.py\n    \u2502   \u251c\u2500\u2500 Canvas.line\n    \u2502   \u2514\u2500\u2500 Canvas.points\n    \u251c\u2500\u2500 glyphs/\n    \u2502   \u251c\u2500\u2500 line.py\n    \u2502   \u2502   \u2514\u2500\u2500 _AntiAliasedLine.set_line_width\n    \u2502   \u2514\u2500\u2500 points.py\n    \u2502       \u2514\u2500\u2500 _PointLike.__init__\n    \u251c\u2500\u2500 pipeline.py\n    \u2502   \u251c\u2500\u2500 Pipeline.__call__\n    \u2502   \u2514\u2500\u2500 Pipeline.__init__\n    \u251c\u2500\u2500 reductions.py\n    \u2502   \u251c\u2500\u2500 SelfIntersectingFloatingReduction.__init__\n    \u2502   \u2514\u2500\u2500 SelfIntersectingOptionalFieldReduction.__init__\n    \u2514\u2500\u2500 transfer_functions/\n        \u2514\u2500\u2500 __init__.py\n            \u251c\u2500\u2500 dynspread\n            \u2514\u2500\u2500 shade\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThis module is designed to facilitate advanced data visualization and aggregation workflows by leveraging the capabilities of Datashader, a library for rendering large-scale datasets into visual representations. Its primary purpose is to define and execute flexible data rendering pipelines that allow users to customize aggregation, transformation, and coloration of datasets with high precision. The module supports operations such as point and line plotting, data aggregation with various reduction functions (e.g., sum, count), and the application of transformation and shading functions for enhanced visualization. By enabling users and developers to efficiently process and render large datasets into pixel-based visual output, it solves the problem of scaling interactive visual analysis to massive datasets, ensuring clarity and performance even with complex data inputs.\n\n## FILE 1: datashader/core.py\n\n- CLASS METHOD: Canvas.points\n  - CLASS SIGNATURE: class Canvas:\n  - SIGNATURE: def points(self, source, x=None, y=None, agg=None, geometry=None):\n  - DOCSTRING: \n```python\n\"\"\"\nCompute a reduction by pixel, mapping data to pixels as points.\n\nParameters\n----------\nsource : pandas.DataFrame, dask.DataFrame, or xarray.DataArray/Dataset\n    The input datasource containing the data to be rendered as points.\nx, y : str, optional\n    Column names for the x and y coordinates of each point. At least one of these must be provided unless `geometry` is used.\nagg : Reduction, optional\n    Reduction to compute from the data; default is `count()`, which counts the number of points in each pixel.\ngeometry : str, optional\n    Column name containing a PointsArray of the coordinates of each point. If provided, `x` and `y` must not be specified.\n\nReturns\n-------\nAggregate results mapped to pixels, as defined by the specified parameters. This aggregate is computed using the `bypixel` function, which handles the data normalization and reduction.\n\nNotes\n-----\n- The function relies on `validate_xy_or_geometry` to ensure that the input parameters are valid, meaning that either both `x` and `y` or `geometry` must be specified.\n- If `geometry` is used, the method may utilize `MultiPointGeometry` from the `.glyphs` module when processing `spatialpandas` or `geopandas` data sources.\n- By default, if `agg` is not provided, a count reduction is used to determine the number of occurrences per pixel.\n\nDependencies\n------------\n- The function imports `Point` and `MultiPointGeometry` from the `.glyphs` module to create appropriate geometrical representations of the data points.\n- It utilizes `count` from the `.reductions` module to provide a default aggregation method.\n- The code also integrates with spatial data libraries such as `spatialpandas` and `geopandas` to handle geographical dataframes and potentially filter data based on viewport partitions.\n\"\"\"\n```\n\n- CLASS METHOD: Canvas.line\n  - CLASS SIGNATURE: class Canvas:\n  - SIGNATURE: def line(self, source, x=None, y=None, agg=None, axis=0, geometry=None, line_width=0, antialias=False):\n  - DOCSTRING: \n```python\n\"\"\"\nCompute a reduction by pixel, representing data as one or more lines on a canvas.\n\nParameters\n----------\nsource : pandas.DataFrame, dask.DataFrame, or xarray.DataArray/Dataset\n    The input data source containing the coordinates.\nx, y : str or number or list or tuple or np.ndarray\n    Specifications for the x and y coordinates of each vertex.\nagg : Reduction, optional\n    Reduction operation to compute, defaulting to `any()`.\naxis : 0 or 1, default 0\n    Axis along which to draw lines, where 0 uses all rows and 1 draws one line per row.\ngeometry : str, optional\n    Name of a LinesArray in the source that defines the line coordinates, if provided x and y may not be.\nline_width : number, optional\n    Specifies the width of the line in pixels. A value of 0 results in a single-pixel width.\nantialias : bool, optional\n    Enables antialiasing for smoother lines; interpreted as `True` for line_width=1 and `False` for line_width=0. Cannot be specified with line_width.\n\nReturns\n-------\nAggregated data representing the lines drawn across the canvas.\n\nNotes\n-----\nThis method interacts with the private method `_source_from_geopandas` to validate and convert source data if it originates from GeoPandas or Dask GeoDataFrames. The line drawing logic utilizes glyphs (from `.glyphs` module) to determine how to render the lines based on the provided coordinates and aggregation operation. \n\nThe function also makes use of constants defined in `reductions` (imported as `rd`), particularly for defining default behavior and supported aggregation types. Furthermore, it checks compatibility with CUDA-backed sources pertaining to line width and antialiasing capabilities.\n\"\"\"\n```\n\n## FILE 2: datashader/glyphs/points.py\n\n- CLASS METHOD: _PointLike.__init__\n  - CLASS SIGNATURE: class _PointLike(Glyph):\n  - SIGNATURE: def __init__(self, x, y):\n  - DOCSTRING: \n```python\n\"\"\"\nInitialize a _PointLike glyph instance representing a point with specified coordinates.\n\nParameters\n----------\nx : str\n    The column name representing the x coordinate of each point.\ny : str\n    The column name representing the y coordinate of each point.\n\nAttributes\n----------\nself.x : str\n    Stores the x coordinate column name.\nself.y : str\n    Stores the y coordinate column name.\n\nThe _PointLike class serves as a base for creating point-like visualizations in data structures where point coordinates are involved. It provides validation and boundary computation functionalities using the specified x and y coordinates.\n\"\"\"\n```\n\n## FILE 3: datashader/pipeline.py\n\n- CLASS METHOD: Pipeline.__call__\n  - CLASS SIGNATURE: class Pipeline:\n  - SIGNATURE: def __call__(self, x_range=None, y_range=None, width=600, height=600):\n  - DOCSTRING: \n```python\n\"\"\"\nCompute an image from the specified data pipeline, which visualizes data from a Pandas or Dask DataFrame.\n\nParameters\n----------\nx_range : tuple, optional\n    The bounding box on the x-axis of the viewport, specified as a tuple of (min, max).\ny_range : tuple, optional\n    The bounding box on the y-axis of the viewport, specified as a tuple of (min, max).\nwidth : int, optional\n    The width of the output image (default is 600).\nheight : int, optional\n    The height of the output image (default is 600).\n\nReturns\n-------\nImage\n    An image generated from the data according to the specified pipeline, which includes \n    binning, transformation, coloring, and spreading.\n\nNotes\n-----\nThe method utilizes the `core.Canvas`, `core.bypixel`, and the provided scaling factors (`width_scale`, `height_scale`) to determine the dimensions of the canvas and the resultant image. The aggregation function (`agg`), transformation function (`transform_fn`), coloring function (`color_fn`), and spreading function (`spread_fn`) are applied sequentially to derive the final image output. The method is designed to work with the `Pipeline` class, which encapsulates the process of visualizing data with customizable options.\n\"\"\"\n```\n\n- CLASS METHOD: Pipeline.__init__\n  - CLASS SIGNATURE: class Pipeline:\n  - SIGNATURE: def __init__(self, df, glyph, agg=reductions.count(), transform_fn=identity, color_fn=tf.shade, spread_fn=tf.dynspread, width_scale=1.0, height_scale=1.0):\n  - DOCSTRING: \n```python\n\"\"\"\nInitialize a Pipeline instance for creating a datashading pipeline callback.\n\nParameters\n----------\ndf : pandas.DataFrame, dask.DataFrame\n    The data source to be visualized.\nglyph : Glyph\n    The type of glyph used for visualization.\nagg : Reduction, optional\n    The aggregation function to compute per-pixel. Defaults to ``count()`` from the reductions module.\ntransform_fn : callable, optional\n    A preprocessing function that transforms the computed aggregate before passing it to the color function. Defaults to the identity function from the toolz library.\ncolor_fn : callable, optional\n    A function to generate an Image from the transformed aggregate. Defaults to the ``shade`` function from the transfer_functions module.\nspread_fn : callable, optional\n    A function for post-processing the Image. Defaults to the ``dynspread`` function from the transfer_functions module.\nwidth_scale : float, optional\n    A scaling factor for the image width. Defaults to 1.0.\nheight_scale : float, optional\n    A scaling factor for the image height. Defaults to 1.0.\n\nAttributes\n----------\nself.df : pandas.DataFrame or dask.DataFrame\n    Stores the input data.\nself.glyph : Glyph\n    Stores the glyph type.\nself.agg : Reduction\n    Stores the aggregation function.\nself.transform_fn : callable\n    Stores the transformation function.\nself.color_fn : callable\n    Stores the color function.\nself.spread_fn : callable\n    Stores the spreading function.\nself.width_scale : float\n    Stores the width scaling factor.\nself.height_scale : float\n    Stores the height scaling factor.\n\"\"\"\n```\n\n## FILE 4: datashader/transfer_functions/__init__.py\n\n- FUNCTION NAME: shade\n  - SIGNATURE: def shade(agg, cmap=['lightblue', 'darkblue'], color_key=Sets1to3, how='eq_hist', alpha=255, min_alpha=40, span=None, name=None, color_baseline=None, rescale_discrete_levels=False):\n  - DOCSTRING: \n```python\n\"\"\"\nConvert a `DataArray` to an image by assigning RGBA pixel colors to each value, based on specified colormap and alpha settings. This function supports both 2D and 3D coordinate `DataArrays`, enabling color mapping for continuous values and discrete categories.\n\nParameters\n----------\nagg : xr.DataArray\n    Input data array to convert into an image. Must be either 2D or 3D.\ncmap : list of colors or matplotlib.colors.Colormap, optional\n    Colormap for coloring 2D aggregates; can be a list of colors or a matplotlib colormap. Default is `[\"lightblue\", \"darkblue\"]`.\ncolor_key : dict or iterable, optional\n    Specifies colors for discrete categories in 2D case or categorical data in 3D case.\nhow : str or callable, optional\n    Interpolation method for colormapping. Options include 'eq_hist' (default), 'cbrt', 'log', and 'linear'.\nalpha : int, optional\n    Alpha value (0-255) for non-NaN data pixels.\nmin_alpha : float, optional\n    Minimum alpha value for data pixels (0-255). Default is 40.\nspan : list of min-max range, optional\n    Overrides default autoranging for 2D colormapping and 3D alpha interpolation.\nname : string, optional\n    Optional name for the resulting Image object.\ncolor_baseline : float or None, optional\n    Baseline for color mixing in categorical data; defaults to None, which uses the minimum aggregate value.\nrescale_discrete_levels : boolean, optional\n    If True, adjusts span for better visibility in case of few discrete values. Default is False.\n\nReturns\n-------\nImage\n    An image representation of the input data, encoded as an Image object.\n\nRaises\n------\nTypeError\n    If `agg` is not an instance of `DataArray`.\nValueError\n    If `min_alpha` and `alpha` are not in the range 0-255, or if `agg` does not have 2D or 3D coordinates.\n\nNotes\n-----\nUtilizes helper functions `_interpolate`, `_colorize`, and `_apply_discrete_colorkey` from the same module for handling different types of data arrays. Color definitions may rely on `Sets1to3` imported from `datashader.colors`. Ensure all dependencies such as `matplotlib` for colormaps are available.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - datashader/transfer_functions/__init__.py:_interpolate\n\n- FUNCTION NAME: dynspread\n  - SIGNATURE: def dynspread(img, threshold=0.5, max_px=3, shape='circle', how=None, name=None):\n  - DOCSTRING: \n```python\n\"\"\"\nSpread pixels in an image dynamically based on the image density, allowing for sparse plots to become more visible. The spread initiates at one pixel and continues until either the specified threshold of adjacent non-empty pixels is reached or the maximum allowed spread (`max_px`) is enforced.\n\nParameters\n----------\nimg : Image\n    The input image that is subject to pixel spreading.\nthreshold : float, optional\n    A value in the range [0, 1] that determines the spreading effect; higher values yield more extensive spreading.\nmax_px : int, optional\n    The maximum number of pixels to spread on each side, with a default of 3.\nshape : str, optional\n    The geometric shape used for spreading, defaulting to 'circle'; can also be 'square'.\nhow : str, optional\n    The compositing operator to combine pixels during spreading. Defaults to 'over' for Image objects and 'add' for others.\nname : string, optional\n    An optional name for the resulting Image object.\n\nReturns\n-------\nImage\n    The modified image with pixels spread according to the specified parameters.\n\nRaises\n------\nValueError\n    If the threshold is not within the range [0, 1] or if `max_px` is not a non-negative integer.\n\nNotes\n-----\nThe function leverages helper functions like `_array_density` and `_rgb_density` to compute the density of non-empty pixels in determining how far to spread. It also checks if the image data is in a cupy array, converting it to a numpy array as necessary. This ensures compatibility across different array types.\n\"\"\"\n```\n\n## FILE 5: datashader/reductions.py\n\n- CLASS METHOD: SelfIntersectingFloatingReduction.__init__\n  - CLASS SIGNATURE: class SelfIntersectingFloatingReduction(FloatingReduction):\n  - SIGNATURE: def __init__(self, column=None, self_intersect=True):\n  - DOCSTRING: \n```python\n\"\"\"\nInitializes a SelfIntersectingFloatingReduction instance which is a specialized reduction class for modified floating-point reductions where self-intersecting geometry may or may not be desired.\n\nParameters\n----------\ncolumn : str, optional\n    The column name to aggregate over. Defaults to None.\nself_intersect : bool, optional\n    A flag indicating whether self-intersecting geometry is allowed. Defaults to True.\n\nAttributes\n----------\nself_intersect : bool\n    Stores the value of the self_intersect parameter, controlling whether the reduction considers self-intersecting geometries during computations.\n\nThis class inherits from FloatingReduction and is primarily used in scenarios where fair aggregation behavior is required when dealing with complex geometries. The `self_intersect` attribute influences whether certain optimizations or processing techniques are applied during the reduction operations.\n\"\"\"\n```\n\n- CLASS METHOD: SelfIntersectingOptionalFieldReduction.__init__\n  - CLASS SIGNATURE: class SelfIntersectingOptionalFieldReduction(OptionalFieldReduction):\n  - SIGNATURE: def __init__(self, column=None, self_intersect=True):\n  - DOCSTRING: \n```python\n\"\"\"\nInitialize a SelfIntersectingOptionalFieldReduction instance.\n\nThis class extends OptionalFieldReduction to support reductions where \nself-intersecting geometry may or may not be desirable when using antialiasing. \nIt combines the concept of optional fields, allowing a reduction without a specific field, \nwhile also factoring in the self-intersection of geometries in the calculation.\n\nParameters\n----------\ncolumn : str or SpecialColumn, optional\n    The name of the column to perform the reduction on. If None, the reduction will \n    not specify a column. The SpecialColumn enumeration is used to represent specific \n    functionalities like RowIndex.\n    \nself_intersect : bool, default=True\n    A flag indicating whether self-intersection of geometries should be considered during \n    the reduction. If set to True, self-intersection is accounted for in the calculation,\n    affecting the way reductions are computed, especially in antialiased rendering.\n\nAttributes\n----------\ncolumn : str or SpecialColumn\n    Stores the name or identifier of the column associated with the reduction.\n\nself_intersect : bool\n    Indicates if self-intersection needs to be considered during the reduction process.\n\nThis initialization is important for setting up the appropriate reduction strategy \nbased on whether the reduction is expected to handle self-intersecting geometries, \nwhich can alter performance and the accuracy of results in graphical computations.\n\"\"\"\n```\n\n## FILE 6: datashader/glyphs/line.py\n\n- CLASS METHOD: _AntiAliasedLine.set_line_width\n  - CLASS SIGNATURE: class _AntiAliasedLine:\n  - SIGNATURE: def set_line_width(self, line_width):\n  - DOCSTRING: \n```python\n\"\"\"\nSet the line width for antialiased line rendering.\n\nParameters\n----------\nline_width : float\n    The width of the line to be rendered. A line width greater than zero enables antialiasing.\n    \nReturns\n-------\nNone\n\nSide Effects\n------------\nIf the instance has an attribute `antialiased`, its value is set to `True` if `line_width` is greater than zero, indicating that antialiasing will be applied to the line rendering.\n\nConstants\n---------\nThe attribute `_line_width` is an integer initialized to 0, which stores the width of the line. This attribute is essential for determining the rendering style (antialiased or not) based on the specified line width.\n\"\"\"\n```\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "datashader/core.py": "from __future__ import annotations\nfrom numbers import Number\nfrom math import log10\nimport warnings\nimport contextlib\nimport numpy as np\nimport pandas as pd\nfrom packaging.version import Version\nfrom xarray import DataArray, Dataset\nfrom .utils import Dispatcher, ngjit, calc_res, calc_bbox, orient_array, dshape_from_xarray_dataset\nfrom .utils import get_indices, dshape_from_pandas, dshape_from_dask\nfrom .utils import Expr\nfrom .resampling import resample_2d, resample_2d_distributed\nfrom . import reductions as rd\ntry:\n    import dask.dataframe as dd\n    import dask.array as da\nexcept ImportError:\n    dd, da = (None, None)\ntry:\n    import cudf\nexcept Exception:\n    cudf = None\ntry:\n    import dask_cudf\nexcept Exception:\n    dask_cudf = None\ntry:\n    import spatialpandas\nexcept Exception:\n    spatialpandas = None\n\nclass Axis:\n    \"\"\"Interface for implementing axis transformations.\n\n    Instances hold implementations of transformations to and from axis space.\n    The default implementation is equivalent to:\n\n    >>> def forward_transform(data_x):\n    ...     scale * mapper(data_x) + t\n    >>> def inverse_transform(axis_x):\n    ...     inverse_mapper((axis_x - t)/s)\n\n    Where ``mapper`` and ``inverse_mapper`` are elementwise functions mapping\n    to and from axis-space respectively, and ``scale`` and ``transform`` are\n    parameters describing a linear scale and translate transformation, computed\n    by the ``compute_scale_and_translate`` method.\n    \"\"\"\n\n    def compute_scale_and_translate(self, range, n):\n        \"\"\"Compute the scale and translate parameters for a linear transformation\n        ``output = s * input + t``, mapping from data space to axis space.\n\n        Parameters\n        ----------\n        range : tuple\n            A tuple representing the range ``[min, max]`` along the axis, in\n            data space. Both min and max are inclusive.\n        n : int\n            The number of bins along the axis.\n\n        Returns\n        -------\n        s, t : floats\n        \"\"\"\n        start, end = map(self.mapper, range)\n        s = n / (end - start)\n        t = -start * s\n        return (s, t)\n\n    def compute_index(self, st, n):\n        \"\"\"Compute a 1D array representing the axis index.\n\n        Parameters\n        ----------\n        st : tuple\n            A tuple of ``(scale, translate)`` parameters.\n        n : int\n            The number of bins along the dimension.\n\n        Returns\n        -------\n        index : ndarray\n        \"\"\"\n        px = np.arange(n) + 0.5\n        s, t = st\n        return self.inverse_mapper((px - t) / s)\n\n    def mapper(val):\n        \"\"\"A mapping from data space to axis space\"\"\"\n        raise NotImplementedError\n\n    def inverse_mapper(val):\n        \"\"\"A mapping from axis space to data space\"\"\"\n        raise NotImplementedError\n\n    def validate(self, range):\n        \"\"\"Given a range (low,high), raise an error if the range is invalid for this axis\"\"\"\n        pass\n\nclass LinearAxis(Axis):\n    \"\"\"A linear Axis\"\"\"\n\n    @staticmethod\n    @ngjit\n    def mapper(val):\n        return val\n\n    @staticmethod\n    @ngjit\n    def inverse_mapper(val):\n        return val\n\nclass LogAxis(Axis):\n    \"\"\"A base-10 logarithmic Axis\"\"\"\n\n    @staticmethod\n    @ngjit\n    def mapper(val):\n        return log10(float(val))\n\n    @staticmethod\n    @ngjit\n    def inverse_mapper(val):\n        y = 10\n        return y ** val\n\n    def validate(self, range):\n        if range is None:\n            return\n        if range[0] <= 0 or range[1] <= 0:\n            raise ValueError('Range values must be >0 for logarithmic axes')\n_axis_lookup = {'linear': LinearAxis(), 'log': LogAxis()}\n\ndef validate_xy_or_geometry(glyph, x, y, geometry):\n    if geometry is None and (x is None or y is None) or (geometry is not None and (x is not None or y is not None)):\n        raise ValueError('\\n{glyph} coordinates may be specified by providing both the x and y arguments, or by\\nproviding the geometry argument. Received:\\n    x: {x}\\n    y: {y}\\n    geometry: {geometry}\\n'.format(glyph=glyph, x=repr(x), y=repr(y), geometry=repr(geometry)))\n\nclass Canvas:\n    \"\"\"An abstract canvas representing the space in which to bin.\n\n    Parameters\n    ----------\n    plot_width, plot_height : int, optional\n        Width and height of the output aggregate in pixels.\n    x_range, y_range : tuple, optional\n        A tuple representing the bounds inclusive space ``[min, max]`` along\n        the axis.\n    x_axis_type, y_axis_type : str, optional\n        The type of the axis. Valid options are ``'linear'`` [default], and\n        ``'log'``.\n    \"\"\"\n\n    def __init__(self, plot_width=600, plot_height=600, x_range=None, y_range=None, x_axis_type='linear', y_axis_type='linear'):\n        self.plot_width = plot_width\n        self.plot_height = plot_height\n        self.x_range = None if x_range is None else tuple(x_range)\n        self.y_range = None if y_range is None else tuple(y_range)\n        self.x_axis = _axis_lookup[x_axis_type]\n        self.y_axis = _axis_lookup[y_axis_type]\n\n    def area(self, source, x, y, agg=None, axis=0, y_stack=None):\n        \"\"\"Compute a reduction by pixel, mapping data to pixels as a filled\n        area region\n\n        Parameters\n        ----------\n        source : pandas.DataFrame, dask.DataFrame, or xarray.DataArray/Dataset\n            The input datasource.\n        x, y : str or number or list or tuple or np.ndarray\n            Specification of the x and y coordinates of each vertex of the\n            line defining the starting edge of the area region.\n            * str or number: Column labels in source\n            * list or tuple: List or tuple of column labels in source\n            * np.ndarray: When axis=1, a literal array of the\n              coordinates to be used for every row\n        agg : Reduction, optional\n            Reduction to compute. Default is ``count()``.\n        axis : 0 or 1, default 0\n            Axis in source to draw lines along\n            * 0: Draw area regions using data from the specified columns\n                 across all rows in source\n            * 1: Draw one area region per row in source using data from the\n                 specified columns\n        y_stack: str or number or list or tuple or np.ndarray or None\n            Specification of the y coordinates of each vertex of the line\n            defining the ending edge of the area region, where the x\n            coordinate is given by the x argument described above.\n\n            If y_stack is None, then the area region is filled to the y=0 line\n\n            If y_stack is not None, then the form of y_stack must match the\n            form of y.\n\n        Examples\n        --------\n        Define a canvas and a pandas DataFrame with 6 rows\n        >>> import pandas as pd  # doctest: +SKIP\n        ... import numpy as np\n        ... import datashader as ds\n        ... from datashader import Canvas\n        ... import datashader.transfer_functions as tf\n        ... cvs = Canvas()\n        ... df = pd.DataFrame({\n        ...    'A1': [1, 1.5, 2, 2.5, 3, 4],\n        ...    'A2': [1.6, 2.1, 2.9, 3.2, 4.2, 5],\n        ...    'B1': [10, 12, 11, 14, 13, 15],\n        ...    'B2': [11, 9, 10, 7, 8, 12],\n        ... }, dtype='float64')\n\n        Aggregate one area region across all rows, that starts with\n        coordinates df.A1 by df.B1 and is filled to the y=0 line\n        >>> agg = cvs.area(df, x='A1', y='B1',  # doctest: +SKIP\n        ...                agg=ds.count(), axis=0)\n        ... tf.shade(agg)\n\n        Aggregate one area region across all rows, that starts with\n        coordinates df.A1 by df.B1 and is filled to the line with coordinates\n        df.A1 by df.B2\n        >>> agg = cvs.area(df, x='A1', y='B1', y_stack='B2', # doctest: +SKIP\n        ...                agg=ds.count(), axis=0)\n        ... tf.shade(agg)\n\n        Aggregate two area regions across all rows. The first starting\n        with coordinates df.A1 by df.B1 and the second with coordinates\n        df.A2 by df.B2. Both regions are filled to the y=0 line\n        >>> agg = cvs.area(df, x=['A1', 'A2'], y=['B1', 'B2'],  # doctest: +SKIP\n                           agg=ds.count(), axis=0)\n        ... tf.shade(agg)\n\n        Aggregate two area regions across all rows where the regions share the\n        same x coordinates. The first region will start with coordinates\n        df.A1 by df.B1 and the second will start with coordinates\n        df.A1 by df.B2. Both regions are filled to the y=0 line\n        >>> agg = cvs.area(df, x='A1', y=['B1', 'B2'], agg=ds.count(), axis=0)  # doctest: +SKIP\n        ... tf.shade(agg)\n\n        Aggregate 6 length-2 area regions, one per row, where the ith region\n        starts with coordinates [df.A1[i], df.A2[i]] by [df.B1[i], df.B2[i]]\n        and is filled to the y=0 line\n        >>> agg = cvs.area(df, x=['A1', 'A2'], y=['B1', 'B2'],  # doctest: +SKIP\n                           agg=ds.count(), axis=1)\n        ... tf.shade(agg)\n\n        Aggregate 6 length-4 area regions, one per row, where the\n        starting x coordinates of every region are [0, 1, 2, 3] and\n        the starting y coordinates of the ith region are\n        [df.A1[i], df.A2[i], df.B1[i], df.B2[i]].  All regions are filled to\n        the y=0 line\n        >>> agg = cvs.area(df,  # doctest: +SKIP\n        ...                x=np.arange(4),\n        ...                y=['A1', 'A2', 'B1', 'B2'],\n        ...                agg=ds.count(),\n        ...                axis=1)\n        ... tf.shade(agg)\n\n        Aggregate RaggedArrays of variable length area regions, one per row.\n        The starting coordinates of the ith region are df_ragged.A1 by\n        df_ragged.B1 and the regions are filled to the y=0 line.\n        (requires pandas >= 0.24.0)\n        >>> df_ragged = pd.DataFrame({  # doctest: +SKIP\n        ...    'A1': pd.array([\n        ...        [1, 1.5], [2, 2.5, 3], [1.5, 2, 3, 4], [3.2, 4, 5]],\n        ...        dtype='Ragged[float32]'),\n        ...    'B1': pd.array([\n        ...        [10, 12], [11, 14, 13], [10, 7, 9, 10], [7, 8, 12]],\n        ...        dtype='Ragged[float32]'),\n        ...    'B2': pd.array([\n        ...        [6, 10], [9, 10, 18], [9, 5, 6, 8], [4, 5, 11]],\n        ...        dtype='Ragged[float32]'),\n        ...    'group': pd.Categorical([0, 1, 2, 1])\n        ... })\n        ...\n        ... agg = cvs.area(df_ragged, x='A1', y='B1', agg=ds.count(), axis=1)\n        ... tf.shade(agg)\n\n        Instead of filling regions to the y=0 line, fill to the line with\n        coordinates df_ragged.A1 by df_ragged.B2\n        >>> agg = cvs.area(df_ragged, x='A1', y='B1', y_stack='B2', # doctest: +SKIP\n        ...                agg=ds.count(), axis=1)\n        ... tf.shade(agg)\n\n        (requires pandas >= 0.24.0)\n        \"\"\"\n        from .glyphs import AreaToZeroAxis0, AreaToLineAxis0, AreaToZeroAxis0Multi, AreaToLineAxis0Multi, AreaToZeroAxis1, AreaToLineAxis1, AreaToZeroAxis1XConstant, AreaToLineAxis1XConstant, AreaToZeroAxis1YConstant, AreaToLineAxis1YConstant, AreaToZeroAxis1Ragged, AreaToLineAxis1Ragged\n        from .reductions import any as any_rdn\n        if agg is None:\n            agg = any_rdn()\n        orig_x, orig_y, orig_y_stack = (x, y, y_stack)\n        x, y, y_stack = _broadcast_column_specifications(x, y, y_stack)\n        if axis == 0:\n            if y_stack is None:\n                if isinstance(x, (Number, str)) and isinstance(y, (Number, str)):\n                    glyph = AreaToZeroAxis0(x, y)\n                elif isinstance(x, (list, tuple)) and isinstance(y, (list, tuple)):\n                    glyph = AreaToZeroAxis0Multi(tuple(x), tuple(y))\n                else:\n                    raise ValueError('\\nInvalid combination of x and y arguments to Canvas.area when axis=0.\\n    Received:\\n        x: {x}\\n        y: {y}\\nSee docstring for more information on valid usage'.format(x=repr(x), y=repr(y)))\n            elif isinstance(x, (Number, str)) and isinstance(y, (Number, str)) and isinstance(y_stack, (Number, str)):\n                glyph = AreaToLineAxis0(x, y, y_stack)\n            elif isinstance(x, (list, tuple)) and isinstance(y, (list, tuple)) and isinstance(y_stack, (list, tuple)):\n                glyph = AreaToLineAxis0Multi(tuple(x), tuple(y), tuple(y_stack))\n            else:\n                raise ValueError('\\nInvalid combination of x, y, and y_stack arguments to Canvas.area when axis=0.\\n    Received:\\n        x: {x}\\n        y: {y}\\n        y_stack: {y_stack}\\nSee docstring for more information on valid usage'.format(x=repr(orig_x), y=repr(orig_y), y_stack=repr(orig_y_stack)))\n        elif axis == 1:\n            if y_stack is None:\n                if isinstance(x, (list, tuple)) and isinstance(y, (list, tuple)):\n                    glyph = AreaToZeroAxis1(tuple(x), tuple(y))\n                elif isinstance(x, np.ndarray) and isinstance(y, (list, tuple)):\n                    glyph = AreaToZeroAxis1XConstant(x, tuple(y))\n                elif isinstance(x, (list, tuple)) and isinstance(y, np.ndarray):\n                    glyph = AreaToZeroAxis1YConstant(tuple(x), y)\n                elif isinstance(x, (Number, str)) and isinstance(y, (Number, str)):\n                    glyph = AreaToZeroAxis1Ragged(x, y)\n                else:\n                    raise ValueError('\\nInvalid combination of x and y arguments to Canvas.area when axis=1.\\n    Received:\\n        x: {x}\\n        y: {y}\\nSee docstring for more information on valid usage'.format(x=repr(x), y=repr(y)))\n            elif isinstance(x, (list, tuple)) and isinstance(y, (list, tuple)) and isinstance(y_stack, (list, tuple)):\n                glyph = AreaToLineAxis1(tuple(x), tuple(y), tuple(y_stack))\n            elif isinstance(x, np.ndarray) and isinstance(y, (list, tuple)) and isinstance(y_stack, (list, tuple)):\n                glyph = AreaToLineAxis1XConstant(x, tuple(y), tuple(y_stack))\n            elif isinstance(x, (list, tuple)) and isinstance(y, np.ndarray) and isinstance(y_stack, np.ndarray):\n                glyph = AreaToLineAxis1YConstant(tuple(x), y, y_stack)\n            elif isinstance(x, (Number, str)) and isinstance(y, (Number, str)) and isinstance(y_stack, (Number, str)):\n                glyph = AreaToLineAxis1Ragged(x, y, y_stack)\n            else:\n                raise ValueError('\\nInvalid combination of x, y, and y_stack arguments to Canvas.area when axis=1.\\n    Received:\\n        x: {x}\\n        y: {y}\\n        y_stack: {y_stack}\\nSee docstring for more information on valid usage'.format(x=repr(orig_x), y=repr(orig_y), y_stack=repr(orig_y_stack)))\n        else:\n            raise ValueError('\\nThe axis argument to Canvas.area must be 0 or 1\\n    Received: {axis}'.format(axis=axis))\n        return bypixel(source, self, glyph, agg)\n\n    def polygons(self, source, geometry, agg=None):\n        \"\"\"Compute a reduction by pixel, mapping data to pixels as one or\n        more filled polygons.\n\n        Parameters\n        ----------\n        source : xarray.DataArray or Dataset\n            The input datasource.\n        geometry : str\n            Column name of a PolygonsArray of the coordinates of each line.\n        agg : Reduction, optional\n            Reduction to compute. Default is ``any()``.\n\n        Returns\n        -------\n        data : xarray.DataArray\n\n        Examples\n        --------\n        >>> import datashader as ds  # doctest: +SKIP\n        ... import datashader.transfer_functions as tf\n        ... from spatialpandas.geometry import PolygonArray\n        ... from spatialpandas import GeoDataFrame\n        ... import pandas as pd\n        ...\n        ... polygons = PolygonArray([\n        ...     # First Element\n        ...     [[0, 0, 1, 0, 2, 2, -1, 4, 0, 0],  # Filled quadrilateral (CCW order)\n        ...      [0.5, 1,  1, 2,  1.5, 1.5,  0.5, 1],     # Triangular hole (CW order)\n        ...      [0, 2, 0, 2.5, 0.5, 2.5, 0.5, 2, 0, 2],  # Rectangular hole (CW order)\n        ...      [2.5, 3, 3.5, 3, 3.5, 4, 2.5, 3],  # Filled triangle\n        ...     ],\n        ...\n        ...     # Second Element\n        ...     [[3, 0, 3, 2, 4, 2, 4, 0, 3, 0],  # Filled rectangle (CCW order)\n        ...      # Rectangular hole (CW order)\n        ...      [3.25, 0.25, 3.75, 0.25, 3.75, 1.75, 3.25, 1.75, 3.25, 0.25],\n        ...     ]\n        ... ])\n        ...\n        ... df = GeoDataFrame({'polygons': polygons, 'v': range(len(polygons))})\n        ...\n        ... cvs = ds.Canvas()\n        ... agg = cvs.polygons(df, geometry='polygons', agg=ds.sum('v'))\n        ... tf.shade(agg)\n        \"\"\"\n        from .glyphs import PolygonGeom\n        from .reductions import any as any_rdn\n        if spatialpandas and isinstance(source, spatialpandas.dask.DaskGeoDataFrame):\n            x_range = self.x_range if self.x_range is not None else (None, None)\n            y_range = self.y_range if self.y_range is not None else (None, None)\n            source = source.cx_partitions[slice(*x_range), slice(*y_range)]\n            glyph = PolygonGeom(geometry)\n        elif spatialpandas and isinstance(source, spatialpandas.GeoDataFrame):\n            glyph = PolygonGeom(geometry)\n        elif (geopandas_source := self._source_from_geopandas(source)) is not None:\n            source = geopandas_source\n            from .glyphs.polygon import GeopandasPolygonGeom\n            glyph = GeopandasPolygonGeom(geometry)\n        else:\n            raise ValueError(f'source must be an instance of spatialpandas.GeoDataFrame, spatialpandas.dask.DaskGeoDataFrame, geopandas.GeoDataFrame or dask_geopandas.GeoDataFrame, not {type(source)}')\n        if agg is None:\n            agg = any_rdn()\n        return bypixel(source, self, glyph, agg)\n\n    def quadmesh(self, source, x=None, y=None, agg=None):\n        \"\"\"Samples a recti- or curvi-linear quadmesh by canvas size and bounds.\n\n        Parameters\n        ----------\n        source : xarray.DataArray or Dataset\n            The input datasource.\n        x, y : str\n            Column names for the x and y coordinates of each point.\n        agg : Reduction, optional\n            Reduction to compute. Default is ``mean()``. Note that agg is ignored when\n            upsampling.\n\n        Returns\n        -------\n        data : xarray.DataArray\n        \"\"\"\n        from .glyphs import QuadMeshRaster, QuadMeshRectilinear, QuadMeshCurvilinear\n        from .reductions import mean as mean_rnd\n        if isinstance(source, Dataset):\n            if agg is None or agg.column is None:\n                name = list(source.data_vars)[0]\n            else:\n                name = agg.column\n            source = source[[name]]\n        elif isinstance(source, DataArray):\n            name = source.name\n            source = source.to_dataset()\n        else:\n            raise ValueError('Invalid input type')\n        if agg is None:\n            agg = mean_rnd(name)\n        if x is None and y is None:\n            y, x = source[name].dims\n        elif not x or not y:\n            raise ValueError('Either specify both x and y coordinatesor allow them to be inferred.')\n        yarr, xarr = (source[y], source[x])\n        if (yarr.ndim > 1 or xarr.ndim > 1) and xarr.dims != yarr.dims:\n            raise ValueError('Ensure that x- and y-coordinate arrays share the same dimensions. x-coordinates are indexed by %s dims while y-coordinates are indexed by %s dims.' % (xarr.dims, yarr.dims))\n        if name is not None and agg.column is not None and (agg.column != name):\n            raise ValueError('DataArray name %r does not match supplied reduction %s.' % (source.name, agg))\n        if xarr.ndim == 1:\n            xaxis_linear = self.x_axis is _axis_lookup['linear']\n            yaxis_linear = self.y_axis is _axis_lookup['linear']\n            even_yspacing = np.allclose(yarr, np.linspace(yarr[0].data, yarr[-1].data, len(yarr)))\n            even_xspacing = np.allclose(xarr, np.linspace(xarr[0].data, xarr[-1].data, len(xarr)))\n            if xaxis_linear and yaxis_linear and even_xspacing and even_yspacing:\n                glyph = QuadMeshRaster(x, y, name)\n                upsample_width, upsample_height = glyph.is_upsample(source, x, y, name, self.x_range, self.y_range, self.plot_width, self.plot_height)\n                if upsample_width and upsample_height:\n                    agg = rd._upsample(name)\n                    return bypixel(source, self, glyph, agg)\n                elif not upsample_width and (not upsample_height):\n                    return bypixel(source, self, glyph, agg)\n                else:\n                    glyph = QuadMeshRectilinear(x, y, name)\n                    return bypixel(source, self, glyph, agg)\n            else:\n                glyph = QuadMeshRectilinear(x, y, name)\n                return bypixel(source, self, glyph, agg)\n        elif xarr.ndim == 2:\n            glyph = QuadMeshCurvilinear(x, y, name)\n            return bypixel(source, self, glyph, agg)\n        else:\n            raise ValueError('x- and y-coordinate arrays must have 1 or 2 dimensions.\\n    Received arrays with dimensions: {dims}'.format(dims=list(xarr.dims)))\n\n    def trimesh(self, vertices, simplices, mesh=None, agg=None, interp=True, interpolate=None):\n        \"\"\"Compute a reduction by pixel, mapping data to pixels as a triangle.\n\n        >>> import datashader as ds\n        >>> verts = pd.DataFrame({'x': [0, 5, 10],\n        ...                         'y': [0, 10, 0],\n        ...                         'weight': [1, 5, 3]},\n        ...                        columns=['x', 'y', 'weight'])\n        >>> tris = pd.DataFrame({'v0': [2], 'v1': [0], 'v2': [1]},\n        ...                       columns=['v0', 'v1', 'v2'])\n        >>> cvs = ds.Canvas(x_range=(verts.x.min(), verts.x.max()),\n        ...                 y_range=(verts.y.min(), verts.y.max()))\n        >>> untested = cvs.trimesh(verts, tris)\n\n        Parameters\n        ----------\n        vertices : pandas.DataFrame, dask.DataFrame\n            The input datasource for triangle vertex coordinates. These can be\n            interpreted as the x/y coordinates of the vertices, with optional\n            weights for value interpolation. Columns should be ordered\n            corresponding to 'x', 'y', followed by zero or more (optional)\n            columns containing vertex values. The rows need not be ordered.\n            The column data types must be floating point or integer.\n        simplices : pandas.DataFrame, dask.DataFrame\n            The input datasource for triangle (simplex) definitions. These can\n            be interpreted as rows of ``vertices``, aka positions in the\n            ``vertices`` index. Columns should be ordered corresponding to\n            'vertex0', 'vertex1', and 'vertex2'. Order of the vertices can be\n            clockwise or counter-clockwise; it does not matter as long as the\n            data is consistent for all simplices in the dataframe. The\n            rows need not be ordered.  The data type for the first\n            three columns in the dataframe must be integer.\n        agg : Reduction, optional\n            Reduction to compute. Default is ``mean()``.\n        mesh : pandas.DataFrame, optional\n            An ordered triangle mesh in tabular form, used for optimization\n            purposes. This dataframe is expected to have come from\n            ``datashader.utils.mesh()``. If this argument is not None, the first\n            two arguments are ignored.\n        interpolate : str, optional default=linear\n            Method to use for interpolation between specified values. ``nearest``\n            means to use a single value for the whole triangle, and ``linear``\n            means to do bilinear interpolation of the pixels within each\n            triangle (a weighted average of the vertex values). For\n            backwards compatibility, also accepts ``interp=True`` for ``linear``\n            and ``interp=False`` for ``nearest``.\n        \"\"\"\n        from .glyphs import Triangles\n        from .reductions import mean as mean_rdn\n        from .utils import mesh as create_mesh\n        source = mesh\n        if interpolate is not None:\n            if interpolate == 'linear':\n                interp = True\n            elif interpolate == 'nearest':\n                interp = False\n            else:\n                raise ValueError('Invalid interpolate method: options include {}'.format(['linear', 'nearest']))\n        if source is None:\n            source = create_mesh(vertices, simplices)\n        verts_have_weights = len(vertices.columns) > 2\n        if verts_have_weights:\n            weight_col = vertices.columns[2]\n        else:\n            weight_col = simplices.columns[3]\n        if agg is None:\n            agg = mean_rdn(weight_col)\n        elif agg.column is None:\n            agg.column = weight_col\n        cols = source.columns\n        x, y, weights = (cols[0], cols[1], cols[2:])\n        return bypixel(source, self, Triangles(x, y, weights, weight_type=verts_have_weights, interp=interp), agg)\n\n    def raster(self, source, layer=None, upsample_method='linear', downsample_method=rd.mean(), nan_value=None, agg=None, interpolate=None, chunksize=None, max_mem=None):\n        \"\"\"Sample a raster dataset by canvas size and bounds.\n\n        Handles 2D or 3D xarray DataArrays, assuming that the last two\n        array dimensions are the y- and x-axis that are to be\n        resampled. If a 3D array is supplied a layer may be specified\n        to resample to select the layer along the first dimension to\n        resample.\n\n        Missing values (those having the value indicated by the\n        \"nodata\" attribute of the raster) are replaced with `NaN` if\n        floats, and 0 if int.\n\n        Also supports resampling out-of-core DataArrays backed by dask\n        Arrays. By default it will try to maintain the same chunksize\n        in the output array but a custom chunksize may be provided.\n        If there are memory constraints they may be defined using the\n        max_mem parameter, which determines how large the chunks in\n        memory may be.\n\n        Parameters\n        ----------\n        source : xarray.DataArray or xr.Dataset\n            2D or 3D labelled array (if Dataset, the agg reduction must\n            define the data variable).\n        layer : float\n            For a 3D array, value along the z dimension : optional default=None\n        ds_method : str (optional)\n            Grid cell aggregation method for a possible downsampling.\n        us_method : str (optional)\n            Grid cell interpolation method for a possible upsampling.\n        nan_value : int or float, optional\n            Optional nan_value which will be masked out when applying\n            the resampling.\n        agg : Reduction, optional default=mean()\n            Resampling mode when downsampling raster. The supported\n            options include: first, last, mean, mode, var, std, min,\n            The agg can be specified as either a string name or as a\n            reduction function, but note that the function object will\n            be used only to extract the agg type (mean, max, etc.) and\n            the optional column name; the hardcoded raster code\n            supports only a fixed set of reductions and ignores the\n            actual code of the provided agg.\n        interpolate : str, optional  default=linear\n            Resampling mode when upsampling raster.\n            options include: nearest, linear.\n        chunksize : tuple(int, int) (optional)\n            Size of the output chunks. By default this the chunk size is\n            inherited from the *src* array.\n        max_mem : int (optional)\n            The maximum number of bytes that should be loaded into memory\n            during the regridding operation.\n\n        Returns\n        -------\n        data : xarray.Dataset\n        \"\"\"\n        if agg is None:\n            agg = downsample_method\n        if interpolate is None:\n            interpolate = upsample_method\n        upsample_methods = ['nearest', 'linear']\n        downsample_methods = {'first': 'first', rd.first: 'first', 'last': 'last', rd.last: 'last', 'mode': 'mode', rd.mode: 'mode', 'mean': 'mean', rd.mean: 'mean', 'var': 'var', rd.var: 'var', 'std': 'std', rd.std: 'std', 'min': 'min', rd.min: 'min', 'max': 'max', rd.max: 'max'}\n        if interpolate not in upsample_methods:\n            raise ValueError('Invalid interpolate method: options include {}'.format(upsample_methods))\n        if not isinstance(source, (DataArray, Dataset)):\n            raise ValueError('Expected xarray DataArray or Dataset as the data source, found %s.' % type(source).__name__)\n        column = None\n        if isinstance(agg, rd.Reduction):\n            agg, column = (type(agg), agg.column)\n            if isinstance(source, DataArray) and column is not None and (source.name != column):\n                agg_repr = '%s(%r)' % (agg.__name__, column)\n                raise ValueError('DataArray name %r does not match supplied reduction %s.' % (source.name, agg_repr))\n        if isinstance(source, Dataset):\n            data_vars = list(source.data_vars)\n            if column is None:\n                raise ValueError('When supplying a Dataset the agg reduction must specify the variable to aggregate. Available data_vars include: %r.' % data_vars)\n            elif column not in source.data_vars:\n                raise KeyError('Supplied reduction column %r not found in Dataset, expected one of the following data variables: %r.' % (column, data_vars))\n            source = source[column]\n        if agg not in downsample_methods.keys():\n            raise ValueError('Invalid aggregation method: options include {}'.format(list(downsample_methods.keys())))\n        ds_method = downsample_methods[agg]\n        if source.ndim not in [2, 3]:\n            raise ValueError('Raster aggregation expects a 2D or 3D DataArray, found %s dimensions' % source.ndim)\n        res = calc_res(source)\n        ydim, xdim = source.dims[-2:]\n        xvals, yvals = (source[xdim].values, source[ydim].values)\n        left, bottom, right, top = calc_bbox(xvals, yvals, res)\n        if layer is not None:\n            source = source.sel(**{source.dims[0]: layer})\n        array = orient_array(source, res)\n        if nan_value is not None:\n            mask = array == nan_value\n            array = np.ma.masked_array(array, mask=mask, fill_value=nan_value)\n            fill_value = nan_value\n        elif np.issubdtype(source.dtype, np.integer):\n            fill_value = 0\n        else:\n            fill_value = np.nan\n        if self.x_range is None:\n            self.x_range = (left, right)\n        if self.y_range is None:\n            self.y_range = (bottom, top)\n        xmin = max(self.x_range[0], left)\n        ymin = max(self.y_range[0], bottom)\n        xmax = min(self.x_range[1], right)\n        ymax = min(self.y_range[1], top)\n        width_ratio = min((xmax - xmin) / (self.x_range[1] - self.x_range[0]), 1)\n        height_ratio = min((ymax - ymin) / (self.y_range[1] - self.y_range[0]), 1)\n        if np.isclose(width_ratio, 0) or np.isclose(height_ratio, 0):\n            raise ValueError('Canvas x_range or y_range values do not match closely enough with the data source to be able to accurately rasterize. Please provide ranges that are more accurate.')\n        w = max(int(round(self.plot_width * width_ratio)), 1)\n        h = max(int(round(self.plot_height * height_ratio)), 1)\n        cmin, cmax = get_indices(xmin, xmax, xvals, res[0])\n        rmin, rmax = get_indices(ymin, ymax, yvals, res[1])\n        kwargs = dict(w=w, h=h, ds_method=ds_method, us_method=interpolate, fill_value=fill_value)\n        if array.ndim == 2:\n            source_window = array[rmin:rmax + 1, cmin:cmax + 1]\n            if ds_method in ['var', 'std']:\n                source_window = source_window.astype('f')\n            if da and isinstance(source_window, da.Array):\n                data = resample_2d_distributed(source_window, chunksize=chunksize, max_mem=max_mem, **kwargs)\n            else:\n                data = resample_2d(source_window, **kwargs)\n            layers = 1\n        else:\n            source_window = array[:, rmin:rmax + 1, cmin:cmax + 1]\n            if ds_method in ['var', 'std']:\n                source_window = source_window.astype('f')\n            arrays = []\n            for arr in source_window:\n                if da and isinstance(arr, da.Array):\n                    arr = resample_2d_distributed(arr, chunksize=chunksize, max_mem=max_mem, **kwargs)\n                else:\n                    arr = resample_2d(arr, **kwargs)\n                arrays.append(arr)\n            data = np.dstack(arrays)\n            layers = len(arrays)\n        if w != self.plot_width or h != self.plot_height:\n            num_height = self.plot_height - h\n            num_width = self.plot_width - w\n            lpad = xmin - self.x_range[0]\n            rpad = self.x_range[1] - xmax\n            lpct = lpad / (lpad + rpad) if lpad + rpad > 0 else 0\n            left = max(int(np.ceil(num_width * lpct)), 0)\n            right = max(num_width - left, 0)\n            lshape, rshape = ((self.plot_height, left), (self.plot_height, right))\n            if layers > 1:\n                lshape, rshape = (lshape + (layers,), rshape + (layers,))\n            left_pad = np.full(lshape, fill_value, source_window.dtype)\n            right_pad = np.full(rshape, fill_value, source_window.dtype)\n            tpad = ymin - self.y_range[0]\n            bpad = self.y_range[1] - ymax\n            tpct = tpad / (tpad + bpad) if tpad + bpad > 0 else 0\n            top = max(int(np.ceil(num_height * tpct)), 0)\n            bottom = max(num_height - top, 0)\n            tshape, bshape = ((top, w), (bottom, w))\n            if layers > 1:\n                tshape, bshape = (tshape + (layers,), bshape + (layers,))\n            top_pad = np.full(tshape, fill_value, source_window.dtype)\n            bottom_pad = np.full(bshape, fill_value, source_window.dtype)\n            concat = da.concatenate if da and isinstance(data, da.Array) else np.concatenate\n            arrays = (top_pad, data) if top_pad.shape[0] > 0 else (data,)\n            if bottom_pad.shape[0] > 0:\n                arrays += (bottom_pad,)\n            data = concat(arrays, axis=0) if len(arrays) > 1 else arrays[0]\n            arrays = (left_pad, data) if left_pad.shape[1] > 0 else (data,)\n            if right_pad.shape[1] > 0:\n                arrays += (right_pad,)\n            data = concat(arrays, axis=1) if len(arrays) > 1 else arrays[0]\n        if res[1] > 0:\n            data = data[::-1]\n        if res[0] < 0:\n            data = data[:, ::-1]\n        close_x = np.allclose([left, right], self.x_range) and np.size(xvals) == self.plot_width\n        close_y = np.allclose([bottom, top], self.y_range) and np.size(yvals) == self.plot_height\n        if close_x:\n            xs = xvals\n        else:\n            x_st = self.x_axis.compute_scale_and_translate(self.x_range, self.plot_width)\n            xs = self.x_axis.compute_index(x_st, self.plot_width)\n            if res[0] < 0:\n                xs = xs[::-1]\n        if close_y:\n            ys = yvals\n        else:\n            y_st = self.y_axis.compute_scale_and_translate(self.y_range, self.plot_height)\n            ys = self.y_axis.compute_index(y_st, self.plot_height)\n            if res[1] > 0:\n                ys = ys[::-1]\n        coords = {xdim: xs, ydim: ys}\n        dims = [ydim, xdim]\n        attrs = dict(res=res[0], x_range=self.x_range, y_range=self.y_range)\n        for a in ['_FillValue', 'missing_value', 'fill_value', 'nodata', 'NODATA']:\n            if a in source.attrs:\n                attrs['nodata'] = source.attrs[a]\n                break\n        if 'nodata' not in attrs:\n            try:\n                attrs['nodata'] = source.attrs['nodatavals'][0]\n            except Exception:\n                pass\n        if data.ndim == 3:\n            data = data.transpose([2, 0, 1])\n            layer_dim = source.dims[0]\n            coords[layer_dim] = source.coords[layer_dim]\n            dims = [layer_dim] + dims\n        return DataArray(data, coords=coords, dims=dims, attrs=attrs)\n\n    def validate_ranges(self, x_range, y_range):\n        self.x_axis.validate(x_range)\n        self.y_axis.validate(y_range)\n\n    def validate_size(self, width, height):\n        if width <= 0 or height <= 0:\n            raise ValueError('Invalid size: plot_width and plot_height must be bigger than 0')\n\n    def validate(self):\n        \"\"\"Check that parameter settings are valid for this object\"\"\"\n        self.validate_ranges(self.x_range, self.y_range)\n        self.validate_size(self.plot_width, self.plot_height)\n\n    def _source_from_geopandas(self, source):\n        \"\"\"\n        Check if the specified source is a geopandas or dask-geopandas GeoDataFrame.\n        If so, spatially filter the source and return it.\n        If not, return None.\n        \"\"\"\n        dfs = []\n        with contextlib.suppress(ImportError):\n            import geopandas\n            dfs.append(geopandas.GeoDataFrame)\n        with contextlib.suppress(ImportError):\n            import dask_geopandas\n            if Version(dask_geopandas.__version__) >= Version('0.4.0'):\n                from dask_geopandas.core import GeoDataFrame as gdf1\n                dfs.append(gdf1)\n                with contextlib.suppress(TypeError):\n                    from dask_geopandas.expr import GeoDataFrame as gdf2\n                    dfs.append(gdf2)\n            else:\n                dfs.append(dask_geopandas.GeoDataFrame)\n        if isinstance(source, tuple(dfs)):\n            from shapely import __version__ as shapely_version\n            if Version(shapely_version) < Version('2.0.0'):\n                raise ImportError('Use of GeoPandas in Datashader requires Shapely >= 2.0.0')\n            if isinstance(source, geopandas.GeoDataFrame):\n                x_range = self.x_range if self.x_range is not None else (-np.inf, np.inf)\n                y_range = self.y_range if self.y_range is not None else (-np.inf, np.inf)\n                from shapely import box\n                query = source.sindex.query(box(x_range[0], y_range[0], x_range[1], y_range[1]))\n                source = source.iloc[query]\n            else:\n                x_range = self.x_range if self.x_range is not None else (None, None)\n                y_range = self.y_range if self.y_range is not None else (None, None)\n                source = source.cx[slice(*x_range), slice(*y_range)]\n            return source\n        else:\n            return None\n\ndef bypixel(source, canvas, glyph, agg, *, antialias=False):\n    \"\"\"Compute an aggregate grouped by pixel sized bins.\n\n    Aggregate input data ``source`` into a grid with shape and axis matching\n    ``canvas``, mapping data to bins by ``glyph``, and aggregating by reduction\n    ``agg``.\n\n    Parameters\n    ----------\n    source : pandas.DataFrame, dask.DataFrame\n        Input datasource\n    canvas : Canvas\n    glyph : Glyph\n    agg : Reduction\n    \"\"\"\n    source, dshape = _bypixel_sanitise(source, glyph, agg)\n    schema = dshape.measure\n    glyph.validate(schema)\n    agg.validate(schema)\n    canvas.validate()\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', 'All-NaN (slice|axis) encountered')\n        return bypixel.pipeline(source, schema, canvas, glyph, agg, antialias=antialias)\n\ndef _bypixel_sanitise(source, glyph, agg):\n    if isinstance(source, DataArray) and source.ndim == 1:\n        if not source.name:\n            source.name = 'value'\n        source = source.reset_coords()\n    if isinstance(source, Dataset) and len(source.dims) == 1:\n        columns = list(source.coords.keys()) + list(source.data_vars.keys())\n        cols_to_keep = _cols_to_keep(columns, glyph, agg)\n        source = source.drop_vars([col for col in columns if col not in cols_to_keep])\n        if dd:\n            source = source.to_dask_dataframe()\n        else:\n            source = source.to_dataframe()\n    if isinstance(source, pd.DataFrame) or (cudf and isinstance(source, cudf.DataFrame)):\n        cols_to_keep = _cols_to_keep(source.columns, glyph, agg)\n        if len(cols_to_keep) < len(source.columns):\n            sindex = None\n            from .glyphs.polygon import PolygonGeom\n            if isinstance(glyph, PolygonGeom):\n                sindex = getattr(source[glyph.geometry].array, '_sindex', None)\n            source = source[cols_to_keep]\n            if sindex is not None and getattr(source[glyph.geometry].array, '_sindex', None) is None:\n                source[glyph.geometry].array._sindex = sindex\n        dshape = dshape_from_pandas(source)\n    elif dd and isinstance(source, dd.DataFrame):\n        dshape, source = dshape_from_dask(source)\n    elif isinstance(source, Dataset):\n        dshape = dshape_from_xarray_dataset(source)\n    else:\n        raise ValueError('source must be a pandas or dask DataFrame')\n    return (source, dshape)\n\ndef _cols_to_keep(columns, glyph, agg):\n    \"\"\"\n    Return which columns from the supplied data source are kept as they are\n    needed by the specified agg. Excludes any SpecialColumn.\n    \"\"\"\n    cols_to_keep = dict({col: False for col in columns})\n    for col in glyph.required_columns():\n        cols_to_keep[col] = True\n\n    def recurse(cols_to_keep, agg):\n        if hasattr(agg, 'values'):\n            for subagg in agg.values:\n                recurse(cols_to_keep, subagg)\n        elif hasattr(agg, 'columns'):\n            for column in agg.columns:\n                if column not in (None, rd.SpecialColumn.RowIndex):\n                    cols_to_keep[column] = True\n        elif agg.column not in (None, rd.SpecialColumn.RowIndex):\n            cols_to_keep[agg.column] = True\n    recurse(cols_to_keep, agg)\n    return [col for col, keepit in cols_to_keep.items() if keepit]\n\ndef _broadcast_column_specifications(*args):\n    lengths = {len(a) for a in args if isinstance(a, (list, tuple))}\n    if len(lengths) != 1:\n        return args\n    else:\n        n = lengths.pop()\n        return tuple(((arg,) * n if isinstance(arg, (Number, str)) else arg for arg in args))\nbypixel.pipeline = Dispatcher()",
    "datashader/glyphs/points.py": "from __future__ import annotations\nfrom packaging.version import Version\nimport numpy as np\nfrom toolz import memoize\nfrom datashader.glyphs.glyph import Glyph\nfrom datashader.utils import isreal, ngjit\nfrom numba import cuda\ntry:\n    import cudf\n    from ..transfer_functions._cuda_utils import cuda_args\nexcept Exception:\n    cudf = None\n    cuda_args = None\ntry:\n    from geopandas.array import GeometryDtype as gpd_GeometryDtype\nexcept ImportError:\n    gpd_GeometryDtype = type(None)\ntry:\n    import spatialpandas\nexcept Exception:\n    spatialpandas = None\n\ndef values(s):\n    if isinstance(s, cudf.Series):\n        if Version(cudf.__version__) >= Version('22.02'):\n            return s.to_cupy(na_value=np.nan)\n        else:\n            return s.to_gpu_array(fillna=np.nan)\n    else:\n        return s.values\n\nclass _GeometryLike(Glyph):\n\n    def __init__(self, geometry):\n        self.geometry = geometry\n        self._cached_bounds = None\n\n    @property\n    def ndims(self):\n        return 1\n\n    @property\n    def inputs(self):\n        return (self.geometry,)\n\n    @property\n    def geom_dtypes(self):\n        if spatialpandas:\n            from spatialpandas.geometry import GeometryDtype\n            return (GeometryDtype,)\n        else:\n            return ()\n\n    def validate(self, in_dshape):\n        if not isinstance(in_dshape[str(self.geometry)], self.geom_dtypes):\n            raise ValueError('{col} must be an array with one of the following types: {typs}'.format(col=self.geometry, typs=', '.join((typ.__name__ for typ in self.geom_dtypes))))\n\n    @property\n    def x_label(self):\n        return 'x'\n\n    @property\n    def y_label(self):\n        return 'y'\n\n    def required_columns(self):\n        return [self.geometry]\n\n    def compute_x_bounds(self, df):\n        col = df[self.geometry]\n        if isinstance(col.dtype, gpd_GeometryDtype):\n            if self._cached_bounds is None:\n                self._cached_bounds = col.total_bounds\n            bounds = self._cached_bounds[::2]\n        else:\n            bounds = col.array.total_bounds_x\n        return self.maybe_expand_bounds(bounds)\n\n    def compute_y_bounds(self, df):\n        col = df[self.geometry]\n        if isinstance(col.dtype, gpd_GeometryDtype):\n            if self._cached_bounds is None:\n                self._cached_bounds = col.total_bounds\n            bounds = self._cached_bounds[1::2]\n        else:\n            bounds = col.array.total_bounds_y\n        return self.maybe_expand_bounds(bounds)\n\n    @memoize\n    def compute_bounds_dask(self, ddf):\n        total_bounds = ddf[self.geometry].total_bounds\n        x_extents = (total_bounds[0], total_bounds[2])\n        y_extents = (total_bounds[1], total_bounds[3])\n        return (self.maybe_expand_bounds(x_extents), self.maybe_expand_bounds(y_extents))\n\nclass _PointLike(Glyph):\n    \"\"\"Shared methods between Point and Line\"\"\"\n\n    @property\n    def ndims(self):\n        return 1\n\n    @property\n    def inputs(self):\n        return (self.x, self.y)\n\n    def validate(self, in_dshape):\n        if not isreal(in_dshape.measure[str(self.x)]):\n            raise ValueError('x must be real')\n        elif not isreal(in_dshape.measure[str(self.y)]):\n            raise ValueError('y must be real')\n\n    @property\n    def x_label(self):\n        return self.x\n\n    @property\n    def y_label(self):\n        return self.y\n\n    def required_columns(self):\n        return [self.x, self.y]\n\n    def compute_x_bounds(self, df):\n        bounds = self._compute_bounds(df[self.x])\n        return self.maybe_expand_bounds(bounds)\n\n    def compute_y_bounds(self, df):\n        bounds = self._compute_bounds(df[self.y])\n        return self.maybe_expand_bounds(bounds)\n\n    @memoize\n    def compute_bounds_dask(self, ddf):\n        r = ddf.map_partitions(lambda df: np.array([[np.nanmin(df[self.x].values).item(), np.nanmax(df[self.x].values).item(), np.nanmin(df[self.y].values).item(), np.nanmax(df[self.y].values).item()]])).compute()\n        x_extents = (np.nanmin(r[:, 0]), np.nanmax(r[:, 1]))\n        y_extents = (np.nanmin(r[:, 2]), np.nanmax(r[:, 3]))\n        return (self.maybe_expand_bounds(x_extents), self.maybe_expand_bounds(y_extents))\n\nclass Point(_PointLike):\n    \"\"\"A point, with center at ``x`` and ``y``.\n\n    Points map each record to a single bin.\n    Points falling exactly on the upper bounds are treated as a special case,\n    mapping into the previous bin rather than being cropped off.\n\n    Parameters\n    ----------\n    x, y : str\n        Column names for the x and y coordinates of each point.\n    \"\"\"\n\n    @memoize\n    def _build_extend(self, x_mapper, y_mapper, info, append, _antialias_stage_2, _antialias_stage_2_funcs):\n        x_name = self.x\n        y_name = self.y\n\n        @ngjit\n        @self.expand_aggs_and_cols(append)\n        def _perform_extend_points(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, xxmax, yymax, *aggs_and_cols):\n            x = xs[i]\n            y = ys[i]\n            if xmin <= x <= xmax and ymin <= y <= ymax:\n                xx = int(x_mapper(x) * sx + tx)\n                yy = int(y_mapper(y) * sy + ty)\n                xi, yi = (xxmax - 1 if xx >= xxmax else xx, yymax - 1 if yy >= yymax else yy)\n                append(i, xi, yi, *aggs_and_cols)\n\n        @ngjit\n        @self.expand_aggs_and_cols(append)\n        def extend_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, xxmax, yymax, *aggs_and_cols):\n            for i in range(xs.shape[0]):\n                _perform_extend_points(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, xxmax, yymax, *aggs_and_cols)\n\n        @cuda.jit\n        @self.expand_aggs_and_cols(append)\n        def extend_cuda(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, xxmax, yymax, *aggs_and_cols):\n            i = cuda.grid(1)\n            if i < xs.shape[0]:\n                _perform_extend_points(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, xxmax, yymax, *aggs_and_cols)\n\n        def extend(aggs, df, vt, bounds):\n            yymax, xxmax = aggs[0].shape[:2]\n            aggs_and_cols = aggs + info(df, aggs[0].shape[:2])\n            sx, tx, sy, ty = vt\n            xmin, xmax, ymin, ymax = bounds\n            if cudf and isinstance(df, cudf.DataFrame):\n                xs = values(df[x_name])\n                ys = values(df[y_name])\n                do_extend = extend_cuda[cuda_args(xs.shape[0])]\n            else:\n                xs = df[x_name].values\n                ys = df[y_name].values\n                do_extend = extend_cpu\n            do_extend(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, xxmax, yymax, *aggs_and_cols)\n        return extend\n\nclass MultiPointGeoPandas(_GeometryLike):\n\n    @property\n    def geom_dtypes(self):\n        from geopandas.array import GeometryDtype\n        return (GeometryDtype,)\n\n    @memoize\n    def _build_extend(self, x_mapper, y_mapper, info, append, _antialias_stage_2, _antialias_stage_2_funcs):\n        import shapely\n        geometry_name = self.geometry\n\n        @ngjit\n        @self.expand_aggs_and_cols(append)\n        def _perform_extend_points(i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, values, *aggs_and_cols):\n            x = values[j]\n            y = values[j + 1]\n            if xmin <= x <= xmax and ymin <= y <= ymax:\n                xx = int(x_mapper(x) * sx + tx)\n                yy = int(y_mapper(y) * sy + ty)\n                xi, yi = (xx - 1 if x == xmax else xx, yy - 1 if y == ymax else yy)\n                append(i, xi, yi, *aggs_and_cols)\n\n        def extend(aggs, df, vt, bounds):\n            aggs_and_cols = aggs + info(df, aggs[0].shape[:2])\n            sx, tx, sy, ty = vt\n            xmin, xmax, ymin, ymax = bounds\n            geometry = df[geometry_name].array\n            ragged = shapely.to_ragged_array(geometry)\n            geometry_type = ragged[0]\n            if geometry_type not in (shapely.GeometryType.MULTIPOINT, shapely.GeometryType.POINT):\n                raise ValueError(f'Canvas.points supports GeoPandas geometry types of POINT and MULTIPOINT, not {repr(geometry_type)}')\n            coords = ragged[1].ravel()\n            if geometry_type == shapely.GeometryType.POINT:\n                extend_point_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax, coords, *aggs_and_cols)\n            else:\n                offsets = ragged[2][0]\n                extend_multipoint_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax, coords, offsets, *aggs_and_cols)\n\n        @ngjit\n        @self.expand_aggs_and_cols(append)\n        def extend_multipoint_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax, values, offsets, *aggs_and_cols):\n            for i in range(len(offsets) - 1):\n                start = offsets[i]\n                stop = offsets[i + 1]\n                for j in range(start, stop):\n                    _perform_extend_points(i, 2 * j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, values, *aggs_and_cols)\n\n        @ngjit\n        @self.expand_aggs_and_cols(append)\n        def extend_point_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax, values, *aggs_and_cols):\n            n = len(values) // 2\n            for i in range(n):\n                _perform_extend_points(i, 2 * i, sx, tx, sy, ty, xmin, xmax, ymin, ymax, values, *aggs_and_cols)\n        return extend\n\nclass MultiPointGeometry(_GeometryLike):\n\n    @property\n    def geom_dtypes(self):\n        from spatialpandas.geometry import PointDtype, MultiPointDtype\n        return (PointDtype, MultiPointDtype)\n\n    @memoize\n    def _build_extend(self, x_mapper, y_mapper, info, append, _antialias_stage_2, _antialias_stage_2_funcs):\n        geometry_name = self.geometry\n\n        @ngjit\n        @self.expand_aggs_and_cols(append)\n        def _perform_extend_points(i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, values, *aggs_and_cols):\n            x = values[j]\n            y = values[j + 1]\n            if xmin <= x <= xmax and ymin <= y <= ymax:\n                xx = int(x_mapper(x) * sx + tx)\n                yy = int(y_mapper(y) * sy + ty)\n                xi, yi = (xx - 1 if x == xmax else xx, yy - 1 if y == ymax else yy)\n                append(i, xi, yi, *aggs_and_cols)\n\n        @ngjit\n        @self.expand_aggs_and_cols(append)\n        def extend_point_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax, values, missing, eligible_inds, *aggs_and_cols):\n            for i in eligible_inds:\n                if missing[i] is True:\n                    continue\n                _perform_extend_points(i, 2 * i, sx, tx, sy, ty, xmin, xmax, ymin, ymax, values, *aggs_and_cols)\n\n        @ngjit\n        @self.expand_aggs_and_cols(append)\n        def extend_multipoint_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax, values, missing, offsets, eligible_inds, *aggs_and_cols):\n            for i in eligible_inds:\n                if missing[i] is True:\n                    continue\n                start = offsets[i]\n                stop = offsets[i + 1]\n                for j in range(start, stop, 2):\n                    _perform_extend_points(i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, values, *aggs_and_cols)\n\n        def extend(aggs, df, vt, bounds):\n            from spatialpandas.geometry import PointArray\n            aggs_and_cols = aggs + info(df, aggs[0].shape[:2])\n            sx, tx, sy, ty = vt\n            xmin, xmax, ymin, ymax = bounds\n            geometry = df[geometry_name].array\n            if geometry._sindex is not None:\n                eligible_inds = geometry.sindex.intersects((xmin, ymin, xmax, ymax))\n            else:\n                eligible_inds = np.arange(0, len(geometry), dtype='uint32')\n            missing = geometry.isna()\n            if isinstance(geometry, PointArray):\n                values = geometry.flat_values\n                extend_point_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax, values, missing, eligible_inds, *aggs_and_cols)\n            else:\n                values = geometry.buffer_values\n                offsets = geometry.buffer_offsets[0]\n                extend_multipoint_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax, values, missing, offsets, eligible_inds, *aggs_and_cols)\n        return extend",
    "datashader/pipeline.py": "from __future__ import annotations\nfrom toolz import identity\nfrom . import transfer_functions as tf\nfrom . import reductions\nfrom . import core\n\nclass Pipeline:\n    \"\"\"A datashading pipeline callback.\n\n    Given a declarative specification, creates a callable with the following\n    signature:\n\n    ``callback(x_range, y_range, width, height)``\n\n    where ``x_range`` and ``y_range`` form the bounding box on the viewport,\n    and ``width`` and ``height`` specify the output image dimensions.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame, dask.DataFrame\n    glyph : Glyph\n        The glyph to bin by.\n    agg : Reduction, optional\n        The reduction to compute per-pixel. Default is ``count()``.\n    transform_fn : callable, optional\n        A callable that takes the computed aggregate as an argument, and\n        returns another aggregate. This can be used to do preprocessing before\n        passing to the ``color_fn`` function.\n    color_fn : callable, optional\n        A callable that takes the output of ``tranform_fn``, and returns an\n        ``Image`` object. Default is ``shade``.\n    spread_fn : callable, optional\n        A callable that takes the output of ``color_fn``, and returns another\n        ``Image`` object. Default is ``dynspread``.\n    height_scale: float, optional\n        Factor by which to scale the provided height\n    width_scale: float, optional\n        Factor by which to scale the provided width\n    \"\"\"",
    "datashader/transfer_functions/__init__.py": "from __future__ import annotations\nfrom collections.abc import Iterator\nfrom io import BytesIO\nimport warnings\nimport numpy as np\nimport numba as nb\nimport toolz as tz\nimport xarray as xr\nfrom PIL.Image import fromarray\nfrom datashader.colors import rgb, Sets1to3\nfrom datashader.utils import nansum_missing, ngjit\ntry:\n    import dask.array as da\nexcept ImportError:\n    da = None\ntry:\n    import cupy\nexcept Exception:\n    cupy = None\n__all__ = ['Image', 'stack', 'shade', 'set_background', 'spread', 'dynspread']\n\nclass Image(xr.DataArray):\n    __slots__ = ()\n    __array_priority__ = 70\n    border = 1\n\n    def to_pil(self, origin='lower'):\n        data = self.data\n        if cupy:\n            data = cupy.asnumpy(data)\n        arr = np.flipud(data) if origin == 'lower' else data\n        return fromarray(arr, 'RGBA')\n\n    def to_bytesio(self, format='png', origin='lower'):\n        fp = BytesIO()\n        self.to_pil(origin).save(fp, format)\n        fp.seek(0)\n        return fp\n\n    def _repr_png_(self):\n        \"\"\"Supports rich PNG display in a Jupyter notebook\"\"\"\n        return self.to_pil()._repr_png_()\n\n    def _repr_html_(self):\n        \"\"\"Supports rich HTML display in a Jupyter notebook\"\"\"\n        from io import BytesIO\n        from base64 import b64encode\n        b = BytesIO()\n        self.to_pil().save(b, format='png')\n        h = '<img style=\"margin: auto; border:' + str(self.border) + 'px solid\" ' + \"src='data:image/png;base64,{0}'/>\".format(b64encode(b.getvalue()).decode('utf-8'))\n        return h\n\nclass Images:\n    \"\"\"\n    A list of HTML-representable objects to display in a table.\n    Primarily intended for Image objects, but could be anything\n    that has _repr_html_.\n    \"\"\"\n\n    def __init__(self, *images):\n        \"\"\"Makes an HTML table from a list of HTML-representable arguments.\"\"\"\n        for i in images:\n            assert hasattr(i, '_repr_html_')\n        self.images = images\n        self.num_cols = None\n\n    def cols(self, n):\n        \"\"\"\n        Set the number of columns to use in the HTML table.\n        Returns self for convenience.\n        \"\"\"\n        self.num_cols = n\n        return self\n\n    def _repr_html_(self):\n        \"\"\"Supports rich display in a Jupyter notebook, using an HTML table\"\"\"\n        htmls = []\n        col = 0\n        tr = '<tr style=\"background-color:white\">'\n        for i in self.images:\n            label = i.name if hasattr(i, 'name') and i.name is not None else ''\n            htmls.append('<td style=\"text-align: center\"><b>' + label + '</b><br><br>{0}</td>'.format(i._repr_html_()))\n            col += 1\n            if self.num_cols is not None and col >= self.num_cols:\n                col = 0\n                htmls.append('</tr>' + tr)\n        return '<table style=\"width:100%; text-align: center\"><tbody>' + tr + ''.join(htmls) + '</tr></tbody></table>'\n\ndef stack(*imgs, **kwargs):\n    \"\"\"Combine images together, overlaying later images onto earlier ones.\n\n    Parameters\n    ----------\n    imgs : iterable of Image\n        The images to combine.\n    how : str, optional\n        The compositing operator to combine pixels. Default is `'over'`.\n    \"\"\"\n    from datashader.composite import composite_op_lookup\n    if not imgs:\n        raise ValueError('No images passed in')\n    shapes = []\n    for i in imgs:\n        if not isinstance(i, Image):\n            raise TypeError('Expected `Image`, got: `{0}`'.format(type(i)))\n        elif not shapes:\n            shapes.append(i.shape)\n        elif shapes and i.shape not in shapes:\n            raise ValueError('The stacked images must have the same shape.')\n    name = kwargs.get('name', None)\n    op = composite_op_lookup[kwargs.get('how', 'over')]\n    if len(imgs) == 1:\n        return imgs[0]\n    imgs = xr.align(*imgs, copy=False, join='outer')\n    with np.errstate(divide='ignore', invalid='ignore'):\n        out = tz.reduce(tz.flip(op), [i.data for i in imgs])\n    return Image(out, coords=imgs[0].coords, dims=imgs[0].dims, name=name)\n\ndef eq_hist(data, mask=None, nbins=256 * 256):\n    \"\"\"Compute the numpy array after histogram equalization.\n\n    For use in `shade`.\n\n    Parameters\n    ----------\n    data : ndarray\n    mask : ndarray, optional\n       Boolean array of missing points. Where True, the output will be `NaN`.\n    nbins : int, optional\n        Maximum number of bins to use. If data is of type boolean or integer\n        this will determine when to switch from exact unique value counts to\n        a binned histogram.\n\n    Returns\n    -------\n    ndarray or tuple(ndarray, int)\n        Returns the array when mask isn't set, otherwise returns the\n        array and the computed number of discrete levels.\n\n    Notes\n    -----\n    This function is adapted from the implementation in scikit-image [1]_.\n\n    References\n    ----------\n    .. [1] http://scikit-image.org/docs/stable/api/skimage.exposure.html#equalize-hist\n    \"\"\"\n    if cupy and isinstance(data, cupy.ndarray):\n        from ._cuda_utils import interp\n        array_module = cupy\n    elif not isinstance(data, np.ndarray):\n        raise TypeError('data must be an ndarray')\n    else:\n        interp = np.interp\n        array_module = np\n    if mask is not None and array_module.all(mask):\n        return (array_module.full_like(data, np.nan), 0)\n    data2 = data if mask is None else data[~mask]\n    if data2.dtype == bool or (array_module.issubdtype(data2.dtype, array_module.integer) and array_module.ptp(data2) < nbins):\n        values, counts = array_module.unique(data2, return_counts=True)\n        vmin, vmax = (values[0].item(), values[-1].item())\n        interval = vmax - vmin\n        bin_centers = array_module.arange(vmin, vmax + 1)\n        hist = array_module.zeros(interval + 1, dtype='uint64')\n        hist[values - vmin] = counts\n        discrete_levels = len(values)\n    else:\n        hist, bin_edges = array_module.histogram(data2, bins=nbins)\n        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n        keep_mask = hist > 0\n        discrete_levels = array_module.count_nonzero(keep_mask)\n        if discrete_levels != len(hist):\n            hist = hist[keep_mask]\n            bin_centers = bin_centers[keep_mask]\n    cdf = hist.cumsum()\n    cdf = cdf / float(cdf[-1])\n    out = interp(data, bin_centers, cdf).reshape(data.shape)\n    return (out if mask is None else array_module.where(mask, array_module.nan, out), discrete_levels)\n_interpolate_lookup = {'log': lambda d, m: np.log1p(np.where(m, np.nan, d)), 'cbrt': lambda d, m: np.where(m, np.nan, d) ** (1 / 3.0), 'linear': lambda d, m: np.where(m, np.nan, d), 'eq_hist': eq_hist}\n\ndef _normalize_interpolate_how(how):\n    if callable(how):\n        return how\n    elif how in _interpolate_lookup:\n        return _interpolate_lookup[how]\n    raise ValueError('Unknown interpolation method: {0}'.format(how))\n\ndef _rescale_discrete_levels(discrete_levels, span):\n    if discrete_levels is None:\n        raise ValueError('interpolator did not return a valid discrete_levels')\n    m = -0.5 / 98.0\n    c = 1.5 - 2 * m\n    multiple = m * discrete_levels + c\n    if multiple > 1:\n        lower_span = max(span[1] - multiple * (span[1] - span[0]), 0)\n        span = (lower_span, 1)\n    return span\n\ndef _interpolate(agg, cmap, how, alpha, span, min_alpha, name, rescale_discrete_levels):\n    if cupy and isinstance(agg.data, cupy.ndarray):\n        from ._cuda_utils import masked_clip_2d, interp\n    else:\n        from ._cpu_utils import masked_clip_2d\n        interp = np.interp\n    if agg.ndim != 2:\n        raise ValueError('agg must be 2D')\n    interpolater = _normalize_interpolate_how(how)\n    data = agg.data\n    if da and isinstance(data, da.Array):\n        data = data.compute()\n    else:\n        data = data.copy()\n    if np.issubdtype(data.dtype, np.bool_):\n        mask = ~data\n        data = data.astype(np.int8)\n    elif data.dtype.kind == 'u':\n        mask = data == 0\n    else:\n        mask = np.isnan(data)\n    if mask.all():\n        return Image(np.zeros(shape=agg.data.shape, dtype=np.uint32), coords=agg.coords, dims=agg.dims, attrs=agg.attrs, name=name)\n    if span is None:\n        offset = np.nanmin(data[~mask])\n    else:\n        offset = np.array(span, dtype=data.dtype)[0]\n        masked_clip_2d(data, mask, *span)\n    data -= offset\n    with np.errstate(invalid='ignore', divide='ignore'):\n        data = interpolater(data, mask)\n        discrete_levels = None\n        if isinstance(data, (list, tuple)):\n            data, discrete_levels = data\n        if span is None:\n            masked_data = np.where(~mask, data, np.nan)\n            span = (np.nanmin(masked_data), np.nanmax(masked_data))\n            if rescale_discrete_levels and discrete_levels is not None:\n                span = _rescale_discrete_levels(discrete_levels, span)\n        else:\n            if how == 'eq_hist':\n                raise ValueError('span is not (yet) valid to use with eq_hist')\n            span = interpolater([0, span[1] - span[0]], 0)\n    if isinstance(cmap, Iterator):\n        cmap = list(cmap)\n    if isinstance(cmap, tuple) and isinstance(cmap[0], str):\n        cmap = list(cmap)\n    if isinstance(cmap, list):\n        rspan, gspan, bspan = np.array(list(zip(*map(rgb, cmap))))\n        span = np.linspace(span[0], span[1], len(cmap))\n        r = np.nan_to_num(interp(data, span, rspan, left=255), copy=False).astype(np.uint8)\n        g = np.nan_to_num(interp(data, span, gspan, left=255), copy=False).astype(np.uint8)\n        b = np.nan_to_num(interp(data, span, bspan, left=255), copy=False).astype(np.uint8)\n        a = np.where(np.isnan(data), 0, alpha).astype(np.uint8)\n        rgba = np.dstack([r, g, b, a])\n    elif isinstance(cmap, str) or isinstance(cmap, tuple):\n        color = rgb(cmap)\n        aspan = np.arange(min_alpha, alpha + 1)\n        span = np.linspace(span[0], span[1], len(aspan))\n        r = np.full(data.shape, color[0], dtype=np.uint8)\n        g = np.full(data.shape, color[1], dtype=np.uint8)\n        b = np.full(data.shape, color[2], dtype=np.uint8)\n        a = np.nan_to_num(interp(data, span, aspan, left=0, right=255), copy=False).astype(np.uint8)\n        rgba = np.dstack([r, g, b, a])\n    elif callable(cmap):\n        scaled_data = (data - span[0]) / (span[1] - span[0])\n        if cupy and isinstance(scaled_data, cupy.ndarray):\n            scaled_data = cupy.asnumpy(scaled_data)\n        rgba = cmap(scaled_data, bytes=True)\n        rgba[:, :, 3] = np.where(np.isnan(scaled_data), 0, alpha).astype(np.uint8)\n    else:\n        raise TypeError(\"Expected `cmap` of `matplotlib.colors.Colormap`, `list`, `str`, or `tuple`; got: '{0}'\".format(type(cmap)))\n    img = rgba.view(np.uint32).reshape(data.shape)\n    if cupy and isinstance(img, cupy.ndarray):\n        img = cupy.asnumpy(img)\n    return Image(img, coords=agg.coords, dims=agg.dims, name=name)\n\ndef _colorize(agg, color_key, how, alpha, span, min_alpha, name, color_baseline, rescale_discrete_levels):\n    if cupy and isinstance(agg.data, cupy.ndarray):\n        array = cupy.array\n    else:\n        array = np.array\n    if not agg.ndim == 3:\n        raise ValueError('agg must be 3D')\n    cats = agg.indexes[agg.dims[-1]]\n    if not len(cats):\n        return Image(np.zeros(agg.shape[0:2], dtype=np.uint32), dims=agg.dims[:-1], coords=dict([(agg.dims[1], agg.coords[agg.dims[1]]), (agg.dims[0], agg.coords[agg.dims[0]])]), name=name)\n    if color_key is None:\n        raise ValueError('Color key must be provided, with at least as many ' + 'colors as there are categorical fields')\n    if not isinstance(color_key, dict):\n        color_key = dict(zip(cats, color_key))\n    if len(color_key) < len(cats):\n        raise ValueError(f'Insufficient colors provided ({len(color_key)}) for the categorical fields available ({len(cats)})')\n    colors = [rgb(color_key[c]) for c in cats]\n    rs, gs, bs = map(array, zip(*colors))\n    agg_t = agg.transpose(*(agg.dims[-1],) + agg.dims[:2])\n    data = agg_t.data.transpose([1, 2, 0])\n    if da and isinstance(data, da.Array):\n        data = data.compute()\n    color_data = data.copy()\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', 'All-NaN slice encountered')\n        baseline = np.nanmin(color_data) if color_baseline is None else color_baseline\n    with np.errstate(invalid='ignore'):\n        if baseline > 0:\n            color_data -= baseline\n        elif baseline < 0:\n            color_data += -baseline\n        if color_data.dtype.kind != 'u' and color_baseline is not None:\n            color_data[color_data < 0] = 0\n    color_total = nansum_missing(color_data, axis=2)\n    color_data[np.isnan(data)] = 0\n    with np.errstate(divide='ignore', invalid='ignore'):\n        r = (color_data.dot(rs) / color_total).astype(np.uint8)\n        g = (color_data.dot(gs) / color_total).astype(np.uint8)\n        b = (color_data.dot(bs) / color_total).astype(np.uint8)\n    color_mask = ~np.isnan(data)\n    cmask_sum = np.sum(color_mask, axis=2)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        r2 = (color_mask.dot(rs) / cmask_sum).astype(np.uint8)\n        g2 = (color_mask.dot(gs) / cmask_sum).astype(np.uint8)\n        b2 = (color_mask.dot(bs) / cmask_sum).astype(np.uint8)\n    missing_colors = np.sum(color_data, axis=2) == 0\n    r = np.where(missing_colors, r2, r)\n    g = np.where(missing_colors, g2, g)\n    b = np.where(missing_colors, b2, b)\n    total = nansum_missing(data, axis=2)\n    mask = np.isnan(total)\n    a = _interpolate_alpha(data, total, mask, how, alpha, span, min_alpha, rescale_discrete_levels)\n    values = np.dstack([r, g, b, a]).view(np.uint32).reshape(a.shape)\n    if cupy and isinstance(values, cupy.ndarray):\n        values = cupy.asnumpy(values)\n    return Image(values, dims=agg.dims[:-1], coords=dict([(agg.dims[1], agg.coords[agg.dims[1]]), (agg.dims[0], agg.coords[agg.dims[0]])]), name=name)\n\ndef _interpolate_alpha(data, total, mask, how, alpha, span, min_alpha, rescale_discrete_levels):\n    if cupy and isinstance(data, cupy.ndarray):\n        from ._cuda_utils import interp, masked_clip_2d\n        array_module = cupy\n    else:\n        from ._cpu_utils import masked_clip_2d\n        interp = np.interp\n        array_module = np\n    if span is None:\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', 'All-NaN slice encountered')\n            offset = np.nanmin(total)\n        if total.dtype.kind == 'u' and offset == 0:\n            mask = mask | (total == 0)\n            if not np.all(mask):\n                offset = total[total > 0].min()\n            total = np.where(~mask, total, np.nan)\n        a_scaled = _normalize_interpolate_how(how)(total - offset, mask)\n        discrete_levels = None\n        if isinstance(a_scaled, (list, tuple)):\n            a_scaled, discrete_levels = a_scaled\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', 'All-NaN (slice|axis) encountered')\n            norm_span = [np.nanmin(a_scaled).item(), np.nanmax(a_scaled).item()]\n        if rescale_discrete_levels and discrete_levels is not None:\n            norm_span = _rescale_discrete_levels(discrete_levels, norm_span)\n    else:\n        if how == 'eq_hist':\n            raise ValueError('span is not (yet) valid to use with eq_hist')\n        offset = np.array(span, dtype=data.dtype)[0]\n        if total.dtype.kind == 'u' and np.nanmin(total) == 0:\n            mask = mask | (total <= 0)\n            total = np.where(~mask, total, np.nan)\n        masked_clip_2d(total, mask, *span)\n        a_scaled = _normalize_interpolate_how(how)(total - offset, mask)\n        if isinstance(a_scaled, (list, tuple)):\n            a_scaled = a_scaled[0]\n        norm_span = _normalize_interpolate_how(how)([0, span[1] - span[0]], 0)\n        if isinstance(norm_span, (list, tuple)):\n            norm_span = norm_span[0]\n    norm_span = array_module.hstack(norm_span)\n    a_float = interp(a_scaled, norm_span, array_module.array([min_alpha, alpha]), left=0, right=255)\n    a = np.nan_to_num(a_float, copy=False).astype(np.uint8)\n    return a\n\ndef _apply_discrete_colorkey(agg, color_key, alpha, name, color_baseline):\n    if cupy and isinstance(agg.data, cupy.ndarray):\n        module = cupy\n        array = cupy.array\n    else:\n        module = np\n        array = np.array\n    if not agg.ndim == 2:\n        raise ValueError('agg must be 2D')\n    if color_key is None or not isinstance(color_key, dict):\n        raise ValueError('Color key must be provided as a dictionary')\n    agg_data = agg.data\n    if da and isinstance(agg_data, da.Array):\n        agg_data = agg_data.compute()\n    cats = color_key.keys()\n    colors = [rgb(color_key[c]) for c in cats]\n    rs, gs, bs = map(array, zip(*colors))\n    data = module.empty_like(agg_data) * module.nan\n    r = module.zeros_like(data, dtype=module.uint8)\n    g = module.zeros_like(data, dtype=module.uint8)\n    b = module.zeros_like(data, dtype=module.uint8)\n    r2 = module.zeros_like(data, dtype=module.uint8)\n    g2 = module.zeros_like(data, dtype=module.uint8)\n    b2 = module.zeros_like(data, dtype=module.uint8)\n    for i, c in enumerate(cats):\n        value_mask = agg_data == c\n        data[value_mask] = 1\n        r2[value_mask] = rs[i]\n        g2[value_mask] = gs[i]\n        b2[value_mask] = bs[i]\n    color_data = data.copy()\n    baseline = module.nanmin(color_data) if color_baseline is None else color_baseline\n    with np.errstate(invalid='ignore'):\n        if baseline > 0:\n            color_data -= baseline\n        elif baseline < 0:\n            color_data += -baseline\n        if color_data.dtype.kind != 'u' and color_baseline is not None:\n            color_data[color_data < 0] = 0\n    color_data[module.isnan(data)] = 0\n    if not color_data.any():\n        r[:] = r2\n        g[:] = g2\n        b[:] = b2\n    missing_colors = color_data == 0\n    r = module.where(missing_colors, r2, r)\n    g = module.where(missing_colors, g2, g)\n    b = module.where(missing_colors, b2, b)\n    a = np.where(np.isnan(data), 0, alpha).astype(np.uint8)\n    values = module.dstack([r, g, b, a]).view(module.uint32).reshape(a.shape)\n    if cupy and isinstance(agg.data, cupy.ndarray):\n        values = cupy.asnumpy(values)\n    return Image(values, dims=agg.dims, coords=agg.coords, name=name)\n\ndef set_background(img, color=None, name=None):\n    \"\"\"Return a new image, with the background set to `color`.\n\n    Parameters\n    ----------\n    img : Image\n    color : color name or tuple, optional\n        The background color. Can be specified either by name, hexcode, or as a\n        tuple of ``(red, green, blue)`` values.\n    \"\"\"\n    from datashader.composite import over\n    if not isinstance(img, Image):\n        raise TypeError('Expected `Image`, got: `{0}`'.format(type(img)))\n    name = img.name if name is None else name\n    if color is None:\n        return img\n    background = np.uint8(rgb(color) + (255,)).view('uint32')[0]\n    data = over(img.data, background)\n    return Image(data, coords=img.coords, dims=img.dims, name=name)\n\ndef spread(img, px=1, shape='circle', how=None, mask=None, name=None):\n    \"\"\"Spread pixels in an image.\n\n    Spreading expands each pixel a certain number of pixels on all sides\n    according to a given shape, merging pixels using a specified compositing\n    operator. This can be useful to make sparse plots more visible.\n\n    Parameters\n    ----------\n    img : Image or other DataArray\n    px : int, optional\n        Number of pixels to spread on all sides\n    shape : str, optional\n        The shape to spread by. Options are 'circle' [default] or 'square'.\n    how : str, optional\n        The name of the compositing operator to use when combining\n        pixels. Default of None uses 'over' operator for Image objects\n        and 'add' operator otherwise.\n    mask : ndarray, shape (M, M), optional\n        The mask to spread over. If provided, this mask is used instead of\n        generating one based on `px` and `shape`. Must be a square array\n        with odd dimensions. Pixels are spread from the center of the mask to\n        locations where the mask is True.\n    name : string name, optional\n        Optional string name to give to the Image object to return,\n        to label results for display.\n    \"\"\"\n    if not isinstance(img, xr.DataArray):\n        raise TypeError('Expected `xr.DataArray`, got: `{0}`'.format(type(img)))\n    is_image = isinstance(img, Image)\n    name = img.name if name is None else name\n    if mask is None:\n        if not isinstance(px, int) or px < 0:\n            raise ValueError('``px`` must be an integer >= 0')\n        if px == 0:\n            return img\n        mask = _mask_lookup[shape](px)\n    elif not (isinstance(mask, np.ndarray) and mask.ndim == 2 and (mask.shape[0] == mask.shape[1]) and (mask.shape[0] % 2 == 1)):\n        raise ValueError('mask must be a square 2 dimensional ndarray with odd dimensions.')\n        mask = mask if mask.dtype == 'bool' else mask.astype('bool')\n    if how is None:\n        how = 'over' if is_image else 'add'\n    w = mask.shape[0]\n    extra = w // 2\n    M, N = img.shape[:2]\n    padded_shape = (M + 2 * extra, N + 2 * extra)\n    float_type = img.dtype in [np.float32, np.float64]\n    fill_value = np.nan if float_type else 0\n    if cupy and isinstance(img.data, cupy.ndarray):\n        img.data = cupy.asnumpy(img.data)\n    if is_image:\n        kernel = _build_spread_kernel(how, is_image)\n    elif float_type:\n        kernel = _build_float_kernel(how, w)\n    else:\n        kernel = _build_int_kernel(how, w, img.dtype == np.uint32)\n\n    def apply_kernel(layer):\n        buf = np.full(padded_shape, fill_value, dtype=layer.dtype)\n        kernel(layer.data, mask, buf)\n        return buf[extra:-extra, extra:-extra].copy()\n    if len(img.shape) == 2:\n        out = apply_kernel(img)\n    else:\n        out = np.dstack([apply_kernel(img[:, :, category]) for category in range(img.shape[2])])\n    return img.__class__(out, dims=img.dims, coords=img.coords, name=name)\n\n@tz.memoize\ndef _build_int_kernel(how, mask_size, ignore_zeros):\n    \"\"\"Build a spreading kernel for a given composite operator\"\"\"\n    from datashader.composite import composite_op_lookup, validate_operator\n    validate_operator(how, is_image=False)\n    op = composite_op_lookup[how + '_arr']\n\n    @ngjit\n    def stencilled(arr, mask, out):\n        M, N = arr.shape\n        for y in range(M):\n            for x in range(N):\n                el = arr[y, x]\n                for i in range(mask_size):\n                    for j in range(mask_size):\n                        if mask[i, j]:\n                            if ignore_zeros and el == 0:\n                                result = out[i + y, j + x]\n                            elif ignore_zeros and out[i + y, j + x] == 0:\n                                result = el\n                            else:\n                                result = op(el, out[i + y, j + x])\n                            out[i + y, j + x] = result\n    return stencilled\n\n@tz.memoize\ndef _build_float_kernel(how, mask_size):\n    \"\"\"Build a spreading kernel for a given composite operator\"\"\"\n    from datashader.composite import composite_op_lookup, validate_operator\n    validate_operator(how, is_image=False)\n    op = composite_op_lookup[how + '_arr']\n\n    @ngjit\n    def stencilled(arr, mask, out):\n        M, N = arr.shape\n        for y in range(M):\n            for x in range(N):\n                el = arr[y, x]\n                for i in range(mask_size):\n                    for j in range(mask_size):\n                        if mask[i, j]:\n                            if np.isnan(el):\n                                result = out[i + y, j + x]\n                            elif np.isnan(out[i + y, j + x]):\n                                result = el\n                            else:\n                                result = op(el, out[i + y, j + x])\n                            out[i + y, j + x] = result\n    return stencilled\n\n@tz.memoize\ndef _build_spread_kernel(how, is_image):\n    \"\"\"Build a spreading kernel for a given composite operator\"\"\"\n    from datashader.composite import composite_op_lookup, validate_operator\n    validate_operator(how, is_image=True)\n    op = composite_op_lookup[how + ('' if is_image else '_arr')]\n\n    @ngjit\n    def kernel(arr, mask, out):\n        M, N = arr.shape\n        w = mask.shape[0]\n        for y in range(M):\n            for x in range(N):\n                el = arr[y, x]\n                process_image = is_image and int(el) >> 24 & 255\n                process_array = not is_image and (not np.isnan(el))\n                if process_image or process_array:\n                    for i in range(w):\n                        for j in range(w):\n                            if mask[i, j]:\n                                if el == 0:\n                                    result = out[i + y, j + x]\n                                if out[i + y, j + x] == 0:\n                                    result = el\n                                else:\n                                    result = op(el, out[i + y, j + x])\n                                out[i + y, j + x] = result\n    return kernel\n\ndef _square_mask(px):\n    \"\"\"Produce a square mask with sides of length ``2 * px + 1``\"\"\"\n    px = int(px)\n    w = 2 * px + 1\n    return np.ones((w, w), dtype='bool')\n\ndef _circle_mask(r):\n    \"\"\"Produce a circular mask with a diameter of ``2 * r + 1``\"\"\"\n    x = np.arange(-r, r + 1, dtype='i4')\n    return np.where(np.sqrt(x ** 2 + x[:, None] ** 2) <= r + 0.5, True, False)\n_mask_lookup = {'square': _square_mask, 'circle': _circle_mask}\n\n@nb.jit(nopython=True, nogil=True, cache=True)\ndef _array_density(arr, float_type, px=1):\n    \"\"\"Compute a density heuristic of an array.\n\n    The density is a number in [0, 1], and indicates the normalized mean number\n    of non-empty pixels that have neighbors in the given px radius.\n    \"\"\"\n    M, N = arr.shape\n    cnt = has_neighbors = 0\n    for y in range(0, M):\n        for x in range(0, N):\n            el = arr[y, x]\n            if float_type and (not np.isnan(el)) or (not float_type and el != 0):\n                cnt += 1\n                neighbors = 0\n                for i in range(max(0, y - px), min(y + px + 1, M)):\n                    for j in range(max(0, x - px), min(x + px + 1, N)):\n                        if float_type and (not np.isnan(arr[i, j])) or (not float_type and arr[i, j] != 0):\n                            neighbors += 1\n                if neighbors > 1:\n                    has_neighbors += 1\n    return has_neighbors / cnt if cnt else np.inf\n\n@nb.jit(nopython=True, nogil=True, cache=True)\ndef _rgb_density(arr, px=1):\n    \"\"\"Compute a density heuristic of an image.\n\n    The density is a number in [0, 1], and indicates the normalized mean number\n    of non-empty pixels that have neighbors in the given px radius.\n    \"\"\"\n    M, N = arr.shape\n    cnt = has_neighbors = 0\n    for y in range(0, M):\n        for x in range(0, N):\n            if arr[y, x] >> 24 & 255:\n                cnt += 1\n                neighbors = 0\n                for i in range(max(0, y - px), min(y + px + 1, M)):\n                    for j in range(max(0, x - px), min(x + px + 1, N)):\n                        if arr[i, j] >> 24 & 255:\n                            neighbors += 1\n                if neighbors > 1:\n                    has_neighbors += 1\n    return has_neighbors / cnt if cnt else np.inf",
    "datashader/reductions.py": "from __future__ import annotations\nimport copy\nfrom enum import Enum\nfrom packaging.version import Version\nimport numpy as np\nfrom datashader.datashape import dshape, isnumeric, Record, Option\nfrom datashader.datashape import coretypes as ct\nfrom toolz import concat, unique\nimport xarray as xr\nfrom datashader.antialias import AntialiasCombination, AntialiasStage2\nfrom datashader.utils import isminus1, isnull\nfrom numba import cuda as nb_cuda\ntry:\n    from datashader.transfer_functions._cuda_utils import cuda_atomic_nanmin, cuda_atomic_nanmax, cuda_args, cuda_row_min_in_place, cuda_nanmax_n_in_place_4d, cuda_nanmax_n_in_place_3d, cuda_nanmin_n_in_place_4d, cuda_nanmin_n_in_place_3d, cuda_row_max_n_in_place_4d, cuda_row_max_n_in_place_3d, cuda_row_min_n_in_place_4d, cuda_row_min_n_in_place_3d, cuda_shift_and_insert\nexcept ImportError:\n    cuda_atomic_nanmin, cuda_atomic_nanmax, cuda_args, cuda_row_min_in_place, cuda_nanmax_n_in_place_4d, cuda_nanmax_n_in_place_3d, cuda_nanmin_n_in_place_4d, cuda_nanmin_n_in_place_3d, cuda_row_max_n_in_place_4d, cuda_row_max_n_in_place_3d, cuda_row_min_n_in_place_4d, cuda_row_min_n_in_place_3d, cuda_shift_and_insert = (None, None, None, None, None, None, None, None, None, None, None, None, None)\ntry:\n    import cudf\n    import cupy as cp\nexcept Exception:\n    cudf = cp = None\nfrom .utils import Expr, ngjit, nansum_missing, nanmax_in_place, nansum_in_place, row_min_in_place, nanmax_n_in_place_4d, nanmax_n_in_place_3d, nanmin_n_in_place_4d, nanmin_n_in_place_3d, row_max_n_in_place_4d, row_max_n_in_place_3d, row_min_n_in_place_4d, row_min_n_in_place_3d, shift_and_insert\n\nclass SpecialColumn(Enum):\n    \"\"\"\n    Internally datashader identifies the columns required by the user's\n    Reductions and extracts them from the supplied source (e.g. DataFrame) to\n    pass through the dynamically-generated append function in compiler.py and\n    end up as arguments to the Reduction._append* functions. Each column is\n    a string name or a SpecialColumn. A column of None is used in Reduction\n    classes to denote that no column is required.\n    \"\"\"\n    RowIndex = 1\n\nclass UsesCudaMutex(Enum):\n    \"\"\"\n    Enum that encapsulates the need for a Reduction to use a CUDA mutex to\n    operate correctly on a GPU. Possible values:\n\n    No: the Reduction append_cuda function is atomic and no mutex is required.\n    Local: Reduction append_cuda needs wrapping in a mutex.\n    Global: the overall compiled append function needs wrapping in a mutex.\n    \"\"\"\n    No = 0\n    Local = 1\n    Global = 2\n\nclass Preprocess(Expr):\n    \"\"\"Base clase for preprocessing steps.\"\"\"\n\n    def __init__(self, column: str | SpecialColumn | None):\n        self.column = column\n\n    @property\n    def inputs(self):\n        return (self.column,)\n\n    @property\n    def nan_check_column(self):\n        return None\n\nclass extract(Preprocess):\n    \"\"\"Extract a column from a dataframe as a numpy array of values.\"\"\"\n\n    def apply(self, df, cuda):\n        if self.column is SpecialColumn.RowIndex:\n            attr_name = '_datashader_row_offset'\n            if isinstance(df, xr.Dataset):\n                row_offset = df.attrs[attr_name]\n                row_length = df.attrs['_datashader_row_length']\n            else:\n                attrs = getattr(df, 'attrs', None)\n                row_offset = getattr(attrs or df, attr_name, 0)\n                row_length = len(df)\n        if cudf and isinstance(df, cudf.DataFrame):\n            if self.column is SpecialColumn.RowIndex:\n                return cp.arange(row_offset, row_offset + row_length, dtype=np.int64)\n            if df[self.column].dtype.kind == 'f':\n                nullval = np.nan\n            else:\n                nullval = 0\n            if Version(cudf.__version__) >= Version('22.02'):\n                return df[self.column].to_cupy(na_value=nullval)\n            return cp.array(df[self.column].to_gpu_array(fillna=nullval))\n        elif self.column is SpecialColumn.RowIndex:\n            if cuda:\n                return cp.arange(row_offset, row_offset + row_length, dtype=np.int64)\n            else:\n                return np.arange(row_offset, row_offset + row_length, dtype=np.int64)\n        elif isinstance(df, xr.Dataset):\n            if cuda and (not isinstance(df[self.column].data, cp.ndarray)):\n                return cp.asarray(df[self.column])\n            else:\n                return df[self.column].data\n        else:\n            return df[self.column].values\n\nclass CategoryPreprocess(Preprocess):\n    \"\"\"Base class for categorizing preprocessors.\"\"\"\n\n    @property\n    def cat_column(self):\n        \"\"\"Returns name of categorized column\"\"\"\n        return self.column\n\n    def categories(self, input_dshape):\n        \"\"\"Returns list of categories corresponding to input shape\"\"\"\n        raise NotImplementedError('categories not implemented')\n\n    def validate(self, in_dshape):\n        \"\"\"Validates input shape\"\"\"\n        raise NotImplementedError('validate not implemented')\n\n    def apply(self, df, cuda):\n        \"\"\"Applies preprocessor to DataFrame and returns array\"\"\"\n        raise NotImplementedError('apply not implemented')\n\nclass category_codes(CategoryPreprocess):\n    \"\"\"\n    Extract just the category codes from a categorical column.\n\n    To create a new type of categorizer, derive a subclass from this\n    class or one of its subclasses, implementing ``__init__``,\n    ``_hashable_inputs``, ``categories``, ``validate``, and ``apply``.\n\n    See the implementation of ``category_modulo`` in ``reductions.py``\n    for an example.\n    \"\"\"\n\n    def categories(self, input_dshape):\n        return input_dshape.measure[self.column].categories\n\n    def validate(self, in_dshape):\n        if self.column not in in_dshape.dict:\n            raise ValueError('specified column not found')\n        if not isinstance(in_dshape.measure[self.column], ct.Categorical):\n            raise ValueError('input must be categorical')\n\n    def apply(self, df, cuda):\n        if cudf and isinstance(df, cudf.DataFrame):\n            if Version(cudf.__version__) >= Version('22.02'):\n                return df[self.column].cat.codes.to_cupy()\n            return df[self.column].cat.codes.to_gpu_array()\n        else:\n            return df[self.column].cat.codes.values\n\nclass category_modulo(category_codes):\n    \"\"\"\n    A variation on category_codes that assigns categories using an integer column, modulo a base.\n    Category is computed as (column_value - offset)%modulo.\n    \"\"\"\n    IntegerTypes = {ct.bool_, ct.uint8, ct.uint16, ct.uint32, ct.uint64, ct.int8, ct.int16, ct.int32, ct.int64}\n\n    def __init__(self, column, modulo, offset=0):\n        super().__init__(column)\n        self.offset = offset\n        self.modulo = modulo\n\n    def _hashable_inputs(self):\n        return super()._hashable_inputs() + (self.offset, self.modulo)\n\n    def categories(self, in_dshape):\n        return list(range(self.modulo))\n\n    def validate(self, in_dshape):\n        if self.column not in in_dshape.dict:\n            raise ValueError('specified column not found')\n        if in_dshape.measure[self.column] not in self.IntegerTypes:\n            raise ValueError('input must be an integer column')\n\n    def apply(self, df, cuda):\n        result = (df[self.column] - self.offset) % self.modulo\n        if cudf and isinstance(df, cudf.Series):\n            if Version(cudf.__version__) >= Version('22.02'):\n                return result.to_cupy()\n            return result.to_gpu_array()\n        else:\n            return result.values\n\nclass category_binning(category_modulo):\n    \"\"\"\n    A variation on category_codes that assigns categories by binning a continuous-valued column.\n    The number of categories returned is always nbins+1.\n    The last category (nbin) is for NaNs in the data column, as well as for values under/over the\n    binned interval (when include_under or include_over is False).\n\n    Parameters\n    ----------\n    column:   column to use\n    lower:    lower bound of first bin\n    upper:    upper bound of last bin\n    nbins:     number of bins\n    include_under: if True, values below bin 0 are assigned to category 0\n    include_over:  if True, values above the last bin (nbins-1) are assigned to category nbin-1\n    \"\"\"\n\n    def __init__(self, column, lower, upper, nbins, include_under=True, include_over=True):\n        super().__init__(column, nbins + 1)\n        self.bin0 = lower\n        self.binsize = (upper - lower) / float(nbins)\n        self.nbins = nbins\n        self.bin_under = 0 if include_under else nbins\n        self.bin_over = nbins - 1 if include_over else nbins\n\n    def _hashable_inputs(self):\n        return super()._hashable_inputs() + (self.bin0, self.binsize, self.bin_under, self.bin_over)\n\n    def validate(self, in_dshape):\n        if self.column not in in_dshape.dict:\n            raise ValueError('specified column not found')\n\n    def apply(self, df, cuda):\n        if cudf and isinstance(df, cudf.DataFrame):\n            if Version(cudf.__version__) >= Version('22.02'):\n                values = df[self.column].to_cupy(na_value=cp.nan)\n            else:\n                values = cp.array(df[self.column].to_gpu_array(fillna=True))\n            nan_values = cp.isnan(values)\n        else:\n            values = df[self.column].to_numpy()\n            nan_values = np.isnan(values)\n        index_float = (values - self.bin0) / self.binsize\n        index_float[nan_values] = 0\n        index = index_float.astype(int)\n        index[index < 0] = self.bin_under\n        index[index >= self.nbins] = self.bin_over\n        index[nan_values] = self.nbins\n        return index\n\nclass category_values(CategoryPreprocess):\n    \"\"\"Extract a category and a value column from a dataframe as (2,N) numpy array of values.\"\"\"\n\n    def __init__(self, categorizer, value_column):\n        super().__init__(value_column)\n        self.categorizer = categorizer\n\n    @property\n    def inputs(self):\n        return (self.categorizer.column, self.column)\n\n    @property\n    def cat_column(self):\n        \"\"\"Returns name of categorized column\"\"\"\n        return self.categorizer.column\n\n    def categories(self, input_dshape):\n        return self.categorizer.categories\n\n    def validate(self, in_dshape):\n        return self.categorizer.validate(in_dshape)\n\n    def apply(self, df, cuda):\n        a = self.categorizer.apply(df, cuda)\n        if cudf and isinstance(df, cudf.DataFrame):\n            import cupy\n            if self.column == SpecialColumn.RowIndex:\n                nullval = -1\n            elif df[self.column].dtype.kind == 'f':\n                nullval = np.nan\n            else:\n                nullval = 0\n            a = cupy.asarray(a)\n            if self.column == SpecialColumn.RowIndex:\n                b = extract(SpecialColumn.RowIndex).apply(df, cuda)\n            elif Version(cudf.__version__) >= Version('22.02'):\n                b = df[self.column].to_cupy(na_value=nullval)\n            else:\n                b = cupy.asarray(df[self.column].fillna(nullval))\n            return cupy.stack((a, b), axis=-1)\n        else:\n            if self.column == SpecialColumn.RowIndex:\n                b = extract(SpecialColumn.RowIndex).apply(df, cuda)\n            else:\n                b = df[self.column].values\n            return np.stack((a, b), axis=-1)\n\nclass Reduction(Expr):\n    \"\"\"Base class for per-bin reductions.\"\"\"\n\n    def __init__(self, column: str | SpecialColumn | None=None):\n        self.column = column\n        self._nan_check_column = None\n\n    @property\n    def nan_check_column(self):\n        if self._nan_check_column is not None:\n            return extract(self._nan_check_column)\n        else:\n            return None\n\n    def uses_cuda_mutex(self) -> UsesCudaMutex:\n        \"\"\"Return ``True`` if this Reduction needs to use a CUDA mutex to\n        ensure that it is threadsafe across CUDA threads.\n\n        If the CUDA append functions are all atomic (i.e. using functions from\n        the numba.cuda.atomic module) then this is ``False``, otherwise it is\n        ``True``.\n        \"\"\"\n        return UsesCudaMutex.No\n\n    def uses_row_index(self, cuda, partitioned):\n        \"\"\"Return ``True`` if this Reduction uses a row index virtual column.\n\n        For some reductions the order of the rows of supplied data is\n        important. These include ``first`` and ``last`` reductions as well as\n        ``where`` reductions that return a row index. In some situations the\n        order is intrinsic such as ``first`` reductions that are processed\n        sequentially (i.e. on a CPU without using Dask) and no extra column is\n        required. But in situations of parallel processing (using a GPU or\n        Dask) extra information is needed that is provided by a row index\n        virtual column.\n\n        Returning ``True`` from this function will cause a row index column to\n        be created and passed to the ``append`` functions in the usual manner.\n        \"\"\"\n        return False\n\n    def validate(self, in_dshape):\n        if self.column == SpecialColumn.RowIndex:\n            return\n        if self.column not in in_dshape.dict:\n            raise ValueError('specified column not found')\n        if not isnumeric(in_dshape.measure[self.column]):\n            raise ValueError('input must be numeric')\n\n    @property\n    def inputs(self):\n        return (extract(self.column),)\n\n    def is_categorical(self):\n        \"\"\"Return ``True`` if this is or contains a categorical reduction.\"\"\"\n        return False\n\n    def is_where(self):\n        \"\"\"Return ``True`` if this is a ``where`` reduction or directly wraps\n        a where reduction.\"\"\"\n        return False\n\n    def _antialias_requires_2_stages(self):\n        return False\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        raise NotImplementedError(f'{type(self)}._antialias_stage_2 is not defined')\n\n    def _build_bases(self, cuda, partitioned):\n        return (self,)\n\n    def _build_combine_temps(self, cuda, partitioned):\n        return ()\n\n    def _build_temps(self, cuda=False):\n        return ()\n\n    def _build_create(self, required_dshape):\n        fields = getattr(required_dshape.measure, 'fields', None)\n        if fields is not None and len(required_dshape.measure.fields) > 0:\n            first_field = required_dshape.measure.fields[0]\n            required_dshape = dshape(first_field[1])\n        if isinstance(required_dshape, Option):\n            required_dshape = dshape(required_dshape.ty)\n        if required_dshape == dshape(ct.bool_):\n            return self._create_bool\n        elif required_dshape == dshape(ct.float32):\n            return self._create_float32_nan\n        elif required_dshape == dshape(ct.float64):\n            return self._create_float64_nan\n        elif required_dshape == dshape(ct.int64):\n            return self._create_int64\n        elif required_dshape == dshape(ct.uint32):\n            return self._create_uint32\n        else:\n            raise NotImplementedError(f'Unexpected dshape {dshape}')\n\n    def _build_append(self, dshape, schema, cuda, antialias, self_intersect):\n        if cuda:\n            if antialias and self.column is None:\n                return self._append_no_field_antialias_cuda\n            elif antialias:\n                return self._append_antialias_cuda\n            elif self.column is None:\n                return self._append_no_field_cuda\n            else:\n                return self._append_cuda\n        elif antialias and self.column is None:\n            return self._append_no_field_antialias\n        elif antialias:\n            return self._append_antialias\n        elif self.column is None:\n            return self._append_no_field\n        else:\n            return self._append\n\n    def _build_combine(self, dshape, antialias, cuda, partitioned, categorical=False):\n        return self._combine\n\n    def _build_finalize(self, dshape):\n        return self._finalize\n\n    @staticmethod\n    def _create_bool(shape, array_module):\n        return array_module.zeros(shape, dtype='bool')\n\n    @staticmethod\n    def _create_float32_nan(shape, array_module):\n        return array_module.full(shape, array_module.nan, dtype='f4')\n\n    @staticmethod\n    def _create_float64_nan(shape, array_module):\n        return array_module.full(shape, array_module.nan, dtype='f8')\n\n    @staticmethod\n    def _create_float64_empty(shape, array_module):\n        return array_module.empty(shape, dtype='f8')\n\n    @staticmethod\n    def _create_float64_zero(shape, array_module):\n        return array_module.zeros(shape, dtype='f8')\n\n    @staticmethod\n    def _create_int64(shape, array_module):\n        return array_module.full(shape, -1, dtype='i8')\n\n    @staticmethod\n    def _create_uint32(shape, array_module):\n        return array_module.zeros(shape, dtype='u4')\n\nclass OptionalFieldReduction(Reduction):\n    \"\"\"Base class for things like ``count`` or ``any`` for which the field is optional\"\"\"\n\n    def __init__(self, column=None):\n        super().__init__(column)\n\n    @property\n    def inputs(self):\n        return (extract(self.column),) if self.column is not None else ()\n\n    def validate(self, in_dshape):\n        if self.column is not None:\n            super().validate(in_dshape)\n\n    @staticmethod\n    def _finalize(bases, cuda=False, **kwargs):\n        return xr.DataArray(bases[0], **kwargs)\n\nclass SelfIntersectingOptionalFieldReduction(OptionalFieldReduction):\n    \"\"\"\n    Base class for optional field reductions for which self-intersecting\n    geometry may or may not be desirable.\n    Ignored if not using antialiasing.\n    \"\"\"\n\n    def _antialias_requires_2_stages(self):\n        return not self.self_intersect\n\n    def _build_append(self, dshape, schema, cuda, antialias, self_intersect):\n        if antialias and (not self_intersect):\n            if cuda:\n                if self.column is None:\n                    return self._append_no_field_antialias_cuda_not_self_intersect\n                else:\n                    return self._append_antialias_cuda_not_self_intersect\n            elif self.column is None:\n                return self._append_no_field_antialias_not_self_intersect\n            else:\n                return self._append_antialias_not_self_intersect\n        return super()._build_append(dshape, schema, cuda, antialias, self_intersect)\n\n    def _hashable_inputs(self):\n        return super()._hashable_inputs() + (self.self_intersect,)\n\nclass count(SelfIntersectingOptionalFieldReduction):\n    \"\"\"Count elements in each bin, returning the result as a uint32, or a\n    float32 if using antialiasing.\n\n    Parameters\n    ----------\n    column : str, optional\n        If provided, only counts elements in ``column`` that are not ``NaN``.\n        Otherwise, counts every element.\n    \"\"\"\n\n    def out_dshape(self, in_dshape, antialias, cuda, partitioned):\n        return dshape(ct.float32) if antialias else dshape(ct.uint32)\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        if self_intersect:\n            return (AntialiasStage2(AntialiasCombination.SUM_1AGG, array_module.nan),)\n        else:\n            return (AntialiasStage2(AntialiasCombination.SUM_2AGG, array_module.nan),)\n\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if not isnull(field):\n            agg[y, x] += 1\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        if not isnull(field):\n            if isnull(agg[y, x]):\n                agg[y, x] = aa_factor - prev_aa_factor\n            else:\n                agg[y, x] += aa_factor - prev_aa_factor\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias_not_self_intersect(x, y, agg, field, aa_factor, prev_aa_factor):\n        if not isnull(field):\n            if isnull(agg[y, x]) or aa_factor > agg[y, x]:\n                agg[y, x] = aa_factor\n                return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_no_field(x, y, agg):\n        agg[y, x] += 1\n        return 0\n\n    @staticmethod\n    @ngjit\n    def _append_no_field_antialias(x, y, agg, aa_factor, prev_aa_factor):\n        if isnull(agg[y, x]):\n            agg[y, x] = aa_factor - prev_aa_factor\n        else:\n            agg[y, x] += aa_factor - prev_aa_factor\n        return 0\n\n    @staticmethod\n    @ngjit\n    def _append_no_field_antialias_not_self_intersect(x, y, agg, aa_factor, prev_aa_factor):\n        if isnull(agg[y, x]) or aa_factor > agg[y, x]:\n            agg[y, x] = aa_factor\n            return 0\n        return -1\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_antialias_cuda(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field * aa_factor\n        if not isnull(value):\n            old = cuda_atomic_nanmax(agg, (y, x), value)\n            if isnull(old) or old < value:\n                return 0\n        return -1\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_no_field_antialias_cuda_not_self_intersect(x, y, agg, aa_factor, prev_aa_factor):\n        if not isnull(aa_factor):\n            old = cuda_atomic_nanmax(agg, (y, x), aa_factor)\n            if isnull(old) or old < aa_factor:\n                return 0\n        return -1\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_cuda(x, y, agg, field):\n        if not isnull(field):\n            nb_cuda.atomic.add(agg, (y, x), 1)\n            return 0\n        return -1\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_no_field_antialias_cuda(x, y, agg, aa_factor, prev_aa_factor):\n        if not isnull(aa_factor):\n            old = cuda_atomic_nanmax(agg, (y, x), aa_factor)\n            if isnull(old) or old < aa_factor:\n                return 0\n        return -1\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_no_field_cuda(x, y, agg):\n        nb_cuda.atomic.add(agg, (y, x), 1)\n        return 0\n\n    def _build_combine(self, dshape, antialias, cuda, partitioned, categorical=False):\n        if antialias:\n            return self._combine_antialias\n        else:\n            return self._combine\n\n    @staticmethod\n    def _combine(aggs):\n        return aggs.sum(axis=0, dtype='u4')\n\n    @staticmethod\n    def _combine_antialias(aggs):\n        ret = aggs[0]\n        for i in range(1, len(aggs)):\n            nansum_in_place(ret, aggs[i])\n        return ret\n\nclass _count_ignore_antialiasing(count):\n    \"\"\"Count reduction but ignores antialiasing. Used by mean reduction.\n    \"\"\"\n\n    def out_dshape(self, in_dshape, antialias, cuda, partitioned):\n        return dshape(ct.uint32)\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        if self_intersect:\n            return (AntialiasStage2(AntialiasCombination.SUM_1AGG, 0),)\n        else:\n            return (AntialiasStage2(AntialiasCombination.SUM_2AGG, 0),)\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        if not isnull(field) and prev_aa_factor == 0.0:\n            agg[y, x] += 1\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias_not_self_intersect(x, y, agg, field, aa_factor, prev_aa_factor):\n        if not isnull(field) and prev_aa_factor == 0.0:\n            agg[y, x] += 1\n            return 0\n        return -1\n\nclass by(Reduction):\n    \"\"\"Apply the provided reduction separately per category.\n\n    Parameters\n    ----------\n    cats: str or CategoryPreprocess instance\n        Name of column to aggregate over, or a categorizer object that returns categories.\n        Resulting aggregate has an outer dimension axis along the categories present.\n    reduction : Reduction\n        Per-category reduction function.\n    \"\"\"\n\n    def __init__(self, cat_column, reduction=count()):\n        super().__init__()\n        if isinstance(cat_column, CategoryPreprocess):\n            self.categorizer = cat_column\n        elif isinstance(cat_column, str):\n            self.categorizer = category_codes(cat_column)\n        else:\n            raise TypeError('first argument must be a column name or a CategoryPreprocess instance')\n        self.column = self.categorizer.column\n        self.columns = (self.categorizer.column,)\n        if (columns := getattr(reduction, 'columns', None)) is not None:\n            self.columns += columns[::-1]\n        else:\n            self.columns += (getattr(reduction, 'column', None),)\n        self.reduction = reduction\n        if self.val_column is not None:\n            self.preprocess = category_values(self.categorizer, self.val_column)\n        else:\n            self.preprocess = self.categorizer\n\n    def __hash__(self):\n        return hash((type(self), self._hashable_inputs(), self.categorizer._hashable_inputs(), self.reduction))\n\n    def _build_temps(self, cuda=False):\n        return tuple((by(self.categorizer, tmp) for tmp in self.reduction._build_temps(cuda)))\n\n    @property\n    def cat_column(self):\n        return self.columns[0]\n\n    @property\n    def val_column(self):\n        return self.columns[1]\n\n    def validate(self, in_dshape):\n        self.preprocess.validate(in_dshape)\n        self.reduction.validate(in_dshape)\n\n    def out_dshape(self, input_dshape, antialias, cuda, partitioned):\n        cats = self.categorizer.categories(input_dshape)\n        red_shape = self.reduction.out_dshape(input_dshape, antialias, cuda, partitioned)\n        return dshape(Record([(c, red_shape) for c in cats]))\n\n    @property\n    def inputs(self):\n        return (self.preprocess,)\n\n    def is_categorical(self):\n        return True\n\n    def is_where(self):\n        return self.reduction.is_where()\n\n    @property\n    def nan_check_column(self):\n        return self.reduction.nan_check_column\n\n    def uses_cuda_mutex(self) -> UsesCudaMutex:\n        return self.reduction.uses_cuda_mutex()\n\n    def uses_row_index(self, cuda, partitioned):\n        return self.reduction.uses_row_index(cuda, partitioned)\n\n    def _antialias_requires_2_stages(self):\n        return self.reduction._antialias_requires_2_stages()\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        ret = self.reduction._antialias_stage_2(self_intersect, array_module)\n        return (AntialiasStage2(combination=ret[0].combination, zero=ret[0].zero, n_reduction=ret[0].n_reduction, categorical=True),)\n\n    def _build_create(self, required_dshape):\n        n_cats = len(required_dshape.measure.fields)\n        return lambda shape, array_module: self.reduction._build_create(required_dshape)(shape + (n_cats,), array_module)\n\n    def _build_bases(self, cuda, partitioned):\n        bases = self.reduction._build_bases(cuda, partitioned)\n        if len(bases) == 1 and bases[0] is self:\n            return bases\n        return tuple((by(self.categorizer, base) for base in bases))\n\n    def _build_append(self, dshape, schema, cuda, antialias, self_intersect):\n        return self.reduction._build_append(dshape, schema, cuda, antialias, self_intersect)\n\n    def _build_combine(self, dshape, antialias, cuda, partitioned, categorical=False):\n        return self.reduction._build_combine(dshape, antialias, cuda, partitioned, True)\n\n    def _build_combine_temps(self, cuda, partitioned):\n        return self.reduction._build_combine_temps(cuda, partitioned)\n\n    def _build_finalize(self, dshape):\n        cats = list(self.categorizer.categories(dshape))\n\n        def finalize(bases, cuda=False, **kwargs):\n            kwargs = copy.deepcopy(kwargs)\n            kwargs['dims'] += [self.cat_column]\n            kwargs['coords'][self.cat_column] = cats\n            return self.reduction._build_finalize(dshape)(bases, cuda=cuda, **kwargs)\n        return finalize\n\nclass any(OptionalFieldReduction):\n    \"\"\"Whether any elements in ``column`` map to each bin.\n\n    Parameters\n    ----------\n    column : str, optional\n        If provided, any elements in ``column`` that are ``NaN`` are skipped.\n    \"\"\"\n\n    def out_dshape(self, in_dshape, antialias, cuda, partitioned):\n        return dshape(ct.float32) if antialias else dshape(ct.bool_)\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        return (AntialiasStage2(AntialiasCombination.MAX, array_module.nan),)\n\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if not isnull(field):\n            agg[y, x] = True\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        if not isnull(field):\n            if isnull(agg[y, x]) or aa_factor > agg[y, x]:\n                agg[y, x] = aa_factor\n                return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_no_field(x, y, agg):\n        agg[y, x] = True\n        return 0\n\n    @staticmethod\n    @ngjit\n    def _append_no_field_antialias(x, y, agg, aa_factor, prev_aa_factor):\n        if isnull(agg[y, x]) or aa_factor > agg[y, x]:\n            agg[y, x] = aa_factor\n            return 0\n        return -1\n    _append_cuda = _append\n    _append_no_field_cuda = _append_no_field\n\n    def _build_combine(self, dshape, antialias, cuda, partitioned, categorical=False):\n        if antialias:\n            return self._combine_antialias\n        else:\n            return self._combine\n\n    @staticmethod\n    def _combine(aggs):\n        return aggs.sum(axis=0, dtype='bool')\n\n    @staticmethod\n    def _combine_antialias(aggs):\n        ret = aggs[0]\n        for i in range(1, len(aggs)):\n            nanmax_in_place(ret, aggs[i])\n        return ret\n\nclass _upsample(Reduction):\n    \"\"\"\"Special internal class used for upsampling\"\"\"\n\n    def out_dshape(self, in_dshape, antialias, cuda, partitioned):\n        return dshape(Option(ct.float64))\n\n    @staticmethod\n    def _finalize(bases, cuda=False, **kwargs):\n        return xr.DataArray(bases[0], **kwargs)\n\n    @property\n    def inputs(self):\n        return (extract(self.column),)\n\n    def _build_create(self, required_dshape):\n        return self._create_float64_empty\n\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        pass\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_cuda(x, y, agg, field):\n        pass\n\n    @staticmethod\n    def _combine(aggs):\n        return np.nanmax(aggs, axis=0)\n\nclass FloatingReduction(Reduction):\n    \"\"\"Base classes for reductions that always have floating-point dtype.\"\"\"\n\n    def out_dshape(self, in_dshape, antialias, cuda, partitioned):\n        return dshape(Option(ct.float64))\n\n    @staticmethod\n    def _finalize(bases, cuda=False, **kwargs):\n        return xr.DataArray(bases[0], **kwargs)\n\nclass _sum_zero(FloatingReduction):\n    \"\"\"Sum of all elements in ``column``.\n\n    Parameters\n    ----------\n    column : str\n        Name of the column to aggregate over. Column data type must be numeric.\n    \"\"\"\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        if self_intersect:\n            return (AntialiasStage2(AntialiasCombination.SUM_1AGG, 0),)\n        else:\n            return (AntialiasStage2(AntialiasCombination.SUM_2AGG, 0),)\n\n    def _build_create(self, required_dshape):\n        return self._create_float64_zero\n\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if not isnull(field):\n            agg[y, x] += field\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field * (aa_factor - prev_aa_factor)\n        if not isnull(value):\n            agg[y, x] += value\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias_not_self_intersect(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field * aa_factor\n        if not isnull(value) and value > agg[y, x]:\n            agg[y, x] = value\n            return 0\n        return -1\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_cuda(x, y, agg, field):\n        if not isnull(field):\n            nb_cuda.atomic.add(agg, (y, x), field)\n            return 0\n        return -1\n\n    @staticmethod\n    def _combine(aggs):\n        return aggs.sum(axis=0, dtype='f8')\n\nclass SelfIntersectingFloatingReduction(FloatingReduction):\n    \"\"\"\n    Base class for floating reductions for which self-intersecting geometry\n    may or may not be desirable.\n    Ignored if not using antialiasing.\n    \"\"\"\n\n    def _antialias_requires_2_stages(self):\n        return not self.self_intersect\n\n    def _build_append(self, dshape, schema, cuda, antialias, self_intersect):\n        if antialias and (not self_intersect):\n            if cuda:\n                raise NotImplementedError('SelfIntersectingOptionalFieldReduction')\n            elif self.column is None:\n                return self._append_no_field_antialias_not_self_intersect\n            else:\n                return self._append_antialias_not_self_intersect\n        return super()._build_append(dshape, schema, cuda, antialias, self_intersect)\n\n    def _hashable_inputs(self):\n        return super()._hashable_inputs() + (self.self_intersect,)\n\nclass sum(SelfIntersectingFloatingReduction):\n    \"\"\"Sum of all elements in ``column``.\n\n    Elements of resulting aggregate are nan if they are not updated.\n\n    Parameters\n    ----------\n    column : str\n        Name of the column to aggregate over. Column data type must be numeric.\n        ``NaN`` values in the column are skipped.\n    \"\"\"\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        if self_intersect:\n            return (AntialiasStage2(AntialiasCombination.SUM_1AGG, array_module.nan),)\n        else:\n            return (AntialiasStage2(AntialiasCombination.SUM_2AGG, array_module.nan),)\n\n    def _build_bases(self, cuda, partitioned):\n        if cuda:\n            return (_sum_zero(self.column), any(self.column))\n        else:\n            return (self,)\n\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if not isnull(field):\n            if isnull(agg[y, x]):\n                agg[y, x] = field\n            else:\n                agg[y, x] += field\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field * (aa_factor - prev_aa_factor)\n        if not isnull(value):\n            if isnull(agg[y, x]):\n                agg[y, x] = value\n            else:\n                agg[y, x] += value\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias_not_self_intersect(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field * aa_factor\n        if not isnull(value):\n            if isnull(agg[y, x]) or value > agg[y, x]:\n                agg[y, x] = value\n                return 0\n        return -1\n\n    @staticmethod\n    def _combine(aggs):\n        return nansum_missing(aggs, axis=0)\n\n    @staticmethod\n    def _finalize(bases, cuda=False, **kwargs):\n        if cuda:\n            sums, anys = bases\n            x = np.where(anys, sums, np.nan)\n            return xr.DataArray(x, **kwargs)\n        else:\n            return xr.DataArray(bases[0], **kwargs)\n\nclass m2(FloatingReduction):\n    \"\"\"Sum of square differences from the mean of all elements in ``column``.\n\n    Intermediate value for computing ``var`` and ``std``, not intended to be\n    used on its own.\n\n    Parameters\n    ----------\n    column : str\n        Name of the column to aggregate over. Column data type must be numeric.\n        ``NaN`` values in the column are skipped.\n    \"\"\"\n\n    def uses_cuda_mutex(self) -> UsesCudaMutex:\n        return UsesCudaMutex.Global\n\n    def _build_append(self, dshape, schema, cuda, antialias, self_intersect):\n        return super()._build_append(dshape, schema, cuda, antialias, self_intersect)\n\n    def _build_create(self, required_dshape):\n        return self._create_float64_zero\n\n    def _build_temps(self, cuda=False):\n        return (_sum_zero(self.column), count(self.column))\n\n    @staticmethod\n    @ngjit\n    def _append(x, y, m2, field, sum, count):\n        if not isnull(field):\n            if count > 0:\n                u1 = np.float64(sum) / count\n                u = np.float64(sum + field) / (count + 1)\n                m2[y, x] += (field - u1) * (field - u)\n                return 0\n        return -1\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_cuda(x, y, m2, field, sum, count):\n        if not isnull(field):\n            if count > 0:\n                u1 = np.float64(sum) / count\n                u = np.float64(sum + field) / (count + 1)\n                m2[y, x] += (field - u1) * (field - u)\n                return 0\n        return -1\n\n    @staticmethod\n    def _combine(Ms, sums, ns):\n        with np.errstate(divide='ignore', invalid='ignore'):\n            mu = np.nansum(sums, axis=0) / ns.sum(axis=0)\n            return np.nansum(Ms + ns * (sums / ns - mu) ** 2, axis=0)\n\nclass min(FloatingReduction):\n    \"\"\"Minimum value of all elements in ``column``.\n\n    Parameters\n    ----------\n    column : str\n        Name of the column to aggregate over. Column data type must be numeric.\n        ``NaN`` values in the column are skipped.\n    \"\"\"\n\n    def _antialias_requires_2_stages(self):\n        return True\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        return (AntialiasStage2(AntialiasCombination.MIN, array_module.nan),)\n\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if not isnull(field) and (isnull(agg[y, x]) or agg[y, x] > field):\n            agg[y, x] = field\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field * aa_factor\n        if not isnull(value) and (isnull(agg[y, x]) or value > agg[y, x]):\n            agg[y, x] = value\n            return 0\n        return -1\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_cuda(x, y, agg, field):\n        if not isnull(field):\n            old = cuda_atomic_nanmin(agg, (y, x), field)\n            if isnull(old) or old > field:\n                return 0\n        return -1\n\n    @staticmethod\n    def _combine(aggs):\n        return np.nanmin(aggs, axis=0)\n\nclass max(FloatingReduction):\n    \"\"\"Maximum value of all elements in ``column``.\n\n    Parameters\n    ----------\n    column : str\n        Name of the column to aggregate over. Column data type must be numeric.\n        ``NaN`` values in the column are skipped.\n    \"\"\"\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        return (AntialiasStage2(AntialiasCombination.MAX, array_module.nan),)\n\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if not isnull(field) and (isnull(agg[y, x]) or agg[y, x] < field):\n            agg[y, x] = field\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field * aa_factor\n        if not isnull(value) and (isnull(agg[y, x]) or value > agg[y, x]):\n            agg[y, x] = value\n            return 0\n        return -1\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_antialias_cuda(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field * aa_factor\n        if not isnull(value):\n            old = cuda_atomic_nanmax(agg, (y, x), value)\n            if isnull(old) or old < value:\n                return 0\n        return -1\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_cuda(x, y, agg, field):\n        if not isnull(field):\n            old = cuda_atomic_nanmax(agg, (y, x), field)\n            if isnull(old) or old < field:\n                return 0\n        return -1\n\n    @staticmethod\n    def _combine(aggs):\n        return np.nanmax(aggs, axis=0)\n\nclass count_cat(by):\n    \"\"\"Count of all elements in ``column``, grouped by category.\n    Alias for `by(...,count())`, for backwards compatibility.\n\n    Parameters\n    ----------\n    column : str\n        Name of the column to aggregate over. Column data type must be\n        categorical. Resulting aggregate has a outer dimension axis along the\n        categories present.\n    \"\"\"\n\n    def __init__(self, column):\n        super().__init__(column, count())\n\nclass mean(Reduction):\n    \"\"\"Mean of all elements in ``column``.\n\n    Parameters\n    ----------\n    column : str\n        Name of the column to aggregate over. Column data type must be numeric.\n        ``NaN`` values in the column are skipped.\n    \"\"\"\n\n    def _build_bases(self, cuda, partitioned):\n        return (_sum_zero(self.column), _count_ignore_antialiasing(self.column))\n\n    @staticmethod\n    def _finalize(bases, cuda=False, **kwargs):\n        sums, counts = bases\n        with np.errstate(divide='ignore', invalid='ignore'):\n            x = np.where(counts > 0, sums / counts, np.nan)\n        return xr.DataArray(x, **kwargs)\n\nclass var(Reduction):\n    \"\"\"Variance of all elements in ``column``.\n\n    Parameters\n    ----------\n    column : str\n        Name of the column to aggregate over. Column data type must be numeric.\n        ``NaN`` values in the column are skipped.\n    \"\"\"\n\n    def _build_bases(self, cuda, partitioned):\n        return (_sum_zero(self.column), count(self.column), m2(self.column))\n\n    @staticmethod\n    def _finalize(bases, cuda=False, **kwargs):\n        sums, counts, m2s = bases\n        with np.errstate(divide='ignore', invalid='ignore'):\n            x = np.where(counts > 0, m2s / counts, np.nan)\n        return xr.DataArray(x, **kwargs)\n\nclass std(Reduction):\n    \"\"\"Standard Deviation of all elements in ``column``.\n\n    Parameters\n    ----------\n    column : str\n        Name of the column to aggregate over. Column data type must be numeric.\n        ``NaN`` values in the column are skipped.\n    \"\"\"\n\n    def _build_bases(self, cuda, partitioned):\n        return (_sum_zero(self.column), count(self.column), m2(self.column))\n\n    @staticmethod\n    def _finalize(bases, cuda=False, **kwargs):\n        sums, counts, m2s = bases\n        with np.errstate(divide='ignore', invalid='ignore'):\n            x = np.where(counts > 0, np.sqrt(m2s / counts), np.nan)\n        return xr.DataArray(x, **kwargs)\n\nclass _first_or_last(Reduction):\n    \"\"\"Abstract base class of first and last reductions.\n    \"\"\"\n\n    def out_dshape(self, in_dshape, antialias, cuda, partitioned):\n        return dshape(ct.float64)\n\n    def uses_row_index(self, cuda, partitioned):\n        return cuda or partitioned\n\n    def _antialias_requires_2_stages(self):\n        return True\n\n    def _build_bases(self, cuda, partitioned):\n        if self.uses_row_index(cuda, partitioned):\n            row_index_selector = self._create_row_index_selector()\n            wrapper = where(selector=row_index_selector, lookup_column=self.column)\n            wrapper._nan_check_column = self.column\n            return row_index_selector._build_bases(cuda, partitioned) + (wrapper,)\n        else:\n            return super()._build_bases(cuda, partitioned)\n\n    @staticmethod\n    def _combine(aggs):\n        if len(aggs) > 1:\n            raise RuntimeError('_combine should never be called with more than one agg')\n        return aggs[0]\n\n    def _create_row_index_selector(self):\n        pass\n\n    @staticmethod\n    def _finalize(bases, cuda=False, **kwargs):\n        return xr.DataArray(bases[-1], **kwargs)\n\nclass first(_first_or_last):\n    \"\"\"First value encountered in ``column``.\n\n    Useful for categorical data where an actual value must always be returned,\n    not an average or other numerical calculation.\n\n    Currently only supported for rasters, externally to this class.\n\n    Parameters\n    ----------\n    column : str\n        Name of the column to aggregate over. If the data type is floating point,\n        ``NaN`` values in the column are skipped.\n    \"\"\"\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        return (AntialiasStage2(AntialiasCombination.FIRST, array_module.nan),)\n\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if not isnull(field) and isnull(agg[y, x]):\n            agg[y, x] = field\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field * aa_factor\n        if not isnull(value) and (isnull(agg[y, x]) or value > agg[y, x]):\n            agg[y, x] = value\n            return 0\n        return -1\n\n    def _create_row_index_selector(self):\n        return _min_row_index()\n\nclass last(_first_or_last):\n    \"\"\"Last value encountered in ``column``.\n\n    Useful for categorical data where an actual value must always be returned,\n    not an average or other numerical calculation.\n\n    Currently only supported for rasters, externally to this class.\n\n    Parameters\n    ----------\n    column : str\n        Name of the column to aggregate over. If the data type is floating point,\n        ``NaN`` values in the column are skipped.\n    \"\"\"\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        return (AntialiasStage2(AntialiasCombination.LAST, array_module.nan),)\n\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if not isnull(field):\n            agg[y, x] = field\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field * aa_factor\n        if not isnull(value) and (isnull(agg[y, x]) or value > agg[y, x]):\n            agg[y, x] = value\n            return 0\n        return -1\n\n    def _create_row_index_selector(self):\n        return _max_row_index()\n\nclass FloatingNReduction(OptionalFieldReduction):\n\n    def __init__(self, column=None, n=1):\n        super().__init__(column)\n        self.n = n if n >= 1 else 1\n\n    def out_dshape(self, in_dshape, antialias, cuda, partitioned):\n        return dshape(ct.float64)\n\n    def _add_finalize_kwargs(self, **kwargs):\n        n_name = 'n'\n        n_values = np.arange(self.n)\n        kwargs = copy.deepcopy(kwargs)\n        kwargs['dims'] += [n_name]\n        kwargs['coords'][n_name] = n_values\n        return kwargs\n\n    def _build_create(self, required_dshape):\n        return lambda shape, array_module: super(FloatingNReduction, self)._build_create(required_dshape)(shape + (self.n,), array_module)\n\n    def _build_finalize(self, dshape):\n\n        def finalize(bases, cuda=False, **kwargs):\n            kwargs = self._add_finalize_kwargs(**kwargs)\n            return self._finalize(bases, cuda=cuda, **kwargs)\n        return finalize\n\n    def _hashable_inputs(self):\n        return super()._hashable_inputs() + (self.n,)\n\nclass _first_n_or_last_n(FloatingNReduction):\n    \"\"\"Abstract base class of first_n and last_n reductions.\n    \"\"\"\n\n    def uses_row_index(self, cuda, partitioned):\n        return cuda or partitioned\n\n    def _antialias_requires_2_stages(self):\n        return True\n\n    def _build_bases(self, cuda, partitioned):\n        if self.uses_row_index(cuda, partitioned):\n            row_index_selector = self._create_row_index_selector()\n            wrapper = where(selector=row_index_selector, lookup_column=self.column)\n            wrapper._nan_check_column = self.column\n            return row_index_selector._build_bases(cuda, partitioned) + (wrapper,)\n        else:\n            return super()._build_bases(cuda, partitioned)\n\n    @staticmethod\n    def _combine(aggs):\n        if len(aggs) > 1:\n            raise RuntimeError('_combine should never be called with more than one agg')\n        return aggs[0]\n\n    def _create_row_index_selector(self):\n        pass\n\n    @staticmethod\n    def _finalize(bases, cuda=False, **kwargs):\n        return xr.DataArray(bases[-1], **kwargs)\n\nclass first_n(_first_n_or_last_n):\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        return (AntialiasStage2(AntialiasCombination.FIRST, array_module.nan, n_reduction=True),)\n\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if not isnull(field):\n            n = agg.shape[2]\n            if not isnull(agg[y, x, n - 1]):\n                return -1\n            for i in range(n):\n                if isnull(agg[y, x, i]):\n                    agg[y, x, i] = field\n                    return i\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field * aa_factor\n        if not isnull(value):\n            n = agg.shape[2]\n            if not isnull(agg[y, x, n - 1]):\n                return -1\n            for i in range(n):\n                if isnull(agg[y, x, i]):\n                    agg[y, x, i] = value\n                    return i\n        return -1\n\n    def _create_row_index_selector(self):\n        return _min_n_row_index(n=self.n)\n\nclass last_n(_first_n_or_last_n):\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        return (AntialiasStage2(AntialiasCombination.LAST, array_module.nan, n_reduction=True),)\n\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if not isnull(field):\n            shift_and_insert(agg[y, x], field, 0)\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field * aa_factor\n        if not isnull(value):\n            shift_and_insert(agg[y, x], value, 0)\n            return 0\n        return -1\n\n    def _create_row_index_selector(self):\n        return _max_n_row_index(n=self.n)\n\nclass max_n(FloatingNReduction):\n\n    def uses_cuda_mutex(self) -> UsesCudaMutex:\n        return UsesCudaMutex.Local\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        return (AntialiasStage2(AntialiasCombination.MAX, array_module.nan, n_reduction=True),)\n\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if not isnull(field):\n            n = agg.shape[2]\n            for i in range(n):\n                if isnull(agg[y, x, i]) or field > agg[y, x, i]:\n                    shift_and_insert(agg[y, x], field, i)\n                    return i\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field * aa_factor\n        if not isnull(value):\n            n = agg.shape[2]\n            for i in range(n):\n                if isnull(agg[y, x, i]) or value > agg[y, x, i]:\n                    shift_and_insert(agg[y, x], value, i)\n                    return i\n        return -1\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_cuda(x, y, agg, field):\n        if not isnull(field):\n            n = agg.shape[2]\n            for i in range(n):\n                if isnull(agg[y, x, i]) or field > agg[y, x, i]:\n                    cuda_shift_and_insert(agg[y, x], field, i)\n                    return i\n        return -1\n\n    def _build_combine(self, dshape, antialias, cuda, partitioned, categorical=False):\n        if cuda:\n            return self._combine_cuda\n        else:\n            return self._combine\n\n    @staticmethod\n    def _combine(aggs):\n        ret = aggs[0]\n        for i in range(1, len(aggs)):\n            if ret.ndim == 3:\n                nanmax_n_in_place_3d(aggs[0], aggs[i])\n            else:\n                nanmax_n_in_place_4d(aggs[0], aggs[i])\n        return ret\n\n    @staticmethod\n    def _combine_cuda(aggs):\n        ret = aggs[0]\n        kernel_args = cuda_args(ret.shape[:-1])\n        for i in range(1, len(aggs)):\n            if ret.ndim == 3:\n                cuda_nanmax_n_in_place_3d[kernel_args](aggs[0], aggs[i])\n            else:\n                cuda_nanmax_n_in_place_4d[kernel_args](aggs[0], aggs[i])\n        return ret\n\nclass min_n(FloatingNReduction):\n\n    def uses_cuda_mutex(self) -> UsesCudaMutex:\n        return UsesCudaMutex.Local\n\n    def _antialias_requires_2_stages(self):\n        return True\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        return (AntialiasStage2(AntialiasCombination.MIN, array_module.nan, n_reduction=True),)\n\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if not isnull(field):\n            n = agg.shape[2]\n            for i in range(n):\n                if isnull(agg[y, x, i]) or field < agg[y, x, i]:\n                    shift_and_insert(agg[y, x], field, i)\n                    return i\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        value = field * aa_factor\n        if not isnull(value):\n            n = agg.shape[2]\n            for i in range(n):\n                if isnull(agg[y, x, i]) or value < agg[y, x, i]:\n                    shift_and_insert(agg[y, x], value, i)\n                    return i\n        return -1\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_cuda(x, y, agg, field):\n        if not isnull(field):\n            n = agg.shape[2]\n            for i in range(n):\n                if isnull(agg[y, x, i]) or field < agg[y, x, i]:\n                    cuda_shift_and_insert(agg[y, x], field, i)\n                    return i\n        return -1\n\n    def _build_combine(self, dshape, antialias, cuda, partitioned, categorical=False):\n        if cuda:\n            return self._combine_cuda\n        else:\n            return self._combine\n\n    @staticmethod\n    def _combine(aggs):\n        ret = aggs[0]\n        for i in range(1, len(aggs)):\n            if ret.ndim == 3:\n                nanmin_n_in_place_3d(aggs[0], aggs[i])\n            else:\n                nanmin_n_in_place_4d(aggs[0], aggs[i])\n        return ret\n\n    @staticmethod\n    def _combine_cuda(aggs):\n        ret = aggs[0]\n        kernel_args = cuda_args(ret.shape[:-1])\n        for i in range(1, len(aggs)):\n            if ret.ndim == 3:\n                cuda_nanmin_n_in_place_3d[kernel_args](aggs[0], aggs[i])\n            else:\n                cuda_nanmin_n_in_place_4d[kernel_args](aggs[0], aggs[i])\n        return ret\n\nclass mode(Reduction):\n    \"\"\"Mode (most common value) of all the values encountered in ``column``.\n\n    Useful for categorical data where an actual value must always be returned,\n    not an average or other numerical calculation.\n\n    Currently only supported for rasters, externally to this class.\n    Implementing it for other glyph types would be difficult due to potentially\n    unbounded data storage requirements to store indefinite point or line\n    data per pixel.\n\n    Parameters\n    ----------\n    column : str\n        Name of the column to aggregate over. If the data type is floating point,\n        ``NaN`` values in the column are skipped.\n    \"\"\"\n\n    def out_dshape(self, in_dshape, antialias, cuda, partitioned):\n        return dshape(Option(ct.float64))\n\n    @staticmethod\n    def _append(x, y, agg):\n        raise NotImplementedError('mode is currently implemented only for rasters')\n\n    @staticmethod\n    def _combine(aggs):\n        raise NotImplementedError('mode is currently implemented only for rasters')\n\n    @staticmethod\n    def _finalize(bases, **kwargs):\n        raise NotImplementedError('mode is currently implemented only for rasters')\n\nclass where(FloatingReduction):\n    \"\"\"\n    Returns values from a ``lookup_column`` corresponding to a ``selector``\n    reduction that is applied to some other column.\n\n    If ``lookup_column`` is ``None`` then it uses the index of the row in the\n    DataFrame instead of a named column. This is returned as an int64\n    aggregation with -1 used to denote no value.\n\n    Examples\n    --------\n    >>> canvas.line(df, 'x', 'y', agg=ds.where(ds.max(\"value\"), \"other\"))  # doctest: +SKIP\n\n    This returns the values of the \"other\" column that correspond to the\n    maximum of the \"value\" column in each bin.\n\n    Parameters\n    ----------\n    selector: Reduction\n        Reduction used to select the values of the ``lookup_column`` which are\n        returned by this ``where`` reduction.\n\n    lookup_column : str | None\n        Column containing values that are returned from this ``where``\n        reduction, or ``None`` to return row indexes instead.\n    \"\"\"\n\n    def __init__(self, selector: Reduction, lookup_column: str | None=None):\n        if not isinstance(selector, (first, first_n, last, last_n, max, max_n, min, min_n, _max_or_min_row_index, _max_n_or_min_n_row_index)):\n            raise TypeError('selector can only be a first, first_n, last, last_n, max, max_n, min or min_n reduction')\n        if lookup_column is None:\n            lookup_column = SpecialColumn.RowIndex\n        super().__init__(lookup_column)\n        self.selector = selector\n        self.columns = (selector.column, lookup_column)\n\n    def __hash__(self):\n        return hash((type(self), self._hashable_inputs(), self.selector))\n\n    def is_where(self):\n        return True\n\n    def out_dshape(self, input_dshape, antialias, cuda, partitioned):\n        if self.column == SpecialColumn.RowIndex:\n            return dshape(ct.int64)\n        else:\n            return dshape(ct.float64)\n\n    def uses_cuda_mutex(self) -> UsesCudaMutex:\n        return UsesCudaMutex.Local\n\n    def uses_row_index(self, cuda, partitioned):\n        return self.column == SpecialColumn.RowIndex or self.selector.uses_row_index(cuda, partitioned)\n\n    def validate(self, in_dshape):\n        if self.column != SpecialColumn.RowIndex:\n            super().validate(in_dshape)\n        self.selector.validate(in_dshape)\n        if self.column != SpecialColumn.RowIndex and self.column == self.selector.column:\n            raise ValueError('where and its contained reduction cannot use the same column')\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        ret = self.selector._antialias_stage_2(self_intersect, array_module)\n        if self.column == SpecialColumn.RowIndex:\n            ret = (AntialiasStage2(combination=ret[0].combination, zero=-1, n_reduction=ret[0].n_reduction),)\n        return ret\n\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field, update_index):\n        if agg.ndim > 2:\n            shift_and_insert(agg[y, x], field, update_index)\n        else:\n            agg[y, x] = field\n        return update_index\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor, update_index):\n        if agg.ndim > 2:\n            shift_and_insert(agg[y, x], field, update_index)\n        else:\n            agg[y, x] = field\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_antialias_cuda(x, y, agg, field, aa_factor, prev_aa_factor, update_index):\n        if agg.ndim > 2:\n            cuda_shift_and_insert(agg[y, x], field, update_index)\n        else:\n            agg[y, x] = field\n        return update_index\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_cuda(x, y, agg, field, update_index):\n        if agg.ndim > 2:\n            cuda_shift_and_insert(agg[y, x], field, update_index)\n        else:\n            agg[y, x] = field\n        return update_index\n\n    def _build_append(self, dshape, schema, cuda, antialias, self_intersect):\n        if cuda:\n            if antialias:\n                return self._append_antialias_cuda\n            else:\n                return self._append_cuda\n        elif antialias:\n            return self._append_antialias\n        else:\n            return self._append\n\n    def _build_bases(self, cuda, partitioned):\n        selector = self.selector\n        if isinstance(selector, (_first_or_last, _first_n_or_last_n)) and selector.uses_row_index(cuda, partitioned):\n            row_index_selector = selector._create_row_index_selector()\n            if self.column == SpecialColumn.RowIndex:\n                row_index_selector._nan_check_column = self.selector.column\n                return row_index_selector._build_bases(cuda, partitioned)\n            else:\n                new_where = where(row_index_selector, self.column)\n                new_where._nan_check_column = self.selector.column\n                return row_index_selector._build_bases(cuda, partitioned) + new_where._build_bases(cuda, partitioned)\n        else:\n            return selector._build_bases(cuda, partitioned) + super()._build_bases(cuda, partitioned)\n\n    def _combine_callback(self, cuda, partitioned, categorical):\n        selector = self.selector\n        is_n_reduction = isinstance(selector, FloatingNReduction)\n        if cuda:\n            append = selector._append_cuda\n        else:\n            append = selector._append\n        invalid = isminus1 if self.selector.uses_row_index(cuda, partitioned) else isnull\n\n        @ngjit\n        def combine_cpu_2d(aggs, selector_aggs):\n            ny, nx = aggs[0].shape\n            for y in range(ny):\n                for x in range(nx):\n                    value = selector_aggs[1][y, x]\n                    if not invalid(value) and append(x, y, selector_aggs[0], value) >= 0:\n                        aggs[0][y, x] = aggs[1][y, x]\n\n        @ngjit\n        def combine_cpu_3d(aggs, selector_aggs):\n            ny, nx, ncat = aggs[0].shape\n            for y in range(ny):\n                for x in range(nx):\n                    for cat in range(ncat):\n                        value = selector_aggs[1][y, x, cat]\n                        if not invalid(value) and append(x, y, selector_aggs[0][:, :, cat], value) >= 0:\n                            aggs[0][y, x, cat] = aggs[1][y, x, cat]\n\n        @ngjit\n        def combine_cpu_n_3d(aggs, selector_aggs):\n            ny, nx, n = aggs[0].shape\n            for y in range(ny):\n                for x in range(nx):\n                    for i in range(n):\n                        value = selector_aggs[1][y, x, i]\n                        if invalid(value):\n                            break\n                        update_index = append(x, y, selector_aggs[0], value)\n                        if update_index < 0:\n                            break\n                        shift_and_insert(aggs[0][y, x], aggs[1][y, x, i], update_index)\n\n        @ngjit\n        def combine_cpu_n_4d(aggs, selector_aggs):\n            ny, nx, ncat, n = aggs[0].shape\n            for y in range(ny):\n                for x in range(nx):\n                    for cat in range(ncat):\n                        for i in range(n):\n                            value = selector_aggs[1][y, x, cat, i]\n                            if invalid(value):\n                                break\n                            update_index = append(x, y, selector_aggs[0][:, :, cat, :], value)\n                            if update_index < 0:\n                                break\n                            shift_and_insert(aggs[0][y, x, cat], aggs[1][y, x, cat, i], update_index)\n\n        @nb_cuda.jit\n        def combine_cuda_2d(aggs, selector_aggs):\n            ny, nx = aggs[0].shape\n            x, y = nb_cuda.grid(2)\n            if x < nx and y < ny:\n                value = selector_aggs[1][y, x]\n                if not invalid(value) and append(x, y, selector_aggs[0], value) >= 0:\n                    aggs[0][y, x] = aggs[1][y, x]\n\n        @nb_cuda.jit\n        def combine_cuda_3d(aggs, selector_aggs):\n            ny, nx, ncat = aggs[0].shape\n            x, y, cat = nb_cuda.grid(3)\n            if x < nx and y < ny and (cat < ncat):\n                value = selector_aggs[1][y, x, cat]\n                if not invalid(value) and append(x, y, selector_aggs[0][:, :, cat], value) >= 0:\n                    aggs[0][y, x, cat] = aggs[1][y, x, cat]\n\n        @nb_cuda.jit\n        def combine_cuda_n_3d(aggs, selector_aggs):\n            ny, nx, n = aggs[0].shape\n            x, y = nb_cuda.grid(2)\n            if x < nx and y < ny:\n                for i in range(n):\n                    value = selector_aggs[1][y, x, i]\n                    if invalid(value):\n                        break\n                    update_index = append(x, y, selector_aggs[0], value)\n                    if update_index < 0:\n                        break\n                    cuda_shift_and_insert(aggs[0][y, x], aggs[1][y, x, i], update_index)\n\n        @nb_cuda.jit\n        def combine_cuda_n_4d(aggs, selector_aggs):\n            ny, nx, ncat, n = aggs[0].shape\n            x, y, cat = nb_cuda.grid(3)\n            if x < nx and y < ny and (cat < ncat):\n                for i in range(n):\n                    value = selector_aggs[1][y, x, cat, i]\n                    if invalid(value):\n                        break\n                    update_index = append(x, y, selector_aggs[0][:, :, cat, :], value)\n                    if update_index < 0:\n                        break\n                    cuda_shift_and_insert(aggs[0][y, x, cat], aggs[1][y, x, cat, i], update_index)\n        if is_n_reduction:\n            if cuda:\n                return combine_cuda_n_4d if categorical else combine_cuda_n_3d\n            else:\n                return combine_cpu_n_4d if categorical else combine_cpu_n_3d\n        elif cuda:\n            return combine_cuda_3d if categorical else combine_cuda_2d\n        else:\n            return combine_cpu_3d if categorical else combine_cpu_2d\n\n    def _build_combine(self, dshape, antialias, cuda, partitioned, categorical=False):\n        combine = self._combine_callback(cuda, partitioned, categorical)\n\n        def wrapped_combine(aggs, selector_aggs):\n            if len(aggs) == 1:\n                pass\n            elif cuda:\n                assert len(aggs) == 2\n                is_n_reduction = isinstance(self.selector, FloatingNReduction)\n                shape = aggs[0].shape[:-1] if is_n_reduction else aggs[0].shape\n                combine[cuda_args(shape)](aggs, selector_aggs)\n            else:\n                for i in range(1, len(aggs)):\n                    combine((aggs[0], aggs[i]), (selector_aggs[0], selector_aggs[i]))\n            return (aggs[0], selector_aggs[0])\n        return wrapped_combine\n\n    def _build_combine_temps(self, cuda, partitioned):\n        return (self.selector,)\n\n    def _build_create(self, required_dshape):\n        if isinstance(self.selector, FloatingNReduction):\n            return lambda shape, array_module: super(where, self)._build_create(required_dshape)(shape + (self.selector.n,), array_module)\n        else:\n            return super()._build_create(required_dshape)\n\n    def _build_finalize(self, dshape):\n        if isinstance(self.selector, FloatingNReduction):\n            add_finalize_kwargs = self.selector._add_finalize_kwargs\n        else:\n            add_finalize_kwargs = None\n\n        def finalize(bases, cuda=False, **kwargs):\n            if add_finalize_kwargs is not None:\n                kwargs = add_finalize_kwargs(**kwargs)\n            return xr.DataArray(bases[-1], **kwargs)\n        return finalize\n\nclass summary(Expr):\n    \"\"\"A collection of named reductions.\n\n    Computes all aggregates simultaneously, output is stored as a\n    ``xarray.Dataset``.\n\n    Examples\n    --------\n    A reduction for computing the mean of column \"a\", and the sum of column \"b\"\n    for each bin, all in a single pass.\n\n    >>> import datashader as ds\n    >>> red = ds.summary(mean_a=ds.mean('a'), sum_b=ds.sum('b'))\n\n    Notes\n    -----\n    A single pass of the source dataset using antialiased lines can either be\n    performed using a single-stage aggregation (e.g. ``self_intersect=True``)\n    or two stages (``self_intersect=False``). If a ``summary`` contains a\n    ``count`` or ``sum`` reduction with ``self_intersect=False``, or any of\n    ``first``, ``last`` or ``min``, then the antialiased line pass will be\n    performed in two stages.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        ks, vs = zip(*sorted(kwargs.items()))\n        self.keys = ks\n        self.values = vs\n\n    def __hash__(self):\n        return hash((type(self), tuple(self.keys), tuple(self.values)))\n\n    def is_categorical(self):\n        for v in self.values:\n            if v.is_categorical():\n                return True\n        return False\n\n    def uses_row_index(self, cuda, partitioned):\n        for v in self.values:\n            if v.uses_row_index(cuda, partitioned):\n                return True\n        return False\n\n    def validate(self, input_dshape):\n        for v in self.values:\n            v.validate(input_dshape)\n        n_values = []\n        for v in self.values:\n            if isinstance(v, where):\n                v = v.selector\n            if isinstance(v, FloatingNReduction):\n                n_values.append(v.n)\n        if len(np.unique(n_values)) > 1:\n            raise ValueError('Using multiple FloatingNReductions with different n values is not supported')\n\n    @property\n    def inputs(self):\n        return tuple(unique(concat((v.inputs for v in self.values))))\n\nclass _max_or_min_row_index(OptionalFieldReduction):\n    \"\"\"Abstract base class of max and min row_index reductions.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(column=SpecialColumn.RowIndex)\n\n    def out_dshape(self, in_dshape, antialias, cuda, partitioned):\n        return dshape(ct.int64)\n\n    def uses_row_index(self, cuda, partitioned):\n        return True\n\nclass _max_row_index(_max_or_min_row_index):\n    \"\"\"Max reduction operating on row index.\n\n    This is a private class as it is not intended to be used explicitly in\n    user code. It is primarily purpose is to support the use of ``last``\n    reductions using dask and/or CUDA.\n    \"\"\"\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        return (AntialiasStage2(AntialiasCombination.MAX, -1),)\n\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if field > agg[y, x]:\n            agg[y, x] = field\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        if field > agg[y, x]:\n            agg[y, x] = field\n            return 0\n        return -1\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_cuda(x, y, agg, field):\n        if field != -1:\n            old = nb_cuda.atomic.max(agg, (y, x), field)\n            if old < field:\n                return 0\n        return -1\n\n    @staticmethod\n    def _combine(aggs):\n        ret = aggs[0]\n        for i in range(1, len(aggs)):\n            np.maximum(ret, aggs[i], out=ret)\n        return ret\n\nclass _min_row_index(_max_or_min_row_index):\n    \"\"\"Min reduction operating on row index.\n\n    This is a private class as it is not intended to be used explicitly in\n    user code. It is primarily purpose is to support the use of ``first``\n    reductions using dask and/or CUDA.\n    \"\"\"\n\n    def _antialias_requires_2_stages(self):\n        return True\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        return (AntialiasStage2(AntialiasCombination.MIN, -1),)\n\n    def uses_cuda_mutex(self) -> UsesCudaMutex:\n        return UsesCudaMutex.Local\n\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if field != -1 and (agg[y, x] == -1 or field < agg[y, x]):\n            agg[y, x] = field\n            return 0\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        if field != -1 and (agg[y, x] == -1 or field < agg[y, x]):\n            agg[y, x] = field\n            return 0\n        return -1\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_cuda(x, y, agg, field):\n        if field != -1 and (agg[y, x] == -1 or field < agg[y, x]):\n            agg[y, x] = field\n            return 0\n        return -1\n\n    def _build_combine(self, dshape, antialias, cuda, partitioned, categorical=False):\n        if cuda:\n            return self._combine_cuda\n        else:\n            return self._combine\n\n    @staticmethod\n    def _combine(aggs):\n        ret = aggs[0]\n        for i in range(1, len(aggs)):\n            row_min_in_place(ret, aggs[i])\n        return ret\n\n    @staticmethod\n    def _combine_cuda(aggs):\n        ret = aggs[0]\n        if len(aggs) > 1:\n            if ret.ndim == 2:\n                aggs = [cp.expand_dims(agg, 2) for agg in aggs]\n            kernel_args = cuda_args(ret.shape[:3])\n            for i in range(1, len(aggs)):\n                cuda_row_min_in_place[kernel_args](aggs[0], aggs[i])\n        return ret\n\nclass _max_n_or_min_n_row_index(FloatingNReduction):\n    \"\"\"Abstract base class of max_n and min_n row_index reductions.\n    \"\"\"\n\n    def __init__(self, n=1):\n        super().__init__(column=SpecialColumn.RowIndex)\n        self.n = n if n >= 1 else 1\n\n    def out_dshape(self, in_dshape, antialias, cuda, partitioned):\n        return dshape(ct.int64)\n\n    def uses_cuda_mutex(self) -> UsesCudaMutex:\n        return UsesCudaMutex.Local\n\n    def uses_row_index(self, cuda, partitioned):\n        return True\n\n    def _build_combine(self, dshape, antialias, cuda, partitioned, categorical=False):\n        if cuda:\n            return self._combine_cuda\n        else:\n            return self._combine\n\nclass _max_n_row_index(_max_n_or_min_n_row_index):\n    \"\"\"Max_n reduction operating on row index.\n\n    This is a private class as it is not intended to be used explicitly in\n    user code. It is primarily purpose is to support the use of ``last_n``\n    reductions using dask and/or CUDA.\n    \"\"\"\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        return (AntialiasStage2(AntialiasCombination.MAX, -1, n_reduction=True),)\n\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if field != -1:\n            n = agg.shape[2]\n            for i in range(n):\n                if agg[y, x, i] == -1 or field > agg[y, x, i]:\n                    shift_and_insert(agg[y, x], field, i)\n                    return i\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        if field != -1:\n            n = agg.shape[2]\n            for i in range(n):\n                if agg[y, x, i] == -1 or field > agg[y, x, i]:\n                    for j in range(n - 1, i, -1):\n                        agg[y, x, j] = agg[y, x, j - 1]\n                    agg[y, x, i] = field\n                    return i\n        return -1\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_cuda(x, y, agg, field):\n        if field != -1:\n            n = agg.shape[2]\n            for i in range(n):\n                if agg[y, x, i] == -1 or field > agg[y, x, i]:\n                    cuda_shift_and_insert(agg[y, x], field, i)\n                    return i\n        return -1\n\n    @staticmethod\n    def _combine(aggs):\n        ret = aggs[0]\n        if len(aggs) > 1:\n            if ret.ndim == 3:\n                row_max_n_in_place_3d(aggs[0], aggs[1])\n            else:\n                row_max_n_in_place_4d(aggs[0], aggs[1])\n        return ret\n\n    @staticmethod\n    def _combine_cuda(aggs):\n        ret = aggs[0]\n        if len(aggs) > 1:\n            kernel_args = cuda_args(ret.shape[:-1])\n            if ret.ndim == 3:\n                cuda_row_max_n_in_place_3d[kernel_args](aggs[0], aggs[1])\n            else:\n                cuda_row_max_n_in_place_4d[kernel_args](aggs[0], aggs[1])\n        return ret\n\nclass _min_n_row_index(_max_n_or_min_n_row_index):\n    \"\"\"Min_n reduction operating on row index.\n\n    This is a private class as it is not intended to be used explicitly in\n    user code. It is primarily purpose is to support the use of ``first_n``\n    reductions using dask and/or CUDA.\n    \"\"\"\n\n    def _antialias_requires_2_stages(self):\n        return True\n\n    def _antialias_stage_2(self, self_intersect, array_module) -> tuple[AntialiasStage2]:\n        return (AntialiasStage2(AntialiasCombination.MIN, -1, n_reduction=True),)\n\n    @staticmethod\n    @ngjit\n    def _append(x, y, agg, field):\n        if field != -1:\n            n = agg.shape[2]\n            for i in range(n):\n                if agg[y, x, i] == -1 or field < agg[y, x, i]:\n                    shift_and_insert(agg[y, x], field, i)\n                    return i\n        return -1\n\n    @staticmethod\n    @ngjit\n    def _append_antialias(x, y, agg, field, aa_factor, prev_aa_factor):\n        if field != -1:\n            n = agg.shape[2]\n            for i in range(n):\n                if agg[y, x, i] == -1 or field < agg[y, x, i]:\n                    shift_and_insert(agg[y, x], field, i)\n                    return i\n        return -1\n\n    @staticmethod\n    @nb_cuda.jit(device=True)\n    def _append_cuda(x, y, agg, field):\n        if field != -1:\n            n = agg.shape[2]\n            for i in range(n):\n                if agg[y, x, i] == -1 or field < agg[y, x, i]:\n                    cuda_shift_and_insert(agg[y, x], field, i)\n                    return i\n        return -1\n\n    @staticmethod\n    def _combine(aggs):\n        ret = aggs[0]\n        if len(aggs) > 1:\n            if ret.ndim == 3:\n                row_min_n_in_place_3d(aggs[0], aggs[1])\n            else:\n                row_min_n_in_place_4d(aggs[0], aggs[1])\n        return ret\n\n    @staticmethod\n    def _combine_cuda(aggs):\n        ret = aggs[0]\n        if len(aggs) > 1:\n            kernel_args = cuda_args(ret.shape[:-1])\n            if ret.ndim == 3:\n                cuda_row_min_n_in_place_3d[kernel_args](aggs[0], aggs[1])\n            else:\n                cuda_row_min_n_in_place_4d[kernel_args](aggs[0], aggs[1])\n        return ret\n__all__ = list(set([_k for _k, _v in locals().items() if isinstance(_v, type) and (issubclass(_v, Reduction) or _v is summary) and (_v not in [Reduction, OptionalFieldReduction, FloatingReduction, m2])])) + ['category_modulo', 'category_binning']",
    "datashader/glyphs/line.py": "from __future__ import annotations\nimport math\nimport numpy as np\nfrom toolz import memoize\nfrom datashader.antialias import two_stage_agg\nfrom datashader.glyphs.points import _PointLike, _GeometryLike\nfrom datashader.utils import isnull, isreal, ngjit\nfrom numba import cuda\nimport numba.types as nb_types\ntry:\n    import cudf\n    import cupy as cp\n    from ..transfer_functions._cuda_utils import cuda_args\nexcept ImportError:\n    cudf = None\n    cp = None\n    cuda_args = None\ntry:\n    import spatialpandas\nexcept Exception:\n    spatialpandas = None\n\nclass _AntiAliasedLine:\n    \"\"\" Methods common to all lines. \"\"\"\n    _line_width = 0\n\n    def _build_extend(self, x_mapper, y_mapper, info, append, antialias_stage_2, antialias_stage_2_funcs):\n        return self._internal_build_extend(x_mapper, y_mapper, info, append, self._line_width, antialias_stage_2, antialias_stage_2_funcs)\n\ndef _line_internal_build_extend(x_mapper, y_mapper, append, line_width, antialias_stage_2, antialias_stage_2_funcs, expand_aggs_and_cols):\n    antialias = line_width > 0\n    map_onto_pixel = _build_map_onto_pixel_for_line(x_mapper, y_mapper, antialias)\n    overwrite, use_2_stage_agg = two_stage_agg(antialias_stage_2)\n    if not use_2_stage_agg:\n        antialias_stage_2_funcs = None\n    draw_segment = _build_draw_segment(append, map_onto_pixel, expand_aggs_and_cols, line_width, overwrite)\n    return (draw_segment, antialias_stage_2_funcs)\n\nclass LineAxis0(_PointLike, _AntiAliasedLine):\n    \"\"\"A line, with vertices defined by ``x`` and ``y``.\n\n    Parameters\n    ----------\n    x, y : str\n        Column names for the x and y coordinates of each vertex.\n    \"\"\"\n\n    @memoize\n    def _internal_build_extend(self, x_mapper, y_mapper, info, append, line_width, antialias_stage_2, antialias_stage_2_funcs):\n        expand_aggs_and_cols = self.expand_aggs_and_cols(append)\n        draw_segment, antialias_stage_2_funcs = _line_internal_build_extend(x_mapper, y_mapper, append, line_width, antialias_stage_2, antialias_stage_2_funcs, expand_aggs_and_cols)\n        extend_cpu, extend_cuda = _build_extend_line_axis0(draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs)\n        x_name = self.x\n        y_name = self.y\n\n        def extend(aggs, df, vt, bounds, plot_start=True):\n            sx, tx, sy, ty = vt\n            xmin, xmax, ymin, ymax = bounds\n            aggs_and_cols = aggs + info(df, aggs[0].shape[:2])\n            if cudf and isinstance(df, cudf.DataFrame):\n                xs = self.to_cupy_array(df, x_name)\n                ys = self.to_cupy_array(df, y_name)\n                do_extend = extend_cuda[cuda_args(xs.shape)]\n            else:\n                xs = df.loc[:, x_name].to_numpy()\n                ys = df.loc[:, y_name].to_numpy()\n                do_extend = extend_cpu\n            do_extend(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, plot_start, antialias_stage_2, *aggs_and_cols)\n        return extend\n\nclass LineAxis0Multi(_PointLike, _AntiAliasedLine):\n    \"\"\"\n    \"\"\"\n\n    def validate(self, in_dshape):\n        if not all([isreal(in_dshape.measure[str(xcol)]) for xcol in self.x]):\n            raise ValueError('x columns must be real')\n        elif not all([isreal(in_dshape.measure[str(ycol)]) for ycol in self.y]):\n            raise ValueError('y columns must be real')\n        if len(self.x) != len(self.y):\n            raise ValueError(f'x and y coordinate lengths do not match: {len(self.x)} != {len(self.y)}')\n\n    @property\n    def x_label(self):\n        return 'x'\n\n    @property\n    def y_label(self):\n        return 'y'\n\n    def required_columns(self):\n        return self.x + self.y\n\n    def compute_x_bounds(self, df):\n        bounds_list = [self._compute_bounds(df[x]) for x in self.x]\n        mins, maxes = zip(*bounds_list)\n        return self.maybe_expand_bounds((min(mins), max(maxes)))\n\n    def compute_y_bounds(self, df):\n        bounds_list = [self._compute_bounds(df[y]) for y in self.y]\n        mins, maxes = zip(*bounds_list)\n        return self.maybe_expand_bounds((min(mins), max(maxes)))\n\n    @memoize\n    def compute_bounds_dask(self, ddf):\n        r = ddf.map_partitions(lambda df: np.array([[np.nanmin([np.nanmin(df[c].values).item() for c in self.x]), np.nanmax([np.nanmax(df[c].values).item() for c in self.x]), np.nanmin([np.nanmin(df[c].values).item() for c in self.y]), np.nanmax([np.nanmax(df[c].values).item() for c in self.y])]])).compute()\n        x_extents = (np.nanmin(r[:, 0]), np.nanmax(r[:, 1]))\n        y_extents = (np.nanmin(r[:, 2]), np.nanmax(r[:, 3]))\n        return (self.maybe_expand_bounds(x_extents), self.maybe_expand_bounds(y_extents))\n\n    @memoize\n    def _internal_build_extend(self, x_mapper, y_mapper, info, append, line_width, antialias_stage_2, antialias_stage_2_funcs):\n        expand_aggs_and_cols = self.expand_aggs_and_cols(append)\n        draw_segment, antialias_stage_2_funcs = _line_internal_build_extend(x_mapper, y_mapper, append, line_width, antialias_stage_2, antialias_stage_2_funcs, expand_aggs_and_cols)\n        extend_cpu, extend_cuda = _build_extend_line_axis0_multi(draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs)\n        x_names = self.x\n        y_names = self.y\n\n        def extend(aggs, df, vt, bounds, plot_start=True):\n            sx, tx, sy, ty = vt\n            xmin, xmax, ymin, ymax = bounds\n            aggs_and_cols = aggs + info(df, aggs[0].shape[:2])\n            if cudf and isinstance(df, cudf.DataFrame):\n                xs = self.to_cupy_array(df, x_names)\n                ys = self.to_cupy_array(df, y_names)\n                do_extend = extend_cuda[cuda_args(xs.shape)]\n            else:\n                xs = df.loc[:, list(x_names)].to_numpy()\n                ys = df.loc[:, list(y_names)].to_numpy()\n                do_extend = extend_cpu\n            do_extend(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, plot_start, antialias_stage_2, *aggs_and_cols)\n        return extend\n\nclass LinesAxis1(_PointLike, _AntiAliasedLine):\n    \"\"\"A collection of lines (on line per row) with vertices defined\n    by the lists of columns in ``x`` and ``y``\n\n    Parameters\n    ----------\n    x, y : list\n        Lists of column names for the x and y coordinates\n    \"\"\"\n\n    def validate(self, in_dshape):\n        if not all([isreal(in_dshape.measure[str(xcol)]) for xcol in self.x]):\n            raise ValueError('x columns must be real')\n        elif not all([isreal(in_dshape.measure[str(ycol)]) for ycol in self.y]):\n            raise ValueError('y columns must be real')\n        unique_x_measures = set((in_dshape.measure[str(xcol)] for xcol in self.x))\n        if len(unique_x_measures) > 1:\n            raise ValueError('x columns must have the same data type')\n        unique_y_measures = set((in_dshape.measure[str(ycol)] for ycol in self.y))\n        if len(unique_y_measures) > 1:\n            raise ValueError('y columns must have the same data type')\n        if len(self.x) != len(self.y):\n            raise ValueError(f'x and y coordinate lengths do not match: {len(self.x)} != {len(self.y)}')\n\n    def required_columns(self):\n        return self.x + self.y\n\n    @property\n    def x_label(self):\n        return 'x'\n\n    @property\n    def y_label(self):\n        return 'y'\n\n    def compute_x_bounds(self, df):\n        xs = tuple((df[xlabel] for xlabel in self.x))\n        bounds_list = [self._compute_bounds(xcol) for xcol in xs]\n        mins, maxes = zip(*bounds_list)\n        return self.maybe_expand_bounds((min(mins), max(maxes)))\n\n    def compute_y_bounds(self, df):\n        ys = tuple((df[ylabel] for ylabel in self.y))\n        bounds_list = [self._compute_bounds(ycol) for ycol in ys]\n        mins, maxes = zip(*bounds_list)\n        return self.maybe_expand_bounds((min(mins), max(maxes)))\n\n    @memoize\n    def compute_bounds_dask(self, ddf):\n        r = ddf.map_partitions(lambda df: np.array([[np.nanmin([np.nanmin(df[c].values).item() for c in self.x]), np.nanmax([np.nanmax(df[c].values).item() for c in self.x]), np.nanmin([np.nanmin(df[c].values).item() for c in self.y]), np.nanmax([np.nanmax(df[c].values).item() for c in self.y])]])).compute()\n        x_extents = (np.nanmin(r[:, 0]), np.nanmax(r[:, 1]))\n        y_extents = (np.nanmin(r[:, 2]), np.nanmax(r[:, 3]))\n        return (self.maybe_expand_bounds(x_extents), self.maybe_expand_bounds(y_extents))\n\n    @memoize\n    def _internal_build_extend(self, x_mapper, y_mapper, info, append, line_width, antialias_stage_2, antialias_stage_2_funcs):\n        expand_aggs_and_cols = self.expand_aggs_and_cols(append)\n        draw_segment, antialias_stage_2_funcs = _line_internal_build_extend(x_mapper, y_mapper, append, line_width, antialias_stage_2, antialias_stage_2_funcs, expand_aggs_and_cols)\n        extend_cpu, extend_cuda = _build_extend_line_axis1_none_constant(draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs)\n        x_names = self.x\n        y_names = self.y\n\n        def extend(aggs, df, vt, bounds, plot_start=True):\n            sx, tx, sy, ty = vt\n            xmin, xmax, ymin, ymax = bounds\n            aggs_and_cols = aggs + info(df, aggs[0].shape[:2])\n            if cudf and isinstance(df, cudf.DataFrame):\n                xs = self.to_cupy_array(df, x_names)\n                ys = self.to_cupy_array(df, y_names)\n                do_extend = extend_cuda[cuda_args(xs.shape)]\n            else:\n                xs = df.loc[:, list(x_names)].to_numpy()\n                ys = df.loc[:, list(y_names)].to_numpy()\n                do_extend = extend_cpu\n            do_extend(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, *aggs_and_cols)\n        return extend\n\nclass LinesAxis1XConstant(LinesAxis1):\n    \"\"\"\n    \"\"\"\n\n    def validate(self, in_dshape):\n        if not all([isreal(in_dshape.measure[str(ycol)]) for ycol in self.y]):\n            raise ValueError('y columns must be real')\n        unique_y_measures = set((in_dshape.measure[str(ycol)] for ycol in self.y))\n        if len(unique_y_measures) > 1:\n            raise ValueError('y columns must have the same data type')\n        if len(self.x) != len(self.y):\n            raise ValueError(f'x and y coordinate lengths do not match: {len(self.x)} != {len(self.y)}')\n\n    def required_columns(self):\n        return self.y\n\n    def compute_x_bounds(self, *args):\n        x_min = np.nanmin(self.x)\n        x_max = np.nanmax(self.x)\n        return self.maybe_expand_bounds((x_min, x_max))\n\n    @memoize\n    def compute_bounds_dask(self, ddf):\n        r = ddf.map_partitions(lambda df: np.array([[np.nanmin([np.nanmin(df[c].values).item() for c in self.y]), np.nanmax([np.nanmax(df[c].values).item() for c in self.y])]])).compute()\n        y_extents = (np.nanmin(r[:, 0]), np.nanmax(r[:, 1]))\n        return (self.compute_x_bounds(), self.maybe_expand_bounds(y_extents))\n\n    @memoize\n    def _internal_build_extend(self, x_mapper, y_mapper, info, append, line_width, antialias_stage_2, antialias_stage_2_funcs):\n        expand_aggs_and_cols = self.expand_aggs_and_cols(append)\n        draw_segment, antialias_stage_2_funcs = _line_internal_build_extend(x_mapper, y_mapper, append, line_width, antialias_stage_2, antialias_stage_2_funcs, expand_aggs_and_cols)\n        extend_cpu, extend_cuda = _build_extend_line_axis1_x_constant(draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs)\n        x_values = self.x\n        y_names = self.y\n\n        def extend(aggs, df, vt, bounds, plot_start=True):\n            sx, tx, sy, ty = vt\n            xmin, xmax, ymin, ymax = bounds\n            aggs_and_cols = aggs + info(df, aggs[0].shape[:2])\n            if cudf and isinstance(df, cudf.DataFrame):\n                xs = cp.asarray(x_values)\n                ys = self.to_cupy_array(df, y_names)\n                do_extend = extend_cuda[cuda_args(ys.shape)]\n            else:\n                xs = x_values\n                ys = df.loc[:, list(y_names)].to_numpy()\n                do_extend = extend_cpu\n            do_extend(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, *aggs_and_cols)\n        return extend\n\nclass LinesAxis1YConstant(LinesAxis1):\n    \"\"\"\n    \"\"\"\n\n    def validate(self, in_dshape):\n        if not all([isreal(in_dshape.measure[str(xcol)]) for xcol in self.x]):\n            raise ValueError('x columns must be real')\n        unique_x_measures = set((in_dshape.measure[str(xcol)] for xcol in self.x))\n        if len(unique_x_measures) > 1:\n            raise ValueError('x columns must have the same data type')\n        if len(self.x) != len(self.y):\n            raise ValueError(f'x and y coordinate lengths do not match: {len(self.x)} != {len(self.y)}')\n\n    def required_columns(self):\n        return self.x\n\n    def compute_y_bounds(self, *args):\n        y_min = np.nanmin(self.y)\n        y_max = np.nanmax(self.y)\n        return self.maybe_expand_bounds((y_min, y_max))\n\n    @memoize\n    def compute_bounds_dask(self, ddf):\n        r = ddf.map_partitions(lambda df: np.array([[np.nanmin([np.nanmin(df[c].values).item() for c in self.x]), np.nanmax([np.nanmax(df[c].values).item() for c in self.x])]])).compute()\n        x_extents = (np.nanmin(r[:, 0]), np.nanmax(r[:, 1]))\n        return (self.maybe_expand_bounds(x_extents), self.compute_y_bounds())\n\n    @memoize\n    def _internal_build_extend(self, x_mapper, y_mapper, info, append, line_width, antialias_stage_2, antialias_stage_2_funcs):\n        expand_aggs_and_cols = self.expand_aggs_and_cols(append)\n        draw_segment, antialias_stage_2_funcs = _line_internal_build_extend(x_mapper, y_mapper, append, line_width, antialias_stage_2, antialias_stage_2_funcs, expand_aggs_and_cols)\n        extend_cpu, extend_cuda = _build_extend_line_axis1_y_constant(draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs)\n        x_names = self.x\n        y_values = self.y\n\n        def extend(aggs, df, vt, bounds, plot_start=True):\n            sx, tx, sy, ty = vt\n            xmin, xmax, ymin, ymax = bounds\n            aggs_and_cols = aggs + info(df, aggs[0].shape[:2])\n            if cudf and isinstance(df, cudf.DataFrame):\n                xs = self.to_cupy_array(df, x_names)\n                ys = cp.asarray(y_values)\n                do_extend = extend_cuda[cuda_args(xs.shape)]\n            else:\n                xs = df.loc[:, list(x_names)].to_numpy()\n                ys = y_values\n                do_extend = extend_cpu\n            do_extend(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, *aggs_and_cols)\n        return extend\n\nclass LinesAxis1Ragged(_PointLike, _AntiAliasedLine):\n\n    def validate(self, in_dshape):\n        try:\n            from datashader.datatypes import RaggedDtype\n        except ImportError:\n            RaggedDtype = type(None)\n        if not isinstance(in_dshape[str(self.x)], RaggedDtype):\n            raise ValueError('x must be a RaggedArray')\n        elif not isinstance(in_dshape[str(self.y)], RaggedDtype):\n            raise ValueError('y must be a RaggedArray')\n\n    def required_columns(self):\n        return (self.x,) + (self.y,)\n\n    def compute_x_bounds(self, df):\n        bounds = self._compute_bounds(df[self.x].array.flat_array)\n        return self.maybe_expand_bounds(bounds)\n\n    def compute_y_bounds(self, df):\n        bounds = self._compute_bounds(df[self.y].array.flat_array)\n        return self.maybe_expand_bounds(bounds)\n\n    @memoize\n    def compute_bounds_dask(self, ddf):\n        r = ddf.map_partitions(lambda df: np.array([[np.nanmin(df[self.x].array.flat_array).item(), np.nanmax(df[self.x].array.flat_array).item(), np.nanmin(df[self.y].array.flat_array).item(), np.nanmax(df[self.y].array.flat_array).item()]])).compute()\n        x_extents = (np.nanmin(r[:, 0]), np.nanmax(r[:, 1]))\n        y_extents = (np.nanmin(r[:, 2]), np.nanmax(r[:, 3]))\n        return (self.maybe_expand_bounds(x_extents), self.maybe_expand_bounds(y_extents))\n\n    @memoize\n    def _internal_build_extend(self, x_mapper, y_mapper, info, append, line_width, antialias_stage_2, antialias_stage_2_funcs):\n        expand_aggs_and_cols = self.expand_aggs_and_cols(append)\n        draw_segment, antialias_stage_2_funcs = _line_internal_build_extend(x_mapper, y_mapper, append, line_width, antialias_stage_2, antialias_stage_2_funcs, expand_aggs_and_cols)\n        extend_cpu = _build_extend_line_axis1_ragged(draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs)\n        x_name = self.x\n        y_name = self.y\n\n        def extend(aggs, df, vt, bounds, plot_start=True):\n            sx, tx, sy, ty = vt\n            xmin, xmax, ymin, ymax = bounds\n            xs = df[x_name].array\n            ys = df[y_name].array\n            aggs_and_cols = aggs + info(df, aggs[0].shape[:2])\n            extend_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, *aggs_and_cols)\n        return extend\n\nclass LineAxis1Geometry(_GeometryLike, _AntiAliasedLine):\n\n    @property\n    def geom_dtypes(self):\n        from spatialpandas.geometry import LineDtype, MultiLineDtype, RingDtype, PolygonDtype, MultiPolygonDtype\n        return (LineDtype, MultiLineDtype, RingDtype, PolygonDtype, MultiPolygonDtype)\n\n    @memoize\n    def _internal_build_extend(self, x_mapper, y_mapper, info, append, line_width, antialias_stage_2, antialias_stage_2_funcs):\n        from spatialpandas.geometry import PolygonArray, MultiPolygonArray, RingArray\n        expand_aggs_and_cols = self.expand_aggs_and_cols(append)\n        draw_segment, antialias_stage_2_funcs = _line_internal_build_extend(x_mapper, y_mapper, append, line_width, antialias_stage_2, antialias_stage_2_funcs, expand_aggs_and_cols)\n        perform_extend_cpu = _build_extend_line_axis1_geometry(draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs)\n        geometry_name = self.geometry\n\n        def extend(aggs, df, vt, bounds, plot_start=True):\n            sx, tx, sy, ty = vt\n            xmin, xmax, ymin, ymax = bounds\n            aggs_and_cols = aggs + info(df, aggs[0].shape[:2])\n            geom_array = df[geometry_name].array\n            if isinstance(geom_array, (PolygonArray, MultiPolygonArray)):\n                geom_array = geom_array.boundary\n                closed_rings = True\n            elif isinstance(geom_array, RingArray):\n                closed_rings = True\n            else:\n                closed_rings = False\n            perform_extend_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax, geom_array, closed_rings, antialias_stage_2, *aggs_and_cols)\n        return extend\n\nclass LineAxis1GeoPandas(_GeometryLike, _AntiAliasedLine):\n\n    @property\n    def geom_dtypes(self):\n        from geopandas.array import GeometryDtype\n        return (GeometryDtype,)\n\n    @memoize\n    def _internal_build_extend(self, x_mapper, y_mapper, info, append, line_width, antialias_stage_2, antialias_stage_2_funcs):\n        expand_aggs_and_cols = self.expand_aggs_and_cols(append)\n        draw_segment, antialias_stage_2_funcs = _line_internal_build_extend(x_mapper, y_mapper, append, line_width, antialias_stage_2, antialias_stage_2_funcs, expand_aggs_and_cols)\n        perform_extend_cpu = _build_extend_line_axis1_geopandas(draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs)\n        geometry_name = self.geometry\n\n        def extend(aggs, df, vt, bounds, plot_start=True):\n            sx, tx, sy, ty = vt\n            xmin, xmax, ymin, ymax = bounds\n            aggs_and_cols = aggs + info(df, aggs[0].shape[:2])\n            geom_array = df[geometry_name].array\n            perform_extend_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax, geom_array, antialias_stage_2, *aggs_and_cols)\n        return extend\n\nclass LinesXarrayCommonX(LinesAxis1):\n\n    def __init__(self, x, y, x_dim_index: int):\n        super().__init__(x, y)\n        self.x_dim_index = x_dim_index\n\n    def __hash__(self):\n        return hash((type(self), self.x_dim_index))\n\n    def compute_x_bounds(self, dataset):\n        bounds = self._compute_bounds(dataset[self.x])\n        return self.maybe_expand_bounds(bounds)\n\n    def compute_y_bounds(self, dataset):\n        bounds = self._compute_bounds(dataset[self.y])\n        return self.maybe_expand_bounds(bounds)\n\n    def compute_bounds_dask(self, xr_ds):\n        return (self.compute_x_bounds(xr_ds), self.compute_y_bounds(xr_ds))\n\n    def validate(self, in_dshape):\n        if not isreal(in_dshape.measure[str(self.x)]):\n            raise ValueError('x column must be real')\n        if not isreal(in_dshape.measure[str(self.y)]):\n            raise ValueError('y column must be real')\n\n    @memoize\n    def _internal_build_extend(self, x_mapper, y_mapper, info, append, line_width, antialias_stage_2, antialias_stage_2_funcs):\n        expand_aggs_and_cols = self.expand_aggs_and_cols(append)\n        draw_segment, antialias_stage_2_funcs = _line_internal_build_extend(x_mapper, y_mapper, append, line_width, antialias_stage_2, antialias_stage_2_funcs, expand_aggs_and_cols)\n        swap_dims = self.x_dim_index == 0\n        extend_cpu, extend_cuda = _build_extend_line_axis1_x_constant(draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs, swap_dims)\n        x_name = self.x\n        y_name = self.y\n\n        def extend(aggs, df, vt, bounds, plot_start=True):\n            sx, tx, sy, ty = vt\n            xmin, xmax, ymin, ymax = bounds\n            aggs_and_cols = aggs + info(df, aggs[0].shape[:2])\n            if cudf and isinstance(df, cudf.DataFrame):\n                xs = cp.asarray(df[x_name])\n                ys = cp.asarray(df[y_name])\n                do_extend = extend_cuda[cuda_args(ys.shape)]\n            elif cp and isinstance(df[y_name].data, cp.ndarray):\n                xs = cp.asarray(df[x_name])\n                ys = df[y_name].data\n                shape = ys.shape[::-1] if swap_dims else ys.shape\n                do_extend = extend_cuda[cuda_args(shape)]\n            else:\n                xs = df[x_name].to_numpy()\n                ys = df[y_name].to_numpy()\n                do_extend = extend_cpu\n            do_extend(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, *aggs_and_cols)\n        return extend\n\ndef _build_map_onto_pixel_for_line(x_mapper, y_mapper, want_antialias=False):\n\n    @ngjit\n    def map_onto_pixel_snap(sx, tx, sy, ty, xmin, xmax, ymin, ymax, x, y):\n        \"\"\"Map points onto pixel grid.\n\n        Points falling on upper bound are mapped into previous bin.\n\n        If the line has been clipped, x and y will have been\n        computed to lie on the bounds; we compare point and bounds\n        in integer space to avoid fp error. In contrast, with\n        auto-ranging, a point on the bounds will be the same\n        floating point number as the bound, so comparison in fp\n        representation of continuous space or in integer space\n        doesn't change anything.\n        \"\"\"\n        xx = int(x_mapper(x) * sx + tx)\n        yy = int(y_mapper(y) * sy + ty)\n        xxmax = round(x_mapper(xmax) * sx + tx)\n        yymax = round(y_mapper(ymax) * sy + ty)\n        return (xx - 1 if xx == xxmax else xx, yy - 1 if yy == yymax else yy)\n\n    @ngjit\n    def map_onto_pixel_no_snap(sx, tx, sy, ty, xmin, xmax, ymin, ymax, x, y):\n        xx = x_mapper(x) * sx + tx - 0.5\n        yy = y_mapper(y) * sy + ty - 0.5\n        return (xx, yy)\n    if want_antialias:\n        return map_onto_pixel_no_snap\n    else:\n        return map_onto_pixel_snap\n\n@ngjit\ndef _liang_barsky(xmin, xmax, ymin, ymax, x0, x1, y0, y1, skip):\n    \"\"\" An implementation of the Liang-Barsky line clipping algorithm.\n\n    https://en.wikipedia.org/wiki/Liang%E2%80%93Barsky_algorithm\n\n    \"\"\"\n    if x0 < xmin and x1 < xmin:\n        skip = True\n    elif x0 > xmax and x1 > xmax:\n        skip = True\n    elif y0 < ymin and y1 < ymin:\n        skip = True\n    elif y0 > ymax and y1 > ymax:\n        skip = True\n    t0, t1 = (0, 1)\n    dx1 = x1 - x0\n    t0, t1, accept = _clipt(-dx1, x0 - xmin, t0, t1)\n    if not accept:\n        skip = True\n    t0, t1, accept = _clipt(dx1, xmax - x0, t0, t1)\n    if not accept:\n        skip = True\n    dy1 = y1 - y0\n    t0, t1, accept = _clipt(-dy1, y0 - ymin, t0, t1)\n    if not accept:\n        skip = True\n    t0, t1, accept = _clipt(dy1, ymax - y0, t0, t1)\n    if not accept:\n        skip = True\n    if t1 < 1:\n        clipped_end = True\n        x1 = x0 + t1 * dx1\n        y1 = y0 + t1 * dy1\n    else:\n        clipped_end = False\n    if t0 > 0:\n        clipped_start = True\n        x0 = x0 + t0 * dx1\n        y0 = y0 + t0 * dy1\n    else:\n        clipped_start = False\n    return (x0, x1, y0, y1, skip, clipped_start, clipped_end)\n\n@ngjit\ndef _clipt(p, q, t0, t1):\n    accept = True\n    if p < 0 and q < 0:\n        r = q / p\n        if r > t1:\n            accept = False\n        elif r > t0:\n            t0 = r\n    elif p > 0 and q < p:\n        r = q / p\n        if r < t0:\n            accept = False\n        elif r < t1:\n            t1 = r\n    elif q < 0:\n        accept = False\n    return (t0, t1, accept)\n\n@ngjit\ndef _clamp(x, low, high):\n    return max(low, min(x, high))\n\n@ngjit\ndef _linearstep(edge0, edge1, x):\n    t = _clamp((x - edge0) / (edge1 - edge0), 0.0, 1.0)\n    return t\n\n@ngjit\ndef _x_intercept(y, cx0, cy0, cx1, cy1):\n    if cy0 == cy1:\n        return cx1\n    frac = (y - cy0) / (cy1 - cy0)\n    return cx0 + frac * (cx1 - cx0)\n\ndef _build_full_antialias(expand_aggs_and_cols):\n    \"\"\"Specialize antialiased line drawing algorithm for a given append/axis combination\"\"\"\n\n    @ngjit\n    @expand_aggs_and_cols\n    def _full_antialias(line_width, overwrite, i, x0, x1, y0, y1, segment_start, segment_end, xm, ym, append, nx, ny, buffer, *aggs_and_cols):\n        \"\"\"Draw an antialiased line segment.\n\n        If overwrite=True can overwrite each pixel multiple times because\n        using max for the overwriting.  If False can only write each pixel\n        once per segment and its previous segment.\n        Argument xm, ym are only valid if overwrite and segment_start are False.\n        \"\"\"\n        if x0 == x1 and y0 == y1:\n            return\n        flip_xy = abs(x0 - x1) < abs(y0 - y1)\n        if flip_xy:\n            x0, y0 = (y0, x0)\n            x1, y1 = (y1, x1)\n            xm, ym = (ym, xm)\n        scale = 1.0\n        if line_width < 1.0:\n            scale *= line_width\n            line_width = 1.0\n        aa = 1.0\n        halfwidth = 0.5 * (line_width + aa)\n        flip_order = y1 < y0 or (y1 == y0 and x1 < x0)\n        alongx = float(x1 - x0)\n        alongy = float(y1 - y0)\n        length = math.sqrt(alongx ** 2 + alongy ** 2)\n        alongx /= length\n        alongy /= length\n        rightx = alongy\n        righty = -alongx\n        if flip_order:\n            buffer[0] = x1 - halfwidth * (rightx - alongx)\n            buffer[1] = x1 - halfwidth * (-rightx - alongx)\n            buffer[2] = x0 - halfwidth * (-rightx + alongx)\n            buffer[3] = x0 - halfwidth * (rightx + alongx)\n            buffer[4] = y1 - halfwidth * (righty - alongy)\n            buffer[5] = y1 - halfwidth * (-righty - alongy)\n            buffer[6] = y0 - halfwidth * (-righty + alongy)\n            buffer[7] = y0 - halfwidth * (righty + alongy)\n        else:\n            buffer[0] = x0 + halfwidth * (rightx - alongx)\n            buffer[1] = x0 + halfwidth * (-rightx - alongx)\n            buffer[2] = x1 + halfwidth * (-rightx + alongx)\n            buffer[3] = x1 + halfwidth * (rightx + alongx)\n            buffer[4] = y0 + halfwidth * (righty - alongy)\n            buffer[5] = y0 + halfwidth * (-righty - alongy)\n            buffer[6] = y1 + halfwidth * (-righty + alongy)\n            buffer[7] = y1 + halfwidth * (righty + alongy)\n        xmax = nx - 1\n        ymax = ny - 1\n        if flip_xy:\n            xmax, ymax = (ymax, xmax)\n        if flip_order:\n            lowindex = 0 if x0 > x1 else 1\n        else:\n            lowindex = 0 if x1 > x0 else 1\n        if not overwrite and (not segment_start):\n            prev_alongx = x0 - xm\n            prev_alongy = y0 - ym\n            prev_length = math.sqrt(prev_alongx ** 2 + prev_alongy ** 2)\n            if prev_length > 0.0:\n                prev_alongx /= prev_length\n                prev_alongy /= prev_length\n                prev_rightx = prev_alongy\n                prev_righty = -prev_alongx\n            else:\n                overwrite = True\n        ystart = _clamp(math.ceil(buffer[4 + lowindex]), 0, ymax)\n        yend = _clamp(math.floor(buffer[4 + (lowindex + 2) % 4]), 0, ymax)\n        ll = lowindex\n        lu = (ll + 1) % 4\n        rl = lowindex\n        ru = (rl + 3) % 4\n        for y in range(ystart, yend + 1):\n            if ll == lowindex and y > buffer[4 + lu]:\n                ll = lu\n                lu = (ll + 1) % 4\n            if rl == lowindex and y > buffer[4 + ru]:\n                rl = ru\n                ru = (rl + 3) % 4\n            xleft = _clamp(math.ceil(_x_intercept(y, buffer[ll], buffer[4 + ll], buffer[lu], buffer[4 + lu])), 0, xmax)\n            xright = _clamp(math.floor(_x_intercept(y, buffer[rl], buffer[4 + rl], buffer[ru], buffer[4 + ru])), 0, xmax)\n            for x in range(xleft, xright + 1):\n                along = (x - x0) * alongx + (y - y0) * alongy\n                prev_correction = False\n                if along < 0.0:\n                    if overwrite or segment_start or (x - x0) * prev_alongx + (y - y0) * prev_alongy > 0.0:\n                        distance = math.sqrt((x - x0) ** 2 + (y - y0) ** 2)\n                    else:\n                        continue\n                elif along > length:\n                    if overwrite or segment_end:\n                        distance = math.sqrt((x - x1) ** 2 + (y - y1) ** 2)\n                    else:\n                        continue\n                else:\n                    distance = abs((x - x0) * rightx + (y - y0) * righty)\n                    if not overwrite and (not segment_start) and (-prev_length <= (x - x0) * prev_alongx + (y - y0) * prev_alongy <= 0.0) and (abs((x - x0) * prev_rightx + (y - y0) * prev_righty) <= halfwidth):\n                        prev_correction = True\n                value = 1.0 - _linearstep(0.5 * (line_width - aa), halfwidth, distance)\n                value *= scale\n                prev_value = 0.0\n                if prev_correction:\n                    prev_distance = abs((x - x0) * prev_rightx + (y - y0) * prev_righty)\n                    prev_value = 1.0 - _linearstep(0.5 * (line_width - aa), halfwidth, prev_distance)\n                    prev_value *= scale\n                    if value <= prev_value:\n                        value = 0.0\n                if value > 0.0:\n                    xx, yy = (y, x) if flip_xy else (x, y)\n                    append(i, xx, yy, value, prev_value, *aggs_and_cols)\n    return _full_antialias\n\ndef _build_bresenham(expand_aggs_and_cols):\n    \"\"\"Specialize a bresenham kernel for a given append/axis combination\"\"\"\n\n    @ngjit\n    @expand_aggs_and_cols\n    def _bresenham(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax, segment_start, x0, x1, y0, y1, clipped, append, *aggs_and_cols):\n        \"\"\"Draw a line segment using Bresenham's algorithm\n        This method plots a line segment with integer coordinates onto a pixel\n        grid.\n        \"\"\"\n        dx = x1 - x0\n        ix = (dx > 0) - (dx < 0)\n        dx = abs(dx) * 2\n        dy = y1 - y0\n        iy = (dy > 0) - (dy < 0)\n        dy = abs(dy) * 2\n        if not clipped and (not dx | dy):\n            append(i, x0, y0, *aggs_and_cols)\n            return\n        if segment_start:\n            append(i, x0, y0, *aggs_and_cols)\n        if dx >= dy:\n            error = 2 * dy - dx\n            while x0 != x1:\n                if error >= 0 and (error or ix > 0):\n                    error -= 2 * dx\n                    y0 += iy\n                error += 2 * dy\n                x0 += ix\n                append(i, x0, y0, *aggs_and_cols)\n        else:\n            error = 2 * dx - dy\n            while y0 != y1:\n                if error >= 0 and (error or iy > 0):\n                    error -= 2 * dy\n                    x0 += ix\n                error += 2 * dx\n                y0 += iy\n                append(i, x0, y0, *aggs_and_cols)\n    return _bresenham\n\ndef _build_draw_segment(append, map_onto_pixel, expand_aggs_and_cols, line_width, overwrite):\n    \"\"\"Specialize a line plotting kernel for a given append/axis combination\"\"\"\n    if line_width > 0.0:\n        _bresenham = None\n        _full_antialias = _build_full_antialias(expand_aggs_and_cols)\n    else:\n        _bresenham = _build_bresenham(expand_aggs_and_cols)\n        _full_antialias = None\n\n    @ngjit\n    @expand_aggs_and_cols\n    def draw_segment(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax, segment_start, segment_end, x0, x1, y0, y1, xm, ym, buffer, *aggs_and_cols):\n        skip = False\n        if isnull(x0) or isnull(y0) or isnull(x1) or isnull(y1):\n            skip = True\n        x0_1, x1_1, y0_1, y1_1, skip, clipped_start, clipped_end = _liang_barsky(xmin, xmax, ymin, ymax, x0, x1, y0, y1, skip)\n        if not skip:\n            clipped = clipped_start or clipped_end\n            segment_start = segment_start or clipped_start\n            x0_2, y0_2 = map_onto_pixel(sx, tx, sy, ty, xmin, xmax, ymin, ymax, x0_1, y0_1)\n            x1_2, y1_2 = map_onto_pixel(sx, tx, sy, ty, xmin, xmax, ymin, ymax, x1_1, y1_1)\n            if line_width > 0.0:\n                if segment_start:\n                    xm_2 = ym_2 = 0.0\n                else:\n                    xm_2, ym_2 = map_onto_pixel(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xm, ym)\n                nx = round((xmax - xmin) * sx)\n                ny = round((ymax - ymin) * sy)\n                _full_antialias(line_width, overwrite, i, x0_2, x1_2, y0_2, y1_2, segment_start, segment_end, xm_2, ym_2, append, nx, ny, buffer, *aggs_and_cols)\n            else:\n                _bresenham(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax, segment_start, x0_2, x1_2, y0_2, y1_2, clipped, append, *aggs_and_cols)\n    return draw_segment\n\ndef _build_extend_line_axis0(draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs):\n    use_2_stage_agg = antialias_stage_2_funcs is not None\n\n    @ngjit\n    @expand_aggs_and_cols\n    def perform_extend_line(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax, plot_start, xs, ys, buffer, *aggs_and_cols):\n        x0 = xs[i]\n        y0 = ys[i]\n        x1 = xs[i + 1]\n        y1 = ys[i + 1]\n        segment_start = plot_start if i == 0 else isnull(xs[i - 1]) or isnull(ys[i - 1])\n        segment_end = i == len(xs) - 2 or isnull(xs[i + 2]) or isnull(ys[i + 2])\n        if segment_start or use_2_stage_agg:\n            xm = 0.0\n            ym = 0.0\n        else:\n            xm = xs[i - 1]\n            ym = ys[i - 1]\n        draw_segment(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax, segment_start, segment_end, x0, x1, y0, y1, xm, ym, buffer, *aggs_and_cols)\n\n    @ngjit\n    @expand_aggs_and_cols\n    def extend_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, plot_start, antialias_stage_2, *aggs_and_cols):\n        \"\"\"Aggregate along a line formed by ``xs`` and ``ys``\"\"\"\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n        nrows = xs.shape[0]\n        for i in range(nrows - 1):\n            perform_extend_line(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax, plot_start, xs, ys, buffer, *aggs_and_cols)\n\n    @cuda.jit\n    @expand_aggs_and_cols\n    def extend_cuda(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, plot_start, antialias_stage_2, *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = cuda.local.array(8, nb_types.float64) if antialias else None\n        i = cuda.grid(1)\n        if i < xs.shape[0] - 1:\n            perform_extend_line(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax, plot_start, xs, ys, buffer, *aggs_and_cols)\n    return (extend_cpu, extend_cuda)\n\ndef _build_extend_line_axis0_multi(draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs):\n    if antialias_stage_2_funcs is not None:\n        aa_stage_2_accumulate, aa_stage_2_clear, aa_stage_2_copy_back = antialias_stage_2_funcs\n    use_2_stage_agg = antialias_stage_2_funcs is not None\n\n    @ngjit\n    @expand_aggs_and_cols\n    def perform_extend_line(i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, plot_start, xs, ys, buffer, *aggs_and_cols):\n        x0 = xs[i, j]\n        y0 = ys[i, j]\n        x1 = xs[i + 1, j]\n        y1 = ys[i + 1, j]\n        segment_start = plot_start if i == 0 else isnull(xs[i - 1, j]) or isnull(ys[i - 1, j])\n        segment_end = i == len(xs) - 2 or isnull(xs[i + 2, j]) or isnull(ys[i + 2, j])\n        if segment_start or use_2_stage_agg:\n            xm = 0.0\n            ym = 0.0\n        else:\n            xm = xs[i - 1, j]\n            ym = ys[i - 1, j]\n        draw_segment(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax, segment_start, segment_end, x0, x1, y0, y1, xm, ym, buffer, *aggs_and_cols)\n\n    @ngjit\n    @expand_aggs_and_cols\n    def extend_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, plot_start, antialias_stage_2, *aggs_and_cols):\n        \"\"\"Aggregate along a line formed by ``xs`` and ``ys``\"\"\"\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n        nrows, ncols = xs.shape\n        for j in range(ncols):\n            for i in range(nrows - 1):\n                perform_extend_line(i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, plot_start, xs, ys, buffer, *aggs_and_cols)\n\n    def extend_cpu_antialias_2agg(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, plot_start, antialias_stage_2, *aggs_and_cols):\n        \"\"\"Aggregate along a line formed by ``xs`` and ``ys``\"\"\"\n        n_aggs = len(antialias_stage_2[0])\n        aggs_and_accums = tuple(((agg, agg.copy()) for agg in aggs_and_cols[:n_aggs]))\n        cpu_antialias_2agg_impl(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, plot_start, antialias_stage_2, aggs_and_accums, *aggs_and_cols)\n\n    @ngjit\n    @expand_aggs_and_cols\n    def cpu_antialias_2agg_impl(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, plot_start, antialias_stage_2, aggs_and_accums, *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n        nrows, ncols = xs.shape\n        for j in range(ncols):\n            for i in range(nrows - 1):\n                perform_extend_line(i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, plot_start, xs, ys, buffer, *aggs_and_cols)\n            if ncols == 1:\n                return\n            aa_stage_2_accumulate(aggs_and_accums, j == 0)\n            if j < ncols - 1:\n                aa_stage_2_clear(aggs_and_accums)\n        aa_stage_2_copy_back(aggs_and_accums)\n\n    @cuda.jit\n    @expand_aggs_and_cols\n    def extend_cuda(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, plot_start, antialias_stage_2, *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = cuda.local.array(8, nb_types.float64) if antialias else None\n        i, j = cuda.grid(2)\n        if i < xs.shape[0] - 1 and j < xs.shape[1]:\n            perform_extend_line(i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, plot_start, xs, ys, buffer, *aggs_and_cols)\n    if use_2_stage_agg:\n        return (extend_cpu_antialias_2agg, extend_cuda)\n    else:\n        return (extend_cpu, extend_cuda)\n\ndef _build_extend_line_axis1_none_constant(draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs):\n    if antialias_stage_2_funcs is not None:\n        aa_stage_2_accumulate, aa_stage_2_clear, aa_stage_2_copy_back = antialias_stage_2_funcs\n    use_2_stage_agg = antialias_stage_2_funcs is not None\n\n    @ngjit\n    @expand_aggs_and_cols\n    def perform_extend_line(i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, buffer, *aggs_and_cols):\n        x0 = xs[i, j]\n        y0 = ys[i, j]\n        x1 = xs[i, j + 1]\n        y1 = ys[i, j + 1]\n        segment_start = j == 0 or isnull(xs[i, j - 1]) or isnull(ys[i, j - 1])\n        segment_end = j == xs.shape[1] - 2 or isnull(xs[i, j + 2]) or isnull(ys[i, j + 2])\n        if segment_start or use_2_stage_agg:\n            xm = 0.0\n            ym = 0.0\n        else:\n            xm = xs[i, j - 1]\n            ym = ys[i, j - 1]\n        draw_segment(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax, segment_start, segment_end, x0, x1, y0, y1, xm, ym, buffer, *aggs_and_cols)\n\n    @ngjit\n    @expand_aggs_and_cols\n    def extend_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n        ncols = xs.shape[1]\n        for i in range(xs.shape[0]):\n            for j in range(ncols - 1):\n                perform_extend_line(i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, buffer, *aggs_and_cols)\n\n    def extend_cpu_antialias_2agg(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, *aggs_and_cols):\n        n_aggs = len(antialias_stage_2[0])\n        aggs_and_accums = tuple(((agg, agg.copy()) for agg in aggs_and_cols[:n_aggs]))\n        cpu_antialias_2agg_impl(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, aggs_and_accums, *aggs_and_cols)\n\n    @ngjit\n    @expand_aggs_and_cols\n    def cpu_antialias_2agg_impl(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, aggs_and_accums, *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n        ncols = xs.shape[1]\n        for i in range(xs.shape[0]):\n            for j in range(ncols - 1):\n                perform_extend_line(i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, buffer, *aggs_and_cols)\n            if xs.shape[0] == 1:\n                return\n            aa_stage_2_accumulate(aggs_and_accums, i == 0)\n            if i < xs.shape[0] - 1:\n                aa_stage_2_clear(aggs_and_accums)\n        aa_stage_2_copy_back(aggs_and_accums)\n\n    @cuda.jit\n    @expand_aggs_and_cols\n    def extend_cuda(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = cuda.local.array(8, nb_types.float64) if antialias else None\n        i, j = cuda.grid(2)\n        if i < xs.shape[0] and j < xs.shape[1] - 1:\n            perform_extend_line(i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, buffer, *aggs_and_cols)\n    if use_2_stage_agg:\n        return (extend_cpu_antialias_2agg, extend_cuda)\n    else:\n        return (extend_cpu, extend_cuda)\n\ndef _build_extend_line_axis1_x_constant(draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs, swap_dims: bool=False):\n    if antialias_stage_2_funcs is not None:\n        aa_stage_2_accumulate, aa_stage_2_clear, aa_stage_2_copy_back = antialias_stage_2_funcs\n    use_2_stage_agg = antialias_stage_2_funcs is not None\n\n    @ngjit\n    @expand_aggs_and_cols\n    def perform_extend_line(i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, buffer, *aggs_and_cols):\n        x0 = xs[j]\n        x1 = xs[j + 1]\n        if swap_dims:\n            y0 = ys[j, i]\n            y1 = ys[j + 1, i]\n            segment_start = j == 0 or isnull(xs[j - 1]) or isnull(ys[j - 1, i])\n            segment_end = j == len(xs) - 2 or isnull(xs[j + 2]) or isnull(ys[j + 2, i])\n        else:\n            y0 = ys[i, j]\n            y1 = ys[i, j + 1]\n            segment_start = j == 0 or isnull(xs[j - 1]) or isnull(ys[i, j - 1])\n            segment_end = j == len(xs) - 2 or isnull(xs[j + 2]) or isnull(ys[i, j + 2])\n        if segment_start or use_2_stage_agg:\n            xm = 0.0\n            ym = 0.0\n        else:\n            xm = xs[j - 1]\n            ym = ys[j - 1, i] if swap_dims else ys[i, j - 1]\n        draw_segment(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax, segment_start, segment_end, x0, x1, y0, y1, xm, ym, buffer, *aggs_and_cols)\n\n    @ngjit\n    @expand_aggs_and_cols\n    def extend_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n        ncols, nrows = ys.shape if swap_dims else ys.shape[::-1]\n        for i in range(nrows):\n            for j in range(ncols - 1):\n                perform_extend_line(i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, buffer, *aggs_and_cols)\n\n    def extend_cpu_antialias_2agg(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, *aggs_and_cols):\n        n_aggs = len(antialias_stage_2[0])\n        aggs_and_accums = tuple(((agg, agg.copy()) for agg in aggs_and_cols[:n_aggs]))\n        cpu_antialias_2agg_impl(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, aggs_and_accums, *aggs_and_cols)\n\n    @ngjit\n    @expand_aggs_and_cols\n    def cpu_antialias_2agg_impl(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, aggs_and_accums, *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n        ncols = ys.shape[1]\n        for i in range(ys.shape[0]):\n            for j in range(ncols - 1):\n                perform_extend_line(i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, buffer, *aggs_and_cols)\n            if ys.shape[0] == 1:\n                return\n            aa_stage_2_accumulate(aggs_and_accums, i == 0)\n            if i < ys.shape[0] - 1:\n                aa_stage_2_clear(aggs_and_accums)\n        aa_stage_2_copy_back(aggs_and_accums)\n\n    @cuda.jit\n    @expand_aggs_and_cols\n    def extend_cuda(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = cuda.local.array(8, nb_types.float64) if antialias else None\n        i, j = cuda.grid(2)\n        ncols, nrows = ys.shape if swap_dims else ys.shape[::-1]\n        if i < nrows and j < ncols - 1:\n            perform_extend_line(i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, buffer, *aggs_and_cols)\n    if use_2_stage_agg:\n        return (extend_cpu_antialias_2agg, extend_cuda)\n    else:\n        return (extend_cpu, extend_cuda)\n\ndef _build_extend_line_axis1_y_constant(draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs):\n    if antialias_stage_2_funcs is not None:\n        aa_stage_2_accumulate, aa_stage_2_clear, aa_stage_2_copy_back = antialias_stage_2_funcs\n    use_2_stage_agg = antialias_stage_2_funcs is not None\n\n    @ngjit\n    @expand_aggs_and_cols\n    def perform_extend_line(i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, buffer, *aggs_and_cols):\n        x0 = xs[i, j]\n        y0 = ys[j]\n        x1 = xs[i, j + 1]\n        y1 = ys[j + 1]\n        segment_start = j == 0 or isnull(xs[i, j - 1]) or isnull(ys[j - 1])\n        segment_end = j == len(ys) - 2 or isnull(xs[i, j + 2]) or isnull(ys[j + 2])\n        if segment_start or use_2_stage_agg:\n            xm = 0.0\n            ym = 0.0\n        else:\n            xm = xs[i, j - 1]\n            ym = ys[j - 1]\n        draw_segment(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax, segment_start, segment_end, x0, x1, y0, y1, xm, ym, buffer, *aggs_and_cols)\n\n    @ngjit\n    @expand_aggs_and_cols\n    def extend_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n        ncols = xs.shape[1]\n        for i in range(xs.shape[0]):\n            for j in range(ncols - 1):\n                perform_extend_line(i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, buffer, *aggs_and_cols)\n\n    def extend_cpu_antialias_2agg(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, *aggs_and_cols):\n        n_aggs = len(antialias_stage_2[0])\n        aggs_and_accums = tuple(((agg, agg.copy()) for agg in aggs_and_cols[:n_aggs]))\n        cpu_antialias_2agg_impl(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, aggs_and_accums, *aggs_and_cols)\n\n    @ngjit\n    @expand_aggs_and_cols\n    def cpu_antialias_2agg_impl(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, aggs_and_accums, *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n        ncols = xs.shape[1]\n        for i in range(xs.shape[0]):\n            for j in range(ncols - 1):\n                perform_extend_line(i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, buffer, *aggs_and_cols)\n            if xs.shape[0] == 1:\n                return\n            aa_stage_2_accumulate(aggs_and_accums, i == 0)\n            if i < xs.shape[0] - 1:\n                aa_stage_2_clear(aggs_and_accums)\n        aa_stage_2_copy_back(aggs_and_accums)\n\n    @cuda.jit\n    @expand_aggs_and_cols\n    def extend_cuda(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = cuda.local.array(8, nb_types.float64) if antialias else None\n        i, j = cuda.grid(2)\n        if i < xs.shape[0] and j < xs.shape[1] - 1:\n            perform_extend_line(i, j, sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, buffer, *aggs_and_cols)\n    if use_2_stage_agg:\n        return (extend_cpu_antialias_2agg, extend_cuda)\n    else:\n        return (extend_cpu, extend_cuda)\n\ndef _build_extend_line_axis1_ragged(draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs):\n    if antialias_stage_2_funcs is not None:\n        aa_stage_2_accumulate, aa_stage_2_clear, aa_stage_2_copy_back = antialias_stage_2_funcs\n    use_2_stage_agg = antialias_stage_2_funcs is not None\n\n    def extend_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, *aggs_and_cols):\n        x_start_i = xs.start_indices\n        x_flat = xs.flat_array\n        y_start_i = ys.start_indices\n        y_flat = ys.flat_array\n        extend_cpu_numba(sx, tx, sy, ty, xmin, xmax, ymin, ymax, x_start_i, x_flat, y_start_i, y_flat, antialias_stage_2, *aggs_and_cols)\n\n    @ngjit\n    @expand_aggs_and_cols\n    def extend_cpu_numba(sx, tx, sy, ty, xmin, xmax, ymin, ymax, x_start_i, x_flat, y_start_i, y_flat, antialias_stage_2, *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n        nrows = len(x_start_i)\n        x_flat_len = len(x_flat)\n        y_flat_len = len(y_flat)\n        for i in range(nrows):\n            x_start_index = x_start_i[i]\n            x_stop_index = x_start_i[i + 1] if i < nrows - 1 else x_flat_len\n            y_start_index = y_start_i[i]\n            y_stop_index = y_start_i[i + 1] if i < nrows - 1 else y_flat_len\n            segment_len = min(x_stop_index - x_start_index, y_stop_index - y_start_index)\n            for j in range(segment_len - 1):\n                x0 = x_flat[x_start_index + j]\n                y0 = y_flat[y_start_index + j]\n                x1 = x_flat[x_start_index + j + 1]\n                y1 = y_flat[y_start_index + j + 1]\n                segment_start = j == 0 or isnull(x_flat[x_start_index + j - 1]) or isnull(y_flat[y_start_index + j - 1])\n                segment_end = j == segment_len - 2 or isnull(x_flat[x_start_index + j + 2]) or isnull(y_flat[y_start_index + j + 2])\n                if segment_start or use_2_stage_agg:\n                    xm = 0.0\n                    ym = 0.0\n                else:\n                    xm = x_flat[x_start_index + j - 1]\n                    ym = y_flat[y_start_index + j - 1]\n                draw_segment(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax, segment_start, segment_end, x0, x1, y0, y1, xm, ym, buffer, *aggs_and_cols)\n\n    def extend_cpu_antialias_2agg(sx, tx, sy, ty, xmin, xmax, ymin, ymax, xs, ys, antialias_stage_2, *aggs_and_cols):\n        x_start_i = xs.start_indices\n        x_flat = xs.flat_array\n        y_start_i = ys.start_indices\n        y_flat = ys.flat_array\n        n_aggs = len(antialias_stage_2[0])\n        aggs_and_accums = tuple(((agg, agg.copy()) for agg in aggs_and_cols[:n_aggs]))\n        extend_cpu_numba_antialias_2agg(sx, tx, sy, ty, xmin, xmax, ymin, ymax, x_start_i, x_flat, y_start_i, y_flat, antialias_stage_2, aggs_and_accums, *aggs_and_cols)\n\n    @ngjit\n    @expand_aggs_and_cols\n    def extend_cpu_numba_antialias_2agg(sx, tx, sy, ty, xmin, xmax, ymin, ymax, x_start_i, x_flat, y_start_i, y_flat, antialias_stage_2, aggs_and_accums, *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n        nrows = len(x_start_i)\n        x_flat_len = len(x_flat)\n        y_flat_len = len(y_flat)\n        for i in range(nrows):\n            x_start_index = x_start_i[i]\n            x_stop_index = x_start_i[i + 1] if i < nrows - 1 else x_flat_len\n            y_start_index = y_start_i[i]\n            y_stop_index = y_start_i[i + 1] if i < nrows - 1 else y_flat_len\n            segment_len = min(x_stop_index - x_start_index, y_stop_index - y_start_index)\n            for j in range(segment_len - 1):\n                x0 = x_flat[x_start_index + j]\n                y0 = y_flat[y_start_index + j]\n                x1 = x_flat[x_start_index + j + 1]\n                y1 = y_flat[y_start_index + j + 1]\n                segment_start = j == 0 or isnull(x_flat[x_start_index + j - 1]) or isnull(y_flat[y_start_index + j - 1])\n                segment_end = j == segment_len - 2 or isnull(x_flat[x_start_index + j + 2]) or isnull(y_flat[y_start_index + j + 2])\n                draw_segment(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax, segment_start, segment_end, x0, x1, y0, y1, 0.0, 0.0, buffer, *aggs_and_cols)\n            if nrows == 1:\n                return\n            aa_stage_2_accumulate(aggs_and_accums, i == 0)\n            if i < nrows - 1:\n                aa_stage_2_clear(aggs_and_accums)\n        aa_stage_2_copy_back(aggs_and_accums)\n    if use_2_stage_agg:\n        return extend_cpu_antialias_2agg\n    else:\n        return extend_cpu\n\ndef _build_extend_line_axis1_geometry(draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs):\n    if antialias_stage_2_funcs is not None:\n        aa_stage_2_accumulate, aa_stage_2_clear, aa_stage_2_copy_back = antialias_stage_2_funcs\n    use_2_stage_agg = antialias_stage_2_funcs is not None\n\n    def extend_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax, geometry, closed_rings, antialias_stage_2, *aggs_and_cols):\n        values = geometry.buffer_values\n        missing = geometry.isna()\n        offsets = geometry.buffer_offsets\n        if len(offsets) == 2:\n            offsets0, offsets1 = offsets\n        else:\n            offsets1 = offsets[0]\n            offsets0 = np.arange(len(offsets1))\n        if geometry._sindex is not None:\n            eligible_inds = geometry.sindex.intersects((xmin, ymin, xmax, ymax))\n        else:\n            eligible_inds = np.arange(0, len(geometry), dtype='uint32')\n        extend_cpu_numba(sx, tx, sy, ty, xmin, xmax, ymin, ymax, values, missing, offsets0, offsets1, eligible_inds, closed_rings, antialias_stage_2, *aggs_and_cols)\n\n    @ngjit\n    @expand_aggs_and_cols\n    def extend_cpu_numba(sx, tx, sy, ty, xmin, xmax, ymin, ymax, values, missing, offsets0, offsets1, eligible_inds, closed_rings, antialias_stage_2, *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n        for i in eligible_inds:\n            if missing[i]:\n                continue\n            start0 = offsets0[i]\n            stop0 = offsets0[i + 1]\n            for j in range(start0, stop0):\n                start1 = offsets1[j]\n                stop1 = offsets1[j + 1]\n                for k in range(start1, stop1 - 2, 2):\n                    x0 = values[k]\n                    if not np.isfinite(x0):\n                        continue\n                    y0 = values[k + 1]\n                    if not np.isfinite(y0):\n                        continue\n                    x1 = values[k + 2]\n                    if not np.isfinite(x1):\n                        continue\n                    y1 = values[k + 3]\n                    if not np.isfinite(y1):\n                        continue\n                    segment_start = k == start1 and (not closed_rings) or (k > start1 and (not np.isfinite(values[k - 2]) or not np.isfinite(values[k - 1])))\n                    segment_end = not closed_rings and k == stop1 - 4 or (k < stop1 - 4 and (not np.isfinite(values[k + 4]) or not np.isfinite(values[k + 5])))\n                    if segment_start or use_2_stage_agg:\n                        xm = 0.0\n                        ym = 0.0\n                    elif k == start1 and closed_rings:\n                        xm = values[stop1 - 4]\n                        ym = values[stop1 - 3]\n                    else:\n                        xm = values[k - 2]\n                        ym = values[k - 1]\n                    draw_segment(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax, segment_start, segment_end, x0, x1, y0, y1, xm, ym, buffer, *aggs_and_cols)\n\n    def extend_cpu_antialias_2agg(sx, tx, sy, ty, xmin, xmax, ymin, ymax, geometry, closed_rings, antialias_stage_2, *aggs_and_cols):\n        values = geometry.buffer_values\n        missing = geometry.isna()\n        offsets = geometry.buffer_offsets\n        if len(offsets) == 2:\n            offsets0, offsets1 = offsets\n        else:\n            offsets1 = offsets[0]\n            offsets0 = np.arange(len(offsets1))\n        if geometry._sindex is not None:\n            eligible_inds = geometry.sindex.intersects((xmin, ymin, xmax, ymax))\n        else:\n            eligible_inds = np.arange(0, len(geometry), dtype='uint32')\n        n_aggs = len(antialias_stage_2[0])\n        aggs_and_accums = tuple(((agg, agg.copy()) for agg in aggs_and_cols[:n_aggs]))\n        extend_cpu_numba_antialias_2agg(sx, tx, sy, ty, xmin, xmax, ymin, ymax, values, missing, offsets0, offsets1, eligible_inds, closed_rings, antialias_stage_2, aggs_and_accums, *aggs_and_cols)\n\n    @ngjit\n    @expand_aggs_and_cols\n    def extend_cpu_numba_antialias_2agg(sx, tx, sy, ty, xmin, xmax, ymin, ymax, values, missing, offsets0, offsets1, eligible_inds, closed_rings, antialias_stage_2, aggs_and_accums, *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n        first_pass = True\n        for i in eligible_inds:\n            if missing[i]:\n                continue\n            start0 = offsets0[i]\n            stop0 = offsets0[i + 1]\n            for j in range(start0, stop0):\n                start1 = offsets1[j]\n                stop1 = offsets1[j + 1]\n                for k in range(start1, stop1 - 2, 2):\n                    x0 = values[k]\n                    if not np.isfinite(x0):\n                        continue\n                    y0 = values[k + 1]\n                    if not np.isfinite(y0):\n                        continue\n                    x1 = values[k + 2]\n                    if not np.isfinite(x1):\n                        continue\n                    y1 = values[k + 3]\n                    if not np.isfinite(y1):\n                        continue\n                    segment_start = k == start1 and (not closed_rings) or (k > start1 and (not np.isfinite(values[k - 2]) or not np.isfinite(values[k - 1])))\n                    segment_end = not closed_rings and k == stop1 - 4 or (k < stop1 - 4 and (not np.isfinite(values[k + 4]) or not np.isfinite(values[k + 5])))\n                    draw_segment(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax, segment_start, segment_end, x0, x1, y0, y1, 0.0, 0.0, buffer, *aggs_and_cols)\n            aa_stage_2_accumulate(aggs_and_accums, first_pass)\n            first_pass = False\n            aa_stage_2_clear(aggs_and_accums)\n        aa_stage_2_copy_back(aggs_and_accums)\n    if use_2_stage_agg:\n        return extend_cpu_antialias_2agg\n    else:\n        return extend_cpu\n\ndef _build_extend_line_axis1_geopandas(draw_segment, expand_aggs_and_cols, antialias_stage_2_funcs):\n    if antialias_stage_2_funcs is not None:\n        aa_stage_2_accumulate, aa_stage_2_clear, aa_stage_2_copy_back = antialias_stage_2_funcs\n    use_2_stage_agg = antialias_stage_2_funcs is not None\n    import shapely\n\n    def _process_geometry(geometry):\n        ragged = shapely.to_ragged_array(geometry)\n        geometry_type = ragged[0]\n        if geometry_type not in (shapely.GeometryType.LINESTRING, shapely.GeometryType.MULTILINESTRING, shapely.GeometryType.MULTIPOLYGON, shapely.GeometryType.POLYGON):\n            raise ValueError(f'Canvas.line supports GeoPandas geometry types of LINESTRING, MULTILINESTRING, MULTIPOLYGON and POLYGON, not {repr(geometry_type)}')\n        coords = ragged[1].ravel()\n        if geometry_type == shapely.GeometryType.LINESTRING:\n            offsets = ragged[2][0]\n            outer_offsets = np.arange(len(offsets))\n            closed_rings = False\n        elif geometry_type == shapely.GeometryType.MULTILINESTRING:\n            offsets, outer_offsets = ragged[2]\n            closed_rings = False\n        elif geometry_type == shapely.GeometryType.MULTIPOLYGON:\n            offsets, temp_offsets, outer_offsets = ragged[2]\n            outer_offsets = temp_offsets[outer_offsets]\n            closed_rings = True\n        else:\n            offsets, outer_offsets = ragged[2]\n            closed_rings = True\n        return (coords, offsets, outer_offsets, closed_rings)\n\n    def extend_cpu(sx, tx, sy, ty, xmin, xmax, ymin, ymax, geometry, antialias_stage_2, *aggs_and_cols):\n        coords, offsets, outer_offsets, closed_rings = _process_geometry(geometry)\n        extend_cpu_numba(sx, tx, sy, ty, xmin, xmax, ymin, ymax, coords, offsets, outer_offsets, closed_rings, antialias_stage_2, *aggs_and_cols)\n\n    @ngjit\n    @expand_aggs_and_cols\n    def extend_cpu_numba(sx, tx, sy, ty, xmin, xmax, ymin, ymax, values, offsets, outer_offsets, closed_rings, antialias_stage_2, *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n        n_multilines = len(outer_offsets) - 1\n        for i in range(n_multilines):\n            start0 = outer_offsets[i]\n            stop0 = outer_offsets[i + 1]\n            for j in range(start0, stop0):\n                start1 = offsets[j]\n                stop1 = offsets[j + 1]\n                for k in range(2 * start1, 2 * stop1 - 2, 2):\n                    x0 = values[k]\n                    y0 = values[k + 1]\n                    x1 = values[k + 2]\n                    y1 = values[k + 3]\n                    if not (np.isfinite(x0) and np.isfinite(y0) and np.isfinite(x1) and np.isfinite(y1)):\n                        continue\n                    segment_start = k == start1 and (not closed_rings) or (k > start1 and (not np.isfinite(values[k - 2]) or not np.isfinite(values[k - 1])))\n                    segment_end = not closed_rings and k == stop1 - 4 or (k < stop1 - 4 and (not np.isfinite(values[k + 4]) or not np.isfinite(values[k + 5])))\n                    if segment_start or use_2_stage_agg:\n                        xm = 0.0\n                        ym = 0.0\n                    elif k == start1 and closed_rings:\n                        xm = values[stop1 - 4]\n                        ym = values[stop1 - 3]\n                    else:\n                        xm = values[k - 2]\n                        ym = values[k - 1]\n                    draw_segment(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax, segment_start, segment_end, x0, x1, y0, y1, xm, ym, buffer, *aggs_and_cols)\n\n    def extend_cpu_antialias_2agg(sx, tx, sy, ty, xmin, xmax, ymin, ymax, geometry, antialias_stage_2, *aggs_and_cols):\n        coords, offsets, outer_offsets, closed_rings = _process_geometry(geometry)\n        n_aggs = len(antialias_stage_2[0])\n        aggs_and_accums = tuple(((agg, agg.copy()) for agg in aggs_and_cols[:n_aggs]))\n        extend_cpu_numba_antialias_2agg(sx, tx, sy, ty, xmin, xmax, ymin, ymax, coords, offsets, outer_offsets, closed_rings, antialias_stage_2, aggs_and_accums, *aggs_and_cols)\n\n    @ngjit\n    @expand_aggs_and_cols\n    def extend_cpu_numba_antialias_2agg(sx, tx, sy, ty, xmin, xmax, ymin, ymax, values, offsets, outer_offsets, closed_rings, antialias_stage_2, aggs_and_accums, *aggs_and_cols):\n        antialias = antialias_stage_2 is not None\n        buffer = np.empty(8) if antialias else None\n        n_multilines = len(outer_offsets) - 1\n        first_pass = True\n        for i in range(n_multilines):\n            start0 = outer_offsets[i]\n            stop0 = outer_offsets[i + 1]\n            for j in range(start0, stop0):\n                start1 = offsets[j]\n                stop1 = offsets[j + 1]\n                for k in range(2 * start1, 2 * stop1 - 2, 2):\n                    x0 = values[k]\n                    y0 = values[k + 1]\n                    x1 = values[k + 2]\n                    y1 = values[k + 3]\n                    if not (np.isfinite(x0) and np.isfinite(y0) and np.isfinite(x1) and np.isfinite(y1)):\n                        continue\n                    segment_start = k == start1 and (not closed_rings) or (k > start1 and (not np.isfinite(values[k - 2]) or not np.isfinite(values[k - 1])))\n                    segment_end = not closed_rings and k == stop1 - 4 or (k < stop1 - 4 and (not np.isfinite(values[k + 4]) or not np.isfinite(values[k + 5])))\n                    draw_segment(i, sx, tx, sy, ty, xmin, xmax, ymin, ymax, segment_start, segment_end, x0, x1, y0, y1, 0.0, 0.0, buffer, *aggs_and_cols)\n            aa_stage_2_accumulate(aggs_and_accums, first_pass)\n            first_pass = False\n            aa_stage_2_clear(aggs_and_accums)\n        aa_stage_2_copy_back(aggs_and_accums)\n    if use_2_stage_agg:\n        return extend_cpu_antialias_2agg\n    else:\n        return extend_cpu"
  }
}