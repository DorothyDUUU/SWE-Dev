{
  "dir_path": "/app/advertools",
  "package_name": "advertools",
  "sample_name": "advertools-test_robotstxt",
  "src_dir": "advertools/",
  "test_dir": "tests/",
  "test_file": "tests/test_robotstxt.py",
  "test_code": "import os\nfrom advertools.robotstxt import robotstxt_to_df, robotstxt_test\nimport pandas as pd\nimport pytest\nfrom .test_sitemaps import full_path\n\nrobots_file = full_path('robots.txt')\n\n\ndef test_robotstxt_to_df():\n    result = robotstxt_to_df('https://www.media-supermarket.com/robots.txt')\n    assert isinstance(result, pd.core.frame.DataFrame)\n    assert all(col in result\n               for col in ['directive', 'content', 'download_date'])\n\n\ndef test_robtostxt_to_df_handles_list():\n    result = robotstxt_to_df([\n        'https://www.media-supermarket.com/robots.txt',\n        robots_file\n    ])\n    assert isinstance(result, pd.core.frame.DataFrame)\n    assert all(col in result\n               for col in ['directive', 'content', 'download_date'])\n\n\ndef test_robotstxt_to_df_saves_single_file():\n    robotstxt_to_df('https://www.media-supermarket.com/robots.txt',\n                    output_file='robots_output.jl')\n    result = pd.read_json('robots_output.jl', lines=True)\n    assert isinstance(result, pd.core.frame.DataFrame)\n    assert all(col in result\n               for col in ['directive', 'content', 'download_date'])\n    os.remove('robots_output.jl')\n\n\ndef test_robotstxt_to_df_saves_file_list():\n    robotstxt_to_df(['https://www.media-supermarket.com/robots.txt',\n                     robots_file],\n                    output_file='robots_output.jl')\n    result = pd.read_json('robots_output.jl', lines=True)\n    assert isinstance(result, pd.core.frame.DataFrame)\n    assert all(col in result\n               for col in ['directive', 'content', 'download_date'])\n    os.remove('robots_output.jl')\n\n\ndef test_robotstxt_to_df_raises_on_wrong_file():\n    with pytest.raises(ValueError):\n        robotstxt_to_df(robots_file, output_file='wrong_extension.pdf')\n\n\ndef test_robotstxt_to_df_contains_errors():\n    result = robotstxt_to_df('wrong_url.html')\n    assert 'errors' in result\n\n\ndef test_robotstxt_test():\n    user_agents = ['Googlebot', 'Baiduspider', '*']\n    urls_to_check = ['/', '/help', 'something.html']\n    result = robotstxt_test(robots_file, user_agents, urls_to_check)\n    assert isinstance(result, pd.core.frame.DataFrame)\n    assert all(col in result for col in\n               ['robotstxt_url', 'user_agent', 'url_path', 'can_fetch'])\n\n\ndef test_robotstxt_raises():\n    with pytest.raises(ValueError):\n        robotstxt_test('http://www.wrong-url.com', '*', '/')\n\n\ndef test_robots_converts_str_to_list():\n    result = robotstxt_test('https://www.apple.com/robots.txt', '*', 'hello')\n    assert isinstance(result, pd.core.frame.DataFrame)\n    assert all(col in result for col in\n               ['robotstxt_url', 'user_agent', 'url_path', 'can_fetch'])\n",
  "GT_file_code": {
    "advertools/robotstxt.py": "\"\"\"\n.. _robotstxt:\n\n\ud83e\udd16 Analyze and Test robots.txt Files on a Large Scale\n=====================================================\n\nEven though they are tiny in size, robots.txt files contain potent instructions\nthat can block major sections of your site, which is what they are supposed to\ndo. Only sometimes you might make the mistake of blocking the wrong section.\n\nSo it is very important to check if certain pages (or groups of pages) are\nblocked for a certain user-agent by a certain robots.txt file. Ideally, you\nwould want to run the same check for all possible user-agents. Even more\nideally, you want to be able to run the check for a large number of pages with\nevery possible combination with user-agents.\n\nTo get the robots.txt file into an easily readable format, you can use the\n:func:`robotstxt_to_df` function to get it in a DataFrame.\n\n.. thebe-button::\n    Run this code\n\n.. code-block::\n    :class: thebe, thebe-init\n\n    import advertools as adv\n\n    amazon = adv.robotstxt_to_df('https://www.amazon.com/robots.txt')\n    amazon\n\n====  ===========  =================================  ==================================  =========================  =================================  ================================\n  ..  directive    content                            etag                                robotstxt_last_modified    robotstxt_url                      download_date\n====  ===========  =================================  ==================================  =========================  =================================  ================================\n   0  User-agent   \\*                                 \"a850165d925db701988daf7ead7492d3\"  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00\n   1  Disallow     /exec/obidos/account-access-login  \"a850165d925db701988daf7ead7492d3\"  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00\n   2  Disallow     /exec/obidos/change-style          \"a850165d925db701988daf7ead7492d3\"  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00\n   3  Disallow     /exec/obidos/flex-sign-in          \"a850165d925db701988daf7ead7492d3\"  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00\n   4  Disallow     /exec/obidos/handle-buy-box        \"a850165d925db701988daf7ead7492d3\"  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00\n ...  ...          ...                                ...                                 ...                        ...                                ...\n 146  Disallow     /hp/video/mystuff                  \"a850165d925db701988daf7ead7492d3\"  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00\n 147  Disallow     /gp/video/profiles                 \"a850165d925db701988daf7ead7492d3\"  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00\n 148  Disallow     /hp/video/profiles                 \"a850165d925db701988daf7ead7492d3\"  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00\n 149  User-agent   EtaoSpider                         \"a850165d925db701988daf7ead7492d3\"  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00\n 150  Disallow     /                                  \"a850165d925db701988daf7ead7492d3\"  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00\n====  ===========  =================================  ==================================  =========================  =================================  ================================\n\nThe returned DataFrame contains columns for directives, their content, the URL\nof the robots.txt file, as well as the date it was downloaded.\n\n*  `directive`: The main commands. Allow, Disallow, Sitemap, Crawl-delay,\n   User-agent, and so on.\n*  `content`: The details of each of the directives.\n*  `robotstxt_last_modified`: The date when the robots.txt file was last\n   modified, if provided (according the response header Last-modified).\n*  `etag`: The entity tag of the response header, if provided.\n*  `robotstxt_url`: The URL of the robots.txt file.\n*  `download_date`: The date and time when the file was downloaded.\n\nAlternatively, you can provide a list of robots URLs if you want to download\nthem all in one go. This might be interesting if:\n\n* You are analyzing an industry and want to keep an eye on many different\n  websites.\n* You are analyzing a website with many sub-domains, and want to get all the\n  robots files together.\n* You are trying to understand a company that has many websites under different\n  domains and sub-domains.\n\nIn this case you simply provide a list of URLs instead of a single one.\n\n.. thebe-button::\n    Run this code\n\n.. code-block::\n    :class: thebe, thebe-init\n\n    robots_urls = ['https://www.google.com/robots.txt',\n                   'https://twitter.com/robots.txt',\n                   'https://facebook.com/robots.txt']\n\n    googtwfb = adv.robotstxt_to_df(robots_urls)\n\n    # How many lines does each robots file have?\n    googtwfb.groupby('robotstxt_url')['directive'].count()\n\n.. code-block::\n\n    robotstxt_url\n    https://facebook.com/robots.txt      541\n    https://twitter.com/robots.txt       108\n    https://www.google.com/robots.txt    289\n    Name: directive, dtype: int64\n\n.. code-block::\n    :class: thebe, thebe-init\n\n    # Display the first five rows of each of the robots files:\n    googtwfb.groupby('robotstxt_url').head()\n\n====  ===========  ===================================================================  =========================  =================================  ================================\n  ..  directive    content                                                              robotstxt_last_modified    robotstxt_url                      download_date\n====  ===========  ===================================================================  =========================  =================================  ================================\n   0  User-agent   \\*                                                                   2022-02-07 22:30:00+00:00  https://www.google.com/robots.txt  2022-02-11 19:52:13.375724+00:00\n   1  Disallow     /search                                                              2022-02-07 22:30:00+00:00  https://www.google.com/robots.txt  2022-02-11 19:52:13.375724+00:00\n   2  Allow        /search/about                                                        2022-02-07 22:30:00+00:00  https://www.google.com/robots.txt  2022-02-11 19:52:13.375724+00:00\n   3  Allow        /search/static                                                       2022-02-07 22:30:00+00:00  https://www.google.com/robots.txt  2022-02-11 19:52:13.375724+00:00\n   4  Allow        /search/howsearchworks                                               2022-02-07 22:30:00+00:00  https://www.google.com/robots.txt  2022-02-11 19:52:13.375724+00:00\n 289  comment      Google Search Engine Robot                                           NaT                        https://twitter.com/robots.txt     2022-02-11 19:52:13.461815+00:00\n 290  comment      \\==========================                                           NaT                        https://twitter.com/robots.txt     2022-02-11 19:52:13.461815+00:00\n 291  User-agent   Googlebot                                                            NaT                        https://twitter.com/robots.txt     2022-02-11 19:52:13.461815+00:00\n 292  Allow        /?_escaped_fragment_                                                 NaT                        https://twitter.com/robots.txt     2022-02-11 19:52:13.461815+00:00\n 293  Allow        /\\*?lang=                                                            NaT                        https://twitter.com/robots.txt     2022-02-11 19:52:13.461815+00:00\n 397  comment      Notice: Collection of data on Facebook through automated means is    NaT                        https://facebook.com/robots.txt    2022-02-11 19:52:13.474456+00:00\n 398  comment      prohibited unless you have express written permission from Facebook  NaT                        https://facebook.com/robots.txt    2022-02-11 19:52:13.474456+00:00\n 399  comment      and may only be conducted for the limited purpose contained in said  NaT                        https://facebook.com/robots.txt    2022-02-11 19:52:13.474456+00:00\n 400  comment      permission.                                                          NaT                        https://facebook.com/robots.txt    2022-02-11 19:52:13.474456+00:00\n 401  comment      See: http://www.facebook.com/apps/site_scraping_tos_terms.php        NaT                        https://facebook.com/robots.txt    2022-02-11 19:52:13.474456+00:00\n====  ===========  ===================================================================  =========================  =================================  ================================\n\nBulk ``robots.txt`` Tester\n--------------------------\n\nThis tester is designed to work on a large scale. The :func:`robotstxt_test`\nfunction runs a test for a given robots.txt file, checking which of the\nprovided user-agents can fetch which of the provided URLs, paths, or patterns.\n\n\n.. thebe-button::\n    Run this code\n\n.. code-block::\n    :class: thebe, thebe-init\n\n    import advertools as adv\n    adv.robotstxt_test(\n        robotstxt_url='https://www.amazon.com/robots.txt',\n        user_agents=['Googlebot', 'baiduspider', 'Bingbot'],\n        urls=['/', '/hello', '/some-page.html'])\n\nAs a result, you get a DataFrame with a row for each combination of\n(user-agent, URL) indicating whether or not that particular user-agent can\nfetch the given URL.\n\nSome reasons why you might want to do that:\n\n* SEO Audits: Especially for large websites with many URL patterns, and many\n  rules for different user-agents.\n* Developer or site owner about to make large changes\n* Interest in strategies of certain companies\n\nUser-agents\n-----------\n\nIn reality there are only two groups of user-agents that you need to worry\nabout:\n\n* User-agents listed in the robots.txt file: For each one of those you need to\n  check whether or not they are blocked from fetching a certain URL\n  (or pattern).\n* ``*`` all other user-agents: The ``*`` includes all other user-agents, so\n  checking the rules that apply to it should take care of the rest.\n\nrobots.txt Testing Approach\n---------------------------\n\n1. Get the robots.txt file that you are interested in\n2. Extract the user-agents from it\n3. Specify the URLs you are interested in testing\n4. Run the :func:`robotstxt_test` function\n\n.. thebe-button::\n    Run this code\n\n.. code-block::\n    :class: thebe, thebe-init\n\n    fb_robots = adv.robotstxt_to_df('https://www.facebook.com/robots.txt')\n    fb_robots\n\n====  ===========  ===================================================================  ===================================  ================================\n  ..  directive    content                                                              robotstxt_url                        download_date\n====  ===========  ===================================================================  ===================================  ================================\n   0  comment      Notice: Collection of data on Facebook through automated means is    https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00\n   1  comment      prohibited unless you have express written permission from Facebook  https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00\n   2  comment      and may only be conducted for the limited purpose contained in said  https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00\n   3  comment      permission.                                                          https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00\n   4  comment      See: http://www.facebook.com/apps/site_scraping_tos_terms.php        https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00\n ...  ...          ...                                                                  ...                                  ...\n 536  Allow        /ajax/pagelet/generic.php/PagePostsSectionPagelet                    https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00\n 537  Allow        /careers/                                                            https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00\n 538  Allow        /safetycheck/                                                        https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00\n 539  User-agent   *                                                                    https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00\n 540  Disallow     /                                                                    https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00\n====  ===========  ===================================================================  ===================================  ================================\n\n\nNow that we have downloaded the file, we can easily extract the list of\nuser-agents that it contains.\n\n.. thebe-button::\n    Run this code\n\n.. code-block::\n    :class: thebe, thebe-init\n\n    fb_useragents = (fb_robots\n                     [fb_robots['directive']=='User-agent']\n                     ['content'].drop_duplicates()\n                    .tolist())\n    fb_useragents\n\n.. code-block::\n\n    ['Applebot',\n     'baiduspider',\n     'Bingbot',\n     'Discordbot',\n     'facebookexternalhit',\n     'Googlebot',\n     'Googlebot-Image',\n     'ia_archiver',\n     'LinkedInBot',\n     'msnbot',\n     'Naverbot',\n     'Pinterestbot',\n     'seznambot',\n     'Slurp',\n     'teoma',\n     'TelegramBot',\n     'Twitterbot',\n     'Yandex',\n     'Yeti',\n     '*']\n\nQuite a long list!\n\nAs a small and quick test, I'm interested in checking the home page, a random\nprofile page (/bbc), groups and hashtags pages.\n\n.. thebe-button::\n    Run this code\n\n.. code-block::\n    :class: thebe, thebe-init\n\n    urls_to_test = ['/', '/bbc', '/groups', '/hashtag/']\n    fb_test = robotstxt_test('https://www.facebook.com/robots.txt',\n                             fb_useragents, urls_to_test)\n    fb_test\n\n====  ===================================  ============  ==========  ===========\n  ..  robotstxt_url                        user_agent    url_path    can_fetch\n====  ===================================  ============  ==========  ===========\n   0  https://www.facebook.com/robots.txt  \\*            /           False\n   1  https://www.facebook.com/robots.txt  \\*            /bbc        False\n   2  https://www.facebook.com/robots.txt  \\*            /groups     False\n   3  https://www.facebook.com/robots.txt  \\*            /hashtag/   False\n   4  https://www.facebook.com/robots.txt  Applebot      /           True\n  ..                                  ...           ...         ...          ...\n  75  https://www.facebook.com/robots.txt  seznambot     /hashtag/   True\n  76  https://www.facebook.com/robots.txt  teoma         /           True\n  77  https://www.facebook.com/robots.txt  teoma         /bbc        True\n  78  https://www.facebook.com/robots.txt  teoma         /groups     True\n  79  https://www.facebook.com/robots.txt  teoma         /hashtag/   True\n====  ===================================  ============  ==========  ===========\n\nFor twenty user-agents and four URLs each, we received a total of eighty test\nresults. You can immediately see that all user-agents not listed (denoted by\n`*` are not allowed to fetch any of the provided URLs).\n\nLet's see who is and who is not allowed to fetch the home page.\n\n.. code-block::\n\n    fb_test.query('url_path== \"/\"')\n\n====  ===================================  ===================  ==========  ===========\n  ..  robotstxt_url                        user_agent           url_path    can_fetch\n====  ===================================  ===================  ==========  ===========\n   0  https://www.facebook.com/robots.txt  \\*                   /           False\n   4  https://www.facebook.com/robots.txt  Applebot             /           True\n   8  https://www.facebook.com/robots.txt  Bingbot              /           True\n  12  https://www.facebook.com/robots.txt  Discordbot           /           False\n  16  https://www.facebook.com/robots.txt  Googlebot            /           True\n  20  https://www.facebook.com/robots.txt  Googlebot-Image      /           True\n  24  https://www.facebook.com/robots.txt  LinkedInBot          /           False\n  28  https://www.facebook.com/robots.txt  Naverbot             /           True\n  32  https://www.facebook.com/robots.txt  Pinterestbot         /           False\n  36  https://www.facebook.com/robots.txt  Slurp                /           True\n  40  https://www.facebook.com/robots.txt  TelegramBot          /           False\n  44  https://www.facebook.com/robots.txt  Twitterbot           /           True\n  48  https://www.facebook.com/robots.txt  Yandex               /           True\n  52  https://www.facebook.com/robots.txt  Yeti                 /           True\n  56  https://www.facebook.com/robots.txt  baiduspider          /           True\n  60  https://www.facebook.com/robots.txt  facebookexternalhit  /           False\n  64  https://www.facebook.com/robots.txt  ia_archiver          /           False\n  68  https://www.facebook.com/robots.txt  msnbot               /           True\n  72  https://www.facebook.com/robots.txt  seznambot            /           True\n  76  https://www.facebook.com/robots.txt  teoma                /           True\n====  ===================================  ===================  ==========  ===========\n\nI'll leave it to you to figure out why LinkedIn and Pinterest are not allowed\nto crawl the home page but Google and Apple are, because I have no clue!\n\"\"\"  # noqa: E501\n\n__all__ = [\"robotstxt_to_df\", \"robotstxt_test\"]\n\nimport gzip\nimport logging\nfrom concurrent import futures\nfrom itertools import product\nfrom urllib.request import Request, urlopen\n\nimport pandas as pd\nfrom protego import Protego\n\nfrom advertools import __version__ as version\n\nheaders = {\"User-Agent\": \"advertools-\" + version}\n\ngzip_start_bytes = b\"\\x1f\\x8b\"\n\nlogging.basicConfig(level=logging.INFO)\n\n\ndef robotstxt_to_df(robotstxt_url, output_file=None):\n    \"\"\"Download the contents of ``robotstxt_url`` into a DataFrame\n\n    Parameters\n    ----------\n    robotstxt_url : str\n      One or more URLs of the robots.txt file(s)\n    output_file : str\n      Optional file path to save the robots.txt files, mainly useful for downloading >\n      500 files. The files are appended as soon as they are downloaded. Only the \".jl\"\n      extension is supported.\n\n    Returns\n    -------\n    robotstxt_df : pandas.DataFrame\n      A DataFrame containing directives, their content, the URL and time of download\n\n    Examples\n    --------\n    You can also use it to download multiple robots files by passing a list of\n    URLs.\n\n    >>> robotstxt_to_df(\"https://www.twitter.com/robots.txt\")\n         directive content   \t                 robotstxt_url\t                   download_date\n    0\tUser-agent\t     *\thttps://www.twitter.com/robots.txt\t2020-09-27 21:57:23.702814+00:00\n    1\t  Disallow\t     /\thttps://www.twitter.com/robots.txt\t2020-09-27 21:57:23.702814+00:00\n\n    >>> robotstxt_to_df(\n    ...     [\"https://www.google.com/robots.txt\", \"https://www.twitter.com/robots.txt\"]\n    ... )\n           directive\t                             content\t    robotstxt_last_modified\t                       robotstxt_url\t                     download_date\n    0\t  User-agent\t                                   *\t  2021-01-11 21:00:00+00:00\t   https://www.google.com/robots.txt\t  2021-01-16 14:08:50.087985+00:00\n    1\t    Disallow\t                             /search\t  2021-01-11 21:00:00+00:00\t   https://www.google.com/robots.txt\t  2021-01-16 14:08:50.087985+00:00\n    2\t       Allow\t                       /search/about\t  2021-01-11 21:00:00+00:00\t   https://www.google.com/robots.txt\t  2021-01-16 14:08:50.087985+00:00\n    3\t       Allow\t                      /search/static\t  2021-01-11 21:00:00+00:00\t   https://www.google.com/robots.txt\t  2021-01-16 14:08:50.087985+00:00\n    4\t       Allow\t              /search/howsearchworks\t  2021-01-11 21:00:00+00:00\t   https://www.google.com/robots.txt\t  2021-01-16 14:08:50.087985+00:00\n    283\t  User-agent\t                 facebookexternalhit\t  2021-01-11 21:00:00+00:00\t   https://www.google.com/robots.txt\t  2021-01-16 14:08:50.087985+00:00\n    284\t       Allow\t                             /imgres\t  2021-01-11 21:00:00+00:00\t   https://www.google.com/robots.txt\t  2021-01-16 14:08:50.087985+00:00\n    285\t     Sitemap\t  https://www.google.com/sitemap.xml\t  2021-01-11 21:00:00+00:00\t   https://www.google.com/robots.txt\t  2021-01-16 14:08:50.087985+00:00\n    286\t  User-agent\t                                   *\t                        NaT\t  https://www.twitter.com/robots.txt\t  2021-01-16 14:08:50.468588+00:00\n    287\t    Disallow\t                                   /\t                        NaT\t  https://www.twitter.com/robots.txt\t  2021-01-16 14:08:50.468588+00:00\n\n    For research purposes and if you want to download more than ~500 files, you\n    might want to use ``output_file`` to save results as they are downloaded.\n    The file extension should be \".jl\", and robots files are appended to that\n    file as soon as they are downloaded, in case you lose your connection, or\n    maybe your patience!\n\n    >>> robotstxt_to_df(\n    ...     [\n    ...         \"https://example.com/robots.txt\",\n    ...         \"https://example.com/robots.txt\",\n    ...         \"https://example.com/robots.txt\",\n    ...     ],\n    ...     output_file=\"robots_output_file.jl\",\n    ... )\n\n    To open the file as a DataFrame:\n\n    >>> import pandas as pd\n    >>> robotsfiles_df = pd.read_json(\"robots_output_file.jl\", lines=True)\n    \"\"\"  # noqa: E501\n    if output_file is not None and (not output_file.endswith(\".jl\")):\n        raise ValueError(\"Please specify a file with a `.jl` extension.\")\n    if isinstance(robotstxt_url, (list, tuple, set, pd.Series)):\n        return _robots_multi(robotstxt_url, output_file)\n    else:\n        try:\n            logging.info(msg=\"Getting: \" + robotstxt_url)\n            robots_open = urlopen(Request(robotstxt_url, headers=headers), timeout=45)\n            robots_read = robots_open.read()\n            if robots_read.startswith(gzip_start_bytes):\n                data = gzip.decompress(robots_read)\n                robots_text = data.decode(\"utf-8-sig\").splitlines()\n            else:\n                robots_text = robots_read.decode(\"utf-8-sig\").splitlines()\n            lines = []\n            for line in robots_text:\n                if line.strip():\n                    if line.strip().startswith(\"#\"):\n                        lines.append([\"comment\", (line.replace(\"#\", \"\").strip())])\n                    else:\n                        split = line.split(\":\", maxsplit=1)\n                        lines.append([split[0].strip(), split[1].strip()])\n            df = pd.DataFrame(lines, columns=[\"directive\", \"content\"])\n            try:\n                etag_lastmod = {\n                    header.lower().replace(\"-\", \"_\"): val\n                    for header, val in robots_open.getheaders()\n                    if header.lower() in [\"etag\", \"last-modified\"]\n                }\n                df = df.assign(**etag_lastmod)\n                if \"last_modified\" in df:\n                    df[\"robotstxt_last_modified\"] = pd.to_datetime(df[\"last_modified\"])\n                    del df[\"last_modified\"]\n            except AttributeError:\n                pass\n        except Exception as e:\n            df = pd.DataFrame({\"errors\": [str(e)]})\n        try:\n            df[\"robotstxt_url\"] = [robots_open.url] if df.empty else robots_open.url\n        except UnboundLocalError:\n            df[\"robotstxt_url\"] = [robotstxt_url] if df.empty else robotstxt_url\n        df[\"download_date\"] = pd.Timestamp.now(tz=\"UTC\")\n        if output_file is not None:\n            with open(output_file, \"a\") as file:\n                file.write(df.to_json(orient=\"records\", lines=True, date_format=\"iso\"))\n                file.write(\"\\n\")\n        else:\n            return df\n\n\ndef _robots_multi(robots_url_list, output_file=None):\n    final_df = pd.DataFrame()\n    with futures.ThreadPoolExecutor(max_workers=24) as executor:\n        to_do = []\n        for robotsurl in robots_url_list:\n            future = executor.submit(robotstxt_to_df, robotsurl)\n            to_do.append(future)\n        done_iter = futures.as_completed(to_do)\n\n        for future in done_iter:\n            future_result = future.result()\n            if output_file is not None:\n                with open(output_file, \"a\") as file:\n                    file.write(\n                        future_result.to_json(\n                            orient=\"records\", lines=True, date_format=\"iso\"\n                        )\n                    )\n                    file.write(\"\\n\")\n            else:\n                final_df = pd.concat([final_df, future_result], ignore_index=True)\n    if output_file is None:\n        return final_df\n\n\ndef robotstxt_test(robotstxt_url, user_agents, urls):\n    \"\"\"Given a :attr:`robotstxt_url` check which of the :attr:`user_agents` is\n    allowed to fetch which of the :attr:`urls`.\n\n    All the combinations of :attr:`user_agents` and :attr:`urls` will be\n    checked and the results returned in one DataFrame.\n\n    Parameters\n    ----------\n\n    robotstxt_url : str\n      The URL of robotx.txt file.\n    user_agents : str, list\n      One or more user agents.\n    urls : str, list\n      One or more paths (relative) or URLs (absolute) to check.\n\n    Returns\n    -------\n    robotstxt_test_df : pandas.DataFrame\n      A DataFrame with the test results per user-agent/rule combination.\n\n    Examples\n    --------\n    >>> robotstxt_test(\n    ...     \"https://facebook.com/robots.txt\",\n    ...     user_agents=[\"*\", \"Googlebot\", \"Applebot\"],\n    ...     urls=[\"/\", \"/bbc\", \"/groups\", \"/hashtag/\"],\n    ... )\n                          robotstxt_url user_agent   url_path  can_fetch\n    0   https://facebook.com/robots.txt          *          /      False\n    1   https://facebook.com/robots.txt          *       /bbc      False\n    2   https://facebook.com/robots.txt          *    /groups      False\n    3   https://facebook.com/robots.txt          *  /hashtag/      False\n    4   https://facebook.com/robots.txt   Applebot          /       True\n    5   https://facebook.com/robots.txt   Applebot       /bbc       True\n    6   https://facebook.com/robots.txt   Applebot    /groups       True\n    7   https://facebook.com/robots.txt   Applebot  /hashtag/      False\n    8   https://facebook.com/robots.txt  Googlebot          /       True\n    9   https://facebook.com/robots.txt  Googlebot       /bbc       True\n    10  https://facebook.com/robots.txt  Googlebot    /groups       True\n    11  https://facebook.com/robots.txt  Googlebot  /hashtag/      False\n\n    \"\"\"\n    if not robotstxt_url.endswith(\"/robots.txt\"):\n        raise ValueError(\"Please make sure you enter a valid robots.txt URL\")\n    if isinstance(user_agents, str):\n        user_agents = [user_agents]\n    if isinstance(urls, str):\n        urls = [urls]\n    robots_open = urlopen(Request(robotstxt_url, headers=headers))\n    robots_bytes = robots_open.readlines()\n    robots_text = \"\".join(line.decode() for line in robots_bytes)\n    rp = Protego.parse(robots_text)\n\n    test_list = []\n    for path, agent in product(urls, user_agents):\n        d = dict()\n        d[\"user_agent\"] = agent\n        d[\"url_path\"] = path\n        d[\"can_fetch\"] = rp.can_fetch(path, agent)\n        test_list.append(d)\n    df = pd.DataFrame(test_list)\n    df.insert(0, \"robotstxt_url\", robotstxt_url)\n    df = df.sort_values([\"user_agent\", \"url_path\"]).reset_index(drop=True)\n    return df\n"
  },
  "GT_src_dict": {
    "advertools/robotstxt.py": {
      "robotstxt_to_df": {
        "code": "def robotstxt_to_df(robotstxt_url, output_file=None):\n    \"\"\"Download and parse the contents of a robots.txt file from a given URL or multiple URLs into a pandas DataFrame.\n\nParameters\n----------\nrobotstxt_url : str\n    A single URL or a list of URLs pointing to robots.txt files.\noutput_file : str, optional\n    An optional file path to save results as they are downloaded, particularly useful for large-scale downloads (must end with '.jl').\n\nReturns\n-------\nrobotstxt_df : pandas.DataFrame\n    A DataFrame containing directives, their content, the URL of the robots.txt file, and the timestamp of when it was downloaded.\n\nRaises\n------\nValueError\n    If `output_file` does not end with \".jl\" or if the provided `robotstxt_url` list is not in a supported format.\n\nDependencies\n------------\n- pandas: For creating and manipulating the DataFrame.\n- urllib: For downloading the robots.txt files from the URLs.\n- gzip: For handling potentially gzipped responses.\n- logging: For logging the download progress.\n\nConstants\n---------\nheaders : dict\n    A dictionary specifying the User-Agent header to be used during the fetching of robots.txt files, constructed using the module's version.\ngzip_start_bytes : bytes\n    A constant byte sequence indicating the start of a gzipped file, used to determine if decompression is necessary.\n\nThis function facilitates the analysis of robots.txt files, allowing users to download and efficiently handle multiple files for further processing, like SEO audits or competitive analysis.\"\"\"\n    'Download the contents of ``robotstxt_url`` into a DataFrame\\n\\n    Parameters\\n    ----------\\n    robotstxt_url : str\\n      One or more URLs of the robots.txt file(s)\\n    output_file : str\\n      Optional file path to save the robots.txt files, mainly useful for downloading >\\n      500 files. The files are appended as soon as they are downloaded. Only the \".jl\"\\n      extension is supported.\\n\\n    Returns\\n    -------\\n    robotstxt_df : pandas.DataFrame\\n      A DataFrame containing directives, their content, the URL and time of download\\n\\n    Examples\\n    --------\\n    You can also use it to download multiple robots files by passing a list of\\n    URLs.\\n\\n    >>> robotstxt_to_df(\"https://www.twitter.com/robots.txt\")\\n         directive content   \\t                 robotstxt_url\\t                   download_date\\n    0\\tUser-agent\\t     *\\thttps://www.twitter.com/robots.txt\\t2020-09-27 21:57:23.702814+00:00\\n    1\\t  Disallow\\t     /\\thttps://www.twitter.com/robots.txt\\t2020-09-27 21:57:23.702814+00:00\\n\\n    >>> robotstxt_to_df(\\n    ...     [\"https://www.google.com/robots.txt\", \"https://www.twitter.com/robots.txt\"]\\n    ... )\\n           directive\\t                             content\\t    robotstxt_last_modified\\t                       robotstxt_url\\t                     download_date\\n    0\\t  User-agent\\t                                   *\\t  2021-01-11 21:00:00+00:00\\t   https://www.google.com/robots.txt\\t  2021-01-16 14:08:50.087985+00:00\\n    1\\t    Disallow\\t                             /search\\t  2021-01-11 21:00:00+00:00\\t   https://www.google.com/robots.txt\\t  2021-01-16 14:08:50.087985+00:00\\n    2\\t       Allow\\t                       /search/about\\t  2021-01-11 21:00:00+00:00\\t   https://www.google.com/robots.txt\\t  2021-01-16 14:08:50.087985+00:00\\n    3\\t       Allow\\t                      /search/static\\t  2021-01-11 21:00:00+00:00\\t   https://www.google.com/robots.txt\\t  2021-01-16 14:08:50.087985+00:00\\n    4\\t       Allow\\t              /search/howsearchworks\\t  2021-01-11 21:00:00+00:00\\t   https://www.google.com/robots.txt\\t  2021-01-16 14:08:50.087985+00:00\\n    283\\t  User-agent\\t                 facebookexternalhit\\t  2021-01-11 21:00:00+00:00\\t   https://www.google.com/robots.txt\\t  2021-01-16 14:08:50.087985+00:00\\n    284\\t       Allow\\t                             /imgres\\t  2021-01-11 21:00:00+00:00\\t   https://www.google.com/robots.txt\\t  2021-01-16 14:08:50.087985+00:00\\n    285\\t     Sitemap\\t  https://www.google.com/sitemap.xml\\t  2021-01-11 21:00:00+00:00\\t   https://www.google.com/robots.txt\\t  2021-01-16 14:08:50.087985+00:00\\n    286\\t  User-agent\\t                                   *\\t                        NaT\\t  https://www.twitter.com/robots.txt\\t  2021-01-16 14:08:50.468588+00:00\\n    287\\t    Disallow\\t                                   /\\t                        NaT\\t  https://www.twitter.com/robots.txt\\t  2021-01-16 14:08:50.468588+00:00\\n\\n    For research purposes and if you want to download more than ~500 files, you\\n    might want to use ``output_file`` to save results as they are downloaded.\\n    The file extension should be \".jl\", and robots files are appended to that\\n    file as soon as they are downloaded, in case you lose your connection, or\\n    maybe your patience!\\n\\n    >>> robotstxt_to_df(\\n    ...     [\\n    ...         \"https://example.com/robots.txt\",\\n    ...         \"https://example.com/robots.txt\",\\n    ...         \"https://example.com/robots.txt\",\\n    ...     ],\\n    ...     output_file=\"robots_output_file.jl\",\\n    ... )\\n\\n    To open the file as a DataFrame:\\n\\n    >>> import pandas as pd\\n    >>> robotsfiles_df = pd.read_json(\"robots_output_file.jl\", lines=True)\\n    '\n    if output_file is not None and (not output_file.endswith('.jl')):\n        raise ValueError('Please specify a file with a `.jl` extension.')\n    if isinstance(robotstxt_url, (list, tuple, set, pd.Series)):\n        return _robots_multi(robotstxt_url, output_file)\n    else:\n        try:\n            logging.info(msg='Getting: ' + robotstxt_url)\n            robots_open = urlopen(Request(robotstxt_url, headers=headers), timeout=45)\n            robots_read = robots_open.read()\n            if robots_read.startswith(gzip_start_bytes):\n                data = gzip.decompress(robots_read)\n                robots_text = data.decode('utf-8-sig').splitlines()\n            else:\n                robots_text = robots_read.decode('utf-8-sig').splitlines()\n            lines = []\n            for line in robots_text:\n                if line.strip():\n                    if line.strip().startswith('#'):\n                        lines.append(['comment', line.replace('#', '').strip()])\n                    else:\n                        split = line.split(':', maxsplit=1)\n                        lines.append([split[0].strip(), split[1].strip()])\n            df = pd.DataFrame(lines, columns=['directive', 'content'])\n            try:\n                etag_lastmod = {header.lower().replace('-', '_'): val for header, val in robots_open.getheaders() if header.lower() in ['etag', 'last-modified']}\n                df = df.assign(**etag_lastmod)\n                if 'last_modified' in df:\n                    df['robotstxt_last_modified'] = pd.to_datetime(df['last_modified'])\n                    del df['last_modified']\n            except AttributeError:\n                pass\n        except Exception as e:\n            df = pd.DataFrame({'errors': [str(e)]})\n        try:\n            df['robotstxt_url'] = [robots_open.url] if df.empty else robots_open.url\n        except UnboundLocalError:\n            df['robotstxt_url'] = [robotstxt_url] if df.empty else robotstxt_url\n        df['download_date'] = pd.Timestamp.now(tz='UTC')\n        if output_file is not None:\n            with open(output_file, 'a') as file:\n                file.write(df.to_json(orient='records', lines=True, date_format='iso'))\n                file.write('\\n')\n        else:\n            return df",
        "docstring": "Download and parse the contents of a robots.txt file from a given URL or multiple URLs into a pandas DataFrame.\n\nParameters\n----------\nrobotstxt_url : str\n    A single URL or a list of URLs pointing to robots.txt files.\noutput_file : str, optional\n    An optional file path to save results as they are downloaded, particularly useful for large-scale downloads (must end with '.jl').\n\nReturns\n-------\nrobotstxt_df : pandas.DataFrame\n    A DataFrame containing directives, their content, the URL of the robots.txt file, and the timestamp of when it was downloaded.\n\nRaises\n------\nValueError\n    If `output_file` does not end with \".jl\" or if the provided `robotstxt_url` list is not in a supported format.\n\nDependencies\n------------\n- pandas: For creating and manipulating the DataFrame.\n- urllib: For downloading the robots.txt files from the URLs.\n- gzip: For handling potentially gzipped responses.\n- logging: For logging the download progress.\n\nConstants\n---------\nheaders : dict\n    A dictionary specifying the User-Agent header to be used during the fetching of robots.txt files, constructed using the module's version.\ngzip_start_bytes : bytes\n    A constant byte sequence indicating the start of a gzipped file, used to determine if decompression is necessary.\n\nThis function facilitates the analysis of robots.txt files, allowing users to download and efficiently handle multiple files for further processing, like SEO audits or competitive analysis.",
        "signature": "def robotstxt_to_df(robotstxt_url, output_file=None):",
        "type": "Function",
        "class_signature": null
      },
      "robotstxt_test": {
        "code": "def robotstxt_test(robotstxt_url, user_agents, urls):\n    \"\"\"Check which user agents are allowed to fetch specified URLs based on a given robots.txt file.\n\nParameters\n----------\nrobotstxt_url : str\n    The URL of the robots.txt file to be tested. It should end with '/robots.txt'.\nuser_agents : str or list\n    One or more user agents to test. If a string is provided, it will be converted to a list.\nurls : str or list\n    One or more relative paths or absolute URLs to check against the robots.txt rules.\n\nReturns\n-------\npandas.DataFrame\n    A DataFrame containing the test results, with columns for the robots.txt URL, user agent, URL path, and a boolean indicating whether the user agent can fetch the URL.\n\nNotes\n-----\nThis function relies on the `Protego` library to parse the robots.txt rules and determine access permissions. It uses the `product` function from `itertools` to create all combinations of user agents and URLs for testing. The resulting DataFrame is sorted by user agent and URL path for clarity.\n\nRaises\n------\nValueError\n    If the provided robots.txt URL does not end with '/robots.txt'.\n\nDependencies\n------------\nThe function imports `urlopen` from the `urllib.request` module and `DataFrame` from the `pandas` library. It also uses the `Protego` library for parsing robots.txt files.\n\nExample Usage\n-------------\n>>> robotstxt_test(\n...     \"https://facebook.com/robots.txt\",\n...     user_agents=[\"*\", \"Googlebot\", \"Applebot\"],\n...     urls=[\"/\", \"/bbc\", \"/groups\", \"/hashtag/\"],\n... )\"\"\"\n    'Given a :attr:`robotstxt_url` check which of the :attr:`user_agents` is\\n    allowed to fetch which of the :attr:`urls`.\\n\\n    All the combinations of :attr:`user_agents` and :attr:`urls` will be\\n    checked and the results returned in one DataFrame.\\n\\n    Parameters\\n    ----------\\n\\n    robotstxt_url : str\\n      The URL of robotx.txt file.\\n    user_agents : str, list\\n      One or more user agents.\\n    urls : str, list\\n      One or more paths (relative) or URLs (absolute) to check.\\n\\n    Returns\\n    -------\\n    robotstxt_test_df : pandas.DataFrame\\n      A DataFrame with the test results per user-agent/rule combination.\\n\\n    Examples\\n    --------\\n    >>> robotstxt_test(\\n    ...     \"https://facebook.com/robots.txt\",\\n    ...     user_agents=[\"*\", \"Googlebot\", \"Applebot\"],\\n    ...     urls=[\"/\", \"/bbc\", \"/groups\", \"/hashtag/\"],\\n    ... )\\n                          robotstxt_url user_agent   url_path  can_fetch\\n    0   https://facebook.com/robots.txt          *          /      False\\n    1   https://facebook.com/robots.txt          *       /bbc      False\\n    2   https://facebook.com/robots.txt          *    /groups      False\\n    3   https://facebook.com/robots.txt          *  /hashtag/      False\\n    4   https://facebook.com/robots.txt   Applebot          /       True\\n    5   https://facebook.com/robots.txt   Applebot       /bbc       True\\n    6   https://facebook.com/robots.txt   Applebot    /groups       True\\n    7   https://facebook.com/robots.txt   Applebot  /hashtag/      False\\n    8   https://facebook.com/robots.txt  Googlebot          /       True\\n    9   https://facebook.com/robots.txt  Googlebot       /bbc       True\\n    10  https://facebook.com/robots.txt  Googlebot    /groups       True\\n    11  https://facebook.com/robots.txt  Googlebot  /hashtag/      False\\n\\n    '\n    if not robotstxt_url.endswith('/robots.txt'):\n        raise ValueError('Please make sure you enter a valid robots.txt URL')\n    if isinstance(user_agents, str):\n        user_agents = [user_agents]\n    if isinstance(urls, str):\n        urls = [urls]\n    robots_open = urlopen(Request(robotstxt_url, headers=headers))\n    robots_bytes = robots_open.readlines()\n    robots_text = ''.join((line.decode() for line in robots_bytes))\n    rp = Protego.parse(robots_text)\n    test_list = []\n    for path, agent in product(urls, user_agents):\n        d = dict()\n        d['user_agent'] = agent\n        d['url_path'] = path\n        d['can_fetch'] = rp.can_fetch(path, agent)\n        test_list.append(d)\n    df = pd.DataFrame(test_list)\n    df.insert(0, 'robotstxt_url', robotstxt_url)\n    df = df.sort_values(['user_agent', 'url_path']).reset_index(drop=True)\n    return df",
        "docstring": "Check which user agents are allowed to fetch specified URLs based on a given robots.txt file.\n\nParameters\n----------\nrobotstxt_url : str\n    The URL of the robots.txt file to be tested. It should end with '/robots.txt'.\nuser_agents : str or list\n    One or more user agents to test. If a string is provided, it will be converted to a list.\nurls : str or list\n    One or more relative paths or absolute URLs to check against the robots.txt rules.\n\nReturns\n-------\npandas.DataFrame\n    A DataFrame containing the test results, with columns for the robots.txt URL, user agent, URL path, and a boolean indicating whether the user agent can fetch the URL.\n\nNotes\n-----\nThis function relies on the `Protego` library to parse the robots.txt rules and determine access permissions. It uses the `product` function from `itertools` to create all combinations of user agents and URLs for testing. The resulting DataFrame is sorted by user agent and URL path for clarity.\n\nRaises\n------\nValueError\n    If the provided robots.txt URL does not end with '/robots.txt'.\n\nDependencies\n------------\nThe function imports `urlopen` from the `urllib.request` module and `DataFrame` from the `pandas` library. It also uses the `Protego` library for parsing robots.txt files.\n\nExample Usage\n-------------\n>>> robotstxt_test(\n...     \"https://facebook.com/robots.txt\",\n...     user_agents=[\"*\", \"Googlebot\", \"Applebot\"],\n...     urls=[\"/\", \"/bbc\", \"/groups\", \"/hashtag/\"],\n... )",
        "signature": "def robotstxt_test(robotstxt_url, user_agents, urls):",
        "type": "Function",
        "class_signature": null
      }
    }
  },
  "dependency_dict": {
    "advertools/robotstxt.py:robotstxt_to_df": {
      "advertools/robotstxt.py": {
        "_robots_multi": {
          "code": "def _robots_multi(robots_url_list, output_file=None):\n    final_df = pd.DataFrame()\n    with futures.ThreadPoolExecutor(max_workers=24) as executor:\n        to_do = []\n        for robotsurl in robots_url_list:\n            future = executor.submit(robotstxt_to_df, robotsurl)\n            to_do.append(future)\n        done_iter = futures.as_completed(to_do)\n        for future in done_iter:\n            future_result = future.result()\n            if output_file is not None:\n                with open(output_file, 'a') as file:\n                    file.write(future_result.to_json(orient='records', lines=True, date_format='iso'))\n                    file.write('\\n')\n            else:\n                final_df = pd.concat([final_df, future_result], ignore_index=True)\n    if output_file is None:\n        return final_df",
          "docstring": "",
          "signature": "def _robots_multi(robots_url_list, output_file=None):",
          "type": "Function",
          "class_signature": null
        }
      }
    }
  },
  "PRD": "# PROJECT NAME: advertools-test_robotstxt\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 advertools/\n    \u2514\u2500\u2500 robotstxt.py\n        \u251c\u2500\u2500 robotstxt_test\n        \u2514\u2500\u2500 robotstxt_to_df\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThis module facilitates the parsing and analysis of `robots.txt` files, providing features for converting them into structured data formats, such as dataframes, for streamlined processing. It enables users to retrieve and examine directives, content, and associated metadata, while offering functionality to test access permissions for specific user agents and URLs against `robots.txt` rules. Additionally, it allows seamless handling of multiple `robots.txt` sources, supports error detection, and provides options for saving the extracted information to output files for further use. This module addresses the challenge developers face in programmatically interpreting and validating `robots.txt` directives, thereby improving the efficiency of managing web crawling and ensuring adherence to site-specific access policies.\n\n## FILE 1: advertools/robotstxt.py\n\n- FUNCTION NAME: robotstxt_to_df\n  - SIGNATURE: def robotstxt_to_df(robotstxt_url, output_file=None):\n  - DOCSTRING: \n```python\n\"\"\"\nDownload and parse the contents of a robots.txt file from a given URL or multiple URLs into a pandas DataFrame.\n\nParameters\n----------\nrobotstxt_url : str\n    A single URL or a list of URLs pointing to robots.txt files.\noutput_file : str, optional\n    An optional file path to save results as they are downloaded, particularly useful for large-scale downloads (must end with '.jl').\n\nReturns\n-------\nrobotstxt_df : pandas.DataFrame\n    A DataFrame containing directives, their content, the URL of the robots.txt file, and the timestamp of when it was downloaded.\n\nRaises\n------\nValueError\n    If `output_file` does not end with \".jl\" or if the provided `robotstxt_url` list is not in a supported format.\n\nDependencies\n------------\n- pandas: For creating and manipulating the DataFrame.\n- urllib: For downloading the robots.txt files from the URLs.\n- gzip: For handling potentially gzipped responses.\n- logging: For logging the download progress.\n\nConstants\n---------\nheaders : dict\n    A dictionary specifying the User-Agent header to be used during the fetching of robots.txt files, constructed using the module's version.\ngzip_start_bytes : bytes\n    A constant byte sequence indicating the start of a gzipped file, used to determine if decompression is necessary.\n\nThis function facilitates the analysis of robots.txt files, allowing users to download and efficiently handle multiple files for further processing, like SEO audits or competitive analysis.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - advertools/robotstxt.py:_robots_multi\n\n- FUNCTION NAME: robotstxt_test\n  - SIGNATURE: def robotstxt_test(robotstxt_url, user_agents, urls):\n  - DOCSTRING: \n```python\n\"\"\"\nCheck which user agents are allowed to fetch specified URLs based on a given robots.txt file.\n\nParameters\n----------\nrobotstxt_url : str\n    The URL of the robots.txt file to be tested. It should end with '/robots.txt'.\nuser_agents : str or list\n    One or more user agents to test. If a string is provided, it will be converted to a list.\nurls : str or list\n    One or more relative paths or absolute URLs to check against the robots.txt rules.\n\nReturns\n-------\npandas.DataFrame\n    A DataFrame containing the test results, with columns for the robots.txt URL, user agent, URL path, and a boolean indicating whether the user agent can fetch the URL.\n\nNotes\n-----\nThis function relies on the `Protego` library to parse the robots.txt rules and determine access permissions. It uses the `product` function from `itertools` to create all combinations of user agents and URLs for testing. The resulting DataFrame is sorted by user agent and URL path for clarity.\n\nRaises\n------\nValueError\n    If the provided robots.txt URL does not end with '/robots.txt'.\n\nDependencies\n------------\nThe function imports `urlopen` from the `urllib.request` module and `DataFrame` from the `pandas` library. It also uses the `Protego` library for parsing robots.txt files.\n\nExample Usage\n-------------\n>>> robotstxt_test(\n...     \"https://facebook.com/robots.txt\",\n...     user_agents=[\"*\", \"Googlebot\", \"Applebot\"],\n...     urls=[\"/\", \"/bbc\", \"/groups\", \"/hashtag/\"],\n... )\n\"\"\"\n```\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "advertools/robotstxt.py": "\"\"\"\n.. _robotstxt:\n\n\ud83e\udd16 Analyze and Test robots.txt Files on a Large Scale\n=====================================================\n\nEven though they are tiny in size, robots.txt files contain potent instructions\nthat can block major sections of your site, which is what they are supposed to\ndo. Only sometimes you might make the mistake of blocking the wrong section.\n\nSo it is very important to check if certain pages (or groups of pages) are\nblocked for a certain user-agent by a certain robots.txt file. Ideally, you\nwould want to run the same check for all possible user-agents. Even more\nideally, you want to be able to run the check for a large number of pages with\nevery possible combination with user-agents.\n\nTo get the robots.txt file into an easily readable format, you can use the\n:func:`robotstxt_to_df` function to get it in a DataFrame.\n\n.. thebe-button::\n    Run this code\n\n.. code-block::\n    :class: thebe, thebe-init\n\n    import advertools as adv\n\n    amazon = adv.robotstxt_to_df('https://www.amazon.com/robots.txt')\n    amazon\n\n====  ===========  =================================  ==================================  =========================  =================================  ================================\n  ..  directive    content                            etag                                robotstxt_last_modified    robotstxt_url                      download_date\n====  ===========  =================================  ==================================  =========================  =================================  ================================\n   0  User-agent   \\\\*                                 \"a850165d925db701988daf7ead7492d3\"  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00\n   1  Disallow     /exec/obidos/account-access-login  \"a850165d925db701988daf7ead7492d3\"  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00\n   2  Disallow     /exec/obidos/change-style          \"a850165d925db701988daf7ead7492d3\"  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00\n   3  Disallow     /exec/obidos/flex-sign-in          \"a850165d925db701988daf7ead7492d3\"  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00\n   4  Disallow     /exec/obidos/handle-buy-box        \"a850165d925db701988daf7ead7492d3\"  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00\n ...  ...          ...                                ...                                 ...                        ...                                ...\n 146  Disallow     /hp/video/mystuff                  \"a850165d925db701988daf7ead7492d3\"  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00\n 147  Disallow     /gp/video/profiles                 \"a850165d925db701988daf7ead7492d3\"  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00\n 148  Disallow     /hp/video/profiles                 \"a850165d925db701988daf7ead7492d3\"  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00\n 149  User-agent   EtaoSpider                         \"a850165d925db701988daf7ead7492d3\"  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00\n 150  Disallow     /                                  \"a850165d925db701988daf7ead7492d3\"  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00\n====  ===========  =================================  ==================================  =========================  =================================  ================================\n\nThe returned DataFrame contains columns for directives, their content, the URL\nof the robots.txt file, as well as the date it was downloaded.\n\n*  `directive`: The main commands. Allow, Disallow, Sitemap, Crawl-delay,\n   User-agent, and so on.\n*  `content`: The details of each of the directives.\n*  `robotstxt_last_modified`: The date when the robots.txt file was last\n   modified, if provided (according the response header Last-modified).\n*  `etag`: The entity tag of the response header, if provided.\n*  `robotstxt_url`: The URL of the robots.txt file.\n*  `download_date`: The date and time when the file was downloaded.\n\nAlternatively, you can provide a list of robots URLs if you want to download\nthem all in one go. This might be interesting if:\n\n* You are analyzing an industry and want to keep an eye on many different\n  websites.\n* You are analyzing a website with many sub-domains, and want to get all the\n  robots files together.\n* You are trying to understand a company that has many websites under different\n  domains and sub-domains.\n\nIn this case you simply provide a list of URLs instead of a single one.\n\n.. thebe-button::\n    Run this code\n\n.. code-block::\n    :class: thebe, thebe-init\n\n    robots_urls = ['https://www.google.com/robots.txt',\n                   'https://twitter.com/robots.txt',\n                   'https://facebook.com/robots.txt']\n\n    googtwfb = adv.robotstxt_to_df(robots_urls)\n\n    # How many lines does each robots file have?\n    googtwfb.groupby('robotstxt_url')['directive'].count()\n\n.. code-block::\n\n    robotstxt_url\n    https://facebook.com/robots.txt      541\n    https://twitter.com/robots.txt       108\n    https://www.google.com/robots.txt    289\n    Name: directive, dtype: int64\n\n.. code-block::\n    :class: thebe, thebe-init\n\n    # Display the first five rows of each of the robots files:\n    googtwfb.groupby('robotstxt_url').head()\n\n====  ===========  ===================================================================  =========================  =================================  ================================\n  ..  directive    content                                                              robotstxt_last_modified    robotstxt_url                      download_date\n====  ===========  ===================================================================  =========================  =================================  ================================\n   0  User-agent   \\\\*                                                                   2022-02-07 22:30:00+00:00  https://www.google.com/robots.txt  2022-02-11 19:52:13.375724+00:00\n   1  Disallow     /search                                                              2022-02-07 22:30:00+00:00  https://www.google.com/robots.txt  2022-02-11 19:52:13.375724+00:00\n   2  Allow        /search/about                                                        2022-02-07 22:30:00+00:00  https://www.google.com/robots.txt  2022-02-11 19:52:13.375724+00:00\n   3  Allow        /search/static                                                       2022-02-07 22:30:00+00:00  https://www.google.com/robots.txt  2022-02-11 19:52:13.375724+00:00\n   4  Allow        /search/howsearchworks                                               2022-02-07 22:30:00+00:00  https://www.google.com/robots.txt  2022-02-11 19:52:13.375724+00:00\n 289  comment      Google Search Engine Robot                                           NaT                        https://twitter.com/robots.txt     2022-02-11 19:52:13.461815+00:00\n 290  comment      \\\\==========================                                           NaT                        https://twitter.com/robots.txt     2022-02-11 19:52:13.461815+00:00\n 291  User-agent   Googlebot                                                            NaT                        https://twitter.com/robots.txt     2022-02-11 19:52:13.461815+00:00\n 292  Allow        /?_escaped_fragment_                                                 NaT                        https://twitter.com/robots.txt     2022-02-11 19:52:13.461815+00:00\n 293  Allow        /\\\\*?lang=                                                            NaT                        https://twitter.com/robots.txt     2022-02-11 19:52:13.461815+00:00\n 397  comment      Notice: Collection of data on Facebook through automated means is    NaT                        https://facebook.com/robots.txt    2022-02-11 19:52:13.474456+00:00\n 398  comment      prohibited unless you have express written permission from Facebook  NaT                        https://facebook.com/robots.txt    2022-02-11 19:52:13.474456+00:00\n 399  comment      and may only be conducted for the limited purpose contained in said  NaT                        https://facebook.com/robots.txt    2022-02-11 19:52:13.474456+00:00\n 400  comment      permission.                                                          NaT                        https://facebook.com/robots.txt    2022-02-11 19:52:13.474456+00:00\n 401  comment      See: http://www.facebook.com/apps/site_scraping_tos_terms.php        NaT                        https://facebook.com/robots.txt    2022-02-11 19:52:13.474456+00:00\n====  ===========  ===================================================================  =========================  =================================  ================================\n\nBulk ``robots.txt`` Tester\n--------------------------\n\nThis tester is designed to work on a large scale. The :func:`robotstxt_test`\nfunction runs a test for a given robots.txt file, checking which of the\nprovided user-agents can fetch which of the provided URLs, paths, or patterns.\n\n\n.. thebe-button::\n    Run this code\n\n.. code-block::\n    :class: thebe, thebe-init\n\n    import advertools as adv\n    adv.robotstxt_test(\n        robotstxt_url='https://www.amazon.com/robots.txt',\n        user_agents=['Googlebot', 'baiduspider', 'Bingbot'],\n        urls=['/', '/hello', '/some-page.html'])\n\nAs a result, you get a DataFrame with a row for each combination of\n(user-agent, URL) indicating whether or not that particular user-agent can\nfetch the given URL.\n\nSome reasons why you might want to do that:\n\n* SEO Audits: Especially for large websites with many URL patterns, and many\n  rules for different user-agents.\n* Developer or site owner about to make large changes\n* Interest in strategies of certain companies\n\nUser-agents\n-----------\n\nIn reality there are only two groups of user-agents that you need to worry\nabout:\n\n* User-agents listed in the robots.txt file: For each one of those you need to\n  check whether or not they are blocked from fetching a certain URL\n  (or pattern).\n* ``*`` all other user-agents: The ``*`` includes all other user-agents, so\n  checking the rules that apply to it should take care of the rest.\n\nrobots.txt Testing Approach\n---------------------------\n\n1. Get the robots.txt file that you are interested in\n2. Extract the user-agents from it\n3. Specify the URLs you are interested in testing\n4. Run the :func:`robotstxt_test` function\n\n.. thebe-button::\n    Run this code\n\n.. code-block::\n    :class: thebe, thebe-init\n\n    fb_robots = adv.robotstxt_to_df('https://www.facebook.com/robots.txt')\n    fb_robots\n\n====  ===========  ===================================================================  ===================================  ================================\n  ..  directive    content                                                              robotstxt_url                        download_date\n====  ===========  ===================================================================  ===================================  ================================\n   0  comment      Notice: Collection of data on Facebook through automated means is    https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00\n   1  comment      prohibited unless you have express written permission from Facebook  https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00\n   2  comment      and may only be conducted for the limited purpose contained in said  https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00\n   3  comment      permission.                                                          https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00\n   4  comment      See: http://www.facebook.com/apps/site_scraping_tos_terms.php        https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00\n ...  ...          ...                                                                  ...                                  ...\n 536  Allow        /ajax/pagelet/generic.php/PagePostsSectionPagelet                    https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00\n 537  Allow        /careers/                                                            https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00\n 538  Allow        /safetycheck/                                                        https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00\n 539  User-agent   *                                                                    https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00\n 540  Disallow     /                                                                    https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00\n====  ===========  ===================================================================  ===================================  ================================\n\n\nNow that we have downloaded the file, we can easily extract the list of\nuser-agents that it contains.\n\n.. thebe-button::\n    Run this code\n\n.. code-block::\n    :class: thebe, thebe-init\n\n    fb_useragents = (fb_robots\n                     [fb_robots['directive']=='User-agent']\n                     ['content'].drop_duplicates()\n                    .tolist())\n    fb_useragents\n\n.. code-block::\n\n    ['Applebot',\n     'baiduspider',\n     'Bingbot',\n     'Discordbot',\n     'facebookexternalhit',\n     'Googlebot',\n     'Googlebot-Image',\n     'ia_archiver',\n     'LinkedInBot',\n     'msnbot',\n     'Naverbot',\n     'Pinterestbot',\n     'seznambot',\n     'Slurp',\n     'teoma',\n     'TelegramBot',\n     'Twitterbot',\n     'Yandex',\n     'Yeti',\n     '*']\n\nQuite a long list!\n\nAs a small and quick test, I'm interested in checking the home page, a random\nprofile page (/bbc), groups and hashtags pages.\n\n.. thebe-button::\n    Run this code\n\n.. code-block::\n    :class: thebe, thebe-init\n\n    urls_to_test = ['/', '/bbc', '/groups', '/hashtag/']\n    fb_test = robotstxt_test('https://www.facebook.com/robots.txt',\n                             fb_useragents, urls_to_test)\n    fb_test\n\n====  ===================================  ============  ==========  ===========\n  ..  robotstxt_url                        user_agent    url_path    can_fetch\n====  ===================================  ============  ==========  ===========\n   0  https://www.facebook.com/robots.txt  \\\\*            /           False\n   1  https://www.facebook.com/robots.txt  \\\\*            /bbc        False\n   2  https://www.facebook.com/robots.txt  \\\\*            /groups     False\n   3  https://www.facebook.com/robots.txt  \\\\*            /hashtag/   False\n   4  https://www.facebook.com/robots.txt  Applebot      /           True\n  ..                                  ...           ...         ...          ...\n  75  https://www.facebook.com/robots.txt  seznambot     /hashtag/   True\n  76  https://www.facebook.com/robots.txt  teoma         /           True\n  77  https://www.facebook.com/robots.txt  teoma         /bbc        True\n  78  https://www.facebook.com/robots.txt  teoma         /groups     True\n  79  https://www.facebook.com/robots.txt  teoma         /hashtag/   True\n====  ===================================  ============  ==========  ===========\n\nFor twenty user-agents and four URLs each, we received a total of eighty test\nresults. You can immediately see that all user-agents not listed (denoted by\n`*` are not allowed to fetch any of the provided URLs).\n\nLet's see who is and who is not allowed to fetch the home page.\n\n.. code-block::\n\n    fb_test.query('url_path== \"/\"')\n\n====  ===================================  ===================  ==========  ===========\n  ..  robotstxt_url                        user_agent           url_path    can_fetch\n====  ===================================  ===================  ==========  ===========\n   0  https://www.facebook.com/robots.txt  \\\\*                   /           False\n   4  https://www.facebook.com/robots.txt  Applebot             /           True\n   8  https://www.facebook.com/robots.txt  Bingbot              /           True\n  12  https://www.facebook.com/robots.txt  Discordbot           /           False\n  16  https://www.facebook.com/robots.txt  Googlebot            /           True\n  20  https://www.facebook.com/robots.txt  Googlebot-Image      /           True\n  24  https://www.facebook.com/robots.txt  LinkedInBot          /           False\n  28  https://www.facebook.com/robots.txt  Naverbot             /           True\n  32  https://www.facebook.com/robots.txt  Pinterestbot         /           False\n  36  https://www.facebook.com/robots.txt  Slurp                /           True\n  40  https://www.facebook.com/robots.txt  TelegramBot          /           False\n  44  https://www.facebook.com/robots.txt  Twitterbot           /           True\n  48  https://www.facebook.com/robots.txt  Yandex               /           True\n  52  https://www.facebook.com/robots.txt  Yeti                 /           True\n  56  https://www.facebook.com/robots.txt  baiduspider          /           True\n  60  https://www.facebook.com/robots.txt  facebookexternalhit  /           False\n  64  https://www.facebook.com/robots.txt  ia_archiver          /           False\n  68  https://www.facebook.com/robots.txt  msnbot               /           True\n  72  https://www.facebook.com/robots.txt  seznambot            /           True\n  76  https://www.facebook.com/robots.txt  teoma                /           True\n====  ===================================  ===================  ==========  ===========\n\nI'll leave it to you to figure out why LinkedIn and Pinterest are not allowed\nto crawl the home page but Google and Apple are, because I have no clue!\n\"\"\"\n__all__ = ['robotstxt_to_df', 'robotstxt_test']\nimport gzip\nimport logging\nfrom concurrent import futures\nfrom itertools import product\nfrom urllib.request import Request, urlopen\nimport pandas as pd\nfrom protego import Protego\nfrom advertools import __version__ as version\nheaders = {'User-Agent': 'advertools-' + version}\ngzip_start_bytes = b'\\x1f\\x8b'\nlogging.basicConfig(level=logging.INFO)\n\ndef _robots_multi(robots_url_list, output_file=None):\n    final_df = pd.DataFrame()\n    with futures.ThreadPoolExecutor(max_workers=24) as executor:\n        to_do = []\n        for robotsurl in robots_url_list:\n            future = executor.submit(robotstxt_to_df, robotsurl)\n            to_do.append(future)\n        done_iter = futures.as_completed(to_do)\n        for future in done_iter:\n            future_result = future.result()\n            if output_file is not None:\n                with open(output_file, 'a') as file:\n                    file.write(future_result.to_json(orient='records', lines=True, date_format='iso'))\n                    file.write('\\n')\n            else:\n                final_df = pd.concat([final_df, future_result], ignore_index=True)\n    if output_file is None:\n        return final_df"
  },
  "call_tree": {
    "tests/test_robotstxt.py:test_robotstxt_to_df": {
      "advertools/robotstxt.py:robotstxt_to_df": {}
    },
    "tests/test_robotstxt.py:test_robtostxt_to_df_handles_list": {
      "advertools/robotstxt.py:robotstxt_to_df": {
        "advertools/robotstxt.py:_robots_multi": {}
      }
    },
    "tests/test_robotstxt.py:test_robotstxt_to_df_saves_single_file": {
      "advertools/robotstxt.py:robotstxt_to_df": {}
    },
    "tests/test_robotstxt.py:test_robotstxt_to_df_saves_file_list": {
      "advertools/robotstxt.py:robotstxt_to_df": {
        "advertools/robotstxt.py:_robots_multi": {}
      }
    },
    "tests/test_robotstxt.py:test_robotstxt_to_df_raises_on_wrong_file": {
      "advertools/robotstxt.py:robotstxt_to_df": {}
    },
    "tests/test_robotstxt.py:test_robotstxt_to_df_contains_errors": {
      "advertools/robotstxt.py:robotstxt_to_df": {}
    },
    "tests/test_robotstxt.py:test_robotstxt_test": {
      "advertools/robotstxt.py:robotstxt_test": {}
    },
    "tests/test_robotstxt.py:test_robotstxt_raises": {
      "advertools/robotstxt.py:robotstxt_test": {}
    },
    "tests/test_robotstxt.py:test_robots_converts_str_to_list": {
      "advertools/robotstxt.py:robotstxt_test": {}
    }
  }
}