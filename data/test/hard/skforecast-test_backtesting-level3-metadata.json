{
  "dir_path": "/app/skforecast",
  "package_name": "skforecast",
  "sample_name": "skforecast-test_backtesting",
  "src_dir": "skforecast/",
  "test_dir": "tests/",
  "test_file": "skforecast/recursive/tests/tests_forecaster_equivalent_date/test_backtesting.py",
  "test_code": "# Unit test backtesting ForecasterEquivalentDate\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom skforecast.recursive import ForecasterEquivalentDate\nfrom skforecast.model_selection._validation import backtesting_forecaster\nfrom skforecast.model_selection._split import TimeSeriesFold\n\n# Fixtures\nfrom .fixtures_forecaster_equivalent_date import y\n\n\ndef test_backtesting_with_ForecasterEquivalentDate():\n    \"\"\"\n    Test backtesting with ForecasterEquivalentDate.\n    \"\"\"\n    forecaster = ForecasterEquivalentDate(\n                     offset    = pd.DateOffset(days=10),\n                     n_offsets = 2 \n                 )\n    cv = TimeSeriesFold(\n        initial_train_size = 30,\n        steps              = 5,\n        refit              = True,\n    )\n\n    metric, predictions = backtesting_forecaster(\n        forecaster = forecaster,\n        y          = y,\n        cv         = cv,\n        metric     = 'mean_absolute_error',\n        verbose    = False,\n        n_jobs     = 'auto'\n    )\n    expected_metric = pd.DataFrame({'mean_absolute_error': [0.2537094475]})\n    expected = pd.DataFrame(\n        data    = np.array([0.48878949, 0.78924075, 0.58151378, 0.3353507, 0.56024382,\n                            0.53047716, 0.27214019, 0.20185749, 0.41263271, 0.58140185,\n                            0.36325295, 0.64156648, 0.57765904, 0.5523543, 0.57413684,\n                            0.31761006, 0.39406999, 0.56082619, 0.61893703, 0.5664064]),\n        index   = pd.date_range(start='2000-01-31', periods=20, freq='D'),\n        columns = ['pred']\n    )\n\n    pd.testing.assert_frame_equal(metric, expected_metric)\n    pd.testing.assert_frame_equal(predictions, expected)\n",
  "GT_file_code": {
    "skforecast/model_selection/_validation.py": "################################################################################\n#                  skforecast.model_selection._validation                      #\n#                                                                              #\n# This work by skforecast team is licensed under the BSD 3-Clause License.     #\n################################################################################\n# coding=utf-8\n\nimport re\nfrom copy import deepcopy\nfrom typing import Union, Tuple, Optional, Callable\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom joblib import Parallel, delayed, cpu_count\nfrom tqdm.auto import tqdm\nfrom ..metrics import add_y_train_argument, _get_metric\nfrom ..exceptions import LongTrainingWarning, IgnoredArgumentWarning\nfrom ..model_selection._split import TimeSeriesFold\nfrom ..model_selection._utils import (\n    _initialize_levels_model_selection_multiseries,\n    check_backtesting_input,\n    select_n_jobs_backtesting,\n    _extract_data_folds_multiseries,\n    _calculate_metrics_backtesting_multiseries\n)\nfrom ..utils import set_skforecast_warnings\n\n\ndef _backtesting_forecaster(\n    forecaster: object,\n    y: pd.Series,\n    metric: Union[str, Callable, list],\n    cv: TimeSeriesFold,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    interval: Optional[list] = None,\n    n_boot: int = 250,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = False,\n    n_jobs: Union[int, str] = 'auto',\n    verbose: bool = False,\n    show_progress: bool = True\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Backtesting of forecaster model following the folds generated by the TimeSeriesFold\n    class and using the metric(s) provided.\n\n    If `forecaster` is already trained and `initial_train_size` is set to `None` in the\n    TimeSeriesFold class, no initial train will be done and all data will be used\n    to evaluate the model. However, the first `len(forecaster.last_window)` observations\n    are needed to create the initial predictors, so no predictions are calculated for\n    them.\n    \n    A copy of the original forecaster is created so that it is not modified during the\n    process.\n    \n    Parameters\n    ----------\n    forecaster : ForecasterRecursive, ForecasterDirect\n        Forecaster model.\n    y : pandas Series\n        Training time series.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n        \n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    cv : TimeSeriesFold\n        TimeSeriesFold object with the information needed to split the data into folds.\n        **New in version 0.14.0**\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. For example, \n        interval of 95% should be as `interval = [2.5, 97.5]`. If `None`, no\n        intervals are estimated.\n    n_boot : int, default `250`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of prediction \n        error to create prediction intervals. If `False`, out_sample_residuals \n        are used if they are already stored inside the forecaster.\n    use_binned_residuals : bool, default `False`\n        If `True`, residuals used in each bootstrapping iteration are selected\n        conditioning on the predicted values. If `False`, residuals are selected\n        randomly without conditioning on the predicted values.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n    verbose : bool, default `False`\n        Print number of folds and index of training and validation sets used \n        for backtesting.\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n\n    Returns\n    -------\n    metric_values : pandas DataFrame\n        Value(s) of the metric(s).\n    backtest_predictions : pandas DataFrame\n        Value of predictions and their estimated interval if `interval` is not `None`.\n\n        - column pred: predictions.\n        - column lower_bound: lower bound of the interval.\n        - column upper_bound: upper bound of the interval.\n    \n    \"\"\"\n\n    forecaster = deepcopy(forecaster)\n    cv = deepcopy(cv)\n\n    cv.set_params({\n        'window_size': forecaster.window_size,\n        'differentiation': forecaster.differentiation,\n        'return_all_indexes': False,\n        'verbose': verbose\n    })\n\n    initial_train_size = cv.initial_train_size\n    refit = cv.refit\n    gap = cv.gap\n    \n    if n_jobs == 'auto':\n        n_jobs = select_n_jobs_backtesting(\n                     forecaster = forecaster,\n                     refit      = refit\n                 )\n    elif not isinstance(refit, bool) and refit != 1 and n_jobs != 1:\n        warnings.warn(\n            \"If `refit` is an integer other than 1 (intermittent refit). `n_jobs` \"\n            \"is set to 1 to avoid unexpected results during parallelization.\",\n            IgnoredArgumentWarning\n        )\n        n_jobs = 1\n    else:\n        n_jobs = n_jobs if n_jobs > 0 else cpu_count()\n\n    if not isinstance(metric, list):\n        metrics = [\n            _get_metric(metric=metric)\n            if isinstance(metric, str)\n            else add_y_train_argument(metric)\n        ]\n    else:\n        metrics = [\n            _get_metric(metric=m)\n            if isinstance(m, str)\n            else add_y_train_argument(m) \n            for m in metric\n        ]\n\n    store_in_sample_residuals = False if interval is None else True\n\n    folds = cv.split(X=y, as_pandas=False)\n    window_size = cv.window_size\n\n    if initial_train_size is not None:\n        # First model training, this is done to allow parallelization when `refit`\n        # is `False`. The initial Forecaster fit is outside the auxiliary function.\n        exog_train = exog.iloc[:initial_train_size, ] if exog is not None else None\n        forecaster.fit(\n            y                         = y.iloc[:initial_train_size, ],\n            exog                      = exog_train,\n            store_in_sample_residuals = store_in_sample_residuals\n        )\n        # This is done to allow parallelization when `refit` is `False`. The initial \n        # Forecaster fit is outside the auxiliary function.\n        folds[0][4] = False\n\n    if refit:\n        n_of_fits = int(len(folds) / refit)\n        if type(forecaster).__name__ != 'ForecasterDirect' and n_of_fits > 50:\n            warnings.warn(\n                (f\"The forecaster will be fit {n_of_fits} times. This can take substantial\"\n                 f\" amounts of time. If not feasible, try with `refit = False`.\\n\"),\n                LongTrainingWarning\n            )\n        elif type(forecaster).__name__ == 'ForecasterDirect' and n_of_fits * forecaster.steps > 50:\n            warnings.warn(\n                (f\"The forecaster will be fit {n_of_fits * forecaster.steps} times \"\n                 f\"({n_of_fits} folds * {forecaster.steps} regressors). This can take \"\n                 f\"substantial amounts of time. If not feasible, try with `refit = False`.\\n\"),\n                LongTrainingWarning\n            )\n\n    if show_progress:\n        folds = tqdm(folds)\n\n    def _fit_predict_forecaster(y, exog, forecaster, interval, fold, gap):\n        \"\"\"\n        Fit the forecaster and predict `steps` ahead. This is an auxiliary \n        function used to parallelize the backtesting_forecaster function.\n        \"\"\"\n\n        train_iloc_start       = fold[0][0]\n        train_iloc_end         = fold[0][1]\n        last_window_iloc_start = fold[1][0]\n        last_window_iloc_end   = fold[1][1]\n        test_iloc_start        = fold[2][0]\n        test_iloc_end          = fold[2][1]\n\n        if fold[4] is False:\n            # When the model is not fitted, last_window must be updated to include\n            # the data needed to make predictions.\n            last_window_y = y.iloc[last_window_iloc_start:last_window_iloc_end]\n        else:\n            # The model is fitted before making predictions. If `fixed_train_size`\n            # the train size doesn't increase but moves by `steps` in each iteration.\n            # If `False` the train size increases by `steps` in each iteration.\n            y_train = y.iloc[train_iloc_start:train_iloc_end, ]\n            exog_train = (\n                exog.iloc[train_iloc_start:train_iloc_end,] if exog is not None else None\n            )\n            last_window_y = None\n            forecaster.fit(\n                y                         = y_train, \n                exog                      = exog_train, \n                store_in_sample_residuals = store_in_sample_residuals\n            )\n\n        next_window_exog = exog.iloc[test_iloc_start:test_iloc_end, ] if exog is not None else None\n\n        steps = len(range(test_iloc_start, test_iloc_end))\n        if type(forecaster).__name__ == 'ForecasterDirect' and gap > 0:\n            # Select only the steps that need to be predicted if gap > 0\n            test_no_gap_iloc_start = fold[3][0]\n            test_no_gap_iloc_end   = fold[3][1]\n            steps = list(\n                np.arange(len(range(test_no_gap_iloc_start, test_no_gap_iloc_end)))\n                + gap\n                + 1\n            )\n\n        if interval is None:\n            pred = forecaster.predict(\n                       steps       = steps,\n                       last_window = last_window_y,\n                       exog        = next_window_exog\n                   )\n        else:\n            pred = forecaster.predict_interval(\n                       steps                   = steps,\n                       last_window             = last_window_y,\n                       exog                    = next_window_exog,\n                       interval                = interval,\n                       n_boot                  = n_boot,\n                       random_state            = random_state,\n                       use_in_sample_residuals = use_in_sample_residuals,\n                       use_binned_residuals    = use_binned_residuals,\n                   )\n\n        if type(forecaster).__name__ != 'ForecasterDirect' and gap > 0:\n            pred = pred.iloc[gap:, ]\n\n        return pred\n\n    backtest_predictions = (\n        Parallel(n_jobs=n_jobs)\n        (delayed(_fit_predict_forecaster)\n        (y=y, exog=exog, forecaster=forecaster, interval=interval, fold=fold, gap=gap)\n         for fold in folds)\n    )\n\n    backtest_predictions = pd.concat(backtest_predictions)\n    if isinstance(backtest_predictions, pd.Series):\n        backtest_predictions = pd.DataFrame(backtest_predictions)\n\n    train_indexes = []\n    for i, fold in enumerate(folds):\n        fit_fold = fold[-1]\n        if i == 0 or fit_fold:\n            train_iloc_start = fold[0][0] + window_size  # Exclude observations used to create predictors\n            train_iloc_end = fold[0][1]\n            train_indexes.append(np.arange(train_iloc_start, train_iloc_end))\n    \n    train_indexes = np.unique(np.concatenate(train_indexes))\n    y_train = y.iloc[train_indexes]\n\n    metric_values = [\n        m(\n            y_true = y.loc[backtest_predictions.index],\n            y_pred = backtest_predictions['pred'],\n            y_train = y_train\n        ) \n        for m in metrics\n    ]\n\n    metric_values = pd.DataFrame(\n        data    = [metric_values],\n        columns = [m.__name__ for m in metrics]\n    )\n    \n    return metric_values, backtest_predictions\n\n\ndef backtesting_forecaster(\n    forecaster: object,\n    y: pd.Series,\n    cv: TimeSeriesFold,\n    metric: Union[str, Callable, list],\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    interval: Optional[list] = None,\n    n_boot: int = 250,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = False,\n    n_jobs: Union[int, str] = 'auto',\n    verbose: bool = False,\n    show_progress: bool = True\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Backtesting of forecaster model following the folds generated by the TimeSeriesFold\n    class and using the metric(s) provided.\n\n    If `forecaster` is already trained and `initial_train_size` is set to `None` in the\n    TimeSeriesFold class, no initial train will be done and all data will be used\n    to evaluate the model. However, the first `len(forecaster.last_window)` observations\n    are needed to create the initial predictors, so no predictions are calculated for\n    them.\n    \n    A copy of the original forecaster is created so that it is not modified during \n    the process.\n\n    Parameters\n    ----------\n    forecaster : ForecasterRecursive, ForecasterDirect\n        Forecaster model.\n    y : pandas Series\n        Training time series.\n    cv : TimeSeriesFold\n        TimeSeriesFold object with the information needed to split the data into folds.\n        **New in version 0.14.0**\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n        \n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. For example, \n        interval of 95% should be as `interval = [2.5, 97.5]`. If `None`, no\n        intervals are estimated.\n    n_boot : int, default `250`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of prediction \n        error to create prediction intervals. If `False`, out_sample_residuals \n        are used if they are already stored inside the forecaster.\n    use_binned_residuals : bool, default `False`\n        If `True`, residuals used in each bootstrapping iteration are selected\n        conditioning on the predicted values. If `False`, residuals are selected\n        randomly without conditioning on the predicted values.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n    verbose : bool, default `False`\n        Print number of folds and index of training and validation sets used \n        for backtesting.\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n\n    Returns\n    -------\n    metric_values : pandas DataFrame\n        Value(s) of the metric(s).\n    backtest_predictions : pandas DataFrame\n        Value of predictions and their estimated interval if `interval` is not `None`.\n\n        - column pred: predictions.\n        - column lower_bound: lower bound of the interval.\n        - column upper_bound: upper bound of the interval.\n    \n    \"\"\"\n\n    forecaters_allowed = [\n        'ForecasterRecursive', \n        'ForecasterDirect',\n        'ForecasterEquivalentDate'\n    ]\n    \n    if type(forecaster).__name__ not in forecaters_allowed:\n        raise TypeError(\n            (f\"`forecaster` must be of type {forecaters_allowed}, for all other types of \"\n             f\" forecasters use the functions available in the other `model_selection` \"\n             f\"modules.\")\n        )\n    \n    check_backtesting_input(\n        forecaster              = forecaster,\n        cv                      = cv,\n        y                       = y,\n        metric                  = metric,\n        interval                = interval,\n        n_boot                  = n_boot,\n        random_state            = random_state,\n        use_in_sample_residuals = use_in_sample_residuals,\n        use_binned_residuals    = use_binned_residuals,\n        n_jobs                  = n_jobs,\n        show_progress           = show_progress\n    )\n    \n    if type(forecaster).__name__ == 'ForecasterDirect' and \\\n       forecaster.steps < cv.steps + cv.gap:\n        raise ValueError(\n            (f\"When using a ForecasterDirect, the combination of steps \"\n             f\"+ gap ({cv.steps + cv.gap}) cannot be greater than the `steps` parameter \"\n             f\"declared when the forecaster is initialized ({forecaster.steps}).\")\n        )\n    \n    metric_values, backtest_predictions = _backtesting_forecaster(\n        forecaster              = forecaster,\n        y                       = y,\n        cv                      = cv,\n        metric                  = metric,\n        exog                    = exog,\n        interval                = interval,\n        n_boot                  = n_boot,\n        random_state            = random_state,\n        use_in_sample_residuals = use_in_sample_residuals,\n        use_binned_residuals    = use_binned_residuals,\n        n_jobs                  = n_jobs,\n        verbose                 = verbose,\n        show_progress           = show_progress\n    )\n\n    return metric_values, backtest_predictions\n\n\ndef _backtesting_forecaster_multiseries(\n    forecaster: object,\n    series: Union[pd.DataFrame, dict],\n    cv: TimeSeriesFold,\n    metric: Union[str, Callable, list],\n    levels: Optional[Union[str, list]] = None,\n    add_aggregated_metric: bool = True,\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    interval: Optional[list] = None,\n    n_boot: int = 250,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    n_jobs: Union[int, str] = 'auto',\n    verbose: bool = False,\n    show_progress: bool = True,\n    suppress_warnings: bool = False\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Backtesting of forecaster model following the folds generated by the TimeSeriesFold\n    class and using the metric(s) provided.\n\n    If `forecaster` is already trained and `initial_train_size` is set to `None` in the\n    TimeSeriesFold class, no initial train will be done and all data will be used\n    to evaluate the model. However, the first `len(forecaster.last_window)` observations\n    are needed to create the initial predictors, so no predictions are calculated for\n    them.\n    \n    A copy of the original forecaster is created so that it is not modified during the\n    process.\n    \n    Parameters\n    ----------\n    forecaster : ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Training time series.\n    cv : TimeSeriesFold\n        TimeSeriesFold object with the information needed to split the data into folds.\n        **New in version 0.14.0**\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n        \n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error'}\n        - If `Callable`: Function with arguments y_true, y_pred that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels will be predicted.\n    add_aggregated_metric : bool, default `False`\n        If `True`, and multiple series (`levels`) are predicted, the aggregated\n        metrics (average, weighted average and pooled) are also returned.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. If `None`, no\n        intervals are estimated.\n    n_boot : int, default `250`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of prediction\n        error to create prediction intervals. If `False`, out_sample_residuals \n        are used if they are already stored inside the forecaster.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n    verbose : bool, default `False`\n        Print number of folds and index of training and validation sets used \n        for backtesting.\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n    suppress_warnings: bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the backtesting \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s). Index are the levels and columns the metrics.\n    backtest_predictions : pandas Dataframe\n        Value of predictions and their estimated interval if `interval` is not `None`. \n        If there is more than one level, this structure will be repeated for each of them.\n\n        - column pred: predictions.\n        - column lower_bound: lower bound of the interval.\n        - column upper_bound: upper bound of the interval.\n    \n    \"\"\"\n\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n\n    forecaster = deepcopy(forecaster)\n    cv = deepcopy(cv)\n\n    cv.set_params({\n        'window_size': forecaster.window_size,\n        'differentiation': forecaster.differentiation,\n        'return_all_indexes': False,\n        'verbose': verbose\n    })\n\n    initial_train_size = cv.initial_train_size\n    refit = cv.refit\n    gap = cv.gap\n\n    if n_jobs == 'auto':\n        n_jobs = select_n_jobs_backtesting(\n                     forecaster = forecaster,\n                     refit      = refit\n                 )\n    elif not isinstance(refit, bool) and refit != 1 and n_jobs != 1:\n        warnings.warn(\n            (\"If `refit` is an integer other than 1 (intermittent refit). `n_jobs` \"\n             \"is set to 1 to avoid unexpected results during parallelization.\"),\n             IgnoredArgumentWarning\n        )\n        n_jobs = 1\n    else:\n        n_jobs = n_jobs if n_jobs > 0 else cpu_count()\n\n    levels = _initialize_levels_model_selection_multiseries(\n                 forecaster = forecaster,\n                 series     = series,\n                 levels     = levels\n             )\n\n    if not isinstance(metric, list):\n        metrics = [\n            _get_metric(metric=metric)\n            if isinstance(metric, str)\n            else add_y_train_argument(metric)\n        ]\n    else:\n        metrics = [\n            _get_metric(metric=m)\n            if isinstance(m, str)\n            else add_y_train_argument(m) \n            for m in metric\n        ]\n\n    store_in_sample_residuals = False if interval is None else True\n\n    folds = cv.split(X=series, as_pandas=False)\n    span_index = cv._extract_index(X=series)\n\n    if initial_train_size is not None:\n        # First model training, this is done to allow parallelization when `refit`\n        # is `False`. The initial Forecaster fit is outside the auxiliary function.\n        data_fold = _extract_data_folds_multiseries(\n                        series             = series,\n                        folds              = [folds[0]],\n                        span_index         = span_index,\n                        window_size        = forecaster.window_size,\n                        exog               = exog,\n                        dropna_last_window = forecaster.dropna_from_series,\n                        externally_fitted  = False\n                    )\n        series_train, _, last_window_levels, exog_train, _, _ = next(data_fold)\n        forecaster.fit(\n            series                    = series_train,\n            exog                      = exog_train,\n            store_last_window         = last_window_levels,\n            store_in_sample_residuals = store_in_sample_residuals,\n            suppress_warnings         = suppress_warnings\n        )\n        # This is done to allow parallelization when `refit` is `False`. The initial \n        # Forecaster fit is outside the auxiliary function.\n        folds[0][4] = False\n        \n    if refit:\n        n_of_fits = int(len(folds) / refit)\n        if type(forecaster).__name__ != 'ForecasterDirectMultiVariate' and n_of_fits > 50:\n            warnings.warn(\n                (f\"The forecaster will be fit {n_of_fits} times. This can take substantial \"\n                 f\"amounts of time. If not feasible, try with `refit = False`.\\n\"),\n                LongTrainingWarning,\n            )\n        elif type(forecaster).__name__ == 'ForecasterDirectMultiVariate' and n_of_fits * forecaster.steps > 50:\n            warnings.warn(\n                (f\"The forecaster will be fit {n_of_fits * forecaster.steps} times \"\n                 f\"({n_of_fits} folds * {forecaster.steps} regressors). This can take \"\n                 f\"substantial amounts of time. If not feasible, try with `refit = False`.\\n\"),\n                LongTrainingWarning\n            )\n\n    if show_progress:\n        folds = tqdm(folds)\n        \n    externally_fitted = True if initial_train_size is None else False\n    data_folds = _extract_data_folds_multiseries(\n                     series             = series,\n                     folds              = folds,\n                     span_index         = span_index,\n                     window_size        = forecaster.window_size,\n                     exog               = exog,\n                     dropna_last_window = forecaster.dropna_from_series,\n                     externally_fitted  = externally_fitted\n                 )\n\n    def _fit_predict_forecaster(data_fold, forecaster, interval, levels, gap):\n        \"\"\"\n        Fit the forecaster and predict `steps` ahead. This is an auxiliary \n        function used to parallelize the backtesting_forecaster_multiseries\n        function.\n        \"\"\"\n\n        (\n            series_train,\n            last_window_series,\n            last_window_levels,\n            exog_train,\n            next_window_exog,\n            fold\n        ) = data_fold\n\n        if fold[4] is True:\n            forecaster.fit(\n                series                    = series_train, \n                exog                      = exog_train,\n                store_last_window         = last_window_levels,\n                store_in_sample_residuals = store_in_sample_residuals,\n                suppress_warnings         = suppress_warnings\n            )\n\n        test_iloc_start = fold[2][0]\n        test_iloc_end   = fold[2][1]\n        steps = len(range(test_iloc_start, test_iloc_end))\n        if type(forecaster).__name__ == 'ForecasterDirectMultiVariate' and gap > 0:\n            # Select only the steps that need to be predicted if gap > 0\n            test_iloc_start = fold[3][0]\n            test_iloc_end   = fold[3][1]\n            steps = list(np.arange(len(range(test_iloc_start, test_iloc_end))) + gap + 1)\n\n        levels_predict = [level for level in levels \n                          if level in last_window_levels]\n        if interval is None:\n\n            pred = forecaster.predict(\n                       steps             = steps, \n                       levels            = levels_predict, \n                       last_window       = last_window_series,\n                       exog              = next_window_exog,\n                       suppress_warnings = suppress_warnings\n                   )\n        else:\n            pred = forecaster.predict_interval(\n                       steps                   = steps,\n                       levels                  = levels_predict, \n                       last_window             = last_window_series,\n                       exog                    = next_window_exog,\n                       interval                = interval,\n                       n_boot                  = n_boot,\n                       random_state            = random_state,\n                       use_in_sample_residuals = use_in_sample_residuals,\n                       suppress_warnings       = suppress_warnings\n                   )\n\n        if type(forecaster).__name__ != 'ForecasterDirectMultiVariate' and gap > 0:\n            pred = pred.iloc[gap:, ]\n\n        return pred\n\n    backtest_predictions = Parallel(n_jobs=n_jobs)(\n        delayed(_fit_predict_forecaster)(\n            data_fold  = data_fold,\n            forecaster = forecaster,\n            interval   = interval,\n            levels     = levels,\n            gap        = gap\n        )\n        for data_fold in data_folds\n    )\n\n    backtest_predictions = pd.concat(backtest_predictions, axis=0)\n\n    levels_in_backtest_predictions = backtest_predictions.columns\n    if interval is not None:\n        levels_in_backtest_predictions = [\n            level \n            for level in levels_in_backtest_predictions\n            if not re.search(r'_lower_bound|_upper_bound', level)\n        ]\n    for level in levels_in_backtest_predictions:\n        valid_index = series[level][series[level].notna()].index\n        no_valid_index = backtest_predictions.index.difference(valid_index, sort=False)\n        cols = [level]\n        if interval:\n            cols = cols + [f'{level}_lower_bound', f'{level}_upper_bound']\n        backtest_predictions.loc[no_valid_index, cols] = np.nan\n\n    metrics_levels = _calculate_metrics_backtesting_multiseries(\n        series                = series,\n        predictions           = backtest_predictions,\n        folds                 = folds,\n        span_index            = span_index,\n        window_size           = forecaster.window_size,\n        metrics               = metrics,\n        levels                = levels,\n        add_aggregated_metric = add_aggregated_metric\n    )\n       \n    set_skforecast_warnings(suppress_warnings, action='default')\n\n    return metrics_levels, backtest_predictions\n\n\ndef backtesting_forecaster_multiseries(\n    forecaster: object,\n    series: Union[pd.DataFrame, dict],\n    cv: TimeSeriesFold,\n    metric: Union[str, Callable, list],\n    levels: Optional[Union[str, list]] = None,\n    add_aggregated_metric: bool = True,\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    interval: Optional[list] = None,\n    n_boot: int = 250,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    n_jobs: Union[int, str] = 'auto',\n    verbose: bool = False,\n    show_progress: bool = True,\n    suppress_warnings: bool = False\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Backtesting of forecaster model following the folds generated by the TimeSeriesFold\n    class and using the metric(s) provided.\n\n    If `forecaster` is already trained and `initial_train_size` is set to `None` in the\n    TimeSeriesFold class, no initial train will be done and all data will be used\n    to evaluate the model. However, the first `len(forecaster.last_window)` observations\n    are needed to create the initial predictors, so no predictions are calculated for\n    them.\n    \n    A copy of the original forecaster is created so that it is not modified during \n    the process.\n\n    Parameters\n    ----------\n    forecaster : ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate, ForecasterRnn\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Training time series.\n    cv : TimeSeriesFold\n        TimeSeriesFold object with the information needed to split the data into folds.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n        \n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels will be predicted.\n    add_aggregated_metric : bool, default `True`\n        If `True`, and multiple series (`levels`) are predicted, the aggregated\n        metrics (average, weighted average and pooled) are also returned.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. If `None`, no\n        intervals are estimated.\n    n_boot : int, default `250`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of prediction \n        error to create prediction intervals. If `False`, out_sample_residuals \n        are used if they are already stored inside the forecaster.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n    verbose : bool, default `False`\n        Print number of folds and index of training and validation sets used \n        for backtesting.\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n    suppress_warnings: bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the backtesting \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s). Index are the levels and columns the metrics.\n    backtest_predictions : pandas DataFrame\n        Value of predictions and their estimated interval if `interval` is not `None`.\n        If there is more than one level, this structure will be repeated for each of them.\n\n        - column pred: predictions.\n        - column lower_bound: lower bound of the interval.\n        - column upper_bound: upper bound of the interval.\n    \n    \"\"\"\n\n    multi_series_forecasters = [\n        'ForecasterRecursiveMultiSeries', \n        'ForecasterDirectMultiVariate',\n        'ForecasterRnn'\n    ]\n\n    forecaster_name = type(forecaster).__name__\n\n    if forecaster_name not in multi_series_forecasters:\n        raise TypeError(\n            (f\"`forecaster` must be of type {multi_series_forecasters}, \"\n             f\"for all other types of forecasters use the functions available in \"\n             f\"the `model_selection` module. Got {forecaster_name}\")\n        )\n    \n    check_backtesting_input(\n        forecaster              = forecaster,\n        cv                      = cv,\n        metric                  = metric,\n        add_aggregated_metric   = add_aggregated_metric,\n        series                  = series,\n        exog                    = exog,\n        interval                = interval,\n        n_boot                  = n_boot,\n        random_state            = random_state,\n        use_in_sample_residuals = use_in_sample_residuals,\n        n_jobs                  = n_jobs,\n        show_progress           = show_progress,\n        suppress_warnings       = suppress_warnings\n    )\n\n    metrics_levels, backtest_predictions = _backtesting_forecaster_multiseries(\n        forecaster              = forecaster,\n        series                  = series,\n        cv                      = cv,\n        levels                  = levels,\n        metric                  = metric,\n        add_aggregated_metric   = add_aggregated_metric,\n        exog                    = exog,\n        interval                = interval,\n        n_boot                  = n_boot,\n        random_state            = random_state,\n        use_in_sample_residuals = use_in_sample_residuals,\n        n_jobs                  = n_jobs,\n        verbose                 = verbose,\n        show_progress           = show_progress,\n        suppress_warnings       = suppress_warnings\n    )\n\n    return metrics_levels, backtest_predictions\n\n\ndef _backtesting_sarimax(\n    forecaster: object,\n    y: pd.Series,\n    metric: Union[str, Callable, list],\n    cv: TimeSeriesFold,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    alpha: Optional[float] = None,\n    interval: Optional[list] = None,\n    n_jobs: Union[int, str] = 'auto',\n    suppress_warnings_fit: bool = False,\n    verbose: bool = False,\n    show_progress: bool = True,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Backtesting of ForecasterSarimax.\n    \n    A copy of the original forecaster is created so that it is not modified during \n    the process.\n    \n    Parameters\n    ----------\n    forecaster : ForecasterSarimax\n        Forecaster model.\n    y : pandas Series\n        Training time series.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n        \n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    cv : TimeSeriesFold\n        TimeSeriesFold object with the information needed to split the data into folds.\n        **New in version 0.14.0**\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    alpha : float, default `0.05`\n        The confidence intervals for the forecasts are (1 - alpha) %.\n        If both, `alpha` and `interval` are provided, `alpha` will be used.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. The values must be\n        symmetric. Sequence of percentiles to compute, which must be between \n        0 and 100 inclusive. For example, interval of 95% should be as \n        `interval = [2.5, 97.5]`. If both, `alpha` and `interval` are \n        provided, `alpha` will be used.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n    verbose : bool, default `False`\n        Print number of folds and index of training and validation sets used \n        for backtesting.\n    suppress_warnings_fit : bool, default `False`\n        If `True`, warnings generated during fitting will be ignored.\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n\n    Returns\n    -------\n    metric_values : pandas DataFrame\n        Value(s) of the metric(s).\n    backtest_predictions : pandas DataFrame\n        Value of predictions and their estimated interval if `interval` is not `None`.\n\n        - column pred: predictions.\n        - column lower_bound: lower bound of the interval.\n        - column upper_bound: upper bound of the interval.\n    \n    \"\"\"\n\n    forecaster = deepcopy(forecaster)\n    cv = deepcopy(cv)\n\n    cv.set_params({\n        'window_size': forecaster.window_size,\n        'return_all_indexes': False,\n        'verbose': verbose\n    })\n\n    steps = cv.steps\n    initial_train_size = cv.initial_train_size\n    refit = cv.refit\n    gap = cv.gap\n    \n    if refit == False:\n        if n_jobs != 'auto' and n_jobs != 1:\n            warnings.warn(\n                (\"If `refit = False`, `n_jobs` is set to 1 to avoid unexpected \"\n                 \"results during parallelization.\"),\n                 IgnoredArgumentWarning\n            )\n        n_jobs = 1\n    else:\n        if n_jobs == 'auto':        \n            n_jobs = select_n_jobs_backtesting(\n                         forecaster = forecaster,\n                         refit      = refit\n                     )\n        elif not isinstance(refit, bool) and refit != 1 and n_jobs != 1:\n            warnings.warn(\n                (\"If `refit` is an integer other than 1 (intermittent refit). `n_jobs` \"\n                 \"is set to 1 to avoid unexpected results during parallelization.\"),\n                 IgnoredArgumentWarning\n            )\n            n_jobs = 1\n        else:\n            n_jobs = n_jobs if n_jobs > 0 else cpu_count()\n\n    if not isinstance(metric, list):\n        metrics = [\n            _get_metric(metric=metric)\n            if isinstance(metric, str)\n            else add_y_train_argument(metric)\n        ]\n    else:\n        metrics = [\n            _get_metric(metric=m)\n            if isinstance(m, str)\n            else add_y_train_argument(m) \n            for m in metric\n        ]\n\n    # initial_train_size cannot be None because of append method in Sarimax\n    # First model training, this is done to allow parallelization when `refit` \n    # is `False`. The initial Forecaster fit is outside the auxiliary function.\n    exog_train = exog.iloc[:initial_train_size, ] if exog is not None else None\n    forecaster.fit(\n        y                 = y.iloc[:initial_train_size, ],\n        exog              = exog_train,\n        suppress_warnings = suppress_warnings_fit\n    )\n    \n    folds = cv.split(X=y, as_pandas=False)\n    # This is done to allow parallelization when `refit` is `False`. The initial \n    # Forecaster fit is outside the auxiliary function.\n    folds[0][4] = False\n    \n    if refit:\n        n_of_fits = int(len(folds) / refit)\n        if n_of_fits > 50:\n            warnings.warn(\n                (f\"The forecaster will be fit {n_of_fits} times. This can take substantial \"\n                 f\"amounts of time. If not feasible, try with `refit = False`.\\n\"),\n                 LongTrainingWarning\n            )\n       \n    folds_tqdm = tqdm(folds) if show_progress else folds\n\n    def _fit_predict_forecaster(y, exog, forecaster, alpha, interval, fold, steps, gap):\n        \"\"\"\n        Fit the forecaster and predict `steps` ahead. This is an auxiliary \n        function used to parallelize the backtesting_forecaster function.\n        \"\"\"\n\n        # In each iteration the model is fitted before making predictions. \n        # if fixed_train_size the train size doesn't increase but moves by `steps` \n        # in each iteration. if False the train size increases by `steps` in each \n        # iteration.\n        train_idx_start = fold[0][0]\n        train_idx_end   = fold[0][1]\n        test_idx_start  = fold[2][0]\n        test_idx_end    = fold[2][1]\n\n        if refit:\n            last_window_start = fold[0][1]  # Same as train_idx_end\n            last_window_end   = fold[1][1]\n        else:\n            last_window_end   = fold[2][0]  # test_idx_start\n            last_window_start = last_window_end - steps\n\n        if fold[4] is False:\n            # When the model is not fitted, last_window and last_window_exog must \n            # be updated to include the data needed to make predictions.\n            last_window_y = y.iloc[last_window_start:last_window_end]\n            last_window_exog = exog.iloc[last_window_start:last_window_end] if exog is not None else None \n        else:\n            # The model is fitted before making predictions. If `fixed_train_size`  \n            # the train size doesn't increase but moves by `steps` in each iteration. \n            # If `False` the train size increases by `steps` in each  iteration.\n            y_train = y.iloc[train_idx_start:train_idx_end, ]\n            exog_train = exog.iloc[train_idx_start:train_idx_end, ] if exog is not None else None\n            \n            last_window_y = None\n            last_window_exog = None\n\n            forecaster.fit(y=y_train, exog=exog_train, suppress_warnings=suppress_warnings_fit)\n\n        next_window_exog = exog.iloc[test_idx_start:test_idx_end, ] if exog is not None else None\n\n        # After the first fit, Sarimax must use the last windows stored in the model\n        if fold == folds[0]:\n            last_window_y = None\n            last_window_exog = None\n\n        steps = len(range(test_idx_start, test_idx_end))\n        if alpha is None and interval is None:\n            pred = forecaster.predict(\n                       steps            = steps,\n                       last_window      = last_window_y,\n                       last_window_exog = last_window_exog,\n                       exog             = next_window_exog\n                   )\n        else:\n            pred = forecaster.predict_interval(\n                       steps            = steps,\n                       exog             = next_window_exog,\n                       alpha            = alpha,\n                       interval         = interval,\n                       last_window      = last_window_y,\n                       last_window_exog = last_window_exog\n                   )\n\n        pred = pred.iloc[gap:, ]            \n        \n        return pred\n\n    backtest_predictions = (\n        Parallel(n_jobs=n_jobs)\n        (delayed(_fit_predict_forecaster)\n        (\n            y=y, \n            exog=exog, \n            forecaster=forecaster, \n            alpha=alpha, \n            interval=interval, \n            fold=fold, \n            steps=steps,\n            gap=gap\n        )\n        for fold in folds_tqdm)\n    )\n    \n    backtest_predictions = pd.concat(backtest_predictions)\n    if isinstance(backtest_predictions, pd.Series):\n        backtest_predictions = pd.DataFrame(backtest_predictions)\n\n    train_indexes = []\n    for i, fold in enumerate(folds):\n        fit_fold = fold[-1]\n        if i == 0 or fit_fold:\n            train_iloc_start = fold[0][0]\n            train_iloc_end = fold[0][1]\n            train_indexes.append(np.arange(train_iloc_start, train_iloc_end))\n    \n    train_indexes = np.unique(np.concatenate(train_indexes))\n    y_train = y.iloc[train_indexes]\n\n    metric_values = [\n        m(\n            y_true = y.loc[backtest_predictions.index],\n            y_pred = backtest_predictions['pred'],\n            y_train = y_train\n        ) \n        for m in metrics\n    ]\n\n    metric_values = pd.DataFrame(\n        data    = [metric_values],\n        columns = [m.__name__ for m in metrics]\n    )\n\n    return metric_values, backtest_predictions\n\n\ndef backtesting_sarimax(\n    forecaster: object,\n    y: pd.Series,\n    cv: TimeSeriesFold,\n    metric: Union[str, Callable, list],\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    alpha: Optional[float] = None,\n    interval: Optional[list] = None,\n    n_jobs: Union[int, str] = 'auto',\n    verbose: bool = False,\n    suppress_warnings_fit: bool = False,\n    show_progress: bool = True\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Backtesting of ForecasterSarimax.\n    \n    A copy of the original forecaster is created so that it is not modified during \n    the process.\n\n    Parameters\n    ----------\n    forecaster : ForecasterSarimax\n        Forecaster model.\n    y : pandas Series\n        Training time series.\n    cv : TimeSeriesFold\n        TimeSeriesFold object with the information needed to split the data into folds.\n        **New in version 0.14.0**\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n        \n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    alpha : float, default `0.05`\n        The confidence intervals for the forecasts are (1 - alpha) %.\n        If both, `alpha` and `interval` are provided, `alpha` will be used.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. The values must be\n        symmetric. Sequence of percentiles to compute, which must be between \n        0 and 100 inclusive. For example, interval of 95% should be as \n        `interval = [2.5, 97.5]`. If both, `alpha` and `interval` are \n        provided, `alpha` will be used.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting. \n    verbose : bool, default `False`\n        Print number of folds and index of training and validation sets used \n        for backtesting.\n    suppress_warnings_fit : bool, default `False`\n        If `True`, warnings generated during fitting will be ignored.\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n\n    Returns\n    -------\n    metric_values : pandas DataFrame\n        Value(s) of the metric(s).\n    backtest_predictions : pandas DataFrame\n        Value of predictions and their estimated interval if `interval` is not `None`.\n\n        - column pred: predictions.\n        - column lower_bound: lower bound of the interval.\n        - column upper_bound: upper bound of the interval.\n    \n    \"\"\"\n    \n    if type(forecaster).__name__ not in ['ForecasterSarimax']:\n        raise TypeError(\n            (\"`forecaster` must be of type `ForecasterSarimax`, for all other \"\n             \"types of forecasters use the functions available in the other \"\n             \"`model_selection` modules.\")\n        )\n    \n    check_backtesting_input(\n        forecaster            = forecaster,\n        cv                    = cv,\n        y                     = y,\n        metric                = metric,\n        interval              = interval,\n        alpha                 = alpha,\n        n_jobs                = n_jobs,\n        show_progress         = show_progress,\n        suppress_warnings_fit = suppress_warnings_fit\n    )\n    \n    metric_values, backtest_predictions = _backtesting_sarimax(\n        forecaster            = forecaster,\n        y                     = y,\n        cv                    = cv,\n        metric                = metric,\n        exog                  = exog,\n        alpha                 = alpha,\n        interval              = interval,\n        n_jobs                = n_jobs,\n        verbose               = verbose,\n        suppress_warnings_fit = suppress_warnings_fit,\n        show_progress         = show_progress\n    )\n\n    return metric_values, backtest_predictions\n",
    "skforecast/model_selection/_split.py": "################################################################################\n#                     skforecast.model_selection._split                        #\n#                                                                              #\n# This work by skforecast team is licensed under the BSD 3-Clause License.     #\n################################################################################\n# coding=utf-8\n\nfrom copy import deepcopy\nfrom typing import Union, Optional, Any\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport itertools\nfrom ..exceptions import IgnoredArgumentWarning\n\n\nclass BaseFold():\n    \"\"\"\n    Base class for all Fold classes in skforecast. All fold classes should specify\n    all the parameters that can be set at the class level in their ``__init__``.\n\n    Parameters\n    ----------\n    steps : int, default `None`\n        Number of observations used to be predicted in each fold. This is also commonly\n        referred to as the forecast horizon or test size.\n    initial_train_size : int, default `None`\n        Number of observations used for initial training.\n    window_size : int, default `None`\n        Number of observations needed to generate the autoregressive predictors.\n    differentiation : int, default `None`\n        Number of observations to use for differentiation. This is used to extend the\n        `last_window` as many observations as the differentiation order.\n    refit : bool, int, default `False`\n        Whether to refit the forecaster in each fold.\n\n        - If `True`, the forecaster is refitted in each fold.\n        - If `False`, the forecaster is trained only in the first fold.\n        - If an integer, the forecaster is trained in the first fold and then refitted\n          every `refit` folds.\n    fixed_train_size : bool, default `True`\n        Whether the training size is fixed or increases in each fold.\n    gap : int, default `0`\n        Number of observations between the end of the training set and the start of the\n        test set.\n    skip_folds : int, list, default `None`\n        Number of folds to skip.\n\n        - If an integer, every 'skip_folds'-th is returned.\n        - If a list, the indexes of the folds to skip.\n\n        For example, if `skip_folds=3` and there are 10 folds, the returned folds are\n        0, 3, 6, and 9. If `skip_folds=[1, 2, 3]`, the returned folds are 0, 4, 5, 6, 7,\n        8, and 9.\n    allow_incomplete_fold : bool, default `True`\n        Whether to allow the last fold to include fewer observations than `steps`.\n        If `False`, the last fold is excluded if it is incomplete.\n    return_all_indexes : bool, default `False`\n        Whether to return all indexes or only the start and end indexes of each fold.\n    verbose : bool, default `True`\n        Whether to print information about generated folds.\n\n    Attributes\n    ----------\n    steps : int\n        Number of observations used to be predicted in each fold. This is also commonly\n        referred to as the forecast horizon or test size.\n    initial_train_size : int\n        Number of observations used for initial training.\n    window_size : int\n        Number of observations needed to generate the autoregressive predictors.\n    differentiation : int\n        Number of observations to use for differentiation. This is used to extend the\n        `last_window` as many observations as the differentiation order.\n    refit : bool, int\n        Whether to refit the forecaster in each fold.\n    fixed_train_size : bool\n        Whether the training size is fixed or increases in each fold.\n    gap : int\n        Number of observations between the end of the training set and the start of the\n        test set.\n    skip_folds : int, list\n        Number of folds to skip.\n    allow_incomplete_fold : bool\n        Whether to allow the last fold to include fewer observations than `steps`.\n    return_all_indexes : bool\n        Whether to return all indexes or only the start and end indexes of each fold.\n    verbose : bool\n        Whether to print information about generated folds.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        steps: Optional[int] = None,\n        initial_train_size: Optional[int] = None,\n        window_size: Optional[int] = None,\n        differentiation: Optional[int] = None,\n        refit: Union[bool, int] = False,\n        fixed_train_size: bool = True,\n        gap: int = 0,\n        skip_folds: Optional[Union[int, list]] = None,\n        allow_incomplete_fold: bool = True,\n        return_all_indexes: bool = False,\n        verbose: bool = True\n    ) -> None:\n\n        self._validate_params(\n            cv_name               = type(self).__name__,\n            steps                 = steps,\n            initial_train_size    = initial_train_size,\n            window_size           = window_size,\n            differentiation       = differentiation,\n            refit                 = refit,\n            fixed_train_size      = fixed_train_size,\n            gap                   = gap,\n            skip_folds            = skip_folds,\n            allow_incomplete_fold = allow_incomplete_fold,\n            return_all_indexes    = return_all_indexes,\n            verbose               = verbose\n        )\n\n        self.steps                 = steps\n        self.initial_train_size    = initial_train_size\n        self.window_size           = window_size\n        self.differentiation       = differentiation\n        self.refit                 = refit\n        self.fixed_train_size      = fixed_train_size\n        self.gap                   = gap\n        self.skip_folds            = skip_folds\n        self.allow_incomplete_fold = allow_incomplete_fold\n        self.return_all_indexes    = return_all_indexes\n        self.verbose               = verbose\n\n    def _validate_params(\n        self,\n        cv_name: str,\n        steps: Optional[int] = None,\n        initial_train_size: Optional[int] = None,\n        window_size: Optional[int] = None,\n        differentiation: Optional[int] = None,\n        refit: Union[bool, int] = False,\n        fixed_train_size: bool = True,\n        gap: int = 0,\n        skip_folds: Optional[Union[int, list]] = None,\n        allow_incomplete_fold: bool = True,\n        return_all_indexes: bool = False,\n        verbose: bool = True\n    ) -> None: \n        \"\"\"\n        Validate all input parameters to ensure correctness.\n        \"\"\"\n\n        if cv_name == \"TimeSeriesFold\":\n            if not isinstance(steps, (int, np.integer)) or steps < 1:\n                raise ValueError(\n                    f\"`steps` must be an integer greater than 0. Got {steps}.\"\n                )\n            if not isinstance(initial_train_size, (int, np.integer, type(None))):\n                raise ValueError(\n                    f\"`initial_train_size` must be an integer greater than 0 or None. \"\n                    f\"Got {initial_train_size}.\"\n                )\n            if initial_train_size is not None and initial_train_size < 1:\n                raise ValueError(\n                    f\"`initial_train_size` must be an integer greater than 0 or None. \"\n                    f\"Got {initial_train_size}.\"\n                )\n            if not isinstance(refit, (bool, int, np.integer)):\n                raise TypeError(\n                    f\"`refit` must be a boolean or an integer equal or greater than 0. \"\n                    f\"Got {refit}.\"\n                )\n            if isinstance(refit, (int, np.integer)) and not isinstance(refit, bool) and refit < 0:\n                raise TypeError(\n                    f\"`refit` must be a boolean or an integer equal or greater than 0. \"\n                    f\"Got {refit}.\"\n                )\n            if not isinstance(fixed_train_size, bool):\n                raise TypeError(\n                    f\"`fixed_train_size` must be a boolean: `True`, `False`. \"\n                    f\"Got {fixed_train_size}.\"\n                )\n            if not isinstance(gap, (int, np.integer)) or gap < 0:\n                raise ValueError(\n                    f\"`gap` must be an integer greater than or equal to 0. Got {gap}.\"\n                )\n            if skip_folds is not None:\n                if not isinstance(skip_folds, (int, np.integer, list, type(None))):\n                    raise TypeError(\n                        f\"`skip_folds` must be an integer greater than 0, a list of \"\n                        f\"integers or `None`. Got {skip_folds}.\"\n                    )\n                if isinstance(skip_folds, (int, np.integer)) and skip_folds < 1:\n                    raise ValueError(\n                        f\"`skip_folds` must be an integer greater than 0, a list of \"\n                        f\"integers or `None`. Got {skip_folds}.\"\n                    )\n                if isinstance(skip_folds, list) and any([x < 1 for x in skip_folds]):\n                    raise ValueError(\n                        f\"`skip_folds` list must contain integers greater than or \"\n                        f\"equal to 1. The first fold is always needed to train the \"\n                        f\"forecaster. Got {skip_folds}.\"\n                    ) \n            if not isinstance(allow_incomplete_fold, bool):\n                raise TypeError(\n                    f\"`allow_incomplete_fold` must be a boolean: `True`, `False`. \"\n                    f\"Got {allow_incomplete_fold}.\"\n                )\n            \n        if cv_name == \"OneStepAheadFold\":\n            if (\n                not isinstance(initial_train_size, (int, np.integer))\n                or initial_train_size < 1\n            ):\n                raise ValueError(\n                    f\"`initial_train_size` must be an integer greater than 0. \"\n                    f\"Got {initial_train_size}.\"\n                )\n        \n        if (\n            not isinstance(window_size, (int, np.integer, pd.DateOffset, type(None)))\n            or isinstance(window_size, (int, np.integer))\n            and window_size < 1\n        ):\n            raise ValueError(\n                f\"`window_size` must be an integer greater than 0. Got {window_size}.\"\n            )\n        \n        if not isinstance(return_all_indexes, bool):\n            raise TypeError(\n                f\"`return_all_indexes` must be a boolean: `True`, `False`. \"\n                f\"Got {return_all_indexes}.\"\n            )\n        if differentiation is not None:\n            if not isinstance(differentiation, (int, np.integer)) or differentiation < 0:\n                raise ValueError(\n                    f\"`differentiation` must be None or an integer greater than or \"\n                    f\"equal to 0. Got {differentiation}.\"\n                )\n        if not isinstance(verbose, bool):\n            raise TypeError(\n                f\"`verbose` must be a boolean: `True`, `False`. \"\n                f\"Got {verbose}.\"\n            )\n\n    def _extract_index(\n        self,\n        X: Union[pd.Series, pd.DataFrame, pd.Index, dict]\n    ) -> pd.Index:\n        \"\"\"\n        Extracts and returns the index from the input data X.\n\n        Parameters\n        ----------\n        X : pandas Series, pandas DataFrame, pandas Index, dict\n            Time series data or index to split.\n\n        Returns\n        -------\n        idx : pandas Index\n            Index extracted from the input data.\n        \n        \"\"\"\n\n        if isinstance(X, (pd.Series, pd.DataFrame)):\n            idx = X.index\n        elif isinstance(X, dict):\n            freqs = [s.index.freq for s in X.values() if s.index.freq is not None]\n            if not freqs:\n                raise ValueError(\"At least one series must have a frequency.\")\n            if not all(f == freqs[0] for f in freqs):\n                raise ValueError(\n                    \"All series with frequency must have the same frequency.\"\n                )\n            min_idx = min([v.index[0] for v in X.values()])\n            max_idx = max([v.index[-1] for v in X.values()])\n            idx = pd.date_range(start=min_idx, end=max_idx, freq=freqs[0])\n        else:\n            idx = X\n            \n        return idx\n\n    def set_params(\n        self, \n        params: dict\n    ) -> None:\n        \"\"\"\n        Set the parameters of the Fold object. Before overwriting the current \n        parameters, the input parameters are validated to ensure correctness.\n\n        Parameters\n        ----------\n        params : dict\n            Dictionary with the parameters to set.\n        \n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        if not isinstance(params, dict):\n            raise TypeError(\n                f\"`params` must be a dictionary. Got {type(params)}.\"\n            )\n\n        current_params = deepcopy(vars(self))\n        unknown_params = set(params.keys()) - set(current_params.keys())\n        if unknown_params:\n            warnings.warn(\n                f\"Unknown parameters: {unknown_params}. They have been ignored.\",\n                IgnoredArgumentWarning\n            )\n\n        filtered_params = {k: v for k, v in params.items() if k in current_params}\n        updated_params = {'cv_name': type(self).__name__, **current_params, **filtered_params}\n\n        self._validate_params(**updated_params)\n        for key, value in updated_params.items():\n            setattr(self, key, value)\n\n\nclass OneStepAheadFold(BaseFold):\n    \"\"\"\n    Class to split time series data into train and test folds for one-step-ahead\n    forecasting.\n\n    Parameters\n    ----------\n    initial_train_size : int\n        Number of observations used for initial training.\n    window_size : int, default `None`\n        Number of observations needed to generate the autoregressive predictors.\n    differentiation : int, default `None`\n        Number of observations to use for differentiation. This is used to extend the\n        `last_window` as many observations as the differentiation order.\n    return_all_indexes : bool, default `False`\n        Whether to return all indexes or only the start and end indexes of each fold.\n    verbose : bool, default `True`\n        Whether to print information about generated folds.\n\n    Attributes\n    ----------\n    initial_train_size : int\n        Number of observations used for initial training.\n    window_size : int\n        Number of observations needed to generate the autoregressive predictors.\n    differentiation : int \n        Number of observations to use for differentiation. This is used to extend the\n        `last_window` as many observations as the differentiation order.\n    return_all_indexes : bool\n        Whether to return all indexes or only the start and end indexes of each fold.\n    verbose : bool\n        Whether to print information about generated folds.\n    steps : Any\n        This attribute is not used in this class. It is included for API consistency.\n    fixed_train_size : Any\n        This attribute is not used in this class. It is included for API consistency.\n    gap : Any\n        This attribute is not used in this class. It is included for API consistency.\n    skip_folds : Any\n        This attribute is not used in this class. It is included for API consistency.\n    allow_incomplete_fold : Any\n        This attribute is not used in this class. It is included for API consistency.\n    refit : Any\n        This attribute is not used in this class. It is included for API consistency.\n    \n    \"\"\"\n\n    def __init__(\n        self,\n        initial_train_size: int,\n        window_size: Optional[int] = None,\n        differentiation: Optional[int] = None,\n        return_all_indexes: bool = False,\n        verbose: bool = True,\n    ) -> None:\n        \n        super().__init__(\n            initial_train_size = initial_train_size,\n            window_size        = window_size,\n            differentiation    = differentiation,\n            return_all_indexes = return_all_indexes,\n            verbose            = verbose\n        )\n\n    def __repr__(\n        self\n    ) -> str:\n        \"\"\"\n        Information displayed when printed.\n        \"\"\"\n            \n        return (\n            f\"OneStepAheadFold(\\n\"\n            f\"    initial_train_size = {self.initial_train_size},\\n\"\n            f\"    window_size        = {self.window_size},\\n\"\n            f\"    differentiation    = {self.differentiation},\\n\"\n            f\"    return_all_indexes = {self.return_all_indexes},\\n\"\n            f\"    verbose            = {self.verbose}\\n\"\n            f\")\"\n        )\n    \n    def split(\n        self,\n        X: Union[pd.Series, pd.DataFrame, pd.Index, dict],\n        as_pandas: bool = False,\n        externally_fitted: Any = None\n    ) -> Union[list, pd.DataFrame]:\n        \"\"\"\n        Split the time series data into train and test folds.\n\n        Parameters\n        ----------\n        X : pandas Series, DataFrame, Index, or dictionary\n            Time series data or index to split.\n        as_pandas : bool, default `False`\n            If True, the folds are returned as a DataFrame. This is useful to visualize\n            the folds in a more interpretable way.\n        externally_fitted : Any\n            This argument is not used in this class. It is included for API consistency.\n        \n        Returns\n        -------\n        fold : list, pandas DataFrame\n            A list of lists containing the indices (position) for for each fold. Each list\n            contains 2 lists the following information:\n\n            - [train_start, train_end]: list with the start and end positions of the\n            training set.\n            - [test_start, test_end]: list with the start and end positions of the test\n            set. These are the observations used to evaluate the forecaster.\n        \n            It is important to note that the returned values are the positions of the\n            observations and not the actual values of the index, so they can be used to\n            slice the data directly using iloc.\n\n            If `as_pandas` is `True`, the folds are returned as a DataFrame with the\n            following columns: 'fold', 'train_start', 'train_end', 'test_start', 'test_end'.\n\n            Following the python convention, the start index is inclusive and the end\n            index is exclusive. This means that the last index is not included in the\n            slice.\n        \n        \"\"\"\n\n        if not isinstance(X, (pd.Series, pd.DataFrame, pd.Index, dict)):\n            raise TypeError(\n                f\"X must be a pandas Series, DataFrame, Index or a dictionary. \"\n                f\"Got {type(X)}.\"\n            )\n\n        index = self._extract_index(X)\n        fold = [\n            [0, self.initial_train_size],\n            [self.initial_train_size, len(X)],\n            True\n        ]\n\n        if self.verbose:\n            self._print_info(\n                index = index,\n                fold = fold,\n            )\n\n        if self.return_all_indexes:\n            fold = [\n                [range(fold[0][0], fold[0][1])],\n                [range(fold[1][0], fold[1][1])],\n                fold[2]\n            ]\n\n        if as_pandas:\n            if not self.return_all_indexes:\n                fold = pd.DataFrame(\n                    data = [list(itertools.chain(*fold[:-1])) + [fold[-1]]],\n                    columns = [\n                        'train_start',\n                        'train_end',\n                        'test_start',\n                        'test_end',\n                        'fit_forecaster'\n                    ],\n                )\n            else:\n                fold = pd.DataFrame(\n                    data = [fold],\n                    columns = [\n                        'train_index',\n                        'test_index',\n                        'fit_forecaster'\n                    ],\n                )\n            fold.insert(0, 'fold', range(len(fold)))\n\n        return fold\n\n    def _print_info(\n        self,\n        index: pd.Index,\n        fold: list,\n    ) -> None:\n        \"\"\"\n        Print information about folds.\n        \"\"\"\n\n        if self.differentiation is None:\n            differentiation = 0\n        else:\n            differentiation = self.differentiation\n\n        initial_train_size = self.initial_train_size - differentiation\n        test_length = len(index) - (initial_train_size + differentiation)\n\n        print(\"Information of folds\")\n        print(\"--------------------\")\n        print(\n            f\"Number of observations in train: {initial_train_size}\"\n        )\n        if self.differentiation is not None:\n            print(\n                f\"    First {differentiation} observation/s in training set \"\n                f\"are used for differentiation\"\n            )\n        print(\n            f\"Number of observations in test: {test_length}\"\n        )\n        \n        training_start = index[fold[0][0] + differentiation]\n        training_end = index[fold[0][-1]]\n        test_start  = index[fold[1][0]]\n        test_end    = index[fold[1][-1] - 1]\n        \n        print(\n            f\"Training : {training_start} -- {training_end} (n={initial_train_size})\"\n        )\n        print(\n            f\"Test     : {test_start} -- {test_end} (n={test_length})\"\n        )\n        print(\"\")\n\n\nclass TimeSeriesFold(BaseFold):\n    \"\"\"\n    Class to split time series data into train and test folds. \n    When used within a backtesting or hyperparameter search, the arguments\n    'initial_train_size', 'window_size' and 'differentiation' are not required\n    as they are automatically set by the backtesting or hyperparameter search\n    functions.\n\n    Parameters\n    ----------\n    steps : int\n        Number of observations used to be predicted in each fold. This is also commonly\n        referred to as the forecast horizon or test size.\n    initial_train_size : int, default `None`\n        Number of observations used for initial training. If `None` or 0, the initial\n        forecaster is not trained in the first fold.\n    window_size : int, default `None`\n        Number of observations needed to generate the autoregressive predictors.\n    differentiation : int, default `None`\n        Number of observations to use for differentiation. This is used to extend the\n        `last_window` as many observations as the differentiation order.\n    refit : bool, int, default `False`\n        Whether to refit the forecaster in each fold.\n\n        - If `True`, the forecaster is refitted in each fold.\n        - If `False`, the forecaster is trained only in the first fold.\n        - If an integer, the forecaster is trained in the first fold and then refitted\n          every `refit` folds.\n    fixed_train_size : bool, default `True`\n        Whether the training size is fixed or increases in each fold.\n    gap : int, default `0`\n        Number of observations between the end of the training set and the start of the\n        test set.\n    skip_folds : int, list, default `None`\n        Number of folds to skip.\n\n        - If an integer, every 'skip_folds'-th is returned.\n        - If a list, the indexes of the folds to skip.\n\n        For example, if `skip_folds=3` and there are 10 folds, the returned folds are\n        0, 3, 6, and 9. If `skip_folds=[1, 2, 3]`, the returned folds are 0, 4, 5, 6, 7,\n        8, and 9.\n    allow_incomplete_fold : bool, default `True`\n        Whether to allow the last fold to include fewer observations than `steps`.\n        If `False`, the last fold is excluded if it is incomplete.\n    return_all_indexes : bool, default `False`\n        Whether to return all indexes or only the start and end indexes of each fold.\n    verbose : bool, default `True`\n        Whether to print information about generated folds.\n\n    Attributes\n    ----------\n    steps : int\n        Number of observations used to be predicted in each fold. This is also commonly\n        referred to as the forecast horizon or test size.\n    initial_train_size : int\n        Number of observations used for initial training. If `None` or 0, the initial\n        forecaster is not trained in the first fold.\n    window_size : int\n        Number of observations needed to generate the autoregressive predictors.\n    differentiation : int\n        Number of observations to use for differentiation. This is used to extend the\n        `last_window` as many observations as the differentiation order.\n    refit : bool, int\n        Whether to refit the forecaster in each fold.\n    fixed_train_size : bool\n        Whether the training size is fixed or increases in each fold.\n    gap : int\n        Number of observations between the end of the training set and the start of the\n        test set.\n    skip_folds : int, list\n        Number of folds to skip.\n    allow_incomplete_fold : bool\n        Whether to allow the last fold to include fewer observations than `steps`.\n    return_all_indexes : bool\n        Whether to return all indexes or only the start and end indexes of each fold.\n    verbose : bool\n        Whether to print information about generated folds.\n\n    Notes\n    -----\n    Returned values are the positions of the observations and not the actual values of\n    the index, so they can be used to slice the data directly using iloc. For example,\n    if the input series is `X = [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]`, the \n    `initial_train_size = 3`, `window_size = 2`, `steps = 4`, and `gap = 1`,\n    the output of the first fold will: [[0, 3], [1, 3], [3, 8], [4, 8], True].\n\n    The first list `[0, 3]` indicates that the training set goes from the first to the\n    third observation. The second list `[1, 3]` indicates that the last window seen by\n    the forecaster during training goes from the second to the third observation. The\n    third list `[3, 8]` indicates that the test set goes from the fourth to the eighth\n    observation. The fourth list `[4, 8]` indicates that the test set including the gap\n    goes from the fifth to the eighth observation. The boolean `False` indicates that the\n    forecaster should not be trained in this fold.\n\n    Following the python convention, the start index is inclusive and the end index is\n    exclusive. This means that the last index is not included in the slice.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        steps: int,\n        initial_train_size: Optional[int] = None,\n        window_size: Optional[int] = None,\n        differentiation: Optional[int] = None,\n        refit: Union[bool, int] = False,\n        fixed_train_size: bool = True,\n        gap: int = 0,\n        skip_folds: Optional[Union[int, list]] = None,\n        allow_incomplete_fold: bool = True,\n        return_all_indexes: bool = False,\n        verbose: bool = True\n    ) -> None:\n        \n        super().__init__(\n            steps                 = steps,\n            initial_train_size    = initial_train_size,\n            window_size           = window_size,\n            differentiation       = differentiation,\n            refit                 = refit,\n            fixed_train_size      = fixed_train_size,\n            gap                   = gap,\n            skip_folds            = skip_folds,\n            allow_incomplete_fold = allow_incomplete_fold,\n            return_all_indexes    = return_all_indexes,\n            verbose               = verbose\n        )\n\n    def __repr__(\n        self\n    ) -> str:\n        \"\"\"\n        Information displayed when printed.\n        \"\"\"\n            \n        return (\n            f\"TimeSeriesFold(\\n\"\n            f\"    steps                 = {self.steps},\\n\"\n            f\"    initial_train_size    = {self.initial_train_size},\\n\"\n            f\"    window_size           = {self.window_size},\\n\"\n            f\"    differentiation       = {self.differentiation},\\n\"\n            f\"    refit                 = {self.refit},\\n\"\n            f\"    fixed_train_size      = {self.fixed_train_size},\\n\"\n            f\"    gap                   = {self.gap},\\n\"\n            f\"    skip_folds            = {self.skip_folds},\\n\"\n            f\"    allow_incomplete_fold = {self.allow_incomplete_fold},\\n\"\n            f\"    return_all_indexes    = {self.return_all_indexes},\\n\"\n            f\"    verbose               = {self.verbose}\\n\"\n            f\")\"\n        )\n\n    def split(\n        self,\n        X: Union[pd.Series, pd.DataFrame, pd.Index, dict],\n        as_pandas: bool = False\n    ) -> Union[list, pd.DataFrame]:\n        \"\"\"\n        Split the time series data into train and test folds.\n\n        Parameters\n        ----------\n        X : pandas Series, pandas DataFrame, pandas Index, dict\n            Time series data or index to split.\n        as_pandas : bool, default `False`\n            If True, the folds are returned as a DataFrame. This is useful to visualize\n            the folds in a more interpretable way.\n\n        Returns\n        -------\n        folds : list, pandas DataFrame\n            A list of lists containing the indices (position) for for each fold. Each list\n            contains 4 lists and a boolean with the following information:\n\n            - [train_start, train_end]: list with the start and end positions of the\n            training set.\n            - [last_window_start, last_window_end]: list with the start and end positions\n            of the last window seen by the forecaster during training. The last window\n            is used to generate the lags use as predictors. If `differentiation` is\n            included, the interval is extended as many observations as the\n            differentiation order. If the argument `window_size` is `None`, this list is\n            empty.\n            - [test_start, test_end]: list with the start and end positions of the test\n            set. These are the observations used to evaluate the forecaster.\n            - [test_start_with_gap, test_end_with_gap]: list with the start and end\n            positions of the test set including the gap. The gap is the number of\n            observations between the end of the training set and the start of the test\n            set.\n            - fit_forecaster: boolean indicating whether the forecaster should be fitted\n            in this fold.\n\n            It is important to note that the returned values are the positions of the\n            observations and not the actual values of the index, so they can be used to\n            slice the data directly using iloc.\n\n            If `as_pandas` is `True`, the folds are returned as a DataFrame with the\n            following columns: 'fold', 'train_start', 'train_end', 'last_window_start',\n            'last_window_end', 'test_start', 'test_end', 'test_start_with_gap',\n            'test_end_with_gap', 'fit_forecaster'.\n\n            Following the python convention, the start index is inclusive and the end\n            index is exclusive. This means that the last index is not included in the\n            slice.\n\n        \"\"\"\n\n        if not isinstance(X, (pd.Series, pd.DataFrame, pd.Index, dict)):\n            raise TypeError(\n                f\"X must be a pandas Series, DataFrame, Index or a dictionary. \"\n                f\"Got {type(X)}.\"\n            )\n        \n        if isinstance(self.window_size, pd.tseries.offsets.DateOffset):\n            # Calculate the window_size in steps. This is not a exact calculation\n            # because the offset follows the calendar rules and the distance between\n            # two dates may not be constant.\n            first_valid_index = X.index[-1] - self.window_size\n            try:\n                window_size_idx_start = X.index.get_loc(first_valid_index)\n                window_size_idx_end = X.index.get_loc(X.index[-1])\n                self.window_size = window_size_idx_end - window_size_idx_start\n            except KeyError:\n                raise ValueError(\n                    f\"The length of `X` ({len(X)}), must be greater than or equal \"\n                    f\"to the window size ({self.window_size}). Try to decrease the \"\n                    f\"size of the offset (forecaster.offset), or increase the \"\n                    f\"size of `y`.\"\n                )\n        \n        if self.initial_train_size is None:\n            if self.window_size is None:\n                raise ValueError(\n                    \"To use split method when `initial_train_size` is None, \"\n                    \"`window_size` must be an integer greater than 0. \"\n                    \"Although no initial training is done and all data is used to \"\n                    \"evaluate the model, the first `window_size` observations are \"\n                    \"needed to create the initial predictors. Got `window_size` = None.\"\n                )\n            if self.refit:\n                raise ValueError(\n                    \"`refit` is only allowed when `initial_train_size` is not `None`. \"\n                    \"Set `refit` to `False` if you want to use `initial_train_size = None`.\"\n                )\n            externally_fitted = True\n            self.initial_train_size = self.window_size  # Reset to None later\n        else:\n            if self.window_size is None:\n                warnings.warn(\n                    \"Last window cannot be calculated because `window_size` is None.\"\n                )\n            externally_fitted = False\n\n        index = self._extract_index(X)\n        idx = range(len(index))\n        folds = []\n        i = 0\n        last_fold_excluded = False\n\n        if len(index) < self.initial_train_size + self.steps:\n            raise ValueError(\n                f\"The time series must have at least `initial_train_size + steps` \"\n                f\"observations. Got {len(index)} observations.\"\n            )\n\n        while self.initial_train_size + (i * self.steps) + self.gap < len(index):\n\n            if self.refit:\n                # If `fixed_train_size` the train size doesn't increase but moves by \n                # `steps` positions in each iteration. If `False`, the train size\n                # increases by `steps` in each iteration.\n                train_iloc_start = i * (self.steps) if self.fixed_train_size else 0\n                train_iloc_end = self.initial_train_size + i * (self.steps)\n                test_iloc_start = train_iloc_end\n            else:\n                # The train size doesn't increase and doesn't move.\n                train_iloc_start = 0\n                train_iloc_end = self.initial_train_size\n                test_iloc_start = self.initial_train_size + i * (self.steps)\n            \n            if self.window_size is not None:\n                last_window_iloc_start = test_iloc_start - self.window_size\n            test_iloc_end = test_iloc_start + self.gap + self.steps\n        \n            partitions = [\n                idx[train_iloc_start : train_iloc_end],\n                idx[last_window_iloc_start : test_iloc_start] if self.window_size is not None else [],\n                idx[test_iloc_start : test_iloc_end],\n                idx[test_iloc_start + self.gap : test_iloc_end]\n            ]\n            folds.append(partitions)\n            i += 1\n\n        if not self.allow_incomplete_fold and len(folds[-1][3]) < self.steps:\n            folds = folds[:-1]\n            last_fold_excluded = True\n\n        # Replace partitions inside folds with length 0 with `None`\n        folds = [\n            [partition if len(partition) > 0 else None for partition in fold] \n             for fold in folds\n        ]\n\n        # Create a flag to know whether to train the forecaster\n        if self.refit == 0:\n            self.refit = False\n            \n        if isinstance(self.refit, bool):\n            fit_forecaster = [self.refit] * len(folds)\n            fit_forecaster[0] = True\n        else:\n            fit_forecaster = [False] * len(folds)\n            for i in range(0, len(fit_forecaster), self.refit): \n                fit_forecaster[i] = True\n        \n        for i in range(len(folds)): \n            folds[i].append(fit_forecaster[i])\n            if fit_forecaster[i] is False:\n                folds[i][0] = folds[i - 1][0]\n\n        index_to_skip = []\n        if self.skip_folds is not None:\n            if isinstance(self.skip_folds, (int, np.integer)) and self.skip_folds > 0:\n                index_to_keep = np.arange(0, len(folds), self.skip_folds)\n                index_to_skip = np.setdiff1d(np.arange(0, len(folds)), index_to_keep, assume_unique=True)\n                index_to_skip = [int(x) for x in index_to_skip]  # Required since numpy 2.0\n            if isinstance(self.skip_folds, list):\n                index_to_skip = [i for i in self.skip_folds if i < len(folds)]        \n        \n        if self.verbose:\n            self._print_info(\n                index              = index,\n                folds              = folds,\n                externally_fitted  = externally_fitted,\n                last_fold_excluded = last_fold_excluded,\n                index_to_skip      = index_to_skip\n            )\n\n        folds = [fold for i, fold in enumerate(folds) if i not in index_to_skip]\n        if not self.return_all_indexes:\n            # +1 to prevent iloc pandas from deleting the last observation\n            folds = [\n                [[fold[0][0], fold[0][-1] + 1], \n                 [fold[1][0], fold[1][-1] + 1] if self.window_size is not None else [],\n                 [fold[2][0], fold[2][-1] + 1],\n                 [fold[3][0], fold[3][-1] + 1],\n                 fold[4]] \n                for fold in folds\n            ]\n\n        if externally_fitted:\n            self.initial_train_size = None\n            folds[0][4] = False\n\n        if as_pandas:\n            if self.window_size is None:\n                for fold in folds:\n                    fold[1] = [None, None]\n\n            if not self.return_all_indexes:\n                folds = pd.DataFrame(\n                    data = [list(itertools.chain(*fold[:-1])) + [fold[-1]] for fold in folds],\n                    columns = [\n                        'train_start',\n                        'train_end',\n                        'last_window_start',\n                        'last_window_end',\n                        'test_start',\n                        'test_end',\n                        'test_start_with_gap',\n                        'test_end_with_gap',\n                        'fit_forecaster'\n                    ],\n                )\n            else:\n                folds = pd.DataFrame(\n                    data = folds,\n                    columns = [\n                        'train_index',\n                        'last_window_index',\n                        'test_index',\n                        'test_index_with_gap',\n                        'fit_forecaster'\n                    ],\n                )\n            folds.insert(0, 'fold', range(len(folds)))\n\n        return folds\n\n    def _print_info(\n        self,\n        index: pd.Index,\n        folds: list,\n        externally_fitted: bool,\n        last_fold_excluded: bool,\n        index_to_skip: list,\n    ) -> None:\n        \"\"\"\n        Print information about folds.\n        \"\"\"\n\n        print(\"Information of folds\")\n        print(\"--------------------\")\n        if externally_fitted:\n            print(\n                f\"An already trained forecaster is to be used. Window size: \"\n                f\"{self.window_size}\"\n            )\n        else:\n            if self.differentiation is None:\n                print(\n                    f\"Number of observations used for initial training: \"\n                    f\"{self.initial_train_size}\"\n                )\n            else:\n                print(\n                    f\"Number of observations used for initial training: \"\n                    f\"{self.initial_train_size - self.differentiation}\"\n                )\n                print(\n                    f\"    First {self.differentiation} observation/s in training sets \"\n                    f\"are used for differentiation\"\n                )\n        print(\n            f\"Number of observations used for backtesting: \"\n            f\"{len(index) - self.initial_train_size}\"\n        )\n        print(f\"    Number of folds: {len(folds)}\")\n        print(\n            f\"    Number skipped folds: \"\n            f\"{len(index_to_skip)} {index_to_skip if index_to_skip else ''}\"\n        )\n        print(f\"    Number of steps per fold: {self.steps}\")\n        print(\n            f\"    Number of steps to exclude between last observed data \"\n            f\"(last window) and predictions (gap): {self.gap}\"\n        )\n        if last_fold_excluded:\n            print(\"    Last fold has been excluded because it was incomplete.\")\n        if len(folds[-1][3]) < self.steps:\n            print(f\"    Last fold only includes {len(folds[-1][3])} observations.\")\n        print(\"\")\n\n        if self.differentiation is None:\n            differentiation = 0\n        else:\n            differentiation = self.differentiation\n        \n        for i, fold in enumerate(folds):\n            is_fold_skipped   = i in index_to_skip\n            has_training      = fold[-1] if i != 0 else True\n            training_start    = (\n                index[fold[0][0] + differentiation] if fold[0] is not None else None\n            )\n            training_end      = index[fold[0][-1]] if fold[0] is not None else None\n            training_length   = (\n                len(fold[0]) - differentiation if fold[0] is not None else 0\n            )\n            validation_start  = index[fold[3][0]]\n            validation_end    = index[fold[3][-1]]\n            validation_length = len(fold[3])\n\n            print(f\"Fold: {i}\")\n            if is_fold_skipped:\n                print(\"    Fold skipped\")\n            elif not externally_fitted and has_training:\n                print(\n                    f\"    Training:   {training_start} -- {training_end}  \"\n                    f\"(n={training_length})\"\n                )\n                print(\n                    f\"    Validation: {validation_start} -- {validation_end}  \"\n                    f\"(n={validation_length})\"\n                )\n            else:\n                print(\"    Training:   No training in this fold\")\n                print(\n                    f\"    Validation: {validation_start} -- {validation_end}  \"\n                    f\"(n={validation_length})\"\n                )\n\n        print(\"\")\n",
    "skforecast/model_selection/_utils.py": "################################################################################\n#                     skforecast.model_selection._utils                        #\n#                                                                              #\n# This work by skforecast team is licensed under the BSD 3-Clause License.     #\n################################################################################\n# coding=utf-8\n\nfrom typing import Union, Tuple, Optional, Callable, Generator\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom joblib import cpu_count\nfrom tqdm.auto import tqdm\nfrom sklearn.pipeline import Pipeline\nimport sklearn.linear_model\nfrom sklearn.exceptions import NotFittedError\n\nfrom ..exceptions import IgnoredArgumentWarning\nfrom ..metrics import add_y_train_argument, _get_metric\nfrom ..utils import check_interval\n\n\ndef initialize_lags_grid(\n    forecaster: object, \n    lags_grid: Optional[Union[list, dict]] = None\n) -> Tuple[dict, str]:\n    \"\"\"\n    Initialize lags grid and lags label for model selection. \n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model. ForecasterRecursive, ForecasterDirect, \n        ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate.\n    lags_grid : list, dict, default `None`\n        Lists of lags to try, containing int, lists, numpy ndarray, or range \n        objects. If `dict`, the keys are used as labels in the `results` \n        DataFrame, and the values are used as the lists of lags to try.\n\n    Returns\n    -------\n    lags_grid : dict\n        Dictionary with lags configuration for each iteration.\n    lags_label : str\n        Label for lags representation in the results object.\n\n    \"\"\"\n\n    if not isinstance(lags_grid, (list, dict, type(None))):\n        raise TypeError(\n            (f\"`lags_grid` argument must be a list, dict or None. \"\n             f\"Got {type(lags_grid)}.\")\n        )\n\n    lags_label = 'values'\n    if isinstance(lags_grid, list):\n        lags_grid = {f'{lags}': lags for lags in lags_grid}\n    elif lags_grid is None:\n        lags = [int(lag) for lag in forecaster.lags]  # Required since numpy 2.0\n        lags_grid = {f'{lags}': lags}\n    else:\n        lags_label = 'keys'\n\n    return lags_grid, lags_label\n\n\ndef check_backtesting_input(\n    forecaster: object,\n    cv: object,\n    metric: Union[str, Callable, list],\n    add_aggregated_metric: bool = True,\n    y: Optional[pd.Series] = None,\n    series: Optional[Union[pd.DataFrame, dict]] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    interval: Optional[list] = None,\n    alpha: Optional[float] = None,\n    n_boot: int = 250,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = False,\n    n_jobs: Union[int, str] = 'auto',\n    show_progress: bool = True,\n    suppress_warnings: bool = False,\n    suppress_warnings_fit: bool = False\n) -> None:\n    \"\"\"\n    This is a helper function to check most inputs of backtesting functions in \n    modules `model_selection`.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model.\n    cv : TimeSeriesFold\n        TimeSeriesFold object with the information needed to split the data into folds.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n    add_aggregated_metric : bool, default `True`\n        If `True`, the aggregated metrics (average, weighted average and pooling)\n        over all levels are also returned (only multiseries).\n    y : pandas Series, default `None`\n        Training time series for uni-series forecasters.\n    series : pandas DataFrame, dict, default `None`\n        Training time series for multi-series forecasters.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive.\n    alpha : float, default `None`\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %. \n    n_boot : int, default `250`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of prediction \n        error to create prediction intervals.  If `False`, out_sample_residuals \n        are used if they are already stored inside the forecaster.\n    use_binned_residuals : bool, default `False`\n        If `True`, residuals used in each bootstrapping iteration are selected\n        conditioning on the predicted values. If `False`, residuals are selected\n        randomly without conditioning on the predicted values.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the fuction\n        skforecast.utils.select_n_jobs_fit_forecaster.\n        **New in version 0.9.0**\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n    suppress_warnings: bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the backtesting \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    suppress_warnings_fit : bool, default `False`\n        If `True`, warnings generated during fitting will be ignored. Only \n        `ForecasterSarimax`.\n\n    Returns\n    -------\n    None\n    \n    \"\"\"\n\n    forecaster_name = type(forecaster).__name__\n    cv_name = type(cv).__name__\n\n    if cv_name != \"TimeSeriesFold\":\n        raise TypeError(f\"`cv` must be a TimeSeriesFold object. Got {cv_name}.\")\n\n    steps = cv.steps\n    initial_train_size = cv.initial_train_size\n    gap = cv.gap\n    allow_incomplete_fold = cv.allow_incomplete_fold\n    refit = cv.refit\n\n    forecasters_uni = [\n        \"ForecasterRecursive\",\n        \"ForecasterDirect\",\n        \"ForecasterSarimax\",\n        \"ForecasterEquivalentDate\",\n    ]\n    forecasters_multi = [\n        \"ForecasterDirectMultiVariate\",\n        \"ForecasterRnn\",\n    ]\n    forecasters_multi_dict = [\n        \"ForecasterRecursiveMultiSeries\"\n    ]\n\n    if forecaster_name in forecasters_uni:\n        if not isinstance(y, pd.Series):\n            raise TypeError(\"`y` must be a pandas Series.\")\n        data_name = 'y'\n        data_length = len(y)\n\n    elif forecaster_name in forecasters_multi:\n        if not isinstance(series, pd.DataFrame):\n            raise TypeError(\"`series` must be a pandas DataFrame.\")\n        data_name = 'series'\n        data_length = len(series)\n    \n    elif forecaster_name in forecasters_multi_dict:\n        if not isinstance(series, (pd.DataFrame, dict)):\n            raise TypeError(\n                f\"`series` must be a pandas DataFrame or a dict of DataFrames or Series. \"\n                f\"Got {type(series)}.\"\n            )\n        \n        data_name = 'series'\n        if isinstance(series, dict):\n            not_valid_series = [\n                k \n                for k, v in series.items()\n                if not isinstance(v, (pd.Series, pd.DataFrame))\n            ]\n            if not_valid_series:\n                raise TypeError(\n                    f\"If `series` is a dictionary, all series must be a named \"\n                    f\"pandas Series or a pandas DataFrame with a single column. \"\n                    f\"Review series: {not_valid_series}\"\n                )\n            not_valid_index = [\n                k \n                for k, v in series.items()\n                if not isinstance(v.index, pd.DatetimeIndex)\n            ]\n            if not_valid_index:\n                raise ValueError(\n                    f\"If `series` is a dictionary, all series must have a Pandas \"\n                    f\"DatetimeIndex as index with the same frequency. \"\n                    f\"Review series: {not_valid_index}\"\n                )\n\n            indexes_freq = [f'{v.index.freq}' for v in series.values()]\n            indexes_freq = sorted(set(indexes_freq))\n            if not len(indexes_freq) == 1:\n                raise ValueError(\n                    f\"If `series` is a dictionary, all series must have a Pandas \"\n                    f\"DatetimeIndex as index with the same frequency. \"\n                    f\"Found frequencies: {indexes_freq}\"\n                )\n            data_length = max([len(series[serie]) for serie in series])\n        else:\n            data_length = len(series)\n\n    if exog is not None:\n        if forecaster_name in forecasters_multi_dict:\n            if not isinstance(exog, (pd.Series, pd.DataFrame, dict)):\n                raise TypeError(\n                    f\"`exog` must be a pandas Series, DataFrame, dictionary of pandas \"\n                    f\"Series/DataFrames or None. Got {type(exog)}.\"\n                )\n            if isinstance(exog, dict):\n                not_valid_exog = [\n                    k \n                    for k, v in exog.items()\n                    if not isinstance(v, (pd.Series, pd.DataFrame, type(None)))\n                ]\n                if not_valid_exog:\n                    raise TypeError(\n                        f\"If `exog` is a dictionary, All exog must be a named pandas \"\n                        f\"Series, a pandas DataFrame or None. Review exog: {not_valid_exog}\"\n                    )\n        else:\n            if not isinstance(exog, (pd.Series, pd.DataFrame)):\n                raise TypeError(\n                    f\"`exog` must be a pandas Series, DataFrame or None. Got {type(exog)}.\"\n                )\n\n    if hasattr(forecaster, 'differentiation'):\n        if forecaster.differentiation != cv.differentiation:\n            raise ValueError(\n                f\"The differentiation included in the forecaster \"\n                f\"({forecaster.differentiation}) differs from the differentiation \"\n                f\"included in the cv ({cv.differentiation}). Set the same value \"\n                f\"for both using the `differentiation` argument.\"\n            )\n\n    if not isinstance(metric, (str, Callable, list)):\n        raise TypeError(\n            f\"`metric` must be a string, a callable function, or a list containing \"\n            f\"multiple strings and/or callables. Got {type(metric)}.\"\n        )\n\n    if forecaster_name == \"ForecasterEquivalentDate\" and isinstance(\n        forecaster.offset, pd.tseries.offsets.DateOffset\n    ):\n        if initial_train_size is None:\n            raise ValueError(\n                f\"`initial_train_size` must be an integer greater than \"\n                f\"the `window_size` of the forecaster ({forecaster.window_size}) \"\n                f\"and smaller than the length of `{data_name}` ({data_length}).\"\n            )\n    elif initial_train_size is not None:\n        if initial_train_size < forecaster.window_size or initial_train_size >= data_length:\n            raise ValueError(\n                f\"If used, `initial_train_size` must be an integer greater than \"\n                f\"the `window_size` of the forecaster ({forecaster.window_size}) \"\n                f\"and smaller than the length of `{data_name}` ({data_length}).\"\n            )\n        if initial_train_size + gap >= data_length:\n            raise ValueError(\n                f\"The combination of initial_train_size {initial_train_size} and \"\n                f\"gap {gap} cannot be greater than the length of `{data_name}` \"\n                f\"({data_length}).\"\n            )\n    else:\n        if forecaster_name in ['ForecasterSarimax', 'ForecasterEquivalentDate']:\n            raise ValueError(\n                f\"`initial_train_size` must be an integer smaller than the \"\n                f\"length of `{data_name}` ({data_length}).\"\n            )\n        else:\n            if not forecaster.is_fitted:\n                raise NotFittedError(\n                    \"`forecaster` must be already trained if no `initial_train_size` \"\n                    \"is provided.\"\n                )\n            if refit:\n                raise ValueError(\n                    \"`refit` is only allowed when `initial_train_size` is not `None`.\"\n                )\n\n    if forecaster_name == 'ForecasterSarimax' and cv.skip_folds is not None:\n        raise ValueError(\n            \"`skip_folds` is not allowed for ForecasterSarimax. Set it to `None`.\"\n        )\n\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError(\"`add_aggregated_metric` must be a boolean: `True`, `False`.\")\n    if not isinstance(n_boot, (int, np.integer)) or n_boot < 0:\n        raise TypeError(f\"`n_boot` must be an integer greater than 0. Got {n_boot}.\")\n    if not isinstance(random_state, (int, np.integer)) or random_state < 0:\n        raise TypeError(f\"`random_state` must be an integer greater than 0. Got {random_state}.\")\n    if not isinstance(use_in_sample_residuals, bool):\n        raise TypeError(\"`use_in_sample_residuals` must be a boolean: `True`, `False`.\")\n    if not isinstance(use_binned_residuals, bool):\n        raise TypeError(\"`use_binned_residuals` must be a boolean: `True`, `False`.\")\n    if not isinstance(n_jobs, int) and n_jobs != 'auto':\n        raise TypeError(f\"`n_jobs` must be an integer or `'auto'`. Got {n_jobs}.\")\n    if not isinstance(show_progress, bool):\n        raise TypeError(\"`show_progress` must be a boolean: `True`, `False`.\")\n    if not isinstance(suppress_warnings, bool):\n        raise TypeError(\"`suppress_warnings` must be a boolean: `True`, `False`.\")\n    if not isinstance(suppress_warnings_fit, bool):\n        raise TypeError(\"`suppress_warnings_fit` must be a boolean: `True`, `False`.\")\n\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n\n    if not allow_incomplete_fold and data_length - (initial_train_size + gap) < steps:\n        raise ValueError(\n            f\"There is not enough data to evaluate {steps} steps in a single \"\n            f\"fold. Set `allow_incomplete_fold` to `True` to allow incomplete folds.\\n\"\n            f\"    Data available for test : {data_length - (initial_train_size + gap)}\\n\"\n            f\"    Steps                   : {steps}\"\n        )\n\n\ndef select_n_jobs_backtesting(\n    forecaster: object,\n    refit: Union[bool, int]\n) -> int:\n    \"\"\"\n    Select the optimal number of jobs to use in the backtesting process. This\n    selection is based on heuristics and is not guaranteed to be optimal.\n\n    The number of jobs is chosen as follows:\n\n    - If `refit` is an integer, then `n_jobs = 1`. This is because parallelization doesn't \n    work with intermittent refit.\n    - If forecaster is 'ForecasterRecursive' and regressor is a linear regressor, \n    then `n_jobs = 1`.\n    - If forecaster is 'ForecasterRecursive' and regressor is not a linear \n    regressor then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and `refit = True`, then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and `refit = False`, then `n_jobs = 1`.\n    - If forecaster is 'ForecasterRecursiveMultiSeries', then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterSarimax' or 'ForecasterEquivalentDate', \n    then `n_jobs = 1`.\n    - If regressor is a `LGBMRegressor(n_jobs=1)`, then `n_jobs = cpu_count() - 1`.\n    - If regressor is a `LGBMRegressor` with internal n_jobs != 1, then `n_jobs = 1`.\n    This is because `lightgbm` is highly optimized for gradient boosting and\n    parallelizes operations at a very fine-grained level, making additional\n    parallelization unnecessary and potentially harmful due to resource contention.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model.\n    refit : bool, int\n        If the forecaster is refitted during the backtesting process.\n\n    Returns\n    -------\n    n_jobs : int\n        The number of jobs to run in parallel.\n    \n    \"\"\"\n\n    forecaster_name = type(forecaster).__name__\n\n    if isinstance(forecaster.regressor, Pipeline):\n        regressor = forecaster.regressor[-1]\n        regressor_name = type(regressor).__name__\n    else:\n        regressor = forecaster.regressor\n        regressor_name = type(regressor).__name__\n\n    linear_regressors = [\n        regressor_name\n        for regressor_name in dir(sklearn.linear_model)\n        if not regressor_name.startswith('_')\n    ]\n\n    refit = False if refit == 0 else refit\n    if not isinstance(refit, bool) and refit != 1:\n        n_jobs = 1\n    else:\n        if forecaster_name in ['ForecasterRecursive']:\n            if regressor_name in linear_regressors:\n                n_jobs = 1\n            elif regressor_name == 'LGBMRegressor':\n                n_jobs = cpu_count() - 1 if regressor.n_jobs == 1 else 1\n            else:\n                n_jobs = cpu_count() - 1\n        elif forecaster_name in ['ForecasterDirect', 'ForecasterDirectMultiVariate']:\n            # Parallelization is applied during the fitting process.\n            n_jobs = 1\n        elif forecaster_name in ['ForecasterRecursiveMultiSeries']:\n            if regressor_name == 'LGBMRegressor':\n                n_jobs = cpu_count() - 1 if regressor.n_jobs == 1 else 1\n            else:\n                n_jobs = cpu_count() - 1\n        elif forecaster_name in ['ForecasterSarimax', 'ForecasterEquivalentDate']:\n            n_jobs = 1\n        else:\n            n_jobs = 1\n\n    return n_jobs\n\n\ndef _calculate_metrics_one_step_ahead(\n    forecaster: object,\n    y: pd.Series,\n    metrics: list,\n    X_train: pd.DataFrame,\n    y_train: Union[pd.Series, dict],\n    X_test: pd.DataFrame,\n    y_test: Union[pd.Series, dict]\n) -> list:\n    \"\"\"\n    Calculate metrics when predictions are one-step-ahead. When forecaster is\n    of type ForecasterDirect only the regressor for step 1 is used.\n\n    Parameters\n    ----------\n    forecaster : object\n        Forecaster model.\n    y : pandas Series\n        Time series data used to train and test the model.\n    metrics : list\n        List of metrics.\n    X_train : pandas DataFrame\n        Predictor values used to train the model.\n    y_train : pandas Series\n        Target values related to each row of `X_train`.\n    X_test : pandas DataFrame\n        Predictor values used to test the model.\n    y_test : pandas Series\n        Target values related to each row of `X_test`.\n\n    Returns\n    -------\n    metric_values : list\n        List with metric values.\n    \n    \"\"\"\n\n    if type(forecaster).__name__ == 'ForecasterDirect':\n\n        step = 1  # Only the model for step 1 is optimized.\n        X_train, y_train = forecaster.filter_train_X_y_for_step(\n                               step    = step,\n                               X_train = X_train,\n                               y_train = y_train\n                           )\n        X_test, y_test = forecaster.filter_train_X_y_for_step(\n                             step    = step,  \n                             X_train = X_test,\n                             y_train = y_test\n                         )\n        forecaster.regressors_[step].fit(X_train, y_train)\n        y_pred = forecaster.regressors_[step].predict(X_test)\n\n    else:\n        forecaster.regressor.fit(X_train, y_train)\n        y_pred = forecaster.regressor.predict(X_test)\n\n    y_true = y_test.to_numpy()\n    y_pred = y_pred.ravel()\n    y_train = y_train.to_numpy()\n\n    if forecaster.differentiation is not None:\n        y_true = forecaster.differentiator.inverse_transform_next_window(y_true)\n        y_pred = forecaster.differentiator.inverse_transform_next_window(y_pred)\n        y_train = forecaster.differentiator.inverse_transform_training(y_train)\n\n    if forecaster.transformer_y is not None:\n        y_true = forecaster.transformer_y.inverse_transform(y_true.reshape(-1, 1))\n        y_pred = forecaster.transformer_y.inverse_transform(y_pred.reshape(-1, 1))\n        y_train = forecaster.transformer_y.inverse_transform(y_train.reshape(-1, 1))\n\n    metric_values = []\n    for m in metrics:\n        metric_values.append(\n            m(y_true=y_true.ravel(), y_pred=y_pred.ravel(), y_train=y_train.ravel())\n        )\n\n    return metric_values\n\n\ndef _initialize_levels_model_selection_multiseries(\n    forecaster: object, \n    series: Union[pd.DataFrame, dict],\n    levels: Optional[Union[str, list]] = None\n) -> list:\n    \"\"\"\n    Initialize levels for model_selection multi-series functions.\n\n    Parameters\n    ----------\n    forecaster : ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate, ForecasterRnn\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Training time series.\n    levels : str, list, default `None`\n        level (`str`) or levels (`list`) at which the forecaster is optimized. \n        If `None`, all levels are taken into account. The resulting metric will be\n        the average of the optimization of all levels.\n\n    Returns\n    -------\n    levels : list\n        List of levels to be used in model_selection multi-series functions.\n    \n    \"\"\"\n\n    multi_series_forecasters_with_levels = [\n        'ForecasterRecursiveMultiSeries', \n        'ForecasterRnn'\n    ]\n\n    if type(forecaster).__name__ in multi_series_forecasters_with_levels  \\\n        and not isinstance(levels, (str, list, type(None))):\n        raise TypeError(\n            (f\"`levels` must be a `list` of column names, a `str` of a column \"\n             f\"name or `None` when using a forecaster of type \"\n             f\"{multi_series_forecasters_with_levels}. If the forecaster is of \"\n             f\"type `ForecasterDirectMultiVariate`, this argument is ignored.\")\n        )\n\n    if type(forecaster).__name__ == 'ForecasterDirectMultiVariate':\n        if levels and levels != forecaster.level and levels != [forecaster.level]:\n            warnings.warn(\n                (f\"`levels` argument have no use when the forecaster is of type \"\n                 f\"`ForecasterDirectMultiVariate`. The level of this forecaster \"\n                 f\"is '{forecaster.level}', to predict another level, change \"\n                 f\"the `level` argument when initializing the forecaster. \\n\"),\n                 IgnoredArgumentWarning\n            )\n        levels = [forecaster.level]\n    else:\n        if levels is None:\n            # Forecaster could be untrained, so self.series_col_names cannot be used.\n            if isinstance(series, pd.DataFrame):\n                levels = list(series.columns)\n            else:\n                levels = list(series.keys())\n        elif isinstance(levels, str):\n            levels = [levels]\n\n    return levels\n\n\ndef _extract_data_folds_multiseries(\n    series: Union[pd.Series, pd.DataFrame, dict],\n    folds: list,\n    span_index: Union[pd.DatetimeIndex, pd.RangeIndex],\n    window_size: int,\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    dropna_last_window: bool = False,\n    externally_fitted: bool = False\n) -> Generator[\n        Tuple[\n            Union[pd.Series, pd.DataFrame, dict],\n            pd.DataFrame,\n            list,\n            Optional[Union[pd.Series, pd.DataFrame, dict]],\n            Optional[Union[pd.Series, pd.DataFrame, dict]],\n            list\n        ],\n        None,\n        None\n    ]:\n    \"\"\"\n    Select the data from series and exog that corresponds to each fold created using the\n    skforecast.model_selection._create_backtesting_folds function.\n\n    Parameters\n    ----------\n    series : pandas Series, pandas DataFrame, dict\n        Time series.\n    folds : list\n        Folds created using the skforecast.model_selection._create_backtesting_folds\n        function.\n    span_index : pandas DatetimeIndex, pandas RangeIndex\n        Full index from the minimum to the maximum index among all series.\n    window_size : int\n        Size of the window needed to create the predictors.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    dropna_last_window : bool, default `False`\n        If `True`, drop the columns of the last window that have NaN values.\n    externally_fitted : bool, default `False`\n        Flag indicating whether the forecaster is already trained. Only used when \n        `initial_train_size` is None and `refit` is False.\n\n    Yield\n    -----\n    series_train : pandas Series, pandas DataFrame, dict\n        Time series corresponding to the training set of the fold.\n    series_last_window: pandas DataFrame\n        Time series corresponding to the last window of the fold.\n    levels_last_window: list\n        Levels of the time series present in the last window of the fold.\n    exog_train: pandas Series, pandas DataFrame, dict, None\n        Exogenous variable corresponding to the training set of the fold.\n    exog_test: pandas Series, pandas DataFrame, dict, None\n        Exogenous variable corresponding to the test set of the fold.\n    fold: list\n        Fold created using the skforecast.model_selection._create_backtesting_folds\n\n    \"\"\"\n\n    for fold in folds:\n        train_iloc_start       = fold[0][0]\n        train_iloc_end         = fold[0][1]\n        last_window_iloc_start = fold[1][0]\n        last_window_iloc_end   = fold[1][1]\n        test_iloc_start        = fold[2][0]\n        test_iloc_end          = fold[2][1]\n\n        if isinstance(series, dict) or isinstance(exog, dict):\n            # Substract 1 to the iloc indexes to get the loc indexes\n            train_loc_start       = span_index[train_iloc_start]\n            train_loc_end         = span_index[train_iloc_end - 1]\n            last_window_loc_start = span_index[last_window_iloc_start]\n            last_window_loc_end   = span_index[last_window_iloc_end - 1]\n            test_loc_start        = span_index[test_iloc_start]\n            test_loc_end          = span_index[test_iloc_end - 1]\n\n        if isinstance(series, pd.DataFrame):\n            series_train = series.iloc[train_iloc_start:train_iloc_end, ]\n\n            series_to_drop = []\n            for col in series_train.columns:\n                if series_train[col].isna().all():\n                    series_to_drop.append(col)\n                else:\n                    first_valid_index = series_train[col].first_valid_index()\n                    last_valid_index = series_train[col].last_valid_index()\n                    if (\n                        len(series_train[col].loc[first_valid_index:last_valid_index])\n                        < window_size\n                    ):\n                        series_to_drop.append(col)\n\n            series_last_window = series.iloc[\n                last_window_iloc_start:last_window_iloc_end,\n            ]\n            \n            series_train = series_train.drop(columns=series_to_drop)\n            if not externally_fitted:\n                series_last_window = series_last_window.drop(columns=series_to_drop)\n        else:\n            series_train = {}\n            for k in series.keys():\n                v = series[k].loc[train_loc_start:train_loc_end]\n                if not v.isna().all():\n                    first_valid_index = v.first_valid_index()\n                    last_valid_index  = v.last_valid_index()\n                    if first_valid_index is not None and last_valid_index is not None:\n                        v = v.loc[first_valid_index : last_valid_index]\n                        if len(v) >= window_size:\n                            series_train[k] = v\n\n            series_last_window = {}\n            for k, v in series.items():\n                v = series[k].loc[last_window_loc_start:last_window_loc_end]\n                if ((externally_fitted or k in series_train) and len(v) >= window_size):\n                    series_last_window[k] = v\n\n            series_last_window = pd.DataFrame(series_last_window)\n\n        if dropna_last_window:\n            series_last_window = series_last_window.dropna(axis=1, how=\"any\")\n            # TODO: add the option to drop the series without minimum non NaN values.\n            # Similar to how pandas does in the rolling window function.\n        \n        levels_last_window = list(series_last_window.columns)\n\n        if exog is not None:\n            if isinstance(exog, (pd.Series, pd.DataFrame)):\n                exog_train = exog.iloc[train_iloc_start:train_iloc_end, ]\n                exog_test = exog.iloc[test_iloc_start:test_iloc_end, ]\n            else:\n                exog_train = {\n                    k: v.loc[train_loc_start:train_loc_end] \n                    for k, v in exog.items()\n                }\n                exog_train = {k: v for k, v in exog_train.items() if len(v) > 0}\n\n                exog_test = {\n                    k: v.loc[test_loc_start:test_loc_end]\n                    for k, v in exog.items()\n                    if externally_fitted or k in exog_train\n                }\n\n                exog_test = {k: v for k, v in exog_test.items() if len(v) > 0}\n        else:\n            exog_train = None\n            exog_test = None\n\n        yield series_train, series_last_window, levels_last_window, exog_train, exog_test, fold\n\n\ndef _calculate_metrics_backtesting_multiseries(\n    series: Union[pd.DataFrame, dict],\n    predictions: pd.DataFrame,\n    folds: Union[list, tqdm],\n    span_index: Union[pd.DatetimeIndex, pd.RangeIndex],\n    window_size: int,\n    metrics: list,\n    levels: list,\n    add_aggregated_metric: bool = True\n) -> pd.DataFrame:\n    \"\"\"   \n    Calculate metrics for each level and also for all levels aggregated using\n    average, weighted average or pooling.\n\n    - 'average': the average (arithmetic mean) of all levels.\n    - 'weighted_average': the average of the metrics weighted by the number of\n    predicted values of each level.\n    - 'pooling': the values of all levels are pooled and then the metric is\n    calculated.\n\n    Parameters\n    ----------\n    series : pandas DataFrame, dict\n        Series data used for backtesting.\n    predictions : pandas DataFrame\n        Predictions generated during the backtesting process.\n    folds : list, tqdm\n        Folds created during the backtesting process.\n    span_index : pandas DatetimeIndex, pandas RangeIndex\n        Full index from the minimum to the maximum index among all series.\n    window_size : int\n        Size of the window used by the forecaster to create the predictors.\n        This is used remove the first `window_size` (differentiation included) \n        values from y_train since they are not part of the training matrix.\n    metrics : list\n        List of metrics to calculate.\n    levels : list\n        Levels to calculate the metrics.\n    add_aggregated_metric : bool, default `True`\n        If `True`, and multiple series (`levels`) are predicted, the aggregated\n        metrics (average, weighted average and pooled) are also returned.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s).\n    \n    \"\"\"\n\n    if not isinstance(series, (pd.DataFrame, dict)):\n        raise TypeError(\n            (\"`series` must be a pandas DataFrame or a dictionary of pandas \"\n             \"DataFrames.\")\n        )\n    if not isinstance(predictions, pd.DataFrame):\n        raise TypeError(\"`predictions` must be a pandas DataFrame.\")\n    if not isinstance(folds, (list, tqdm)):\n        raise TypeError(\"`folds` must be a list or a tqdm object.\")\n    if not isinstance(span_index, (pd.DatetimeIndex, pd.RangeIndex)):\n        raise TypeError(\"`span_index` must be a pandas DatetimeIndex or pandas RangeIndex.\")\n    if not isinstance(window_size, (int, np.integer)):\n        raise TypeError(\"`window_size` must be an integer.\")\n    if not isinstance(metrics, list):\n        raise TypeError(\"`metrics` must be a list.\")\n    if not isinstance(levels, list):\n        raise TypeError(\"`levels` must be a list.\")\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError(\"`add_aggregated_metric` must be a boolean.\")\n    \n    metric_names = [(m if isinstance(m, str) else m.__name__) for m in metrics]\n\n    y_true_pred_levels = []\n    y_train_levels = []\n    for level in levels:\n        y_true_pred_level = None\n        y_train = None\n        if level in predictions.columns:\n            # TODO: avoid merges inside the loop, instead merge outside and then filter\n            y_true_pred_level = pd.merge(\n                series[level],\n                predictions[level],\n                left_index  = True,\n                right_index = True,\n                how         = \"inner\",\n            ).dropna(axis=0, how=\"any\")\n            y_true_pred_level.columns = ['y_true', 'y_pred']\n\n            train_indexes = []\n            for i, fold in enumerate(folds):\n                fit_fold = fold[-1]\n                if i == 0 or fit_fold:\n                    train_iloc_start = fold[0][0]\n                    train_iloc_end = fold[0][1]\n                    train_indexes.append(np.arange(train_iloc_start, train_iloc_end))\n            train_indexes = np.unique(np.concatenate(train_indexes))\n            train_indexes = span_index[train_indexes]\n            y_train = series[level].loc[series[level].index.intersection(train_indexes)]\n\n        y_true_pred_levels.append(y_true_pred_level)\n        y_train_levels.append(y_train)\n            \n    metrics_levels = []\n    for i, level in enumerate(levels):\n        if y_true_pred_levels[i] is not None and not y_true_pred_levels[i].empty:\n            metrics_level = [\n                m(\n                    y_true = y_true_pred_levels[i].iloc[:, 0],\n                    y_pred = y_true_pred_levels[i].iloc[:, 1],\n                    y_train = y_train_levels[i].iloc[window_size:]  # Exclude observations used to create predictors\n                )\n                for m in metrics\n            ]\n            metrics_levels.append(metrics_level)\n        else:\n            metrics_levels.append([None for _ in metrics])\n\n    metrics_levels = pd.DataFrame(\n                         data    = metrics_levels,\n                         columns = [m if isinstance(m, str) else m.__name__\n                                    for m in metrics]\n                     )\n    metrics_levels.insert(0, 'levels', levels)\n\n    if len(levels) < 2:\n        add_aggregated_metric = False\n    \n    if add_aggregated_metric:\n\n        # aggragation: average\n        average = metrics_levels.drop(columns='levels').mean(skipna=True)\n        average = average.to_frame().transpose()\n        average['levels'] = 'average'\n\n        # aggregation: weighted_average\n        weighted_averages = {}\n        n_predictions_levels = (\n            predictions\n            .notna()\n            .sum()\n            .to_frame(name='n_predictions')\n            .reset_index(names='levels')\n        )\n        metrics_levels_no_missing = (\n            metrics_levels.merge(n_predictions_levels, on='levels', how='inner')\n        )\n        for col in metric_names:\n            weighted_averages[col] = np.average(\n                metrics_levels_no_missing[col],\n                weights=metrics_levels_no_missing['n_predictions']\n            )\n        weighted_average = pd.DataFrame(weighted_averages, index=[0])\n        weighted_average['levels'] = 'weighted_average'\n\n        # aggregation: pooling\n        y_true_pred_levels, y_train_levels = zip(\n            *[\n                (a, b.iloc[window_size:])  # Exclude observations used to create predictors\n                for a, b in zip(y_true_pred_levels, y_train_levels)\n                if a is not None\n            ]\n        )\n        y_train_levels = list(y_train_levels)\n        y_true_pred_levels = pd.concat(y_true_pred_levels)\n        y_train_levels_concat = pd.concat(y_train_levels)\n\n        pooled = []\n        for m, m_name in zip(metrics, metric_names):\n            if m_name in ['mean_absolute_scaled_error', 'root_mean_squared_scaled_error']:\n                pooled.append(\n                    m(\n                        y_true = y_true_pred_levels.loc[:, 'y_true'],\n                        y_pred = y_true_pred_levels.loc[:, 'y_pred'],\n                        y_train = y_train_levels\n                    )\n                )\n            else:\n                pooled.append(\n                    m(\n                        y_true = y_true_pred_levels.loc[:, 'y_true'],\n                        y_pred = y_true_pred_levels.loc[:, 'y_pred'],\n                        y_train = y_train_levels_concat\n                    )\n                )\n        pooled = pd.DataFrame([pooled], columns=metric_names)\n        pooled['levels'] = 'pooling'\n\n        metrics_levels = pd.concat(\n            [metrics_levels, average, weighted_average, pooled],\n            axis=0,\n            ignore_index=True\n        )\n\n    return metrics_levels\n\n\ndef _predict_and_calculate_metrics_one_step_ahead_multiseries(\n    forecaster: object,\n    series: Union[pd.DataFrame, dict],\n    X_train: pd.DataFrame,\n    y_train: Union[pd.Series, dict],\n    X_test: pd.DataFrame,\n    y_test: Union[pd.Series, dict],\n    X_train_encoding: pd.Series,\n    X_test_encoding: pd.Series,\n    levels: list,\n    metrics: list,\n    add_aggregated_metric: bool = True\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"   \n    One-step-ahead predictions and metrics for each level and also for all levels\n    aggregated using average, weighted average or pooling.\n    Input matrices (X_train, y_train, X_train_encoding, X_test, y_test, X_test_encoding)\n    should have been generated using the forecaster._train_test_split_one_step_ahead().\n\n    - 'average': the average (arithmetic mean) of all levels.\n    - 'weighted_average': the average of the metrics weighted by the number of\n    predicted values of each level.\n    - 'pooling': the values of all levels are pooled and then the metric is\n    calculated.\n\n    Parameters\n    ----------\n    forecaster : object\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Series data used to train and test the forecaster.\n    X_train : pandas DataFrame\n        Training matrix.\n    y_train : pandas Series, dict\n        Target values of the training set.\n    X_test : pandas DataFrame\n        Test matrix.\n    y_test : pandas Series, dict\n        Target values of the test set.\n    X_train_encoding : pandas Series\n        Series identifiers for each row of `X_train`.\n    X_test_encoding : pandas Series\n        Series identifiers for each row of `X_test`.\n    levels : list\n        Levels to calculate the metrics.\n    metrics : list\n        List of metrics to calculate.\n    add_aggregated_metric : bool, default `True`\n        If `True`, and multiple series (`levels`) are predicted, the aggregated\n        metrics (average, weighted average and pooled) are also returned.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s).\n    predictions : pandas DataFrame\n        Value of predictions for each level.\n    \n    \"\"\"\n\n    if not isinstance(series, (pd.DataFrame, dict)):\n        raise TypeError(\n            \"`series` must be a pandas DataFrame or a dictionary of pandas \"\n            \"DataFrames.\"\n        )\n    if not isinstance(X_train, pd.DataFrame):\n        raise TypeError(f\"`X_train` must be a pandas DataFrame. Got: {type(X_train)}\")\n    if not isinstance(y_train, (pd.Series, dict)):\n        raise TypeError(\n            f\"`y_train` must be a pandas Series or a dictionary of pandas Series. \"\n            f\"Got: {type(y_train)}\"\n        )        \n    if not isinstance(X_test, pd.DataFrame):\n        raise TypeError(f\"`X_test` must be a pandas DataFrame. Got: {type(X_test)}\")\n    if not isinstance(y_test, (pd.Series, dict)):\n        raise TypeError(\n            f\"`y_test` must be a pandas Series or a dictionary of pandas Series. \"\n            f\"Got: {type(y_test)}\"\n        )\n    if not isinstance(X_train_encoding, pd.Series):\n        raise TypeError(\n            f\"`X_train_encoding` must be a pandas Series. Got: {type(X_train_encoding)}\"\n        )\n    if not isinstance(X_test_encoding, pd.Series):\n        raise TypeError(\n            f\"`X_test_encoding` must be a pandas Series. Got: {type(X_test_encoding)}\"\n        )\n    if not isinstance(levels, list):\n        raise TypeError(f\"`levels` must be a list. Got: {type(levels)}\")\n    if not isinstance(metrics, list):\n        raise TypeError(f\"`metrics` must be a list. Got: {type(metrics)}\")\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError(\n            f\"`add_aggregated_metric` must be a boolean. Got: {type(add_aggregated_metric)}\"\n        )\n    \n    metrics = [\n        _get_metric(metric=m)\n        if isinstance(m, str)\n        else add_y_train_argument(m) \n        for m in metrics\n    ]\n    metric_names = [(m if isinstance(m, str) else m.__name__) for m in metrics]\n\n    if isinstance(series[levels[0]].index, pd.DatetimeIndex):\n        freq = series[levels[0]].index.freq\n    else:\n        freq = series[levels[0]].index.step\n\n    if type(forecaster).__name__ == 'ForecasterDirectMultiVariate':\n        step = 1\n        X_train, y_train = forecaster.filter_train_X_y_for_step(\n                               step    = step,\n                               X_train = X_train,\n                               y_train = y_train\n                           )\n        X_test, y_test = forecaster.filter_train_X_y_for_step(\n                             step    = step,  \n                             X_train = X_test,\n                             y_train = y_test\n                         )                 \n        forecaster.regressors_[step].fit(X_train, y_train)\n        pred = forecaster.regressors_[step].predict(X_test)\n    else:\n        forecaster.regressor.fit(X_train, y_train)\n        pred = forecaster.regressor.predict(X_test)\n\n    predictions_per_level = pd.DataFrame(\n        {\n            'y_true': y_test,\n            'y_pred': pred,\n            '_level_skforecast': X_test_encoding,\n        },\n        index=y_test.index,\n    ).groupby('_level_skforecast')\n    predictions_per_level = {key: group for key, group in predictions_per_level}\n\n    y_train_per_level = pd.DataFrame(\n        {\"y_train\": y_train, \"_level_skforecast\": X_train_encoding},\n        index=y_train.index,\n    ).groupby(\"_level_skforecast\")\n    # Interleaved Nan values were excluded fom y_train. They are reestored\n    y_train_per_level = {key: group.asfreq(freq) for key, group in y_train_per_level}\n\n    if forecaster.differentiation is not None:\n        for level in predictions_per_level:\n            predictions_per_level[level][\"y_true\"] = (\n                forecaster.differentiator_[level].inverse_transform_next_window(\n                    predictions_per_level[level][\"y_true\"].to_numpy()\n                )\n            )\n            predictions_per_level[level][\"y_pred\"] = (\n                forecaster.differentiator_[level].inverse_transform_next_window(\n                    predictions_per_level[level][\"y_pred\"].to_numpy()\n                )   \n            )\n            y_train_per_level[level][\"y_train\"] = (\n                forecaster.differentiator_[level].inverse_transform_training(\n                    y_train_per_level[level][\"y_train\"].to_numpy()\n                )\n            )\n\n    if forecaster.transformer_series is not None:\n        for level in predictions_per_level:\n            transformer = forecaster.transformer_series_[level]\n            predictions_per_level[level][\"y_true\"] = transformer.inverse_transform(\n                predictions_per_level[level][[\"y_true\"]]\n            )\n            predictions_per_level[level][\"y_pred\"] = transformer.inverse_transform(\n                predictions_per_level[level][[\"y_pred\"]]\n            )\n            y_train_per_level[level][\"y_train\"] = transformer.inverse_transform(\n                y_train_per_level[level][[\"y_train\"]]\n            )\n    \n    metrics_levels = []\n    for level in levels:\n        if level in predictions_per_level:\n            metrics_level = [\n                m(\n                    y_true  = predictions_per_level[level].loc[:, 'y_true'],\n                    y_pred  = predictions_per_level[level].loc[:, 'y_pred'],\n                    y_train = y_train_per_level[level].loc[:, 'y_train']\n                )\n                for m in metrics\n            ]\n            metrics_levels.append(metrics_level)\n        else:\n            metrics_levels.append([None for _ in metrics])\n\n    metrics_levels = pd.DataFrame(\n                         data    = metrics_levels,\n                         columns = [m if isinstance(m, str) else m.__name__\n                                    for m in metrics]\n                     )\n    metrics_levels.insert(0, 'levels', levels)\n\n    if len(levels) < 2:\n        add_aggregated_metric = False\n\n    if add_aggregated_metric:\n\n        # aggragation: average\n        average = metrics_levels.drop(columns='levels').mean(skipna=True)\n        average = average.to_frame().transpose()\n        average['levels'] = 'average'\n\n        # aggregation: weighted_average\n        weighted_averages = {}\n        n_predictions_levels = {\n            k: v['y_pred'].notna().sum()\n            for k, v in predictions_per_level.items()\n        }\n        n_predictions_levels = pd.DataFrame(\n            n_predictions_levels.items(),\n            columns=['levels', 'n_predictions']\n        )\n        metrics_levels_no_missing = (\n            metrics_levels.merge(n_predictions_levels, on='levels', how='inner')\n        )\n        for col in metric_names:\n            weighted_averages[col] = np.average(\n                metrics_levels_no_missing[col],\n                weights=metrics_levels_no_missing['n_predictions']\n            )\n        weighted_average = pd.DataFrame(weighted_averages, index=[0])\n        weighted_average['levels'] = 'weighted_average'\n\n        # aggregation: pooling\n        list_y_train_by_level = [\n            v['y_train'].to_numpy()\n            for k, v in y_train_per_level.items()\n            if k in predictions_per_level\n        ]\n        predictions_pooled = pd.concat(predictions_per_level.values())\n        y_train_pooled = pd.concat(\n            [v for k, v in y_train_per_level.items() if k in predictions_per_level]\n        )\n        pooled = []\n        for m, m_name in zip(metrics, metric_names):\n            if m_name in ['mean_absolute_scaled_error', 'root_mean_squared_scaled_error']:\n                pooled.append(\n                    m(\n                        y_true  = predictions_pooled['y_true'],\n                        y_pred  = predictions_pooled['y_pred'],\n                        y_train = list_y_train_by_level\n                    )\n                )\n            else:\n                pooled.append(\n                    m(\n                        y_true  = predictions_pooled['y_true'],\n                        y_pred  = predictions_pooled['y_pred'],\n                        y_train = y_train_pooled['y_train']\n                    )\n                )\n        pooled = pd.DataFrame([pooled], columns=metric_names)\n        pooled['levels'] = 'pooling'\n\n        metrics_levels = pd.concat(\n            [metrics_levels, average, weighted_average, pooled],\n            axis=0,\n            ignore_index=True\n        )\n\n    predictions = (\n        pd.concat(predictions_per_level.values())\n        .loc[:, [\"y_pred\", \"_level_skforecast\"]]\n        .pivot(columns=\"_level_skforecast\", values=\"y_pred\")\n        .rename_axis(columns=None, index=None)\n    )\n    predictions = predictions.asfreq(X_test.index.freq)\n\n    return metrics_levels, predictions\n",
    "skforecast/utils/utils.py": "################################################################################\n#                               skforecast.utils                               #\n#                                                                              #\n# This work by skforecast team is licensed under the BSD 3-Clause License.     #\n################################################################################\n# coding=utf-8\n\nimport importlib\nimport inspect\nimport warnings\nfrom copy import deepcopy\nfrom typing import Any, Callable, Optional, Tuple, Union\nfrom pathlib import Path\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport sklearn.linear_model\nfrom sklearn.base import clone\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.exceptions import NotFittedError\nimport skforecast\nfrom ..exceptions import warn_skforecast_categories\nfrom ..exceptions import (\n    MissingValuesWarning,\n    MissingExogWarning,\n    DataTypeWarning,\n    UnknownLevelWarning,\n    IgnoredArgumentWarning,\n    SaveLoadSkforecastWarning,\n    SkforecastVersionWarning\n)\n\noptional_dependencies = {\n    'sarimax': [\n        'statsmodels>=0.12, <0.15'\n    ],\n    'deeplearning': [\n        'matplotlib>=3.3, <3.10',\n        'keras>=2.6, <4.0',\n    ],\n    'plotting': [\n        'matplotlib>=3.3, <3.10', \n        'seaborn>=0.11, <0.14', \n        'statsmodels>=0.12, <0.15'\n    ]\n}\n\n\ndef initialize_lags(\n    forecaster_name: str,\n    lags: Any\n) -> Union[Optional[np.ndarray], Optional[list], Optional[int]]:\n    \"\"\"\n    Check lags argument input and generate the corresponding numpy ndarray.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    lags : Any\n        Lags used as predictors.\n\n    Returns\n    -------\n    lags : numpy ndarray, None\n        Lags used as predictors.\n    lags_names : list, None\n        Names of the lags used as predictors.\n    max_lag : int, None\n        Maximum value of the lags.\n    \n    \"\"\"\n\n    lags_names = None\n    max_lag = None\n    if lags is not None:\n        if isinstance(lags, int):\n            if lags < 1:\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n            lags = np.arange(1, lags + 1)\n\n        if isinstance(lags, (list, tuple, range)):\n            lags = np.array(lags)\n        \n        if isinstance(lags, np.ndarray):\n            if lags.size == 0:\n                return None, None, None\n            if lags.ndim != 1:\n                raise ValueError(\"`lags` must be a 1-dimensional array.\")\n            if not np.issubdtype(lags.dtype, np.integer):\n                raise TypeError(\"All values in `lags` must be integers.\")\n            if np.any(lags < 1):\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n        else:\n            if forecaster_name != 'ForecasterDirectMultiVariate':\n                raise TypeError(\n                    (f\"`lags` argument must be an int, 1d numpy ndarray, range, \"\n                     f\"tuple or list. Got {type(lags)}.\")\n                )\n            else:\n                raise TypeError(\n                    (f\"`lags` argument must be a dict, int, 1d numpy ndarray, range, \"\n                     f\"tuple or list. Got {type(lags)}.\")\n                )\n        \n        lags_names = [f'lag_{i}' for i in lags]\n        max_lag = max(lags)\n\n    return lags, lags_names, max_lag\n\n\ndef initialize_window_features(\n    window_features: Any\n) -> Union[Optional[list], Optional[list], Optional[int]]:\n    \"\"\"\n    Check window_features argument input and generate the corresponding list.\n\n    Parameters\n    ----------\n    window_features : Any\n        Classes used to create window features.\n\n    Returns\n    -------\n    window_features : list, None\n        List of classes used to create window features.\n    window_features_names : list, None\n        List with all the features names of the window features.\n    max_size_window_features : int, None\n        Maximum value of the `window_sizes` attribute of all classes.\n    \n    \"\"\"\n\n    needed_atts = ['window_sizes', 'features_names']\n    needed_methods = ['transform_batch', 'transform']\n\n    max_window_sizes = None\n    window_features_names = None\n    max_size_window_features = None\n    if window_features is not None:\n        if isinstance(window_features, list) and len(window_features) < 1:\n            raise ValueError(\n                \"Argument `window_features` must contain at least one element.\"\n            )\n        if not isinstance(window_features, list):\n            window_features = [window_features]\n\n        link_to_docs = (\n            \"\\nVisit the documentation for more information about how to create \"\n            \"custom window features:\\n\"\n            \"https://skforecast.org/latest/user_guides/window-features-and-custom-features.html#create-your-custom-window-features\"\n        )\n        \n        max_window_sizes = []\n        window_features_names = []\n        for wf in window_features:\n            wf_name = type(wf).__name__\n            atts_methods = set([a for a in dir(wf)])\n            if not set(needed_atts).issubset(atts_methods):\n                raise ValueError(\n                    f\"{wf_name} must have the attributes: {needed_atts}.\" + link_to_docs\n                )\n            if not set(needed_methods).issubset(atts_methods):\n                raise ValueError(\n                    f\"{wf_name} must have the methods: {needed_methods}.\" + link_to_docs\n                )\n            \n            window_sizes = wf.window_sizes\n            if not isinstance(window_sizes, (int, list)):\n                raise TypeError(\n                    f\"Attribute `window_sizes` of {wf_name} must be an int or a list \"\n                    f\"of ints. Got {type(window_sizes)}.\" + link_to_docs\n                )\n            \n            if isinstance(window_sizes, int):\n                if window_sizes < 1:\n                    raise ValueError(\n                        f\"If argument `window_sizes` is an integer, it must be equal to or \"\n                        f\"greater than 1. Got {window_sizes} from {wf_name}.\" + link_to_docs\n                    )\n                max_window_sizes.append(window_sizes)\n            else:\n                if not all(isinstance(ws, int) for ws in window_sizes) or not all(\n                    ws >= 1 for ws in window_sizes\n                ):                    \n                    raise ValueError(\n                        f\"If argument `window_sizes` is a list, all elements must be integers \"\n                        f\"equal to or greater than 1. Got {window_sizes} from {wf_name}.\" + link_to_docs\n                    )\n                max_window_sizes.append(max(window_sizes))\n\n            features_names = wf.features_names\n            if not isinstance(features_names, (str, list)):\n                raise TypeError(\n                    f\"Attribute `features_names` of {wf_name} must be a str or \"\n                    f\"a list of strings. Got {type(features_names)}.\" + link_to_docs\n                )\n            if isinstance(features_names, str):\n                window_features_names.append(features_names)\n            else:\n                if not all(isinstance(fn, str) for fn in features_names):\n                    raise TypeError(\n                        f\"If argument `features_names` is a list, all elements \"\n                        f\"must be strings. Got {features_names} from {wf_name}.\" + link_to_docs\n                    )\n                window_features_names.extend(features_names)\n\n        max_size_window_features = max(max_window_sizes)\n        if len(set(window_features_names)) != len(window_features_names):\n            raise ValueError(\n                f\"All window features names must be unique. Got {window_features_names}.\"\n            )\n\n    return window_features, window_features_names, max_size_window_features\n\n\ndef initialize_weights(\n    forecaster_name: str,\n    regressor: object,\n    weight_func: Union[Callable, dict],\n    series_weights: dict\n) -> Tuple[Union[Callable, dict], Union[str, dict], dict]:\n    \"\"\"\n    Check weights arguments, `weight_func` and `series_weights` for the different \n    forecasters. Create `source_code_weight_func`, source code of the custom \n    function(s) used to create weights.\n    \n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        Regressor of the forecaster.\n    weight_func : Callable, dict\n        Argument `weight_func` of the forecaster.\n    series_weights : dict\n        Argument `series_weights` of the forecaster.\n\n    Returns\n    -------\n    weight_func : Callable, dict\n        Argument `weight_func` of the forecaster.\n    source_code_weight_func : str, dict\n        Argument `source_code_weight_func` of the forecaster.\n    series_weights : dict\n        Argument `series_weights` of the forecaster.\n    \n    \"\"\"\n\n    source_code_weight_func = None\n\n    if weight_func is not None:\n\n        if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n            if not isinstance(weight_func, (Callable, dict)):\n                raise TypeError(\n                    (f\"Argument `weight_func` must be a Callable or a dict of \"\n                     f\"Callables. Got {type(weight_func)}.\")\n                )\n        elif not isinstance(weight_func, Callable):\n            raise TypeError(\n                f\"Argument `weight_func` must be a Callable. Got {type(weight_func)}.\"\n            )\n        \n        if isinstance(weight_func, dict):\n            source_code_weight_func = {}\n            for key in weight_func:\n                source_code_weight_func[key] = inspect.getsource(weight_func[key])\n        else:\n            source_code_weight_func = inspect.getsource(weight_func)\n\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(\n                (f\"Argument `weight_func` is ignored since regressor {regressor} \"\n                 f\"does not accept `sample_weight` in its `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n            weight_func = None\n            source_code_weight_func = None\n\n    if series_weights is not None:\n        if not isinstance(series_weights, dict):\n            raise TypeError(\n                (f\"Argument `series_weights` must be a dict of floats or ints.\"\n                 f\"Got {type(series_weights)}.\")\n            )\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(\n                (f\"Argument `series_weights` is ignored since regressor {regressor} \"\n                 f\"does not accept `sample_weight` in its `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n            series_weights = None\n\n    return weight_func, source_code_weight_func, series_weights\n\n\ndef initialize_transformer_series(\n    forecaster_name: str,\n    series_names_in_: list,\n    encoding: Optional[str] = None,\n    transformer_series: Optional[Union[object, dict]] = None\n) -> dict:\n    \"\"\"\n    Initialize `transformer_series_` attribute for the Forecasters Multiseries.\n\n    - If `transformer_series` is `None`, no transformation is applied.\n    - If `transformer_series` is a scikit-learn transformer (object), the same \n    transformer is applied to all series (`series_names_in_`).\n    - If `transformer_series` is a `dict`, a different transformer can be\n    applied to each series. The keys of the dictionary must be the same as the\n    names of the series in `series_names_in_`.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    series_names_in_ : list\n        Names of the series (levels) used during training.\n    encoding : str, default `None`\n        Encoding used to identify the different series (`ForecasterRecursiveMultiSeries`).\n    transformer_series : object, dict, default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and \n        inverse_transform. \n\n    Returns\n    -------\n    transformer_series_ : dict\n        Dictionary with the transformer for each series. It is created cloning the \n        objects in `transformer_series` and is used internally to avoid overwriting.\n    \n    \"\"\"\n\n    multiseries_forecasters = [\n        'ForecasterRecursiveMultiSeries'\n    ]\n\n    if forecaster_name in multiseries_forecasters:\n        if encoding is None:\n            series_names_in_ = ['_unknown_level']\n        else:\n            series_names_in_ = series_names_in_ + ['_unknown_level']\n\n    if transformer_series is None:\n        transformer_series_ = {serie: None for serie in series_names_in_}\n    elif not isinstance(transformer_series, dict):\n        transformer_series_ = {serie: clone(transformer_series) \n                               for serie in series_names_in_}\n    else:\n        transformer_series_ = {serie: None for serie in series_names_in_}\n        # Only elements already present in transformer_series_ are updated\n        transformer_series_.update(\n            (k, v) for k, v in deepcopy(transformer_series).items() \n            if k in transformer_series_\n        )\n\n        series_not_in_transformer_series = (\n            set(series_names_in_) - set(transformer_series.keys())\n        ) - {'_unknown_level'}\n        if series_not_in_transformer_series:\n            warnings.warn(\n                (f\"{series_not_in_transformer_series} not present in `transformer_series`.\"\n                f\" No transformation is applied to these series.\"),\n                IgnoredArgumentWarning\n            )\n\n    return transformer_series_\n\n\ndef check_select_fit_kwargs(\n    regressor: object,\n    fit_kwargs: Optional[dict] = None\n) -> dict:\n    \"\"\"\n    Check if `fit_kwargs` is a dict and select only the keys that are used by\n    the `fit` method of the regressor.\n\n    Parameters\n    ----------\n    regressor : object\n        Regressor object.\n    fit_kwargs : dict, default `None`\n        Dictionary with the arguments to pass to the `fit' method of the forecaster.\n\n    Returns\n    -------\n    fit_kwargs : dict\n        Dictionary with the arguments to be passed to the `fit` method of the \n        regressor after removing the unused keys.\n    \n    \"\"\"\n\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    else:\n        if not isinstance(fit_kwargs, dict):\n            raise TypeError(\n                f\"Argument `fit_kwargs` must be a dict. Got {type(fit_kwargs)}.\"\n            )\n\n        # Non used keys\n        non_used_keys = [k for k in fit_kwargs.keys()\n                         if k not in inspect.signature(regressor.fit).parameters]\n        if non_used_keys:\n            warnings.warn(\n                (f\"Argument/s {non_used_keys} ignored since they are not used by the \"\n                 f\"regressor's `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n\n        if 'sample_weight' in fit_kwargs.keys():\n            warnings.warn(\n                (\"The `sample_weight` argument is ignored. Use `weight_func` to pass \"\n                 \"a function that defines the individual weights for each sample \"\n                 \"based on its index.\"),\n                 IgnoredArgumentWarning\n            )\n            del fit_kwargs['sample_weight']\n\n        # Select only the keyword arguments allowed by the regressor's `fit` method.\n        fit_kwargs = {k: v for k, v in fit_kwargs.items()\n                      if k in inspect.signature(regressor.fit).parameters}\n\n    return fit_kwargs\n\n\ndef check_y(\n    y: Any,\n    series_id: str = \"`y`\"\n) -> None:\n    \"\"\"\n    Raise Exception if `y` is not pandas Series or if it has missing values.\n    \n    Parameters\n    ----------\n    y : Any\n        Time series values.\n    series_id : str, default '`y`'\n        Identifier of the series used in the warning message.\n    \n    Returns\n    -------\n    None\n    \n    \"\"\"\n    \n    if not isinstance(y, pd.Series):\n        raise TypeError(f\"{series_id} must be a pandas Series.\")\n        \n    if y.isnull().any():\n        raise ValueError(f\"{series_id} has missing values.\")\n    \n    return\n\n\ndef check_exog(\n    exog: Union[pd.Series, pd.DataFrame],\n    allow_nan: bool = True,\n    series_id: str = \"`exog`\"\n) -> None:\n    \"\"\"\n    Raise Exception if `exog` is not pandas Series or pandas DataFrame.\n    If `allow_nan = True`, issue a warning if `exog` contains NaN values.\n    \n    Parameters\n    ----------\n    exog : pandas DataFrame, pandas Series\n        Exogenous variable/s included as predictor/s.\n    allow_nan : bool, default `True`\n        If True, allows the presence of NaN values in `exog`. If False (default),\n        issue a warning if `exog` contains NaN values.\n    series_id : str, default '`exog`'\n        Identifier of the series for which the exogenous variable/s are used\n        in the warning message.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    \n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(\n            f\"{series_id} must be a pandas Series or DataFrame. Got {type(exog)}.\"\n        )\n    \n    if isinstance(exog, pd.Series) and exog.name is None:\n        raise ValueError(f\"When {series_id} is a pandas Series, it must have a name.\")\n\n    if not allow_nan:\n        if exog.isnull().any().any():\n            warnings.warn(\n                (f\"{series_id} has missing values. Most machine learning models \"\n                 f\"do not allow missing values. Fitting the forecaster may fail.\"), \n                 MissingValuesWarning\n            )\n    \n    return\n\n\ndef get_exog_dtypes(\n    exog: Union[pd.DataFrame, pd.Series]\n) -> dict:\n    \"\"\"\n    Store dtypes of `exog`.\n\n    Parameters\n    ----------\n    exog : pandas DataFrame, pandas Series\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    exog_dtypes : dict\n        Dictionary with the dtypes in `exog`.\n    \n    \"\"\"\n\n    if isinstance(exog, pd.Series):\n        exog_dtypes = {exog.name: exog.dtypes}\n    else:\n        exog_dtypes = exog.dtypes.to_dict()\n    \n    return exog_dtypes\n\n\ndef check_exog_dtypes(\n    exog: Union[pd.DataFrame, pd.Series],\n    call_check_exog: bool = True,\n    series_id: str = \"`exog`\"\n) -> None:\n    \"\"\"\n    Raise Exception if `exog` has categorical columns with non integer values.\n    This is needed when using machine learning regressors that allow categorical\n    features.\n    Issue a Warning if `exog` has columns that are not `init`, `float`, or `category`.\n    \n    Parameters\n    ----------\n    exog : pandas DataFrame, pandas Series\n        Exogenous variable/s included as predictor/s.\n    call_check_exog : bool, default `True`\n        If `True`, call `check_exog` function.\n    series_id : str, default '`exog`'\n        Identifier of the series for which the exogenous variable/s are used\n        in the warning message.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if call_check_exog:\n        check_exog(exog=exog, allow_nan=False, series_id=series_id)\n\n    if isinstance(exog, pd.DataFrame):\n        if not exog.select_dtypes(exclude=[np.number, 'category']).columns.empty:\n            warnings.warn(\n                (f\"{series_id} may contain only `int`, `float` or `category` dtypes. \"\n                 f\"Most machine learning models do not allow other types of values. \"\n                 f\"Fitting the forecaster may fail.\"), \n                 DataTypeWarning\n            )\n        for col in exog.select_dtypes(include='category'):\n            if exog[col].cat.categories.dtype not in [int, np.int32, np.int64]:\n                raise TypeError(\n                    (\"Categorical dtypes in exog must contain only integer values. \"\n                     \"See skforecast docs for more info about how to include \"\n                     \"categorical features https://skforecast.org/\"\n                     \"latest/user_guides/categorical-features.html\")\n                )\n    else:\n        if exog.dtype.name not in ['int', 'int8', 'int16', 'int32', 'int64', 'float', \n        'float16', 'float32', 'float64', 'uint8', 'uint16', 'uint32', 'uint64', 'category']:\n            warnings.warn(\n                (f\"{series_id} may contain only `int`, `float` or `category` dtypes. Most \"\n                 f\"machine learning models do not allow other types of values. \"\n                 f\"Fitting the forecaster may fail.\"), \n                 DataTypeWarning\n            )\n        if exog.dtype.name == 'category' and exog.cat.categories.dtype not in [int,\n        np.int32, np.int64]:\n            raise TypeError(\n                (\"Categorical dtypes in exog must contain only integer values. \"\n                 \"See skforecast docs for more info about how to include \"\n                 \"categorical features https://skforecast.org/\"\n                 \"latest/user_guides/categorical-features.html\")\n            )\n         \n    return\n\n\ndef check_interval(\n    interval: list = None,\n    quantiles: float = None,\n    alpha: float = None\n) -> None:\n    \"\"\"\n    Check provided confidence interval sequence is valid.\n\n    Parameters\n    ----------\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. For example, \n        interval of 95% should be as `interval = [2.5, 97.5]`.\n    quantiles : list, default `None`\n        Sequence of quantiles to compute, which must be between 0 and 1 \n        inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n        `quantiles = [0.05, 0.5, 0.95]`.\n    alpha : float, default `None`\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %.\n\n    Returns\n    -------\n    None\n    \n    \"\"\"\n\n    if interval is not None:\n        if not isinstance(interval, list):\n            raise TypeError(\n                (\"`interval` must be a `list`. For example, interval of 95% \"\n                 \"should be as `interval = [2.5, 97.5]`.\")\n            )\n\n        if len(interval) != 2:\n            raise ValueError(\n                (\"`interval` must contain exactly 2 values, respectively the \"\n                 \"lower and upper interval bounds. For example, interval of 95% \"\n                 \"should be as `interval = [2.5, 97.5]`.\")\n            )\n\n        if (interval[0] < 0.) or (interval[0] >= 100.):\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be >= 0 and < 100.\"\n            )\n\n        if (interval[1] <= 0.) or (interval[1] > 100.):\n            raise ValueError(\n                f\"Upper interval bound ({interval[1]}) must be > 0 and <= 100.\"\n            )\n\n        if interval[0] >= interval[1]:\n            raise ValueError(\n                (f\"Lower interval bound ({interval[0]}) must be less than the \"\n                 f\"upper interval bound ({interval[1]}).\")\n            )\n        \n    if quantiles is not None:\n        if not isinstance(quantiles, list):\n            raise TypeError(\n                (\"`quantiles` must be a `list`. For example, quantiles 0.05, \"\n                 \"0.5, and 0.95 should be as `quantiles = [0.05, 0.5, 0.95]`.\")\n            )\n        \n        for q in quantiles:\n            if (q < 0.) or (q > 1.):\n                raise ValueError(\n                    (\"All elements in `quantiles` must be >= 0 and <= 1.\")\n                )\n    \n    if alpha is not None:\n        if not isinstance(alpha, float):\n            raise TypeError(\n                (\"`alpha` must be a `float`. For example, interval of 95% \"\n                 \"should be as `alpha = 0.05`.\")\n            )\n\n        if (alpha <= 0.) or (alpha >= 1):\n            raise ValueError(\n                f\"`alpha` must have a value between 0 and 1. Got {alpha}.\"\n            )\n\n    return\n\n\ndef check_predict_input(\n    forecaster_name: str,\n    steps: Union[int, list],\n    is_fitted: bool,\n    exog_in_: bool,\n    index_type_: type,\n    index_freq_: str,\n    window_size: int,\n    last_window: Union[pd.Series, pd.DataFrame, None],\n    last_window_exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    exog_type_in_: Optional[type] = None,\n    exog_names_in_: Optional[list] = None,\n    interval: Optional[list] = None,\n    alpha: Optional[float] = None,\n    max_steps: Optional[int] = None,\n    levels: Optional[Union[str, list]] = None,\n    levels_forecaster: Optional[Union[str, list]] = None,\n    series_names_in_: Optional[list] = None,\n    encoding: Optional[str] = None\n) -> None:\n    \"\"\"\n    Check all inputs of predict method. This is a helper function to validate\n    that inputs used in predict method match attributes of a forecaster already\n    trained.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    steps : int, list\n        Number of future steps predicted.\n    is_fitted: bool\n        Tag to identify if the regressor has been fitted (trained).\n    exog_in_ : bool\n        If the forecaster has been trained using exogenous variable/s.\n    index_type_ : type\n        Type of index of the input used in training.\n    index_freq_ : str\n        Frequency of Index of the input used in training.\n    window_size: int\n        Size of the window needed to create the predictors. It is equal to \n        `max_lag`.\n    last_window : pandas Series, pandas DataFrame, None\n        Values of the series used to create the predictors (lags) need in the \n        first iteration of prediction (t + 1).\n    last_window_exog : pandas Series, pandas DataFrame, default `None`\n        Values of the exogenous variables aligned with `last_window` in \n        ForecasterSarimax predictions.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    exog_type_in_ : type, default `None`\n        Type of exogenous variable/s used in training.\n    exog_names_in_ : list, default `None`\n        Names of the exogenous variables used during training.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. For example, \n        interval of 95% should be as `interval = [2.5, 97.5]`.\n    alpha : float, default `None`\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %.\n    max_steps: int, default `None`\n        Maximum number of steps allowed (`ForecasterDirect` and \n        `ForecasterDirectMultiVariate`).\n    levels : str, list, default `None`\n        Time series to be predicted (`ForecasterRecursiveMultiSeries`\n        and `ForecasterRnn).\n    levels_forecaster : str, list, default `None`\n        Time series used as output data of a multiseries problem in a RNN problem\n        (`ForecasterRnn`).\n    series_names_in_ : list, default `None`\n        Names of the columns used during fit (`ForecasterRecursiveMultiSeries`, \n        `ForecasterDirectMultiVariate` and `ForecasterRnn`).\n    encoding : str, default `None`\n        Encoding used to identify the different series (`ForecasterRecursiveMultiSeries`).\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not is_fitted:\n        raise NotFittedError(\n            (\"This Forecaster instance is not fitted yet. Call `fit` with \"\n             \"appropriate arguments before using predict.\")\n        )\n\n    if isinstance(steps, (int, np.integer)) and steps < 1:\n        raise ValueError(\n            f\"`steps` must be an integer greater than or equal to 1. Got {steps}.\"\n        )\n\n    if isinstance(steps, list) and min(steps) < 1:\n        raise ValueError(\n           (f\"The minimum value of `steps` must be equal to or greater than 1. \"\n            f\"Got {min(steps)}.\")\n        )\n\n    if max_steps is not None:\n        if max(steps) > max_steps:\n            raise ValueError(\n                (f\"The maximum value of `steps` must be less than or equal to \"\n                 f\"the value of steps defined when initializing the forecaster. \"\n                 f\"Got {max(steps)}, but the maximum is {max_steps}.\")\n            )\n\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n\n    if forecaster_name in ['ForecasterRecursiveMultiSeries', \n                           'ForecasterRnn']:\n        if not isinstance(levels, (type(None), str, list)):\n            raise TypeError(\n                (\"`levels` must be a `list` of column names, a `str` of a \"\n                 \"column name or `None`.\")\n            )\n\n        levels_to_check = (\n            levels_forecaster if forecaster_name == 'ForecasterRnn'\n            else series_names_in_\n        )\n        unknown_levels = set(levels) - set(levels_to_check)\n        if forecaster_name == 'ForecasterRnn':\n            if len(unknown_levels) != 0:\n                raise ValueError(\n                    (f\"`levels` names must be included in the series used during fit \"\n                     f\"({levels_to_check}). Got {levels}.\")\n                )\n        else:\n            if len(unknown_levels) != 0 and last_window is not None and encoding is not None:\n                if encoding == 'onehot':\n                    warnings.warn(\n                        (f\"`levels` {unknown_levels} were not included in training. The resulting \"\n                         f\"one-hot encoded columns for this feature will be all zeros.\"),\n                         UnknownLevelWarning\n                    )\n                else:\n                    warnings.warn(\n                        (f\"`levels` {unknown_levels} were not included in training. \"\n                         f\"Unknown levels are encoded as NaN, which may cause the \"\n                         f\"prediction to fail if the regressor does not accept NaN values.\"),\n                         UnknownLevelWarning\n                    )\n\n    if exog is None and exog_in_:\n        raise ValueError(\n            (\"Forecaster trained with exogenous variable/s. \"\n             \"Same variable/s must be provided when predicting.\")\n        )\n\n    if exog is not None and not exog_in_:\n        raise ValueError(\n            (\"Forecaster trained without exogenous variable/s. \"\n             \"`exog` must be `None` when predicting.\")\n        )\n\n    # Checks last_window\n    # Check last_window type (pd.Series or pd.DataFrame according to forecaster)\n    if isinstance(last_window, type(None)) and forecaster_name not in [\n        'ForecasterRecursiveMultiSeries', \n        'ForecasterRnn'\n    ]:\n        raise ValueError(\n            (\"`last_window` was not stored during training. If you don't want \"\n             \"to retrain the Forecaster, provide `last_window` as argument.\")\n        )\n\n    if forecaster_name in ['ForecasterRecursiveMultiSeries', \n                           'ForecasterDirectMultiVariate',\n                           'ForecasterRnn']:\n        if not isinstance(last_window, pd.DataFrame):\n            raise TypeError(\n                f\"`last_window` must be a pandas DataFrame. Got {type(last_window)}.\"\n            )\n\n        last_window_cols = last_window.columns.to_list()\n\n        if forecaster_name in ['ForecasterRecursiveMultiSeries', \n                               'ForecasterRnn'] and \\\n            len(set(levels) - set(last_window_cols)) != 0:\n            raise ValueError(\n                (f\"`last_window` must contain a column(s) named as the level(s) \"\n                 f\"to be predicted.\\n\"\n                 f\"    `levels` : {levels}\\n\"\n                 f\"    `last_window` columns : {last_window_cols}\")\n            )\n\n        if forecaster_name == 'ForecasterDirectMultiVariate':\n            if len(set(series_names_in_) - set(last_window_cols)) > 0:\n                raise ValueError(\n                    (f\"`last_window` columns must be the same as the `series` \"\n                     f\"column names used to create the X_train matrix.\\n\"\n                     f\"    `last_window` columns    : {last_window_cols}\\n\"\n                     f\"    `series` columns X train : {series_names_in_}\")\n                )\n    else:\n        if not isinstance(last_window, (pd.Series, pd.DataFrame)):\n            raise TypeError(\n                f\"`last_window` must be a pandas Series or DataFrame. \"\n                f\"Got {type(last_window)}.\"\n            )\n\n    # Check last_window len, nulls and index (type and freq)\n    if len(last_window) < window_size:\n        raise ValueError(\n            (f\"`last_window` must have as many values as needed to \"\n             f\"generate the predictors. For this forecaster it is {window_size}.\")\n        )\n    if last_window.isnull().any().all():\n        warnings.warn(\n            (\"`last_window` has missing values. Most of machine learning models do \"\n             \"not allow missing values. Prediction method may fail.\"), \n             MissingValuesWarning\n        )\n    _, last_window_index = preprocess_last_window(\n                               last_window   = last_window.iloc[:0],\n                               return_values = False\n                           ) \n    if not isinstance(last_window_index, index_type_):\n        raise TypeError(\n            (f\"Expected index of type {index_type_} for `last_window`. \"\n             f\"Got {type(last_window_index)}.\")\n        )\n    if isinstance(last_window_index, pd.DatetimeIndex):\n        if not last_window_index.freqstr == index_freq_:\n            raise TypeError(\n                (f\"Expected frequency of type {index_freq_} for `last_window`. \"\n                 f\"Got {last_window_index.freqstr}.\")\n            )\n\n    # Checks exog\n    if exog is not None:\n\n        # Check type, nulls and expected type\n        if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n            if not isinstance(exog, (pd.Series, pd.DataFrame, dict)):\n                raise TypeError(\n                    f\"`exog` must be a pandas Series, DataFrame or dict. Got {type(exog)}.\"\n                )\n            if exog_type_in_ == dict and not isinstance(exog, dict):\n                raise TypeError(\n                    f\"Expected type for `exog`: {exog_type_in_}. Got {type(exog)}.\"\n                )\n        else:\n            if not isinstance(exog, (pd.Series, pd.DataFrame)):\n                raise TypeError(\n                    f\"`exog` must be a pandas Series or DataFrame. Got {type(exog)}.\"\n                )\n\n        if isinstance(exog, dict):\n            no_exog_levels = set(levels) - set(exog.keys())\n            if no_exog_levels:\n                warnings.warn(\n                    (f\"`exog` does not contain keys for levels {no_exog_levels}. \"\n                     f\"Missing levels are filled with NaN. Most of machine learning \"\n                     f\"models do not allow missing values. Prediction method may fail.\"),\n                     MissingExogWarning\n                )\n            exogs_to_check = [\n                (f\"`exog` for series '{k}'\", v) \n                for k, v in exog.items() \n                if v is not None and k in levels\n            ]\n        else:\n            exogs_to_check = [('`exog`', exog)]\n\n        for exog_name, exog_to_check in exogs_to_check:\n\n            if not isinstance(exog_to_check, (pd.Series, pd.DataFrame)):\n                raise TypeError(\n                    f\"{exog_name} must be a pandas Series or DataFrame. Got {type(exog_to_check)}\"\n                )\n\n            if exog_to_check.isnull().any().any():\n                warnings.warn(\n                    (f\"{exog_name} has missing values. Most of machine learning models \"\n                     f\"do not allow missing values. Prediction method may fail.\"), \n                     MissingValuesWarning\n                )\n\n            # Check exog has many values as distance to max step predicted\n            last_step = max(steps) if isinstance(steps, list) else steps\n            if len(exog_to_check) < last_step:\n                if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n                    warnings.warn(\n                        (f\"{exog_name} doesn't have as many values as steps \"\n                         f\"predicted, {last_step}. Missing values are filled \"\n                         f\"with NaN. Most of machine learning models do not \"\n                         f\"allow missing values. Prediction method may fail.\"),\n                         MissingValuesWarning\n                    )\n                else: \n                    raise ValueError(\n                        (f\"{exog_name} must have at least as many values as \"\n                         f\"steps predicted, {last_step}.\")\n                    )\n\n            # Check name/columns are in exog_names_in_\n            if isinstance(exog_to_check, pd.DataFrame):\n                col_missing = set(exog_names_in_).difference(set(exog_to_check.columns))\n                if col_missing:\n                    if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n                        warnings.warn(\n                            (f\"{col_missing} not present in {exog_name}. All \"\n                             f\"values will be NaN.\"),\n                             MissingExogWarning\n                        ) \n                    else:\n                        raise ValueError(\n                            (f\"Missing columns in {exog_name}. Expected {exog_names_in_}. \"\n                             f\"Got {exog_to_check.columns.to_list()}.\")\n                        )\n            else:\n                if exog_to_check.name is None:\n                    raise ValueError(\n                        (f\"When {exog_name} is a pandas Series, it must have a name. Got None.\")\n                    )\n\n                if exog_to_check.name not in exog_names_in_:\n                    if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n                        warnings.warn(\n                            (f\"'{exog_to_check.name}' was not observed during training. \"\n                             f\"{exog_name} is ignored. Exogenous variables must be one \"\n                             f\"of: {exog_names_in_}.\"),\n                             IgnoredArgumentWarning\n                        )\n                    else:\n                        raise ValueError(\n                            (f\"'{exog_to_check.name}' was not observed during training. \"\n                             f\"Exogenous variables must be: {exog_names_in_}.\")\n                        )\n\n            # Check index dtype and freq\n            _, exog_index = preprocess_exog(\n                                exog          = exog_to_check.iloc[:0, ],\n                                return_values = False\n                            )\n            if not isinstance(exog_index, index_type_):\n                raise TypeError(\n                    (f\"Expected index of type {index_type_} for {exog_name}. \"\n                     f\"Got {type(exog_index)}.\")\n                )\n            if forecaster_name not in ['ForecasterRecursiveMultiSeries']:\n                if isinstance(exog_index, pd.DatetimeIndex):\n                    if not exog_index.freqstr == index_freq_:\n                        raise TypeError(\n                            (f\"Expected frequency of type {index_freq_} for {exog_name}. \"\n                             f\"Got {exog_index.freqstr}.\")\n                        )\n\n            # Check exog starts one step ahead of last_window end.\n            expected_index = expand_index(last_window.index, 1)[0]\n            if expected_index != exog_to_check.index[0]:\n                if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n                    warnings.warn(\n                        (f\"To make predictions {exog_name} must start one step \"\n                         f\"ahead of `last_window`. Missing values are filled \"\n                         f\"with NaN.\\n\"\n                         f\"    `last_window` ends at : {last_window.index[-1]}.\\n\"\n                         f\"    {exog_name} starts at : {exog_to_check.index[0]}.\\n\"\n                         f\"     Expected index       : {expected_index}.\"),\n                         MissingValuesWarning\n                    )  \n                else:\n                    raise ValueError(\n                        (f\"To make predictions {exog_name} must start one step \"\n                         f\"ahead of `last_window`.\\n\"\n                         f\"    `last_window` ends at : {last_window.index[-1]}.\\n\"\n                         f\"    {exog_name} starts at : {exog_to_check.index[0]}.\\n\"\n                         f\"     Expected index : {expected_index}.\")\n                    )\n\n    # Checks ForecasterSarimax\n    if forecaster_name == 'ForecasterSarimax':\n        # Check last_window_exog type, len, nulls and index (type and freq)\n        if last_window_exog is not None:\n            if not exog_in_:\n                raise ValueError(\n                    (\"Forecaster trained without exogenous variable/s. \"\n                     \"`last_window_exog` must be `None` when predicting.\")\n                )\n\n            if not isinstance(last_window_exog, (pd.Series, pd.DataFrame)):\n                raise TypeError(\n                    (f\"`last_window_exog` must be a pandas Series or a \"\n                     f\"pandas DataFrame. Got {type(last_window_exog)}.\")\n                )\n            if len(last_window_exog) < window_size:\n                raise ValueError(\n                    (f\"`last_window_exog` must have as many values as needed to \"\n                     f\"generate the predictors. For this forecaster it is {window_size}.\")\n                )\n            if last_window_exog.isnull().any().all():\n                warnings.warn(\n                    (\"`last_window_exog` has missing values. Most of machine learning \"\n                     \"models do not allow missing values. Prediction method may fail.\"),\n                     MissingValuesWarning\n            )\n            _, last_window_exog_index = preprocess_last_window(\n                                            last_window   = last_window_exog.iloc[:0],\n                                            return_values = False\n                                        ) \n            if not isinstance(last_window_exog_index, index_type_):\n                raise TypeError(\n                    (f\"Expected index of type {index_type_} for `last_window_exog`. \"\n                     f\"Got {type(last_window_exog_index)}.\")\n                )\n            if isinstance(last_window_exog_index, pd.DatetimeIndex):\n                if not last_window_exog_index.freqstr == index_freq_:\n                    raise TypeError(\n                        (f\"Expected frequency of type {index_freq_} for \"\n                         f\"`last_window_exog`. Got {last_window_exog_index.freqstr}.\")\n                    )\n\n            # Check all columns are in the pd.DataFrame, last_window_exog\n            if isinstance(last_window_exog, pd.DataFrame):\n                col_missing = set(exog_names_in_).difference(set(last_window_exog.columns))\n                if col_missing:\n                    raise ValueError(\n                        (f\"Missing columns in `last_window_exog`. Expected {exog_names_in_}. \"\n                         f\"Got {last_window_exog.columns.to_list()}.\") \n                    )\n            else:\n                if last_window_exog.name is None:\n                    raise ValueError(\n                        (\"When `last_window_exog` is a pandas Series, it must have a \"\n                         \"name. Got None.\")\n                    )\n\n                if last_window_exog.name not in exog_names_in_:\n                    raise ValueError(\n                        (f\"'{last_window_exog.name}' was not observed during training. \"\n                         f\"Exogenous variables must be: {exog_names_in_}.\")\n                    )\n\n    return\n\n\ndef preprocess_y(\n    y: Union[pd.Series, pd.DataFrame],\n    return_values: bool = True\n) -> Tuple[Union[None, np.ndarray], pd.Index]:\n    \"\"\"\n    Return values and index of series separately. Index is overwritten \n    according to the next rules:\n    \n    - If index is of type `DatetimeIndex` and has frequency, nothing is \n    changed.\n    - If index is of type `RangeIndex`, nothing is changed.\n    - If index is of type `DatetimeIndex` but has no frequency, a \n    `RangeIndex` is created.\n    - If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\n    \n    Parameters\n    ----------\n    y : pandas Series, pandas DataFrame\n        Time series.\n    return_values : bool, default `True`\n        If `True` return the values of `y` as numpy ndarray. This option is \n        intended to avoid copying data when it is not necessary.\n\n    Returns\n    -------\n    y_values : None, numpy ndarray\n        Numpy array with values of `y`.\n    y_index : pandas Index\n        Index of `y` modified according to the rules.\n    \n    \"\"\"\n    \n    if isinstance(y.index, pd.DatetimeIndex) and y.index.freq is not None:\n        y_index = y.index\n    elif isinstance(y.index, pd.RangeIndex):\n        y_index = y.index\n    elif isinstance(y.index, pd.DatetimeIndex) and y.index.freq is None:\n        warnings.warn(\n            (\"Series has DatetimeIndex index but no frequency. \"\n             \"Index is overwritten with a RangeIndex of step 1.\")\n        )\n        y_index = pd.RangeIndex(\n                      start = 0,\n                      stop  = len(y),\n                      step  = 1\n                  )\n    else:\n        warnings.warn(\n            (\"Series has no DatetimeIndex nor RangeIndex index. \"\n             \"Index is overwritten with a RangeIndex.\")\n        )\n        y_index = pd.RangeIndex(\n                      start = 0,\n                      stop  = len(y),\n                      step  = 1\n                  )\n\n    y_values = y.to_numpy(copy=True).ravel() if return_values else None\n\n    return y_values, y_index\n\n\ndef preprocess_last_window(\n    last_window: Union[pd.Series, pd.DataFrame],\n    return_values: bool = True\n ) -> Tuple[np.ndarray, pd.Index]:\n    \"\"\"\n    Return values and index of series separately. Index is overwritten \n    according to the next rules:\n    \n    - If index is of type `DatetimeIndex` and has frequency, nothing is \n    changed.\n    - If index is of type `RangeIndex`, nothing is changed.\n    - If index is of type `DatetimeIndex` but has no frequency, a \n    `RangeIndex` is created.\n    - If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\n    \n    Parameters\n    ----------\n    last_window : pandas Series, pandas DataFrame\n        Time series values.\n    return_values : bool, default `True`\n        If `True` return the values of `last_window` as numpy ndarray. This option \n        is intended to avoid copying data when it is not necessary.\n\n    Returns\n    -------\n    last_window_values : numpy ndarray\n        Numpy array with values of `last_window`.\n    last_window_index : pandas Index\n        Index of `last_window` modified according to the rules.\n    \n    \"\"\"\n    \n    if isinstance(last_window.index, pd.DatetimeIndex) and last_window.index.freq is not None:\n        last_window_index = last_window.index\n    elif isinstance(last_window.index, pd.RangeIndex):\n        last_window_index = last_window.index\n    elif isinstance(last_window.index, pd.DatetimeIndex) and last_window.index.freq is None:\n        warnings.warn(\n            (\"`last_window` has DatetimeIndex index but no frequency. \"\n             \"Index is overwritten with a RangeIndex of step 1.\")\n        )\n        last_window_index = pd.RangeIndex(\n                                start = 0,\n                                stop  = len(last_window),\n                                step  = 1\n                            )\n    else:\n        warnings.warn(\n            (\"`last_window` has no DatetimeIndex nor RangeIndex index. \"\n             \"Index is overwritten with a RangeIndex.\")\n        )\n        last_window_index = pd.RangeIndex(\n                                start = 0,\n                                stop  = len(last_window),\n                                step  = 1\n                            )\n\n    last_window_values = last_window.to_numpy(copy=True).ravel() if return_values else None\n\n    return last_window_values, last_window_index\n\n\ndef preprocess_exog(\n    exog: Union[pd.Series, pd.DataFrame],\n    return_values: bool = True\n) -> Tuple[Union[None, np.ndarray], pd.Index]:\n    \"\"\"\n    Return values and index of series or data frame separately. Index is\n    overwritten  according to the next rules:\n    \n    - If index is of type `DatetimeIndex` and has frequency, nothing is \n    changed.\n    - If index is of type `RangeIndex`, nothing is changed.\n    - If index is of type `DatetimeIndex` but has no frequency, a \n    `RangeIndex` is created.\n    - If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\n\n    Parameters\n    ----------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables.\n    return_values : bool, default `True`\n        If `True` return the values of `exog` as numpy ndarray. This option is \n        intended to avoid copying data when it is not necessary.\n\n    Returns\n    -------\n    exog_values : None, numpy ndarray\n        Numpy array with values of `exog`.\n    exog_index : pandas Index\n        Index of `exog` modified according to the rules.\n    \n    \"\"\"\n    \n    if isinstance(exog.index, pd.DatetimeIndex) and exog.index.freq is not None:\n        exog_index = exog.index\n    elif isinstance(exog.index, pd.RangeIndex):\n        exog_index = exog.index\n    elif isinstance(exog.index, pd.DatetimeIndex) and exog.index.freq is None:\n        warnings.warn(\n            (\"`exog` has DatetimeIndex index but no frequency. \"\n             \"Index is overwritten with a RangeIndex of step 1.\")\n        )\n        exog_index = pd.RangeIndex(\n                         start = 0,\n                         stop  = len(exog),\n                         step  = 1\n                     )\n\n    else:\n        warnings.warn(\n            (\"`exog` has no DatetimeIndex nor RangeIndex index. \"\n             \"Index is overwritten with a RangeIndex.\")\n        )\n        exog_index = pd.RangeIndex(\n                         start = 0,\n                         stop  = len(exog),\n                         step  = 1\n                     )\n\n    exog_values = exog.to_numpy(copy=True) if return_values else None\n\n    return exog_values, exog_index\n\n\ndef input_to_frame(\n    data: Union[pd.Series, pd.DataFrame],\n    input_name: str\n) -> pd.DataFrame:\n    \"\"\"\n    Convert data to a pandas DataFrame. If data is a pandas Series, it is \n    converted to a DataFrame with a single column. If data is a DataFrame, \n    it is returned as is.\n\n    Parameters\n    ----------\n    data : pandas Series, pandas DataFrame\n        Input data.\n    input_name : str\n        Name of the input data. Accepted values are 'y', 'last_window' and 'exog'.\n\n    Returns\n    -------\n    data : pandas DataFrame\n        Input data as a DataFrame.\n\n    \"\"\"\n\n    output_col_name = {\n        'y': 'y',\n        'last_window': 'y',\n        'exog': 'exog'\n    }\n\n    if isinstance(data, pd.Series):\n        data = data.to_frame(\n            name=data.name if data.name is not None else output_col_name[input_name]\n        )\n\n    return data\n\n\ndef cast_exog_dtypes(\n    exog: Union[pd.Series, pd.DataFrame],\n    exog_dtypes: dict,\n) -> Union[pd.Series, pd.DataFrame]:  # pragma: no cover\n    \"\"\"\n    Cast `exog` to a specified types. This is done because, for a forecaster to \n    accept a categorical exog, it must contain only integer values. Due to the \n    internal modifications of numpy, the values may be casted to `float`, so \n    they have to be re-converted to `int`.\n\n    - If `exog` is a pandas Series, `exog_dtypes` must be a dict with a \n    single value.\n    - If `exog_dtypes` is `category` but the current type of `exog` is `float`, \n    then the type is cast to `int` and then to `category`. \n\n    Parameters\n    ----------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables.\n    exog_dtypes: dict\n        Dictionary with name and type of the series or data frame columns.\n\n    Returns\n    -------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables casted to the indicated dtypes.\n\n    \"\"\"\n\n    # Remove keys from exog_dtypes not in exog.columns\n    exog_dtypes = {k: v for k, v in exog_dtypes.items() if k in exog.columns}\n    \n    if isinstance(exog, pd.Series) and exog.dtypes != list(exog_dtypes.values())[0]:\n        exog = exog.astype(list(exog_dtypes.values())[0])\n    elif isinstance(exog, pd.DataFrame):\n        for col, initial_dtype in exog_dtypes.items():\n            if exog[col].dtypes != initial_dtype:\n                if initial_dtype == \"category\" and exog[col].dtypes == float:\n                    exog[col] = exog[col].astype(int).astype(\"category\")\n                else:\n                    exog[col] = exog[col].astype(initial_dtype)\n\n    return exog\n\n\ndef exog_to_direct(\n    exog: Union[pd.Series, pd.DataFrame],\n    steps: int\n) -> Union[pd.DataFrame, list]:\n    \"\"\"\n    Transforms `exog` to a pandas DataFrame with the shape needed for Direct\n    forecasting.\n    \n    Parameters\n    ----------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables.\n    steps : int\n        Number of steps that will be predicted using exog.\n\n    Returns\n    -------\n    exog_direct : pandas DataFrame\n        Exogenous variables transformed.\n    exog_direct_names : list\n        Names of the columns of the exogenous variables transformed. Only \n        created if `exog` is a pandas Series or DataFrame.\n    \n    \"\"\"\n\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(f\"`exog` must be a pandas Series or DataFrame. Got {type(exog)}.\")\n\n    if isinstance(exog, pd.Series):\n        exog = exog.to_frame()\n\n    n_rows = len(exog)\n    exog_idx = exog.index\n    exog_cols = exog.columns\n    exog_direct = []\n    for i in range(steps):\n        exog_step = exog.iloc[i : n_rows - (steps - 1 - i), ]\n        exog_step.index = pd.RangeIndex(len(exog_step))\n        exog_step.columns = [f\"{col}_step_{i + 1}\" for col in exog_cols]\n        exog_direct.append(exog_step)\n\n    if len(exog_direct) > 1:\n        exog_direct = pd.concat(exog_direct, axis=1, copy=False)\n    else:\n        exog_direct = exog_direct[0]\n\n    exog_direct_names = exog_direct.columns.to_list()\n    exog_direct.index = exog_idx[-len(exog_direct):]\n    \n    return exog_direct, exog_direct_names\n\n\ndef exog_to_direct_numpy(\n    exog: Union[np.ndarray, pd.Series, pd.DataFrame],\n    steps: int\n) -> Tuple[np.ndarray, Optional[list]]:\n    \"\"\"\n    Transforms `exog` to numpy ndarray with the shape needed for Direct\n    forecasting.\n    \n    Parameters\n    ----------\n    exog : numpy ndarray, pandas Series, pandas DataFrame\n        Exogenous variables, shape(samples,). If exog is a pandas format, the \n        direct exog names are created.\n    steps : int\n        Number of steps that will be predicted using exog.\n\n    Returns\n    -------\n    exog_direct : numpy ndarray\n        Exogenous variables transformed.\n    exog_direct_names : list, None\n        Names of the columns of the exogenous variables transformed. Only \n        created if `exog` is a pandas Series or DataFrame.\n\n    \"\"\"\n\n    if isinstance(exog, (pd.Series, pd.DataFrame)):\n        exog_cols = exog.columns if isinstance(exog, pd.DataFrame) else [exog.name]\n        exog_direct_names = [\n            f\"{col}_step_{i + 1}\" for i in range(steps) for col in exog_cols\n        ]\n        exog = exog.to_numpy()\n    else:\n        exog_direct_names = None\n        if not isinstance(exog, np.ndarray):\n            raise TypeError(\n                f\"`exog` must be a numpy ndarray, pandas Series or DataFrame. \"\n                f\"Got {type(exog)}.\"\n            )\n\n    if exog.ndim == 1:\n        exog = np.expand_dims(exog, axis=1)\n\n    n_rows = len(exog)\n    exog_direct = []\n    for i in range(steps):\n        exog_step = exog[i : n_rows - (steps - 1 - i)]\n        exog_direct.append(exog_step)\n\n    if len(exog_direct) > 1:\n        exog_direct = np.concatenate(exog_direct, axis=1)\n    else:\n        exog_direct = exog_direct[0]\n    \n    return exog_direct, exog_direct_names\n\n\ndef date_to_index_position(\n    index: pd.Index,\n    date_input: Union[int, str, pd.Timestamp],\n    date_literal: str = 'steps',\n    kwargs_pd_to_datetime: dict = {}\n) -> int:\n    \"\"\"\n    Transform a datetime string or pandas Timestamp to an integer. The integer\n    represents the position of the datetime in the index.\n    \n    Parameters\n    ----------\n    index : pandas Index\n        Original datetime index (must be a pandas DatetimeIndex if `date_input` \n        is not an int).\n    date_input : int, str, pandas Timestamp\n        Datetime to transform to integer.\n        \n        + If int, returns the same integer.\n        + If str or pandas Timestamp, it is converted and expanded into the index.\n    date_literal : str, default 'steps'\n        Variable name used in error messages.\n    kwargs_pd_to_datetime : dict, default {}\n        Additional keyword arguments to pass to `pd.to_datetime()`.\n    \n    Returns\n    -------\n    date_position : int\n        Integer representing the position of the datetime in the index.\n    \n    \"\"\"\n    \n    if isinstance(date_input, (str, pd.Timestamp)):\n        if not isinstance(index, pd.DatetimeIndex):\n            raise TypeError(\n                f\"Index must be a pandas DatetimeIndex when `{date_literal}` is \"\n                f\"not an integer. Check input series or last window.\"\n            )\n        \n        target_date = pd.to_datetime(date_input, **kwargs_pd_to_datetime)\n        last_date = pd.to_datetime(index[-1])\n        if target_date <= last_date:\n            raise ValueError(\n                \"The provided date must be later than the last date in the index.\"\n            )\n        \n        steps_diff = pd.date_range(start=last_date, end=target_date, freq=index.freq)\n        date_position = len(steps_diff) - 1\n    \n    elif isinstance(date_input, (int, np.integer)):\n        date_position = date_input\n    else:\n        raise TypeError(\n            f\"`{date_literal}` must be an integer, string, or pandas Timestamp.\"\n        )\n    \n    return date_position\n\n\ndef expand_index(\n    index: Union[pd.Index, None], \n    steps: int\n) -> pd.Index:\n    \"\"\"\n    Create a new index of length `steps` starting at the end of the index.\n    \n    Parameters\n    ----------\n    index : pandas Index, None\n        Original index.\n    steps : int\n        Number of steps to expand.\n\n    Returns\n    -------\n    new_index : pandas Index\n        New index.\n\n    \"\"\"\n\n    if not isinstance(steps, (int, np.integer)):\n        raise TypeError(f\"`steps` must be an integer. Got {type(steps)}.\")\n\n    if isinstance(index, pd.Index):\n        \n        if isinstance(index, pd.DatetimeIndex):\n            new_index = pd.date_range(\n                            start   = index[-1] + index.freq,\n                            periods = steps,\n                            freq    = index.freq\n                        )\n        elif isinstance(index, pd.RangeIndex):\n            new_index = pd.RangeIndex(\n                            start = index[-1] + 1,\n                            stop  = index[-1] + 1 + steps\n                        )\n        else:\n            raise TypeError(\n                \"Argument `index` must be a pandas DatetimeIndex or RangeIndex.\"\n            )\n    else:\n        new_index = pd.RangeIndex(\n                        start = 0,\n                        stop  = steps\n                    )\n    \n    return new_index\n\n\ndef transform_numpy(\n    array: np.ndarray,\n    transformer,\n    fit: bool = False,\n    inverse_transform: bool = False\n) -> np.ndarray:\n    \"\"\"\n    Transform raw values of a numpy ndarray with a scikit-learn alike \n    transformer, preprocessor or ColumnTransformer. The transformer used must \n    have the following methods: fit, transform, fit_transform and \n    inverse_transform. ColumnTransformers are not allowed since they do not \n    have inverse_transform method.\n\n    Parameters\n    ----------\n    array : numpy ndarray\n        Array to be transformed.\n    transformer : scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n        Scikit-learn alike transformer (preprocessor) with methods: fit, transform,\n        fit_transform and inverse_transform.\n    fit : bool, default `False`\n        Train the transformer before applying it.\n    inverse_transform : bool, default `False`\n        Transform back the data to the original representation. This is not available\n        when using transformers of class scikit-learn ColumnTransformers.\n\n    Returns\n    -------\n    array_transformed : numpy ndarray\n        Transformed array.\n\n    \"\"\"\n    \n    if not isinstance(array, np.ndarray):\n        raise TypeError(\n            f\"`array` argument must be a numpy ndarray. Got {type(array)}\"\n        )\n\n    if transformer is None:\n        return array\n    \n    array_ndim = array.ndim\n    if array_ndim == 1:\n        array = array.reshape(-1, 1)\n\n    if inverse_transform and isinstance(transformer, ColumnTransformer):\n        raise ValueError(\n            \"`inverse_transform` is not available when using ColumnTransformers.\"\n        )\n\n    if not inverse_transform:\n        if fit:\n            array_transformed = transformer.fit_transform(array)\n        else:\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\n                    \"ignore\", \n                    message=\"X does not have valid feature names\", \n                    category=UserWarning\n                )\n                array_transformed = transformer.transform(array)\n    else:\n        array_transformed = transformer.inverse_transform(array)\n\n    if hasattr(array_transformed, 'toarray'):\n        # If the returned values are in sparse matrix format, it is converted to dense\n        array_transformed = array_transformed.toarray()\n\n    if array_ndim == 1:\n        array_transformed = array_transformed.ravel()\n\n    return array_transformed\n\n\ndef transform_series(\n    series: pd.Series,\n    transformer,\n    fit: bool = False,\n    inverse_transform: bool = False\n) -> Union[pd.Series, pd.DataFrame]:\n    \"\"\"\n    Transform raw values of pandas Series with a scikit-learn alike \n    transformer, preprocessor or ColumnTransformer. The transformer used must \n    have the following methods: fit, transform, fit_transform and \n    inverse_transform. ColumnTransformers are not allowed since they do not \n    have inverse_transform method.\n\n    Parameters\n    ----------\n    series : pandas Series\n        Series to be transformed.\n    transformer : scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n        Scikit-learn alike transformer (preprocessor) with methods: fit, transform,\n        fit_transform and inverse_transform.\n    fit : bool, default `False`\n        Train the transformer before applying it.\n    inverse_transform : bool, default `False`\n        Transform back the data to the original representation. This is not available\n        when using transformers of class scikit-learn ColumnTransformers.\n\n    Returns\n    -------\n    series_transformed : pandas Series, pandas DataFrame\n        Transformed Series. Depending on the transformer used, the output may \n        be a Series or a DataFrame.\n\n    \"\"\"\n    \n    if not isinstance(series, pd.Series):\n        raise TypeError(\n            (f\"`series` argument must be a pandas Series. Got {type(series)}.\")\n        )\n        \n    if transformer is None:\n        return series\n\n    if series.name is None:\n        series.name = 'no_name'\n        \n    data = series.to_frame()\n\n    if fit and hasattr(transformer, 'fit'):\n        transformer.fit(data)\n\n    # If argument feature_names_in_ exits, is overwritten to allow using the \n    # transformer on other series than those that were passed during fit.\n    if hasattr(transformer, 'feature_names_in_') and transformer.feature_names_in_[0] != data.columns[0]:\n        transformer = deepcopy(transformer)\n        transformer.feature_names_in_ = np.array([data.columns[0]], dtype=object)\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=UserWarning)\n        if inverse_transform:\n            values_transformed = transformer.inverse_transform(data)\n        else:\n            values_transformed = transformer.transform(data)   \n\n    if hasattr(values_transformed, 'toarray'):\n        # If the returned values are in sparse matrix format, it is converted to dense array.\n        values_transformed = values_transformed.toarray()\n    \n    if isinstance(values_transformed, np.ndarray) and values_transformed.shape[1] == 1:\n        series_transformed = pd.Series(\n                                 data  = values_transformed.ravel(),\n                                 index = data.index,\n                                 name  = data.columns[0]\n                             )\n    elif isinstance(values_transformed, pd.DataFrame) and values_transformed.shape[1] == 1:\n        series_transformed = values_transformed.squeeze()\n    else:\n        series_transformed = pd.DataFrame(\n                                 data    = values_transformed,\n                                 index   = data.index,\n                                 columns = transformer.get_feature_names_out()\n                             )\n\n    return series_transformed\n\n\ndef transform_dataframe(\n    df: pd.DataFrame,\n    transformer,\n    fit: bool = False,\n    inverse_transform: bool = False\n) -> pd.DataFrame:\n    \"\"\"\n    Transform raw values of pandas DataFrame with a scikit-learn alike \n    transformer, preprocessor or ColumnTransformer. The transformer used must \n    have the following methods: fit, transform, fit_transform and \n    inverse_transform. ColumnTransformers are not allowed since they do not \n    have inverse_transform method.\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        DataFrame to be transformed.\n    transformer : scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n        Scikit-learn alike transformer (preprocessor) with methods: fit, transform,\n        fit_transform and inverse_transform.\n    fit : bool, default `False`\n        Train the transformer before applying it.\n    inverse_transform : bool, default `False`\n        Transform back the data to the original representation. This is not available\n        when using transformers of class scikit-learn ColumnTransformers.\n\n    Returns\n    -------\n    df_transformed : pandas DataFrame\n        Transformed DataFrame.\n\n    \"\"\"\n    \n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\n            f\"`df` argument must be a pandas DataFrame. Got {type(df)}\"\n        )\n\n    if transformer is None:\n        return df\n\n    if inverse_transform and isinstance(transformer, ColumnTransformer):\n        raise ValueError(\n            \"`inverse_transform` is not available when using ColumnTransformers.\"\n        )\n \n    if not inverse_transform:\n        if fit:\n            values_transformed = transformer.fit_transform(df)\n        else:\n            values_transformed = transformer.transform(df)\n    else:\n        values_transformed = transformer.inverse_transform(df)\n\n    if hasattr(values_transformed, 'toarray'):\n        # If the returned values are in sparse matrix format, it is converted to dense\n        values_transformed = values_transformed.toarray()\n\n    if hasattr(transformer, 'get_feature_names_out'):\n        feature_names_out = transformer.get_feature_names_out()\n    elif hasattr(transformer, 'categories_'):   \n        feature_names_out = transformer.categories_\n    else:\n        feature_names_out = df.columns\n\n    df_transformed = pd.DataFrame(\n                         data    = values_transformed,\n                         index   = df.index,\n                         columns = feature_names_out\n                     )\n\n    return df_transformed\n\n\ndef save_forecaster(\n    forecaster: object, \n    file_name: str,\n    save_custom_functions: bool = True, \n    verbose: bool = True\n) -> None:\n    \"\"\"\n    Save forecaster model using joblib. If custom functions are used to create\n    weights, they are saved as .py files.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster created with skforecast library.\n    file_name : str\n        File name given to the object. The save extension will be .joblib.\n    save_custom_functions : bool, default True\n        If True, save custom functions used in the forecaster (weight_func) as \n        .py files. Custom functions need to be available in the environment \n        where the forecaster is going to be loaded.\n    verbose : bool, default True\n        Print summary about the forecaster saved.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    \n    file_name = Path(file_name).with_suffix('.joblib')\n\n    # Save forecaster\n    joblib.dump(forecaster, filename=file_name)\n\n    if save_custom_functions:\n        # Save custom functions to create weights\n        if hasattr(forecaster, 'weight_func') and forecaster.weight_func is not None:\n            if isinstance(forecaster.weight_func, dict):\n                for fun in set(forecaster.weight_func.values()):\n                    file_name = fun.__name__ + '.py'\n                    with open(file_name, 'w') as file:\n                        file.write(inspect.getsource(fun))\n            else:\n                file_name = forecaster.weight_func.__name__ + '.py'\n                with open(file_name, 'w') as file:\n                    file.write(inspect.getsource(forecaster.weight_func))\n    else:\n        if hasattr(forecaster, 'weight_func') and forecaster.weight_func is not None:\n            warnings.warn(\n                \"Custom function(s) used to create weights are not saved. To save them, \"\n                \"set `save_custom_functions` to `True`.\",\n                SaveLoadSkforecastWarning\n            )\n\n    if hasattr(forecaster, 'window_features') and forecaster.window_features is not None:\n        skforecast_classes = {'RollingFeatures'}\n        custom_classes = set(forecaster.window_features_class_names) - skforecast_classes\n        if custom_classes:\n            warnings.warn(\n                \"The Forecaster includes custom user-defined classes in the \"\n                \"`window_features` argument. These classes are not saved automatically \"\n                \"when saving the Forecaster. Please ensure you save these classes \"\n                \"manually and import them before loading the Forecaster.\\n\"\n                \"    Custom classes: \" + ', '.join(custom_classes) + \"\\n\"\n                \"Visit the documentation for more information: \"\n                \"https://skforecast.org/latest/user_guides/save-load-forecaster.html#saving-and-loading-a-forecaster-model-with-custom-features\",\n                SaveLoadSkforecastWarning\n            )\n\n    if verbose:\n        forecaster.summary()\n\n\ndef load_forecaster(\n    file_name: str,\n    verbose: bool = True\n) -> object:\n    \"\"\"\n    Load forecaster model using joblib. If the forecaster was saved with \n    custom user-defined classes as as window features or custom\n    functions to create weights, these objects must be available\n    in the environment where the forecaster is going to be loaded.\n\n    Parameters\n    ----------\n    file_name: str\n        Object file name.\n    verbose: bool, default `True`\n        Print summary about the forecaster loaded.\n\n    Returns\n    -------\n    forecaster: Forecaster\n        Forecaster created with skforecast library.\n    \n    \"\"\"\n\n    forecaster = joblib.load(filename=Path(file_name))\n\n    skforecast_v = skforecast.__version__\n    forecaster_v = forecaster.skforecast_version\n\n    if forecaster_v != skforecast_v:\n        warnings.warn(\n            f\"The skforecast version installed in the environment differs \"\n            f\"from the version used to create the forecaster.\\n\"\n            f\"    Installed Version  : {skforecast_v}\\n\"\n            f\"    Forecaster Version : {forecaster_v}\\n\"\n            f\"This may create incompatibilities when using the library.\",\n             SkforecastVersionWarning\n        )\n\n    if verbose:\n        forecaster.summary()\n\n    return forecaster\n\n\ndef _find_optional_dependency(\n    package_name: str, \n    optional_dependencies: dict = optional_dependencies\n) -> Tuple[str, str]:\n    \"\"\"\n    Find if a package is an optional dependency. If True, find the version and \n    the extension it belongs to.\n\n    Parameters\n    ----------\n    package_name : str\n        Name of the package to check.\n    optional_dependencies : dict, default `optional_dependencies`\n        Skforecast optional dependencies.\n\n    Returns\n    -------\n    extra: str\n        Name of the extra extension where the optional dependency is needed.\n    package_version: srt\n        Name and versions of the dependency.\n\n    \"\"\"\n\n    for extra, packages in optional_dependencies.items():\n        package_version = [package for package in packages if package_name in package]\n        if package_version:\n            return extra, package_version[0]\n\n\ndef check_optional_dependency(\n    package_name: str\n) -> None:\n    \"\"\"\n    Check if an optional dependency is installed, if not raise an ImportError  \n    with installation instructions.\n\n    Parameters\n    ----------\n    package_name : str\n        Name of the package to check.\n\n    Returns\n    -------\n    None\n    \n    \"\"\"\n\n    if importlib.util.find_spec(package_name) is None:\n        try:\n            extra, package_version = _find_optional_dependency(package_name=package_name)\n            msg = (\n                f\"\\n'{package_name}' is an optional dependency not included in the default \"\n                f\"skforecast installation. Please run: `pip install \\\"{package_version}\\\"` to install it.\"\n                f\"\\n\\nAlternately, you can install it by running `pip install skforecast[{extra}]`\"\n            )\n        except:\n            msg = f\"\\n'{package_name}' is needed but not installed. Please install it.\"\n        \n        raise ImportError(msg)\n\n\ndef multivariate_time_series_corr(\n    time_series: pd.Series,\n    other: pd.DataFrame,\n    lags: Union[int, list, np.array],\n    method: str = 'pearson'\n) -> pd.DataFrame:\n    \"\"\"\n    Compute correlation between a time_series and the lagged values of other \n    time series. \n\n    Parameters\n    ----------\n    time_series : pandas Series\n        Target time series.\n    other : pandas DataFrame\n        Time series whose lagged values are correlated to `time_series`.\n    lags : int, list, numpy ndarray\n        Lags to be included in the correlation analysis.\n    method : str, default 'pearson'\n        - 'pearson': standard correlation coefficient.\n        - 'kendall': Kendall Tau correlation coefficient.\n        - 'spearman': Spearman rank correlation.\n\n    Returns\n    -------\n    corr : pandas DataFrame\n        Correlation values.\n\n    \"\"\"\n\n    if not len(time_series) == len(other):\n        raise ValueError(\"`time_series` and `other` must have the same length.\")\n\n    if not (time_series.index == other.index).all():\n        raise ValueError(\"`time_series` and `other` must have the same index.\")\n\n    if isinstance(lags, int):\n        lags = range(lags)\n\n    corr = {}\n    for col in other.columns:\n        lag_values = {}\n        for lag in lags:\n            lag_values[lag] = other[col].shift(lag)\n\n        lag_values = pd.DataFrame(lag_values)\n        lag_values.insert(0, None, time_series)\n        corr[col] = lag_values.corr(method=method).iloc[1:, 0]\n\n    corr = pd.DataFrame(corr)\n    corr.index = corr.index.astype('int64')\n    corr.index.name = \"lag\"\n    \n    return corr\n\n\ndef select_n_jobs_fit_forecaster(\n    forecaster_name: str,\n    regressor: object,\n) -> int:\n    \"\"\"\n    Select the optimal number of jobs to use in the fitting process. This\n    selection is based on heuristics and is not guaranteed to be optimal. \n    \n    The number of jobs is chosen as follows:\n    \n    - If forecaster_name is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and regressor_name is a linear regressor then `n_jobs = 1`, \n    otherwise `n_jobs = cpu_count() - 1`.\n    - If regressor is a `LGBMRegressor(n_jobs=1)`, then `n_jobs = cpu_count() - 1`.\n    - If regressor is a `LGBMRegressor` with internal n_jobs != 1, then `n_jobs = 1`.\n    This is because `lightgbm` is highly optimized for gradient boosting and\n    parallelizes operations at a very fine-grained level, making additional\n    parallelization unnecessary and potentially harmful due to resource contention.\n    \n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n\n    Returns\n    -------\n    n_jobs : int\n        The number of jobs to run in parallel.\n    \n    \"\"\"\n\n    if isinstance(regressor, Pipeline):\n        regressor = regressor[-1]\n        regressor_name = type(regressor).__name__\n    else:\n        regressor_name = type(regressor).__name__\n\n    linear_regressors = [\n        regressor_name\n        for regressor_name in dir(sklearn.linear_model)\n        if not regressor_name.startswith('_')\n    ]\n\n    if forecaster_name in ['ForecasterDirect', \n                           'ForecasterDirectMultiVariate']:\n        if regressor_name in linear_regressors:\n            n_jobs = 1\n        elif regressor_name == 'LGBMRegressor':\n            n_jobs = joblib.cpu_count() - 1 if regressor.n_jobs == 1 else 1\n        else:\n            n_jobs = joblib.cpu_count() - 1\n    else:\n        n_jobs = 1\n\n    return n_jobs\n\n\ndef check_preprocess_series(\n    series: Union[pd.DataFrame, dict],\n) -> Tuple[dict, pd.Index]:\n    \"\"\"\n    Check and preprocess `series` argument in `ForecasterRecursiveMultiSeries` class.\n\n    - If `series` is a pandas DataFrame, it is converted to a dict of pandas \n    Series and index is overwritten according to the rules of preprocess_y.\n    - If `series` is a dict, all values are converted to pandas Series. Checks\n    if all index are pandas DatetimeIndex and, at least, one Series has a non-null\n    frequency. No multiple frequency is allowed.\n\n    Parameters\n    ----------\n    series : pandas DataFrame, dict\n        Training time series.\n\n    Returns\n    -------\n    series_dict : dict\n        Dictionary with the series used during training.\n    series_indexes : dict\n        Dictionary with the index of each series.\n    \n    \"\"\"\n\n    if isinstance(series, pd.DataFrame):\n\n        _, series_index = preprocess_y(y=series, return_values=False)\n        series = series.copy()\n        series.index = series_index\n        series_dict = series.to_dict(\"series\")\n\n    elif isinstance(series, dict):\n\n        not_valid_series = [\n            k \n            for k, v in series.items()\n            if not isinstance(v, (pd.Series, pd.DataFrame))\n        ]\n        if not_valid_series:\n            raise TypeError(\n                (f\"If `series` is a dictionary, all series must be a named \"\n                 f\"pandas Series or a pandas DataFrame with a single column. \"\n                 f\"Review series: {not_valid_series}\")\n            )\n\n        series_dict = {\n            k: v.copy()\n            for k, v in series.items()\n        }\n\n        for k, v in series_dict.items():\n            if isinstance(v, pd.DataFrame):\n                if v.shape[1] != 1:\n                    raise ValueError(\n                        (f\"If `series` is a dictionary, all series must be a named \"\n                         f\"pandas Series or a pandas DataFrame with a single column. \"\n                         f\"Review series: '{k}'\")\n                    )\n                series_dict[k] = v.iloc[:, 0]\n\n            series_dict[k].name = k\n\n        not_valid_index = [\n            k \n            for k, v in series_dict.items()\n            if not isinstance(v.index, pd.DatetimeIndex)\n        ]\n        if not_valid_index:\n            raise TypeError(\n                (f\"If `series` is a dictionary, all series must have a Pandas \"\n                 f\"DatetimeIndex as index with the same frequency. \"\n                 f\"Review series: {not_valid_index}\")\n            )\n\n        indexes_freq = [f\"{v.index.freq}\" for v in series_dict.values()]\n        indexes_freq = sorted(set(indexes_freq))\n        if not len(indexes_freq) == 1:\n            raise ValueError(\n                (f\"If `series` is a dictionary, all series must have a Pandas \"\n                 f\"DatetimeIndex as index with the same frequency. \"\n                 f\"Found frequencies: {indexes_freq}\")\n            )\n    else:\n        raise TypeError(\n            (f\"`series` must be a pandas DataFrame or a dict of DataFrames or Series. \"\n             f\"Got {type(series)}.\")\n        )\n\n    for k, v in series_dict.items():\n        if np.isnan(v).all():\n            raise ValueError(f\"All values of series '{k}' are NaN.\")\n\n    series_indexes = {\n        k: v.index\n        for k, v in series_dict.items()\n    }\n\n    return series_dict, series_indexes\n\n\ndef check_preprocess_exog_multiseries(\n    input_series_is_dict: bool,\n    series_indexes: dict,\n    series_names_in_: list,\n    exog: Union[pd.Series, pd.DataFrame, dict],\n    exog_dict: dict,\n) -> Tuple[dict, list]:\n    \"\"\"\n    Check and preprocess `exog` argument in `ForecasterRecursiveMultiSeries` class.\n\n    - If input series is a pandas DataFrame (input_series_is_dict = False),  \n    checks that input exog (pandas Series, DataFrame or dict) has the same index \n    (type, length and frequency). Index is overwritten according to the rules \n    of preprocess_exog. Create a dict of exog with the same keys as series.\n    - If input series is a dict (input_series_is_dict = True), then input \n    exog must be a dict. Check exog has a pandas DatetimeIndex and convert all\n    values to pandas DataFrames.\n\n    Parameters\n    ----------\n    input_series_is_dict : bool\n        Indicates if input series argument is a dict.\n    series_indexes : dict\n        Dictionary with the index of each series.\n    series_names_in_ : list\n        Names of the series (levels) used during training.\n    exog : pandas Series, pandas DataFrame, dict\n        Exogenous variable/s used during training.\n    exog_dict : dict\n        Dictionary with the exogenous variable/s used during training.\n\n    Returns\n    -------\n    exog_dict : dict\n        Dictionary with the exogenous variable/s used during training.\n    exog_names_in_ : list\n        Names of the exogenous variables used during training.\n    \n    \"\"\"\n\n    if not isinstance(exog, (pd.Series, pd.DataFrame, dict)):\n        raise TypeError(\n            (f\"`exog` must be a pandas Series, DataFrame, dictionary of pandas \"\n             f\"Series/DataFrames or None. Got {type(exog)}.\")\n        )\n\n    if not input_series_is_dict:\n        # If input series is a pandas DataFrame, all index are the same.\n        # Select the first index to check exog\n        series_index = series_indexes[series_names_in_[0]]\n\n    if isinstance(exog, (pd.Series, pd.DataFrame)): \n\n        if input_series_is_dict:\n            raise TypeError(\n                (f\"`exog` must be a dict of DataFrames or Series if \"\n                 f\"`series` is a dict. Got {type(exog)}.\")\n            )\n\n        _, exog_index = preprocess_exog(exog=exog, return_values=False)\n        exog = exog.copy().to_frame() if isinstance(exog, pd.Series) else exog.copy()\n        exog.index = exog_index\n\n        if len(exog) != len(series_index):\n            raise ValueError(\n                (f\"`exog` must have same number of samples as `series`. \"\n                 f\"length `exog`: ({len(exog)}), length `series`: ({len(series_index)})\")\n            )\n\n        if not (exog_index == series_index).all():\n            raise ValueError(\n                (\"Different index for `series` and `exog`. They must be equal \"\n                 \"to ensure the correct alignment of values.\")\n            )\n\n        exog_dict = {serie: exog for serie in series_names_in_}\n\n    else:\n\n        not_valid_exog = [\n            k \n            for k, v in exog.items()\n            if not isinstance(v, (pd.Series, pd.DataFrame, type(None)))\n        ]\n        if not_valid_exog:\n            raise TypeError(\n                (f\"If `exog` is a dictionary, all exog must be a named pandas \"\n                 f\"Series, a pandas DataFrame or None. Review exog: {not_valid_exog}\")\n            )\n\n        # Only elements already present in exog_dict are updated\n        exog_dict.update(\n            (k, v.copy())\n            for k, v in exog.items() \n            if k in exog_dict and v is not None\n        )\n\n        series_not_in_exog = set(series_names_in_) - set(exog.keys())\n        if series_not_in_exog:\n            warnings.warn(\n                (f\"{series_not_in_exog} not present in `exog`. All values \"\n                 f\"of the exogenous variables for these series will be NaN.\"),\n                 MissingExogWarning\n            )\n\n        for k, v in exog_dict.items():\n            if v is not None:\n                check_exog(exog=v, allow_nan=True)\n                if isinstance(v, pd.Series):\n                    v = v.to_frame()\n                exog_dict[k] = v\n\n        if not input_series_is_dict:\n            for k, v in exog_dict.items():\n                if v is not None:\n                    if len(v) != len(series_index):\n                        raise ValueError(\n                            (f\"`exog` for series '{k}' must have same number of \"\n                             f\"samples as `series`. length `exog`: ({len(v)}), \"\n                             f\"length `series`: ({len(series_index)})\")\n                        )\n\n                    _, v_index = preprocess_exog(exog=v, return_values=False)\n                    exog_dict[k].index = v_index\n                    if not (exog_dict[k].index == series_index).all():\n                        raise ValueError(\n                            (f\"Different index for series '{k}' and its exog. \"\n                             f\"When `series` is a pandas DataFrame, they must be \"\n                             f\"equal to ensure the correct alignment of values.\")\n                        )\n        else:\n            not_valid_index = [\n                k\n                for k, v in exog_dict.items()\n                if v is not None and not isinstance(v.index, pd.DatetimeIndex)\n            ]\n            if not_valid_index:\n                raise TypeError(\n                    (f\"All exog must have a Pandas DatetimeIndex as index with the \"\n                     f\"same frequency. Check exog for series: {not_valid_index}\")\n                )\n            \n        # Check that all exog have the same dtypes for common columns\n        exog_dtypes_buffer = [df.dtypes for df in exog_dict.values() if df is not None]\n        exog_dtypes_buffer = pd.concat(exog_dtypes_buffer, axis=1)\n        exog_dtypes_nunique = exog_dtypes_buffer.nunique(axis=1).eq(1)\n        if not exog_dtypes_nunique.all():\n            non_unique_dtyeps_exogs = exog_dtypes_nunique[exog_dtypes_nunique != 1].index.to_list()\n            raise TypeError(f\"Exog/s: {non_unique_dtyeps_exogs} have different dtypes in different series.\")\n\n    exog_names_in_ = list(\n        set(\n            column\n            for df in exog_dict.values()\n            if df is not None\n            for column in df.columns.to_list()\n        )\n    )\n\n    if len(set(exog_names_in_) - set(series_names_in_)) != len(exog_names_in_):\n        raise ValueError(\n            (f\"`exog` cannot contain a column named the same as one of the series.\\n\"\n             f\"    `series` columns : {series_names_in_}.\\n\"\n             f\"    `exog`   columns : {exog_names_in_}.\")\n        )\n\n    return exog_dict, exog_names_in_\n\n\ndef align_series_and_exog_multiseries(\n    series_dict: dict,\n    input_series_is_dict: bool,\n    exog_dict: dict = None\n) -> Tuple[Union[pd.Series, pd.DataFrame], Union[pd.Series, pd.DataFrame]]:\n    \"\"\"\n    Align series and exog according to their index. If needed, reindexing is\n    applied. Heading and trailing NaNs are removed from all series in \n    `series_dict`.\n\n    - If input series is a pandas DataFrame (input_series_is_dict = False),  \n    input exog (pandas Series, DataFrame or dict) must have the same index \n    (type, length and frequency). Reindexing is not applied.\n    - If input series is a dict (input_series_is_dict = True), then input \n    exog must be a dict. Both must have a pandas DatetimeIndex, but can have \n    different lengths. Reindexing is applied.\n\n    Parameters\n    ----------\n    series_dict : dict\n        Dictionary with the series used during training.\n    input_series_is_dict : bool\n        Indicates if input series argument is a dict.\n    exog_dict : dict, default `None`\n        Dictionary with the exogenous variable/s used during training.\n\n    Returns\n    -------\n    series_dict : dict\n        Dictionary with the series used during training.\n    exog_dict : dict\n        Dictionary with the exogenous variable/s used during training.\n    \n    \"\"\"\n\n    for k in series_dict.keys():\n\n        first_valid_index = series_dict[k].first_valid_index()\n        last_valid_index = series_dict[k].last_valid_index()\n\n        series_dict[k] = series_dict[k].loc[first_valid_index : last_valid_index]\n\n        if exog_dict[k] is not None:\n            if input_series_is_dict:\n                index_intersection = (\n                    series_dict[k].index.intersection(exog_dict[k].index)\n                )\n                if len(index_intersection) == 0:\n                    warnings.warn(\n                        (f\"Series '{k}' and its `exog` do not have the same index. \"\n                         f\"All exog values will be NaN for the period of the series.\"),\n                         MissingValuesWarning\n                    )\n                elif len(index_intersection) != len(series_dict[k]):\n                    warnings.warn(\n                        (f\"Series '{k}' and its `exog` do not have the same length. \"\n                         f\"Exog values will be NaN for the not matched period of the series.\"),\n                         MissingValuesWarning\n                    )  \n                exog_dict[k] = exog_dict[k].loc[index_intersection]\n                if len(index_intersection) != len(series_dict[k]):\n                    exog_dict[k] = exog_dict[k].reindex(\n                                       series_dict[k].index, \n                                       fill_value = np.nan\n                                   )\n            else:\n                exog_dict[k] = exog_dict[k].loc[first_valid_index : last_valid_index]\n\n    return series_dict, exog_dict\n\n\ndef prepare_levels_multiseries(\n    X_train_series_names_in_: list,\n    levels: Optional[Union[str, list]] = None\n) -> Tuple[list, bool]:\n    \"\"\"\n    Prepare list of levels to be predicted in multiseries Forecasters.\n\n    Parameters\n    ----------\n    X_train_series_names_in_ : list\n        Names of the series (levels) included in the matrix `X_train`.\n    levels : str, list, default `None`\n        Names of the series (levels) to be predicted.\n\n    Returns\n    -------\n    levels : list\n        Names of the series (levels) to be predicted.\n\n    \"\"\"\n\n    input_levels_is_list = False\n    if levels is None:\n        levels = X_train_series_names_in_\n    elif isinstance(levels, str):\n        levels = [levels]\n    else:\n        input_levels_is_list = True\n\n    return levels, input_levels_is_list\n\n\ndef preprocess_levels_self_last_window_multiseries(\n    levels: list,\n    input_levels_is_list: bool,\n    last_window_: dict\n) -> Tuple[list, pd.DataFrame]:\n    \"\"\"\n    Preprocess `levels` and `last_window` (when using self.last_window_) arguments \n    in multiseries Forecasters when predicting. Only levels whose last window \n    ends at the same datetime index will be predicted together.\n\n    Parameters\n    ----------\n    levels : list\n        Names of the series (levels) to be predicted.\n    input_levels_is_list : bool\n        Indicates if input levels argument is a list.\n    last_window_ : dict\n        Dictionary with the last window of each series (self.last_window_).\n\n    Returns\n    -------\n    levels : list\n        Names of the series (levels) to be predicted.\n    last_window : pandas DataFrame\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n\n    \"\"\"\n\n    available_last_windows = set() if last_window_ is None else set(last_window_.keys())\n    not_available_last_window = set(levels) - available_last_windows\n    if not_available_last_window:\n        levels = [level for level in levels \n                  if level not in not_available_last_window]\n        if not levels:\n            raise ValueError(\n                (f\"No series to predict. None of the series {not_available_last_window} \"\n                 f\"are present in `last_window_` attribute. Provide `last_window` \"\n                 f\"as argument in predict method.\")\n            )\n        else:\n            warnings.warn(\n                (f\"Levels {not_available_last_window} are excluded from \"\n                 f\"prediction since they were not stored in `last_window_` \"\n                 f\"attribute during training. If you don't want to retrain \"\n                 f\"the Forecaster, provide `last_window` as argument.\"),\n                 IgnoredArgumentWarning\n            )\n\n    last_index_levels = [\n        v.index[-1] \n        for k, v in last_window_.items()\n        if k in levels\n    ]\n    if len(set(last_index_levels)) > 1:\n        max_index_levels = max(last_index_levels)\n        selected_levels = [\n            k\n            for k, v in last_window_.items()\n            if k in levels and v.index[-1] == max_index_levels\n        ]\n\n        series_excluded_from_last_window = set(levels) - set(selected_levels)\n        levels = selected_levels\n\n        if input_levels_is_list and series_excluded_from_last_window:\n            warnings.warn(\n                (f\"Only series whose last window ends at the same index \"\n                 f\"can be predicted together. Series that do not reach \"\n                 f\"the maximum index, '{max_index_levels}', are excluded \"\n                 f\"from prediction: {series_excluded_from_last_window}.\"),\n                IgnoredArgumentWarning\n            )\n\n    last_window = pd.DataFrame(\n        {k: v \n         for k, v in last_window_.items() \n         if k in levels}\n    )\n\n    return levels, last_window\n\n\ndef prepare_residuals_multiseries(\n    levels: list,\n    use_in_sample_residuals: bool,\n    encoding: Optional[str] = None,\n    in_sample_residuals_: Optional[dict] = None,\n    out_sample_residuals_: Optional[dict] = None\n) -> Tuple[list, bool]:\n    \"\"\"\n    Prepare residuals for bootstrapping prediction in multiseries Forecasters.\n\n    Parameters\n    ----------\n    levels : list\n        Names of the series (levels) to be predicted.\n    use_in_sample_residuals : bool\n        Indicates if `forecaster.in_sample_residuals_` are used.\n    encoding : str, default `None`\n        Encoding used to identify the different series (`ForecasterRecursiveMultiSeries`).\n    in_sample_residuals_ : dict, default `None`\n        Residuals of the model when predicting training data. Only stored up to\n        1000 values in the form `{level: residuals}`. If `transformer_series` \n        is not `None`, residuals are stored in the transformed scale.\n    out_sample_residuals_ : dict, default `None`\n        Residuals of the model when predicting non-training data. Only stored\n        up to 1000 values in the form `{level: residuals}`. If `transformer_series` \n        is not `None`, residuals are assumed to be in the transformed scale. Use \n        `set_out_sample_residuals()` method to set values.\n\n    Returns\n    -------\n    levels : list\n        Names of the series (levels) to be predicted.\n    residuals : dict\n        Residuals of the model for each level to use in bootstrapping prediction.\n\n    \"\"\"\n\n    if use_in_sample_residuals:\n        unknown_levels = set(levels) - set(in_sample_residuals_.keys())\n        if unknown_levels and encoding is not None:\n            warnings.warn(\n                (f\"`levels` {unknown_levels} are not present in `forecaster.in_sample_residuals_`, \"\n                 f\"most likely because they were not present in the training data. \"\n                 f\"A random sample of the residuals from other levels will be used. \"\n                 f\"This can lead to inaccurate intervals for the unknown levels.\"),\n                 UnknownLevelWarning\n            )\n        residuals = in_sample_residuals_.copy()\n    else:\n        if out_sample_residuals_ is None:\n            raise ValueError(\n                (\"`forecaster.out_sample_residuals_` is `None`. Use \"\n                 \"`use_in_sample_residuals=True` or the \"\n                 \"`set_out_sample_residuals()` method before predicting.\")\n            )\n        else:\n            unknown_levels = set(levels) - set(out_sample_residuals_.keys())\n            if unknown_levels and encoding is not None:\n                warnings.warn(\n                    (f\"`levels` {unknown_levels} are not present in `forecaster.out_sample_residuals_`. \"\n                     f\"A random sample of the residuals from other levels will be used. \"\n                     f\"This can lead to inaccurate intervals for the unknown levels. \"\n                     f\"Otherwise, Use the `set_out_sample_residuals()` method before \"\n                     f\"predicting to set the residuals for these levels.\"),\n                     UnknownLevelWarning\n                )\n            residuals = out_sample_residuals_.copy()\n\n    check_residuals = (\n        \"forecaster.in_sample_residuals_\" if use_in_sample_residuals\n        else \"forecaster.out_sample_residuals_\"\n    )\n    for level in levels:\n        if level in unknown_levels:\n            residuals[level] = residuals['_unknown_level']\n        if residuals[level] is None or len(residuals[level]) == 0:\n            raise ValueError(\n                (f\"Not available residuals for level '{level}'. \"\n                 f\"Check `{check_residuals}`.\")\n            )\n        elif (any(element is None for element in residuals[level]) or\n              np.any(np.isnan(residuals[level]))):\n            raise ValueError(\n                (f\"forecaster residuals for level '{level}' contains `None` \"\n                 f\"or `NaNs` values. Check `{check_residuals}`.\")\n            )\n        \n    return residuals\n\n\ndef prepare_steps_direct(\n    max_step: int,\n    steps: Optional[Union[int, list]] = None\n) -> list:\n    \"\"\"\n    Prepare list of steps to be predicted in Direct Forecasters.\n\n    Parameters\n    ----------\n    max_step : int\n        Maximum number of future steps the forecaster will predict \n        when using method `predict()`.\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n    \n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n\n    Returns\n    -------\n    steps : list\n        Steps to be predicted.\n\n    \"\"\"\n\n    if isinstance(steps, int):\n        steps = list(np.arange(steps) + 1)\n    elif steps is None:\n        steps = list(np.arange(max_step) + 1)\n    elif isinstance(steps, list):\n        steps = list(np.array(steps))\n    \n    for step in steps:\n        if not isinstance(step, (int, np.int64, np.int32)):\n            raise TypeError(\n                (f\"`steps` argument must be an int, a list of ints or `None`. \"\n                 f\"Got {type(steps)}.\")\n            )\n    # Required since numpy 2.0\n    steps = [int(step) for step in steps if step is not None]\n\n    return steps\n\n\ndef set_skforecast_warnings(\n    suppress_warnings: bool,\n    action: str = 'default'\n) -> None:\n    \"\"\"\n    Set skforecast warnings action.\n\n    Parameters\n    ----------\n    suppress_warnings : bool\n        If `True`, skforecast warnings will be suppressed. If `False`, skforecast\n        warnings will be shown as default. See \n        skforecast.exceptions.warn_skforecast_categories for more information.\n    action : str, default `'default'`\n        Action to be taken when a warning is raised. See the warnings module\n        for more information.\n\n    Returns\n    -------\n    None\n    \n    \"\"\"\n\n    if suppress_warnings:\n        for category in warn_skforecast_categories:\n            warnings.filterwarnings(action, category=category)\n",
    "skforecast/metrics/metrics.py": "################################################################################\n#                                metrics                                       #\n#                                                                              #\n# This work by skforecast team is licensed under the BSD 3-Clause License.     #\n################################################################################\n# coding=utf-8\n\nfrom typing import Union, Callable\nimport numpy as np\nimport pandas as pd\nimport inspect\nfrom functools import wraps\nfrom sklearn.metrics import (\n    mean_squared_error,\n    mean_absolute_error,\n    mean_absolute_percentage_error,\n    mean_squared_log_error,\n    median_absolute_error,\n)\n\n\ndef _get_metric(metric: str) -> Callable:\n    \"\"\"\n    Get the corresponding scikit-learn function to calculate the metric.\n\n    Parameters\n    ----------\n    metric : str\n        Metric used to quantify the goodness of fit of the model.\n\n    Returns\n    -------\n    metric : Callable\n        scikit-learn function to calculate the desired metric.\n\n    \"\"\"\n\n    allowed_metrics = [\n        \"mean_squared_error\",\n        \"mean_absolute_error\",\n        \"mean_absolute_percentage_error\",\n        \"mean_squared_log_error\",\n        \"mean_absolute_scaled_error\",\n        \"root_mean_squared_scaled_error\",\n        \"median_absolute_error\",\n    ]\n\n    if metric not in allowed_metrics:\n        raise ValueError((f\"Allowed metrics are: {allowed_metrics}. Got {metric}.\"))\n\n    metrics = {\n        \"mean_squared_error\": mean_squared_error,\n        \"mean_absolute_error\": mean_absolute_error,\n        \"mean_absolute_percentage_error\": mean_absolute_percentage_error,\n        \"mean_squared_log_error\": mean_squared_log_error,\n        \"mean_absolute_scaled_error\": mean_absolute_scaled_error,\n        \"root_mean_squared_scaled_error\": root_mean_squared_scaled_error,\n        \"median_absolute_error\": median_absolute_error,\n    }\n\n    metric = add_y_train_argument(metrics[metric])\n\n    return metric\n\n\ndef add_y_train_argument(func: Callable) -> Callable:\n    \"\"\"\n    Add `y_train` argument to a function if it is not already present.\n\n    Parameters\n    ----------\n    func : callable\n        Function to which the argument is added.\n\n    Returns\n    -------\n    wrapper : callable\n        Function with `y_train` argument added.\n    \n    \"\"\"\n\n    sig = inspect.signature(func)\n    \n    if \"y_train\" in sig.parameters:\n        return func\n\n    new_params = list(sig.parameters.values()) + [\n        inspect.Parameter(\"y_train\", inspect.Parameter.KEYWORD_ONLY, default=None)\n    ]\n    new_sig = sig.replace(parameters=new_params)\n\n    @wraps(func)\n    def wrapper(*args, y_train=None, **kwargs):\n        return func(*args, **kwargs)\n    \n    wrapper.__signature__ = new_sig\n    \n    return wrapper\n\n\ndef mean_absolute_scaled_error(\n    y_true: Union[pd.Series, np.ndarray],\n    y_pred: Union[pd.Series, np.ndarray],\n    y_train: Union[list, pd.Series, np.ndarray],\n) -> float:\n    \"\"\"\n    Mean Absolute Scaled Error (MASE)\n\n    MASE is a scale-independent error metric that measures the accuracy of\n    a forecast. It is the mean absolute error of the forecast divided by the\n    mean absolute error of a naive forecast in the training set. The naive\n    forecast is the one obtained by shifting the time series by one period.\n    If y_train is a list of numpy arrays or pandas Series, it is considered\n    that each element is the true value of the target variable in the training\n    set for each time series. In this case, the naive forecast is calculated\n    for each time series separately.\n\n    Parameters\n    ----------\n    y_true : pandas Series, numpy ndarray\n        True values of the target variable.\n    y_pred : pandas Series, numpy ndarray\n        Predicted values of the target variable.\n    y_train : list, pandas Series, numpy ndarray\n        True values of the target variable in the training set. If `list`, it\n        is consider that each element is the true value of the target variable\n        in the training set for each time series.\n\n    Returns\n    -------\n    mase : float\n        MASE value.\n    \n    \"\"\"\n\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_true` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_pred` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_train, (list, pd.Series, np.ndarray)):\n        raise TypeError(\"`y_train` must be a list, pandas Series or numpy ndarray.\")\n    if isinstance(y_train, list):\n        for x in y_train:\n            if not isinstance(x, (pd.Series, np.ndarray)):\n                raise TypeError(\n                    (\"When `y_train` is a list, each element must be a pandas Series \"\n                     \"or numpy ndarray.\")\n                )\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"`y_true` and `y_pred` must have the same length.\")\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError(\"`y_true` and `y_pred` must have at least one element.\")\n\n    if isinstance(y_train, list):\n        naive_forecast = np.concatenate([np.diff(x) for x in y_train])\n    else:\n        naive_forecast = np.diff(y_train)\n\n    mase = np.mean(np.abs(y_true - y_pred)) / np.nanmean(np.abs(naive_forecast))\n\n    return mase\n\n\ndef root_mean_squared_scaled_error(\n    y_true: Union[pd.Series, np.ndarray],\n    y_pred: Union[pd.Series, np.ndarray],\n    y_train: Union[list, pd.Series, np.ndarray],\n) -> float:\n    \"\"\"\n    Root Mean Squared Scaled Error (RMSSE)\n\n    RMSSE is a scale-independent error metric that measures the accuracy of\n    a forecast. It is the root mean squared error of the forecast divided by\n    the root mean squared error of a naive forecast in the training set. The\n    naive forecast is the one obtained by shifting the time series by one period.\n    If y_train is a list of numpy arrays or pandas Series, it is considered\n    that each element is the true value of the target variable in the training\n    set for each time series. In this case, the naive forecast is calculated\n    for each time series separately.\n\n    Parameters\n    ----------\n    y_true : pandas Series, numpy ndarray\n        True values of the target variable.\n    y_pred : pandas Series, numpy ndarray\n        Predicted values of the target variable.\n    y_train : list, pandas Series, numpy ndarray\n        True values of the target variable in the training set. If list, it\n        is consider that each element is the true value of the target variable\n        in the training set for each time series.\n\n    Returns\n    -------\n    rmsse : float\n        RMSSE value.\n    \n    \"\"\"\n\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_true` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_pred` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_train, (list, pd.Series, np.ndarray)):\n        raise TypeError(\"`y_train` must be a list, pandas Series or numpy ndarray.\")\n    if isinstance(y_train, list):\n        for x in y_train:\n            if not isinstance(x, (pd.Series, np.ndarray)):\n                raise TypeError(\n                    (\"When `y_train` is a list, each element must be a pandas Series \"\n                     \"or numpy ndarray.\")\n                )\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"`y_true` and `y_pred` must have the same length.\")\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError(\"`y_true` and `y_pred` must have at least one element.\")\n\n    if isinstance(y_train, list):\n        naive_forecast = np.concatenate([np.diff(x) for x in y_train])\n    else:\n        naive_forecast = np.diff(y_train)\n    \n    rmsse = np.sqrt(np.mean((y_true - y_pred) ** 2)) / np.sqrt(np.nanmean(naive_forecast ** 2))\n    \n    return rmsse\n",
    "skforecast/recursive/_forecaster_equivalent_date.py": "################################################################################\n#                           ForecasterEquivalentDate                           #\n#                                                                              #\n# This work by skforecast team is licensed under the BSD 3-Clause License.     #\n################################################################################\n# coding=utf-8\n\nfrom typing import Union, Optional, Callable, Any\nimport warnings\nimport sys\nimport numpy as np\nimport pandas as pd\n\nimport skforecast\nfrom ..utils import check_predict_input\nfrom ..utils import preprocess_y\nfrom ..utils import preprocess_last_window\nfrom ..utils import expand_index\n\n\nclass ForecasterEquivalentDate():\n    \"\"\"\n    This forecaster predicts future values based on the most recent equivalent\n    date. It also allows to aggregate multiple past values of the equivalent\n    date using a function (e.g. mean, median, max, min, etc.). The equivalent\n    date is calculated by moving back in time a specified number of steps (offset).\n    The offset can be defined as an integer or as a pandas DateOffset. This\n    approach is useful as a baseline, but it is a simplistic method and may not\n    capture complex underlying patterns.\n    \n    Parameters\n    ----------\n    offset : int, pandas.tseries.offsets.DateOffset\n        Number of steps to go back in time to find the most recent equivalent\n        date to the target period.\n        If `offset` is an integer, it represents the number of steps to go back\n        in time. For example, if the frequency of the time series is daily, \n        `offset = 7` means that the most recent data similar to the target\n        period is the value observed 7 days ago.\n        Pandas DateOffsets can also be used to move forward a given number of \n        valid dates. For example, Bday(2) can be used to move back two business \n        days. If the date does not start on a valid date, it is first moved to a \n        valid date. For example, if the date is a Saturday, it is moved to the \n        previous Friday. Then, the offset is applied. If the result is a non-valid \n        date, it is moved to the next valid date. For example, if the date\n        is a Sunday, it is moved to the next Monday. \n        For more information about offsets, see\n        https://pandas.pydata.org/docs/reference/offset_frequency.html.\n    n_offsets : int, default `1`\n        Number of equivalent dates (multiple of offset) used in the prediction.\n        If `n_offsets` is greater than 1, the values at the equivalent dates are\n        aggregated using the `agg_func` function. For example, if the frequency\n        of the time series is daily, `offset = 7`, `n_offsets = 2` and\n        `agg_func = np.mean`, the predicted value will be the mean of the values\n        observed 7 and 14 days ago.\n    agg_func : Callable, default `np.mean`\n        Function used to aggregate the values of the equivalent dates when the\n        number of equivalent dates (`n_offsets`) is greater than 1.\n    forecaster_id : str, int, default `None`\n        Name used as an identifier of the forecaster.\n    \n    Attributes\n    ----------\n    offset : int, pandas.tseries.offsets.DateOffset\n        Number of steps to go back in time to find the most recent equivalent\n        date to the target period.\n        If `offset` is an integer, it represents the number of steps to go back\n        in time. For example, if the frequency of the time series is daily, \n        `offset = 7` means that the most recent data similar to the target\n        period is the value observed 7 days ago.\n        Pandas DateOffsets can also be used to move forward a given number of \n        valid dates. For example, Bday(2) can be used to move back two business \n        days. If the date does not start on a valid date, it is first moved to a \n        valid date. For example, if the date is a Saturday, it is moved to the \n        previous Friday. Then, the offset is applied. If the result is a non-valid \n        date, it is moved to the next valid date. For example, if the date\n        is a Sunday, it is moved to the next Monday. \n        For more information about offsets, see\n        https://pandas.pydata.org/docs/reference/offset_frequency.html.\n    n_offsets : int\n        Number of equivalent dates (multiple of offset) used in the prediction.\n        If `offset` is greater than 1, the value at the equivalent dates is\n        aggregated using the `agg_func` function. For example, if the frequency\n        of the time series is daily, `offset = 7`, `n_offsets = 2` and\n        `agg_func = np.mean`, the predicted value will be the mean of the values\n        observed 7 and 14 days ago.\n    agg_func : Callable\n        Function used to aggregate the values of the equivalent dates when the\n        number of equivalent dates (`n_offsets`) is greater than 1.\n    window_size : int\n        Number of past values needed to include the last equivalent dates according\n        to the `offset` and `n_offsets`.\n    last_window_ : pandas Series\n        This window represents the most recent data observed by the predictor\n        during its training phase. It contains the past values needed to include\n        the last equivalent date according the `offset` and `n_offsets`.\n    index_type_ : type\n        Type of index of the input used in training.\n    index_freq_ : str\n        Frequency of Index of the input used in training.\n    training_range_ : pandas Index\n        First and last values of index of the data used during training.\n    creation_date : str\n        Date of creation.\n    is_fitted : bool\n        Tag to identify if the regressor has been fitted (trained).\n    fit_date : str\n        Date of last fit.\n    skforecast_version : str\n        Version of skforecast library used to create the forecaster.\n    python_version : str\n        Version of python used to create the forecaster.\n    forecaster_id : str, int\n        Name used as an identifier of the forecaster.\n    regressor : Ignored\n        Not used, present here for API consistency by convention.\n    differentiation : Ignored\n        Not used, present here for API consistency by convention.\n\n    \"\"\"\n    \n    def __init__(\n        self,\n        offset: Union[int, pd.tseries.offsets.DateOffset],\n        n_offsets: int = 1,\n        agg_func: Callable = np.mean,\n        forecaster_id: Optional[Union[str, int]] = None\n    ) -> None:\n        \n        self.offset             = offset\n        self.n_offsets          = n_offsets\n        self.agg_func           = agg_func\n        self.last_window_       = None\n        self.index_type_        = None\n        self.index_freq_        = None\n        self.training_range_    = None\n        self.creation_date      = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.is_fitted          = False\n        self.fit_date           = None\n        self.skforecast_version = skforecast.__version__\n        self.python_version     = sys.version.split(\" \")[0]\n        self.forecaster_id      = forecaster_id\n        self.regressor          = None\n        self.differentiation    = None\n       \n        if not isinstance(self.offset, (int, pd.tseries.offsets.DateOffset)):\n            raise TypeError(\n                (\"`offset` must be an integer greater than 0 or a \"\n                 \"pandas.tseries.offsets. Find more information about offsets in \"\n                 \"https://pandas.pydata.org/docs/reference/offset_frequency.html\")\n            )\n        \n        self.window_size = self.offset * self.n_offsets\n\n    def __repr__(\n        self\n    ) -> str:\n        \"\"\"\n        Information displayed when a Forecaster object is printed.\n        \"\"\"\n\n        info = (\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"{type(self).__name__} \\n\"\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"Offset: {self.offset} \\n\"\n            f\"Number of offsets: {self.n_offsets} \\n\"\n            f\"Aggregation function: {self.agg_func.__name__} \\n\"\n            f\"Window size: {self.window_size} \\n\"\n            f\"Training range: {self.training_range_.to_list() if self.is_fitted else None} \\n\"\n            f\"Training index type: {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None} \\n\"\n            f\"Training index frequency: {self.index_freq_ if self.is_fitted else None} \\n\"\n            f\"Creation date: {self.creation_date} \\n\"\n            f\"Last fit date: {self.fit_date} \\n\"\n            f\"Skforecast version: {self.skforecast_version} \\n\"\n            f\"Python version: {self.python_version} \\n\"\n            f\"Forecaster id: {self.forecaster_id} \\n\"\n        )\n\n        return info\n\n    def fit(\n        self,\n        y: pd.Series,\n        exog: Any = None,\n        store_in_sample_residuals: Any = None\n    ) -> None:\n        \"\"\"\n        Training Forecaster.\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : Ignored\n            Not used, present here for API consistency by convention.\n        store_in_sample_residuals : Ignored\n            Not used, present here for API consistency by convention.\n        \n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n            if not isinstance(y.index, pd.DatetimeIndex):\n                raise TypeError(\n                    (\"If `offset` is a pandas DateOffset, the index of `y` must be a \"\n                     \"pandas DatetimeIndex with frequency.\")\n                )\n            elif y.index.freq is None:\n                raise TypeError(\n                    (\"If `offset` is a pandas DateOffset, the index of `y` must be a \"\n                     \"pandas DatetimeIndex with frequency.\")\n                )\n        \n        # Reset values in case the forecaster has already been fitted.\n        self.last_window_    = None\n        self.index_type_     = None\n        self.index_freq_     = None\n        self.training_range_ = None\n        self.is_fitted       = False\n\n        _, y_index = preprocess_y(y=y, return_values=False)\n\n        if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n            # Calculate the window_size in steps for compatibility with the\n            # check_predict_input function. This is not a exact calculation\n            # because the offset follows the calendar rules and the distance\n            # between two dates may not be constant.\n            first_valid_index = (y_index[-1] - self.offset * self.n_offsets)\n\n            try:\n                window_size_idx_start = y_index.get_loc(first_valid_index)\n                window_size_idx_end = y_index.get_loc(y_index[-1])\n                self.window_size = window_size_idx_end - window_size_idx_start\n            except KeyError:\n                raise ValueError(\n                    (f\"The length of `y` ({len(y)}), must be greater than or equal \"\n                     f\"to the window size ({self.window_size}). This is because  \"\n                     f\"the offset ({self.offset}) is larger than the available \"\n                     f\"data. Try to decrease the size of the offset ({self.offset}), \"\n                     f\"the number of n_offsets ({self.n_offsets}) or increase the \"\n                     f\"size of `y`.\")\n                )\n        else:\n            if len(y) < self.window_size:\n                raise ValueError(\n                    (f\"The length of `y` ({len(y)}), must be greater than or equal \"\n                     f\"to the window size ({self.window_size}). This is because  \"\n                     f\"the offset ({self.offset}) is larger than the available \"\n                     f\"data. Try to decrease the size of the offset ({self.offset}), \"\n                     f\"the number of n_offsets ({self.n_offsets}) or increase the \"\n                     f\"size of `y`.\")\n                )\n        \n        self.is_fitted = True\n        self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.training_range_ = y_index[[0, -1]]\n        self.index_type_ = type(y_index)\n        self.index_freq_ = (\n            y_index.freqstr if isinstance(y_index, pd.DatetimeIndex) else y_index.step\n        )\n        \n        # The last time window of training data is stored so that equivalent\n        # dates are available when calling the `predict` method.\n        # Store the whole series to avoid errors when the offset is larger \n        # than the data available.\n        self.last_window_ = y.copy()\n\n    def predict(\n        self,\n        steps: int,\n        last_window: Optional[pd.Series] = None,\n        exog: Any = None,\n    ) -> pd.Series:\n        \"\"\"\n        Predict n steps ahead.\n        \n        Parameters\n        ----------\n        steps : int\n            Number of future steps predicted.\n        last_window : pandas Series, default `None`\n            Past values needed to select the last equivalent dates according to \n            the offset. If `last_window = None`, the values stored in \n            `self.last_window_` are used and the predictions start immediately \n            after the training data.\n        exog : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        predictions : pandas Series\n            Predicted values.\n        \n        \"\"\"\n\n        if last_window is None:\n            last_window = self.last_window_\n\n        check_predict_input(\n            forecaster_name = type(self).__name__,\n            steps           = steps,\n            is_fitted       = self.is_fitted,\n            exog_in_        = False,\n            index_type_     = self.index_type_,\n            index_freq_     = self.index_freq_,\n            window_size     = self.window_size,\n            last_window     = last_window\n        )\n\n        last_window = last_window.copy()\n\n        last_window_values, last_window_index = preprocess_last_window(\n                                                    last_window = last_window\n                                                )\n        prediction_index = expand_index(index=last_window_index, steps=steps)\n        \n        if isinstance(self.offset, int):\n\n            equivalent_indexes = np.tile(\n                                     np.arange(-self.offset, 0),\n                                     int(np.ceil(steps / self.offset))\n                                 )\n            equivalent_indexes = equivalent_indexes[:steps]\n\n            if self.n_offsets == 1:\n                equivalent_values = last_window_values[equivalent_indexes]\n                predictions = equivalent_values.ravel()\n\n            if self.n_offsets > 1:\n                equivalent_indexes = [equivalent_indexes - n * self.offset \n                                      for n in np.arange(self.n_offsets)]\n                equivalent_indexes = np.vstack(equivalent_indexes)\n                equivalent_values = last_window_values[equivalent_indexes]\n                predictions = np.apply_along_axis(\n                                  self.agg_func,\n                                  axis = 0,\n                                  arr  = equivalent_values\n                              )\n            \n            predictions = pd.Series(\n                              data  = predictions,\n                              index = prediction_index,\n                              name  = 'pred'\n                          )\n\n        if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n\n            predictions_index = prediction_index\n            max_allowed_date = last_window_index[-1]\n\n            # For every date in predictions_index, calculate the n offsets\n            offset_dates = []\n            for date in predictions_index:\n                selected_offsets = []\n                while len(selected_offsets) < self.n_offsets:\n                    offset_date = date - self.offset\n                    if offset_date <= max_allowed_date:\n                        selected_offsets.append(offset_date)\n                    date = offset_date\n                offset_dates.append(selected_offsets)\n            \n            offset_dates = np.array(offset_dates)\n    \n            # Select the values of the time series corresponding to the each\n            # offset date. If the offset date is not in the time series, the\n            # value is set to NaN.\n            equivalent_values = (\n                last_window.\n                reindex(offset_dates.ravel())\n                .to_numpy()\n                .reshape(-1, self.n_offsets)\n            )\n            equivalent_values = pd.DataFrame(\n                                    data    = equivalent_values,\n                                    index   = predictions_index,\n                                    columns = [f'offset_{i}' \n                                               for i in range(self.n_offsets)]\n                                )\n            \n            # Error if all values are missing\n            if equivalent_values.isnull().all().all():\n                raise ValueError(\n                    (f\"All equivalent values are missing. This is caused by using \"\n                     f\"an offset ({self.offset}) larger than the available data. \"\n                     f\"Try to decrease the size of the offset ({self.offset}), \"\n                     f\"the number of n_offsets ({self.n_offsets}) or increase the \"\n                     f\"size of `last_window`. In backtesting, this error may be \"\n                     f\"caused by using an `initial_train_size` too small.\")\n                )\n            \n            # Warning if equivalent values are missing\n            incomplete_offsets = equivalent_values.isnull().any(axis=1)\n            incomplete_offsets = incomplete_offsets[incomplete_offsets].index\n            if not incomplete_offsets.empty:\n                warnings.warn(\n                    (f\"Steps: {incomplete_offsets.strftime('%Y-%m-%d').to_list()} \"\n                     f\"are calculated with less than {self.n_offsets} n_offsets. \"\n                     f\"To avoid this, increase the `last_window` size or decrease \"\n                     f\"the number of n_offsets. The current configuration requires \" \n                     f\"a total offset of {self.offset * self.n_offsets}.\")\n                )\n            \n            aggregate_values = equivalent_values.apply(self.agg_func, axis=1)\n            predictions = aggregate_values.rename('pred')\n        \n        return predictions\n\n    def summary(self) -> None:\n        \"\"\"\n        Show forecaster information.\n        \n        Parameters\n        ----------\n        self\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        \n        print(self)\n"
  },
  "GT_src_dict": {
    "skforecast/model_selection/_validation.py": {
      "_backtesting_forecaster": {
        "code": "def _backtesting_forecaster(forecaster: object, y: pd.Series, metric: Union[str, Callable, list], cv: TimeSeriesFold, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, interval: Optional[list]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False, n_jobs: Union[int, str]='auto', verbose: bool=False, show_progress: bool=True) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Backtests a forecaster model using the specified time series data and performance metrics, applying the cross-validation method defined in the TimeSeriesFold class.\n\nParameters\n----------\nforecaster : object\n    An instance of a forecaster model (e.g., ForecasterRecursive, ForecasterDirect).\ny : pd.Series\n    The training time series data to be used for backtesting.\nmetric : Union[str, Callable, list]\n    The metric(s) to measure the model's performance. This can be a single metric specified as a string, a callable function, or a list of multiple metrics.\ncv : TimeSeriesFold\n    An instance of the TimeSeriesFold class that defines how to split the time series into training and validation sets.\nexog : Optional[Union[pd.Series, pd.DataFrame]], default `None`\n    Exogenous variables that can be used as additional predictors aligned with `y`.\ninterval : Optional[list], default `None`\n    A list defining the confidence level for prediction intervals; example: [2.5, 97.5] for a 95% interval.\nn_boot : int, default `250`\n    The number of bootstrap iterations for estimating prediction intervals.\nrandom_state : int, default `123`\n    Seed for random number generation to ensure reproducibility.\nuse_in_sample_residuals : bool, default `True`\n    Determines whether to use residuals from training data for creating prediction intervals.\nuse_binned_residuals : bool, default `False`\n    If True, uses binned residuals for bootstrapping; if False, selects residuals randomly.\nn_jobs : Union[int, str], default `'auto'`\n    The number of parallel jobs to run; can be set to 'auto' to utilize available CPU cores.\nverbose : bool, default `False`\n    If True, prints additional information about the backtesting process.\nshow_progress : bool, default `True`\n    If True, displays a progress bar during execution.\n\nReturns\n-------\nmetric_values : pd.DataFrame\n    DataFrame containing the results of the specified metrics.\nbacktest_predictions : pd.DataFrame\n    DataFrame containing the predictions and, if requested, their prediction intervals, including columns for lower and upper bounds.\n\nThis function creates a deep copy of the `forecaster` and `cv` to ensure the original objects remain unchanged during the backtesting process. It adapts the training and validation splits based on the `TimeSeriesFold` object, fitting the forecaster multiple times as needed, and parallelizing the predictions to enhance performance.\"\"\"\n    \"\\n    Backtesting of forecaster model following the folds generated by the TimeSeriesFold\\n    class and using the metric(s) provided.\\n\\n    If `forecaster` is already trained and `initial_train_size` is set to `None` in the\\n    TimeSeriesFold class, no initial train will be done and all data will be used\\n    to evaluate the model. However, the first `len(forecaster.last_window)` observations\\n    are needed to create the initial predictors, so no predictions are calculated for\\n    them.\\n    \\n    A copy of the original forecaster is created so that it is not modified during the\\n    process.\\n    \\n    Parameters\\n    ----------\\n    forecaster : ForecasterRecursive, ForecasterDirect\\n        Forecaster model.\\n    y : pandas Series\\n        Training time series.\\n    metric : str, Callable, list\\n        Metric used to quantify the goodness of fit of the model.\\n        \\n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\\n        (Optional) that returns a float.\\n        - If `list`: List containing multiple strings and/or Callables.\\n    cv : TimeSeriesFold\\n        TimeSeriesFold object with the information needed to split the data into folds.\\n        **New in version 0.14.0**\\n    exog : pandas Series, pandas DataFrame, default `None`\\n        Exogenous variable/s included as predictor/s. Must have the same\\n        number of observations as `y` and should be aligned so that y[i] is\\n        regressed on exog[i].\\n    interval : list, default `None`\\n        Confidence of the prediction interval estimated. Sequence of percentiles\\n        to compute, which must be between 0 and 100 inclusive. For example, \\n        interval of 95% should be as `interval = [2.5, 97.5]`. If `None`, no\\n        intervals are estimated.\\n    n_boot : int, default `250`\\n        Number of bootstrapping iterations used to estimate prediction\\n        intervals.\\n    random_state : int, default `123`\\n        Sets a seed to the random generator, so that boot intervals are always \\n        deterministic.\\n    use_in_sample_residuals : bool, default `True`\\n        If `True`, residuals from the training data are used as proxy of prediction \\n        error to create prediction intervals. If `False`, out_sample_residuals \\n        are used if they are already stored inside the forecaster.\\n    use_binned_residuals : bool, default `False`\\n        If `True`, residuals used in each bootstrapping iteration are selected\\n        conditioning on the predicted values. If `False`, residuals are selected\\n        randomly without conditioning on the predicted values.\\n    n_jobs : int, 'auto', default `'auto'`\\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \\n        set to the number of cores. If 'auto', `n_jobs` is set using the function\\n        skforecast.utils.select_n_jobs_backtesting.\\n    verbose : bool, default `False`\\n        Print number of folds and index of training and validation sets used \\n        for backtesting.\\n    show_progress : bool, default `True`\\n        Whether to show a progress bar.\\n\\n    Returns\\n    -------\\n    metric_values : pandas DataFrame\\n        Value(s) of the metric(s).\\n    backtest_predictions : pandas DataFrame\\n        Value of predictions and their estimated interval if `interval` is not `None`.\\n\\n        - column pred: predictions.\\n        - column lower_bound: lower bound of the interval.\\n        - column upper_bound: upper bound of the interval.\\n    \\n    \"\n    forecaster = deepcopy(forecaster)\n    cv = deepcopy(cv)\n    cv.set_params({'window_size': forecaster.window_size, 'differentiation': forecaster.differentiation, 'return_all_indexes': False, 'verbose': verbose})\n    initial_train_size = cv.initial_train_size\n    refit = cv.refit\n    gap = cv.gap\n    if n_jobs == 'auto':\n        n_jobs = select_n_jobs_backtesting(forecaster=forecaster, refit=refit)\n    elif not isinstance(refit, bool) and refit != 1 and (n_jobs != 1):\n        warnings.warn('If `refit` is an integer other than 1 (intermittent refit). `n_jobs` is set to 1 to avoid unexpected results during parallelization.', IgnoredArgumentWarning)\n        n_jobs = 1\n    else:\n        n_jobs = n_jobs if n_jobs > 0 else cpu_count()\n    if not isinstance(metric, list):\n        metrics = [_get_metric(metric=metric) if isinstance(metric, str) else add_y_train_argument(metric)]\n    else:\n        metrics = [_get_metric(metric=m) if isinstance(m, str) else add_y_train_argument(m) for m in metric]\n    store_in_sample_residuals = False if interval is None else True\n    folds = cv.split(X=y, as_pandas=False)\n    window_size = cv.window_size\n    if initial_train_size is not None:\n        exog_train = exog.iloc[:initial_train_size,] if exog is not None else None\n        forecaster.fit(y=y.iloc[:initial_train_size,], exog=exog_train, store_in_sample_residuals=store_in_sample_residuals)\n        folds[0][4] = False\n    if refit:\n        n_of_fits = int(len(folds) / refit)\n        if type(forecaster).__name__ != 'ForecasterDirect' and n_of_fits > 50:\n            warnings.warn(f'The forecaster will be fit {n_of_fits} times. This can take substantial amounts of time. If not feasible, try with `refit = False`.\\n', LongTrainingWarning)\n        elif type(forecaster).__name__ == 'ForecasterDirect' and n_of_fits * forecaster.steps > 50:\n            warnings.warn(f'The forecaster will be fit {n_of_fits * forecaster.steps} times ({n_of_fits} folds * {forecaster.steps} regressors). This can take substantial amounts of time. If not feasible, try with `refit = False`.\\n', LongTrainingWarning)\n    if show_progress:\n        folds = tqdm(folds)\n\n    def _fit_predict_forecaster(y, exog, forecaster, interval, fold, gap):\n        \"\"\"Fit the forecaster on the training data and predict future values based on the fitted model. This auxiliary function is used to parallelize the backtesting process of the forecaster.\n\nParameters\n----------\ny : pandas Series\n    The target time series used for training and prediction.\nexog : pandas Series, pandas DataFrame, optional\n    Exogenous variable(s) used as predictors, must have the same number of observations as `y`.\nforecaster : object\n    The forecaster model that will be fitted and used to make predictions.\ninterval : list, optional\n    Confidence interval percentiles for the predicted values.\nfold : list\n    Information regarding the training and test indices defined by the `TimeSeriesFold` class.\ngap : int\n    Number of observations to drop from the predictions when there is a gap between training and testing data.\n\nReturns\n-------\npred : pandas DataFrame\n    Predicted values based on the fitted forecaster, may include lower and upper bounds if intervals are specified.\n\nNotes\n-----\nThe function handles the fitting of the forecaster by separating the data into training and testing segments identified in `fold`. If the model is not yet fitted, it updates the last window data necessary for making predictions. The predictions are generated using either the standard prediction method or the interval estimation method depending on the presence of the `interval` parameter. The number of steps predicted accommodates any defined gaps based on the forecaster type.\"\"\"\n        '\\n        Fit the forecaster and predict `steps` ahead. This is an auxiliary \\n        function used to parallelize the backtesting_forecaster function.\\n        '\n        train_iloc_start = fold[0][0]\n        train_iloc_end = fold[0][1]\n        last_window_iloc_start = fold[1][0]\n        last_window_iloc_end = fold[1][1]\n        test_iloc_start = fold[2][0]\n        test_iloc_end = fold[2][1]\n        if fold[4] is False:\n            last_window_y = y.iloc[last_window_iloc_start:last_window_iloc_end]\n        else:\n            y_train = y.iloc[train_iloc_start:train_iloc_end,]\n            exog_train = exog.iloc[train_iloc_start:train_iloc_end,] if exog is not None else None\n            last_window_y = None\n            forecaster.fit(y=y_train, exog=exog_train, store_in_sample_residuals=store_in_sample_residuals)\n        next_window_exog = exog.iloc[test_iloc_start:test_iloc_end,] if exog is not None else None\n        steps = len(range(test_iloc_start, test_iloc_end))\n        if type(forecaster).__name__ == 'ForecasterDirect' and gap > 0:\n            test_no_gap_iloc_start = fold[3][0]\n            test_no_gap_iloc_end = fold[3][1]\n            steps = list(np.arange(len(range(test_no_gap_iloc_start, test_no_gap_iloc_end))) + gap + 1)\n        if interval is None:\n            pred = forecaster.predict(steps=steps, last_window=last_window_y, exog=next_window_exog)\n        else:\n            pred = forecaster.predict_interval(steps=steps, last_window=last_window_y, exog=next_window_exog, interval=interval, n_boot=n_boot, random_state=random_state, use_in_sample_residuals=use_in_sample_residuals, use_binned_residuals=use_binned_residuals)\n        if type(forecaster).__name__ != 'ForecasterDirect' and gap > 0:\n            pred = pred.iloc[gap:,]\n        return pred\n    backtest_predictions = Parallel(n_jobs=n_jobs)((delayed(_fit_predict_forecaster)(y=y, exog=exog, forecaster=forecaster, interval=interval, fold=fold, gap=gap) for fold in folds))\n    backtest_predictions = pd.concat(backtest_predictions)\n    if isinstance(backtest_predictions, pd.Series):\n        backtest_predictions = pd.DataFrame(backtest_predictions)\n    train_indexes = []\n    for i, fold in enumerate(folds):\n        fit_fold = fold[-1]\n        if i == 0 or fit_fold:\n            train_iloc_start = fold[0][0] + window_size\n            train_iloc_end = fold[0][1]\n            train_indexes.append(np.arange(train_iloc_start, train_iloc_end))\n    train_indexes = np.unique(np.concatenate(train_indexes))\n    y_train = y.iloc[train_indexes]\n    metric_values = [m(y_true=y.loc[backtest_predictions.index], y_pred=backtest_predictions['pred'], y_train=y_train) for m in metrics]\n    metric_values = pd.DataFrame(data=[metric_values], columns=[m.__name__ for m in metrics])\n    return (metric_values, backtest_predictions)",
        "docstring": "Backtests a forecaster model using the specified time series data and performance metrics, applying the cross-validation method defined in the TimeSeriesFold class.\n\nParameters\n----------\nforecaster : object\n    An instance of a forecaster model (e.g., ForecasterRecursive, ForecasterDirect).\ny : pd.Series\n    The training time series data to be used for backtesting.\nmetric : Union[str, Callable, list]\n    The metric(s) to measure the model's performance. This can be a single metric specified as a string, a callable function, or a list of multiple metrics.\ncv : TimeSeriesFold\n    An instance of the TimeSeriesFold class that defines how to split the time series into training and validation sets.\nexog : Optional[Union[pd.Series, pd.DataFrame]], default `None`\n    Exogenous variables that can be used as additional predictors aligned with `y`.\ninterval : Optional[list], default `None`\n    A list defining the confidence level for prediction intervals; example: [2.5, 97.5] for a 95% interval.\nn_boot : int, default `250`\n    The number of bootstrap iterations for estimating prediction intervals.\nrandom_state : int, default `123`\n    Seed for random number generation to ensure reproducibility.\nuse_in_sample_residuals : bool, default `True`\n    Determines whether to use residuals from training data for creating prediction intervals.\nuse_binned_residuals : bool, default `False`\n    If True, uses binned residuals for bootstrapping; if False, selects residuals randomly.\nn_jobs : Union[int, str], default `'auto'`\n    The number of parallel jobs to run; can be set to 'auto' to utilize available CPU cores.\nverbose : bool, default `False`\n    If True, prints additional information about the backtesting process.\nshow_progress : bool, default `True`\n    If True, displays a progress bar during execution.\n\nReturns\n-------\nmetric_values : pd.DataFrame\n    DataFrame containing the results of the specified metrics.\nbacktest_predictions : pd.DataFrame\n    DataFrame containing the predictions and, if requested, their prediction intervals, including columns for lower and upper bounds.\n\nThis function creates a deep copy of the `forecaster` and `cv` to ensure the original objects remain unchanged during the backtesting process. It adapts the training and validation splits based on the `TimeSeriesFold` object, fitting the forecaster multiple times as needed, and parallelizing the predictions to enhance performance.",
        "signature": "def _backtesting_forecaster(forecaster: object, y: pd.Series, metric: Union[str, Callable, list], cv: TimeSeriesFold, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, interval: Optional[list]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False, n_jobs: Union[int, str]='auto', verbose: bool=False, show_progress: bool=True) -> Tuple[pd.DataFrame, pd.DataFrame]:",
        "type": "Function",
        "class_signature": null
      },
      "backtesting_forecaster": {
        "code": "def backtesting_forecaster(forecaster: object, y: pd.Series, cv: TimeSeriesFold, metric: Union[str, Callable, list], exog: Optional[Union[pd.Series, pd.DataFrame]]=None, interval: Optional[list]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False, n_jobs: Union[int, str]='auto', verbose: bool=False, show_progress: bool=True) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Backtesting of a forecaster model using the TimeSeriesFold class to generate the necessary data splits. This function evaluates the performance of the model based on specified metrics during backtesting, allowing for the assessment of predictive accuracy and reliability over time periods.\n\nParameters\n----------\nforecaster : object\n    A forecaster model (must be either 'ForecasterRecursive' or 'ForecasterDirect').\ny : pd.Series\n    The time series data to be used for training and validation.\ncv : TimeSeriesFold\n    An object that defines how to split the dataset into training and validation folds.\nmetric : Union[str, Callable, list]\n    A metric or list of metrics to evaluate model performance, which can be a predefined string or a custom function.\nexog : Optional[Union[pd.Series, pd.DataFrame]], default=None\n    Optional exogenous variables to include as predictors, aligned with `y`.\ninterval : Optional[list], default=None\n    List of percentiles for estimating prediction intervals, e.g., [2.5, 97.5].\nn_boot : int, default=250\n    Number of bootstrap iterations for estimating intervals.\nrandom_state : int, default=123\n    Seed for reproducibility of random processes in bootstrapping.\nuse_in_sample_residuals : bool, default=True\n    If True, uses in-sample residuals for interval estimation; otherwise, uses out-of-sample residuals if available.\nuse_binned_residuals : bool, default=False\n    If True, residuals are binned based on predicted values during bootstrapping.\nn_jobs : Union[int, str], default='auto'\n    Number of parallel jobs for computation. If 'auto', utilizes the system's available cores.\nverbose : bool, default=False\n    If True, prints detailed information about the backtesting process.\nshow_progress : bool, default=True\n    If True, displays a progress bar during execution.\n\nReturns\n-------\nmetric_values : pd.DataFrame\n    DataFrame containing the calculated metric values.\nbacktest_predictions : pd.DataFrame\n    DataFrame containing predicted values and their corresponding prediction intervals, if provided.\n\nRaises\n------\nTypeError\n    If the forecaster is not of an expected type.\nValueError\n    If the steps and gap parameters are inconsistent with the forecaster settings.\n\nThe function interacts closely with the TimeSeriesFold class for data splitting and requires input validation via the `check_backtesting_input` function to ensure proper configuration before execution.\"\"\"\n    \"\\n    Backtesting of forecaster model following the folds generated by the TimeSeriesFold\\n    class and using the metric(s) provided.\\n\\n    If `forecaster` is already trained and `initial_train_size` is set to `None` in the\\n    TimeSeriesFold class, no initial train will be done and all data will be used\\n    to evaluate the model. However, the first `len(forecaster.last_window)` observations\\n    are needed to create the initial predictors, so no predictions are calculated for\\n    them.\\n    \\n    A copy of the original forecaster is created so that it is not modified during \\n    the process.\\n\\n    Parameters\\n    ----------\\n    forecaster : ForecasterRecursive, ForecasterDirect\\n        Forecaster model.\\n    y : pandas Series\\n        Training time series.\\n    cv : TimeSeriesFold\\n        TimeSeriesFold object with the information needed to split the data into folds.\\n        **New in version 0.14.0**\\n    metric : str, Callable, list\\n        Metric used to quantify the goodness of fit of the model.\\n        \\n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\\n        (Optional) that returns a float.\\n        - If `list`: List containing multiple strings and/or Callables.\\n    exog : pandas Series, pandas DataFrame, default `None`\\n        Exogenous variable/s included as predictor/s. Must have the same\\n        number of observations as `y` and should be aligned so that y[i] is\\n        regressed on exog[i].\\n    interval : list, default `None`\\n        Confidence of the prediction interval estimated. Sequence of percentiles\\n        to compute, which must be between 0 and 100 inclusive. For example, \\n        interval of 95% should be as `interval = [2.5, 97.5]`. If `None`, no\\n        intervals are estimated.\\n    n_boot : int, default `250`\\n        Number of bootstrapping iterations used to estimate prediction\\n        intervals.\\n    random_state : int, default `123`\\n        Sets a seed to the random generator, so that boot intervals are always \\n        deterministic.\\n    use_in_sample_residuals : bool, default `True`\\n        If `True`, residuals from the training data are used as proxy of prediction \\n        error to create prediction intervals. If `False`, out_sample_residuals \\n        are used if they are already stored inside the forecaster.\\n    use_binned_residuals : bool, default `False`\\n        If `True`, residuals used in each bootstrapping iteration are selected\\n        conditioning on the predicted values. If `False`, residuals are selected\\n        randomly without conditioning on the predicted values.\\n    n_jobs : int, 'auto', default `'auto'`\\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \\n        set to the number of cores. If 'auto', `n_jobs` is set using the function\\n        skforecast.utils.select_n_jobs_backtesting.\\n    verbose : bool, default `False`\\n        Print number of folds and index of training and validation sets used \\n        for backtesting.\\n    show_progress : bool, default `True`\\n        Whether to show a progress bar.\\n\\n    Returns\\n    -------\\n    metric_values : pandas DataFrame\\n        Value(s) of the metric(s).\\n    backtest_predictions : pandas DataFrame\\n        Value of predictions and their estimated interval if `interval` is not `None`.\\n\\n        - column pred: predictions.\\n        - column lower_bound: lower bound of the interval.\\n        - column upper_bound: upper bound of the interval.\\n    \\n    \"\n    forecaters_allowed = ['ForecasterRecursive', 'ForecasterDirect', 'ForecasterEquivalentDate']\n    if type(forecaster).__name__ not in forecaters_allowed:\n        raise TypeError(f'`forecaster` must be of type {forecaters_allowed}, for all other types of  forecasters use the functions available in the other `model_selection` modules.')\n    check_backtesting_input(forecaster=forecaster, cv=cv, y=y, metric=metric, interval=interval, n_boot=n_boot, random_state=random_state, use_in_sample_residuals=use_in_sample_residuals, use_binned_residuals=use_binned_residuals, n_jobs=n_jobs, show_progress=show_progress)\n    if type(forecaster).__name__ == 'ForecasterDirect' and forecaster.steps < cv.steps + cv.gap:\n        raise ValueError(f'When using a ForecasterDirect, the combination of steps + gap ({cv.steps + cv.gap}) cannot be greater than the `steps` parameter declared when the forecaster is initialized ({forecaster.steps}).')\n    metric_values, backtest_predictions = _backtesting_forecaster(forecaster=forecaster, y=y, cv=cv, metric=metric, exog=exog, interval=interval, n_boot=n_boot, random_state=random_state, use_in_sample_residuals=use_in_sample_residuals, use_binned_residuals=use_binned_residuals, n_jobs=n_jobs, verbose=verbose, show_progress=show_progress)\n    return (metric_values, backtest_predictions)",
        "docstring": "Backtesting of a forecaster model using the TimeSeriesFold class to generate the necessary data splits. This function evaluates the performance of the model based on specified metrics during backtesting, allowing for the assessment of predictive accuracy and reliability over time periods.\n\nParameters\n----------\nforecaster : object\n    A forecaster model (must be either 'ForecasterRecursive' or 'ForecasterDirect').\ny : pd.Series\n    The time series data to be used for training and validation.\ncv : TimeSeriesFold\n    An object that defines how to split the dataset into training and validation folds.\nmetric : Union[str, Callable, list]\n    A metric or list of metrics to evaluate model performance, which can be a predefined string or a custom function.\nexog : Optional[Union[pd.Series, pd.DataFrame]], default=None\n    Optional exogenous variables to include as predictors, aligned with `y`.\ninterval : Optional[list], default=None\n    List of percentiles for estimating prediction intervals, e.g., [2.5, 97.5].\nn_boot : int, default=250\n    Number of bootstrap iterations for estimating intervals.\nrandom_state : int, default=123\n    Seed for reproducibility of random processes in bootstrapping.\nuse_in_sample_residuals : bool, default=True\n    If True, uses in-sample residuals for interval estimation; otherwise, uses out-of-sample residuals if available.\nuse_binned_residuals : bool, default=False\n    If True, residuals are binned based on predicted values during bootstrapping.\nn_jobs : Union[int, str], default='auto'\n    Number of parallel jobs for computation. If 'auto', utilizes the system's available cores.\nverbose : bool, default=False\n    If True, prints detailed information about the backtesting process.\nshow_progress : bool, default=True\n    If True, displays a progress bar during execution.\n\nReturns\n-------\nmetric_values : pd.DataFrame\n    DataFrame containing the calculated metric values.\nbacktest_predictions : pd.DataFrame\n    DataFrame containing predicted values and their corresponding prediction intervals, if provided.\n\nRaises\n------\nTypeError\n    If the forecaster is not of an expected type.\nValueError\n    If the steps and gap parameters are inconsistent with the forecaster settings.\n\nThe function interacts closely with the TimeSeriesFold class for data splitting and requires input validation via the `check_backtesting_input` function to ensure proper configuration before execution.",
        "signature": "def backtesting_forecaster(forecaster: object, y: pd.Series, cv: TimeSeriesFold, metric: Union[str, Callable, list], exog: Optional[Union[pd.Series, pd.DataFrame]]=None, interval: Optional[list]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False, n_jobs: Union[int, str]='auto', verbose: bool=False, show_progress: bool=True) -> Tuple[pd.DataFrame, pd.DataFrame]:",
        "type": "Function",
        "class_signature": null
      },
      "_fit_predict_forecaster": {
        "code": "    def _fit_predict_forecaster(y, exog, forecaster, alpha, interval, fold, steps, gap):\n        \"\"\"Fit the forecaster model to training data and predict future values, facilitating parallelization during backtesting.\n\nParameters\n----------\ny : pandas Series\n    The target time series data used for training and prediction.\nexog : pandas Series, pandas DataFrame, optional\n    Exogenous variables for predictions, must align with `y`.\nforecaster : object\n    The forecasting model that supports fitting and predicting.\nalpha : float, optional\n    Confidence level for prediction intervals (1 - alpha) if provided with `interval`.\ninterval : list, optional\n    Confidence percentiles for prediction intervals, must be between 0 and 100.\nfold : tuple\n    A tuple containing train and test indices for splitting the data into folds.\nsteps : int\n    The number of steps ahead to predict.\ngap : int\n    The number of observations to skip in the predictions.\n\nReturns\n-------\npred : pandas DataFrame\n    Predicted values for the specified steps ahead, including upper and lower bounds if intervals are requested.\n\nThis function interacts with the overall backtesting process by leveraging the `fold` to manage data splits and maintaining a clear distinction between training and testing phases. The model is fitted each time before making predictions, thereby adapting to varying training sizes as specified by `refit`. Existing variables such as `last_window_y` and `last_window_exog` are used for making predictions based on the last fitted state of the forecaster. The function is utilized within the `_backtesting_forecaster` and `_backtesting_sarimax` functions, allowing for efficient model evaluation across folds.\"\"\"\n        '\\n        Fit the forecaster and predict `steps` ahead. This is an auxiliary \\n        function used to parallelize the backtesting_forecaster function.\\n        '\n        train_idx_start = fold[0][0]\n        train_idx_end = fold[0][1]\n        test_idx_start = fold[2][0]\n        test_idx_end = fold[2][1]\n        if refit:\n            last_window_start = fold[0][1]\n            last_window_end = fold[1][1]\n        else:\n            last_window_end = fold[2][0]\n            last_window_start = last_window_end - steps\n        if fold[4] is False:\n            last_window_y = y.iloc[last_window_start:last_window_end]\n            last_window_exog = exog.iloc[last_window_start:last_window_end] if exog is not None else None\n        else:\n            y_train = y.iloc[train_idx_start:train_idx_end,]\n            exog_train = exog.iloc[train_idx_start:train_idx_end,] if exog is not None else None\n            last_window_y = None\n            last_window_exog = None\n            forecaster.fit(y=y_train, exog=exog_train, suppress_warnings=suppress_warnings_fit)\n        next_window_exog = exog.iloc[test_idx_start:test_idx_end,] if exog is not None else None\n        if fold == folds[0]:\n            last_window_y = None\n            last_window_exog = None\n        steps = len(range(test_idx_start, test_idx_end))\n        if alpha is None and interval is None:\n            pred = forecaster.predict(steps=steps, last_window=last_window_y, last_window_exog=last_window_exog, exog=next_window_exog)\n        else:\n            pred = forecaster.predict_interval(steps=steps, exog=next_window_exog, alpha=alpha, interval=interval, last_window=last_window_y, last_window_exog=last_window_exog)\n        pred = pred.iloc[gap:,]\n        return pred",
        "docstring": "Fit the forecaster model to training data and predict future values, facilitating parallelization during backtesting.\n\nParameters\n----------\ny : pandas Series\n    The target time series data used for training and prediction.\nexog : pandas Series, pandas DataFrame, optional\n    Exogenous variables for predictions, must align with `y`.\nforecaster : object\n    The forecasting model that supports fitting and predicting.\nalpha : float, optional\n    Confidence level for prediction intervals (1 - alpha) if provided with `interval`.\ninterval : list, optional\n    Confidence percentiles for prediction intervals, must be between 0 and 100.\nfold : tuple\n    A tuple containing train and test indices for splitting the data into folds.\nsteps : int\n    The number of steps ahead to predict.\ngap : int\n    The number of observations to skip in the predictions.\n\nReturns\n-------\npred : pandas DataFrame\n    Predicted values for the specified steps ahead, including upper and lower bounds if intervals are requested.\n\nThis function interacts with the overall backtesting process by leveraging the `fold` to manage data splits and maintaining a clear distinction between training and testing phases. The model is fitted each time before making predictions, thereby adapting to varying training sizes as specified by `refit`. Existing variables such as `last_window_y` and `last_window_exog` are used for making predictions based on the last fitted state of the forecaster. The function is utilized within the `_backtesting_forecaster` and `_backtesting_sarimax` functions, allowing for efficient model evaluation across folds.",
        "signature": "def _fit_predict_forecaster(y, exog, forecaster, alpha, interval, fold, steps, gap):",
        "type": "Function",
        "class_signature": null
      }
    },
    "skforecast/model_selection/_split.py": {
      "BaseFold.__init__": {
        "code": "    def __init__(self, steps: Optional[int]=None, initial_train_size: Optional[int]=None, window_size: Optional[int]=None, differentiation: Optional[int]=None, refit: Union[bool, int]=False, fixed_train_size: bool=True, gap: int=0, skip_folds: Optional[Union[int, list]]=None, allow_incomplete_fold: bool=True, return_all_indexes: bool=False, verbose: bool=True) -> None:\n        \"\"\"__init__ method for the BaseFold class, which serves as a foundational class for all fold classes in skforecast designed for time series cross-validation. This method initializes various parameters that control the behavior of the time series splitting process, including the number of observations predicted in each fold (steps), the initial size of the training set, and refitting options.\n\nParameters\n----------\nsteps : Optional[int], default `None`\n    The number of observations to be predicted in each fold, also known as forecast horizon or test size.\ninitial_train_size : Optional[int], default `None`\n    The number of observations used for initial training.\nwindow_size : Optional[int], default `None`\n    The number of observations required to generate autoregressive predictors.\ndifferentiation : Optional[int], default `None`\n    The number of observations to use for differentiation, extending the last window.\nrefit : Union[bool, int], default `False`\n    Controls whether the forecaster is refitted in each fold.\nfixed_train_size : bool, default `True`\n    Indicates whether the training size is fixed or increases in each fold.\ngap : int, default `0`\n    The number of observations between the training set and the test set.\nskip_folds : Optional[Union[int, list]], default `None`\n    Specifies how many folds to skip during the split.\nallow_incomplete_fold : bool, default `True`\n    Determines if the last fold can contain fewer observations than steps.\nreturn_all_indexes : bool, default `False`\n    Indicates whether to return all indexes or only start and end indexes of each fold.\nverbose : bool, default `True`\n    If True, provides verbose output with information about generated folds.\n\nReturns\n-------\nNone\n    This method does not return any value but raises exceptions if the input parameters are invalid.\n\nNotes\n-----\nThis method calls the `_validate_params` method to ensure that the input parameters adhere to defined constraints and initializes instance attributes accordingly. Each parameter influences how the class handles time series data and generates folds, making correct initialization essential for proper functioning.\"\"\"\n        self._validate_params(cv_name=type(self).__name__, steps=steps, initial_train_size=initial_train_size, window_size=window_size, differentiation=differentiation, refit=refit, fixed_train_size=fixed_train_size, gap=gap, skip_folds=skip_folds, allow_incomplete_fold=allow_incomplete_fold, return_all_indexes=return_all_indexes, verbose=verbose)\n        self.steps = steps\n        self.initial_train_size = initial_train_size\n        self.window_size = window_size\n        self.differentiation = differentiation\n        self.refit = refit\n        self.fixed_train_size = fixed_train_size\n        self.gap = gap\n        self.skip_folds = skip_folds\n        self.allow_incomplete_fold = allow_incomplete_fold\n        self.return_all_indexes = return_all_indexes\n        self.verbose = verbose",
        "docstring": "__init__ method for the BaseFold class, which serves as a foundational class for all fold classes in skforecast designed for time series cross-validation. This method initializes various parameters that control the behavior of the time series splitting process, including the number of observations predicted in each fold (steps), the initial size of the training set, and refitting options.\n\nParameters\n----------\nsteps : Optional[int], default `None`\n    The number of observations to be predicted in each fold, also known as forecast horizon or test size.\ninitial_train_size : Optional[int], default `None`\n    The number of observations used for initial training.\nwindow_size : Optional[int], default `None`\n    The number of observations required to generate autoregressive predictors.\ndifferentiation : Optional[int], default `None`\n    The number of observations to use for differentiation, extending the last window.\nrefit : Union[bool, int], default `False`\n    Controls whether the forecaster is refitted in each fold.\nfixed_train_size : bool, default `True`\n    Indicates whether the training size is fixed or increases in each fold.\ngap : int, default `0`\n    The number of observations between the training set and the test set.\nskip_folds : Optional[Union[int, list]], default `None`\n    Specifies how many folds to skip during the split.\nallow_incomplete_fold : bool, default `True`\n    Determines if the last fold can contain fewer observations than steps.\nreturn_all_indexes : bool, default `False`\n    Indicates whether to return all indexes or only start and end indexes of each fold.\nverbose : bool, default `True`\n    If True, provides verbose output with information about generated folds.\n\nReturns\n-------\nNone\n    This method does not return any value but raises exceptions if the input parameters are invalid.\n\nNotes\n-----\nThis method calls the `_validate_params` method to ensure that the input parameters adhere to defined constraints and initializes instance attributes accordingly. Each parameter influences how the class handles time series data and generates folds, making correct initialization essential for proper functioning.",
        "signature": "def __init__(self, steps: Optional[int]=None, initial_train_size: Optional[int]=None, window_size: Optional[int]=None, differentiation: Optional[int]=None, refit: Union[bool, int]=False, fixed_train_size: bool=True, gap: int=0, skip_folds: Optional[Union[int, list]]=None, allow_incomplete_fold: bool=True, return_all_indexes: bool=False, verbose: bool=True) -> None:",
        "type": "Method",
        "class_signature": "class BaseFold:"
      },
      "BaseFold._validate_params": {
        "code": "    def _validate_params(self, cv_name: str, steps: Optional[int]=None, initial_train_size: Optional[int]=None, window_size: Optional[int]=None, differentiation: Optional[int]=None, refit: Union[bool, int]=False, fixed_train_size: bool=True, gap: int=0, skip_folds: Optional[Union[int, list]]=None, allow_incomplete_fold: bool=True, return_all_indexes: bool=False, verbose: bool=True) -> None:\n        \"\"\"Validate all input parameters for the Fold class to ensure they conform to expected types and constraints. \n    This method raises exceptions if any parameter is invalid, ensuring that derived classes can function correctly based on the provided parameters.\n\n    Parameters\n    ----------\n    cv_name : str\n        The name of the cross-validation method (e.g., \"TimeSeriesFold\" or \"OneStepAheadFold\") which influences validation rules.\n    steps : Optional[int], default `None`\n        Number of observations to predict in each fold, must be an integer greater than 0 for \"TimeSeriesFold\".\n    initial_train_size : Optional[int], default `None`\n        Number of observations for initial training; should be greater than 0 unless `None`.\n    window_size : Optional[int], default `None`\n        Number of observations required to create autoregressive predictors; must be greater than 0 if provided.\n    differentiation : Optional[int], default `None`\n        Indicates how many observations to use for differentiation, must be an integer greater than or equal to 0 if provided.\n    refit : Union[bool, int], default `False`\n        Determines whether to refit the forecaster, accepts boolean or integer greater than or equal to 0.\n    fixed_train_size : bool, default `True`\n        Specifies if the training size remains constant across folds; requires a boolean value.\n    gap : int, default `0`\n        Number of observations between training and testing; must be an integer greater than or equal to 0.\n    skip_folds : Optional[Union[int, list]], default `None`\n        Skips certain folds based on the provided integer or list; integers must be greater than 0.\n    allow_incomplete_fold : bool, default `True`\n        If set to `False`, the last fold must be complete, otherwise it is ignored.\n    return_all_indexes : bool, default `False`\n        Specifies if all fold indexes should be returned or just the start and end indexes.\n    verbose : bool, default `True`\n        Controls if validation information is printed during processing.\n\n    Returns\n    -------\n    None\n        This method validates parameters directly and raises exceptions upon receiving invalid arguments.\n\n    Raises\n    ------\n    ValueError\n        If `steps`, `initial_train_size`, `gap`, or any other constraint is not satisfied.\n    TypeError\n        If the types of any parameters do not match the expected types.\"\"\"\n        '\\n        Validate all input parameters to ensure correctness.\\n        '\n        if cv_name == 'TimeSeriesFold':\n            if not isinstance(steps, (int, np.integer)) or steps < 1:\n                raise ValueError(f'`steps` must be an integer greater than 0. Got {steps}.')\n            if not isinstance(initial_train_size, (int, np.integer, type(None))):\n                raise ValueError(f'`initial_train_size` must be an integer greater than 0 or None. Got {initial_train_size}.')\n            if initial_train_size is not None and initial_train_size < 1:\n                raise ValueError(f'`initial_train_size` must be an integer greater than 0 or None. Got {initial_train_size}.')\n            if not isinstance(refit, (bool, int, np.integer)):\n                raise TypeError(f'`refit` must be a boolean or an integer equal or greater than 0. Got {refit}.')\n            if isinstance(refit, (int, np.integer)) and (not isinstance(refit, bool)) and (refit < 0):\n                raise TypeError(f'`refit` must be a boolean or an integer equal or greater than 0. Got {refit}.')\n            if not isinstance(fixed_train_size, bool):\n                raise TypeError(f'`fixed_train_size` must be a boolean: `True`, `False`. Got {fixed_train_size}.')\n            if not isinstance(gap, (int, np.integer)) or gap < 0:\n                raise ValueError(f'`gap` must be an integer greater than or equal to 0. Got {gap}.')\n            if skip_folds is not None:\n                if not isinstance(skip_folds, (int, np.integer, list, type(None))):\n                    raise TypeError(f'`skip_folds` must be an integer greater than 0, a list of integers or `None`. Got {skip_folds}.')\n                if isinstance(skip_folds, (int, np.integer)) and skip_folds < 1:\n                    raise ValueError(f'`skip_folds` must be an integer greater than 0, a list of integers or `None`. Got {skip_folds}.')\n                if isinstance(skip_folds, list) and any([x < 1 for x in skip_folds]):\n                    raise ValueError(f'`skip_folds` list must contain integers greater than or equal to 1. The first fold is always needed to train the forecaster. Got {skip_folds}.')\n            if not isinstance(allow_incomplete_fold, bool):\n                raise TypeError(f'`allow_incomplete_fold` must be a boolean: `True`, `False`. Got {allow_incomplete_fold}.')\n        if cv_name == 'OneStepAheadFold':\n            if not isinstance(initial_train_size, (int, np.integer)) or initial_train_size < 1:\n                raise ValueError(f'`initial_train_size` must be an integer greater than 0. Got {initial_train_size}.')\n        if not isinstance(window_size, (int, np.integer, pd.DateOffset, type(None))) or (isinstance(window_size, (int, np.integer)) and window_size < 1):\n            raise ValueError(f'`window_size` must be an integer greater than 0. Got {window_size}.')\n        if not isinstance(return_all_indexes, bool):\n            raise TypeError(f'`return_all_indexes` must be a boolean: `True`, `False`. Got {return_all_indexes}.')\n        if differentiation is not None:\n            if not isinstance(differentiation, (int, np.integer)) or differentiation < 0:\n                raise ValueError(f'`differentiation` must be None or an integer greater than or equal to 0. Got {differentiation}.')\n        if not isinstance(verbose, bool):\n            raise TypeError(f'`verbose` must be a boolean: `True`, `False`. Got {verbose}.')",
        "docstring": "Validate all input parameters for the Fold class to ensure they conform to expected types and constraints. \nThis method raises exceptions if any parameter is invalid, ensuring that derived classes can function correctly based on the provided parameters.\n\nParameters\n----------\ncv_name : str\n    The name of the cross-validation method (e.g., \"TimeSeriesFold\" or \"OneStepAheadFold\") which influences validation rules.\nsteps : Optional[int], default `None`\n    Number of observations to predict in each fold, must be an integer greater than 0 for \"TimeSeriesFold\".\ninitial_train_size : Optional[int], default `None`\n    Number of observations for initial training; should be greater than 0 unless `None`.\nwindow_size : Optional[int], default `None`\n    Number of observations required to create autoregressive predictors; must be greater than 0 if provided.\ndifferentiation : Optional[int], default `None`\n    Indicates how many observations to use for differentiation, must be an integer greater than or equal to 0 if provided.\nrefit : Union[bool, int], default `False`\n    Determines whether to refit the forecaster, accepts boolean or integer greater than or equal to 0.\nfixed_train_size : bool, default `True`\n    Specifies if the training size remains constant across folds; requires a boolean value.\ngap : int, default `0`\n    Number of observations between training and testing; must be an integer greater than or equal to 0.\nskip_folds : Optional[Union[int, list]], default `None`\n    Skips certain folds based on the provided integer or list; integers must be greater than 0.\nallow_incomplete_fold : bool, default `True`\n    If set to `False`, the last fold must be complete, otherwise it is ignored.\nreturn_all_indexes : bool, default `False`\n    Specifies if all fold indexes should be returned or just the start and end indexes.\nverbose : bool, default `True`\n    Controls if validation information is printed during processing.\n\nReturns\n-------\nNone\n    This method validates parameters directly and raises exceptions upon receiving invalid arguments.\n\nRaises\n------\nValueError\n    If `steps`, `initial_train_size`, `gap`, or any other constraint is not satisfied.\nTypeError\n    If the types of any parameters do not match the expected types.",
        "signature": "def _validate_params(self, cv_name: str, steps: Optional[int]=None, initial_train_size: Optional[int]=None, window_size: Optional[int]=None, differentiation: Optional[int]=None, refit: Union[bool, int]=False, fixed_train_size: bool=True, gap: int=0, skip_folds: Optional[Union[int, list]]=None, allow_incomplete_fold: bool=True, return_all_indexes: bool=False, verbose: bool=True) -> None:",
        "type": "Method",
        "class_signature": "class BaseFold:"
      },
      "BaseFold._extract_index": {
        "code": "    def _extract_index(self, X: Union[pd.Series, pd.DataFrame, pd.Index, dict]) -> pd.Index:\n        \"\"\"Extracts and returns the index from the input time series data X. \n\nThe method supports various input types including pandas Series, DataFrames, Dictionaries, and Index objects. If X is a Series or DataFrame, its index is returned directly. If X is a dictionary, the function verifies that the contained Series share the same frequency and creates a date range from the minimum to maximum index of the Series. If X is of an unsupported type, the original X is returned as the index.\n\nParameters\n----------\nX : pandas Series, pandas DataFrame, pandas Index, dict\n    The time series data or index from which to extract the index.\n\nReturns\n-------\npd.Index\n    The extracted index from the input data.\n\nRaises\n------\nValueError\n    If the input dictionary lacks frequency information or if the Series in the dictionary have different frequencies.\n\nThis method is a utility used within the BaseFold class to standardize how indices are extracted across different input types, ensuring consistency in the subsequent processing within methods such as split. It plays a crucial role in preparing data for time series cross-validation by providing a reliable index format.\"\"\"\n        '\\n        Extracts and returns the index from the input data X.\\n\\n        Parameters\\n        ----------\\n        X : pandas Series, pandas DataFrame, pandas Index, dict\\n            Time series data or index to split.\\n\\n        Returns\\n        -------\\n        idx : pandas Index\\n            Index extracted from the input data.\\n        \\n        '\n        if isinstance(X, (pd.Series, pd.DataFrame)):\n            idx = X.index\n        elif isinstance(X, dict):\n            freqs = [s.index.freq for s in X.values() if s.index.freq is not None]\n            if not freqs:\n                raise ValueError('At least one series must have a frequency.')\n            if not all((f == freqs[0] for f in freqs)):\n                raise ValueError('All series with frequency must have the same frequency.')\n            min_idx = min([v.index[0] for v in X.values()])\n            max_idx = max([v.index[-1] for v in X.values()])\n            idx = pd.date_range(start=min_idx, end=max_idx, freq=freqs[0])\n        else:\n            idx = X\n        return idx",
        "docstring": "Extracts and returns the index from the input time series data X. \n\nThe method supports various input types including pandas Series, DataFrames, Dictionaries, and Index objects. If X is a Series or DataFrame, its index is returned directly. If X is a dictionary, the function verifies that the contained Series share the same frequency and creates a date range from the minimum to maximum index of the Series. If X is of an unsupported type, the original X is returned as the index.\n\nParameters\n----------\nX : pandas Series, pandas DataFrame, pandas Index, dict\n    The time series data or index from which to extract the index.\n\nReturns\n-------\npd.Index\n    The extracted index from the input data.\n\nRaises\n------\nValueError\n    If the input dictionary lacks frequency information or if the Series in the dictionary have different frequencies.\n\nThis method is a utility used within the BaseFold class to standardize how indices are extracted across different input types, ensuring consistency in the subsequent processing within methods such as split. It plays a crucial role in preparing data for time series cross-validation by providing a reliable index format.",
        "signature": "def _extract_index(self, X: Union[pd.Series, pd.DataFrame, pd.Index, dict]) -> pd.Index:",
        "type": "Method",
        "class_signature": "class BaseFold:"
      },
      "BaseFold.set_params": {
        "code": "    def set_params(self, params: dict) -> None:\n        \"\"\"Set the parameters of the Fold object, validating their correctness before applying any updates. This method checks if the input is a dictionary and filters the parameters to ensure only valid attributes of the class are modified.\n\n    Parameters\n    ----------\n    params : dict\n        A dictionary of parameters to set for the Fold instance. It should contain keys that correspond to the instance's attributes.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    TypeError\n        If `params` is not a dictionary.\n    \n    Side Effects\n    -------------\n    Modifies the attributes of the instance based on the filtered parameters. If there are unknown parameters not defined in the instance, a warning is issued using the `IgnoredArgumentWarning`. The current parameters are copied using `deepcopy` to ensure that the original instance remains intact while the new parameters are validated and assigned.\n\n    Notes\n    -----\n    This method interacts with `_validate_params`, which ensures all updated parameters comply with defined constraints. The attributes affected are parameters defining the behavior of cross-validation in time series forecasting, such as `steps`, `initial_train_size`, and other configuration options.\"\"\"\n        '\\n        Set the parameters of the Fold object. Before overwriting the current \\n        parameters, the input parameters are validated to ensure correctness.\\n\\n        Parameters\\n        ----------\\n        params : dict\\n            Dictionary with the parameters to set.\\n        \\n        Returns\\n        -------\\n        None\\n        \\n        '\n        if not isinstance(params, dict):\n            raise TypeError(f'`params` must be a dictionary. Got {type(params)}.')\n        current_params = deepcopy(vars(self))\n        unknown_params = set(params.keys()) - set(current_params.keys())\n        if unknown_params:\n            warnings.warn(f'Unknown parameters: {unknown_params}. They have been ignored.', IgnoredArgumentWarning)\n        filtered_params = {k: v for k, v in params.items() if k in current_params}\n        updated_params = {'cv_name': type(self).__name__, **current_params, **filtered_params}\n        self._validate_params(**updated_params)\n        for key, value in updated_params.items():\n            setattr(self, key, value)",
        "docstring": "Set the parameters of the Fold object, validating their correctness before applying any updates. This method checks if the input is a dictionary and filters the parameters to ensure only valid attributes of the class are modified.\n\nParameters\n----------\nparams : dict\n    A dictionary of parameters to set for the Fold instance. It should contain keys that correspond to the instance's attributes.\n\nReturns\n-------\nNone\n\nRaises\n------\nTypeError\n    If `params` is not a dictionary.\n\nSide Effects\n-------------\nModifies the attributes of the instance based on the filtered parameters. If there are unknown parameters not defined in the instance, a warning is issued using the `IgnoredArgumentWarning`. The current parameters are copied using `deepcopy` to ensure that the original instance remains intact while the new parameters are validated and assigned.\n\nNotes\n-----\nThis method interacts with `_validate_params`, which ensures all updated parameters comply with defined constraints. The attributes affected are parameters defining the behavior of cross-validation in time series forecasting, such as `steps`, `initial_train_size`, and other configuration options.",
        "signature": "def set_params(self, params: dict) -> None:",
        "type": "Method",
        "class_signature": "class BaseFold:"
      },
      "TimeSeriesFold.__init__": {
        "code": "    def __init__(self, steps: int, initial_train_size: Optional[int]=None, window_size: Optional[int]=None, differentiation: Optional[int]=None, refit: Union[bool, int]=False, fixed_train_size: bool=True, gap: int=0, skip_folds: Optional[Union[int, list]]=None, allow_incomplete_fold: bool=True, return_all_indexes: bool=False, verbose: bool=True) -> None:\n        \"\"\"Initialize a TimeSeriesFold instance for splitting time series data into training\nand testing folds. This class allows for flexible configurations for cross-validation\nin time series modeling, where various parameters dictate the nature of the folds,\nincluding the size of the training and testing sets, the presence of differentiation,\nand options for refitting models.\n\nParameters\n----------\nsteps : int\n    The number of observations to predict in each fold, also referred to as the \n    forecast horizon or test size.\ninitial_train_size : Optional[int], default=None\n    The number of observations to use for the initial training set. If None, the forecaster\n    is not trained during the first fold.\nwindow_size : Optional[int], default=None\n    The number of observations required to generate autoregressive predictors.\ndifferentiation : Optional[int], default=None\n    The number of observations to extend the last window for differentiation.\nrefit : Union[bool, int], default=False\n    Determines if the forecaster should be refitted in each fold. Can be set to \n    True or a positive integer value.\nfixed_train_size : bool, default=True\n    Indicates whether the training size remains constant across folds.\ngap : int, default=0\n    The number of observations to omit between the end of the training set and the start of the test set.\nskip_folds : Optional[Union[int, list]], default=None\n    Specifies the number of folds to skip during the fold generation.\nallow_incomplete_fold : bool, default=True\n    Whether to permit the final fold to contain fewer observations than `steps`.\nreturn_all_indexes : bool, default=False\n    If True, all indices for the training and testing sets will be returned; otherwise, only start and end indices are provided.\nverbose : bool, default=True\n    Controls the printing of information regarding the generated folds.\n\nAttributes\n----------\nThis class inherits from BaseFold, acquiring its parameters and validation methods,\nadding specific functionality for handling time series data in forecasting scenarios.\"\"\"\n        super().__init__(steps=steps, initial_train_size=initial_train_size, window_size=window_size, differentiation=differentiation, refit=refit, fixed_train_size=fixed_train_size, gap=gap, skip_folds=skip_folds, allow_incomplete_fold=allow_incomplete_fold, return_all_indexes=return_all_indexes, verbose=verbose)",
        "docstring": "Initialize a TimeSeriesFold instance for splitting time series data into training\nand testing folds. This class allows for flexible configurations for cross-validation\nin time series modeling, where various parameters dictate the nature of the folds,\nincluding the size of the training and testing sets, the presence of differentiation,\nand options for refitting models.\n\nParameters\n----------\nsteps : int\n    The number of observations to predict in each fold, also referred to as the \n    forecast horizon or test size.\ninitial_train_size : Optional[int], default=None\n    The number of observations to use for the initial training set. If None, the forecaster\n    is not trained during the first fold.\nwindow_size : Optional[int], default=None\n    The number of observations required to generate autoregressive predictors.\ndifferentiation : Optional[int], default=None\n    The number of observations to extend the last window for differentiation.\nrefit : Union[bool, int], default=False\n    Determines if the forecaster should be refitted in each fold. Can be set to \n    True or a positive integer value.\nfixed_train_size : bool, default=True\n    Indicates whether the training size remains constant across folds.\ngap : int, default=0\n    The number of observations to omit between the end of the training set and the start of the test set.\nskip_folds : Optional[Union[int, list]], default=None\n    Specifies the number of folds to skip during the fold generation.\nallow_incomplete_fold : bool, default=True\n    Whether to permit the final fold to contain fewer observations than `steps`.\nreturn_all_indexes : bool, default=False\n    If True, all indices for the training and testing sets will be returned; otherwise, only start and end indices are provided.\nverbose : bool, default=True\n    Controls the printing of information regarding the generated folds.\n\nAttributes\n----------\nThis class inherits from BaseFold, acquiring its parameters and validation methods,\nadding specific functionality for handling time series data in forecasting scenarios.",
        "signature": "def __init__(self, steps: int, initial_train_size: Optional[int]=None, window_size: Optional[int]=None, differentiation: Optional[int]=None, refit: Union[bool, int]=False, fixed_train_size: bool=True, gap: int=0, skip_folds: Optional[Union[int, list]]=None, allow_incomplete_fold: bool=True, return_all_indexes: bool=False, verbose: bool=True) -> None:",
        "type": "Method",
        "class_signature": "class TimeSeriesFold(BaseFold):"
      },
      "TimeSeriesFold.split": {
        "code": "    def split(self, X: Union[pd.Series, pd.DataFrame, pd.Index, dict], as_pandas: bool=False) -> Union[list, pd.DataFrame]:\n        \"\"\"Split the time series data into train and test folds appropriate for time series forecasting. This method generates a series of indices that define the training and testing periods for model evaluation, considering factors such as initial training size, window size, differentiation, refitting strategy, and the potential for skipped folds. \n\nParameters\n----------\nX : Union[pd.Series, pd.DataFrame, pd.Index, dict]\n    The time series data or index to split, allowing various formats including pandas Series, DataFrame, Index, or dictionaries of Series.\nas_pandas : bool, default `False`\n    If True, the output is returned as a DataFrame, enabling easier interpretation of fold structures.\n\nReturns\n-------\nUnion[list, pd.DataFrame]\n    A list of lists where each inner list contains indices for training and testing sets, including:\n    - [train_start, train_end]: Indices for the training data.\n    - [last_window_start, last_window_end]: Indices for the most recent observation used as predictors.\n    - [test_start, test_end]: Indices for the validation/test data.\n    - [test_start_with_gap, test_end_with_gap]: Indices for the validation/test data including a gap from the training period.\n    - fit_forecaster: A boolean indicating whether the forecaster should be fitted in this fold.\n\n    If `as_pandas` is True, the result is a DataFrame with columns documenting the details of each fold.\n\nNotes\n-----\nKey class attributes used in the method include:\n- `self.initial_train_size`: The number of data points to use for initial training; must be at least the sum of `self.steps` (the forecast horizon) and `self.initial_train_size`.\n- `self.window_size`: The size of the autoregressive window used to create predictor variables.\n- `self.steps`: Represents the number of observations to forecast. \n\nAdditionally, the method considers various flags such as `self.refit` for model retraining, `self.gap` for spacing between training and testing periods, and `self.skip_folds` to determine if certain folds should be omitted from the output, affecting how folds are constructed.\"\"\"\n        \"\\n        Split the time series data into train and test folds.\\n\\n        Parameters\\n        ----------\\n        X : pandas Series, pandas DataFrame, pandas Index, dict\\n            Time series data or index to split.\\n        as_pandas : bool, default `False`\\n            If True, the folds are returned as a DataFrame. This is useful to visualize\\n            the folds in a more interpretable way.\\n\\n        Returns\\n        -------\\n        folds : list, pandas DataFrame\\n            A list of lists containing the indices (position) for for each fold. Each list\\n            contains 4 lists and a boolean with the following information:\\n\\n            - [train_start, train_end]: list with the start and end positions of the\\n            training set.\\n            - [last_window_start, last_window_end]: list with the start and end positions\\n            of the last window seen by the forecaster during training. The last window\\n            is used to generate the lags use as predictors. If `differentiation` is\\n            included, the interval is extended as many observations as the\\n            differentiation order. If the argument `window_size` is `None`, this list is\\n            empty.\\n            - [test_start, test_end]: list with the start and end positions of the test\\n            set. These are the observations used to evaluate the forecaster.\\n            - [test_start_with_gap, test_end_with_gap]: list with the start and end\\n            positions of the test set including the gap. The gap is the number of\\n            observations between the end of the training set and the start of the test\\n            set.\\n            - fit_forecaster: boolean indicating whether the forecaster should be fitted\\n            in this fold.\\n\\n            It is important to note that the returned values are the positions of the\\n            observations and not the actual values of the index, so they can be used to\\n            slice the data directly using iloc.\\n\\n            If `as_pandas` is `True`, the folds are returned as a DataFrame with the\\n            following columns: 'fold', 'train_start', 'train_end', 'last_window_start',\\n            'last_window_end', 'test_start', 'test_end', 'test_start_with_gap',\\n            'test_end_with_gap', 'fit_forecaster'.\\n\\n            Following the python convention, the start index is inclusive and the end\\n            index is exclusive. This means that the last index is not included in the\\n            slice.\\n\\n        \"\n        if not isinstance(X, (pd.Series, pd.DataFrame, pd.Index, dict)):\n            raise TypeError(f'X must be a pandas Series, DataFrame, Index or a dictionary. Got {type(X)}.')\n        if isinstance(self.window_size, pd.tseries.offsets.DateOffset):\n            first_valid_index = X.index[-1] - self.window_size\n            try:\n                window_size_idx_start = X.index.get_loc(first_valid_index)\n                window_size_idx_end = X.index.get_loc(X.index[-1])\n                self.window_size = window_size_idx_end - window_size_idx_start\n            except KeyError:\n                raise ValueError(f'The length of `X` ({len(X)}), must be greater than or equal to the window size ({self.window_size}). Try to decrease the size of the offset (forecaster.offset), or increase the size of `y`.')\n        if self.initial_train_size is None:\n            if self.window_size is None:\n                raise ValueError('To use split method when `initial_train_size` is None, `window_size` must be an integer greater than 0. Although no initial training is done and all data is used to evaluate the model, the first `window_size` observations are needed to create the initial predictors. Got `window_size` = None.')\n            if self.refit:\n                raise ValueError('`refit` is only allowed when `initial_train_size` is not `None`. Set `refit` to `False` if you want to use `initial_train_size = None`.')\n            externally_fitted = True\n            self.initial_train_size = self.window_size\n        else:\n            if self.window_size is None:\n                warnings.warn('Last window cannot be calculated because `window_size` is None.')\n            externally_fitted = False\n        index = self._extract_index(X)\n        idx = range(len(index))\n        folds = []\n        i = 0\n        last_fold_excluded = False\n        if len(index) < self.initial_train_size + self.steps:\n            raise ValueError(f'The time series must have at least `initial_train_size + steps` observations. Got {len(index)} observations.')\n        while self.initial_train_size + i * self.steps + self.gap < len(index):\n            if self.refit:\n                train_iloc_start = i * self.steps if self.fixed_train_size else 0\n                train_iloc_end = self.initial_train_size + i * self.steps\n                test_iloc_start = train_iloc_end\n            else:\n                train_iloc_start = 0\n                train_iloc_end = self.initial_train_size\n                test_iloc_start = self.initial_train_size + i * self.steps\n            if self.window_size is not None:\n                last_window_iloc_start = test_iloc_start - self.window_size\n            test_iloc_end = test_iloc_start + self.gap + self.steps\n            partitions = [idx[train_iloc_start:train_iloc_end], idx[last_window_iloc_start:test_iloc_start] if self.window_size is not None else [], idx[test_iloc_start:test_iloc_end], idx[test_iloc_start + self.gap:test_iloc_end]]\n            folds.append(partitions)\n            i += 1\n        if not self.allow_incomplete_fold and len(folds[-1][3]) < self.steps:\n            folds = folds[:-1]\n            last_fold_excluded = True\n        folds = [[partition if len(partition) > 0 else None for partition in fold] for fold in folds]\n        if self.refit == 0:\n            self.refit = False\n        if isinstance(self.refit, bool):\n            fit_forecaster = [self.refit] * len(folds)\n            fit_forecaster[0] = True\n        else:\n            fit_forecaster = [False] * len(folds)\n            for i in range(0, len(fit_forecaster), self.refit):\n                fit_forecaster[i] = True\n        for i in range(len(folds)):\n            folds[i].append(fit_forecaster[i])\n            if fit_forecaster[i] is False:\n                folds[i][0] = folds[i - 1][0]\n        index_to_skip = []\n        if self.skip_folds is not None:\n            if isinstance(self.skip_folds, (int, np.integer)) and self.skip_folds > 0:\n                index_to_keep = np.arange(0, len(folds), self.skip_folds)\n                index_to_skip = np.setdiff1d(np.arange(0, len(folds)), index_to_keep, assume_unique=True)\n                index_to_skip = [int(x) for x in index_to_skip]\n            if isinstance(self.skip_folds, list):\n                index_to_skip = [i for i in self.skip_folds if i < len(folds)]\n        if self.verbose:\n            self._print_info(index=index, folds=folds, externally_fitted=externally_fitted, last_fold_excluded=last_fold_excluded, index_to_skip=index_to_skip)\n        folds = [fold for i, fold in enumerate(folds) if i not in index_to_skip]\n        if not self.return_all_indexes:\n            folds = [[[fold[0][0], fold[0][-1] + 1], [fold[1][0], fold[1][-1] + 1] if self.window_size is not None else [], [fold[2][0], fold[2][-1] + 1], [fold[3][0], fold[3][-1] + 1], fold[4]] for fold in folds]\n        if externally_fitted:\n            self.initial_train_size = None\n            folds[0][4] = False\n        if as_pandas:\n            if self.window_size is None:\n                for fold in folds:\n                    fold[1] = [None, None]\n            if not self.return_all_indexes:\n                folds = pd.DataFrame(data=[list(itertools.chain(*fold[:-1])) + [fold[-1]] for fold in folds], columns=['train_start', 'train_end', 'last_window_start', 'last_window_end', 'test_start', 'test_end', 'test_start_with_gap', 'test_end_with_gap', 'fit_forecaster'])\n            else:\n                folds = pd.DataFrame(data=folds, columns=['train_index', 'last_window_index', 'test_index', 'test_index_with_gap', 'fit_forecaster'])\n            folds.insert(0, 'fold', range(len(folds)))\n        return folds",
        "docstring": "Split the time series data into train and test folds appropriate for time series forecasting. This method generates a series of indices that define the training and testing periods for model evaluation, considering factors such as initial training size, window size, differentiation, refitting strategy, and the potential for skipped folds. \n\nParameters\n----------\nX : Union[pd.Series, pd.DataFrame, pd.Index, dict]\n    The time series data or index to split, allowing various formats including pandas Series, DataFrame, Index, or dictionaries of Series.\nas_pandas : bool, default `False`\n    If True, the output is returned as a DataFrame, enabling easier interpretation of fold structures.\n\nReturns\n-------\nUnion[list, pd.DataFrame]\n    A list of lists where each inner list contains indices for training and testing sets, including:\n    - [train_start, train_end]: Indices for the training data.\n    - [last_window_start, last_window_end]: Indices for the most recent observation used as predictors.\n    - [test_start, test_end]: Indices for the validation/test data.\n    - [test_start_with_gap, test_end_with_gap]: Indices for the validation/test data including a gap from the training period.\n    - fit_forecaster: A boolean indicating whether the forecaster should be fitted in this fold.\n\n    If `as_pandas` is True, the result is a DataFrame with columns documenting the details of each fold.\n\nNotes\n-----\nKey class attributes used in the method include:\n- `self.initial_train_size`: The number of data points to use for initial training; must be at least the sum of `self.steps` (the forecast horizon) and `self.initial_train_size`.\n- `self.window_size`: The size of the autoregressive window used to create predictor variables.\n- `self.steps`: Represents the number of observations to forecast. \n\nAdditionally, the method considers various flags such as `self.refit` for model retraining, `self.gap` for spacing between training and testing periods, and `self.skip_folds` to determine if certain folds should be omitted from the output, affecting how folds are constructed.",
        "signature": "def split(self, X: Union[pd.Series, pd.DataFrame, pd.Index, dict], as_pandas: bool=False) -> Union[list, pd.DataFrame]:",
        "type": "Method",
        "class_signature": "class TimeSeriesFold(BaseFold):"
      }
    },
    "skforecast/model_selection/_utils.py": {
      "check_backtesting_input": {
        "code": "def check_backtesting_input(forecaster: object, cv: object, metric: Union[str, Callable, list], add_aggregated_metric: bool=True, y: Optional[pd.Series]=None, series: Optional[Union[pd.DataFrame, dict]]=None, exog: Optional[Union[pd.Series, pd.DataFrame, dict]]=None, interval: Optional[list]=None, alpha: Optional[float]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False, n_jobs: Union[int, str]='auto', show_progress: bool=True, suppress_warnings: bool=False, suppress_warnings_fit: bool=False) -> None:\n    \"\"\"Check the validity of input parameters for backtesting functions in the `skforecast` model selection module. This function ensures that the provided forecaster, cross-validation object, metrics, and additional parameters conform to expected formats and types. It performs various validation checks depending on the type of forecaster (uni-series or multi-series) and may raise errors if the inputs are inconsistent or incorrect.\n\nParameters\n----------\nforecaster : object\n    The forecaster model, which can be of types like `ForecasterRecursive`, `ForecasterDirect`, etc.\ncv : object\n    The cross-validation object, specifically of type `TimeSeriesFold`, which contains functions to split data into training and testing sets.\nmetric : Union[str, Callable, list]\n    The metric(s) used to evaluate model performance, can be a string name, a callable function, or a list of multiple metrics.\nadd_aggregated_metric : bool, default `True`\n    Indicates whether aggregated metrics over all levels should be included in the results, applicable for multi-series forecasters.\ny : Optional[pd.Series], default `None`\n    Training time series for uni-series forecasters; should be a pandas Series.\nseries : Optional[Union[pd.DataFrame, dict]], default `None`\n    Training time series for multi-series forecasters; can be a pandas DataFrame or a dictionary of Series.\nexog : Optional[Union[pd.Series, pd.DataFrame, dict]], default `None`\n    Exogenous variables, can also be in various types similar to series.\ninterval : Optional[list], default `None`\n    List of percentiles for confidence interval estimates, must be between 0 and 100.\nalpha : Optional[float], default `None`\n    Confidence level used in `ForecasterSarimax`.\nn_boot : int, default `250`\n    Number of bootstrapping iterations to estimate prediction intervals.\nrandom_state : int, default `123`\n    Seed for random number generation, ensuring deterministic results.\nuse_in_sample_residuals : bool, default `True`\n    Indicates whether to use training data residuals for predictions.\nuse_binned_residuals : bool, default `False`\n    Indicates if residuals should be conditioned on predicted values in bootstrapping.\nn_jobs : Union[int, str], default `'auto'`\n    Number of parallel jobs for computation; if 'auto', automatically determines based on system capabilities.\nshow_progress : bool, default `True`\n    If True, displays a progress bar during execution.\nsuppress_warnings : bool, default `False`\n    If True, suppresses warnings from the `skforecast` library.\nsuppress_warnings_fit : bool, default `False`\n    If True, ignores fitting warnings from specific forecasters.\n\nReturns\n-------\nNone\n    This function does not return a value but raises exceptions for invalid inputs.\n\nRaises\n------\nTypeError\n    Raised if the provided types of any parameters do not match expected types.\nValueError\n    Raised for logical inconsistencies in the parameters and settings.\nNotFittedError\n    Raised if the forecaster requires training before certain operations.\n\nThis function interacts with constants and functions defined in the module to perform validations, ensuring the compatibility and correctness of inputs across different forecaster types.\"\"\"\n    \"\\n    This is a helper function to check most inputs of backtesting functions in \\n    modules `model_selection`.\\n\\n    Parameters\\n    ----------\\n    forecaster : Forecaster\\n        Forecaster model.\\n    cv : TimeSeriesFold\\n        TimeSeriesFold object with the information needed to split the data into folds.\\n    metric : str, Callable, list\\n        Metric used to quantify the goodness of fit of the model.\\n    add_aggregated_metric : bool, default `True`\\n        If `True`, the aggregated metrics (average, weighted average and pooling)\\n        over all levels are also returned (only multiseries).\\n    y : pandas Series, default `None`\\n        Training time series for uni-series forecasters.\\n    series : pandas DataFrame, dict, default `None`\\n        Training time series for multi-series forecasters.\\n    exog : pandas Series, pandas DataFrame, dict, default `None`\\n        Exogenous variables.\\n    interval : list, default `None`\\n        Confidence of the prediction interval estimated. Sequence of percentiles\\n        to compute, which must be between 0 and 100 inclusive.\\n    alpha : float, default `None`\\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %. \\n    n_boot : int, default `250`\\n        Number of bootstrapping iterations used to estimate prediction\\n        intervals.\\n    random_state : int, default `123`\\n        Sets a seed to the random generator, so that boot intervals are always \\n        deterministic.\\n    use_in_sample_residuals : bool, default `True`\\n        If `True`, residuals from the training data are used as proxy of prediction \\n        error to create prediction intervals.  If `False`, out_sample_residuals \\n        are used if they are already stored inside the forecaster.\\n    use_binned_residuals : bool, default `False`\\n        If `True`, residuals used in each bootstrapping iteration are selected\\n        conditioning on the predicted values. If `False`, residuals are selected\\n        randomly without conditioning on the predicted values.\\n    n_jobs : int, 'auto', default `'auto'`\\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \\n        set to the number of cores. If 'auto', `n_jobs` is set using the fuction\\n        skforecast.utils.select_n_jobs_fit_forecaster.\\n        **New in version 0.9.0**\\n    show_progress : bool, default `True`\\n        Whether to show a progress bar.\\n    suppress_warnings: bool, default `False`\\n        If `True`, skforecast warnings will be suppressed during the backtesting \\n        process. See skforecast.exceptions.warn_skforecast_categories for more\\n        information.\\n    suppress_warnings_fit : bool, default `False`\\n        If `True`, warnings generated during fitting will be ignored. Only \\n        `ForecasterSarimax`.\\n\\n    Returns\\n    -------\\n    None\\n    \\n    \"\n    forecaster_name = type(forecaster).__name__\n    cv_name = type(cv).__name__\n    if cv_name != 'TimeSeriesFold':\n        raise TypeError(f'`cv` must be a TimeSeriesFold object. Got {cv_name}.')\n    steps = cv.steps\n    initial_train_size = cv.initial_train_size\n    gap = cv.gap\n    allow_incomplete_fold = cv.allow_incomplete_fold\n    refit = cv.refit\n    forecasters_uni = ['ForecasterRecursive', 'ForecasterDirect', 'ForecasterSarimax', 'ForecasterEquivalentDate']\n    forecasters_multi = ['ForecasterDirectMultiVariate', 'ForecasterRnn']\n    forecasters_multi_dict = ['ForecasterRecursiveMultiSeries']\n    if forecaster_name in forecasters_uni:\n        if not isinstance(y, pd.Series):\n            raise TypeError('`y` must be a pandas Series.')\n        data_name = 'y'\n        data_length = len(y)\n    elif forecaster_name in forecasters_multi:\n        if not isinstance(series, pd.DataFrame):\n            raise TypeError('`series` must be a pandas DataFrame.')\n        data_name = 'series'\n        data_length = len(series)\n    elif forecaster_name in forecasters_multi_dict:\n        if not isinstance(series, (pd.DataFrame, dict)):\n            raise TypeError(f'`series` must be a pandas DataFrame or a dict of DataFrames or Series. Got {type(series)}.')\n        data_name = 'series'\n        if isinstance(series, dict):\n            not_valid_series = [k for k, v in series.items() if not isinstance(v, (pd.Series, pd.DataFrame))]\n            if not_valid_series:\n                raise TypeError(f'If `series` is a dictionary, all series must be a named pandas Series or a pandas DataFrame with a single column. Review series: {not_valid_series}')\n            not_valid_index = [k for k, v in series.items() if not isinstance(v.index, pd.DatetimeIndex)]\n            if not_valid_index:\n                raise ValueError(f'If `series` is a dictionary, all series must have a Pandas DatetimeIndex as index with the same frequency. Review series: {not_valid_index}')\n            indexes_freq = [f'{v.index.freq}' for v in series.values()]\n            indexes_freq = sorted(set(indexes_freq))\n            if not len(indexes_freq) == 1:\n                raise ValueError(f'If `series` is a dictionary, all series must have a Pandas DatetimeIndex as index with the same frequency. Found frequencies: {indexes_freq}')\n            data_length = max([len(series[serie]) for serie in series])\n        else:\n            data_length = len(series)\n    if exog is not None:\n        if forecaster_name in forecasters_multi_dict:\n            if not isinstance(exog, (pd.Series, pd.DataFrame, dict)):\n                raise TypeError(f'`exog` must be a pandas Series, DataFrame, dictionary of pandas Series/DataFrames or None. Got {type(exog)}.')\n            if isinstance(exog, dict):\n                not_valid_exog = [k for k, v in exog.items() if not isinstance(v, (pd.Series, pd.DataFrame, type(None)))]\n                if not_valid_exog:\n                    raise TypeError(f'If `exog` is a dictionary, All exog must be a named pandas Series, a pandas DataFrame or None. Review exog: {not_valid_exog}')\n        elif not isinstance(exog, (pd.Series, pd.DataFrame)):\n            raise TypeError(f'`exog` must be a pandas Series, DataFrame or None. Got {type(exog)}.')\n    if hasattr(forecaster, 'differentiation'):\n        if forecaster.differentiation != cv.differentiation:\n            raise ValueError(f'The differentiation included in the forecaster ({forecaster.differentiation}) differs from the differentiation included in the cv ({cv.differentiation}). Set the same value for both using the `differentiation` argument.')\n    if not isinstance(metric, (str, Callable, list)):\n        raise TypeError(f'`metric` must be a string, a callable function, or a list containing multiple strings and/or callables. Got {type(metric)}.')\n    if forecaster_name == 'ForecasterEquivalentDate' and isinstance(forecaster.offset, pd.tseries.offsets.DateOffset):\n        if initial_train_size is None:\n            raise ValueError(f'`initial_train_size` must be an integer greater than the `window_size` of the forecaster ({forecaster.window_size}) and smaller than the length of `{data_name}` ({data_length}).')\n    elif initial_train_size is not None:\n        if initial_train_size < forecaster.window_size or initial_train_size >= data_length:\n            raise ValueError(f'If used, `initial_train_size` must be an integer greater than the `window_size` of the forecaster ({forecaster.window_size}) and smaller than the length of `{data_name}` ({data_length}).')\n        if initial_train_size + gap >= data_length:\n            raise ValueError(f'The combination of initial_train_size {initial_train_size} and gap {gap} cannot be greater than the length of `{data_name}` ({data_length}).')\n    elif forecaster_name in ['ForecasterSarimax', 'ForecasterEquivalentDate']:\n        raise ValueError(f'`initial_train_size` must be an integer smaller than the length of `{data_name}` ({data_length}).')\n    else:\n        if not forecaster.is_fitted:\n            raise NotFittedError('`forecaster` must be already trained if no `initial_train_size` is provided.')\n        if refit:\n            raise ValueError('`refit` is only allowed when `initial_train_size` is not `None`.')\n    if forecaster_name == 'ForecasterSarimax' and cv.skip_folds is not None:\n        raise ValueError('`skip_folds` is not allowed for ForecasterSarimax. Set it to `None`.')\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError('`add_aggregated_metric` must be a boolean: `True`, `False`.')\n    if not isinstance(n_boot, (int, np.integer)) or n_boot < 0:\n        raise TypeError(f'`n_boot` must be an integer greater than 0. Got {n_boot}.')\n    if not isinstance(random_state, (int, np.integer)) or random_state < 0:\n        raise TypeError(f'`random_state` must be an integer greater than 0. Got {random_state}.')\n    if not isinstance(use_in_sample_residuals, bool):\n        raise TypeError('`use_in_sample_residuals` must be a boolean: `True`, `False`.')\n    if not isinstance(use_binned_residuals, bool):\n        raise TypeError('`use_binned_residuals` must be a boolean: `True`, `False`.')\n    if not isinstance(n_jobs, int) and n_jobs != 'auto':\n        raise TypeError(f\"`n_jobs` must be an integer or `'auto'`. Got {n_jobs}.\")\n    if not isinstance(show_progress, bool):\n        raise TypeError('`show_progress` must be a boolean: `True`, `False`.')\n    if not isinstance(suppress_warnings, bool):\n        raise TypeError('`suppress_warnings` must be a boolean: `True`, `False`.')\n    if not isinstance(suppress_warnings_fit, bool):\n        raise TypeError('`suppress_warnings_fit` must be a boolean: `True`, `False`.')\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n    if not allow_incomplete_fold and data_length - (initial_train_size + gap) < steps:\n        raise ValueError(f'There is not enough data to evaluate {steps} steps in a single fold. Set `allow_incomplete_fold` to `True` to allow incomplete folds.\\n    Data available for test : {data_length - (initial_train_size + gap)}\\n    Steps                   : {steps}')",
        "docstring": "Check the validity of input parameters for backtesting functions in the `skforecast` model selection module. This function ensures that the provided forecaster, cross-validation object, metrics, and additional parameters conform to expected formats and types. It performs various validation checks depending on the type of forecaster (uni-series or multi-series) and may raise errors if the inputs are inconsistent or incorrect.\n\nParameters\n----------\nforecaster : object\n    The forecaster model, which can be of types like `ForecasterRecursive`, `ForecasterDirect`, etc.\ncv : object\n    The cross-validation object, specifically of type `TimeSeriesFold`, which contains functions to split data into training and testing sets.\nmetric : Union[str, Callable, list]\n    The metric(s) used to evaluate model performance, can be a string name, a callable function, or a list of multiple metrics.\nadd_aggregated_metric : bool, default `True`\n    Indicates whether aggregated metrics over all levels should be included in the results, applicable for multi-series forecasters.\ny : Optional[pd.Series], default `None`\n    Training time series for uni-series forecasters; should be a pandas Series.\nseries : Optional[Union[pd.DataFrame, dict]], default `None`\n    Training time series for multi-series forecasters; can be a pandas DataFrame or a dictionary of Series.\nexog : Optional[Union[pd.Series, pd.DataFrame, dict]], default `None`\n    Exogenous variables, can also be in various types similar to series.\ninterval : Optional[list], default `None`\n    List of percentiles for confidence interval estimates, must be between 0 and 100.\nalpha : Optional[float], default `None`\n    Confidence level used in `ForecasterSarimax`.\nn_boot : int, default `250`\n    Number of bootstrapping iterations to estimate prediction intervals.\nrandom_state : int, default `123`\n    Seed for random number generation, ensuring deterministic results.\nuse_in_sample_residuals : bool, default `True`\n    Indicates whether to use training data residuals for predictions.\nuse_binned_residuals : bool, default `False`\n    Indicates if residuals should be conditioned on predicted values in bootstrapping.\nn_jobs : Union[int, str], default `'auto'`\n    Number of parallel jobs for computation; if 'auto', automatically determines based on system capabilities.\nshow_progress : bool, default `True`\n    If True, displays a progress bar during execution.\nsuppress_warnings : bool, default `False`\n    If True, suppresses warnings from the `skforecast` library.\nsuppress_warnings_fit : bool, default `False`\n    If True, ignores fitting warnings from specific forecasters.\n\nReturns\n-------\nNone\n    This function does not return a value but raises exceptions for invalid inputs.\n\nRaises\n------\nTypeError\n    Raised if the provided types of any parameters do not match expected types.\nValueError\n    Raised for logical inconsistencies in the parameters and settings.\nNotFittedError\n    Raised if the forecaster requires training before certain operations.\n\nThis function interacts with constants and functions defined in the module to perform validations, ensuring the compatibility and correctness of inputs across different forecaster types.",
        "signature": "def check_backtesting_input(forecaster: object, cv: object, metric: Union[str, Callable, list], add_aggregated_metric: bool=True, y: Optional[pd.Series]=None, series: Optional[Union[pd.DataFrame, dict]]=None, exog: Optional[Union[pd.Series, pd.DataFrame, dict]]=None, interval: Optional[list]=None, alpha: Optional[float]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False, n_jobs: Union[int, str]='auto', show_progress: bool=True, suppress_warnings: bool=False, suppress_warnings_fit: bool=False) -> None:",
        "type": "Function",
        "class_signature": null
      },
      "select_n_jobs_backtesting": {
        "code": "def select_n_jobs_backtesting(forecaster: object, refit: Union[bool, int]) -> int:\n    \"\"\"Select the optimal number of jobs for parallel processing in the backtesting process of time series forecasting. This function uses heuristics based on the type of forecaster and its regressor to determine the number of jobs to run in parallel, which can enhance performance during model training and evaluation.\n\nParameters\n----------\nforecaster : object\n    A forecasting model that implements the necessary methods for training and prediction, such as those found in the `Forecaster` classes.\nrefit : bool, int\n    Indicates whether the forecaster is refitted during backtesting; if `refit` is an integer, the function will set `n_jobs` to 1.\n\nReturns\n-------\nn_jobs : int\n    The number of jobs to run in parallel during the backtesting process.\n\nNotes\n-----\nThe function utilizes the `cpu_count()` from the `joblib` library to determine the number of available cores, and `Pipeline` from `sklearn` is considered to extract the regressor type. It checks if the regressor is a linear model to decide on parallelism efficiency, particularly for the `ForecasterRecursive` model. Additionally, it handles custom regressors like `LGBMRegressor` for optimal job settings during model fitting. The function also relies on constants defined in the sklearn library related to regression models.\"\"\"\n    \"\\n    Select the optimal number of jobs to use in the backtesting process. This\\n    selection is based on heuristics and is not guaranteed to be optimal.\\n\\n    The number of jobs is chosen as follows:\\n\\n    - If `refit` is an integer, then `n_jobs = 1`. This is because parallelization doesn't \\n    work with intermittent refit.\\n    - If forecaster is 'ForecasterRecursive' and regressor is a linear regressor, \\n    then `n_jobs = 1`.\\n    - If forecaster is 'ForecasterRecursive' and regressor is not a linear \\n    regressor then `n_jobs = cpu_count() - 1`.\\n    - If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\\n    and `refit = True`, then `n_jobs = cpu_count() - 1`.\\n    - If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\\n    and `refit = False`, then `n_jobs = 1`.\\n    - If forecaster is 'ForecasterRecursiveMultiSeries', then `n_jobs = cpu_count() - 1`.\\n    - If forecaster is 'ForecasterSarimax' or 'ForecasterEquivalentDate', \\n    then `n_jobs = 1`.\\n    - If regressor is a `LGBMRegressor(n_jobs=1)`, then `n_jobs = cpu_count() - 1`.\\n    - If regressor is a `LGBMRegressor` with internal n_jobs != 1, then `n_jobs = 1`.\\n    This is because `lightgbm` is highly optimized for gradient boosting and\\n    parallelizes operations at a very fine-grained level, making additional\\n    parallelization unnecessary and potentially harmful due to resource contention.\\n\\n    Parameters\\n    ----------\\n    forecaster : Forecaster\\n        Forecaster model.\\n    refit : bool, int\\n        If the forecaster is refitted during the backtesting process.\\n\\n    Returns\\n    -------\\n    n_jobs : int\\n        The number of jobs to run in parallel.\\n    \\n    \"\n    forecaster_name = type(forecaster).__name__\n    if isinstance(forecaster.regressor, Pipeline):\n        regressor = forecaster.regressor[-1]\n        regressor_name = type(regressor).__name__\n    else:\n        regressor = forecaster.regressor\n        regressor_name = type(regressor).__name__\n    linear_regressors = [regressor_name for regressor_name in dir(sklearn.linear_model) if not regressor_name.startswith('_')]\n    refit = False if refit == 0 else refit\n    if not isinstance(refit, bool) and refit != 1:\n        n_jobs = 1\n    elif forecaster_name in ['ForecasterRecursive']:\n        if regressor_name in linear_regressors:\n            n_jobs = 1\n        elif regressor_name == 'LGBMRegressor':\n            n_jobs = cpu_count() - 1 if regressor.n_jobs == 1 else 1\n        else:\n            n_jobs = cpu_count() - 1\n    elif forecaster_name in ['ForecasterDirect', 'ForecasterDirectMultiVariate']:\n        n_jobs = 1\n    elif forecaster_name in ['ForecasterRecursiveMultiSeries']:\n        if regressor_name == 'LGBMRegressor':\n            n_jobs = cpu_count() - 1 if regressor.n_jobs == 1 else 1\n        else:\n            n_jobs = cpu_count() - 1\n    elif forecaster_name in ['ForecasterSarimax', 'ForecasterEquivalentDate']:\n        n_jobs = 1\n    else:\n        n_jobs = 1\n    return n_jobs",
        "docstring": "Select the optimal number of jobs for parallel processing in the backtesting process of time series forecasting. This function uses heuristics based on the type of forecaster and its regressor to determine the number of jobs to run in parallel, which can enhance performance during model training and evaluation.\n\nParameters\n----------\nforecaster : object\n    A forecasting model that implements the necessary methods for training and prediction, such as those found in the `Forecaster` classes.\nrefit : bool, int\n    Indicates whether the forecaster is refitted during backtesting; if `refit` is an integer, the function will set `n_jobs` to 1.\n\nReturns\n-------\nn_jobs : int\n    The number of jobs to run in parallel during the backtesting process.\n\nNotes\n-----\nThe function utilizes the `cpu_count()` from the `joblib` library to determine the number of available cores, and `Pipeline` from `sklearn` is considered to extract the regressor type. It checks if the regressor is a linear model to decide on parallelism efficiency, particularly for the `ForecasterRecursive` model. Additionally, it handles custom regressors like `LGBMRegressor` for optimal job settings during model fitting. The function also relies on constants defined in the sklearn library related to regression models.",
        "signature": "def select_n_jobs_backtesting(forecaster: object, refit: Union[bool, int]) -> int:",
        "type": "Function",
        "class_signature": null
      }
    },
    "skforecast/utils/utils.py": {
      "check_predict_input": {
        "code": "def check_predict_input(forecaster_name: str, steps: Union[int, list], is_fitted: bool, exog_in_: bool, index_type_: type, index_freq_: str, window_size: int, last_window: Union[pd.Series, pd.DataFrame, None], last_window_exog: Optional[Union[pd.Series, pd.DataFrame]]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, exog_type_in_: Optional[type]=None, exog_names_in_: Optional[list]=None, interval: Optional[list]=None, alpha: Optional[float]=None, max_steps: Optional[int]=None, levels: Optional[Union[str, list]]=None, levels_forecaster: Optional[Union[str, list]]=None, series_names_in_: Optional[list]=None, encoding: Optional[str]=None) -> None:\n    \"\"\"Check the inputs of the predict method for a forecaster to ensure they match the attributes of a trained forecaster. This function validates various parameters required for making predictions, such as the number of steps, exogenous variables, last window data, and levels to be predicted.\n\nParameters\n----------\nforecaster_name : str\n    The name of the forecaster being used, which determines the checks applied.\nsteps : int, list\n    The number of future steps to predict, either as an integer or a list of integers.\nis_fitted : bool\n    A flag indicating whether the forecaster has been trained.\nexog_in_ : bool\n    A flag indicating whether the forecaster was trained with exogenous variables.\nindex_type_ : type\n    The expected type of the index for the data used during training.\nindex_freq_ : str\n    The frequency of the index from the training data.\nwindow_size : int\n    The size of the window needed for creating lagged predictors, equal to `max_lag`.\nlast_window : pandas Series, pandas DataFrame, None\n    The most recent values used to generate predictors for the first prediction step.\nlast_window_exog : pandas Series, pandas DataFrame, optional\n    The exogenous variables aligned with the last_window for predictions.\nexog : pandas Series, pandas DataFrame, optional\n    Exogenous variable(s) used as predictors.\nexog_type_in_ : type, optional\n    The expected type of the exogenous variable(s) used during training.\nexog_names_in_ : list, optional\n    A list of the names of exogenous variables used during training.\ninterval : list, optional\n    A list indicating the confidence interval for the prediction.\nalpha : float, optional\n    A float indicating the alpha value for confidence intervals in ForecasterSarimax.\nmax_steps : int, optional\n    The maximum number of steps allowed for predictions in specific forecasters.\nlevels : str, list, optional\n    The names of the time series to be predicted.\nlevels_forecaster : str, list, optional\n    Time series used as output data in a multiseries problem for specific forecasters.\nseries_names_in_ : list, optional\n    The names of the series used during training.\nencoding : str, optional\n    The encoding method used to identify different series.\n\nReturns\n-------\nNone\n    Raises exceptions if any of the input validations fail, ensuring that the conditions for making predictions are met.\n\nThis function depends on other helper functions like `check_interval`, `preprocess_last_window`, and `expand_index`, which facilitate various checks and preprocessing steps. It also raises specific warnings and errors that guide the user in rectifying input issues based on the forecaster's configuration and training.\"\"\"\n    '\\n    Check all inputs of predict method. This is a helper function to validate\\n    that inputs used in predict method match attributes of a forecaster already\\n    trained.\\n\\n    Parameters\\n    ----------\\n    forecaster_name : str\\n        Forecaster name.\\n    steps : int, list\\n        Number of future steps predicted.\\n    is_fitted: bool\\n        Tag to identify if the regressor has been fitted (trained).\\n    exog_in_ : bool\\n        If the forecaster has been trained using exogenous variable/s.\\n    index_type_ : type\\n        Type of index of the input used in training.\\n    index_freq_ : str\\n        Frequency of Index of the input used in training.\\n    window_size: int\\n        Size of the window needed to create the predictors. It is equal to \\n        `max_lag`.\\n    last_window : pandas Series, pandas DataFrame, None\\n        Values of the series used to create the predictors (lags) need in the \\n        first iteration of prediction (t + 1).\\n    last_window_exog : pandas Series, pandas DataFrame, default `None`\\n        Values of the exogenous variables aligned with `last_window` in \\n        ForecasterSarimax predictions.\\n    exog : pandas Series, pandas DataFrame, default `None`\\n        Exogenous variable/s included as predictor/s.\\n    exog_type_in_ : type, default `None`\\n        Type of exogenous variable/s used in training.\\n    exog_names_in_ : list, default `None`\\n        Names of the exogenous variables used during training.\\n    interval : list, default `None`\\n        Confidence of the prediction interval estimated. Sequence of percentiles\\n        to compute, which must be between 0 and 100 inclusive. For example, \\n        interval of 95% should be as `interval = [2.5, 97.5]`.\\n    alpha : float, default `None`\\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %.\\n    max_steps: int, default `None`\\n        Maximum number of steps allowed (`ForecasterDirect` and \\n        `ForecasterDirectMultiVariate`).\\n    levels : str, list, default `None`\\n        Time series to be predicted (`ForecasterRecursiveMultiSeries`\\n        and `ForecasterRnn).\\n    levels_forecaster : str, list, default `None`\\n        Time series used as output data of a multiseries problem in a RNN problem\\n        (`ForecasterRnn`).\\n    series_names_in_ : list, default `None`\\n        Names of the columns used during fit (`ForecasterRecursiveMultiSeries`, \\n        `ForecasterDirectMultiVariate` and `ForecasterRnn`).\\n    encoding : str, default `None`\\n        Encoding used to identify the different series (`ForecasterRecursiveMultiSeries`).\\n\\n    Returns\\n    -------\\n    None\\n\\n    '\n    if not is_fitted:\n        raise NotFittedError('This Forecaster instance is not fitted yet. Call `fit` with appropriate arguments before using predict.')\n    if isinstance(steps, (int, np.integer)) and steps < 1:\n        raise ValueError(f'`steps` must be an integer greater than or equal to 1. Got {steps}.')\n    if isinstance(steps, list) and min(steps) < 1:\n        raise ValueError(f'The minimum value of `steps` must be equal to or greater than 1. Got {min(steps)}.')\n    if max_steps is not None:\n        if max(steps) > max_steps:\n            raise ValueError(f'The maximum value of `steps` must be less than or equal to the value of steps defined when initializing the forecaster. Got {max(steps)}, but the maximum is {max_steps}.')\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n    if forecaster_name in ['ForecasterRecursiveMultiSeries', 'ForecasterRnn']:\n        if not isinstance(levels, (type(None), str, list)):\n            raise TypeError('`levels` must be a `list` of column names, a `str` of a column name or `None`.')\n        levels_to_check = levels_forecaster if forecaster_name == 'ForecasterRnn' else series_names_in_\n        unknown_levels = set(levels) - set(levels_to_check)\n        if forecaster_name == 'ForecasterRnn':\n            if len(unknown_levels) != 0:\n                raise ValueError(f'`levels` names must be included in the series used during fit ({levels_to_check}). Got {levels}.')\n        elif len(unknown_levels) != 0 and last_window is not None and (encoding is not None):\n            if encoding == 'onehot':\n                warnings.warn(f'`levels` {unknown_levels} were not included in training. The resulting one-hot encoded columns for this feature will be all zeros.', UnknownLevelWarning)\n            else:\n                warnings.warn(f'`levels` {unknown_levels} were not included in training. Unknown levels are encoded as NaN, which may cause the prediction to fail if the regressor does not accept NaN values.', UnknownLevelWarning)\n    if exog is None and exog_in_:\n        raise ValueError('Forecaster trained with exogenous variable/s. Same variable/s must be provided when predicting.')\n    if exog is not None and (not exog_in_):\n        raise ValueError('Forecaster trained without exogenous variable/s. `exog` must be `None` when predicting.')\n    if isinstance(last_window, type(None)) and forecaster_name not in ['ForecasterRecursiveMultiSeries', 'ForecasterRnn']:\n        raise ValueError(\"`last_window` was not stored during training. If you don't want to retrain the Forecaster, provide `last_window` as argument.\")\n    if forecaster_name in ['ForecasterRecursiveMultiSeries', 'ForecasterDirectMultiVariate', 'ForecasterRnn']:\n        if not isinstance(last_window, pd.DataFrame):\n            raise TypeError(f'`last_window` must be a pandas DataFrame. Got {type(last_window)}.')\n        last_window_cols = last_window.columns.to_list()\n        if forecaster_name in ['ForecasterRecursiveMultiSeries', 'ForecasterRnn'] and len(set(levels) - set(last_window_cols)) != 0:\n            raise ValueError(f'`last_window` must contain a column(s) named as the level(s) to be predicted.\\n    `levels` : {levels}\\n    `last_window` columns : {last_window_cols}')\n        if forecaster_name == 'ForecasterDirectMultiVariate':\n            if len(set(series_names_in_) - set(last_window_cols)) > 0:\n                raise ValueError(f'`last_window` columns must be the same as the `series` column names used to create the X_train matrix.\\n    `last_window` columns    : {last_window_cols}\\n    `series` columns X train : {series_names_in_}')\n    elif not isinstance(last_window, (pd.Series, pd.DataFrame)):\n        raise TypeError(f'`last_window` must be a pandas Series or DataFrame. Got {type(last_window)}.')\n    if len(last_window) < window_size:\n        raise ValueError(f'`last_window` must have as many values as needed to generate the predictors. For this forecaster it is {window_size}.')\n    if last_window.isnull().any().all():\n        warnings.warn('`last_window` has missing values. Most of machine learning models do not allow missing values. Prediction method may fail.', MissingValuesWarning)\n    _, last_window_index = preprocess_last_window(last_window=last_window.iloc[:0], return_values=False)\n    if not isinstance(last_window_index, index_type_):\n        raise TypeError(f'Expected index of type {index_type_} for `last_window`. Got {type(last_window_index)}.')\n    if isinstance(last_window_index, pd.DatetimeIndex):\n        if not last_window_index.freqstr == index_freq_:\n            raise TypeError(f'Expected frequency of type {index_freq_} for `last_window`. Got {last_window_index.freqstr}.')\n    if exog is not None:\n        if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n            if not isinstance(exog, (pd.Series, pd.DataFrame, dict)):\n                raise TypeError(f'`exog` must be a pandas Series, DataFrame or dict. Got {type(exog)}.')\n            if exog_type_in_ == dict and (not isinstance(exog, dict)):\n                raise TypeError(f'Expected type for `exog`: {exog_type_in_}. Got {type(exog)}.')\n        elif not isinstance(exog, (pd.Series, pd.DataFrame)):\n            raise TypeError(f'`exog` must be a pandas Series or DataFrame. Got {type(exog)}.')\n        if isinstance(exog, dict):\n            no_exog_levels = set(levels) - set(exog.keys())\n            if no_exog_levels:\n                warnings.warn(f'`exog` does not contain keys for levels {no_exog_levels}. Missing levels are filled with NaN. Most of machine learning models do not allow missing values. Prediction method may fail.', MissingExogWarning)\n            exogs_to_check = [(f\"`exog` for series '{k}'\", v) for k, v in exog.items() if v is not None and k in levels]\n        else:\n            exogs_to_check = [('`exog`', exog)]\n        for exog_name, exog_to_check in exogs_to_check:\n            if not isinstance(exog_to_check, (pd.Series, pd.DataFrame)):\n                raise TypeError(f'{exog_name} must be a pandas Series or DataFrame. Got {type(exog_to_check)}')\n            if exog_to_check.isnull().any().any():\n                warnings.warn(f'{exog_name} has missing values. Most of machine learning models do not allow missing values. Prediction method may fail.', MissingValuesWarning)\n            last_step = max(steps) if isinstance(steps, list) else steps\n            if len(exog_to_check) < last_step:\n                if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n                    warnings.warn(f\"{exog_name} doesn't have as many values as steps predicted, {last_step}. Missing values are filled with NaN. Most of machine learning models do not allow missing values. Prediction method may fail.\", MissingValuesWarning)\n                else:\n                    raise ValueError(f'{exog_name} must have at least as many values as steps predicted, {last_step}.')\n            if isinstance(exog_to_check, pd.DataFrame):\n                col_missing = set(exog_names_in_).difference(set(exog_to_check.columns))\n                if col_missing:\n                    if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n                        warnings.warn(f'{col_missing} not present in {exog_name}. All values will be NaN.', MissingExogWarning)\n                    else:\n                        raise ValueError(f'Missing columns in {exog_name}. Expected {exog_names_in_}. Got {exog_to_check.columns.to_list()}.')\n            else:\n                if exog_to_check.name is None:\n                    raise ValueError(f'When {exog_name} is a pandas Series, it must have a name. Got None.')\n                if exog_to_check.name not in exog_names_in_:\n                    if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n                        warnings.warn(f\"'{exog_to_check.name}' was not observed during training. {exog_name} is ignored. Exogenous variables must be one of: {exog_names_in_}.\", IgnoredArgumentWarning)\n                    else:\n                        raise ValueError(f\"'{exog_to_check.name}' was not observed during training. Exogenous variables must be: {exog_names_in_}.\")\n            _, exog_index = preprocess_exog(exog=exog_to_check.iloc[:0,], return_values=False)\n            if not isinstance(exog_index, index_type_):\n                raise TypeError(f'Expected index of type {index_type_} for {exog_name}. Got {type(exog_index)}.')\n            if forecaster_name not in ['ForecasterRecursiveMultiSeries']:\n                if isinstance(exog_index, pd.DatetimeIndex):\n                    if not exog_index.freqstr == index_freq_:\n                        raise TypeError(f'Expected frequency of type {index_freq_} for {exog_name}. Got {exog_index.freqstr}.')\n            expected_index = expand_index(last_window.index, 1)[0]\n            if expected_index != exog_to_check.index[0]:\n                if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n                    warnings.warn(f'To make predictions {exog_name} must start one step ahead of `last_window`. Missing values are filled with NaN.\\n    `last_window` ends at : {last_window.index[-1]}.\\n    {exog_name} starts at : {exog_to_check.index[0]}.\\n     Expected index       : {expected_index}.', MissingValuesWarning)\n                else:\n                    raise ValueError(f'To make predictions {exog_name} must start one step ahead of `last_window`.\\n    `last_window` ends at : {last_window.index[-1]}.\\n    {exog_name} starts at : {exog_to_check.index[0]}.\\n     Expected index : {expected_index}.')\n    if forecaster_name == 'ForecasterSarimax':\n        if last_window_exog is not None:\n            if not exog_in_:\n                raise ValueError('Forecaster trained without exogenous variable/s. `last_window_exog` must be `None` when predicting.')\n            if not isinstance(last_window_exog, (pd.Series, pd.DataFrame)):\n                raise TypeError(f'`last_window_exog` must be a pandas Series or a pandas DataFrame. Got {type(last_window_exog)}.')\n            if len(last_window_exog) < window_size:\n                raise ValueError(f'`last_window_exog` must have as many values as needed to generate the predictors. For this forecaster it is {window_size}.')\n            if last_window_exog.isnull().any().all():\n                warnings.warn('`last_window_exog` has missing values. Most of machine learning models do not allow missing values. Prediction method may fail.', MissingValuesWarning)\n            _, last_window_exog_index = preprocess_last_window(last_window=last_window_exog.iloc[:0], return_values=False)\n            if not isinstance(last_window_exog_index, index_type_):\n                raise TypeError(f'Expected index of type {index_type_} for `last_window_exog`. Got {type(last_window_exog_index)}.')\n            if isinstance(last_window_exog_index, pd.DatetimeIndex):\n                if not last_window_exog_index.freqstr == index_freq_:\n                    raise TypeError(f'Expected frequency of type {index_freq_} for `last_window_exog`. Got {last_window_exog_index.freqstr}.')\n            if isinstance(last_window_exog, pd.DataFrame):\n                col_missing = set(exog_names_in_).difference(set(last_window_exog.columns))\n                if col_missing:\n                    raise ValueError(f'Missing columns in `last_window_exog`. Expected {exog_names_in_}. Got {last_window_exog.columns.to_list()}.')\n            else:\n                if last_window_exog.name is None:\n                    raise ValueError('When `last_window_exog` is a pandas Series, it must have a name. Got None.')\n                if last_window_exog.name not in exog_names_in_:\n                    raise ValueError(f\"'{last_window_exog.name}' was not observed during training. Exogenous variables must be: {exog_names_in_}.\")\n    return",
        "docstring": "Check the inputs of the predict method for a forecaster to ensure they match the attributes of a trained forecaster. This function validates various parameters required for making predictions, such as the number of steps, exogenous variables, last window data, and levels to be predicted.\n\nParameters\n----------\nforecaster_name : str\n    The name of the forecaster being used, which determines the checks applied.\nsteps : int, list\n    The number of future steps to predict, either as an integer or a list of integers.\nis_fitted : bool\n    A flag indicating whether the forecaster has been trained.\nexog_in_ : bool\n    A flag indicating whether the forecaster was trained with exogenous variables.\nindex_type_ : type\n    The expected type of the index for the data used during training.\nindex_freq_ : str\n    The frequency of the index from the training data.\nwindow_size : int\n    The size of the window needed for creating lagged predictors, equal to `max_lag`.\nlast_window : pandas Series, pandas DataFrame, None\n    The most recent values used to generate predictors for the first prediction step.\nlast_window_exog : pandas Series, pandas DataFrame, optional\n    The exogenous variables aligned with the last_window for predictions.\nexog : pandas Series, pandas DataFrame, optional\n    Exogenous variable(s) used as predictors.\nexog_type_in_ : type, optional\n    The expected type of the exogenous variable(s) used during training.\nexog_names_in_ : list, optional\n    A list of the names of exogenous variables used during training.\ninterval : list, optional\n    A list indicating the confidence interval for the prediction.\nalpha : float, optional\n    A float indicating the alpha value for confidence intervals in ForecasterSarimax.\nmax_steps : int, optional\n    The maximum number of steps allowed for predictions in specific forecasters.\nlevels : str, list, optional\n    The names of the time series to be predicted.\nlevels_forecaster : str, list, optional\n    Time series used as output data in a multiseries problem for specific forecasters.\nseries_names_in_ : list, optional\n    The names of the series used during training.\nencoding : str, optional\n    The encoding method used to identify different series.\n\nReturns\n-------\nNone\n    Raises exceptions if any of the input validations fail, ensuring that the conditions for making predictions are met.\n\nThis function depends on other helper functions like `check_interval`, `preprocess_last_window`, and `expand_index`, which facilitate various checks and preprocessing steps. It also raises specific warnings and errors that guide the user in rectifying input issues based on the forecaster's configuration and training.",
        "signature": "def check_predict_input(forecaster_name: str, steps: Union[int, list], is_fitted: bool, exog_in_: bool, index_type_: type, index_freq_: str, window_size: int, last_window: Union[pd.Series, pd.DataFrame, None], last_window_exog: Optional[Union[pd.Series, pd.DataFrame]]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, exog_type_in_: Optional[type]=None, exog_names_in_: Optional[list]=None, interval: Optional[list]=None, alpha: Optional[float]=None, max_steps: Optional[int]=None, levels: Optional[Union[str, list]]=None, levels_forecaster: Optional[Union[str, list]]=None, series_names_in_: Optional[list]=None, encoding: Optional[str]=None) -> None:",
        "type": "Function",
        "class_signature": null
      },
      "preprocess_y": {
        "code": "def preprocess_y(y: Union[pd.Series, pd.DataFrame], return_values: bool=True) -> Tuple[Union[None, np.ndarray], pd.Index]:\n    \"\"\"Preprocess the input time series `y` by returning its values and modifying its index. The function ensures that the index is of a proper type and handles various cases for `DatetimeIndex` and `RangeIndex`. \n\nParameters\n----------\ny : Union[pd.Series, pd.DataFrame]\n    The time series to preprocess, which can be either a pandas Series or a DataFrame.\nreturn_values : bool, default `True`\n    Indicates whether to return the values from `y` as a numpy ndarray.\n\nReturns\n-------\ny_values : Union[None, np.ndarray]\n    A numpy array containing the values of `y` if `return_values` is True; otherwise, None.\ny_index : pd.Index\n    The modified index of `y`, adjusted according to the rules outlined in the function description.\n\nNotes\n-----\n- If the index of `y` is a `DatetimeIndex` with a specified frequency, it remains unchanged. \n- If the index is a `DatetimeIndex` without a frequency or of a different type, it is replaced with a `RangeIndex`.\n- This function is useful for ensuring consistency in input data handling within forecasting tasks.\"\"\"\n    '\\n    Return values and index of series separately. Index is overwritten \\n    according to the next rules:\\n    \\n    - If index is of type `DatetimeIndex` and has frequency, nothing is \\n    changed.\\n    - If index is of type `RangeIndex`, nothing is changed.\\n    - If index is of type `DatetimeIndex` but has no frequency, a \\n    `RangeIndex` is created.\\n    - If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\\n    \\n    Parameters\\n    ----------\\n    y : pandas Series, pandas DataFrame\\n        Time series.\\n    return_values : bool, default `True`\\n        If `True` return the values of `y` as numpy ndarray. This option is \\n        intended to avoid copying data when it is not necessary.\\n\\n    Returns\\n    -------\\n    y_values : None, numpy ndarray\\n        Numpy array with values of `y`.\\n    y_index : pandas Index\\n        Index of `y` modified according to the rules.\\n    \\n    '\n    if isinstance(y.index, pd.DatetimeIndex) and y.index.freq is not None:\n        y_index = y.index\n    elif isinstance(y.index, pd.RangeIndex):\n        y_index = y.index\n    elif isinstance(y.index, pd.DatetimeIndex) and y.index.freq is None:\n        warnings.warn('Series has DatetimeIndex index but no frequency. Index is overwritten with a RangeIndex of step 1.')\n        y_index = pd.RangeIndex(start=0, stop=len(y), step=1)\n    else:\n        warnings.warn('Series has no DatetimeIndex nor RangeIndex index. Index is overwritten with a RangeIndex.')\n        y_index = pd.RangeIndex(start=0, stop=len(y), step=1)\n    y_values = y.to_numpy(copy=True).ravel() if return_values else None\n    return (y_values, y_index)",
        "docstring": "Preprocess the input time series `y` by returning its values and modifying its index. The function ensures that the index is of a proper type and handles various cases for `DatetimeIndex` and `RangeIndex`. \n\nParameters\n----------\ny : Union[pd.Series, pd.DataFrame]\n    The time series to preprocess, which can be either a pandas Series or a DataFrame.\nreturn_values : bool, default `True`\n    Indicates whether to return the values from `y` as a numpy ndarray.\n\nReturns\n-------\ny_values : Union[None, np.ndarray]\n    A numpy array containing the values of `y` if `return_values` is True; otherwise, None.\ny_index : pd.Index\n    The modified index of `y`, adjusted according to the rules outlined in the function description.\n\nNotes\n-----\n- If the index of `y` is a `DatetimeIndex` with a specified frequency, it remains unchanged. \n- If the index is a `DatetimeIndex` without a frequency or of a different type, it is replaced with a `RangeIndex`.\n- This function is useful for ensuring consistency in input data handling within forecasting tasks.",
        "signature": "def preprocess_y(y: Union[pd.Series, pd.DataFrame], return_values: bool=True) -> Tuple[Union[None, np.ndarray], pd.Index]:",
        "type": "Function",
        "class_signature": null
      },
      "preprocess_last_window": {
        "code": "def preprocess_last_window(last_window: Union[pd.Series, pd.DataFrame], return_values: bool=True) -> Tuple[np.ndarray, pd.Index]:\n    \"\"\"Preprocess the `last_window` series or DataFrame by extracting its values and index, while modifying the index based on predefined rules.\n\nParameters\n----------\nlast_window : pandas Series, pandas DataFrame\n    The input time series values, from which the values and index will be extracted.\nreturn_values : bool, default `True`\n    If `True`, the function returns the values of `last_window` as a numpy ndarray. If `False`, it does not return the values.\n\nReturns\n-------\nlast_window_values : numpy ndarray\n    A numpy array containing the values of `last_window`, if `return_values` is `True`.\nlast_window_index : pandas Index\n    The index of `last_window`, modified according to the following rules:\n    - If the index is a `DatetimeIndex` with frequency, it remains unchanged.\n    - If the index is a `RangeIndex`, it remains unchanged.\n    - If the index is a `DatetimeIndex` without frequency, it is replaced with a `RangeIndex`.\n    - If the index is of any other type, it is replaced with a `RangeIndex`.\n\nSide Effects\n-------------\nIssues warnings if the index is a `DatetimeIndex` without frequency or is of another type. This aids users in identifying potential issues with index types.\n\nThe function interacts with pandas and numpy libraries, utilizing their capabilities to handle dataframes, series, and ndarrays. It ensures the output is formatted correctly for further processing in forecasting or modeling tasks.\"\"\"\n    '\\n    Return values and index of series separately. Index is overwritten \\n    according to the next rules:\\n    \\n    - If index is of type `DatetimeIndex` and has frequency, nothing is \\n    changed.\\n    - If index is of type `RangeIndex`, nothing is changed.\\n    - If index is of type `DatetimeIndex` but has no frequency, a \\n    `RangeIndex` is created.\\n    - If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\\n    \\n    Parameters\\n    ----------\\n    last_window : pandas Series, pandas DataFrame\\n        Time series values.\\n    return_values : bool, default `True`\\n        If `True` return the values of `last_window` as numpy ndarray. This option \\n        is intended to avoid copying data when it is not necessary.\\n\\n    Returns\\n    -------\\n    last_window_values : numpy ndarray\\n        Numpy array with values of `last_window`.\\n    last_window_index : pandas Index\\n        Index of `last_window` modified according to the rules.\\n    \\n    '\n    if isinstance(last_window.index, pd.DatetimeIndex) and last_window.index.freq is not None:\n        last_window_index = last_window.index\n    elif isinstance(last_window.index, pd.RangeIndex):\n        last_window_index = last_window.index\n    elif isinstance(last_window.index, pd.DatetimeIndex) and last_window.index.freq is None:\n        warnings.warn('`last_window` has DatetimeIndex index but no frequency. Index is overwritten with a RangeIndex of step 1.')\n        last_window_index = pd.RangeIndex(start=0, stop=len(last_window), step=1)\n    else:\n        warnings.warn('`last_window` has no DatetimeIndex nor RangeIndex index. Index is overwritten with a RangeIndex.')\n        last_window_index = pd.RangeIndex(start=0, stop=len(last_window), step=1)\n    last_window_values = last_window.to_numpy(copy=True).ravel() if return_values else None\n    return (last_window_values, last_window_index)",
        "docstring": "Preprocess the `last_window` series or DataFrame by extracting its values and index, while modifying the index based on predefined rules.\n\nParameters\n----------\nlast_window : pandas Series, pandas DataFrame\n    The input time series values, from which the values and index will be extracted.\nreturn_values : bool, default `True`\n    If `True`, the function returns the values of `last_window` as a numpy ndarray. If `False`, it does not return the values.\n\nReturns\n-------\nlast_window_values : numpy ndarray\n    A numpy array containing the values of `last_window`, if `return_values` is `True`.\nlast_window_index : pandas Index\n    The index of `last_window`, modified according to the following rules:\n    - If the index is a `DatetimeIndex` with frequency, it remains unchanged.\n    - If the index is a `RangeIndex`, it remains unchanged.\n    - If the index is a `DatetimeIndex` without frequency, it is replaced with a `RangeIndex`.\n    - If the index is of any other type, it is replaced with a `RangeIndex`.\n\nSide Effects\n-------------\nIssues warnings if the index is a `DatetimeIndex` without frequency or is of another type. This aids users in identifying potential issues with index types.\n\nThe function interacts with pandas and numpy libraries, utilizing their capabilities to handle dataframes, series, and ndarrays. It ensures the output is formatted correctly for further processing in forecasting or modeling tasks.",
        "signature": "def preprocess_last_window(last_window: Union[pd.Series, pd.DataFrame], return_values: bool=True) -> Tuple[np.ndarray, pd.Index]:",
        "type": "Function",
        "class_signature": null
      },
      "expand_index": {
        "code": "def expand_index(index: Union[pd.Index, None], steps: int) -> pd.Index:\n    \"\"\"Create a new index of specified length starting from the end of the given index.\n\nParameters\n----------\nindex : pandas Index, None\n    The original index from which to generate the new index. It can be of type \n    `DatetimeIndex` or `RangeIndex`. If `None`, a default `RangeIndex` is created.\nsteps : int\n    The number of additional steps to add to the end of the index.\n\nReturns\n-------\nnew_index : pandas Index\n    A new index created based on the specified number of steps, starting after \n    the last entry of the provided index.\n\nRaises\n------\nTypeError\n    If `steps` is not an integer or if the `index` is not a valid pandas Index type.\n\nThe function interacts with various parts of the code by providing a mechanism to \ngenerate forecast indices for time series, especially when handling predictions \nin forecasting models. When using `DatetimeIndex`, it retains the frequency, \nensuring proper alignment of time series data during expansions.\"\"\"\n    '\\n    Create a new index of length `steps` starting at the end of the index.\\n    \\n    Parameters\\n    ----------\\n    index : pandas Index, None\\n        Original index.\\n    steps : int\\n        Number of steps to expand.\\n\\n    Returns\\n    -------\\n    new_index : pandas Index\\n        New index.\\n\\n    '\n    if not isinstance(steps, (int, np.integer)):\n        raise TypeError(f'`steps` must be an integer. Got {type(steps)}.')\n    if isinstance(index, pd.Index):\n        if isinstance(index, pd.DatetimeIndex):\n            new_index = pd.date_range(start=index[-1] + index.freq, periods=steps, freq=index.freq)\n        elif isinstance(index, pd.RangeIndex):\n            new_index = pd.RangeIndex(start=index[-1] + 1, stop=index[-1] + 1 + steps)\n        else:\n            raise TypeError('Argument `index` must be a pandas DatetimeIndex or RangeIndex.')\n    else:\n        new_index = pd.RangeIndex(start=0, stop=steps)\n    return new_index",
        "docstring": "Create a new index of specified length starting from the end of the given index.\n\nParameters\n----------\nindex : pandas Index, None\n    The original index from which to generate the new index. It can be of type \n    `DatetimeIndex` or `RangeIndex`. If `None`, a default `RangeIndex` is created.\nsteps : int\n    The number of additional steps to add to the end of the index.\n\nReturns\n-------\nnew_index : pandas Index\n    A new index created based on the specified number of steps, starting after \n    the last entry of the provided index.\n\nRaises\n------\nTypeError\n    If `steps` is not an integer or if the `index` is not a valid pandas Index type.\n\nThe function interacts with various parts of the code by providing a mechanism to \ngenerate forecast indices for time series, especially when handling predictions \nin forecasting models. When using `DatetimeIndex`, it retains the frequency, \nensuring proper alignment of time series data during expansions.",
        "signature": "def expand_index(index: Union[pd.Index, None], steps: int) -> pd.Index:",
        "type": "Function",
        "class_signature": null
      }
    },
    "skforecast/metrics/metrics.py": {
      "_get_metric": {
        "code": "def _get_metric(metric: str) -> Callable:\n    \"\"\"Get the corresponding scikit-learn function to calculate the specified metric for model evaluation.\n\nParameters\n----------\nmetric : str\n    The name of the metric to be used for quantifying the goodness of fit of the model. Allowed values include:\n    - \"mean_squared_error\"\n    - \"mean_absolute_error\"\n    - \"mean_absolute_percentage_error\"\n    - \"mean_squared_log_error\"\n    - \"mean_absolute_scaled_error\"\n    - \"root_mean_squared_scaled_error\"\n    - \"median_absolute_error\"\n\nReturns\n-------\nCallable\n    A function that computes the specified metric. This function is wrapped to include an additional `y_train` argument if necessary.\n\nRaises\n------\nValueError\n    If the provided metric name is not included in the list of allowed metrics.\n\nDependencies\n------------\nThis function interacts with the `add_y_train_argument` utility function to append the `y_train` parameter\nto the corresponding metric function when required. Additionally, it relies on several imported\nfunctions from the `sklearn.metrics` module to perform the actual metric calculations.\"\"\"\n    '\\n    Get the corresponding scikit-learn function to calculate the metric.\\n\\n    Parameters\\n    ----------\\n    metric : str\\n        Metric used to quantify the goodness of fit of the model.\\n\\n    Returns\\n    -------\\n    metric : Callable\\n        scikit-learn function to calculate the desired metric.\\n\\n    '\n    allowed_metrics = ['mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error', 'median_absolute_error']\n    if metric not in allowed_metrics:\n        raise ValueError(f'Allowed metrics are: {allowed_metrics}. Got {metric}.')\n    metrics = {'mean_squared_error': mean_squared_error, 'mean_absolute_error': mean_absolute_error, 'mean_absolute_percentage_error': mean_absolute_percentage_error, 'mean_squared_log_error': mean_squared_log_error, 'mean_absolute_scaled_error': mean_absolute_scaled_error, 'root_mean_squared_scaled_error': root_mean_squared_scaled_error, 'median_absolute_error': median_absolute_error}\n    metric = add_y_train_argument(metrics[metric])\n    return metric",
        "docstring": "Get the corresponding scikit-learn function to calculate the specified metric for model evaluation.\n\nParameters\n----------\nmetric : str\n    The name of the metric to be used for quantifying the goodness of fit of the model. Allowed values include:\n    - \"mean_squared_error\"\n    - \"mean_absolute_error\"\n    - \"mean_absolute_percentage_error\"\n    - \"mean_squared_log_error\"\n    - \"mean_absolute_scaled_error\"\n    - \"root_mean_squared_scaled_error\"\n    - \"median_absolute_error\"\n\nReturns\n-------\nCallable\n    A function that computes the specified metric. This function is wrapped to include an additional `y_train` argument if necessary.\n\nRaises\n------\nValueError\n    If the provided metric name is not included in the list of allowed metrics.\n\nDependencies\n------------\nThis function interacts with the `add_y_train_argument` utility function to append the `y_train` parameter\nto the corresponding metric function when required. Additionally, it relies on several imported\nfunctions from the `sklearn.metrics` module to perform the actual metric calculations.",
        "signature": "def _get_metric(metric: str) -> Callable:",
        "type": "Function",
        "class_signature": null
      },
      "add_y_train_argument": {
        "code": "def add_y_train_argument(func: Callable) -> Callable:\n    \"\"\"Add a `y_train` keyword-only argument to the given function if it is not already present in its signature. This allows for further customization when using the metric functions that require comparison with training data.\n\nParameters\n----------\nfunc : Callable\n    A callable function to which the `y_train` argument is added.\n\nReturns\n-------\nwrapper : Callable\n    A wrapped function with the added `y_train` argument, maintaining the original function\u2019s signature and behavior.\n\nNotes\n-----\nThe function uses `inspect.signature` to check the parameters of the provided function and alters its signature to include `y_train` as a keyword-only argument with a default value of `None`. The wrapper function preserves any additional positional and keyword arguments when calling the original function.\"\"\"\n    '\\n    Add `y_train` argument to a function if it is not already present.\\n\\n    Parameters\\n    ----------\\n    func : callable\\n        Function to which the argument is added.\\n\\n    Returns\\n    -------\\n    wrapper : callable\\n        Function with `y_train` argument added.\\n    \\n    '\n    sig = inspect.signature(func)\n    if 'y_train' in sig.parameters:\n        return func\n    new_params = list(sig.parameters.values()) + [inspect.Parameter('y_train', inspect.Parameter.KEYWORD_ONLY, default=None)]\n    new_sig = sig.replace(parameters=new_params)\n\n    @wraps(func)\n    def wrapper(*args, y_train=None, **kwargs):\n        \"\"\"Wrapper function that adds an optional `y_train` keyword argument to a specified metric function, ensuring compatibility with functions that require this additional data for calculations. It accepts any positional and keyword arguments that the original function does.\n\nParameters\n----------\n*args : \n    Positional arguments to pass to the original function.\ny_train : optional\n    A keyword-only argument representing the true values of the target variable in the training set. Defaults to None.\n**kwargs : \n    Additional keyword arguments to pass to the original function.\n\nReturns\n-------\nAny\n    The return value of the original function after invoking it with the provided arguments.\n\nThis wrapper interacts with the function it decorates by allowing the inclusion of `y_train`, which may be essential for certain metrics, such as mean absolute scaled error (MASE) and root mean squared scaled error (RMSSE). It leverages the `inspect` module to modify the function signature without altering the original function's behavior.\"\"\"\n        return func(*args, **kwargs)\n    wrapper.__signature__ = new_sig\n    return wrapper",
        "docstring": "Add a `y_train` keyword-only argument to the given function if it is not already present in its signature. This allows for further customization when using the metric functions that require comparison with training data.\n\nParameters\n----------\nfunc : Callable\n    A callable function to which the `y_train` argument is added.\n\nReturns\n-------\nwrapper : Callable\n    A wrapped function with the added `y_train` argument, maintaining the original function\u2019s signature and behavior.\n\nNotes\n-----\nThe function uses `inspect.signature` to check the parameters of the provided function and alters its signature to include `y_train` as a keyword-only argument with a default value of `None`. The wrapper function preserves any additional positional and keyword arguments when calling the original function.",
        "signature": "def add_y_train_argument(func: Callable) -> Callable:",
        "type": "Function",
        "class_signature": null
      },
      "wrapper": {
        "code": "    def wrapper(*args, y_train=None, **kwargs):\n        \"\"\"Wrapper function that adds an optional `y_train` keyword argument to a specified metric function, ensuring compatibility with functions that require this additional data for calculations. It accepts any positional and keyword arguments that the original function does.\n\nParameters\n----------\n*args : \n    Positional arguments to pass to the original function.\ny_train : optional\n    A keyword-only argument representing the true values of the target variable in the training set. Defaults to None.\n**kwargs : \n    Additional keyword arguments to pass to the original function.\n\nReturns\n-------\nAny\n    The return value of the original function after invoking it with the provided arguments.\n\nThis wrapper interacts with the function it decorates by allowing the inclusion of `y_train`, which may be essential for certain metrics, such as mean absolute scaled error (MASE) and root mean squared scaled error (RMSSE). It leverages the `inspect` module to modify the function signature without altering the original function's behavior.\"\"\"\n        return func(*args, **kwargs)",
        "docstring": "Wrapper function that adds an optional `y_train` keyword argument to a specified metric function, ensuring compatibility with functions that require this additional data for calculations. It accepts any positional and keyword arguments that the original function does.\n\nParameters\n----------\n*args : \n    Positional arguments to pass to the original function.\ny_train : optional\n    A keyword-only argument representing the true values of the target variable in the training set. Defaults to None.\n**kwargs : \n    Additional keyword arguments to pass to the original function.\n\nReturns\n-------\nAny\n    The return value of the original function after invoking it with the provided arguments.\n\nThis wrapper interacts with the function it decorates by allowing the inclusion of `y_train`, which may be essential for certain metrics, such as mean absolute scaled error (MASE) and root mean squared scaled error (RMSSE). It leverages the `inspect` module to modify the function signature without altering the original function's behavior.",
        "signature": "def wrapper(*args, y_train=None, **kwargs):",
        "type": "Function",
        "class_signature": null
      }
    },
    "skforecast/recursive/_forecaster_equivalent_date.py": {
      "ForecasterEquivalentDate.__init__": {
        "code": "    def __init__(self, offset: Union[int, pd.tseries.offsets.DateOffset], n_offsets: int=1, agg_func: Callable=np.mean, forecaster_id: Optional[Union[str, int]]=None) -> None:\n        \"\"\"Initializes the ForecasterEquivalentDate class, which predicts future values based on the most recent equivalent date, using a specified offset and aggregation function.\n\n    Parameters\n    ----------\n    offset : Union[int, pd.tseries.offsets.DateOffset]\n        Specifies how many steps (or valid dates) to move back in time to find the equivalent date for predictions.\n    n_offsets : int, default 1\n        Determines the number of equivalent dates to use for prediction; if greater than 1, the values are aggregated using the specified aggregation function.\n    agg_func : Callable, default np.mean\n        The function applied to aggregate the values of the equivalent dates when multiple offsets are used.\n    forecaster_id : Optional[Union[str, int]], default None\n        An identifier for the forecaster.\n\n    Attributes\n    ----------\n    offset : Holds the defined offset for equivalent date calculation.\n    n_offsets : Holds the number of equivalent dates to use in predictions.\n    agg_func : Stores the aggregation function.\n    last_window_ : Placeholder for the most recent data observed during training, initialized to None.\n    index_type_ : Stores the type of index used in the training data.\n    index_freq_ : Holds the frequency of the index from the training data.\n    training_range_ : Captures the first and last values of the index used during training.\n    creation_date : Timestamp of when the forecaster was created.\n    is_fitted : Boolean flag indicating if the forecaster has been trained.\n    fit_date : Timestamp of the last fitting.\n    skforecast_version : Version of the skforecast library being used.\n    python_version : Python version used for creating the forecaster.\n\n    Raises\n    ------\n    TypeError\n        If `offset` is not an integer or a pandas DateOffset.\"\"\"\n        self.offset = offset\n        self.n_offsets = n_offsets\n        self.agg_func = agg_func\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.creation_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.is_fitted = False\n        self.fit_date = None\n        self.skforecast_version = skforecast.__version__\n        self.python_version = sys.version.split(' ')[0]\n        self.forecaster_id = forecaster_id\n        self.regressor = None\n        self.differentiation = None\n        if not isinstance(self.offset, (int, pd.tseries.offsets.DateOffset)):\n            raise TypeError('`offset` must be an integer greater than 0 or a pandas.tseries.offsets. Find more information about offsets in https://pandas.pydata.org/docs/reference/offset_frequency.html')\n        self.window_size = self.offset * self.n_offsets",
        "docstring": "Initializes the ForecasterEquivalentDate class, which predicts future values based on the most recent equivalent date, using a specified offset and aggregation function.\n\nParameters\n----------\noffset : Union[int, pd.tseries.offsets.DateOffset]\n    Specifies how many steps (or valid dates) to move back in time to find the equivalent date for predictions.\nn_offsets : int, default 1\n    Determines the number of equivalent dates to use for prediction; if greater than 1, the values are aggregated using the specified aggregation function.\nagg_func : Callable, default np.mean\n    The function applied to aggregate the values of the equivalent dates when multiple offsets are used.\nforecaster_id : Optional[Union[str, int]], default None\n    An identifier for the forecaster.\n\nAttributes\n----------\noffset : Holds the defined offset for equivalent date calculation.\nn_offsets : Holds the number of equivalent dates to use in predictions.\nagg_func : Stores the aggregation function.\nlast_window_ : Placeholder for the most recent data observed during training, initialized to None.\nindex_type_ : Stores the type of index used in the training data.\nindex_freq_ : Holds the frequency of the index from the training data.\ntraining_range_ : Captures the first and last values of the index used during training.\ncreation_date : Timestamp of when the forecaster was created.\nis_fitted : Boolean flag indicating if the forecaster has been trained.\nfit_date : Timestamp of the last fitting.\nskforecast_version : Version of the skforecast library being used.\npython_version : Python version used for creating the forecaster.\n\nRaises\n------\nTypeError\n    If `offset` is not an integer or a pandas DateOffset.",
        "signature": "def __init__(self, offset: Union[int, pd.tseries.offsets.DateOffset], n_offsets: int=1, agg_func: Callable=np.mean, forecaster_id: Optional[Union[str, int]]=None) -> None:",
        "type": "Method",
        "class_signature": "class ForecasterEquivalentDate:"
      },
      "ForecasterEquivalentDate.fit": {
        "code": "    def fit(self, y: pd.Series, exog: Any=None, store_in_sample_residuals: Any=None) -> None:\n        \"\"\"Fit the ForecasterEquivalentDate model to the provided training time series.\n\nParameters\n----------\ny : pandas Series\n    A time series used for training the forecaster. The index must be of type pandas DatetimeIndex if `offset` is a pandas DateOffset, and it must have a defined frequency.\nexog : Ignored\n    Not utilized within this method, included for API consistency.\nstore_in_sample_residuals : Ignored\n    Not utilized within this method, included for API consistency.\n\nReturns\n-------\nNone\n\nRaises\n------\nTypeError\n    If `y.index` is not a pandas DatetimeIndex (when using a pandas DateOffset) or if the index does not have a frequency.\n\nValueError\n    If the length of `y` is less than the computed `window_size`, indicating insufficient data for the defined offsets.\n\nNotes\n-----\n`self.offset` defines how far back in the time series to look for equivalent dates, and `self.n_offsets` specifies how many past equivalent dates to aggregate for predictions. If the `offset` is an integer, it represents a count of steps (e.g., days), whereas if it is a pandas DateOffset, it represents a time duration (e.g., business days).\n\nThis method resets relevant internal state variables before fitting, including `self.last_window_`, `self.index_type_`, `self.index_freq_`, and `self.training_range_`. After fitting, it stores the input series for later predictions and updates the fitted status and timestamps.\"\"\"\n        '\\n        Training Forecaster.\\n        \\n        Parameters\\n        ----------\\n        y : pandas Series\\n            Training time series.\\n        exog : Ignored\\n            Not used, present here for API consistency by convention.\\n        store_in_sample_residuals : Ignored\\n            Not used, present here for API consistency by convention.\\n        \\n        Returns\\n        -------\\n        None\\n        \\n        '\n        if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n            if not isinstance(y.index, pd.DatetimeIndex):\n                raise TypeError('If `offset` is a pandas DateOffset, the index of `y` must be a pandas DatetimeIndex with frequency.')\n            elif y.index.freq is None:\n                raise TypeError('If `offset` is a pandas DateOffset, the index of `y` must be a pandas DatetimeIndex with frequency.')\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.is_fitted = False\n        _, y_index = preprocess_y(y=y, return_values=False)\n        if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n            first_valid_index = y_index[-1] - self.offset * self.n_offsets\n            try:\n                window_size_idx_start = y_index.get_loc(first_valid_index)\n                window_size_idx_end = y_index.get_loc(y_index[-1])\n                self.window_size = window_size_idx_end - window_size_idx_start\n            except KeyError:\n                raise ValueError(f'The length of `y` ({len(y)}), must be greater than or equal to the window size ({self.window_size}). This is because  the offset ({self.offset}) is larger than the available data. Try to decrease the size of the offset ({self.offset}), the number of n_offsets ({self.n_offsets}) or increase the size of `y`.')\n        elif len(y) < self.window_size:\n            raise ValueError(f'The length of `y` ({len(y)}), must be greater than or equal to the window size ({self.window_size}). This is because  the offset ({self.offset}) is larger than the available data. Try to decrease the size of the offset ({self.offset}), the number of n_offsets ({self.n_offsets}) or increase the size of `y`.')\n        self.is_fitted = True\n        self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.training_range_ = y_index[[0, -1]]\n        self.index_type_ = type(y_index)\n        self.index_freq_ = y_index.freqstr if isinstance(y_index, pd.DatetimeIndex) else y_index.step\n        self.last_window_ = y.copy()",
        "docstring": "Fit the ForecasterEquivalentDate model to the provided training time series.\n\nParameters\n----------\ny : pandas Series\n    A time series used for training the forecaster. The index must be of type pandas DatetimeIndex if `offset` is a pandas DateOffset, and it must have a defined frequency.\nexog : Ignored\n    Not utilized within this method, included for API consistency.\nstore_in_sample_residuals : Ignored\n    Not utilized within this method, included for API consistency.\n\nReturns\n-------\nNone\n\nRaises\n------\nTypeError\n    If `y.index` is not a pandas DatetimeIndex (when using a pandas DateOffset) or if the index does not have a frequency.\n\nValueError\n    If the length of `y` is less than the computed `window_size`, indicating insufficient data for the defined offsets.\n\nNotes\n-----\n`self.offset` defines how far back in the time series to look for equivalent dates, and `self.n_offsets` specifies how many past equivalent dates to aggregate for predictions. If the `offset` is an integer, it represents a count of steps (e.g., days), whereas if it is a pandas DateOffset, it represents a time duration (e.g., business days).\n\nThis method resets relevant internal state variables before fitting, including `self.last_window_`, `self.index_type_`, `self.index_freq_`, and `self.training_range_`. After fitting, it stores the input series for later predictions and updates the fitted status and timestamps.",
        "signature": "def fit(self, y: pd.Series, exog: Any=None, store_in_sample_residuals: Any=None) -> None:",
        "type": "Method",
        "class_signature": "class ForecasterEquivalentDate:"
      },
      "ForecasterEquivalentDate.predict": {
        "code": "    def predict(self, steps: int, last_window: Optional[pd.Series]=None, exog: Any=None) -> pd.Series:\n        \"\"\"Predict future values based on the most recent equivalent dates using a specified offset and aggregation function.\n\nParameters\n----------\nsteps : int\n    The number of future steps to predict.\nlast_window : pandas Series, default `None`\n    The past values required to select the last equivalent dates. If not provided, the values stored in `self.last_window_` will be used, allowing predictions to start immediately after the training data.\nexog : Any, ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\npredictions : pandas Series\n    A Series containing the predicted values for the specified number of steps.\n\nThis method relies on the `offset`, `n_offsets`, and `agg_func` attributes defined during the initialization of the class. It uses these parameters to compute equivalent dates from past observations, applying the aggregation function to collate values over multiple offsets if needed. Additionally, this method leverages helper functions (`check_predict_input`, `preprocess_last_window`, `expand_index`) to ensure the inputs are formatted correctly for the predictions.\"\"\"\n        '\\n        Predict n steps ahead.\\n        \\n        Parameters\\n        ----------\\n        steps : int\\n            Number of future steps predicted.\\n        last_window : pandas Series, default `None`\\n            Past values needed to select the last equivalent dates according to \\n            the offset. If `last_window = None`, the values stored in \\n            `self.last_window_` are used and the predictions start immediately \\n            after the training data.\\n        exog : Ignored\\n            Not used, present here for API consistency by convention.\\n\\n        Returns\\n        -------\\n        predictions : pandas Series\\n            Predicted values.\\n        \\n        '\n        if last_window is None:\n            last_window = self.last_window_\n        check_predict_input(forecaster_name=type(self).__name__, steps=steps, is_fitted=self.is_fitted, exog_in_=False, index_type_=self.index_type_, index_freq_=self.index_freq_, window_size=self.window_size, last_window=last_window)\n        last_window = last_window.copy()\n        last_window_values, last_window_index = preprocess_last_window(last_window=last_window)\n        prediction_index = expand_index(index=last_window_index, steps=steps)\n        if isinstance(self.offset, int):\n            equivalent_indexes = np.tile(np.arange(-self.offset, 0), int(np.ceil(steps / self.offset)))\n            equivalent_indexes = equivalent_indexes[:steps]\n            if self.n_offsets == 1:\n                equivalent_values = last_window_values[equivalent_indexes]\n                predictions = equivalent_values.ravel()\n            if self.n_offsets > 1:\n                equivalent_indexes = [equivalent_indexes - n * self.offset for n in np.arange(self.n_offsets)]\n                equivalent_indexes = np.vstack(equivalent_indexes)\n                equivalent_values = last_window_values[equivalent_indexes]\n                predictions = np.apply_along_axis(self.agg_func, axis=0, arr=equivalent_values)\n            predictions = pd.Series(data=predictions, index=prediction_index, name='pred')\n        if isinstance(self.offset, pd.tseries.offsets.DateOffset):\n            predictions_index = prediction_index\n            max_allowed_date = last_window_index[-1]\n            offset_dates = []\n            for date in predictions_index:\n                selected_offsets = []\n                while len(selected_offsets) < self.n_offsets:\n                    offset_date = date - self.offset\n                    if offset_date <= max_allowed_date:\n                        selected_offsets.append(offset_date)\n                    date = offset_date\n                offset_dates.append(selected_offsets)\n            offset_dates = np.array(offset_dates)\n            equivalent_values = last_window.reindex(offset_dates.ravel()).to_numpy().reshape(-1, self.n_offsets)\n            equivalent_values = pd.DataFrame(data=equivalent_values, index=predictions_index, columns=[f'offset_{i}' for i in range(self.n_offsets)])\n            if equivalent_values.isnull().all().all():\n                raise ValueError(f'All equivalent values are missing. This is caused by using an offset ({self.offset}) larger than the available data. Try to decrease the size of the offset ({self.offset}), the number of n_offsets ({self.n_offsets}) or increase the size of `last_window`. In backtesting, this error may be caused by using an `initial_train_size` too small.')\n            incomplete_offsets = equivalent_values.isnull().any(axis=1)\n            incomplete_offsets = incomplete_offsets[incomplete_offsets].index\n            if not incomplete_offsets.empty:\n                warnings.warn(f'Steps: {incomplete_offsets.strftime('%Y-%m-%d').to_list()} are calculated with less than {self.n_offsets} n_offsets. To avoid this, increase the `last_window` size or decrease the number of n_offsets. The current configuration requires a total offset of {self.offset * self.n_offsets}.')\n            aggregate_values = equivalent_values.apply(self.agg_func, axis=1)\n            predictions = aggregate_values.rename('pred')\n        return predictions",
        "docstring": "Predict future values based on the most recent equivalent dates using a specified offset and aggregation function.\n\nParameters\n----------\nsteps : int\n    The number of future steps to predict.\nlast_window : pandas Series, default `None`\n    The past values required to select the last equivalent dates. If not provided, the values stored in `self.last_window_` will be used, allowing predictions to start immediately after the training data.\nexog : Any, ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\npredictions : pandas Series\n    A Series containing the predicted values for the specified number of steps.\n\nThis method relies on the `offset`, `n_offsets`, and `agg_func` attributes defined during the initialization of the class. It uses these parameters to compute equivalent dates from past observations, applying the aggregation function to collate values over multiple offsets if needed. Additionally, this method leverages helper functions (`check_predict_input`, `preprocess_last_window`, `expand_index`) to ensure the inputs are formatted correctly for the predictions.",
        "signature": "def predict(self, steps: int, last_window: Optional[pd.Series]=None, exog: Any=None) -> pd.Series:",
        "type": "Method",
        "class_signature": "class ForecasterEquivalentDate:"
      }
    }
  },
  "dependency_dict": {
    "skforecast/model_selection/_split.py:TimeSeriesFold:__init__": {},
    "skforecast/model_selection/_split.py:BaseFold:__init__": {},
    "skforecast/model_selection/_validation.py:backtesting_forecaster": {},
    "skforecast/model_selection/_utils.py:check_backtesting_input": {},
    "skforecast/model_selection/_validation.py:_backtesting_forecaster": {},
    "skforecast/model_selection/_split.py:BaseFold:set_params": {},
    "skforecast/metrics/metrics.py:_get_metric": {},
    "skforecast/model_selection/_split.py:TimeSeriesFold:split": {},
    "skforecast/recursive/_forecaster_equivalent_date.py:ForecasterEquivalentDate:fit": {},
    "skforecast/model_selection/_validation.py:_fit_predict_forecaster": {},
    "skforecast/recursive/_forecaster_equivalent_date.py:ForecasterEquivalentDate:predict": {},
    "skforecast/utils/utils.py:check_predict_input": {}
  },
  "PRD": "# PROJECT NAME: skforecast-test_backtesting\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 skforecast/\n    \u251c\u2500\u2500 metrics/\n    \u2502   \u2514\u2500\u2500 metrics.py\n    \u2502       \u251c\u2500\u2500 _get_metric\n    \u2502       \u251c\u2500\u2500 add_y_train_argument\n    \u2502       \u2514\u2500\u2500 wrapper\n    \u251c\u2500\u2500 model_selection/\n    \u2502   \u251c\u2500\u2500 _split.py\n    \u2502   \u2502   \u251c\u2500\u2500 BaseFold.__init__\n    \u2502   \u2502   \u251c\u2500\u2500 BaseFold._extract_index\n    \u2502   \u2502   \u251c\u2500\u2500 BaseFold._validate_params\n    \u2502   \u2502   \u251c\u2500\u2500 BaseFold.set_params\n    \u2502   \u2502   \u251c\u2500\u2500 TimeSeriesFold.__init__\n    \u2502   \u2502   \u2514\u2500\u2500 TimeSeriesFold.split\n    \u2502   \u251c\u2500\u2500 _utils.py\n    \u2502   \u2502   \u251c\u2500\u2500 check_backtesting_input\n    \u2502   \u2502   \u2514\u2500\u2500 select_n_jobs_backtesting\n    \u2502   \u2514\u2500\u2500 _validation.py\n    \u2502       \u251c\u2500\u2500 _backtesting_forecaster\n    \u2502       \u251c\u2500\u2500 _fit_predict_forecaster\n    \u2502       \u2514\u2500\u2500 backtesting_forecaster\n    \u251c\u2500\u2500 recursive/\n    \u2502   \u2514\u2500\u2500 _forecaster_equivalent_date.py\n    \u2502       \u251c\u2500\u2500 ForecasterEquivalentDate.__init__\n    \u2502       \u251c\u2500\u2500 ForecasterEquivalentDate.fit\n    \u2502       \u2514\u2500\u2500 ForecasterEquivalentDate.predict\n    \u2514\u2500\u2500 utils/\n        \u2514\u2500\u2500 utils.py\n            \u251c\u2500\u2500 check_predict_input\n            \u251c\u2500\u2500 expand_index\n            \u251c\u2500\u2500 preprocess_last_window\n            \u2514\u2500\u2500 preprocess_y\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThe module is designed to facilitate backtesting of time series forecasting models, specifically leveraging the `ForecasterEquivalentDate` functionality. It enables users to evaluate the performance of forecasts by simulating predictions under conditions with equivalent temporal offsets, utilizing cross-validation techniques and a defined metric for accuracy assessment. The module provides capabilities such as automatic handling of date offsets, iterative forecasting with specified intervals, and seamless integration with cross-validation schemes tailored for time series data. By streamlining the evaluation process of forecasting models, this module addresses the challenges of validating predictive performance in scenarios where temporal continuity and appropriate train/test splits are critical.\n\n## FILE 1: skforecast/model_selection/_validation.py\n\n- FUNCTION NAME: backtesting_forecaster\n  - SIGNATURE: def backtesting_forecaster(forecaster: object, y: pd.Series, cv: TimeSeriesFold, metric: Union[str, Callable, list], exog: Optional[Union[pd.Series, pd.DataFrame]]=None, interval: Optional[list]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False, n_jobs: Union[int, str]='auto', verbose: bool=False, show_progress: bool=True) -> Tuple[pd.DataFrame, pd.DataFrame]:\n  - DOCSTRING: \n```python\n\"\"\"\nBacktesting of a forecaster model using the TimeSeriesFold class to generate the necessary data splits. This function evaluates the performance of the model based on specified metrics during backtesting, allowing for the assessment of predictive accuracy and reliability over time periods.\n\nParameters\n----------\nforecaster : object\n    A forecaster model (must be either 'ForecasterRecursive' or 'ForecasterDirect').\ny : pd.Series\n    The time series data to be used for training and validation.\ncv : TimeSeriesFold\n    An object that defines how to split the dataset into training and validation folds.\nmetric : Union[str, Callable, list]\n    A metric or list of metrics to evaluate model performance, which can be a predefined string or a custom function.\nexog : Optional[Union[pd.Series, pd.DataFrame]], default=None\n    Optional exogenous variables to include as predictors, aligned with `y`.\ninterval : Optional[list], default=None\n    List of percentiles for estimating prediction intervals, e.g., [2.5, 97.5].\nn_boot : int, default=250\n    Number of bootstrap iterations for estimating intervals.\nrandom_state : int, default=123\n    Seed for reproducibility of random processes in bootstrapping.\nuse_in_sample_residuals : bool, default=True\n    If True, uses in-sample residuals for interval estimation; otherwise, uses out-of-sample residuals if available.\nuse_binned_residuals : bool, default=False\n    If True, residuals are binned based on predicted values during bootstrapping.\nn_jobs : Union[int, str], default='auto'\n    Number of parallel jobs for computation. If 'auto', utilizes the system's available cores.\nverbose : bool, default=False\n    If True, prints detailed information about the backtesting process.\nshow_progress : bool, default=True\n    If True, displays a progress bar during execution.\n\nReturns\n-------\nmetric_values : pd.DataFrame\n    DataFrame containing the calculated metric values.\nbacktest_predictions : pd.DataFrame\n    DataFrame containing predicted values and their corresponding prediction intervals, if provided.\n\nRaises\n------\nTypeError\n    If the forecaster is not of an expected type.\nValueError\n    If the steps and gap parameters are inconsistent with the forecaster settings.\n\nThe function interacts closely with the TimeSeriesFold class for data splitting and requires input validation via the `check_backtesting_input` function to ensure proper configuration before execution.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - skforecast/model_selection/_utils.py:check_backtesting_input\n    - skforecast/model_selection/_validation.py:_backtesting_forecaster\n\n- FUNCTION NAME: _backtesting_forecaster\n  - SIGNATURE: def _backtesting_forecaster(forecaster: object, y: pd.Series, metric: Union[str, Callable, list], cv: TimeSeriesFold, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, interval: Optional[list]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False, n_jobs: Union[int, str]='auto', verbose: bool=False, show_progress: bool=True) -> Tuple[pd.DataFrame, pd.DataFrame]:\n  - DOCSTRING: \n```python\n\"\"\"\nBacktests a forecaster model using the specified time series data and performance metrics, applying the cross-validation method defined in the TimeSeriesFold class.\n\nParameters\n----------\nforecaster : object\n    An instance of a forecaster model (e.g., ForecasterRecursive, ForecasterDirect).\ny : pd.Series\n    The training time series data to be used for backtesting.\nmetric : Union[str, Callable, list]\n    The metric(s) to measure the model's performance. This can be a single metric specified as a string, a callable function, or a list of multiple metrics.\ncv : TimeSeriesFold\n    An instance of the TimeSeriesFold class that defines how to split the time series into training and validation sets.\nexog : Optional[Union[pd.Series, pd.DataFrame]], default `None`\n    Exogenous variables that can be used as additional predictors aligned with `y`.\ninterval : Optional[list], default `None`\n    A list defining the confidence level for prediction intervals; example: [2.5, 97.5] for a 95% interval.\nn_boot : int, default `250`\n    The number of bootstrap iterations for estimating prediction intervals.\nrandom_state : int, default `123`\n    Seed for random number generation to ensure reproducibility.\nuse_in_sample_residuals : bool, default `True`\n    Determines whether to use residuals from training data for creating prediction intervals.\nuse_binned_residuals : bool, default `False`\n    If True, uses binned residuals for bootstrapping; if False, selects residuals randomly.\nn_jobs : Union[int, str], default `'auto'`\n    The number of parallel jobs to run; can be set to 'auto' to utilize available CPU cores.\nverbose : bool, default `False`\n    If True, prints additional information about the backtesting process.\nshow_progress : bool, default `True`\n    If True, displays a progress bar during execution.\n\nReturns\n-------\nmetric_values : pd.DataFrame\n    DataFrame containing the results of the specified metrics.\nbacktest_predictions : pd.DataFrame\n    DataFrame containing the predictions and, if requested, their prediction intervals, including columns for lower and upper bounds.\n\nThis function creates a deep copy of the `forecaster` and `cv` to ensure the original objects remain unchanged during the backtesting process. It adapts the training and validation splits based on the `TimeSeriesFold` object, fitting the forecaster multiple times as needed, and parallelizing the predictions to enhance performance.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - skforecast/recursive/_forecaster_equivalent_date.py:ForecasterEquivalentDate:fit\n    - skforecast/model_selection/_validation.py:backtesting_forecaster\n    - skforecast/model_selection/_split.py:BaseFold:set_params\n    - skforecast/model_selection/_utils.py:select_n_jobs_backtesting\n    - skforecast/model_selection/_split.py:TimeSeriesFold:split\n    - skforecast/model_selection/_validation.py:_fit_predict_forecaster\n    - skforecast/metrics/metrics.py:wrapper\n    - skforecast/metrics/metrics.py:_get_metric\n\n- FUNCTION NAME: _fit_predict_forecaster\n  - SIGNATURE: def _fit_predict_forecaster(y, exog, forecaster, alpha, interval, fold, steps, gap):\n  - DOCSTRING: \n```python\n\"\"\"\nFit the forecaster model to training data and predict future values, facilitating parallelization during backtesting.\n\nParameters\n----------\ny : pandas Series\n    The target time series data used for training and prediction.\nexog : pandas Series, pandas DataFrame, optional\n    Exogenous variables for predictions, must align with `y`.\nforecaster : object\n    The forecasting model that supports fitting and predicting.\nalpha : float, optional\n    Confidence level for prediction intervals (1 - alpha) if provided with `interval`.\ninterval : list, optional\n    Confidence percentiles for prediction intervals, must be between 0 and 100.\nfold : tuple\n    A tuple containing train and test indices for splitting the data into folds.\nsteps : int\n    The number of steps ahead to predict.\ngap : int\n    The number of observations to skip in the predictions.\n\nReturns\n-------\npred : pandas DataFrame\n    Predicted values for the specified steps ahead, including upper and lower bounds if intervals are requested.\n\nThis function interacts with the overall backtesting process by leveraging the `fold` to manage data splits and maintaining a clear distinction between training and testing phases. The model is fitted each time before making predictions, thereby adapting to varying training sizes as specified by `refit`. Existing variables such as `last_window_y` and `last_window_exog` are used for making predictions based on the last fitted state of the forecaster. The function is utilized within the `_backtesting_forecaster` and `_backtesting_sarimax` functions, allowing for efficient model evaluation across folds.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - skforecast/recursive/_forecaster_equivalent_date.py:ForecasterEquivalentDate:predict\n    - skforecast/recursive/_forecaster_equivalent_date.py:ForecasterEquivalentDate:fit\n\n## FILE 2: skforecast/model_selection/_split.py\n\n- CLASS METHOD: BaseFold.set_params\n  - CLASS SIGNATURE: class BaseFold:\n  - SIGNATURE: def set_params(self, params: dict) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nSet the parameters of the Fold object, validating their correctness before applying any updates. This method checks if the input is a dictionary and filters the parameters to ensure only valid attributes of the class are modified.\n\nParameters\n----------\nparams : dict\n    A dictionary of parameters to set for the Fold instance. It should contain keys that correspond to the instance's attributes.\n\nReturns\n-------\nNone\n\nRaises\n------\nTypeError\n    If `params` is not a dictionary.\n\nSide Effects\n-------------\nModifies the attributes of the instance based on the filtered parameters. If there are unknown parameters not defined in the instance, a warning is issued using the `IgnoredArgumentWarning`. The current parameters are copied using `deepcopy` to ensure that the original instance remains intact while the new parameters are validated and assigned.\n\nNotes\n-----\nThis method interacts with `_validate_params`, which ensures all updated parameters comply with defined constraints. The attributes affected are parameters defining the behavior of cross-validation in time series forecasting, such as `steps`, `initial_train_size`, and other configuration options.\n\"\"\"\n```\n\n- CLASS METHOD: TimeSeriesFold.__init__\n  - CLASS SIGNATURE: class TimeSeriesFold(BaseFold):\n  - SIGNATURE: def __init__(self, steps: int, initial_train_size: Optional[int]=None, window_size: Optional[int]=None, differentiation: Optional[int]=None, refit: Union[bool, int]=False, fixed_train_size: bool=True, gap: int=0, skip_folds: Optional[Union[int, list]]=None, allow_incomplete_fold: bool=True, return_all_indexes: bool=False, verbose: bool=True) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nInitialize a TimeSeriesFold instance for splitting time series data into training\nand testing folds. This class allows for flexible configurations for cross-validation\nin time series modeling, where various parameters dictate the nature of the folds,\nincluding the size of the training and testing sets, the presence of differentiation,\nand options for refitting models.\n\nParameters\n----------\nsteps : int\n    The number of observations to predict in each fold, also referred to as the \n    forecast horizon or test size.\ninitial_train_size : Optional[int], default=None\n    The number of observations to use for the initial training set. If None, the forecaster\n    is not trained during the first fold.\nwindow_size : Optional[int], default=None\n    The number of observations required to generate autoregressive predictors.\ndifferentiation : Optional[int], default=None\n    The number of observations to extend the last window for differentiation.\nrefit : Union[bool, int], default=False\n    Determines if the forecaster should be refitted in each fold. Can be set to \n    True or a positive integer value.\nfixed_train_size : bool, default=True\n    Indicates whether the training size remains constant across folds.\ngap : int, default=0\n    The number of observations to omit between the end of the training set and the start of the test set.\nskip_folds : Optional[Union[int, list]], default=None\n    Specifies the number of folds to skip during the fold generation.\nallow_incomplete_fold : bool, default=True\n    Whether to permit the final fold to contain fewer observations than `steps`.\nreturn_all_indexes : bool, default=False\n    If True, all indices for the training and testing sets will be returned; otherwise, only start and end indices are provided.\nverbose : bool, default=True\n    Controls the printing of information regarding the generated folds.\n\nAttributes\n----------\nThis class inherits from BaseFold, acquiring its parameters and validation methods,\nadding specific functionality for handling time series data in forecasting scenarios.\n\"\"\"\n```\n\n- CLASS METHOD: BaseFold._extract_index\n  - CLASS SIGNATURE: class BaseFold:\n  - SIGNATURE: def _extract_index(self, X: Union[pd.Series, pd.DataFrame, pd.Index, dict]) -> pd.Index:\n  - DOCSTRING: \n```python\n\"\"\"\nExtracts and returns the index from the input time series data X. \n\nThe method supports various input types including pandas Series, DataFrames, Dictionaries, and Index objects. If X is a Series or DataFrame, its index is returned directly. If X is a dictionary, the function verifies that the contained Series share the same frequency and creates a date range from the minimum to maximum index of the Series. If X is of an unsupported type, the original X is returned as the index.\n\nParameters\n----------\nX : pandas Series, pandas DataFrame, pandas Index, dict\n    The time series data or index from which to extract the index.\n\nReturns\n-------\npd.Index\n    The extracted index from the input data.\n\nRaises\n------\nValueError\n    If the input dictionary lacks frequency information or if the Series in the dictionary have different frequencies.\n\nThis method is a utility used within the BaseFold class to standardize how indices are extracted across different input types, ensuring consistency in the subsequent processing within methods such as split. It plays a crucial role in preparing data for time series cross-validation by providing a reliable index format.\n\"\"\"\n```\n\n- CLASS METHOD: BaseFold._validate_params\n  - CLASS SIGNATURE: class BaseFold:\n  - SIGNATURE: def _validate_params(self, cv_name: str, steps: Optional[int]=None, initial_train_size: Optional[int]=None, window_size: Optional[int]=None, differentiation: Optional[int]=None, refit: Union[bool, int]=False, fixed_train_size: bool=True, gap: int=0, skip_folds: Optional[Union[int, list]]=None, allow_incomplete_fold: bool=True, return_all_indexes: bool=False, verbose: bool=True) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nValidate all input parameters for the Fold class to ensure they conform to expected types and constraints. \nThis method raises exceptions if any parameter is invalid, ensuring that derived classes can function correctly based on the provided parameters.\n\nParameters\n----------\ncv_name : str\n    The name of the cross-validation method (e.g., \"TimeSeriesFold\" or \"OneStepAheadFold\") which influences validation rules.\nsteps : Optional[int], default `None`\n    Number of observations to predict in each fold, must be an integer greater than 0 for \"TimeSeriesFold\".\ninitial_train_size : Optional[int], default `None`\n    Number of observations for initial training; should be greater than 0 unless `None`.\nwindow_size : Optional[int], default `None`\n    Number of observations required to create autoregressive predictors; must be greater than 0 if provided.\ndifferentiation : Optional[int], default `None`\n    Indicates how many observations to use for differentiation, must be an integer greater than or equal to 0 if provided.\nrefit : Union[bool, int], default `False`\n    Determines whether to refit the forecaster, accepts boolean or integer greater than or equal to 0.\nfixed_train_size : bool, default `True`\n    Specifies if the training size remains constant across folds; requires a boolean value.\ngap : int, default `0`\n    Number of observations between training and testing; must be an integer greater than or equal to 0.\nskip_folds : Optional[Union[int, list]], default `None`\n    Skips certain folds based on the provided integer or list; integers must be greater than 0.\nallow_incomplete_fold : bool, default `True`\n    If set to `False`, the last fold must be complete, otherwise it is ignored.\nreturn_all_indexes : bool, default `False`\n    Specifies if all fold indexes should be returned or just the start and end indexes.\nverbose : bool, default `True`\n    Controls if validation information is printed during processing.\n\nReturns\n-------\nNone\n    This method validates parameters directly and raises exceptions upon receiving invalid arguments.\n\nRaises\n------\nValueError\n    If `steps`, `initial_train_size`, `gap`, or any other constraint is not satisfied.\nTypeError\n    If the types of any parameters do not match the expected types.\n\"\"\"\n```\n\n- CLASS METHOD: TimeSeriesFold.split\n  - CLASS SIGNATURE: class TimeSeriesFold(BaseFold):\n  - SIGNATURE: def split(self, X: Union[pd.Series, pd.DataFrame, pd.Index, dict], as_pandas: bool=False) -> Union[list, pd.DataFrame]:\n  - DOCSTRING: \n```python\n\"\"\"\nSplit the time series data into train and test folds appropriate for time series forecasting. This method generates a series of indices that define the training and testing periods for model evaluation, considering factors such as initial training size, window size, differentiation, refitting strategy, and the potential for skipped folds. \n\nParameters\n----------\nX : Union[pd.Series, pd.DataFrame, pd.Index, dict]\n    The time series data or index to split, allowing various formats including pandas Series, DataFrame, Index, or dictionaries of Series.\nas_pandas : bool, default `False`\n    If True, the output is returned as a DataFrame, enabling easier interpretation of fold structures.\n\nReturns\n-------\nUnion[list, pd.DataFrame]\n    A list of lists where each inner list contains indices for training and testing sets, including:\n    - [train_start, train_end]: Indices for the training data.\n    - [last_window_start, last_window_end]: Indices for the most recent observation used as predictors.\n    - [test_start, test_end]: Indices for the validation/test data.\n    - [test_start_with_gap, test_end_with_gap]: Indices for the validation/test data including a gap from the training period.\n    - fit_forecaster: A boolean indicating whether the forecaster should be fitted in this fold.\n\n    If `as_pandas` is True, the result is a DataFrame with columns documenting the details of each fold.\n\nNotes\n-----\nKey class attributes used in the method include:\n- `self.initial_train_size`: The number of data points to use for initial training; must be at least the sum of `self.steps` (the forecast horizon) and `self.initial_train_size`.\n- `self.window_size`: The size of the autoregressive window used to create predictor variables.\n- `self.steps`: Represents the number of observations to forecast. \n\nAdditionally, the method considers various flags such as `self.refit` for model retraining, `self.gap` for spacing between training and testing periods, and `self.skip_folds` to determine if certain folds should be omitted from the output, affecting how folds are constructed.\n\"\"\"\n```\n\n- CLASS METHOD: BaseFold.__init__\n  - CLASS SIGNATURE: class BaseFold:\n  - SIGNATURE: def __init__(self, steps: Optional[int]=None, initial_train_size: Optional[int]=None, window_size: Optional[int]=None, differentiation: Optional[int]=None, refit: Union[bool, int]=False, fixed_train_size: bool=True, gap: int=0, skip_folds: Optional[Union[int, list]]=None, allow_incomplete_fold: bool=True, return_all_indexes: bool=False, verbose: bool=True) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\n__init__ method for the BaseFold class, which serves as a foundational class for all fold classes in skforecast designed for time series cross-validation. This method initializes various parameters that control the behavior of the time series splitting process, including the number of observations predicted in each fold (steps), the initial size of the training set, and refitting options.\n\nParameters\n----------\nsteps : Optional[int], default `None`\n    The number of observations to be predicted in each fold, also known as forecast horizon or test size.\ninitial_train_size : Optional[int], default `None`\n    The number of observations used for initial training.\nwindow_size : Optional[int], default `None`\n    The number of observations required to generate autoregressive predictors.\ndifferentiation : Optional[int], default `None`\n    The number of observations to use for differentiation, extending the last window.\nrefit : Union[bool, int], default `False`\n    Controls whether the forecaster is refitted in each fold.\nfixed_train_size : bool, default `True`\n    Indicates whether the training size is fixed or increases in each fold.\ngap : int, default `0`\n    The number of observations between the training set and the test set.\nskip_folds : Optional[Union[int, list]], default `None`\n    Specifies how many folds to skip during the split.\nallow_incomplete_fold : bool, default `True`\n    Determines if the last fold can contain fewer observations than steps.\nreturn_all_indexes : bool, default `False`\n    Indicates whether to return all indexes or only start and end indexes of each fold.\nverbose : bool, default `True`\n    If True, provides verbose output with information about generated folds.\n\nReturns\n-------\nNone\n    This method does not return any value but raises exceptions if the input parameters are invalid.\n\nNotes\n-----\nThis method calls the `_validate_params` method to ensure that the input parameters adhere to defined constraints and initializes instance attributes accordingly. Each parameter influences how the class handles time series data and generates folds, making correct initialization essential for proper functioning.\n\"\"\"\n```\n\n## FILE 3: skforecast/model_selection/_utils.py\n\n- FUNCTION NAME: select_n_jobs_backtesting\n  - SIGNATURE: def select_n_jobs_backtesting(forecaster: object, refit: Union[bool, int]) -> int:\n  - DOCSTRING: \n```python\n\"\"\"\nSelect the optimal number of jobs for parallel processing in the backtesting process of time series forecasting. This function uses heuristics based on the type of forecaster and its regressor to determine the number of jobs to run in parallel, which can enhance performance during model training and evaluation.\n\nParameters\n----------\nforecaster : object\n    A forecasting model that implements the necessary methods for training and prediction, such as those found in the `Forecaster` classes.\nrefit : bool, int\n    Indicates whether the forecaster is refitted during backtesting; if `refit` is an integer, the function will set `n_jobs` to 1.\n\nReturns\n-------\nn_jobs : int\n    The number of jobs to run in parallel during the backtesting process.\n\nNotes\n-----\nThe function utilizes the `cpu_count()` from the `joblib` library to determine the number of available cores, and `Pipeline` from `sklearn` is considered to extract the regressor type. It checks if the regressor is a linear model to decide on parallelism efficiency, particularly for the `ForecasterRecursive` model. Additionally, it handles custom regressors like `LGBMRegressor` for optimal job settings during model fitting. The function also relies on constants defined in the sklearn library related to regression models.\n\"\"\"\n```\n\n- FUNCTION NAME: check_backtesting_input\n  - SIGNATURE: def check_backtesting_input(forecaster: object, cv: object, metric: Union[str, Callable, list], add_aggregated_metric: bool=True, y: Optional[pd.Series]=None, series: Optional[Union[pd.DataFrame, dict]]=None, exog: Optional[Union[pd.Series, pd.DataFrame, dict]]=None, interval: Optional[list]=None, alpha: Optional[float]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False, n_jobs: Union[int, str]='auto', show_progress: bool=True, suppress_warnings: bool=False, suppress_warnings_fit: bool=False) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nCheck the validity of input parameters for backtesting functions in the `skforecast` model selection module. This function ensures that the provided forecaster, cross-validation object, metrics, and additional parameters conform to expected formats and types. It performs various validation checks depending on the type of forecaster (uni-series or multi-series) and may raise errors if the inputs are inconsistent or incorrect.\n\nParameters\n----------\nforecaster : object\n    The forecaster model, which can be of types like `ForecasterRecursive`, `ForecasterDirect`, etc.\ncv : object\n    The cross-validation object, specifically of type `TimeSeriesFold`, which contains functions to split data into training and testing sets.\nmetric : Union[str, Callable, list]\n    The metric(s) used to evaluate model performance, can be a string name, a callable function, or a list of multiple metrics.\nadd_aggregated_metric : bool, default `True`\n    Indicates whether aggregated metrics over all levels should be included in the results, applicable for multi-series forecasters.\ny : Optional[pd.Series], default `None`\n    Training time series for uni-series forecasters; should be a pandas Series.\nseries : Optional[Union[pd.DataFrame, dict]], default `None`\n    Training time series for multi-series forecasters; can be a pandas DataFrame or a dictionary of Series.\nexog : Optional[Union[pd.Series, pd.DataFrame, dict]], default `None`\n    Exogenous variables, can also be in various types similar to series.\ninterval : Optional[list], default `None`\n    List of percentiles for confidence interval estimates, must be between 0 and 100.\nalpha : Optional[float], default `None`\n    Confidence level used in `ForecasterSarimax`.\nn_boot : int, default `250`\n    Number of bootstrapping iterations to estimate prediction intervals.\nrandom_state : int, default `123`\n    Seed for random number generation, ensuring deterministic results.\nuse_in_sample_residuals : bool, default `True`\n    Indicates whether to use training data residuals for predictions.\nuse_binned_residuals : bool, default `False`\n    Indicates if residuals should be conditioned on predicted values in bootstrapping.\nn_jobs : Union[int, str], default `'auto'`\n    Number of parallel jobs for computation; if 'auto', automatically determines based on system capabilities.\nshow_progress : bool, default `True`\n    If True, displays a progress bar during execution.\nsuppress_warnings : bool, default `False`\n    If True, suppresses warnings from the `skforecast` library.\nsuppress_warnings_fit : bool, default `False`\n    If True, ignores fitting warnings from specific forecasters.\n\nReturns\n-------\nNone\n    This function does not return a value but raises exceptions for invalid inputs.\n\nRaises\n------\nTypeError\n    Raised if the provided types of any parameters do not match expected types.\nValueError\n    Raised for logical inconsistencies in the parameters and settings.\nNotFittedError\n    Raised if the forecaster requires training before certain operations.\n\nThis function interacts with constants and functions defined in the module to perform validations, ensuring the compatibility and correctness of inputs across different forecaster types.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - skforecast/model_selection/_validation.py:backtesting_forecaster\n\n## FILE 4: skforecast/utils/utils.py\n\n- FUNCTION NAME: preprocess_last_window\n  - SIGNATURE: def preprocess_last_window(last_window: Union[pd.Series, pd.DataFrame], return_values: bool=True) -> Tuple[np.ndarray, pd.Index]:\n  - DOCSTRING: \n```python\n\"\"\"\nPreprocess the `last_window` series or DataFrame by extracting its values and index, while modifying the index based on predefined rules.\n\nParameters\n----------\nlast_window : pandas Series, pandas DataFrame\n    The input time series values, from which the values and index will be extracted.\nreturn_values : bool, default `True`\n    If `True`, the function returns the values of `last_window` as a numpy ndarray. If `False`, it does not return the values.\n\nReturns\n-------\nlast_window_values : numpy ndarray\n    A numpy array containing the values of `last_window`, if `return_values` is `True`.\nlast_window_index : pandas Index\n    The index of `last_window`, modified according to the following rules:\n    - If the index is a `DatetimeIndex` with frequency, it remains unchanged.\n    - If the index is a `RangeIndex`, it remains unchanged.\n    - If the index is a `DatetimeIndex` without frequency, it is replaced with a `RangeIndex`.\n    - If the index is of any other type, it is replaced with a `RangeIndex`.\n\nSide Effects\n-------------\nIssues warnings if the index is a `DatetimeIndex` without frequency or is of another type. This aids users in identifying potential issues with index types.\n\nThe function interacts with pandas and numpy libraries, utilizing their capabilities to handle dataframes, series, and ndarrays. It ensures the output is formatted correctly for further processing in forecasting or modeling tasks.\n\"\"\"\n```\n\n- FUNCTION NAME: check_predict_input\n  - SIGNATURE: def check_predict_input(forecaster_name: str, steps: Union[int, list], is_fitted: bool, exog_in_: bool, index_type_: type, index_freq_: str, window_size: int, last_window: Union[pd.Series, pd.DataFrame, None], last_window_exog: Optional[Union[pd.Series, pd.DataFrame]]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, exog_type_in_: Optional[type]=None, exog_names_in_: Optional[list]=None, interval: Optional[list]=None, alpha: Optional[float]=None, max_steps: Optional[int]=None, levels: Optional[Union[str, list]]=None, levels_forecaster: Optional[Union[str, list]]=None, series_names_in_: Optional[list]=None, encoding: Optional[str]=None) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nCheck the inputs of the predict method for a forecaster to ensure they match the attributes of a trained forecaster. This function validates various parameters required for making predictions, such as the number of steps, exogenous variables, last window data, and levels to be predicted.\n\nParameters\n----------\nforecaster_name : str\n    The name of the forecaster being used, which determines the checks applied.\nsteps : int, list\n    The number of future steps to predict, either as an integer or a list of integers.\nis_fitted : bool\n    A flag indicating whether the forecaster has been trained.\nexog_in_ : bool\n    A flag indicating whether the forecaster was trained with exogenous variables.\nindex_type_ : type\n    The expected type of the index for the data used during training.\nindex_freq_ : str\n    The frequency of the index from the training data.\nwindow_size : int\n    The size of the window needed for creating lagged predictors, equal to `max_lag`.\nlast_window : pandas Series, pandas DataFrame, None\n    The most recent values used to generate predictors for the first prediction step.\nlast_window_exog : pandas Series, pandas DataFrame, optional\n    The exogenous variables aligned with the last_window for predictions.\nexog : pandas Series, pandas DataFrame, optional\n    Exogenous variable(s) used as predictors.\nexog_type_in_ : type, optional\n    The expected type of the exogenous variable(s) used during training.\nexog_names_in_ : list, optional\n    A list of the names of exogenous variables used during training.\ninterval : list, optional\n    A list indicating the confidence interval for the prediction.\nalpha : float, optional\n    A float indicating the alpha value for confidence intervals in ForecasterSarimax.\nmax_steps : int, optional\n    The maximum number of steps allowed for predictions in specific forecasters.\nlevels : str, list, optional\n    The names of the time series to be predicted.\nlevels_forecaster : str, list, optional\n    Time series used as output data in a multiseries problem for specific forecasters.\nseries_names_in_ : list, optional\n    The names of the series used during training.\nencoding : str, optional\n    The encoding method used to identify different series.\n\nReturns\n-------\nNone\n    Raises exceptions if any of the input validations fail, ensuring that the conditions for making predictions are met.\n\nThis function depends on other helper functions like `check_interval`, `preprocess_last_window`, and `expand_index`, which facilitate various checks and preprocessing steps. It also raises specific warnings and errors that guide the user in rectifying input issues based on the forecaster's configuration and training.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - skforecast/utils/utils.py:preprocess_last_window\n\n- FUNCTION NAME: expand_index\n  - SIGNATURE: def expand_index(index: Union[pd.Index, None], steps: int) -> pd.Index:\n  - DOCSTRING: \n```python\n\"\"\"\nCreate a new index of specified length starting from the end of the given index.\n\nParameters\n----------\nindex : pandas Index, None\n    The original index from which to generate the new index. It can be of type \n    `DatetimeIndex` or `RangeIndex`. If `None`, a default `RangeIndex` is created.\nsteps : int\n    The number of additional steps to add to the end of the index.\n\nReturns\n-------\nnew_index : pandas Index\n    A new index created based on the specified number of steps, starting after \n    the last entry of the provided index.\n\nRaises\n------\nTypeError\n    If `steps` is not an integer or if the `index` is not a valid pandas Index type.\n\nThe function interacts with various parts of the code by providing a mechanism to \ngenerate forecast indices for time series, especially when handling predictions \nin forecasting models. When using `DatetimeIndex`, it retains the frequency, \nensuring proper alignment of time series data during expansions.\n\"\"\"\n```\n\n- FUNCTION NAME: preprocess_y\n  - SIGNATURE: def preprocess_y(y: Union[pd.Series, pd.DataFrame], return_values: bool=True) -> Tuple[Union[None, np.ndarray], pd.Index]:\n  - DOCSTRING: \n```python\n\"\"\"\nPreprocess the input time series `y` by returning its values and modifying its index. The function ensures that the index is of a proper type and handles various cases for `DatetimeIndex` and `RangeIndex`. \n\nParameters\n----------\ny : Union[pd.Series, pd.DataFrame]\n    The time series to preprocess, which can be either a pandas Series or a DataFrame.\nreturn_values : bool, default `True`\n    Indicates whether to return the values from `y` as a numpy ndarray.\n\nReturns\n-------\ny_values : Union[None, np.ndarray]\n    A numpy array containing the values of `y` if `return_values` is True; otherwise, None.\ny_index : pd.Index\n    The modified index of `y`, adjusted according to the rules outlined in the function description.\n\nNotes\n-----\n- If the index of `y` is a `DatetimeIndex` with a specified frequency, it remains unchanged. \n- If the index is a `DatetimeIndex` without a frequency or of a different type, it is replaced with a `RangeIndex`.\n- This function is useful for ensuring consistency in input data handling within forecasting tasks.\n\"\"\"\n```\n\n## FILE 5: skforecast/metrics/metrics.py\n\n- FUNCTION NAME: wrapper\n  - SIGNATURE: def wrapper(*args, y_train=None, **kwargs):\n  - DOCSTRING: \n```python\n\"\"\"\nWrapper function that adds an optional `y_train` keyword argument to a specified metric function, ensuring compatibility with functions that require this additional data for calculations. It accepts any positional and keyword arguments that the original function does.\n\nParameters\n----------\n*args : \n    Positional arguments to pass to the original function.\ny_train : optional\n    A keyword-only argument representing the true values of the target variable in the training set. Defaults to None.\n**kwargs : \n    Additional keyword arguments to pass to the original function.\n\nReturns\n-------\nAny\n    The return value of the original function after invoking it with the provided arguments.\n\nThis wrapper interacts with the function it decorates by allowing the inclusion of `y_train`, which may be essential for certain metrics, such as mean absolute scaled error (MASE) and root mean squared scaled error (RMSSE). It leverages the `inspect` module to modify the function signature without altering the original function's behavior.\n\"\"\"\n```\n\n- FUNCTION NAME: add_y_train_argument\n  - SIGNATURE: def add_y_train_argument(func: Callable) -> Callable:\n  - DOCSTRING: \n```python\n\"\"\"\nAdd a `y_train` keyword-only argument to the given function if it is not already present in its signature. This allows for further customization when using the metric functions that require comparison with training data.\n\nParameters\n----------\nfunc : Callable\n    A callable function to which the `y_train` argument is added.\n\nReturns\n-------\nwrapper : Callable\n    A wrapped function with the added `y_train` argument, maintaining the original function\u2019s signature and behavior.\n\nNotes\n-----\nThe function uses `inspect.signature` to check the parameters of the provided function and alters its signature to include `y_train` as a keyword-only argument with a default value of `None`. The wrapper function preserves any additional positional and keyword arguments when calling the original function.\n\"\"\"\n```\n\n- FUNCTION NAME: _get_metric\n  - SIGNATURE: def _get_metric(metric: str) -> Callable:\n  - DOCSTRING: \n```python\n\"\"\"\nGet the corresponding scikit-learn function to calculate the specified metric for model evaluation.\n\nParameters\n----------\nmetric : str\n    The name of the metric to be used for quantifying the goodness of fit of the model. Allowed values include:\n    - \"mean_squared_error\"\n    - \"mean_absolute_error\"\n    - \"mean_absolute_percentage_error\"\n    - \"mean_squared_log_error\"\n    - \"mean_absolute_scaled_error\"\n    - \"root_mean_squared_scaled_error\"\n    - \"median_absolute_error\"\n\nReturns\n-------\nCallable\n    A function that computes the specified metric. This function is wrapped to include an additional `y_train` argument if necessary.\n\nRaises\n------\nValueError\n    If the provided metric name is not included in the list of allowed metrics.\n\nDependencies\n------------\nThis function interacts with the `add_y_train_argument` utility function to append the `y_train` parameter\nto the corresponding metric function when required. Additionally, it relies on several imported\nfunctions from the `sklearn.metrics` module to perform the actual metric calculations.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - skforecast/metrics/metrics.py:add_y_train_argument\n\n## FILE 6: skforecast/recursive/_forecaster_equivalent_date.py\n\n- CLASS METHOD: ForecasterEquivalentDate.fit\n  - CLASS SIGNATURE: class ForecasterEquivalentDate:\n  - SIGNATURE: def fit(self, y: pd.Series, exog: Any=None, store_in_sample_residuals: Any=None) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nFit the ForecasterEquivalentDate model to the provided training time series.\n\nParameters\n----------\ny : pandas Series\n    A time series used for training the forecaster. The index must be of type pandas DatetimeIndex if `offset` is a pandas DateOffset, and it must have a defined frequency.\nexog : Ignored\n    Not utilized within this method, included for API consistency.\nstore_in_sample_residuals : Ignored\n    Not utilized within this method, included for API consistency.\n\nReturns\n-------\nNone\n\nRaises\n------\nTypeError\n    If `y.index` is not a pandas DatetimeIndex (when using a pandas DateOffset) or if the index does not have a frequency.\n\nValueError\n    If the length of `y` is less than the computed `window_size`, indicating insufficient data for the defined offsets.\n\nNotes\n-----\n`self.offset` defines how far back in the time series to look for equivalent dates, and `self.n_offsets` specifies how many past equivalent dates to aggregate for predictions. If the `offset` is an integer, it represents a count of steps (e.g., days), whereas if it is a pandas DateOffset, it represents a time duration (e.g., business days).\n\nThis method resets relevant internal state variables before fitting, including `self.last_window_`, `self.index_type_`, `self.index_freq_`, and `self.training_range_`. After fitting, it stores the input series for later predictions and updates the fitted status and timestamps.\n\"\"\"\n```\n\n- CLASS METHOD: ForecasterEquivalentDate.__init__\n  - CLASS SIGNATURE: class ForecasterEquivalentDate:\n  - SIGNATURE: def __init__(self, offset: Union[int, pd.tseries.offsets.DateOffset], n_offsets: int=1, agg_func: Callable=np.mean, forecaster_id: Optional[Union[str, int]]=None) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nInitializes the ForecasterEquivalentDate class, which predicts future values based on the most recent equivalent date, using a specified offset and aggregation function.\n\nParameters\n----------\noffset : Union[int, pd.tseries.offsets.DateOffset]\n    Specifies how many steps (or valid dates) to move back in time to find the equivalent date for predictions.\nn_offsets : int, default 1\n    Determines the number of equivalent dates to use for prediction; if greater than 1, the values are aggregated using the specified aggregation function.\nagg_func : Callable, default np.mean\n    The function applied to aggregate the values of the equivalent dates when multiple offsets are used.\nforecaster_id : Optional[Union[str, int]], default None\n    An identifier for the forecaster.\n\nAttributes\n----------\noffset : Holds the defined offset for equivalent date calculation.\nn_offsets : Holds the number of equivalent dates to use in predictions.\nagg_func : Stores the aggregation function.\nlast_window_ : Placeholder for the most recent data observed during training, initialized to None.\nindex_type_ : Stores the type of index used in the training data.\nindex_freq_ : Holds the frequency of the index from the training data.\ntraining_range_ : Captures the first and last values of the index used during training.\ncreation_date : Timestamp of when the forecaster was created.\nis_fitted : Boolean flag indicating if the forecaster has been trained.\nfit_date : Timestamp of the last fitting.\nskforecast_version : Version of the skforecast library being used.\npython_version : Python version used for creating the forecaster.\n\nRaises\n------\nTypeError\n    If `offset` is not an integer or a pandas DateOffset.\n\"\"\"\n```\n\n- CLASS METHOD: ForecasterEquivalentDate.predict\n  - CLASS SIGNATURE: class ForecasterEquivalentDate:\n  - SIGNATURE: def predict(self, steps: int, last_window: Optional[pd.Series]=None, exog: Any=None) -> pd.Series:\n  - DOCSTRING: \n```python\n\"\"\"\nPredict future values based on the most recent equivalent dates using a specified offset and aggregation function.\n\nParameters\n----------\nsteps : int\n    The number of future steps to predict.\nlast_window : pandas Series, default `None`\n    The past values required to select the last equivalent dates. If not provided, the values stored in `self.last_window_` will be used, allowing predictions to start immediately after the training data.\nexog : Any, ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\npredictions : pandas Series\n    A Series containing the predicted values for the specified number of steps.\n\nThis method relies on the `offset`, `n_offsets`, and `agg_func` attributes defined during the initialization of the class. It uses these parameters to compute equivalent dates from past observations, applying the aggregation function to collate values over multiple offsets if needed. Additionally, this method leverages helper functions (`check_predict_input`, `preprocess_last_window`, `expand_index`) to ensure the inputs are formatted correctly for the predictions.\n\"\"\"\n```\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "skforecast/model_selection/_validation.py": "import re\nfrom copy import deepcopy\nfrom typing import Union, Tuple, Optional, Callable\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom joblib import Parallel, delayed, cpu_count\nfrom tqdm.auto import tqdm\nfrom ..metrics import add_y_train_argument, _get_metric\nfrom ..exceptions import LongTrainingWarning, IgnoredArgumentWarning\nfrom ..model_selection._split import TimeSeriesFold\nfrom ..model_selection._utils import _initialize_levels_model_selection_multiseries, check_backtesting_input, select_n_jobs_backtesting, _extract_data_folds_multiseries, _calculate_metrics_backtesting_multiseries\nfrom ..utils import set_skforecast_warnings\n\ndef _backtesting_forecaster_multiseries(forecaster: object, series: Union[pd.DataFrame, dict], cv: TimeSeriesFold, metric: Union[str, Callable, list], levels: Optional[Union[str, list]]=None, add_aggregated_metric: bool=True, exog: Optional[Union[pd.Series, pd.DataFrame, dict]]=None, interval: Optional[list]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, n_jobs: Union[int, str]='auto', verbose: bool=False, show_progress: bool=True, suppress_warnings: bool=False) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Backtesting of forecaster model following the folds generated by the TimeSeriesFold\n    class and using the metric(s) provided.\n\n    If `forecaster` is already trained and `initial_train_size` is set to `None` in the\n    TimeSeriesFold class, no initial train will be done and all data will be used\n    to evaluate the model. However, the first `len(forecaster.last_window)` observations\n    are needed to create the initial predictors, so no predictions are calculated for\n    them.\n    \n    A copy of the original forecaster is created so that it is not modified during the\n    process.\n    \n    Parameters\n    ----------\n    forecaster : ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Training time series.\n    cv : TimeSeriesFold\n        TimeSeriesFold object with the information needed to split the data into folds.\n        **New in version 0.14.0**\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n        \n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error'}\n        - If `Callable`: Function with arguments y_true, y_pred that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels will be predicted.\n    add_aggregated_metric : bool, default `False`\n        If `True`, and multiple series (`levels`) are predicted, the aggregated\n        metrics (average, weighted average and pooled) are also returned.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. If `None`, no\n        intervals are estimated.\n    n_boot : int, default `250`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of prediction\n        error to create prediction intervals. If `False`, out_sample_residuals \n        are used if they are already stored inside the forecaster.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n    verbose : bool, default `False`\n        Print number of folds and index of training and validation sets used \n        for backtesting.\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n    suppress_warnings: bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the backtesting \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s). Index are the levels and columns the metrics.\n    backtest_predictions : pandas Dataframe\n        Value of predictions and their estimated interval if `interval` is not `None`. \n        If there is more than one level, this structure will be repeated for each of them.\n\n        - column pred: predictions.\n        - column lower_bound: lower bound of the interval.\n        - column upper_bound: upper bound of the interval.\n    \n    \"\"\"\n    set_skforecast_warnings(suppress_warnings, action='ignore')\n    forecaster = deepcopy(forecaster)\n    cv = deepcopy(cv)\n    cv.set_params({'window_size': forecaster.window_size, 'differentiation': forecaster.differentiation, 'return_all_indexes': False, 'verbose': verbose})\n    initial_train_size = cv.initial_train_size\n    refit = cv.refit\n    gap = cv.gap\n    if n_jobs == 'auto':\n        n_jobs = select_n_jobs_backtesting(forecaster=forecaster, refit=refit)\n    elif not isinstance(refit, bool) and refit != 1 and (n_jobs != 1):\n        warnings.warn('If `refit` is an integer other than 1 (intermittent refit). `n_jobs` is set to 1 to avoid unexpected results during parallelization.', IgnoredArgumentWarning)\n        n_jobs = 1\n    else:\n        n_jobs = n_jobs if n_jobs > 0 else cpu_count()\n    levels = _initialize_levels_model_selection_multiseries(forecaster=forecaster, series=series, levels=levels)\n    if not isinstance(metric, list):\n        metrics = [_get_metric(metric=metric) if isinstance(metric, str) else add_y_train_argument(metric)]\n    else:\n        metrics = [_get_metric(metric=m) if isinstance(m, str) else add_y_train_argument(m) for m in metric]\n    store_in_sample_residuals = False if interval is None else True\n    folds = cv.split(X=series, as_pandas=False)\n    span_index = cv._extract_index(X=series)\n    if initial_train_size is not None:\n        data_fold = _extract_data_folds_multiseries(series=series, folds=[folds[0]], span_index=span_index, window_size=forecaster.window_size, exog=exog, dropna_last_window=forecaster.dropna_from_series, externally_fitted=False)\n        series_train, _, last_window_levels, exog_train, _, _ = next(data_fold)\n        forecaster.fit(series=series_train, exog=exog_train, store_last_window=last_window_levels, store_in_sample_residuals=store_in_sample_residuals, suppress_warnings=suppress_warnings)\n        folds[0][4] = False\n    if refit:\n        n_of_fits = int(len(folds) / refit)\n        if type(forecaster).__name__ != 'ForecasterDirectMultiVariate' and n_of_fits > 50:\n            warnings.warn(f'The forecaster will be fit {n_of_fits} times. This can take substantial amounts of time. If not feasible, try with `refit = False`.\\n', LongTrainingWarning)\n        elif type(forecaster).__name__ == 'ForecasterDirectMultiVariate' and n_of_fits * forecaster.steps > 50:\n            warnings.warn(f'The forecaster will be fit {n_of_fits * forecaster.steps} times ({n_of_fits} folds * {forecaster.steps} regressors). This can take substantial amounts of time. If not feasible, try with `refit = False`.\\n', LongTrainingWarning)\n    if show_progress:\n        folds = tqdm(folds)\n    externally_fitted = True if initial_train_size is None else False\n    data_folds = _extract_data_folds_multiseries(series=series, folds=folds, span_index=span_index, window_size=forecaster.window_size, exog=exog, dropna_last_window=forecaster.dropna_from_series, externally_fitted=externally_fitted)\n    backtest_predictions = Parallel(n_jobs=n_jobs)((delayed(_fit_predict_forecaster)(data_fold=data_fold, forecaster=forecaster, interval=interval, levels=levels, gap=gap) for data_fold in data_folds))\n    backtest_predictions = pd.concat(backtest_predictions, axis=0)\n    levels_in_backtest_predictions = backtest_predictions.columns\n    if interval is not None:\n        levels_in_backtest_predictions = [level for level in levels_in_backtest_predictions if not re.search('_lower_bound|_upper_bound', level)]\n    for level in levels_in_backtest_predictions:\n        valid_index = series[level][series[level].notna()].index\n        no_valid_index = backtest_predictions.index.difference(valid_index, sort=False)\n        cols = [level]\n        if interval:\n            cols = cols + [f'{level}_lower_bound', f'{level}_upper_bound']\n        backtest_predictions.loc[no_valid_index, cols] = np.nan\n    metrics_levels = _calculate_metrics_backtesting_multiseries(series=series, predictions=backtest_predictions, folds=folds, span_index=span_index, window_size=forecaster.window_size, metrics=metrics, levels=levels, add_aggregated_metric=add_aggregated_metric)\n    set_skforecast_warnings(suppress_warnings, action='default')\n    return (metrics_levels, backtest_predictions)\n\ndef backtesting_forecaster_multiseries(forecaster: object, series: Union[pd.DataFrame, dict], cv: TimeSeriesFold, metric: Union[str, Callable, list], levels: Optional[Union[str, list]]=None, add_aggregated_metric: bool=True, exog: Optional[Union[pd.Series, pd.DataFrame, dict]]=None, interval: Optional[list]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, n_jobs: Union[int, str]='auto', verbose: bool=False, show_progress: bool=True, suppress_warnings: bool=False) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Backtesting of forecaster model following the folds generated by the TimeSeriesFold\n    class and using the metric(s) provided.\n\n    If `forecaster` is already trained and `initial_train_size` is set to `None` in the\n    TimeSeriesFold class, no initial train will be done and all data will be used\n    to evaluate the model. However, the first `len(forecaster.last_window)` observations\n    are needed to create the initial predictors, so no predictions are calculated for\n    them.\n    \n    A copy of the original forecaster is created so that it is not modified during \n    the process.\n\n    Parameters\n    ----------\n    forecaster : ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate, ForecasterRnn\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Training time series.\n    cv : TimeSeriesFold\n        TimeSeriesFold object with the information needed to split the data into folds.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n        \n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    levels : str, list, default `None`\n        Time series to be predicted. If `None` all levels will be predicted.\n    add_aggregated_metric : bool, default `True`\n        If `True`, and multiple series (`levels`) are predicted, the aggregated\n        metrics (average, weighted average and pooled) are also returned.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. If `None`, no\n        intervals are estimated.\n    n_boot : int, default `250`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of prediction \n        error to create prediction intervals. If `False`, out_sample_residuals \n        are used if they are already stored inside the forecaster.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n    verbose : bool, default `False`\n        Print number of folds and index of training and validation sets used \n        for backtesting.\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n    suppress_warnings: bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the backtesting \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s). Index are the levels and columns the metrics.\n    backtest_predictions : pandas DataFrame\n        Value of predictions and their estimated interval if `interval` is not `None`.\n        If there is more than one level, this structure will be repeated for each of them.\n\n        - column pred: predictions.\n        - column lower_bound: lower bound of the interval.\n        - column upper_bound: upper bound of the interval.\n    \n    \"\"\"\n    multi_series_forecasters = ['ForecasterRecursiveMultiSeries', 'ForecasterDirectMultiVariate', 'ForecasterRnn']\n    forecaster_name = type(forecaster).__name__\n    if forecaster_name not in multi_series_forecasters:\n        raise TypeError(f'`forecaster` must be of type {multi_series_forecasters}, for all other types of forecasters use the functions available in the `model_selection` module. Got {forecaster_name}')\n    check_backtesting_input(forecaster=forecaster, cv=cv, metric=metric, add_aggregated_metric=add_aggregated_metric, series=series, exog=exog, interval=interval, n_boot=n_boot, random_state=random_state, use_in_sample_residuals=use_in_sample_residuals, n_jobs=n_jobs, show_progress=show_progress, suppress_warnings=suppress_warnings)\n    metrics_levels, backtest_predictions = _backtesting_forecaster_multiseries(forecaster=forecaster, series=series, cv=cv, levels=levels, metric=metric, add_aggregated_metric=add_aggregated_metric, exog=exog, interval=interval, n_boot=n_boot, random_state=random_state, use_in_sample_residuals=use_in_sample_residuals, n_jobs=n_jobs, verbose=verbose, show_progress=show_progress, suppress_warnings=suppress_warnings)\n    return (metrics_levels, backtest_predictions)\n\ndef _backtesting_sarimax(forecaster: object, y: pd.Series, metric: Union[str, Callable, list], cv: TimeSeriesFold, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, alpha: Optional[float]=None, interval: Optional[list]=None, n_jobs: Union[int, str]='auto', suppress_warnings_fit: bool=False, verbose: bool=False, show_progress: bool=True) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Backtesting of ForecasterSarimax.\n    \n    A copy of the original forecaster is created so that it is not modified during \n    the process.\n    \n    Parameters\n    ----------\n    forecaster : ForecasterSarimax\n        Forecaster model.\n    y : pandas Series\n        Training time series.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n        \n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    cv : TimeSeriesFold\n        TimeSeriesFold object with the information needed to split the data into folds.\n        **New in version 0.14.0**\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    alpha : float, default `0.05`\n        The confidence intervals for the forecasts are (1 - alpha) %.\n        If both, `alpha` and `interval` are provided, `alpha` will be used.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. The values must be\n        symmetric. Sequence of percentiles to compute, which must be between \n        0 and 100 inclusive. For example, interval of 95% should be as \n        `interval = [2.5, 97.5]`. If both, `alpha` and `interval` are \n        provided, `alpha` will be used.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting.\n    verbose : bool, default `False`\n        Print number of folds and index of training and validation sets used \n        for backtesting.\n    suppress_warnings_fit : bool, default `False`\n        If `True`, warnings generated during fitting will be ignored.\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n\n    Returns\n    -------\n    metric_values : pandas DataFrame\n        Value(s) of the metric(s).\n    backtest_predictions : pandas DataFrame\n        Value of predictions and their estimated interval if `interval` is not `None`.\n\n        - column pred: predictions.\n        - column lower_bound: lower bound of the interval.\n        - column upper_bound: upper bound of the interval.\n    \n    \"\"\"\n    forecaster = deepcopy(forecaster)\n    cv = deepcopy(cv)\n    cv.set_params({'window_size': forecaster.window_size, 'return_all_indexes': False, 'verbose': verbose})\n    steps = cv.steps\n    initial_train_size = cv.initial_train_size\n    refit = cv.refit\n    gap = cv.gap\n    if refit == False:\n        if n_jobs != 'auto' and n_jobs != 1:\n            warnings.warn('If `refit = False`, `n_jobs` is set to 1 to avoid unexpected results during parallelization.', IgnoredArgumentWarning)\n        n_jobs = 1\n    elif n_jobs == 'auto':\n        n_jobs = select_n_jobs_backtesting(forecaster=forecaster, refit=refit)\n    elif not isinstance(refit, bool) and refit != 1 and (n_jobs != 1):\n        warnings.warn('If `refit` is an integer other than 1 (intermittent refit). `n_jobs` is set to 1 to avoid unexpected results during parallelization.', IgnoredArgumentWarning)\n        n_jobs = 1\n    else:\n        n_jobs = n_jobs if n_jobs > 0 else cpu_count()\n    if not isinstance(metric, list):\n        metrics = [_get_metric(metric=metric) if isinstance(metric, str) else add_y_train_argument(metric)]\n    else:\n        metrics = [_get_metric(metric=m) if isinstance(m, str) else add_y_train_argument(m) for m in metric]\n    exog_train = exog.iloc[:initial_train_size,] if exog is not None else None\n    forecaster.fit(y=y.iloc[:initial_train_size,], exog=exog_train, suppress_warnings=suppress_warnings_fit)\n    folds = cv.split(X=y, as_pandas=False)\n    folds[0][4] = False\n    if refit:\n        n_of_fits = int(len(folds) / refit)\n        if n_of_fits > 50:\n            warnings.warn(f'The forecaster will be fit {n_of_fits} times. This can take substantial amounts of time. If not feasible, try with `refit = False`.\\n', LongTrainingWarning)\n    folds_tqdm = tqdm(folds) if show_progress else folds\n    backtest_predictions = Parallel(n_jobs=n_jobs)((delayed(_fit_predict_forecaster)(y=y, exog=exog, forecaster=forecaster, alpha=alpha, interval=interval, fold=fold, steps=steps, gap=gap) for fold in folds_tqdm))\n    backtest_predictions = pd.concat(backtest_predictions)\n    if isinstance(backtest_predictions, pd.Series):\n        backtest_predictions = pd.DataFrame(backtest_predictions)\n    train_indexes = []\n    for i, fold in enumerate(folds):\n        fit_fold = fold[-1]\n        if i == 0 or fit_fold:\n            train_iloc_start = fold[0][0]\n            train_iloc_end = fold[0][1]\n            train_indexes.append(np.arange(train_iloc_start, train_iloc_end))\n    train_indexes = np.unique(np.concatenate(train_indexes))\n    y_train = y.iloc[train_indexes]\n    metric_values = [m(y_true=y.loc[backtest_predictions.index], y_pred=backtest_predictions['pred'], y_train=y_train) for m in metrics]\n    metric_values = pd.DataFrame(data=[metric_values], columns=[m.__name__ for m in metrics])\n    return (metric_values, backtest_predictions)\n\ndef backtesting_sarimax(forecaster: object, y: pd.Series, cv: TimeSeriesFold, metric: Union[str, Callable, list], exog: Optional[Union[pd.Series, pd.DataFrame]]=None, alpha: Optional[float]=None, interval: Optional[list]=None, n_jobs: Union[int, str]='auto', verbose: bool=False, suppress_warnings_fit: bool=False, show_progress: bool=True) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Backtesting of ForecasterSarimax.\n    \n    A copy of the original forecaster is created so that it is not modified during \n    the process.\n\n    Parameters\n    ----------\n    forecaster : ForecasterSarimax\n        Forecaster model.\n    y : pandas Series\n        Training time series.\n    cv : TimeSeriesFold\n        TimeSeriesFold object with the information needed to split the data into folds.\n        **New in version 0.14.0**\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n        \n        - If `string`: {'mean_squared_error', 'mean_absolute_error',\n        'mean_absolute_percentage_error', 'mean_squared_log_error',\n        'mean_absolute_scaled_error', 'root_mean_squared_scaled_error'}\n        - If `Callable`: Function with arguments `y_true`, `y_pred` and `y_train`\n        (Optional) that returns a float.\n        - If `list`: List containing multiple strings and/or Callables.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must have the same\n        number of observations as `y` and should be aligned so that y[i] is\n        regressed on exog[i].\n    alpha : float, default `0.05`\n        The confidence intervals for the forecasts are (1 - alpha) %.\n        If both, `alpha` and `interval` are provided, `alpha` will be used.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. The values must be\n        symmetric. Sequence of percentiles to compute, which must be between \n        0 and 100 inclusive. For example, interval of 95% should be as \n        `interval = [2.5, 97.5]`. If both, `alpha` and `interval` are \n        provided, `alpha` will be used.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_backtesting. \n    verbose : bool, default `False`\n        Print number of folds and index of training and validation sets used \n        for backtesting.\n    suppress_warnings_fit : bool, default `False`\n        If `True`, warnings generated during fitting will be ignored.\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n\n    Returns\n    -------\n    metric_values : pandas DataFrame\n        Value(s) of the metric(s).\n    backtest_predictions : pandas DataFrame\n        Value of predictions and their estimated interval if `interval` is not `None`.\n\n        - column pred: predictions.\n        - column lower_bound: lower bound of the interval.\n        - column upper_bound: upper bound of the interval.\n    \n    \"\"\"\n    if type(forecaster).__name__ not in ['ForecasterSarimax']:\n        raise TypeError('`forecaster` must be of type `ForecasterSarimax`, for all other types of forecasters use the functions available in the other `model_selection` modules.')\n    check_backtesting_input(forecaster=forecaster, cv=cv, y=y, metric=metric, interval=interval, alpha=alpha, n_jobs=n_jobs, show_progress=show_progress, suppress_warnings_fit=suppress_warnings_fit)\n    metric_values, backtest_predictions = _backtesting_sarimax(forecaster=forecaster, y=y, cv=cv, metric=metric, exog=exog, alpha=alpha, interval=interval, n_jobs=n_jobs, verbose=verbose, suppress_warnings_fit=suppress_warnings_fit, show_progress=show_progress)\n    return (metric_values, backtest_predictions)",
    "skforecast/model_selection/_split.py": "from copy import deepcopy\nfrom typing import Union, Optional, Any\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport itertools\nfrom ..exceptions import IgnoredArgumentWarning\n\nclass BaseFold:\n    \"\"\"\n    Base class for all Fold classes in skforecast. All fold classes should specify\n    all the parameters that can be set at the class level in their ``__init__``.\n\n    Parameters\n    ----------\n    steps : int, default `None`\n        Number of observations used to be predicted in each fold. This is also commonly\n        referred to as the forecast horizon or test size.\n    initial_train_size : int, default `None`\n        Number of observations used for initial training.\n    window_size : int, default `None`\n        Number of observations needed to generate the autoregressive predictors.\n    differentiation : int, default `None`\n        Number of observations to use for differentiation. This is used to extend the\n        `last_window` as many observations as the differentiation order.\n    refit : bool, int, default `False`\n        Whether to refit the forecaster in each fold.\n\n        - If `True`, the forecaster is refitted in each fold.\n        - If `False`, the forecaster is trained only in the first fold.\n        - If an integer, the forecaster is trained in the first fold and then refitted\n          every `refit` folds.\n    fixed_train_size : bool, default `True`\n        Whether the training size is fixed or increases in each fold.\n    gap : int, default `0`\n        Number of observations between the end of the training set and the start of the\n        test set.\n    skip_folds : int, list, default `None`\n        Number of folds to skip.\n\n        - If an integer, every 'skip_folds'-th is returned.\n        - If a list, the indexes of the folds to skip.\n\n        For example, if `skip_folds=3` and there are 10 folds, the returned folds are\n        0, 3, 6, and 9. If `skip_folds=[1, 2, 3]`, the returned folds are 0, 4, 5, 6, 7,\n        8, and 9.\n    allow_incomplete_fold : bool, default `True`\n        Whether to allow the last fold to include fewer observations than `steps`.\n        If `False`, the last fold is excluded if it is incomplete.\n    return_all_indexes : bool, default `False`\n        Whether to return all indexes or only the start and end indexes of each fold.\n    verbose : bool, default `True`\n        Whether to print information about generated folds.\n\n    Attributes\n    ----------\n    steps : int\n        Number of observations used to be predicted in each fold. This is also commonly\n        referred to as the forecast horizon or test size.\n    initial_train_size : int\n        Number of observations used for initial training.\n    window_size : int\n        Number of observations needed to generate the autoregressive predictors.\n    differentiation : int\n        Number of observations to use for differentiation. This is used to extend the\n        `last_window` as many observations as the differentiation order.\n    refit : bool, int\n        Whether to refit the forecaster in each fold.\n    fixed_train_size : bool\n        Whether the training size is fixed or increases in each fold.\n    gap : int\n        Number of observations between the end of the training set and the start of the\n        test set.\n    skip_folds : int, list\n        Number of folds to skip.\n    allow_incomplete_fold : bool\n        Whether to allow the last fold to include fewer observations than `steps`.\n    return_all_indexes : bool\n        Whether to return all indexes or only the start and end indexes of each fold.\n    verbose : bool\n        Whether to print information about generated folds.\n\n    \"\"\"\n\nclass OneStepAheadFold(BaseFold):\n    \"\"\"\n    Class to split time series data into train and test folds for one-step-ahead\n    forecasting.\n\n    Parameters\n    ----------\n    initial_train_size : int\n        Number of observations used for initial training.\n    window_size : int, default `None`\n        Number of observations needed to generate the autoregressive predictors.\n    differentiation : int, default `None`\n        Number of observations to use for differentiation. This is used to extend the\n        `last_window` as many observations as the differentiation order.\n    return_all_indexes : bool, default `False`\n        Whether to return all indexes or only the start and end indexes of each fold.\n    verbose : bool, default `True`\n        Whether to print information about generated folds.\n\n    Attributes\n    ----------\n    initial_train_size : int\n        Number of observations used for initial training.\n    window_size : int\n        Number of observations needed to generate the autoregressive predictors.\n    differentiation : int \n        Number of observations to use for differentiation. This is used to extend the\n        `last_window` as many observations as the differentiation order.\n    return_all_indexes : bool\n        Whether to return all indexes or only the start and end indexes of each fold.\n    verbose : bool\n        Whether to print information about generated folds.\n    steps : Any\n        This attribute is not used in this class. It is included for API consistency.\n    fixed_train_size : Any\n        This attribute is not used in this class. It is included for API consistency.\n    gap : Any\n        This attribute is not used in this class. It is included for API consistency.\n    skip_folds : Any\n        This attribute is not used in this class. It is included for API consistency.\n    allow_incomplete_fold : Any\n        This attribute is not used in this class. It is included for API consistency.\n    refit : Any\n        This attribute is not used in this class. It is included for API consistency.\n    \n    \"\"\"\n\n    def __init__(self, initial_train_size: int, window_size: Optional[int]=None, differentiation: Optional[int]=None, return_all_indexes: bool=False, verbose: bool=True) -> None:\n        super().__init__(initial_train_size=initial_train_size, window_size=window_size, differentiation=differentiation, return_all_indexes=return_all_indexes, verbose=verbose)\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Information displayed when printed.\n        \"\"\"\n        return f'OneStepAheadFold(\\n    initial_train_size = {self.initial_train_size},\\n    window_size        = {self.window_size},\\n    differentiation    = {self.differentiation},\\n    return_all_indexes = {self.return_all_indexes},\\n    verbose            = {self.verbose}\\n)'\n\n    def split(self, X: Union[pd.Series, pd.DataFrame, pd.Index, dict], as_pandas: bool=False, externally_fitted: Any=None) -> Union[list, pd.DataFrame]:\n        \"\"\"\n        Split the time series data into train and test folds.\n\n        Parameters\n        ----------\n        X : pandas Series, DataFrame, Index, or dictionary\n            Time series data or index to split.\n        as_pandas : bool, default `False`\n            If True, the folds are returned as a DataFrame. This is useful to visualize\n            the folds in a more interpretable way.\n        externally_fitted : Any\n            This argument is not used in this class. It is included for API consistency.\n        \n        Returns\n        -------\n        fold : list, pandas DataFrame\n            A list of lists containing the indices (position) for for each fold. Each list\n            contains 2 lists the following information:\n\n            - [train_start, train_end]: list with the start and end positions of the\n            training set.\n            - [test_start, test_end]: list with the start and end positions of the test\n            set. These are the observations used to evaluate the forecaster.\n        \n            It is important to note that the returned values are the positions of the\n            observations and not the actual values of the index, so they can be used to\n            slice the data directly using iloc.\n\n            If `as_pandas` is `True`, the folds are returned as a DataFrame with the\n            following columns: 'fold', 'train_start', 'train_end', 'test_start', 'test_end'.\n\n            Following the python convention, the start index is inclusive and the end\n            index is exclusive. This means that the last index is not included in the\n            slice.\n        \n        \"\"\"\n        if not isinstance(X, (pd.Series, pd.DataFrame, pd.Index, dict)):\n            raise TypeError(f'X must be a pandas Series, DataFrame, Index or a dictionary. Got {type(X)}.')\n        index = self._extract_index(X)\n        fold = [[0, self.initial_train_size], [self.initial_train_size, len(X)], True]\n        if self.verbose:\n            self._print_info(index=index, fold=fold)\n        if self.return_all_indexes:\n            fold = [[range(fold[0][0], fold[0][1])], [range(fold[1][0], fold[1][1])], fold[2]]\n        if as_pandas:\n            if not self.return_all_indexes:\n                fold = pd.DataFrame(data=[list(itertools.chain(*fold[:-1])) + [fold[-1]]], columns=['train_start', 'train_end', 'test_start', 'test_end', 'fit_forecaster'])\n            else:\n                fold = pd.DataFrame(data=[fold], columns=['train_index', 'test_index', 'fit_forecaster'])\n            fold.insert(0, 'fold', range(len(fold)))\n        return fold\n\n    def _print_info(self, index: pd.Index, fold: list) -> None:\n        \"\"\"\n        Print information about folds.\n        \"\"\"\n        if self.differentiation is None:\n            differentiation = 0\n        else:\n            differentiation = self.differentiation\n        initial_train_size = self.initial_train_size - differentiation\n        test_length = len(index) - (initial_train_size + differentiation)\n        print('Information of folds')\n        print('--------------------')\n        print(f'Number of observations in train: {initial_train_size}')\n        if self.differentiation is not None:\n            print(f'    First {differentiation} observation/s in training set are used for differentiation')\n        print(f'Number of observations in test: {test_length}')\n        training_start = index[fold[0][0] + differentiation]\n        training_end = index[fold[0][-1]]\n        test_start = index[fold[1][0]]\n        test_end = index[fold[1][-1] - 1]\n        print(f'Training : {training_start} -- {training_end} (n={initial_train_size})')\n        print(f'Test     : {test_start} -- {test_end} (n={test_length})')\n        print('')\n\nclass TimeSeriesFold(BaseFold):\n    \"\"\"\n    Class to split time series data into train and test folds. \n    When used within a backtesting or hyperparameter search, the arguments\n    'initial_train_size', 'window_size' and 'differentiation' are not required\n    as they are automatically set by the backtesting or hyperparameter search\n    functions.\n\n    Parameters\n    ----------\n    steps : int\n        Number of observations used to be predicted in each fold. This is also commonly\n        referred to as the forecast horizon or test size.\n    initial_train_size : int, default `None`\n        Number of observations used for initial training. If `None` or 0, the initial\n        forecaster is not trained in the first fold.\n    window_size : int, default `None`\n        Number of observations needed to generate the autoregressive predictors.\n    differentiation : int, default `None`\n        Number of observations to use for differentiation. This is used to extend the\n        `last_window` as many observations as the differentiation order.\n    refit : bool, int, default `False`\n        Whether to refit the forecaster in each fold.\n\n        - If `True`, the forecaster is refitted in each fold.\n        - If `False`, the forecaster is trained only in the first fold.\n        - If an integer, the forecaster is trained in the first fold and then refitted\n          every `refit` folds.\n    fixed_train_size : bool, default `True`\n        Whether the training size is fixed or increases in each fold.\n    gap : int, default `0`\n        Number of observations between the end of the training set and the start of the\n        test set.\n    skip_folds : int, list, default `None`\n        Number of folds to skip.\n\n        - If an integer, every 'skip_folds'-th is returned.\n        - If a list, the indexes of the folds to skip.\n\n        For example, if `skip_folds=3` and there are 10 folds, the returned folds are\n        0, 3, 6, and 9. If `skip_folds=[1, 2, 3]`, the returned folds are 0, 4, 5, 6, 7,\n        8, and 9.\n    allow_incomplete_fold : bool, default `True`\n        Whether to allow the last fold to include fewer observations than `steps`.\n        If `False`, the last fold is excluded if it is incomplete.\n    return_all_indexes : bool, default `False`\n        Whether to return all indexes or only the start and end indexes of each fold.\n    verbose : bool, default `True`\n        Whether to print information about generated folds.\n\n    Attributes\n    ----------\n    steps : int\n        Number of observations used to be predicted in each fold. This is also commonly\n        referred to as the forecast horizon or test size.\n    initial_train_size : int\n        Number of observations used for initial training. If `None` or 0, the initial\n        forecaster is not trained in the first fold.\n    window_size : int\n        Number of observations needed to generate the autoregressive predictors.\n    differentiation : int\n        Number of observations to use for differentiation. This is used to extend the\n        `last_window` as many observations as the differentiation order.\n    refit : bool, int\n        Whether to refit the forecaster in each fold.\n    fixed_train_size : bool\n        Whether the training size is fixed or increases in each fold.\n    gap : int\n        Number of observations between the end of the training set and the start of the\n        test set.\n    skip_folds : int, list\n        Number of folds to skip.\n    allow_incomplete_fold : bool\n        Whether to allow the last fold to include fewer observations than `steps`.\n    return_all_indexes : bool\n        Whether to return all indexes or only the start and end indexes of each fold.\n    verbose : bool\n        Whether to print information about generated folds.\n\n    Notes\n    -----\n    Returned values are the positions of the observations and not the actual values of\n    the index, so they can be used to slice the data directly using iloc. For example,\n    if the input series is `X = [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]`, the \n    `initial_train_size = 3`, `window_size = 2`, `steps = 4`, and `gap = 1`,\n    the output of the first fold will: [[0, 3], [1, 3], [3, 8], [4, 8], True].\n\n    The first list `[0, 3]` indicates that the training set goes from the first to the\n    third observation. The second list `[1, 3]` indicates that the last window seen by\n    the forecaster during training goes from the second to the third observation. The\n    third list `[3, 8]` indicates that the test set goes from the fourth to the eighth\n    observation. The fourth list `[4, 8]` indicates that the test set including the gap\n    goes from the fifth to the eighth observation. The boolean `False` indicates that the\n    forecaster should not be trained in this fold.\n\n    Following the python convention, the start index is inclusive and the end index is\n    exclusive. This means that the last index is not included in the slice.\n\n    \"\"\"\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Information displayed when printed.\n        \"\"\"\n        return f'TimeSeriesFold(\\n    steps                 = {self.steps},\\n    initial_train_size    = {self.initial_train_size},\\n    window_size           = {self.window_size},\\n    differentiation       = {self.differentiation},\\n    refit                 = {self.refit},\\n    fixed_train_size      = {self.fixed_train_size},\\n    gap                   = {self.gap},\\n    skip_folds            = {self.skip_folds},\\n    allow_incomplete_fold = {self.allow_incomplete_fold},\\n    return_all_indexes    = {self.return_all_indexes},\\n    verbose               = {self.verbose}\\n)'\n\n    def _print_info(self, index: pd.Index, folds: list, externally_fitted: bool, last_fold_excluded: bool, index_to_skip: list) -> None:\n        \"\"\"\n        Print information about folds.\n        \"\"\"\n        print('Information of folds')\n        print('--------------------')\n        if externally_fitted:\n            print(f'An already trained forecaster is to be used. Window size: {self.window_size}')\n        elif self.differentiation is None:\n            print(f'Number of observations used for initial training: {self.initial_train_size}')\n        else:\n            print(f'Number of observations used for initial training: {self.initial_train_size - self.differentiation}')\n            print(f'    First {self.differentiation} observation/s in training sets are used for differentiation')\n        print(f'Number of observations used for backtesting: {len(index) - self.initial_train_size}')\n        print(f'    Number of folds: {len(folds)}')\n        print(f'    Number skipped folds: {len(index_to_skip)} {(index_to_skip if index_to_skip else '')}')\n        print(f'    Number of steps per fold: {self.steps}')\n        print(f'    Number of steps to exclude between last observed data (last window) and predictions (gap): {self.gap}')\n        if last_fold_excluded:\n            print('    Last fold has been excluded because it was incomplete.')\n        if len(folds[-1][3]) < self.steps:\n            print(f'    Last fold only includes {len(folds[-1][3])} observations.')\n        print('')\n        if self.differentiation is None:\n            differentiation = 0\n        else:\n            differentiation = self.differentiation\n        for i, fold in enumerate(folds):\n            is_fold_skipped = i in index_to_skip\n            has_training = fold[-1] if i != 0 else True\n            training_start = index[fold[0][0] + differentiation] if fold[0] is not None else None\n            training_end = index[fold[0][-1]] if fold[0] is not None else None\n            training_length = len(fold[0]) - differentiation if fold[0] is not None else 0\n            validation_start = index[fold[3][0]]\n            validation_end = index[fold[3][-1]]\n            validation_length = len(fold[3])\n            print(f'Fold: {i}')\n            if is_fold_skipped:\n                print('    Fold skipped')\n            elif not externally_fitted and has_training:\n                print(f'    Training:   {training_start} -- {training_end}  (n={training_length})')\n                print(f'    Validation: {validation_start} -- {validation_end}  (n={validation_length})')\n            else:\n                print('    Training:   No training in this fold')\n                print(f'    Validation: {validation_start} -- {validation_end}  (n={validation_length})')\n        print('')",
    "skforecast/model_selection/_utils.py": "from typing import Union, Tuple, Optional, Callable, Generator\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom joblib import cpu_count\nfrom tqdm.auto import tqdm\nfrom sklearn.pipeline import Pipeline\nimport sklearn.linear_model\nfrom sklearn.exceptions import NotFittedError\nfrom ..exceptions import IgnoredArgumentWarning\nfrom ..metrics import add_y_train_argument, _get_metric\nfrom ..utils import check_interval\n\ndef initialize_lags_grid(forecaster: object, lags_grid: Optional[Union[list, dict]]=None) -> Tuple[dict, str]:\n    \"\"\"\n    Initialize lags grid and lags label for model selection. \n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model. ForecasterRecursive, ForecasterDirect, \n        ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate.\n    lags_grid : list, dict, default `None`\n        Lists of lags to try, containing int, lists, numpy ndarray, or range \n        objects. If `dict`, the keys are used as labels in the `results` \n        DataFrame, and the values are used as the lists of lags to try.\n\n    Returns\n    -------\n    lags_grid : dict\n        Dictionary with lags configuration for each iteration.\n    lags_label : str\n        Label for lags representation in the results object.\n\n    \"\"\"\n    if not isinstance(lags_grid, (list, dict, type(None))):\n        raise TypeError(f'`lags_grid` argument must be a list, dict or None. Got {type(lags_grid)}.')\n    lags_label = 'values'\n    if isinstance(lags_grid, list):\n        lags_grid = {f'{lags}': lags for lags in lags_grid}\n    elif lags_grid is None:\n        lags = [int(lag) for lag in forecaster.lags]\n        lags_grid = {f'{lags}': lags}\n    else:\n        lags_label = 'keys'\n    return (lags_grid, lags_label)\n\ndef _calculate_metrics_one_step_ahead(forecaster: object, y: pd.Series, metrics: list, X_train: pd.DataFrame, y_train: Union[pd.Series, dict], X_test: pd.DataFrame, y_test: Union[pd.Series, dict]) -> list:\n    \"\"\"\n    Calculate metrics when predictions are one-step-ahead. When forecaster is\n    of type ForecasterDirect only the regressor for step 1 is used.\n\n    Parameters\n    ----------\n    forecaster : object\n        Forecaster model.\n    y : pandas Series\n        Time series data used to train and test the model.\n    metrics : list\n        List of metrics.\n    X_train : pandas DataFrame\n        Predictor values used to train the model.\n    y_train : pandas Series\n        Target values related to each row of `X_train`.\n    X_test : pandas DataFrame\n        Predictor values used to test the model.\n    y_test : pandas Series\n        Target values related to each row of `X_test`.\n\n    Returns\n    -------\n    metric_values : list\n        List with metric values.\n    \n    \"\"\"\n    if type(forecaster).__name__ == 'ForecasterDirect':\n        step = 1\n        X_train, y_train = forecaster.filter_train_X_y_for_step(step=step, X_train=X_train, y_train=y_train)\n        X_test, y_test = forecaster.filter_train_X_y_for_step(step=step, X_train=X_test, y_train=y_test)\n        forecaster.regressors_[step].fit(X_train, y_train)\n        y_pred = forecaster.regressors_[step].predict(X_test)\n    else:\n        forecaster.regressor.fit(X_train, y_train)\n        y_pred = forecaster.regressor.predict(X_test)\n    y_true = y_test.to_numpy()\n    y_pred = y_pred.ravel()\n    y_train = y_train.to_numpy()\n    if forecaster.differentiation is not None:\n        y_true = forecaster.differentiator.inverse_transform_next_window(y_true)\n        y_pred = forecaster.differentiator.inverse_transform_next_window(y_pred)\n        y_train = forecaster.differentiator.inverse_transform_training(y_train)\n    if forecaster.transformer_y is not None:\n        y_true = forecaster.transformer_y.inverse_transform(y_true.reshape(-1, 1))\n        y_pred = forecaster.transformer_y.inverse_transform(y_pred.reshape(-1, 1))\n        y_train = forecaster.transformer_y.inverse_transform(y_train.reshape(-1, 1))\n    metric_values = []\n    for m in metrics:\n        metric_values.append(m(y_true=y_true.ravel(), y_pred=y_pred.ravel(), y_train=y_train.ravel()))\n    return metric_values\n\ndef _initialize_levels_model_selection_multiseries(forecaster: object, series: Union[pd.DataFrame, dict], levels: Optional[Union[str, list]]=None) -> list:\n    \"\"\"\n    Initialize levels for model_selection multi-series functions.\n\n    Parameters\n    ----------\n    forecaster : ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate, ForecasterRnn\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Training time series.\n    levels : str, list, default `None`\n        level (`str`) or levels (`list`) at which the forecaster is optimized. \n        If `None`, all levels are taken into account. The resulting metric will be\n        the average of the optimization of all levels.\n\n    Returns\n    -------\n    levels : list\n        List of levels to be used in model_selection multi-series functions.\n    \n    \"\"\"\n    multi_series_forecasters_with_levels = ['ForecasterRecursiveMultiSeries', 'ForecasterRnn']\n    if type(forecaster).__name__ in multi_series_forecasters_with_levels and (not isinstance(levels, (str, list, type(None)))):\n        raise TypeError(f'`levels` must be a `list` of column names, a `str` of a column name or `None` when using a forecaster of type {multi_series_forecasters_with_levels}. If the forecaster is of type `ForecasterDirectMultiVariate`, this argument is ignored.')\n    if type(forecaster).__name__ == 'ForecasterDirectMultiVariate':\n        if levels and levels != forecaster.level and (levels != [forecaster.level]):\n            warnings.warn(f\"`levels` argument have no use when the forecaster is of type `ForecasterDirectMultiVariate`. The level of this forecaster is '{forecaster.level}', to predict another level, change the `level` argument when initializing the forecaster. \\n\", IgnoredArgumentWarning)\n        levels = [forecaster.level]\n    elif levels is None:\n        if isinstance(series, pd.DataFrame):\n            levels = list(series.columns)\n        else:\n            levels = list(series.keys())\n    elif isinstance(levels, str):\n        levels = [levels]\n    return levels\n\ndef _extract_data_folds_multiseries(series: Union[pd.Series, pd.DataFrame, dict], folds: list, span_index: Union[pd.DatetimeIndex, pd.RangeIndex], window_size: int, exog: Optional[Union[pd.Series, pd.DataFrame, dict]]=None, dropna_last_window: bool=False, externally_fitted: bool=False) -> Generator[Tuple[Union[pd.Series, pd.DataFrame, dict], pd.DataFrame, list, Optional[Union[pd.Series, pd.DataFrame, dict]], Optional[Union[pd.Series, pd.DataFrame, dict]], list], None, None]:\n    \"\"\"\n    Select the data from series and exog that corresponds to each fold created using the\n    skforecast.model_selection._create_backtesting_folds function.\n\n    Parameters\n    ----------\n    series : pandas Series, pandas DataFrame, dict\n        Time series.\n    folds : list\n        Folds created using the skforecast.model_selection._create_backtesting_folds\n        function.\n    span_index : pandas DatetimeIndex, pandas RangeIndex\n        Full index from the minimum to the maximum index among all series.\n    window_size : int\n        Size of the window needed to create the predictors.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    dropna_last_window : bool, default `False`\n        If `True`, drop the columns of the last window that have NaN values.\n    externally_fitted : bool, default `False`\n        Flag indicating whether the forecaster is already trained. Only used when \n        `initial_train_size` is None and `refit` is False.\n\n    Yield\n    -----\n    series_train : pandas Series, pandas DataFrame, dict\n        Time series corresponding to the training set of the fold.\n    series_last_window: pandas DataFrame\n        Time series corresponding to the last window of the fold.\n    levels_last_window: list\n        Levels of the time series present in the last window of the fold.\n    exog_train: pandas Series, pandas DataFrame, dict, None\n        Exogenous variable corresponding to the training set of the fold.\n    exog_test: pandas Series, pandas DataFrame, dict, None\n        Exogenous variable corresponding to the test set of the fold.\n    fold: list\n        Fold created using the skforecast.model_selection._create_backtesting_folds\n\n    \"\"\"\n    for fold in folds:\n        train_iloc_start = fold[0][0]\n        train_iloc_end = fold[0][1]\n        last_window_iloc_start = fold[1][0]\n        last_window_iloc_end = fold[1][1]\n        test_iloc_start = fold[2][0]\n        test_iloc_end = fold[2][1]\n        if isinstance(series, dict) or isinstance(exog, dict):\n            train_loc_start = span_index[train_iloc_start]\n            train_loc_end = span_index[train_iloc_end - 1]\n            last_window_loc_start = span_index[last_window_iloc_start]\n            last_window_loc_end = span_index[last_window_iloc_end - 1]\n            test_loc_start = span_index[test_iloc_start]\n            test_loc_end = span_index[test_iloc_end - 1]\n        if isinstance(series, pd.DataFrame):\n            series_train = series.iloc[train_iloc_start:train_iloc_end,]\n            series_to_drop = []\n            for col in series_train.columns:\n                if series_train[col].isna().all():\n                    series_to_drop.append(col)\n                else:\n                    first_valid_index = series_train[col].first_valid_index()\n                    last_valid_index = series_train[col].last_valid_index()\n                    if len(series_train[col].loc[first_valid_index:last_valid_index]) < window_size:\n                        series_to_drop.append(col)\n            series_last_window = series.iloc[last_window_iloc_start:last_window_iloc_end,]\n            series_train = series_train.drop(columns=series_to_drop)\n            if not externally_fitted:\n                series_last_window = series_last_window.drop(columns=series_to_drop)\n        else:\n            series_train = {}\n            for k in series.keys():\n                v = series[k].loc[train_loc_start:train_loc_end]\n                if not v.isna().all():\n                    first_valid_index = v.first_valid_index()\n                    last_valid_index = v.last_valid_index()\n                    if first_valid_index is not None and last_valid_index is not None:\n                        v = v.loc[first_valid_index:last_valid_index]\n                        if len(v) >= window_size:\n                            series_train[k] = v\n            series_last_window = {}\n            for k, v in series.items():\n                v = series[k].loc[last_window_loc_start:last_window_loc_end]\n                if (externally_fitted or k in series_train) and len(v) >= window_size:\n                    series_last_window[k] = v\n            series_last_window = pd.DataFrame(series_last_window)\n        if dropna_last_window:\n            series_last_window = series_last_window.dropna(axis=1, how='any')\n        levels_last_window = list(series_last_window.columns)\n        if exog is not None:\n            if isinstance(exog, (pd.Series, pd.DataFrame)):\n                exog_train = exog.iloc[train_iloc_start:train_iloc_end,]\n                exog_test = exog.iloc[test_iloc_start:test_iloc_end,]\n            else:\n                exog_train = {k: v.loc[train_loc_start:train_loc_end] for k, v in exog.items()}\n                exog_train = {k: v for k, v in exog_train.items() if len(v) > 0}\n                exog_test = {k: v.loc[test_loc_start:test_loc_end] for k, v in exog.items() if externally_fitted or k in exog_train}\n                exog_test = {k: v for k, v in exog_test.items() if len(v) > 0}\n        else:\n            exog_train = None\n            exog_test = None\n        yield (series_train, series_last_window, levels_last_window, exog_train, exog_test, fold)\n\ndef _calculate_metrics_backtesting_multiseries(series: Union[pd.DataFrame, dict], predictions: pd.DataFrame, folds: Union[list, tqdm], span_index: Union[pd.DatetimeIndex, pd.RangeIndex], window_size: int, metrics: list, levels: list, add_aggregated_metric: bool=True) -> pd.DataFrame:\n    \"\"\"   \n    Calculate metrics for each level and also for all levels aggregated using\n    average, weighted average or pooling.\n\n    - 'average': the average (arithmetic mean) of all levels.\n    - 'weighted_average': the average of the metrics weighted by the number of\n    predicted values of each level.\n    - 'pooling': the values of all levels are pooled and then the metric is\n    calculated.\n\n    Parameters\n    ----------\n    series : pandas DataFrame, dict\n        Series data used for backtesting.\n    predictions : pandas DataFrame\n        Predictions generated during the backtesting process.\n    folds : list, tqdm\n        Folds created during the backtesting process.\n    span_index : pandas DatetimeIndex, pandas RangeIndex\n        Full index from the minimum to the maximum index among all series.\n    window_size : int\n        Size of the window used by the forecaster to create the predictors.\n        This is used remove the first `window_size` (differentiation included) \n        values from y_train since they are not part of the training matrix.\n    metrics : list\n        List of metrics to calculate.\n    levels : list\n        Levels to calculate the metrics.\n    add_aggregated_metric : bool, default `True`\n        If `True`, and multiple series (`levels`) are predicted, the aggregated\n        metrics (average, weighted average and pooled) are also returned.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s).\n    \n    \"\"\"\n    if not isinstance(series, (pd.DataFrame, dict)):\n        raise TypeError('`series` must be a pandas DataFrame or a dictionary of pandas DataFrames.')\n    if not isinstance(predictions, pd.DataFrame):\n        raise TypeError('`predictions` must be a pandas DataFrame.')\n    if not isinstance(folds, (list, tqdm)):\n        raise TypeError('`folds` must be a list or a tqdm object.')\n    if not isinstance(span_index, (pd.DatetimeIndex, pd.RangeIndex)):\n        raise TypeError('`span_index` must be a pandas DatetimeIndex or pandas RangeIndex.')\n    if not isinstance(window_size, (int, np.integer)):\n        raise TypeError('`window_size` must be an integer.')\n    if not isinstance(metrics, list):\n        raise TypeError('`metrics` must be a list.')\n    if not isinstance(levels, list):\n        raise TypeError('`levels` must be a list.')\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError('`add_aggregated_metric` must be a boolean.')\n    metric_names = [m if isinstance(m, str) else m.__name__ for m in metrics]\n    y_true_pred_levels = []\n    y_train_levels = []\n    for level in levels:\n        y_true_pred_level = None\n        y_train = None\n        if level in predictions.columns:\n            y_true_pred_level = pd.merge(series[level], predictions[level], left_index=True, right_index=True, how='inner').dropna(axis=0, how='any')\n            y_true_pred_level.columns = ['y_true', 'y_pred']\n            train_indexes = []\n            for i, fold in enumerate(folds):\n                fit_fold = fold[-1]\n                if i == 0 or fit_fold:\n                    train_iloc_start = fold[0][0]\n                    train_iloc_end = fold[0][1]\n                    train_indexes.append(np.arange(train_iloc_start, train_iloc_end))\n            train_indexes = np.unique(np.concatenate(train_indexes))\n            train_indexes = span_index[train_indexes]\n            y_train = series[level].loc[series[level].index.intersection(train_indexes)]\n        y_true_pred_levels.append(y_true_pred_level)\n        y_train_levels.append(y_train)\n    metrics_levels = []\n    for i, level in enumerate(levels):\n        if y_true_pred_levels[i] is not None and (not y_true_pred_levels[i].empty):\n            metrics_level = [m(y_true=y_true_pred_levels[i].iloc[:, 0], y_pred=y_true_pred_levels[i].iloc[:, 1], y_train=y_train_levels[i].iloc[window_size:]) for m in metrics]\n            metrics_levels.append(metrics_level)\n        else:\n            metrics_levels.append([None for _ in metrics])\n    metrics_levels = pd.DataFrame(data=metrics_levels, columns=[m if isinstance(m, str) else m.__name__ for m in metrics])\n    metrics_levels.insert(0, 'levels', levels)\n    if len(levels) < 2:\n        add_aggregated_metric = False\n    if add_aggregated_metric:\n        average = metrics_levels.drop(columns='levels').mean(skipna=True)\n        average = average.to_frame().transpose()\n        average['levels'] = 'average'\n        weighted_averages = {}\n        n_predictions_levels = predictions.notna().sum().to_frame(name='n_predictions').reset_index(names='levels')\n        metrics_levels_no_missing = metrics_levels.merge(n_predictions_levels, on='levels', how='inner')\n        for col in metric_names:\n            weighted_averages[col] = np.average(metrics_levels_no_missing[col], weights=metrics_levels_no_missing['n_predictions'])\n        weighted_average = pd.DataFrame(weighted_averages, index=[0])\n        weighted_average['levels'] = 'weighted_average'\n        y_true_pred_levels, y_train_levels = zip(*[(a, b.iloc[window_size:]) for a, b in zip(y_true_pred_levels, y_train_levels) if a is not None])\n        y_train_levels = list(y_train_levels)\n        y_true_pred_levels = pd.concat(y_true_pred_levels)\n        y_train_levels_concat = pd.concat(y_train_levels)\n        pooled = []\n        for m, m_name in zip(metrics, metric_names):\n            if m_name in ['mean_absolute_scaled_error', 'root_mean_squared_scaled_error']:\n                pooled.append(m(y_true=y_true_pred_levels.loc[:, 'y_true'], y_pred=y_true_pred_levels.loc[:, 'y_pred'], y_train=y_train_levels))\n            else:\n                pooled.append(m(y_true=y_true_pred_levels.loc[:, 'y_true'], y_pred=y_true_pred_levels.loc[:, 'y_pred'], y_train=y_train_levels_concat))\n        pooled = pd.DataFrame([pooled], columns=metric_names)\n        pooled['levels'] = 'pooling'\n        metrics_levels = pd.concat([metrics_levels, average, weighted_average, pooled], axis=0, ignore_index=True)\n    return metrics_levels\n\ndef _predict_and_calculate_metrics_one_step_ahead_multiseries(forecaster: object, series: Union[pd.DataFrame, dict], X_train: pd.DataFrame, y_train: Union[pd.Series, dict], X_test: pd.DataFrame, y_test: Union[pd.Series, dict], X_train_encoding: pd.Series, X_test_encoding: pd.Series, levels: list, metrics: list, add_aggregated_metric: bool=True) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"   \n    One-step-ahead predictions and metrics for each level and also for all levels\n    aggregated using average, weighted average or pooling.\n    Input matrices (X_train, y_train, X_train_encoding, X_test, y_test, X_test_encoding)\n    should have been generated using the forecaster._train_test_split_one_step_ahead().\n\n    - 'average': the average (arithmetic mean) of all levels.\n    - 'weighted_average': the average of the metrics weighted by the number of\n    predicted values of each level.\n    - 'pooling': the values of all levels are pooled and then the metric is\n    calculated.\n\n    Parameters\n    ----------\n    forecaster : object\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Series data used to train and test the forecaster.\n    X_train : pandas DataFrame\n        Training matrix.\n    y_train : pandas Series, dict\n        Target values of the training set.\n    X_test : pandas DataFrame\n        Test matrix.\n    y_test : pandas Series, dict\n        Target values of the test set.\n    X_train_encoding : pandas Series\n        Series identifiers for each row of `X_train`.\n    X_test_encoding : pandas Series\n        Series identifiers for each row of `X_test`.\n    levels : list\n        Levels to calculate the metrics.\n    metrics : list\n        List of metrics to calculate.\n    add_aggregated_metric : bool, default `True`\n        If `True`, and multiple series (`levels`) are predicted, the aggregated\n        metrics (average, weighted average and pooled) are also returned.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s).\n    predictions : pandas DataFrame\n        Value of predictions for each level.\n    \n    \"\"\"\n    if not isinstance(series, (pd.DataFrame, dict)):\n        raise TypeError('`series` must be a pandas DataFrame or a dictionary of pandas DataFrames.')\n    if not isinstance(X_train, pd.DataFrame):\n        raise TypeError(f'`X_train` must be a pandas DataFrame. Got: {type(X_train)}')\n    if not isinstance(y_train, (pd.Series, dict)):\n        raise TypeError(f'`y_train` must be a pandas Series or a dictionary of pandas Series. Got: {type(y_train)}')\n    if not isinstance(X_test, pd.DataFrame):\n        raise TypeError(f'`X_test` must be a pandas DataFrame. Got: {type(X_test)}')\n    if not isinstance(y_test, (pd.Series, dict)):\n        raise TypeError(f'`y_test` must be a pandas Series or a dictionary of pandas Series. Got: {type(y_test)}')\n    if not isinstance(X_train_encoding, pd.Series):\n        raise TypeError(f'`X_train_encoding` must be a pandas Series. Got: {type(X_train_encoding)}')\n    if not isinstance(X_test_encoding, pd.Series):\n        raise TypeError(f'`X_test_encoding` must be a pandas Series. Got: {type(X_test_encoding)}')\n    if not isinstance(levels, list):\n        raise TypeError(f'`levels` must be a list. Got: {type(levels)}')\n    if not isinstance(metrics, list):\n        raise TypeError(f'`metrics` must be a list. Got: {type(metrics)}')\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError(f'`add_aggregated_metric` must be a boolean. Got: {type(add_aggregated_metric)}')\n    metrics = [_get_metric(metric=m) if isinstance(m, str) else add_y_train_argument(m) for m in metrics]\n    metric_names = [m if isinstance(m, str) else m.__name__ for m in metrics]\n    if isinstance(series[levels[0]].index, pd.DatetimeIndex):\n        freq = series[levels[0]].index.freq\n    else:\n        freq = series[levels[0]].index.step\n    if type(forecaster).__name__ == 'ForecasterDirectMultiVariate':\n        step = 1\n        X_train, y_train = forecaster.filter_train_X_y_for_step(step=step, X_train=X_train, y_train=y_train)\n        X_test, y_test = forecaster.filter_train_X_y_for_step(step=step, X_train=X_test, y_train=y_test)\n        forecaster.regressors_[step].fit(X_train, y_train)\n        pred = forecaster.regressors_[step].predict(X_test)\n    else:\n        forecaster.regressor.fit(X_train, y_train)\n        pred = forecaster.regressor.predict(X_test)\n    predictions_per_level = pd.DataFrame({'y_true': y_test, 'y_pred': pred, '_level_skforecast': X_test_encoding}, index=y_test.index).groupby('_level_skforecast')\n    predictions_per_level = {key: group for key, group in predictions_per_level}\n    y_train_per_level = pd.DataFrame({'y_train': y_train, '_level_skforecast': X_train_encoding}, index=y_train.index).groupby('_level_skforecast')\n    y_train_per_level = {key: group.asfreq(freq) for key, group in y_train_per_level}\n    if forecaster.differentiation is not None:\n        for level in predictions_per_level:\n            predictions_per_level[level]['y_true'] = forecaster.differentiator_[level].inverse_transform_next_window(predictions_per_level[level]['y_true'].to_numpy())\n            predictions_per_level[level]['y_pred'] = forecaster.differentiator_[level].inverse_transform_next_window(predictions_per_level[level]['y_pred'].to_numpy())\n            y_train_per_level[level]['y_train'] = forecaster.differentiator_[level].inverse_transform_training(y_train_per_level[level]['y_train'].to_numpy())\n    if forecaster.transformer_series is not None:\n        for level in predictions_per_level:\n            transformer = forecaster.transformer_series_[level]\n            predictions_per_level[level]['y_true'] = transformer.inverse_transform(predictions_per_level[level][['y_true']])\n            predictions_per_level[level]['y_pred'] = transformer.inverse_transform(predictions_per_level[level][['y_pred']])\n            y_train_per_level[level]['y_train'] = transformer.inverse_transform(y_train_per_level[level][['y_train']])\n    metrics_levels = []\n    for level in levels:\n        if level in predictions_per_level:\n            metrics_level = [m(y_true=predictions_per_level[level].loc[:, 'y_true'], y_pred=predictions_per_level[level].loc[:, 'y_pred'], y_train=y_train_per_level[level].loc[:, 'y_train']) for m in metrics]\n            metrics_levels.append(metrics_level)\n        else:\n            metrics_levels.append([None for _ in metrics])\n    metrics_levels = pd.DataFrame(data=metrics_levels, columns=[m if isinstance(m, str) else m.__name__ for m in metrics])\n    metrics_levels.insert(0, 'levels', levels)\n    if len(levels) < 2:\n        add_aggregated_metric = False\n    if add_aggregated_metric:\n        average = metrics_levels.drop(columns='levels').mean(skipna=True)\n        average = average.to_frame().transpose()\n        average['levels'] = 'average'\n        weighted_averages = {}\n        n_predictions_levels = {k: v['y_pred'].notna().sum() for k, v in predictions_per_level.items()}\n        n_predictions_levels = pd.DataFrame(n_predictions_levels.items(), columns=['levels', 'n_predictions'])\n        metrics_levels_no_missing = metrics_levels.merge(n_predictions_levels, on='levels', how='inner')\n        for col in metric_names:\n            weighted_averages[col] = np.average(metrics_levels_no_missing[col], weights=metrics_levels_no_missing['n_predictions'])\n        weighted_average = pd.DataFrame(weighted_averages, index=[0])\n        weighted_average['levels'] = 'weighted_average'\n        list_y_train_by_level = [v['y_train'].to_numpy() for k, v in y_train_per_level.items() if k in predictions_per_level]\n        predictions_pooled = pd.concat(predictions_per_level.values())\n        y_train_pooled = pd.concat([v for k, v in y_train_per_level.items() if k in predictions_per_level])\n        pooled = []\n        for m, m_name in zip(metrics, metric_names):\n            if m_name in ['mean_absolute_scaled_error', 'root_mean_squared_scaled_error']:\n                pooled.append(m(y_true=predictions_pooled['y_true'], y_pred=predictions_pooled['y_pred'], y_train=list_y_train_by_level))\n            else:\n                pooled.append(m(y_true=predictions_pooled['y_true'], y_pred=predictions_pooled['y_pred'], y_train=y_train_pooled['y_train']))\n        pooled = pd.DataFrame([pooled], columns=metric_names)\n        pooled['levels'] = 'pooling'\n        metrics_levels = pd.concat([metrics_levels, average, weighted_average, pooled], axis=0, ignore_index=True)\n    predictions = pd.concat(predictions_per_level.values()).loc[:, ['y_pred', '_level_skforecast']].pivot(columns='_level_skforecast', values='y_pred').rename_axis(columns=None, index=None)\n    predictions = predictions.asfreq(X_test.index.freq)\n    return (metrics_levels, predictions)",
    "skforecast/utils/utils.py": "import importlib\nimport inspect\nimport warnings\nfrom copy import deepcopy\nfrom typing import Any, Callable, Optional, Tuple, Union\nfrom pathlib import Path\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport sklearn.linear_model\nfrom sklearn.base import clone\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.exceptions import NotFittedError\nimport skforecast\nfrom ..exceptions import warn_skforecast_categories\nfrom ..exceptions import MissingValuesWarning, MissingExogWarning, DataTypeWarning, UnknownLevelWarning, IgnoredArgumentWarning, SaveLoadSkforecastWarning, SkforecastVersionWarning\noptional_dependencies = {'sarimax': ['statsmodels>=0.12, <0.15'], 'deeplearning': ['matplotlib>=3.3, <3.10', 'keras>=2.6, <4.0'], 'plotting': ['matplotlib>=3.3, <3.10', 'seaborn>=0.11, <0.14', 'statsmodels>=0.12, <0.15']}\n\ndef initialize_lags(forecaster_name: str, lags: Any) -> Union[Optional[np.ndarray], Optional[list], Optional[int]]:\n    \"\"\"\n    Check lags argument input and generate the corresponding numpy ndarray.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    lags : Any\n        Lags used as predictors.\n\n    Returns\n    -------\n    lags : numpy ndarray, None\n        Lags used as predictors.\n    lags_names : list, None\n        Names of the lags used as predictors.\n    max_lag : int, None\n        Maximum value of the lags.\n    \n    \"\"\"\n    lags_names = None\n    max_lag = None\n    if lags is not None:\n        if isinstance(lags, int):\n            if lags < 1:\n                raise ValueError('Minimum value of lags allowed is 1.')\n            lags = np.arange(1, lags + 1)\n        if isinstance(lags, (list, tuple, range)):\n            lags = np.array(lags)\n        if isinstance(lags, np.ndarray):\n            if lags.size == 0:\n                return (None, None, None)\n            if lags.ndim != 1:\n                raise ValueError('`lags` must be a 1-dimensional array.')\n            if not np.issubdtype(lags.dtype, np.integer):\n                raise TypeError('All values in `lags` must be integers.')\n            if np.any(lags < 1):\n                raise ValueError('Minimum value of lags allowed is 1.')\n        elif forecaster_name != 'ForecasterDirectMultiVariate':\n            raise TypeError(f'`lags` argument must be an int, 1d numpy ndarray, range, tuple or list. Got {type(lags)}.')\n        else:\n            raise TypeError(f'`lags` argument must be a dict, int, 1d numpy ndarray, range, tuple or list. Got {type(lags)}.')\n        lags_names = [f'lag_{i}' for i in lags]\n        max_lag = max(lags)\n    return (lags, lags_names, max_lag)\n\ndef initialize_window_features(window_features: Any) -> Union[Optional[list], Optional[list], Optional[int]]:\n    \"\"\"\n    Check window_features argument input and generate the corresponding list.\n\n    Parameters\n    ----------\n    window_features : Any\n        Classes used to create window features.\n\n    Returns\n    -------\n    window_features : list, None\n        List of classes used to create window features.\n    window_features_names : list, None\n        List with all the features names of the window features.\n    max_size_window_features : int, None\n        Maximum value of the `window_sizes` attribute of all classes.\n    \n    \"\"\"\n    needed_atts = ['window_sizes', 'features_names']\n    needed_methods = ['transform_batch', 'transform']\n    max_window_sizes = None\n    window_features_names = None\n    max_size_window_features = None\n    if window_features is not None:\n        if isinstance(window_features, list) and len(window_features) < 1:\n            raise ValueError('Argument `window_features` must contain at least one element.')\n        if not isinstance(window_features, list):\n            window_features = [window_features]\n        link_to_docs = '\\nVisit the documentation for more information about how to create custom window features:\\nhttps://skforecast.org/latest/user_guides/window-features-and-custom-features.html#create-your-custom-window-features'\n        max_window_sizes = []\n        window_features_names = []\n        for wf in window_features:\n            wf_name = type(wf).__name__\n            atts_methods = set([a for a in dir(wf)])\n            if not set(needed_atts).issubset(atts_methods):\n                raise ValueError(f'{wf_name} must have the attributes: {needed_atts}.' + link_to_docs)\n            if not set(needed_methods).issubset(atts_methods):\n                raise ValueError(f'{wf_name} must have the methods: {needed_methods}.' + link_to_docs)\n            window_sizes = wf.window_sizes\n            if not isinstance(window_sizes, (int, list)):\n                raise TypeError(f'Attribute `window_sizes` of {wf_name} must be an int or a list of ints. Got {type(window_sizes)}.' + link_to_docs)\n            if isinstance(window_sizes, int):\n                if window_sizes < 1:\n                    raise ValueError(f'If argument `window_sizes` is an integer, it must be equal to or greater than 1. Got {window_sizes} from {wf_name}.' + link_to_docs)\n                max_window_sizes.append(window_sizes)\n            else:\n                if not all((isinstance(ws, int) for ws in window_sizes)) or not all((ws >= 1 for ws in window_sizes)):\n                    raise ValueError(f'If argument `window_sizes` is a list, all elements must be integers equal to or greater than 1. Got {window_sizes} from {wf_name}.' + link_to_docs)\n                max_window_sizes.append(max(window_sizes))\n            features_names = wf.features_names\n            if not isinstance(features_names, (str, list)):\n                raise TypeError(f'Attribute `features_names` of {wf_name} must be a str or a list of strings. Got {type(features_names)}.' + link_to_docs)\n            if isinstance(features_names, str):\n                window_features_names.append(features_names)\n            else:\n                if not all((isinstance(fn, str) for fn in features_names)):\n                    raise TypeError(f'If argument `features_names` is a list, all elements must be strings. Got {features_names} from {wf_name}.' + link_to_docs)\n                window_features_names.extend(features_names)\n        max_size_window_features = max(max_window_sizes)\n        if len(set(window_features_names)) != len(window_features_names):\n            raise ValueError(f'All window features names must be unique. Got {window_features_names}.')\n    return (window_features, window_features_names, max_size_window_features)\n\ndef initialize_weights(forecaster_name: str, regressor: object, weight_func: Union[Callable, dict], series_weights: dict) -> Tuple[Union[Callable, dict], Union[str, dict], dict]:\n    \"\"\"\n    Check weights arguments, `weight_func` and `series_weights` for the different \n    forecasters. Create `source_code_weight_func`, source code of the custom \n    function(s) used to create weights.\n    \n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        Regressor of the forecaster.\n    weight_func : Callable, dict\n        Argument `weight_func` of the forecaster.\n    series_weights : dict\n        Argument `series_weights` of the forecaster.\n\n    Returns\n    -------\n    weight_func : Callable, dict\n        Argument `weight_func` of the forecaster.\n    source_code_weight_func : str, dict\n        Argument `source_code_weight_func` of the forecaster.\n    series_weights : dict\n        Argument `series_weights` of the forecaster.\n    \n    \"\"\"\n    source_code_weight_func = None\n    if weight_func is not None:\n        if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n            if not isinstance(weight_func, (Callable, dict)):\n                raise TypeError(f'Argument `weight_func` must be a Callable or a dict of Callables. Got {type(weight_func)}.')\n        elif not isinstance(weight_func, Callable):\n            raise TypeError(f'Argument `weight_func` must be a Callable. Got {type(weight_func)}.')\n        if isinstance(weight_func, dict):\n            source_code_weight_func = {}\n            for key in weight_func:\n                source_code_weight_func[key] = inspect.getsource(weight_func[key])\n        else:\n            source_code_weight_func = inspect.getsource(weight_func)\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(f'Argument `weight_func` is ignored since regressor {regressor} does not accept `sample_weight` in its `fit` method.', IgnoredArgumentWarning)\n            weight_func = None\n            source_code_weight_func = None\n    if series_weights is not None:\n        if not isinstance(series_weights, dict):\n            raise TypeError(f'Argument `series_weights` must be a dict of floats or ints.Got {type(series_weights)}.')\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(f'Argument `series_weights` is ignored since regressor {regressor} does not accept `sample_weight` in its `fit` method.', IgnoredArgumentWarning)\n            series_weights = None\n    return (weight_func, source_code_weight_func, series_weights)\n\ndef initialize_transformer_series(forecaster_name: str, series_names_in_: list, encoding: Optional[str]=None, transformer_series: Optional[Union[object, dict]]=None) -> dict:\n    \"\"\"\n    Initialize `transformer_series_` attribute for the Forecasters Multiseries.\n\n    - If `transformer_series` is `None`, no transformation is applied.\n    - If `transformer_series` is a scikit-learn transformer (object), the same \n    transformer is applied to all series (`series_names_in_`).\n    - If `transformer_series` is a `dict`, a different transformer can be\n    applied to each series. The keys of the dictionary must be the same as the\n    names of the series in `series_names_in_`.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    series_names_in_ : list\n        Names of the series (levels) used during training.\n    encoding : str, default `None`\n        Encoding used to identify the different series (`ForecasterRecursiveMultiSeries`).\n    transformer_series : object, dict, default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and \n        inverse_transform. \n\n    Returns\n    -------\n    transformer_series_ : dict\n        Dictionary with the transformer for each series. It is created cloning the \n        objects in `transformer_series` and is used internally to avoid overwriting.\n    \n    \"\"\"\n    multiseries_forecasters = ['ForecasterRecursiveMultiSeries']\n    if forecaster_name in multiseries_forecasters:\n        if encoding is None:\n            series_names_in_ = ['_unknown_level']\n        else:\n            series_names_in_ = series_names_in_ + ['_unknown_level']\n    if transformer_series is None:\n        transformer_series_ = {serie: None for serie in series_names_in_}\n    elif not isinstance(transformer_series, dict):\n        transformer_series_ = {serie: clone(transformer_series) for serie in series_names_in_}\n    else:\n        transformer_series_ = {serie: None for serie in series_names_in_}\n        transformer_series_.update(((k, v) for k, v in deepcopy(transformer_series).items() if k in transformer_series_))\n        series_not_in_transformer_series = set(series_names_in_) - set(transformer_series.keys()) - {'_unknown_level'}\n        if series_not_in_transformer_series:\n            warnings.warn(f'{series_not_in_transformer_series} not present in `transformer_series`. No transformation is applied to these series.', IgnoredArgumentWarning)\n    return transformer_series_\n\ndef check_select_fit_kwargs(regressor: object, fit_kwargs: Optional[dict]=None) -> dict:\n    \"\"\"\n    Check if `fit_kwargs` is a dict and select only the keys that are used by\n    the `fit` method of the regressor.\n\n    Parameters\n    ----------\n    regressor : object\n        Regressor object.\n    fit_kwargs : dict, default `None`\n        Dictionary with the arguments to pass to the `fit' method of the forecaster.\n\n    Returns\n    -------\n    fit_kwargs : dict\n        Dictionary with the arguments to be passed to the `fit` method of the \n        regressor after removing the unused keys.\n    \n    \"\"\"\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    else:\n        if not isinstance(fit_kwargs, dict):\n            raise TypeError(f'Argument `fit_kwargs` must be a dict. Got {type(fit_kwargs)}.')\n        non_used_keys = [k for k in fit_kwargs.keys() if k not in inspect.signature(regressor.fit).parameters]\n        if non_used_keys:\n            warnings.warn(f\"Argument/s {non_used_keys} ignored since they are not used by the regressor's `fit` method.\", IgnoredArgumentWarning)\n        if 'sample_weight' in fit_kwargs.keys():\n            warnings.warn('The `sample_weight` argument is ignored. Use `weight_func` to pass a function that defines the individual weights for each sample based on its index.', IgnoredArgumentWarning)\n            del fit_kwargs['sample_weight']\n        fit_kwargs = {k: v for k, v in fit_kwargs.items() if k in inspect.signature(regressor.fit).parameters}\n    return fit_kwargs\n\ndef check_y(y: Any, series_id: str='`y`') -> None:\n    \"\"\"\n    Raise Exception if `y` is not pandas Series or if it has missing values.\n    \n    Parameters\n    ----------\n    y : Any\n        Time series values.\n    series_id : str, default '`y`'\n        Identifier of the series used in the warning message.\n    \n    Returns\n    -------\n    None\n    \n    \"\"\"\n    if not isinstance(y, pd.Series):\n        raise TypeError(f'{series_id} must be a pandas Series.')\n    if y.isnull().any():\n        raise ValueError(f'{series_id} has missing values.')\n    return\n\ndef check_exog(exog: Union[pd.Series, pd.DataFrame], allow_nan: bool=True, series_id: str='`exog`') -> None:\n    \"\"\"\n    Raise Exception if `exog` is not pandas Series or pandas DataFrame.\n    If `allow_nan = True`, issue a warning if `exog` contains NaN values.\n    \n    Parameters\n    ----------\n    exog : pandas DataFrame, pandas Series\n        Exogenous variable/s included as predictor/s.\n    allow_nan : bool, default `True`\n        If True, allows the presence of NaN values in `exog`. If False (default),\n        issue a warning if `exog` contains NaN values.\n    series_id : str, default '`exog`'\n        Identifier of the series for which the exogenous variable/s are used\n        in the warning message.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(f'{series_id} must be a pandas Series or DataFrame. Got {type(exog)}.')\n    if isinstance(exog, pd.Series) and exog.name is None:\n        raise ValueError(f'When {series_id} is a pandas Series, it must have a name.')\n    if not allow_nan:\n        if exog.isnull().any().any():\n            warnings.warn(f'{series_id} has missing values. Most machine learning models do not allow missing values. Fitting the forecaster may fail.', MissingValuesWarning)\n    return\n\ndef get_exog_dtypes(exog: Union[pd.DataFrame, pd.Series]) -> dict:\n    \"\"\"\n    Store dtypes of `exog`.\n\n    Parameters\n    ----------\n    exog : pandas DataFrame, pandas Series\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    exog_dtypes : dict\n        Dictionary with the dtypes in `exog`.\n    \n    \"\"\"\n    if isinstance(exog, pd.Series):\n        exog_dtypes = {exog.name: exog.dtypes}\n    else:\n        exog_dtypes = exog.dtypes.to_dict()\n    return exog_dtypes\n\ndef check_exog_dtypes(exog: Union[pd.DataFrame, pd.Series], call_check_exog: bool=True, series_id: str='`exog`') -> None:\n    \"\"\"\n    Raise Exception if `exog` has categorical columns with non integer values.\n    This is needed when using machine learning regressors that allow categorical\n    features.\n    Issue a Warning if `exog` has columns that are not `init`, `float`, or `category`.\n    \n    Parameters\n    ----------\n    exog : pandas DataFrame, pandas Series\n        Exogenous variable/s included as predictor/s.\n    call_check_exog : bool, default `True`\n        If `True`, call `check_exog` function.\n    series_id : str, default '`exog`'\n        Identifier of the series for which the exogenous variable/s are used\n        in the warning message.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    if call_check_exog:\n        check_exog(exog=exog, allow_nan=False, series_id=series_id)\n    if isinstance(exog, pd.DataFrame):\n        if not exog.select_dtypes(exclude=[np.number, 'category']).columns.empty:\n            warnings.warn(f'{series_id} may contain only `int`, `float` or `category` dtypes. Most machine learning models do not allow other types of values. Fitting the forecaster may fail.', DataTypeWarning)\n        for col in exog.select_dtypes(include='category'):\n            if exog[col].cat.categories.dtype not in [int, np.int32, np.int64]:\n                raise TypeError('Categorical dtypes in exog must contain only integer values. See skforecast docs for more info about how to include categorical features https://skforecast.org/latest/user_guides/categorical-features.html')\n    else:\n        if exog.dtype.name not in ['int', 'int8', 'int16', 'int32', 'int64', 'float', 'float16', 'float32', 'float64', 'uint8', 'uint16', 'uint32', 'uint64', 'category']:\n            warnings.warn(f'{series_id} may contain only `int`, `float` or `category` dtypes. Most machine learning models do not allow other types of values. Fitting the forecaster may fail.', DataTypeWarning)\n        if exog.dtype.name == 'category' and exog.cat.categories.dtype not in [int, np.int32, np.int64]:\n            raise TypeError('Categorical dtypes in exog must contain only integer values. See skforecast docs for more info about how to include categorical features https://skforecast.org/latest/user_guides/categorical-features.html')\n    return\n\ndef check_interval(interval: list=None, quantiles: float=None, alpha: float=None) -> None:\n    \"\"\"\n    Check provided confidence interval sequence is valid.\n\n    Parameters\n    ----------\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. For example, \n        interval of 95% should be as `interval = [2.5, 97.5]`.\n    quantiles : list, default `None`\n        Sequence of quantiles to compute, which must be between 0 and 1 \n        inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n        `quantiles = [0.05, 0.5, 0.95]`.\n    alpha : float, default `None`\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %.\n\n    Returns\n    -------\n    None\n    \n    \"\"\"\n    if interval is not None:\n        if not isinstance(interval, list):\n            raise TypeError('`interval` must be a `list`. For example, interval of 95% should be as `interval = [2.5, 97.5]`.')\n        if len(interval) != 2:\n            raise ValueError('`interval` must contain exactly 2 values, respectively the lower and upper interval bounds. For example, interval of 95% should be as `interval = [2.5, 97.5]`.')\n        if interval[0] < 0.0 or interval[0] >= 100.0:\n            raise ValueError(f'Lower interval bound ({interval[0]}) must be >= 0 and < 100.')\n        if interval[1] <= 0.0 or interval[1] > 100.0:\n            raise ValueError(f'Upper interval bound ({interval[1]}) must be > 0 and <= 100.')\n        if interval[0] >= interval[1]:\n            raise ValueError(f'Lower interval bound ({interval[0]}) must be less than the upper interval bound ({interval[1]}).')\n    if quantiles is not None:\n        if not isinstance(quantiles, list):\n            raise TypeError('`quantiles` must be a `list`. For example, quantiles 0.05, 0.5, and 0.95 should be as `quantiles = [0.05, 0.5, 0.95]`.')\n        for q in quantiles:\n            if q < 0.0 or q > 1.0:\n                raise ValueError('All elements in `quantiles` must be >= 0 and <= 1.')\n    if alpha is not None:\n        if not isinstance(alpha, float):\n            raise TypeError('`alpha` must be a `float`. For example, interval of 95% should be as `alpha = 0.05`.')\n        if alpha <= 0.0 or alpha >= 1:\n            raise ValueError(f'`alpha` must have a value between 0 and 1. Got {alpha}.')\n    return\n\ndef preprocess_exog(exog: Union[pd.Series, pd.DataFrame], return_values: bool=True) -> Tuple[Union[None, np.ndarray], pd.Index]:\n    \"\"\"\n    Return values and index of series or data frame separately. Index is\n    overwritten  according to the next rules:\n    \n    - If index is of type `DatetimeIndex` and has frequency, nothing is \n    changed.\n    - If index is of type `RangeIndex`, nothing is changed.\n    - If index is of type `DatetimeIndex` but has no frequency, a \n    `RangeIndex` is created.\n    - If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\n\n    Parameters\n    ----------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables.\n    return_values : bool, default `True`\n        If `True` return the values of `exog` as numpy ndarray. This option is \n        intended to avoid copying data when it is not necessary.\n\n    Returns\n    -------\n    exog_values : None, numpy ndarray\n        Numpy array with values of `exog`.\n    exog_index : pandas Index\n        Index of `exog` modified according to the rules.\n    \n    \"\"\"\n    if isinstance(exog.index, pd.DatetimeIndex) and exog.index.freq is not None:\n        exog_index = exog.index\n    elif isinstance(exog.index, pd.RangeIndex):\n        exog_index = exog.index\n    elif isinstance(exog.index, pd.DatetimeIndex) and exog.index.freq is None:\n        warnings.warn('`exog` has DatetimeIndex index but no frequency. Index is overwritten with a RangeIndex of step 1.')\n        exog_index = pd.RangeIndex(start=0, stop=len(exog), step=1)\n    else:\n        warnings.warn('`exog` has no DatetimeIndex nor RangeIndex index. Index is overwritten with a RangeIndex.')\n        exog_index = pd.RangeIndex(start=0, stop=len(exog), step=1)\n    exog_values = exog.to_numpy(copy=True) if return_values else None\n    return (exog_values, exog_index)\n\ndef input_to_frame(data: Union[pd.Series, pd.DataFrame], input_name: str) -> pd.DataFrame:\n    \"\"\"\n    Convert data to a pandas DataFrame. If data is a pandas Series, it is \n    converted to a DataFrame with a single column. If data is a DataFrame, \n    it is returned as is.\n\n    Parameters\n    ----------\n    data : pandas Series, pandas DataFrame\n        Input data.\n    input_name : str\n        Name of the input data. Accepted values are 'y', 'last_window' and 'exog'.\n\n    Returns\n    -------\n    data : pandas DataFrame\n        Input data as a DataFrame.\n\n    \"\"\"\n    output_col_name = {'y': 'y', 'last_window': 'y', 'exog': 'exog'}\n    if isinstance(data, pd.Series):\n        data = data.to_frame(name=data.name if data.name is not None else output_col_name[input_name])\n    return data\n\ndef cast_exog_dtypes(exog: Union[pd.Series, pd.DataFrame], exog_dtypes: dict) -> Union[pd.Series, pd.DataFrame]:\n    \"\"\"\n    Cast `exog` to a specified types. This is done because, for a forecaster to \n    accept a categorical exog, it must contain only integer values. Due to the \n    internal modifications of numpy, the values may be casted to `float`, so \n    they have to be re-converted to `int`.\n\n    - If `exog` is a pandas Series, `exog_dtypes` must be a dict with a \n    single value.\n    - If `exog_dtypes` is `category` but the current type of `exog` is `float`, \n    then the type is cast to `int` and then to `category`. \n\n    Parameters\n    ----------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables.\n    exog_dtypes: dict\n        Dictionary with name and type of the series or data frame columns.\n\n    Returns\n    -------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables casted to the indicated dtypes.\n\n    \"\"\"\n    exog_dtypes = {k: v for k, v in exog_dtypes.items() if k in exog.columns}\n    if isinstance(exog, pd.Series) and exog.dtypes != list(exog_dtypes.values())[0]:\n        exog = exog.astype(list(exog_dtypes.values())[0])\n    elif isinstance(exog, pd.DataFrame):\n        for col, initial_dtype in exog_dtypes.items():\n            if exog[col].dtypes != initial_dtype:\n                if initial_dtype == 'category' and exog[col].dtypes == float:\n                    exog[col] = exog[col].astype(int).astype('category')\n                else:\n                    exog[col] = exog[col].astype(initial_dtype)\n    return exog\n\ndef exog_to_direct(exog: Union[pd.Series, pd.DataFrame], steps: int) -> Union[pd.DataFrame, list]:\n    \"\"\"\n    Transforms `exog` to a pandas DataFrame with the shape needed for Direct\n    forecasting.\n    \n    Parameters\n    ----------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables.\n    steps : int\n        Number of steps that will be predicted using exog.\n\n    Returns\n    -------\n    exog_direct : pandas DataFrame\n        Exogenous variables transformed.\n    exog_direct_names : list\n        Names of the columns of the exogenous variables transformed. Only \n        created if `exog` is a pandas Series or DataFrame.\n    \n    \"\"\"\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(f'`exog` must be a pandas Series or DataFrame. Got {type(exog)}.')\n    if isinstance(exog, pd.Series):\n        exog = exog.to_frame()\n    n_rows = len(exog)\n    exog_idx = exog.index\n    exog_cols = exog.columns\n    exog_direct = []\n    for i in range(steps):\n        exog_step = exog.iloc[i:n_rows - (steps - 1 - i),]\n        exog_step.index = pd.RangeIndex(len(exog_step))\n        exog_step.columns = [f'{col}_step_{i + 1}' for col in exog_cols]\n        exog_direct.append(exog_step)\n    if len(exog_direct) > 1:\n        exog_direct = pd.concat(exog_direct, axis=1, copy=False)\n    else:\n        exog_direct = exog_direct[0]\n    exog_direct_names = exog_direct.columns.to_list()\n    exog_direct.index = exog_idx[-len(exog_direct):]\n    return (exog_direct, exog_direct_names)\n\ndef exog_to_direct_numpy(exog: Union[np.ndarray, pd.Series, pd.DataFrame], steps: int) -> Tuple[np.ndarray, Optional[list]]:\n    \"\"\"\n    Transforms `exog` to numpy ndarray with the shape needed for Direct\n    forecasting.\n    \n    Parameters\n    ----------\n    exog : numpy ndarray, pandas Series, pandas DataFrame\n        Exogenous variables, shape(samples,). If exog is a pandas format, the \n        direct exog names are created.\n    steps : int\n        Number of steps that will be predicted using exog.\n\n    Returns\n    -------\n    exog_direct : numpy ndarray\n        Exogenous variables transformed.\n    exog_direct_names : list, None\n        Names of the columns of the exogenous variables transformed. Only \n        created if `exog` is a pandas Series or DataFrame.\n\n    \"\"\"\n    if isinstance(exog, (pd.Series, pd.DataFrame)):\n        exog_cols = exog.columns if isinstance(exog, pd.DataFrame) else [exog.name]\n        exog_direct_names = [f'{col}_step_{i + 1}' for i in range(steps) for col in exog_cols]\n        exog = exog.to_numpy()\n    else:\n        exog_direct_names = None\n        if not isinstance(exog, np.ndarray):\n            raise TypeError(f'`exog` must be a numpy ndarray, pandas Series or DataFrame. Got {type(exog)}.')\n    if exog.ndim == 1:\n        exog = np.expand_dims(exog, axis=1)\n    n_rows = len(exog)\n    exog_direct = []\n    for i in range(steps):\n        exog_step = exog[i:n_rows - (steps - 1 - i)]\n        exog_direct.append(exog_step)\n    if len(exog_direct) > 1:\n        exog_direct = np.concatenate(exog_direct, axis=1)\n    else:\n        exog_direct = exog_direct[0]\n    return (exog_direct, exog_direct_names)\n\ndef date_to_index_position(index: pd.Index, date_input: Union[int, str, pd.Timestamp], date_literal: str='steps', kwargs_pd_to_datetime: dict={}) -> int:\n    \"\"\"\n    Transform a datetime string or pandas Timestamp to an integer. The integer\n    represents the position of the datetime in the index.\n    \n    Parameters\n    ----------\n    index : pandas Index\n        Original datetime index (must be a pandas DatetimeIndex if `date_input` \n        is not an int).\n    date_input : int, str, pandas Timestamp\n        Datetime to transform to integer.\n        \n        + If int, returns the same integer.\n        + If str or pandas Timestamp, it is converted and expanded into the index.\n    date_literal : str, default 'steps'\n        Variable name used in error messages.\n    kwargs_pd_to_datetime : dict, default {}\n        Additional keyword arguments to pass to `pd.to_datetime()`.\n    \n    Returns\n    -------\n    date_position : int\n        Integer representing the position of the datetime in the index.\n    \n    \"\"\"\n    if isinstance(date_input, (str, pd.Timestamp)):\n        if not isinstance(index, pd.DatetimeIndex):\n            raise TypeError(f'Index must be a pandas DatetimeIndex when `{date_literal}` is not an integer. Check input series or last window.')\n        target_date = pd.to_datetime(date_input, **kwargs_pd_to_datetime)\n        last_date = pd.to_datetime(index[-1])\n        if target_date <= last_date:\n            raise ValueError('The provided date must be later than the last date in the index.')\n        steps_diff = pd.date_range(start=last_date, end=target_date, freq=index.freq)\n        date_position = len(steps_diff) - 1\n    elif isinstance(date_input, (int, np.integer)):\n        date_position = date_input\n    else:\n        raise TypeError(f'`{date_literal}` must be an integer, string, or pandas Timestamp.')\n    return date_position\n\ndef transform_numpy(array: np.ndarray, transformer, fit: bool=False, inverse_transform: bool=False) -> np.ndarray:\n    \"\"\"\n    Transform raw values of a numpy ndarray with a scikit-learn alike \n    transformer, preprocessor or ColumnTransformer. The transformer used must \n    have the following methods: fit, transform, fit_transform and \n    inverse_transform. ColumnTransformers are not allowed since they do not \n    have inverse_transform method.\n\n    Parameters\n    ----------\n    array : numpy ndarray\n        Array to be transformed.\n    transformer : scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n        Scikit-learn alike transformer (preprocessor) with methods: fit, transform,\n        fit_transform and inverse_transform.\n    fit : bool, default `False`\n        Train the transformer before applying it.\n    inverse_transform : bool, default `False`\n        Transform back the data to the original representation. This is not available\n        when using transformers of class scikit-learn ColumnTransformers.\n\n    Returns\n    -------\n    array_transformed : numpy ndarray\n        Transformed array.\n\n    \"\"\"\n    if not isinstance(array, np.ndarray):\n        raise TypeError(f'`array` argument must be a numpy ndarray. Got {type(array)}')\n    if transformer is None:\n        return array\n    array_ndim = array.ndim\n    if array_ndim == 1:\n        array = array.reshape(-1, 1)\n    if inverse_transform and isinstance(transformer, ColumnTransformer):\n        raise ValueError('`inverse_transform` is not available when using ColumnTransformers.')\n    if not inverse_transform:\n        if fit:\n            array_transformed = transformer.fit_transform(array)\n        else:\n            with warnings.catch_warnings():\n                warnings.filterwarnings('ignore', message='X does not have valid feature names', category=UserWarning)\n                array_transformed = transformer.transform(array)\n    else:\n        array_transformed = transformer.inverse_transform(array)\n    if hasattr(array_transformed, 'toarray'):\n        array_transformed = array_transformed.toarray()\n    if array_ndim == 1:\n        array_transformed = array_transformed.ravel()\n    return array_transformed\n\ndef transform_series(series: pd.Series, transformer, fit: bool=False, inverse_transform: bool=False) -> Union[pd.Series, pd.DataFrame]:\n    \"\"\"\n    Transform raw values of pandas Series with a scikit-learn alike \n    transformer, preprocessor or ColumnTransformer. The transformer used must \n    have the following methods: fit, transform, fit_transform and \n    inverse_transform. ColumnTransformers are not allowed since they do not \n    have inverse_transform method.\n\n    Parameters\n    ----------\n    series : pandas Series\n        Series to be transformed.\n    transformer : scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n        Scikit-learn alike transformer (preprocessor) with methods: fit, transform,\n        fit_transform and inverse_transform.\n    fit : bool, default `False`\n        Train the transformer before applying it.\n    inverse_transform : bool, default `False`\n        Transform back the data to the original representation. This is not available\n        when using transformers of class scikit-learn ColumnTransformers.\n\n    Returns\n    -------\n    series_transformed : pandas Series, pandas DataFrame\n        Transformed Series. Depending on the transformer used, the output may \n        be a Series or a DataFrame.\n\n    \"\"\"\n    if not isinstance(series, pd.Series):\n        raise TypeError(f'`series` argument must be a pandas Series. Got {type(series)}.')\n    if transformer is None:\n        return series\n    if series.name is None:\n        series.name = 'no_name'\n    data = series.to_frame()\n    if fit and hasattr(transformer, 'fit'):\n        transformer.fit(data)\n    if hasattr(transformer, 'feature_names_in_') and transformer.feature_names_in_[0] != data.columns[0]:\n        transformer = deepcopy(transformer)\n        transformer.feature_names_in_ = np.array([data.columns[0]], dtype=object)\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore', category=UserWarning)\n        if inverse_transform:\n            values_transformed = transformer.inverse_transform(data)\n        else:\n            values_transformed = transformer.transform(data)\n    if hasattr(values_transformed, 'toarray'):\n        values_transformed = values_transformed.toarray()\n    if isinstance(values_transformed, np.ndarray) and values_transformed.shape[1] == 1:\n        series_transformed = pd.Series(data=values_transformed.ravel(), index=data.index, name=data.columns[0])\n    elif isinstance(values_transformed, pd.DataFrame) and values_transformed.shape[1] == 1:\n        series_transformed = values_transformed.squeeze()\n    else:\n        series_transformed = pd.DataFrame(data=values_transformed, index=data.index, columns=transformer.get_feature_names_out())\n    return series_transformed\n\ndef transform_dataframe(df: pd.DataFrame, transformer, fit: bool=False, inverse_transform: bool=False) -> pd.DataFrame:\n    \"\"\"\n    Transform raw values of pandas DataFrame with a scikit-learn alike \n    transformer, preprocessor or ColumnTransformer. The transformer used must \n    have the following methods: fit, transform, fit_transform and \n    inverse_transform. ColumnTransformers are not allowed since they do not \n    have inverse_transform method.\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        DataFrame to be transformed.\n    transformer : scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n        Scikit-learn alike transformer (preprocessor) with methods: fit, transform,\n        fit_transform and inverse_transform.\n    fit : bool, default `False`\n        Train the transformer before applying it.\n    inverse_transform : bool, default `False`\n        Transform back the data to the original representation. This is not available\n        when using transformers of class scikit-learn ColumnTransformers.\n\n    Returns\n    -------\n    df_transformed : pandas DataFrame\n        Transformed DataFrame.\n\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(f'`df` argument must be a pandas DataFrame. Got {type(df)}')\n    if transformer is None:\n        return df\n    if inverse_transform and isinstance(transformer, ColumnTransformer):\n        raise ValueError('`inverse_transform` is not available when using ColumnTransformers.')\n    if not inverse_transform:\n        if fit:\n            values_transformed = transformer.fit_transform(df)\n        else:\n            values_transformed = transformer.transform(df)\n    else:\n        values_transformed = transformer.inverse_transform(df)\n    if hasattr(values_transformed, 'toarray'):\n        values_transformed = values_transformed.toarray()\n    if hasattr(transformer, 'get_feature_names_out'):\n        feature_names_out = transformer.get_feature_names_out()\n    elif hasattr(transformer, 'categories_'):\n        feature_names_out = transformer.categories_\n    else:\n        feature_names_out = df.columns\n    df_transformed = pd.DataFrame(data=values_transformed, index=df.index, columns=feature_names_out)\n    return df_transformed\n\ndef save_forecaster(forecaster: object, file_name: str, save_custom_functions: bool=True, verbose: bool=True) -> None:\n    \"\"\"\n    Save forecaster model using joblib. If custom functions are used to create\n    weights, they are saved as .py files.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster created with skforecast library.\n    file_name : str\n        File name given to the object. The save extension will be .joblib.\n    save_custom_functions : bool, default True\n        If True, save custom functions used in the forecaster (weight_func) as \n        .py files. Custom functions need to be available in the environment \n        where the forecaster is going to be loaded.\n    verbose : bool, default True\n        Print summary about the forecaster saved.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    file_name = Path(file_name).with_suffix('.joblib')\n    joblib.dump(forecaster, filename=file_name)\n    if save_custom_functions:\n        if hasattr(forecaster, 'weight_func') and forecaster.weight_func is not None:\n            if isinstance(forecaster.weight_func, dict):\n                for fun in set(forecaster.weight_func.values()):\n                    file_name = fun.__name__ + '.py'\n                    with open(file_name, 'w') as file:\n                        file.write(inspect.getsource(fun))\n            else:\n                file_name = forecaster.weight_func.__name__ + '.py'\n                with open(file_name, 'w') as file:\n                    file.write(inspect.getsource(forecaster.weight_func))\n    elif hasattr(forecaster, 'weight_func') and forecaster.weight_func is not None:\n        warnings.warn('Custom function(s) used to create weights are not saved. To save them, set `save_custom_functions` to `True`.', SaveLoadSkforecastWarning)\n    if hasattr(forecaster, 'window_features') and forecaster.window_features is not None:\n        skforecast_classes = {'RollingFeatures'}\n        custom_classes = set(forecaster.window_features_class_names) - skforecast_classes\n        if custom_classes:\n            warnings.warn('The Forecaster includes custom user-defined classes in the `window_features` argument. These classes are not saved automatically when saving the Forecaster. Please ensure you save these classes manually and import them before loading the Forecaster.\\n    Custom classes: ' + ', '.join(custom_classes) + '\\nVisit the documentation for more information: https://skforecast.org/latest/user_guides/save-load-forecaster.html#saving-and-loading-a-forecaster-model-with-custom-features', SaveLoadSkforecastWarning)\n    if verbose:\n        forecaster.summary()\n\ndef load_forecaster(file_name: str, verbose: bool=True) -> object:\n    \"\"\"\n    Load forecaster model using joblib. If the forecaster was saved with \n    custom user-defined classes as as window features or custom\n    functions to create weights, these objects must be available\n    in the environment where the forecaster is going to be loaded.\n\n    Parameters\n    ----------\n    file_name: str\n        Object file name.\n    verbose: bool, default `True`\n        Print summary about the forecaster loaded.\n\n    Returns\n    -------\n    forecaster: Forecaster\n        Forecaster created with skforecast library.\n    \n    \"\"\"\n    forecaster = joblib.load(filename=Path(file_name))\n    skforecast_v = skforecast.__version__\n    forecaster_v = forecaster.skforecast_version\n    if forecaster_v != skforecast_v:\n        warnings.warn(f'The skforecast version installed in the environment differs from the version used to create the forecaster.\\n    Installed Version  : {skforecast_v}\\n    Forecaster Version : {forecaster_v}\\nThis may create incompatibilities when using the library.', SkforecastVersionWarning)\n    if verbose:\n        forecaster.summary()\n    return forecaster\n\ndef _find_optional_dependency(package_name: str, optional_dependencies: dict=optional_dependencies) -> Tuple[str, str]:\n    \"\"\"\n    Find if a package is an optional dependency. If True, find the version and \n    the extension it belongs to.\n\n    Parameters\n    ----------\n    package_name : str\n        Name of the package to check.\n    optional_dependencies : dict, default `optional_dependencies`\n        Skforecast optional dependencies.\n\n    Returns\n    -------\n    extra: str\n        Name of the extra extension where the optional dependency is needed.\n    package_version: srt\n        Name and versions of the dependency.\n\n    \"\"\"\n    for extra, packages in optional_dependencies.items():\n        package_version = [package for package in packages if package_name in package]\n        if package_version:\n            return (extra, package_version[0])\n\ndef check_optional_dependency(package_name: str) -> None:\n    \"\"\"\n    Check if an optional dependency is installed, if not raise an ImportError  \n    with installation instructions.\n\n    Parameters\n    ----------\n    package_name : str\n        Name of the package to check.\n\n    Returns\n    -------\n    None\n    \n    \"\"\"\n    if importlib.util.find_spec(package_name) is None:\n        try:\n            extra, package_version = _find_optional_dependency(package_name=package_name)\n            msg = f'''\\n'{package_name}' is an optional dependency not included in the default skforecast installation. Please run: `pip install \"{package_version}\"` to install it.\\n\\nAlternately, you can install it by running `pip install skforecast[{extra}]`'''\n        except:\n            msg = f\"\\n'{package_name}' is needed but not installed. Please install it.\"\n        raise ImportError(msg)\n\ndef multivariate_time_series_corr(time_series: pd.Series, other: pd.DataFrame, lags: Union[int, list, np.array], method: str='pearson') -> pd.DataFrame:\n    \"\"\"\n    Compute correlation between a time_series and the lagged values of other \n    time series. \n\n    Parameters\n    ----------\n    time_series : pandas Series\n        Target time series.\n    other : pandas DataFrame\n        Time series whose lagged values are correlated to `time_series`.\n    lags : int, list, numpy ndarray\n        Lags to be included in the correlation analysis.\n    method : str, default 'pearson'\n        - 'pearson': standard correlation coefficient.\n        - 'kendall': Kendall Tau correlation coefficient.\n        - 'spearman': Spearman rank correlation.\n\n    Returns\n    -------\n    corr : pandas DataFrame\n        Correlation values.\n\n    \"\"\"\n    if not len(time_series) == len(other):\n        raise ValueError('`time_series` and `other` must have the same length.')\n    if not (time_series.index == other.index).all():\n        raise ValueError('`time_series` and `other` must have the same index.')\n    if isinstance(lags, int):\n        lags = range(lags)\n    corr = {}\n    for col in other.columns:\n        lag_values = {}\n        for lag in lags:\n            lag_values[lag] = other[col].shift(lag)\n        lag_values = pd.DataFrame(lag_values)\n        lag_values.insert(0, None, time_series)\n        corr[col] = lag_values.corr(method=method).iloc[1:, 0]\n    corr = pd.DataFrame(corr)\n    corr.index = corr.index.astype('int64')\n    corr.index.name = 'lag'\n    return corr\n\ndef select_n_jobs_fit_forecaster(forecaster_name: str, regressor: object) -> int:\n    \"\"\"\n    Select the optimal number of jobs to use in the fitting process. This\n    selection is based on heuristics and is not guaranteed to be optimal. \n    \n    The number of jobs is chosen as follows:\n    \n    - If forecaster_name is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and regressor_name is a linear regressor then `n_jobs = 1`, \n    otherwise `n_jobs = cpu_count() - 1`.\n    - If regressor is a `LGBMRegressor(n_jobs=1)`, then `n_jobs = cpu_count() - 1`.\n    - If regressor is a `LGBMRegressor` with internal n_jobs != 1, then `n_jobs = 1`.\n    This is because `lightgbm` is highly optimized for gradient boosting and\n    parallelizes operations at a very fine-grained level, making additional\n    parallelization unnecessary and potentially harmful due to resource contention.\n    \n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n\n    Returns\n    -------\n    n_jobs : int\n        The number of jobs to run in parallel.\n    \n    \"\"\"\n    if isinstance(regressor, Pipeline):\n        regressor = regressor[-1]\n        regressor_name = type(regressor).__name__\n    else:\n        regressor_name = type(regressor).__name__\n    linear_regressors = [regressor_name for regressor_name in dir(sklearn.linear_model) if not regressor_name.startswith('_')]\n    if forecaster_name in ['ForecasterDirect', 'ForecasterDirectMultiVariate']:\n        if regressor_name in linear_regressors:\n            n_jobs = 1\n        elif regressor_name == 'LGBMRegressor':\n            n_jobs = joblib.cpu_count() - 1 if regressor.n_jobs == 1 else 1\n        else:\n            n_jobs = joblib.cpu_count() - 1\n    else:\n        n_jobs = 1\n    return n_jobs\n\ndef check_preprocess_series(series: Union[pd.DataFrame, dict]) -> Tuple[dict, pd.Index]:\n    \"\"\"\n    Check and preprocess `series` argument in `ForecasterRecursiveMultiSeries` class.\n\n    - If `series` is a pandas DataFrame, it is converted to a dict of pandas \n    Series and index is overwritten according to the rules of preprocess_y.\n    - If `series` is a dict, all values are converted to pandas Series. Checks\n    if all index are pandas DatetimeIndex and, at least, one Series has a non-null\n    frequency. No multiple frequency is allowed.\n\n    Parameters\n    ----------\n    series : pandas DataFrame, dict\n        Training time series.\n\n    Returns\n    -------\n    series_dict : dict\n        Dictionary with the series used during training.\n    series_indexes : dict\n        Dictionary with the index of each series.\n    \n    \"\"\"\n    if isinstance(series, pd.DataFrame):\n        _, series_index = preprocess_y(y=series, return_values=False)\n        series = series.copy()\n        series.index = series_index\n        series_dict = series.to_dict('series')\n    elif isinstance(series, dict):\n        not_valid_series = [k for k, v in series.items() if not isinstance(v, (pd.Series, pd.DataFrame))]\n        if not_valid_series:\n            raise TypeError(f'If `series` is a dictionary, all series must be a named pandas Series or a pandas DataFrame with a single column. Review series: {not_valid_series}')\n        series_dict = {k: v.copy() for k, v in series.items()}\n        for k, v in series_dict.items():\n            if isinstance(v, pd.DataFrame):\n                if v.shape[1] != 1:\n                    raise ValueError(f\"If `series` is a dictionary, all series must be a named pandas Series or a pandas DataFrame with a single column. Review series: '{k}'\")\n                series_dict[k] = v.iloc[:, 0]\n            series_dict[k].name = k\n        not_valid_index = [k for k, v in series_dict.items() if not isinstance(v.index, pd.DatetimeIndex)]\n        if not_valid_index:\n            raise TypeError(f'If `series` is a dictionary, all series must have a Pandas DatetimeIndex as index with the same frequency. Review series: {not_valid_index}')\n        indexes_freq = [f'{v.index.freq}' for v in series_dict.values()]\n        indexes_freq = sorted(set(indexes_freq))\n        if not len(indexes_freq) == 1:\n            raise ValueError(f'If `series` is a dictionary, all series must have a Pandas DatetimeIndex as index with the same frequency. Found frequencies: {indexes_freq}')\n    else:\n        raise TypeError(f'`series` must be a pandas DataFrame or a dict of DataFrames or Series. Got {type(series)}.')\n    for k, v in series_dict.items():\n        if np.isnan(v).all():\n            raise ValueError(f\"All values of series '{k}' are NaN.\")\n    series_indexes = {k: v.index for k, v in series_dict.items()}\n    return (series_dict, series_indexes)\n\ndef check_preprocess_exog_multiseries(input_series_is_dict: bool, series_indexes: dict, series_names_in_: list, exog: Union[pd.Series, pd.DataFrame, dict], exog_dict: dict) -> Tuple[dict, list]:\n    \"\"\"\n    Check and preprocess `exog` argument in `ForecasterRecursiveMultiSeries` class.\n\n    - If input series is a pandas DataFrame (input_series_is_dict = False),  \n    checks that input exog (pandas Series, DataFrame or dict) has the same index \n    (type, length and frequency). Index is overwritten according to the rules \n    of preprocess_exog. Create a dict of exog with the same keys as series.\n    - If input series is a dict (input_series_is_dict = True), then input \n    exog must be a dict. Check exog has a pandas DatetimeIndex and convert all\n    values to pandas DataFrames.\n\n    Parameters\n    ----------\n    input_series_is_dict : bool\n        Indicates if input series argument is a dict.\n    series_indexes : dict\n        Dictionary with the index of each series.\n    series_names_in_ : list\n        Names of the series (levels) used during training.\n    exog : pandas Series, pandas DataFrame, dict\n        Exogenous variable/s used during training.\n    exog_dict : dict\n        Dictionary with the exogenous variable/s used during training.\n\n    Returns\n    -------\n    exog_dict : dict\n        Dictionary with the exogenous variable/s used during training.\n    exog_names_in_ : list\n        Names of the exogenous variables used during training.\n    \n    \"\"\"\n    if not isinstance(exog, (pd.Series, pd.DataFrame, dict)):\n        raise TypeError(f'`exog` must be a pandas Series, DataFrame, dictionary of pandas Series/DataFrames or None. Got {type(exog)}.')\n    if not input_series_is_dict:\n        series_index = series_indexes[series_names_in_[0]]\n    if isinstance(exog, (pd.Series, pd.DataFrame)):\n        if input_series_is_dict:\n            raise TypeError(f'`exog` must be a dict of DataFrames or Series if `series` is a dict. Got {type(exog)}.')\n        _, exog_index = preprocess_exog(exog=exog, return_values=False)\n        exog = exog.copy().to_frame() if isinstance(exog, pd.Series) else exog.copy()\n        exog.index = exog_index\n        if len(exog) != len(series_index):\n            raise ValueError(f'`exog` must have same number of samples as `series`. length `exog`: ({len(exog)}), length `series`: ({len(series_index)})')\n        if not (exog_index == series_index).all():\n            raise ValueError('Different index for `series` and `exog`. They must be equal to ensure the correct alignment of values.')\n        exog_dict = {serie: exog for serie in series_names_in_}\n    else:\n        not_valid_exog = [k for k, v in exog.items() if not isinstance(v, (pd.Series, pd.DataFrame, type(None)))]\n        if not_valid_exog:\n            raise TypeError(f'If `exog` is a dictionary, all exog must be a named pandas Series, a pandas DataFrame or None. Review exog: {not_valid_exog}')\n        exog_dict.update(((k, v.copy()) for k, v in exog.items() if k in exog_dict and v is not None))\n        series_not_in_exog = set(series_names_in_) - set(exog.keys())\n        if series_not_in_exog:\n            warnings.warn(f'{series_not_in_exog} not present in `exog`. All values of the exogenous variables for these series will be NaN.', MissingExogWarning)\n        for k, v in exog_dict.items():\n            if v is not None:\n                check_exog(exog=v, allow_nan=True)\n                if isinstance(v, pd.Series):\n                    v = v.to_frame()\n                exog_dict[k] = v\n        if not input_series_is_dict:\n            for k, v in exog_dict.items():\n                if v is not None:\n                    if len(v) != len(series_index):\n                        raise ValueError(f\"`exog` for series '{k}' must have same number of samples as `series`. length `exog`: ({len(v)}), length `series`: ({len(series_index)})\")\n                    _, v_index = preprocess_exog(exog=v, return_values=False)\n                    exog_dict[k].index = v_index\n                    if not (exog_dict[k].index == series_index).all():\n                        raise ValueError(f\"Different index for series '{k}' and its exog. When `series` is a pandas DataFrame, they must be equal to ensure the correct alignment of values.\")\n        else:\n            not_valid_index = [k for k, v in exog_dict.items() if v is not None and (not isinstance(v.index, pd.DatetimeIndex))]\n            if not_valid_index:\n                raise TypeError(f'All exog must have a Pandas DatetimeIndex as index with the same frequency. Check exog for series: {not_valid_index}')\n        exog_dtypes_buffer = [df.dtypes for df in exog_dict.values() if df is not None]\n        exog_dtypes_buffer = pd.concat(exog_dtypes_buffer, axis=1)\n        exog_dtypes_nunique = exog_dtypes_buffer.nunique(axis=1).eq(1)\n        if not exog_dtypes_nunique.all():\n            non_unique_dtyeps_exogs = exog_dtypes_nunique[exog_dtypes_nunique != 1].index.to_list()\n            raise TypeError(f'Exog/s: {non_unique_dtyeps_exogs} have different dtypes in different series.')\n    exog_names_in_ = list(set((column for df in exog_dict.values() if df is not None for column in df.columns.to_list())))\n    if len(set(exog_names_in_) - set(series_names_in_)) != len(exog_names_in_):\n        raise ValueError(f'`exog` cannot contain a column named the same as one of the series.\\n    `series` columns : {series_names_in_}.\\n    `exog`   columns : {exog_names_in_}.')\n    return (exog_dict, exog_names_in_)\n\ndef align_series_and_exog_multiseries(series_dict: dict, input_series_is_dict: bool, exog_dict: dict=None) -> Tuple[Union[pd.Series, pd.DataFrame], Union[pd.Series, pd.DataFrame]]:\n    \"\"\"\n    Align series and exog according to their index. If needed, reindexing is\n    applied. Heading and trailing NaNs are removed from all series in \n    `series_dict`.\n\n    - If input series is a pandas DataFrame (input_series_is_dict = False),  \n    input exog (pandas Series, DataFrame or dict) must have the same index \n    (type, length and frequency). Reindexing is not applied.\n    - If input series is a dict (input_series_is_dict = True), then input \n    exog must be a dict. Both must have a pandas DatetimeIndex, but can have \n    different lengths. Reindexing is applied.\n\n    Parameters\n    ----------\n    series_dict : dict\n        Dictionary with the series used during training.\n    input_series_is_dict : bool\n        Indicates if input series argument is a dict.\n    exog_dict : dict, default `None`\n        Dictionary with the exogenous variable/s used during training.\n\n    Returns\n    -------\n    series_dict : dict\n        Dictionary with the series used during training.\n    exog_dict : dict\n        Dictionary with the exogenous variable/s used during training.\n    \n    \"\"\"\n    for k in series_dict.keys():\n        first_valid_index = series_dict[k].first_valid_index()\n        last_valid_index = series_dict[k].last_valid_index()\n        series_dict[k] = series_dict[k].loc[first_valid_index:last_valid_index]\n        if exog_dict[k] is not None:\n            if input_series_is_dict:\n                index_intersection = series_dict[k].index.intersection(exog_dict[k].index)\n                if len(index_intersection) == 0:\n                    warnings.warn(f\"Series '{k}' and its `exog` do not have the same index. All exog values will be NaN for the period of the series.\", MissingValuesWarning)\n                elif len(index_intersection) != len(series_dict[k]):\n                    warnings.warn(f\"Series '{k}' and its `exog` do not have the same length. Exog values will be NaN for the not matched period of the series.\", MissingValuesWarning)\n                exog_dict[k] = exog_dict[k].loc[index_intersection]\n                if len(index_intersection) != len(series_dict[k]):\n                    exog_dict[k] = exog_dict[k].reindex(series_dict[k].index, fill_value=np.nan)\n            else:\n                exog_dict[k] = exog_dict[k].loc[first_valid_index:last_valid_index]\n    return (series_dict, exog_dict)\n\ndef prepare_levels_multiseries(X_train_series_names_in_: list, levels: Optional[Union[str, list]]=None) -> Tuple[list, bool]:\n    \"\"\"\n    Prepare list of levels to be predicted in multiseries Forecasters.\n\n    Parameters\n    ----------\n    X_train_series_names_in_ : list\n        Names of the series (levels) included in the matrix `X_train`.\n    levels : str, list, default `None`\n        Names of the series (levels) to be predicted.\n\n    Returns\n    -------\n    levels : list\n        Names of the series (levels) to be predicted.\n\n    \"\"\"\n    input_levels_is_list = False\n    if levels is None:\n        levels = X_train_series_names_in_\n    elif isinstance(levels, str):\n        levels = [levels]\n    else:\n        input_levels_is_list = True\n    return (levels, input_levels_is_list)\n\ndef preprocess_levels_self_last_window_multiseries(levels: list, input_levels_is_list: bool, last_window_: dict) -> Tuple[list, pd.DataFrame]:\n    \"\"\"\n    Preprocess `levels` and `last_window` (when using self.last_window_) arguments \n    in multiseries Forecasters when predicting. Only levels whose last window \n    ends at the same datetime index will be predicted together.\n\n    Parameters\n    ----------\n    levels : list\n        Names of the series (levels) to be predicted.\n    input_levels_is_list : bool\n        Indicates if input levels argument is a list.\n    last_window_ : dict\n        Dictionary with the last window of each series (self.last_window_).\n\n    Returns\n    -------\n    levels : list\n        Names of the series (levels) to be predicted.\n    last_window : pandas DataFrame\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n\n    \"\"\"\n    available_last_windows = set() if last_window_ is None else set(last_window_.keys())\n    not_available_last_window = set(levels) - available_last_windows\n    if not_available_last_window:\n        levels = [level for level in levels if level not in not_available_last_window]\n        if not levels:\n            raise ValueError(f'No series to predict. None of the series {not_available_last_window} are present in `last_window_` attribute. Provide `last_window` as argument in predict method.')\n        else:\n            warnings.warn(f\"Levels {not_available_last_window} are excluded from prediction since they were not stored in `last_window_` attribute during training. If you don't want to retrain the Forecaster, provide `last_window` as argument.\", IgnoredArgumentWarning)\n    last_index_levels = [v.index[-1] for k, v in last_window_.items() if k in levels]\n    if len(set(last_index_levels)) > 1:\n        max_index_levels = max(last_index_levels)\n        selected_levels = [k for k, v in last_window_.items() if k in levels and v.index[-1] == max_index_levels]\n        series_excluded_from_last_window = set(levels) - set(selected_levels)\n        levels = selected_levels\n        if input_levels_is_list and series_excluded_from_last_window:\n            warnings.warn(f\"Only series whose last window ends at the same index can be predicted together. Series that do not reach the maximum index, '{max_index_levels}', are excluded from prediction: {series_excluded_from_last_window}.\", IgnoredArgumentWarning)\n    last_window = pd.DataFrame({k: v for k, v in last_window_.items() if k in levels})\n    return (levels, last_window)\n\ndef prepare_residuals_multiseries(levels: list, use_in_sample_residuals: bool, encoding: Optional[str]=None, in_sample_residuals_: Optional[dict]=None, out_sample_residuals_: Optional[dict]=None) -> Tuple[list, bool]:\n    \"\"\"\n    Prepare residuals for bootstrapping prediction in multiseries Forecasters.\n\n    Parameters\n    ----------\n    levels : list\n        Names of the series (levels) to be predicted.\n    use_in_sample_residuals : bool\n        Indicates if `forecaster.in_sample_residuals_` are used.\n    encoding : str, default `None`\n        Encoding used to identify the different series (`ForecasterRecursiveMultiSeries`).\n    in_sample_residuals_ : dict, default `None`\n        Residuals of the model when predicting training data. Only stored up to\n        1000 values in the form `{level: residuals}`. If `transformer_series` \n        is not `None`, residuals are stored in the transformed scale.\n    out_sample_residuals_ : dict, default `None`\n        Residuals of the model when predicting non-training data. Only stored\n        up to 1000 values in the form `{level: residuals}`. If `transformer_series` \n        is not `None`, residuals are assumed to be in the transformed scale. Use \n        `set_out_sample_residuals()` method to set values.\n\n    Returns\n    -------\n    levels : list\n        Names of the series (levels) to be predicted.\n    residuals : dict\n        Residuals of the model for each level to use in bootstrapping prediction.\n\n    \"\"\"\n    if use_in_sample_residuals:\n        unknown_levels = set(levels) - set(in_sample_residuals_.keys())\n        if unknown_levels and encoding is not None:\n            warnings.warn(f'`levels` {unknown_levels} are not present in `forecaster.in_sample_residuals_`, most likely because they were not present in the training data. A random sample of the residuals from other levels will be used. This can lead to inaccurate intervals for the unknown levels.', UnknownLevelWarning)\n        residuals = in_sample_residuals_.copy()\n    elif out_sample_residuals_ is None:\n        raise ValueError('`forecaster.out_sample_residuals_` is `None`. Use `use_in_sample_residuals=True` or the `set_out_sample_residuals()` method before predicting.')\n    else:\n        unknown_levels = set(levels) - set(out_sample_residuals_.keys())\n        if unknown_levels and encoding is not None:\n            warnings.warn(f'`levels` {unknown_levels} are not present in `forecaster.out_sample_residuals_`. A random sample of the residuals from other levels will be used. This can lead to inaccurate intervals for the unknown levels. Otherwise, Use the `set_out_sample_residuals()` method before predicting to set the residuals for these levels.', UnknownLevelWarning)\n        residuals = out_sample_residuals_.copy()\n    check_residuals = 'forecaster.in_sample_residuals_' if use_in_sample_residuals else 'forecaster.out_sample_residuals_'\n    for level in levels:\n        if level in unknown_levels:\n            residuals[level] = residuals['_unknown_level']\n        if residuals[level] is None or len(residuals[level]) == 0:\n            raise ValueError(f\"Not available residuals for level '{level}'. Check `{check_residuals}`.\")\n        elif any((element is None for element in residuals[level])) or np.any(np.isnan(residuals[level])):\n            raise ValueError(f\"forecaster residuals for level '{level}' contains `None` or `NaNs` values. Check `{check_residuals}`.\")\n    return residuals\n\ndef prepare_steps_direct(max_step: int, steps: Optional[Union[int, list]]=None) -> list:\n    \"\"\"\n    Prepare list of steps to be predicted in Direct Forecasters.\n\n    Parameters\n    ----------\n    max_step : int\n        Maximum number of future steps the forecaster will predict \n        when using method `predict()`.\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n    \n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n\n    Returns\n    -------\n    steps : list\n        Steps to be predicted.\n\n    \"\"\"\n    if isinstance(steps, int):\n        steps = list(np.arange(steps) + 1)\n    elif steps is None:\n        steps = list(np.arange(max_step) + 1)\n    elif isinstance(steps, list):\n        steps = list(np.array(steps))\n    for step in steps:\n        if not isinstance(step, (int, np.int64, np.int32)):\n            raise TypeError(f'`steps` argument must be an int, a list of ints or `None`. Got {type(steps)}.')\n    steps = [int(step) for step in steps if step is not None]\n    return steps\n\ndef set_skforecast_warnings(suppress_warnings: bool, action: str='default') -> None:\n    \"\"\"\n    Set skforecast warnings action.\n\n    Parameters\n    ----------\n    suppress_warnings : bool\n        If `True`, skforecast warnings will be suppressed. If `False`, skforecast\n        warnings will be shown as default. See \n        skforecast.exceptions.warn_skforecast_categories for more information.\n    action : str, default `'default'`\n        Action to be taken when a warning is raised. See the warnings module\n        for more information.\n\n    Returns\n    -------\n    None\n    \n    \"\"\"\n    if suppress_warnings:\n        for category in warn_skforecast_categories:\n            warnings.filterwarnings(action, category=category)",
    "skforecast/metrics/metrics.py": "from typing import Union, Callable\nimport numpy as np\nimport pandas as pd\nimport inspect\nfrom functools import wraps\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, mean_squared_log_error, median_absolute_error\n\ndef mean_absolute_scaled_error(y_true: Union[pd.Series, np.ndarray], y_pred: Union[pd.Series, np.ndarray], y_train: Union[list, pd.Series, np.ndarray]) -> float:\n    \"\"\"\n    Mean Absolute Scaled Error (MASE)\n\n    MASE is a scale-independent error metric that measures the accuracy of\n    a forecast. It is the mean absolute error of the forecast divided by the\n    mean absolute error of a naive forecast in the training set. The naive\n    forecast is the one obtained by shifting the time series by one period.\n    If y_train is a list of numpy arrays or pandas Series, it is considered\n    that each element is the true value of the target variable in the training\n    set for each time series. In this case, the naive forecast is calculated\n    for each time series separately.\n\n    Parameters\n    ----------\n    y_true : pandas Series, numpy ndarray\n        True values of the target variable.\n    y_pred : pandas Series, numpy ndarray\n        Predicted values of the target variable.\n    y_train : list, pandas Series, numpy ndarray\n        True values of the target variable in the training set. If `list`, it\n        is consider that each element is the true value of the target variable\n        in the training set for each time series.\n\n    Returns\n    -------\n    mase : float\n        MASE value.\n    \n    \"\"\"\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError('`y_true` must be a pandas Series or numpy ndarray.')\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError('`y_pred` must be a pandas Series or numpy ndarray.')\n    if not isinstance(y_train, (list, pd.Series, np.ndarray)):\n        raise TypeError('`y_train` must be a list, pandas Series or numpy ndarray.')\n    if isinstance(y_train, list):\n        for x in y_train:\n            if not isinstance(x, (pd.Series, np.ndarray)):\n                raise TypeError('When `y_train` is a list, each element must be a pandas Series or numpy ndarray.')\n    if len(y_true) != len(y_pred):\n        raise ValueError('`y_true` and `y_pred` must have the same length.')\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError('`y_true` and `y_pred` must have at least one element.')\n    if isinstance(y_train, list):\n        naive_forecast = np.concatenate([np.diff(x) for x in y_train])\n    else:\n        naive_forecast = np.diff(y_train)\n    mase = np.mean(np.abs(y_true - y_pred)) / np.nanmean(np.abs(naive_forecast))\n    return mase\n\ndef root_mean_squared_scaled_error(y_true: Union[pd.Series, np.ndarray], y_pred: Union[pd.Series, np.ndarray], y_train: Union[list, pd.Series, np.ndarray]) -> float:\n    \"\"\"\n    Root Mean Squared Scaled Error (RMSSE)\n\n    RMSSE is a scale-independent error metric that measures the accuracy of\n    a forecast. It is the root mean squared error of the forecast divided by\n    the root mean squared error of a naive forecast in the training set. The\n    naive forecast is the one obtained by shifting the time series by one period.\n    If y_train is a list of numpy arrays or pandas Series, it is considered\n    that each element is the true value of the target variable in the training\n    set for each time series. In this case, the naive forecast is calculated\n    for each time series separately.\n\n    Parameters\n    ----------\n    y_true : pandas Series, numpy ndarray\n        True values of the target variable.\n    y_pred : pandas Series, numpy ndarray\n        Predicted values of the target variable.\n    y_train : list, pandas Series, numpy ndarray\n        True values of the target variable in the training set. If list, it\n        is consider that each element is the true value of the target variable\n        in the training set for each time series.\n\n    Returns\n    -------\n    rmsse : float\n        RMSSE value.\n    \n    \"\"\"\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError('`y_true` must be a pandas Series or numpy ndarray.')\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError('`y_pred` must be a pandas Series or numpy ndarray.')\n    if not isinstance(y_train, (list, pd.Series, np.ndarray)):\n        raise TypeError('`y_train` must be a list, pandas Series or numpy ndarray.')\n    if isinstance(y_train, list):\n        for x in y_train:\n            if not isinstance(x, (pd.Series, np.ndarray)):\n                raise TypeError('When `y_train` is a list, each element must be a pandas Series or numpy ndarray.')\n    if len(y_true) != len(y_pred):\n        raise ValueError('`y_true` and `y_pred` must have the same length.')\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError('`y_true` and `y_pred` must have at least one element.')\n    if isinstance(y_train, list):\n        naive_forecast = np.concatenate([np.diff(x) for x in y_train])\n    else:\n        naive_forecast = np.diff(y_train)\n    rmsse = np.sqrt(np.mean((y_true - y_pred) ** 2)) / np.sqrt(np.nanmean(naive_forecast ** 2))\n    return rmsse",
    "skforecast/recursive/_forecaster_equivalent_date.py": "from typing import Union, Optional, Callable, Any\nimport warnings\nimport sys\nimport numpy as np\nimport pandas as pd\nimport skforecast\nfrom ..utils import check_predict_input\nfrom ..utils import preprocess_y\nfrom ..utils import preprocess_last_window\nfrom ..utils import expand_index\n\nclass ForecasterEquivalentDate:\n    \"\"\"\n    This forecaster predicts future values based on the most recent equivalent\n    date. It also allows to aggregate multiple past values of the equivalent\n    date using a function (e.g. mean, median, max, min, etc.). The equivalent\n    date is calculated by moving back in time a specified number of steps (offset).\n    The offset can be defined as an integer or as a pandas DateOffset. This\n    approach is useful as a baseline, but it is a simplistic method and may not\n    capture complex underlying patterns.\n    \n    Parameters\n    ----------\n    offset : int, pandas.tseries.offsets.DateOffset\n        Number of steps to go back in time to find the most recent equivalent\n        date to the target period.\n        If `offset` is an integer, it represents the number of steps to go back\n        in time. For example, if the frequency of the time series is daily, \n        `offset = 7` means that the most recent data similar to the target\n        period is the value observed 7 days ago.\n        Pandas DateOffsets can also be used to move forward a given number of \n        valid dates. For example, Bday(2) can be used to move back two business \n        days. If the date does not start on a valid date, it is first moved to a \n        valid date. For example, if the date is a Saturday, it is moved to the \n        previous Friday. Then, the offset is applied. If the result is a non-valid \n        date, it is moved to the next valid date. For example, if the date\n        is a Sunday, it is moved to the next Monday. \n        For more information about offsets, see\n        https://pandas.pydata.org/docs/reference/offset_frequency.html.\n    n_offsets : int, default `1`\n        Number of equivalent dates (multiple of offset) used in the prediction.\n        If `n_offsets` is greater than 1, the values at the equivalent dates are\n        aggregated using the `agg_func` function. For example, if the frequency\n        of the time series is daily, `offset = 7`, `n_offsets = 2` and\n        `agg_func = np.mean`, the predicted value will be the mean of the values\n        observed 7 and 14 days ago.\n    agg_func : Callable, default `np.mean`\n        Function used to aggregate the values of the equivalent dates when the\n        number of equivalent dates (`n_offsets`) is greater than 1.\n    forecaster_id : str, int, default `None`\n        Name used as an identifier of the forecaster.\n    \n    Attributes\n    ----------\n    offset : int, pandas.tseries.offsets.DateOffset\n        Number of steps to go back in time to find the most recent equivalent\n        date to the target period.\n        If `offset` is an integer, it represents the number of steps to go back\n        in time. For example, if the frequency of the time series is daily, \n        `offset = 7` means that the most recent data similar to the target\n        period is the value observed 7 days ago.\n        Pandas DateOffsets can also be used to move forward a given number of \n        valid dates. For example, Bday(2) can be used to move back two business \n        days. If the date does not start on a valid date, it is first moved to a \n        valid date. For example, if the date is a Saturday, it is moved to the \n        previous Friday. Then, the offset is applied. If the result is a non-valid \n        date, it is moved to the next valid date. For example, if the date\n        is a Sunday, it is moved to the next Monday. \n        For more information about offsets, see\n        https://pandas.pydata.org/docs/reference/offset_frequency.html.\n    n_offsets : int\n        Number of equivalent dates (multiple of offset) used in the prediction.\n        If `offset` is greater than 1, the value at the equivalent dates is\n        aggregated using the `agg_func` function. For example, if the frequency\n        of the time series is daily, `offset = 7`, `n_offsets = 2` and\n        `agg_func = np.mean`, the predicted value will be the mean of the values\n        observed 7 and 14 days ago.\n    agg_func : Callable\n        Function used to aggregate the values of the equivalent dates when the\n        number of equivalent dates (`n_offsets`) is greater than 1.\n    window_size : int\n        Number of past values needed to include the last equivalent dates according\n        to the `offset` and `n_offsets`.\n    last_window_ : pandas Series\n        This window represents the most recent data observed by the predictor\n        during its training phase. It contains the past values needed to include\n        the last equivalent date according the `offset` and `n_offsets`.\n    index_type_ : type\n        Type of index of the input used in training.\n    index_freq_ : str\n        Frequency of Index of the input used in training.\n    training_range_ : pandas Index\n        First and last values of index of the data used during training.\n    creation_date : str\n        Date of creation.\n    is_fitted : bool\n        Tag to identify if the regressor has been fitted (trained).\n    fit_date : str\n        Date of last fit.\n    skforecast_version : str\n        Version of skforecast library used to create the forecaster.\n    python_version : str\n        Version of python used to create the forecaster.\n    forecaster_id : str, int\n        Name used as an identifier of the forecaster.\n    regressor : Ignored\n        Not used, present here for API consistency by convention.\n    differentiation : Ignored\n        Not used, present here for API consistency by convention.\n\n    \"\"\"\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Information displayed when a Forecaster object is printed.\n        \"\"\"\n        info = f'{'=' * len(type(self).__name__)} \\n{type(self).__name__} \\n{'=' * len(type(self).__name__)} \\nOffset: {self.offset} \\nNumber of offsets: {self.n_offsets} \\nAggregation function: {self.agg_func.__name__} \\nWindow size: {self.window_size} \\nTraining range: {(self.training_range_.to_list() if self.is_fitted else None)} \\nTraining index type: {(str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None)} \\nTraining index frequency: {(self.index_freq_ if self.is_fitted else None)} \\nCreation date: {self.creation_date} \\nLast fit date: {self.fit_date} \\nSkforecast version: {self.skforecast_version} \\nPython version: {self.python_version} \\nForecaster id: {self.forecaster_id} \\n'\n        return info\n\n    def summary(self) -> None:\n        \"\"\"\n        Show forecaster information.\n        \n        Parameters\n        ----------\n        self\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        print(self)"
  },
  "call_tree": {
    "skforecast/recursive/tests/tests_forecaster_equivalent_date/test_backtesting.py:test_backtesting_with_ForecasterEquivalentDate": {
      "skforecast/recursive/_forecaster_equivalent_date.py:ForecasterEquivalentDate:__init__": {},
      "skforecast/model_selection/_split.py:TimeSeriesFold:__init__": {
        "skforecast/model_selection/_split.py:BaseFold:__init__": {
          "skforecast/model_selection/_split.py:BaseFold:_validate_params": {}
        }
      },
      "skforecast/model_selection/_validation.py:backtesting_forecaster": {
        "skforecast/model_selection/_utils.py:check_backtesting_input": {},
        "skforecast/model_selection/_validation.py:_backtesting_forecaster": {
          "skforecast/model_selection/_split.py:BaseFold:set_params": {
            "skforecast/model_selection/_split.py:BaseFold:_validate_params": {}
          },
          "skforecast/model_selection/_utils.py:select_n_jobs_backtesting": {},
          "skforecast/metrics/metrics.py:_get_metric": {
            "skforecast/metrics/metrics.py:add_y_train_argument": {}
          },
          "skforecast/model_selection/_split.py:TimeSeriesFold:split": {
            "skforecast/model_selection/_split.py:BaseFold:_extract_index": {}
          },
          "skforecast/recursive/_forecaster_equivalent_date.py:ForecasterEquivalentDate:fit": {
            "skforecast/utils/utils.py:preprocess_y": {}
          },
          "skforecast/model_selection/_validation.py:_fit_predict_forecaster": {
            "skforecast/recursive/_forecaster_equivalent_date.py:ForecasterEquivalentDate:predict": {
              "skforecast/utils/utils.py:check_predict_input": {
                "skforecast/utils/utils.py:preprocess_last_window": {}
              },
              "skforecast/utils/utils.py:preprocess_last_window": {},
              "skforecast/utils/utils.py:expand_index": {}
            },
            "skforecast/recursive/_forecaster_equivalent_date.py:ForecasterEquivalentDate:fit": {
              "skforecast/utils/utils.py:preprocess_y": {}
            }
          },
          "skforecast/metrics/metrics.py:wrapper": {}
        }
      }
    }
  }
}