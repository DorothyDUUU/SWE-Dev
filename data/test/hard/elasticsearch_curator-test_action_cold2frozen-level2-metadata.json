{
  "dir_path": "/app/elasticsearch_curator",
  "package_name": "elasticsearch_curator",
  "sample_name": "elasticsearch_curator-test_action_cold2frozen",
  "src_dir": "curator/",
  "test_dir": "tests/",
  "test_file": "tests/unit/test_action_cold2frozen.py",
  "test_code": "\"\"\"test_action_cold2frozen\"\"\"\n# pylint: disable=attribute-defined-outside-init\nfrom unittest import TestCase\nfrom unittest.mock import Mock\nimport pytest\nfrom curator.actions import Cold2Frozen\nfrom curator.exceptions import CuratorException, SearchableSnapshotException\nfrom curator import IndexList\n# Get test variables and constants from a single source\nfrom . import testvars\n\nclass TestActionCold2Frozen(TestCase):\n    \"\"\"TestActionCold2Frozen\"\"\"\n    VERSION = {'version': {'number': '8.0.0'} }\n    def builder(self):\n        \"\"\"Environment builder\"\"\"\n        self.client = Mock()\n        self.client.info.return_value = self.VERSION\n        self.client.cat.indices.return_value = testvars.state_one\n        self.client.indices.get_settings.return_value = testvars.settings_one\n        self.client.indices.stats.return_value = testvars.stats_one\n        self.client.indices.exists_alias.return_value = False\n        self.ilo = IndexList(self.client)\n    def test_init_raise_bad_index_list(self):\n        \"\"\"test_init_raise_bad_index_list\"\"\"\n        self.assertRaises(TypeError, Cold2Frozen, 'invalid')\n        with pytest.raises(TypeError):\n            Cold2Frozen('not_an_IndexList')\n    def test_init_add_kwargs(self):\n        \"\"\"test_init_add_kwargs\"\"\"\n        self.builder()\n        testval = {'key': 'value'}\n        c2f = Cold2Frozen(self.ilo, index_settings=testval)\n        assert c2f.index_settings == testval\n    def test_action_generator1(self):\n        \"\"\"test_action_generator1\"\"\"\n        self.builder()\n        settings_ss   = {\n            testvars.named_index: {\n                'aliases': {'my_alias': {}},\n                'settings': {\n                    'index': {\n                        'creation_date': '1456963200172',\n                        'refresh_interval': '5s',\n                        'lifecycle': {\n                            'indexing_complete': True\n                        },\n                        'store': {\n                            'type': 'snapshot',\n                            'snapshot': {\n                                'snapshot_name': 'snapname',\n                                'index_name': testvars.named_index,\n                                'repository_name': 'reponame',\n                            }\n                        }\n                    }\n                }\n            }\n        }\n\n        self.client.indices.get_settings.return_value = settings_ss\n        self.client.indices.get_alias.return_value = settings_ss\n        roles = ['data_content']\n        self.client.nodes.info.return_value = {'nodes': {'nodename': {'roles': roles}}}\n        c2f = Cold2Frozen(self.ilo)\n        snap = 'snapname'\n        repo = 'reponame'\n        renamed = f'partial-{testvars.named_index}'\n        settings = {\n            \"routing\": {\n                \"allocation\": {\n                    \"include\": {\n                        \"_tier_preference\": roles[0]\n                    }\n                }\n            }\n        }\n        expected = {\n                'repository': repo, 'snapshot': snap, 'index': testvars.named_index,\n                'renamed_index': renamed, 'index_settings': settings,\n                'ignore_index_settings': ['index.refresh_interval'],\n                'storage': 'shared_cache', 'wait_for_completion': True,\n                'aliases': {'my_alias': {}}, 'current_idx': testvars.named_index\n        }\n        for result in c2f.action_generator():\n            assert result == expected\n        c2f.do_dry_run() # Do this here as it uses the same generator output.\n    def test_action_generator2(self):\n        \"\"\"test_action_generator2\"\"\"\n        self.builder()\n        settings_ss   = {\n            testvars.named_index: {\n                'settings': {'index': {'lifecycle': {'name': 'guaranteed_fail'}}}\n            }\n        }\n        self.client.indices.get_settings.return_value = settings_ss\n        c2f = Cold2Frozen(self.ilo)\n        with pytest.raises(CuratorException, match='associated with an ILM policy'):\n            for result in c2f.action_generator():\n                _ = result\n    def test_action_generator3(self):\n        \"\"\"test_action_generator3\"\"\"\n        self.builder()\n        settings_ss   = {\n            testvars.named_index: {\n                'settings': {\n                    'index': {\n                        'lifecycle': {'indexing_complete': True},\n                        'store': {'snapshot': {'partial': True}}\n                    }\n                }\n            }\n        }\n        self.client.indices.get_settings.return_value = settings_ss\n        c2f = Cold2Frozen(self.ilo)\n        with pytest.raises(SearchableSnapshotException, match='Index is already in frozen tier'):\n            for result in c2f.action_generator():\n                _ = result\n",
  "GT_file_code": {
    "curator/helpers/getters.py": "\"\"\"Utility functions that get things\"\"\"\n\nimport logging\nfrom elasticsearch8 import exceptions as es8exc\nfrom curator.exceptions import (\n    ConfigurationError,\n    CuratorException,\n    FailedExecution,\n    MissingArgument,\n)\n\n\ndef byte_size(num, suffix='B'):\n    \"\"\"\n    :param num: The number of byte\n    :param suffix: An arbitrary suffix, like ``Bytes``\n\n    :type num: int\n    :type suffix: str\n\n    :returns: A formatted string indicating the size in bytes, with the proper unit,\n        e.g. KB, MB, GB, TB, etc.\n    :rtype: float\n    \"\"\"\n    for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:\n        if abs(num) < 1024.0:\n            return f'{num:3.1f}{unit}{suffix}'\n        num /= 1024.0\n    return f'{num:.1f}Y{suffix}'\n\n\ndef escape_dots(stringval):\n    \"\"\"\n    Escape any dots (periods) in ``stringval``.\n\n    Primarily used for ``filter_path`` where dots are indicators of path nesting\n\n    :param stringval: A string, ostensibly an index name\n\n    :type stringval: str\n\n    :returns: ``stringval``, but with any periods escaped with a backslash\n    :retval: str\n    \"\"\"\n    return stringval.replace('.', r'\\.')\n\n\ndef get_alias_actions(oldidx, newidx, aliases):\n    \"\"\"\n    :param oldidx: The old index name\n    :param newidx: The new index name\n    :param aliases: The aliases\n\n    :type oldidx: str\n    :type newidx: str\n    :type aliases: dict\n\n    :returns: A list of actions suitable for\n        :py:meth:`~.elasticsearch.client.IndicesClient.update_aliases` ``actions``\n        kwarg.\n    :rtype: list\n    \"\"\"\n    actions = []\n    for alias in aliases.keys():\n        actions.append({'remove': {'index': oldidx, 'alias': alias}})\n        actions.append({'add': {'index': newidx, 'alias': alias}})\n    return actions\n\n\ndef get_data_tiers(client):\n    \"\"\"\n    Get all valid data tiers from the node roles of each node in the cluster by\n    polling each node\n\n    :param client: A client connection object\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n\n    :returns: The available data tiers in ``tier: bool`` form.\n    :rtype: dict\n    \"\"\"\n\n    def role_check(role, node_info):\n        if role in node_info['roles']:\n            return True\n        return False\n\n    info = client.nodes.info()['nodes']\n    retval = {\n        'data_hot': False,\n        'data_warm': False,\n        'data_cold': False,\n        'data_frozen': False,\n    }\n    for node in info:\n        for role in ['data_hot', 'data_warm', 'data_cold', 'data_frozen']:\n            # This guarantees we don't overwrite a True with a False.\n            # We only add True values\n            if role_check(role, info[node]):\n                retval[role] = True\n    return retval\n\n\ndef get_indices(client, search_pattern='_all'):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.CatClient.indices`\n\n    :param client: A client connection object\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n\n    :returns: The current list of indices from the cluster\n    :rtype: list\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    indices = []\n    try:\n        # Doing this in two stages because IndexList also calls for these args,\n        # and the unit tests need to Mock this call the same exact way.\n        resp = client.cat.indices(\n            index=search_pattern,\n            expand_wildcards='open,closed',\n            h='index,status',\n            format='json',\n        )\n    except Exception as err:\n        raise FailedExecution(f'Failed to get indices. Error: {err}') from err\n    if not resp:\n        return indices\n    for entry in resp:\n        indices.append(entry['index'])\n    logger.debug('All indices: %s', indices)\n    return indices\n\n\ndef get_repository(client, repository=''):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get_repository`\n\n    :param client: A client connection object\n    :param repository: The Elasticsearch snapshot repository to use\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type repository: str\n\n    :returns: Configuration information for ``repository``.\n    :rtype: dict\n    \"\"\"\n    try:\n        return client.snapshot.get_repository(name=repository)\n    except (es8exc.TransportError, es8exc.NotFoundError) as err:\n        msg = (\n            f'Unable to get repository {repository}.  Error: {err} Check Elasticsearch '\n            f'logs for more information.'\n        )\n        raise CuratorException(msg) from err\n\n\ndef get_snapshot(client, repository=None, snapshot=''):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get`\n\n    :param client: A client connection object\n    :param repository: The Elasticsearch snapshot repository to use\n    :param snapshot: The snapshot name, or a comma-separated list of snapshots\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type repository: str\n    :type snapshot: str\n\n    :returns: Information about the provided ``snapshot``, a snapshot (or a\n        comma-separated list of snapshots). If no snapshot specified, it will\n        collect info for all snapshots.  If none exist, an empty :py:class:`dict`\n        will be returned.\n    :rtype: dict\n    \"\"\"\n    if not repository:\n        raise MissingArgument('No value for \"repository\" provided')\n    snapname = '*' if snapshot == '' else snapshot\n    try:\n        return client.snapshot.get(repository=repository, snapshot=snapshot)\n    except (es8exc.TransportError, es8exc.NotFoundError) as err:\n        msg = (\n            f'Unable to get information about snapshot {snapname} from repository: '\n            f'{repository}.  Error: {err}'\n        )\n        raise FailedExecution(msg) from err\n\n\ndef get_snapshot_data(client, repository=None):\n    \"\"\"\n    Get all snapshots from repository and return a list.\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get`\n\n    :param client: A client connection object\n    :param repository: The Elasticsearch snapshot repository to use\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type repository: str\n\n    :returns: The list of all snapshots from ``repository``\n    :rtype: list\n    \"\"\"\n    if not repository:\n        raise MissingArgument('No value for \"repository\" provided')\n    try:\n        return client.snapshot.get(repository=repository, snapshot=\"*\")['snapshots']\n    except (es8exc.TransportError, es8exc.NotFoundError) as err:\n        msg = (\n            f'Unable to get snapshot information from repository: '\n            f'{repository}. Error: {err}'\n        )\n        raise FailedExecution(msg) from err\n\n\ndef get_tier_preference(client, target_tier='data_frozen'):\n    \"\"\"Do the tier preference thing in reverse order from coldest to hottest\n    Based on the value of ``target_tier``, build out the list to use.\n\n    :param client: A client connection object\n    :param target_tier: The target data tier, e.g. data_warm.\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type target_tier: str\n\n    :returns: A suitable tier preference string in csv format\n    :rtype: str\n    \"\"\"\n    tiermap = {\n        'data_content': 0,\n        'data_hot': 1,\n        'data_warm': 2,\n        'data_cold': 3,\n        'data_frozen': 4,\n    }\n    tiers = get_data_tiers(client)\n    test_list = []\n    for tier in ['data_hot', 'data_warm', 'data_cold', 'data_frozen']:\n        if tier in tiers and tiermap[tier] <= tiermap[target_tier]:\n            test_list.insert(0, tier)\n    if target_tier == 'data_frozen':\n        # We're migrating to frozen here. If a frozen tier exists, frozen searchable\n        # snapshot mounts should only ever go to the frozen tier.\n        if 'data_frozen' in tiers and tiers['data_frozen']:\n            return 'data_frozen'\n    # If there are no  nodes with the 'data_frozen' role...\n    preflist = []\n    for key in test_list:\n        # This ordering ensures that colder tiers are prioritized\n        if key in tiers and tiers[key]:\n            preflist.append(key)\n    # If all of these are false, then we have no data tiers and must use 'data_content'\n    if not preflist:\n        return 'data_content'\n    # This will join from coldest to hottest as csv string,\n    # e.g. 'data_cold,data_warm,data_hot'\n    return ','.join(preflist)\n\n\ndef get_write_index(client, alias):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.get_alias`\n\n    :param client: A client connection object\n    :param alias: An alias name\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type alias: str\n\n    :returns: The the index name associated with the alias that is designated\n        ``is_write_index``\n    :rtype: str\n    \"\"\"\n    try:\n        response = client.indices.get_alias(index=alias)\n    except Exception as exc:\n        raise CuratorException(f'Alias {alias} not found') from exc\n    # If there are more than one in the list, one needs to be the write index\n    # otherwise the alias is a one to many, and can't do rollover.\n    retval = None\n    if len(list(response.keys())) > 1:\n        for index in list(response.keys()):\n            try:\n                if response[index]['aliases'][alias]['is_write_index']:\n                    retval = index\n            except KeyError as exc:\n                raise FailedExecution(\n                    'Invalid alias: is_write_index not found in 1 to many alias'\n                ) from exc\n    else:\n        # There's only one, so this is it\n        retval = list(response.keys())[0]\n    return retval\n\n\ndef index_size(client, idx, value='total'):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.stats`\n\n    :param client: A client connection object\n    :param idx: An index name\n    :param value: One of either ``primaries`` or ``total``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type idx: str\n    :type value: str\n\n    :returns: The sum of either ``primaries`` or ``total`` shards for index ``idx``\n    :rtype: integer\n    \"\"\"\n    fpath = f'indices.{escape_dots(idx)}.{value}.store.size_in_bytes'\n    return client.indices.stats(index=idx, filter_path=fpath)['indices'][idx][value][\n        'store'\n    ]['size_in_bytes']\n\n\ndef meta_getter(client, idx, get=None):\n    \"\"\"Meta Getter\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.get_settings` or\n    :py:meth:`~.elasticsearch.client.IndicesClient.get_alias`\n\n    :param client: A client connection object\n    :param idx: An Elasticsearch index\n    :param get: The kind of get to perform, e.g. settings or alias\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type idx: str\n    :type get: str\n\n    :returns: The settings from the get call to the named index\n    :rtype: dict\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    acceptable = ['settings', 'alias']\n    if not get:\n        raise ConfigurationError('\"get\" can not be a NoneType')\n    if get not in acceptable:\n        raise ConfigurationError(f'\"get\" must be one of {acceptable}')\n    retval = {}\n    try:\n        if get == 'settings':\n            retval = client.indices.get_settings(index=idx)[idx]['settings']['index']\n        elif get == 'alias':\n            retval = client.indices.get_alias(index=idx)[idx]['aliases']\n    except es8exc.NotFoundError as missing:\n        logger.error('Index %s was not found!', idx)\n        raise es8exc.NotFoundError from missing\n    except KeyError as err:\n        logger.error('Key not found: %s', err)\n        raise KeyError from err\n    # pylint: disable=broad-except\n    except Exception as exc:\n        logger.error('Exception encountered: %s', exc)\n    return retval\n\n\ndef name_to_node_id(client, name):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.NodesClient.info`\n\n    :param client: A client connection object\n    :param name: The node ``name``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type name: str\n\n    :returns: The node_id of the node identified by ``name``\n    :rtype: str\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    fpath = 'nodes'\n    info = client.nodes.info(filter_path=fpath)\n    for node in info['nodes']:\n        if info['nodes'][node]['name'] == name:\n            logger.debug('Found node_id \"%s\" for name \"%s\".', node, name)\n            return node\n    logger.error('No node_id found matching name: \"%s\"', name)\n    return None\n\n\ndef node_id_to_name(client, node_id):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.NodesClient.info`\n\n    :param client: A client connection object\n    :param node_id: The node ``node_id``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type node_id: str\n\n    :returns: The name of the node identified by ``node_id``\n    :rtype: str\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    fpath = f'nodes.{node_id}.name'\n    info = client.nodes.info(filter_path=fpath)\n    name = None\n    if node_id in info['nodes']:\n        name = info['nodes'][node_id]['name']\n    else:\n        logger.error('No node_id found matching: \"%s\"', node_id)\n    logger.debug('Name associated with node_id \"%s\": %s', node_id, name)\n    return name\n\n\ndef node_roles(client, node_id):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.NodesClient.info`\n\n    :param client: A client connection object\n    :param node_id: The node ``node_id``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type node_id: str\n\n    :returns: The list of roles assigned to the node identified by ``node_id``\n    :rtype: list\n    \"\"\"\n    fpath = f'nodes.{node_id}.roles'\n    return client.nodes.info(filter_path=fpath)['nodes'][node_id]['roles']\n\n\ndef single_data_path(client, node_id):\n    \"\"\"\n    In order for a shrink to work, it should be on a single filesystem, as shards\n    cannot span filesystems. Calls :py:meth:`~.elasticsearch.client.NodesClient.stats`\n\n    :param client: A client connection object\n    :param node_id: The node ``node_id``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type node_id: str\n\n    :returns: ``True`` if the node has a single filesystem, else ``False``\n    :rtype: bool\n    \"\"\"\n    fpath = f'nodes.{node_id}.fs.data'\n    response = client.nodes.stats(filter_path=fpath)\n    return len(response['nodes'][node_id]['fs']['data']) == 1\n",
    "curator/helpers/testers.py": "\"\"\"Utility functions that get things\"\"\"\n\nimport logging\nfrom voluptuous import Schema\nfrom elasticsearch8 import Elasticsearch\nfrom elasticsearch8.exceptions import NotFoundError\nfrom es_client.helpers.schemacheck import SchemaCheck\nfrom es_client.helpers.utils import prune_nones\nfrom curator.helpers.getters import get_repository, get_write_index\nfrom curator.exceptions import (\n    ConfigurationError,\n    MissingArgument,\n    RepositoryException,\n    SearchableSnapshotException,\n)\nfrom curator.defaults.settings import (\n    index_filtertypes,\n    snapshot_actions,\n    snapshot_filtertypes,\n)\nfrom curator.validators import actions, options\nfrom curator.validators.filter_functions import validfilters\nfrom curator.helpers.utils import report_failure\n\n\ndef has_lifecycle_name(idx_settings):\n    \"\"\"\n    :param idx_settings: The settings for an index being tested\n    :type idx_settings: dict\n\n    :returns: ``True`` if a lifecycle name exists in settings, else ``False``\n    :rtype: bool\n    \"\"\"\n    if 'lifecycle' in idx_settings:\n        if 'name' in idx_settings['lifecycle']:\n            return True\n    return False\n\n\ndef is_idx_partial(idx_settings):\n    \"\"\"\n    :param idx_settings: The settings for an index being tested\n    :type idx_settings: dict\n\n    :returns: ``True`` if store.snapshot.partial exists in settings, else ``False``\n    :rtype: bool\n    \"\"\"\n    if 'store' in idx_settings:\n        if 'snapshot' in idx_settings['store']:\n            if 'partial' in idx_settings['store']['snapshot']:\n                if idx_settings['store']['snapshot']['partial']:\n                    return True\n                # store.snapshot.partial exists but is False -- Not a frozen tier mount\n                return False\n            # store.snapshot exists, but partial isn't there --\n            # Possibly a cold tier mount\n            return False\n        raise SearchableSnapshotException('Index not a mounted searchable snapshot')\n    raise SearchableSnapshotException('Index not a mounted searchable snapshot')\n\n\ndef ilm_policy_check(client, alias):\n    \"\"\"Test if alias is associated with an ILM policy\n\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.get_settings`\n\n    :param client: A client connection object\n    :param alias: The alias name\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type alias: str\n    :rtype: bool\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    # alias = action_obj.options['name']\n    write_index = get_write_index(client, alias)\n    try:\n        idx_settings = client.indices.get_settings(index=write_index)\n        if 'name' in idx_settings[write_index]['settings']['index']['lifecycle']:\n            # logger.info('Alias %s is associated with ILM policy.', alias)\n            # logger.info('Skipping action %s because allow_ilm_indices is false.', idx)\n            return True\n    except KeyError:\n        logger.debug('No ILM policies associated with %s', alias)\n    return False\n\n\ndef repository_exists(client, repository=None):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get_repository`\n\n    :param client: A client connection object\n    :param repository: The Elasticsearch snapshot repository to use\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type repository: str\n\n    :returns: ``True`` if ``repository`` exists, else ``False``\n    :rtype: bool\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    if not repository:\n        raise MissingArgument('No value for \"repository\" provided')\n    try:\n        test_result = get_repository(client, repository)\n        if repository in test_result:\n            logger.debug(\"Repository %s exists.\", repository)\n            response = True\n        else:\n            logger.debug(\"Repository %s not found...\", repository)\n            response = False\n    # pylint: disable=broad-except\n    except Exception as err:\n        logger.debug('Unable to find repository \"%s\": Error: %s', repository, err)\n        response = False\n    return response\n\n\ndef rollable_alias(client, alias):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.get_alias`\n\n    :param client: A client connection object\n    :param alias: An Elasticsearch alias\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type alias: str\n\n\n    :returns: ``True`` or ``False`` depending on whether ``alias`` is an alias that\n        points to an index that can be used by the ``_rollover`` API.\n    :rtype: bool\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    try:\n        response = client.indices.get_alias(name=alias)\n    except NotFoundError:\n        logger.error('Alias \"%s\" not found.', alias)\n        return False\n    # Response should be like:\n    # {'there_should_be_only_one': {'aliases': {'value of \"alias\" here': {}}}}\n    # where 'there_should_be_only_one' is a single index name that ends in a number,\n    # and 'value of \"alias\" here' reflects the value of the passed parameter, except\n    # where the ``is_write_index`` setting makes it possible to have more than one\n    # index associated with a rollover index\n    for idx in response:\n        if 'is_write_index' in response[idx]['aliases'][alias]:\n            if response[idx]['aliases'][alias]['is_write_index']:\n                return True\n    # implied ``else``: If not ``is_write_index``, it has to fit the following criteria:\n    if len(response) > 1:\n        logger.error(\n            '\"alias\" must only reference one index, but points to %s', response\n        )\n        return False\n    index = list(response.keys())[0]\n    rollable = False\n    # In order for `rollable` to be True, the last 2 digits of the index\n    # must be digits, or a hyphen followed by a digit.\n    # NOTE: This is not a guarantee that the rest of the index name is\n    # necessarily correctly formatted.\n    if index[-2:][1].isdigit():\n        if index[-2:][0].isdigit():\n            rollable = True\n        elif index[-2:][0] == '-':\n            rollable = True\n    return rollable\n\n\ndef snapshot_running(client):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get_repository`\n\n    Return ``True`` if a snapshot is in progress, and ``False`` if not\n\n    :param client: A client connection object\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n\n    :rtype: bool\n    \"\"\"\n    try:\n        status = client.snapshot.status()['snapshots']\n    # pylint: disable=broad-except\n    except Exception as exc:\n        report_failure(exc)\n    # We will only accept a positively identified False.  Anything else is\n    # suspect. That's why this statement, rather than just ``return status``\n    # pylint: disable=simplifiable-if-expression\n    return False if not status else True\n\n\ndef validate_actions(data):\n    \"\"\"\n    Validate the ``actions`` configuration dictionary, as imported from actions.yml,\n    for example.\n\n    :param data: The configuration dictionary\n\n    :type data: dict\n\n    :returns: The validated and sanitized configuration dictionary.\n    :rtype: dict\n    \"\"\"\n    # data is the ENTIRE schema...\n    clean_config = {}\n    # Let's break it down into smaller chunks...\n    # First, let's make sure it has \"actions\" as a key, with a subdictionary\n    root = SchemaCheck(data, actions.root(), 'Actions File', 'root').result()\n    # We've passed the first step.  Now let's iterate over the actions...\n    for action_id in root['actions']:\n        # Now, let's ensure that the basic action structure is correct, with\n        # the proper possibilities for 'action'\n        action_dict = root['actions'][action_id]\n        loc = f'Action ID \"{action_id}\"'\n        valid_structure = SchemaCheck(\n            action_dict, actions.structure(action_dict, loc), 'structure', loc\n        ).result()\n        # With the basic structure validated, now we extract the action name\n        current_action = valid_structure['action']\n        # And let's update the location with the action.\n        loc = f'Action ID \"{action_id}\", action \"{current_action}\"'\n        clean_options = SchemaCheck(\n            prune_nones(valid_structure['options']),\n            options.get_schema(current_action),\n            'options',\n            loc,\n        ).result()\n        clean_config[action_id] = {\n            'action': current_action,\n            'description': valid_structure['description'],\n            'options': clean_options,\n        }\n        if current_action == 'alias':\n            add_remove = {}\n            for k in ['add', 'remove']:\n                if k in valid_structure:\n                    current_filters = SchemaCheck(\n                        valid_structure[k]['filters'],\n                        Schema(validfilters(current_action, location=loc)),\n                        f'\"{k}\" filters',\n                        f'{loc}, \"filters\"',\n                    ).result()\n                    add_remove.update(\n                        {\n                            k: {\n                                'filters': SchemaCheck(\n                                    current_filters,\n                                    Schema(validfilters(current_action, location=loc)),\n                                    'filters',\n                                    f'{loc}, \"{k}\", \"filters\"',\n                                ).result()\n                            }\n                        }\n                    )\n            # Add/Remove here\n            clean_config[action_id].update(add_remove)\n        elif current_action in ['cluster_routing', 'create_index', 'rollover']:\n            # neither cluster_routing nor create_index should have filters\n            pass\n        else:  # Filters key only appears in non-alias actions\n            valid_filters = SchemaCheck(\n                valid_structure['filters'],\n                Schema(validfilters(current_action, location=loc)),\n                'filters',\n                f'{loc}, \"filters\"',\n            ).result()\n            clean_filters = validate_filters(current_action, valid_filters)\n            clean_config[action_id].update({'filters': clean_filters})\n        # This is a special case for remote reindex\n        if current_action == 'reindex':\n            # Check only if populated with something.\n            if 'remote_filters' in valid_structure['options']:\n                valid_filters = SchemaCheck(\n                    valid_structure['options']['remote_filters'],\n                    Schema(validfilters(current_action, location=loc)),\n                    'filters',\n                    f'{loc}, \"filters\"',\n                ).result()\n                clean_remote_filters = validate_filters(current_action, valid_filters)\n                clean_config[action_id]['options'].update(\n                    {'remote_filters': clean_remote_filters}\n                )\n\n    # if we've gotten this far without any Exceptions raised, it's valid!\n    return {'actions': clean_config}\n\n\ndef validate_filters(action, myfilters):\n    \"\"\"\n    Validate that myfilters are appropriate for the action type, e.g. no\n    index filters applied to a snapshot list.\n\n    :param action: An action name\n    :param myfilters: A list of filters to test.\n\n    :type action: str\n    :type myfilters: list\n\n    :returns: Validated list of filters\n    :rtype: list\n    \"\"\"\n    # Define which set of filtertypes to use for testing\n    if action in snapshot_actions():\n        filtertypes = snapshot_filtertypes()\n    else:\n        filtertypes = index_filtertypes()\n    for fil in myfilters:\n        if fil['filtertype'] not in filtertypes:\n            raise ConfigurationError(\n                f\"\\\"{fil['filtertype']}\\\" filtertype is not compatible with \"\n                f\"action \\\"{action}\\\"\"\n            )\n    # If we get to this point, we're still valid.  Return the original list\n    return myfilters\n\n\ndef verify_client_object(test):\n    \"\"\"\n    :param test: The variable or object to test\n\n    :type test: :py:class:`~.elasticsearch.Elasticsearch`\n\n    :returns: ``True`` if ``test`` is a proper :py:class:`~.elasticsearch.Elasticsearch`\n        client object and raise a :py:exc:`TypeError` exception if it is not.\n    :rtype: bool\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    # Ignore mock type for testing\n    if str(type(test)) == \"<class 'unittest.mock.Mock'>\":\n        pass\n    elif not isinstance(test, Elasticsearch):\n        msg = f'Not a valid client object. Type: {type(test)} was passed'\n        logger.error(msg)\n        raise TypeError(msg)\n\n\ndef verify_index_list(test):\n    \"\"\"\n    :param test: The variable or object to test\n\n    :type test: :py:class:`~.curator.IndexList`\n\n    :returns: ``None`` if ``test`` is a proper :py:class:`~.curator.indexlist.IndexList`\n        object, else raise a :py:class:`TypeError` exception.\n    :rtype: None\n    \"\"\"\n    # It breaks if this import isn't local to this function:\n    # ImportError: cannot import name 'IndexList' from partially initialized module\n    # 'curator.indexlist' (most likely due to a circular import)\n    # pylint: disable=import-outside-toplevel\n    from curator.indexlist import IndexList\n\n    logger = logging.getLogger(__name__)\n    if not isinstance(test, IndexList):\n        msg = f'Not a valid IndexList object. Type: {type(test)} was passed'\n        logger.error(msg)\n        raise TypeError(msg)\n\n\ndef verify_repository(client, repository=None):\n    \"\"\"\n    Do :py:meth:`~.elasticsearch.snapshot.verify_repository` call. If it fails, raise a\n    :py:exc:`~.curator.exceptions.RepositoryException`.\n\n    :param client: A client connection object\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :param repository: A repository name\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type repository: str\n\n    :rtype: None\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    try:\n        nodes = client.snapshot.verify_repository(name=repository)['nodes']\n        logger.debug('All nodes can write to the repository')\n        logger.debug('Nodes with verified repository access: %s', nodes)\n    except Exception as err:\n        try:\n            if err.status_code == 404:\n                msg = (\n                    f'--- Repository \"{repository}\" not found. Error: '\n                    f'{err.meta.status}, {err.error}'\n                )\n            else:\n                msg = (\n                    f'--- Got a {err.meta.status} response from Elasticsearch.  '\n                    f'Error message: {err.error}'\n                )\n        except AttributeError:\n            msg = f'--- Error message: {err}'.format()\n        report = f'Failed to verify all nodes have repository access: {msg}'\n        raise RepositoryException(report) from err\n\n\ndef verify_snapshot_list(test):\n    \"\"\"\n    :param test: The variable or object to test\n\n    :type test: :py:class:`~.curator.SnapshotList`\n\n    :returns: ``None`` if ``test`` is a proper\n        :py:class:`~.curator.snapshotlist.SnapshotList` object, else raise a\n        :py:class:`TypeError` exception.\n    :rtype: None\n    \"\"\"\n    # It breaks if this import isn't local to this function:\n    # ImportError: cannot import name 'SnapshotList' from partially initialized module\n    # 'curator.snapshotlist' (most likely due to a circular import)\n    # pylint: disable=import-outside-toplevel\n    from curator.snapshotlist import SnapshotList\n\n    logger = logging.getLogger(__name__)\n    if not isinstance(test, SnapshotList):\n        msg = f'Not a valid SnapshotList object. Type: {type(test)} was passed'\n        logger.error(msg)\n        raise TypeError(msg)\n",
    "curator/actions/cold2frozen.py": "\"\"\"Snapshot and Restore action classes\"\"\"\nimport logging\nfrom curator.helpers.getters import get_alias_actions, get_tier_preference, meta_getter\nfrom curator.helpers.testers import has_lifecycle_name, is_idx_partial, verify_index_list\nfrom curator.helpers.utils import report_failure\nfrom curator.exceptions import CuratorException, FailedExecution, SearchableSnapshotException\n\nclass Cold2Frozen:\n    \"\"\"Cold to Frozen Tier Searchable Snapshot Action Class\n\n    For manually migrating snapshots not associated with ILM from the cold tier to the frozen tier.\n    \"\"\"\n\n    DEFAULTS = {\n        'index_settings': None,\n        'ignore_index_settings': ['index.refresh_interval'],\n        'wait_for_completion': True,\n    }\n    def __init__(self, ilo, **kwargs):\n        \"\"\"\n        :param ilo: An IndexList Object\n        :param index_settings: (Optional) Settings that should be added to the index when it is\n            mounted. If not set, set the ``_tier_preference`` to the tiers available, coldest\n            first.\n        :param ignore_index_settings: (Optional, array of strings) Names of settings that should\n            be removed from the index when it is mounted.\n        :param wait_for_completion: Wait for completion before returning.\n\n        :type ilo: :py:class:`~.curator.indexlist.IndexList`\n        :type index_settings: dict\n        :type ignore_index_settings: list\n        :type wait_for_completion: bool\n        \"\"\"\n        self.loggit = logging.getLogger('curator.actions.cold2frozen')\n        verify_index_list(ilo)\n        # Check here and don't bother with the rest of this if there are no\n        # indices in the index list.\n        ilo.empty_list_check()\n\n        #: The :py:class:`~.curator.indexlist.IndexList` object passed from param ``ilo``\n        self.index_list = ilo\n        #: The :py:class:`~.elasticsearch.Elasticsearch` client object derived from\n        #: :py:attr:`index_list`\n        self.client = ilo.client\n        #: Object attribute that contains the :py:func:`~.curator.helpers.utils.to_csv` output of\n        #: the indices in :py:attr:`index_list`.\n        self.indices = ilo\n        #: Object attribute that gets the value of ``index_settings``.\n        self.index_settings = None\n        #: Object attribute that gets the value of ``ignore_index_settings``.\n        self.ignore_index_settings = None\n        #: Object attribute that gets the value of param ``wait_for_completion``.\n        self.wait_for_completion = None\n\n        # Parse the kwargs into attributes\n        self.assign_kwargs(**kwargs)\n\n    def assign_kwargs(self, **kwargs):\n        \"\"\"\n        Assign the kwargs to the attribute of the same name with the passed value or the default\n        from DEFAULTS\n        \"\"\"\n        # Handy little loop here only adds kwargs that exist in DEFAULTS, or the default value.\n        # It ignores any non-relevant kwargs\n        for key, value in self.DEFAULTS.items():\n            if key in kwargs:\n                setattr(self, key, kwargs[key])\n            else:\n                setattr(self, key, value)\n\n    def action_generator(self):\n        \"\"\"Yield a dict for use in :py:meth:`do_action` and :py:meth:`do_dry_run`\n\n        :returns: A generator object containing the settings necessary to migrate indices from cold\n            to frozen\n        :rtype: dict\n        \"\"\"\n        for idx in self.index_list.indices:\n            idx_settings = meta_getter(self.client, idx, get='settings')\n            self.loggit.debug('Index %s has settings: %s', idx, idx_settings)\n            if has_lifecycle_name(idx_settings):\n                self.loggit.critical(\n                    'Index %s is associated with an ILM policy and this action will never work on '\n                    'an index associated with an ILM policy', idx)\n                raise CuratorException(f'Index {idx} is associated with an ILM policy')\n            if is_idx_partial(idx_settings):\n                self.loggit.critical('Index %s is already in the frozen tier', idx)\n                raise SearchableSnapshotException('Index is already in frozen tier')\n\n            snap = idx_settings['store']['snapshot']['snapshot_name']\n            snap_idx = idx_settings['store']['snapshot']['index_name']\n            repo = idx_settings['store']['snapshot']['repository_name']\n            msg = f'Index {idx} Snapshot name: {snap}, Snapshot index: {snap_idx}, repo: {repo}'\n            self.loggit.debug(msg)\n\n            aliases = meta_getter(self.client, idx, get='alias')\n\n            renamed = f'partial-{idx}'\n\n            if not self.index_settings:\n                self.index_settings = {\n                    \"routing\": {\n                        \"allocation\": {\n                            \"include\": {\n                                \"_tier_preference\": get_tier_preference(self.client)\n                            }\n                        }\n                    }\n                }\n            yield {\n                'repository': repo, 'snapshot': snap, 'index': snap_idx,\n                'renamed_index': renamed, 'index_settings': self.index_settings,\n                'ignore_index_settings': self.ignore_index_settings,\n                'storage': 'shared_cache', 'wait_for_completion': self.wait_for_completion,\n                'aliases': aliases, 'current_idx': idx\n            }\n\n    def do_dry_run(self):\n        \"\"\"Log what the output would be, but take no action.\"\"\"\n        self.loggit.info('DRY-RUN MODE.  No changes will be made.')\n        for kwargs in self.action_generator():\n            aliases = kwargs.pop('aliases')\n            current_idx = kwargs.pop('current_idx')\n            msg = (\n                f'DRY-RUN: cold2frozen: from snapshot {kwargs[\"snapshot\"]} in repository '\n                f'{kwargs[\"repository\"]}, mount index {kwargs[\"index\"]} renamed as '\n                f'{kwargs[\"renamed_index\"]} with index settings: {kwargs[\"index_settings\"]} '\n                f'and ignoring settings: {kwargs[\"ignore_index_settings\"]}. wait_for_completion: '\n                f'{kwargs[\"wait_for_completion\"]}. Restore aliases: {aliases}. Current index '\n                f'name: {current_idx}'\n            )\n            self.loggit.info(msg)\n\n    def mount_index(self, newidx, kwargs):\n        \"\"\"\n        Call :py:meth:`~.elasticsearch.client.SearchableSnapshotsClient.mount` to mount the indices\n        in :py:attr:`ilo` in the Frozen tier.\n        \"\"\"\n        try:\n            self.loggit.debug('Mounting new index %s in frozen tier...', newidx)\n            self.client.searchable_snapshots.mount(**kwargs)\n        # pylint: disable=broad-except\n        except Exception as err:\n            report_failure(err)\n\n    def verify_mount(self, newidx):\n        \"\"\"\n        Verify that newidx is a mounted index\n        \"\"\"\n        self.loggit.debug('Verifying new index %s is mounted properly...', newidx)\n        idx_settings = self.client.indices.get(index=newidx)[newidx]\n        if is_idx_partial(idx_settings['settings']['index']):\n            self.loggit.info('Index %s is mounted for frozen tier', newidx)\n        else:\n            report_failure(SearchableSnapshotException(\n                f'Index {newidx} not a mounted searchable snapshot'))\n\n    def update_aliases(self, current_idx, newidx, aliases):\n        \"\"\"\n        Call :py:meth:`~.elasticsearch.client.IndicesClient.update_aliases` to update each new\n        frozen index with the aliases from the old cold-tier index.\n\n        Verify aliases look good.\n        \"\"\"\n        alias_names = aliases.keys()\n        if not alias_names:\n            self.loggit.warning('No aliases associated with index %s', current_idx)\n        else:\n            self.loggit.debug('Transferring aliases to new index %s', newidx)\n            self.client.indices.update_aliases(\n                actions=get_alias_actions(current_idx, newidx, aliases))\n            verify = self.client.indices.get(index=newidx)[newidx]['aliases'].keys()\n            if alias_names != verify:\n                self.loggit.error(\n                    'Alias names do not match! %s does not match: %s', alias_names, verify)\n                report_failure(FailedExecution('Aliases failed to transfer to new index'))\n\n    def cleanup(self, current_idx, newidx):\n        \"\"\"\n        Call :py:meth:`~.elasticsearch.client.IndicesClient.delete` to delete the cold tier index.\n        \"\"\"\n        self.loggit.debug('Deleting old index: %s', current_idx)\n        try:\n            self.client.indices.delete(index=current_idx)\n        # pylint: disable=broad-except\n        except Exception as err:\n            report_failure(err)\n        self.loggit.info(\n            'Successfully migrated %s to the frozen tier as %s', current_idx, newidx)\n\n    def do_action(self):\n        \"\"\"\n        Do the actions outlined:\n        Extract values from generated kwargs\n        Mount\n        Verify\n        Update Aliases\n        Cleanup\n        \"\"\"\n        for kwargs in self.action_generator():\n            aliases = kwargs.pop('aliases')\n            current_idx = kwargs.pop('current_idx')\n            newidx = kwargs['renamed_index']\n\n            # Mount the index\n            self.mount_index(newidx, kwargs)\n\n            # Verify it's mounted as a partial now:\n            self.verify_mount(newidx)\n\n            # Update Aliases\n            self.update_aliases(current_idx, newidx, aliases)\n\n            # Clean up old index\n            self.cleanup(current_idx, newidx)\n",
    "curator/exceptions.py": "\"\"\"Curator Exceptions\"\"\"\nclass CuratorException(Exception):\n    \"\"\"\n    Base class for all exceptions raised by Curator which are not Elasticsearch\n    exceptions.\n    \"\"\"\n\nclass ConfigurationError(CuratorException):\n    \"\"\"\n    Exception raised when a misconfiguration is detected\n    \"\"\"\n\nclass MissingArgument(CuratorException):\n    \"\"\"\n    Exception raised when a needed argument is not passed.\n    \"\"\"\n\nclass NoIndices(CuratorException):\n    \"\"\"\n    Exception raised when an operation is attempted against an empty index_list\n    \"\"\"\n\nclass NoSnapshots(CuratorException):\n    \"\"\"\n    Exception raised when an operation is attempted against an empty snapshot_list\n    \"\"\"\n\nclass ActionError(CuratorException):\n    \"\"\"\n    Exception raised when an action (against an index_list or snapshot_list) cannot be taken.\n    \"\"\"\n\nclass FailedExecution(CuratorException):\n    \"\"\"\n    Exception raised when an action fails to execute for some reason.\n    \"\"\"\n\nclass SnapshotInProgress(ActionError):\n    \"\"\"\n    Exception raised when a snapshot is already in progress\n    \"\"\"\n\nclass ActionTimeout(CuratorException):\n    \"\"\"\n    Exception raised when an action fails to complete in the allotted time\n    \"\"\"\n\nclass FailedSnapshot(CuratorException):\n    \"\"\"\n    Exception raised when a snapshot does not complete with state SUCCESS\n    \"\"\"\n\nclass FailedRestore(CuratorException):\n    \"\"\"\n    Exception raised when a Snapshot Restore does not restore all selected indices\n    \"\"\"\n\nclass FailedReindex(CuratorException):\n    \"\"\"\n    Exception raised when failures are found in the reindex task response\n    \"\"\"\n\nclass ClientException(CuratorException):\n    \"\"\"\n    Exception raised when the Elasticsearch client and/or connection is the source of the problem.\n    \"\"\"\n\nclass LoggingException(CuratorException):\n    \"\"\"\n    Exception raised when Curator cannot either log or configure logging\n    \"\"\"\n\nclass RepositoryException(CuratorException):\n    \"\"\"\n    Exception raised when Curator cannot verify a snapshot repository\n    \"\"\"\n\nclass SearchableSnapshotException(CuratorException):\n    \"\"\"\n    Exception raised when Curator finds something out of order with a Searchable Snapshot\n    \"\"\"\n",
    "curator/indexlist.py": "\"\"\"Index List Class\"\"\"\n\nimport re\nimport itertools\nimport logging\nfrom elasticsearch8.exceptions import NotFoundError, TransportError\nfrom es_client.helpers.schemacheck import SchemaCheck\nfrom es_client.helpers.utils import ensure_list\nfrom curator.defaults import settings\nfrom curator.exceptions import (\n    ActionError,\n    ConfigurationError,\n    MissingArgument,\n    NoIndices,\n)\nfrom curator.helpers.date_ops import (\n    absolute_date_range,\n    date_range,\n    fix_epoch,\n    get_date_regex,\n    get_point_of_reference,\n    get_unit_count_from_name,\n    TimestringSearch,\n)\nfrom curator.helpers.getters import byte_size, get_indices\nfrom curator.helpers.testers import verify_client_object\nfrom curator.helpers.utils import chunk_index_list, report_failure, to_csv\nfrom curator.validators.filter_functions import filterstructure\n\n\nclass IndexList:\n    \"\"\"IndexList class\"\"\"\n\n    def __init__(self, client, search_pattern='_all'):\n        verify_client_object(client)\n        self.loggit = logging.getLogger('curator.indexlist')\n        #: An :py:class:`~.elasticsearch.Elasticsearch` client object passed from\n        #: param ``client``\n        self.client = client\n        #: Information extracted from indices, such as segment count, age, etc.\n        #: Populated at instance creation time by private helper methods.\n        #: **Type:** :py:class:`dict`\n        self.index_info = {}\n        #: The running list of indices which will be used by one of the\n        #: :py:mod:`~.curator.actions` classes. Populated at instance creation\n        #: time by private helper methods. **Type:** :py:class:`list`\n        self.indices = []\n        #: All indices in the cluster at instance creation time.\n        #: **Type:** :py:class:`list`\n        self.all_indices = []\n        self.__get_indices(search_pattern)\n        self.age_keyfield = None\n\n    def __actionable(self, idx):\n        self.loggit.debug('Index %s is actionable and remains in the list.', idx)\n\n    def __not_actionable(self, idx):\n        self.loggit.debug('Index %s is not actionable, removing from list.', idx)\n        self.indices.remove(idx)\n\n    def __excludify(self, condition, exclude, index, msg=None):\n        if condition is True:\n            if exclude:\n                text = \"Removed from actionable list\"\n                self.__not_actionable(index)\n            else:\n                text = \"Remains in actionable list\"\n                self.__actionable(index)\n        else:\n            if exclude:\n                text = \"Remains in actionable list\"\n                self.__actionable(index)\n            else:\n                text = \"Removed from actionable list\"\n                self.__not_actionable(index)\n        if msg:\n            self.loggit.debug('%s: %s', text, msg)\n\n    def __get_indices(self, pattern):\n        \"\"\"\n        Pull all indices into ``all_indices``, then populate ``indices`` and\n        ``index_info``\n        \"\"\"\n        self.loggit.debug('Getting indices matching search_pattern: \"%s\"', pattern)\n        self.all_indices = get_indices(self.client, search_pattern=pattern)\n        self.indices = self.all_indices[:]\n        # if self.indices:\n        #     for index in self.indices:\n        #         self.__build_index_info(index)\n\n    def __build_index_info(self, index):\n        \"\"\"\n        Ensure that ``index`` is a key in ``index_info``. If not, create a\n        sub-dictionary structure under that key.\n        \"\"\"\n        self.loggit.debug('Building preliminary index metadata for %s', index)\n        if index not in self.index_info:\n            self.index_info[index] = self.__zero_values()\n\n    def __map_method(self, ftype):\n        methods = {\n            'alias': self.filter_by_alias,\n            'age': self.filter_by_age,\n            'allocated': self.filter_allocated,\n            'closed': self.filter_closed,\n            'count': self.filter_by_count,\n            'empty': self.filter_empty,\n            'forcemerged': self.filter_forceMerged,\n            'ilm': self.filter_ilm,\n            'kibana': self.filter_kibana,\n            'none': self.filter_none,\n            'opened': self.filter_opened,\n            'period': self.filter_period,\n            'pattern': self.filter_by_regex,\n            'space': self.filter_by_space,\n            'shards': self.filter_by_shards,\n            'size': self.filter_by_size,\n        }\n        return methods[ftype]\n\n    def __remove_missing(self, err):\n        \"\"\"\n        Remove missing index found in ``err`` from self.indices and return that name\n        \"\"\"\n        missing = err.info['error']['index']\n        self.loggit.warning('Index was initiallly present, but now is not: %s', missing)\n        self.loggit.debug('Removing %s from active IndexList', missing)\n        self.indices.remove(missing)\n        return missing\n\n    def __zero_values(self):\n        \"\"\"The default values for index metadata\"\"\"\n        return {\n            'age': {'creation_date': 0, 'name': 0},\n            'docs': 0,\n            'number_of_replicas': 0,\n            'number_of_shards': 0,\n            'primary_size_in_bytes': 0,\n            'routing': {},\n            'segments': 0,\n            'size_in_bytes': 0,\n            'state': '',\n        }\n\n    def _get_indices_segments(self, data):\n        return self.client.indices.segments(index=to_csv(data))['indices'].copy()\n\n    def _get_indices_settings(self, data):\n        return self.client.indices.get_settings(index=to_csv(data))\n\n    def _get_indices_stats(self, data):\n        return self.client.indices.stats(index=to_csv(data), metric='store,docs')[\n            'indices'\n        ]\n\n    def _bulk_queries(self, data, exec_func):\n        slice_number = 10\n        query_result = {}\n        loop_number = (\n            round(len(data) / slice_number)\n            if round(len(data) / slice_number) > 0\n            else 1\n        )\n        self.loggit.debug(\"Bulk Queries - number requests created: %s\", loop_number)\n        for num in range(0, loop_number):\n            if num == (loop_number - 1):\n                data_sliced = data[num * slice_number :]\n            else:\n                data_sliced = data[num * slice_number : (num + 1) * slice_number]\n            query_result.update(exec_func(data_sliced))\n        return query_result\n\n    def mitigate_alias(self, index):\n        \"\"\"\n        Mitigate when an alias is detected instead of an index name\n\n        :param index: The index name that is showing up *instead* of what was expected\n\n        :type index: str\n\n        :returns: No return value:\n        :rtype: None\n        \"\"\"\n        self.loggit.debug('BEGIN mitigate_alias')\n        self.loggit.debug(\n            'Correcting an instance where an alias name points to index \"%s\"', index\n        )\n        data = self.client.indices.get(index=index)\n        aliases = list(data[index]['aliases'])\n        if aliases:\n            for alias in aliases:\n                if alias in self.indices:\n                    self.loggit.warning(\n                        'Removing alias \"%s\" from IndexList.indices', alias\n                    )\n                    self.indices.remove(alias)\n                if alias in list(self.index_info):\n                    self.loggit.warning(\n                        'Removing alias \"%s\" from IndexList.index_info', alias\n                    )\n                    del self.index_info[alias]\n        self.loggit.debug('Adding \"%s\" to IndexList.indices', index)\n        self.indices.append(index)\n        self.loggit.debug(\n            'Adding preliminary metadata for \"%s\" to IndexList.index_info', index\n        )\n        self.__build_index_info(index)\n        self.loggit.debug('END mitigate_alias')\n\n    def alias_index_check(self, data):\n        \"\"\"\n        Check each index in data to see if it's an alias.\n        \"\"\"\n        # self.loggit.debug('BEGIN alias_index_check')\n        working_list = data[:]\n        for entry in working_list:\n            if self.client.indices.exists_alias(name=entry):\n                index = list(self.client.indices.get_alias(name=entry).keys())[0]\n                self.loggit.warning(\n                    '\"%s\" is actually an alias for index \"%s\"', entry, index\n                )\n                self.mitigate_alias(index)\n                # The mitigate_alias step ensures that the class ivars are handled\n                # properly. The following ensure that we pass back a modified list\n                data.remove(entry)\n                data.append(index)\n        # self.loggit.debug('END alias_index_check')\n        return data\n\n    def indices_exist(self, data, exec_func):\n        \"\"\"Check if indices exist. If one doesn't, remove it. Loop until all exist\"\"\"\n        self.loggit.debug('BEGIN indices_exist')\n        checking = True\n        working_list = {}\n        verified_data = self.alias_index_check(data)\n        while checking:\n            try:\n                working_list.update(exec_func(verified_data))\n            except NotFoundError as err:\n                data.remove(self.__remove_missing(err))\n                continue\n            except TransportError as err:\n                if '413' in err.errors:\n                    msg = (\n                        'Huge Payload 413 Err - Trying to get information via '\n                        'multiple requests'\n                    )\n                    self.loggit.debug(msg)\n                    working_list.update(self._bulk_queries(verified_data, exec_func))\n            checking = False\n        # self.loggit.debug('END indices_exist')\n        return working_list\n\n    def data_getter(self, data, exec_func):\n        \"\"\"\n        Function that prevents unnecessary code repetition for different data\n        getter methods\n        \"\"\"\n        self.loggit.debug('BEGIN data_getter')\n        checking = True\n        while checking:\n            working_list = self.indices_exist(data, exec_func)\n            if working_list:\n                for index in list(working_list.keys()):\n                    try:\n                        sii = self.index_info[index]\n                    except KeyError:\n                        self.loggit.warning(\n                            'Index %s was not present at IndexList initialization, '\n                            ' and may be behind an alias',\n                            index,\n                        )\n                        self.mitigate_alias(index)\n                        sii = self.index_info[index]\n                        working_list = {}\n                        try:\n                            working_list.update(self._bulk_queries(data, exec_func))\n                        except NotFoundError as err:\n                            data.remove(self.__remove_missing(err))\n                            continue\n                    yield sii, working_list[index], index\n            checking = False\n        # self.loggit.debug('END data_getter')\n\n    def population_check(self, index, key):\n        \"\"\"Verify that key is in self.index_info[index], and that it is populated\"\"\"\n        retval = True\n        # self.loggit.debug('BEGIN population_check')\n        # self.loggit.debug('population_check: %s, %s', index, key)\n        if index not in self.index_info:\n            # This is just in case the index was somehow not populated\n            self.__build_index_info(index)\n        if key not in self.index_info[index]:\n            self.index_info[index][key] = self.__zero_values()[key]\n        if self.index_info[index][key] == self.__zero_values()[key]:\n            retval = False\n        # self.loggit.debug('END population_check')\n        return retval\n\n    def needs_data(self, indices, fields):\n        \"\"\"Check for data population in self.index_info\"\"\"\n        self.loggit.debug('Indices: %s, Fields: %s', indices, fields)\n        needful = []\n        working_list = self.indices_exist(indices, self._get_indices_settings)\n        for idx in working_list:\n            count = 0\n            for field in fields:\n                # If the return value is True for this field, it means it's populated\n                if self.population_check(idx, field):\n                    count += 1\n            if count == 0:\n                # All values are the default/zero\n                needful.append(idx)\n        if fields == ['state']:\n            self.loggit.debug('Always check open/close for all passed indices')\n            needful = list(working_list.keys())\n        self.loggit.debug('These indices need data in index_info: %s', needful)\n        return needful\n\n    def get_index_settings(self):\n        \"\"\"\n        For each index in self.indices, populate ``index_info`` with:\n            creation_date\n            number_of_replicas\n            number_of_shards\n            routing information (if present)\n        \"\"\"\n        self.loggit.debug('Getting index settings -- BEGIN')\n        self.empty_list_check()\n        fields = ['age', 'number_of_replicas', 'number_of_shards', 'routing']\n        for lst in chunk_index_list(self.indices):\n            # This portion here is to ensure that we're not polling for data\n            # unless we must\n            needful = self.needs_data(lst, fields)\n            if not needful:\n                # All indices are populated with some data, so we can skip\n                # data collection\n                continue\n            # Now we only need to run on the 'needful'\n            for sii, wli, _ in self.data_getter(needful, self._get_indices_settings):\n                sii['age']['creation_date'] = fix_epoch(\n                    wli['settings']['index']['creation_date']\n                )\n                sii['number_of_replicas'] = wli['settings']['index'][\n                    'number_of_replicas'\n                ]\n                sii['number_of_shards'] = wli['settings']['index']['number_of_shards']\n                if 'routing' in wli['settings']['index']:\n                    sii['routing'] = wli['settings']['index']['routing']\n        self.loggit.debug('Getting index settings -- END')\n\n    def get_index_state(self):\n        \"\"\"\n        For each index in self.indices, populate ``index_info`` with:\n\n            state (open or closed)\n\n        from the cat API\n        \"\"\"\n        self.loggit.debug('Getting index state -- BEGIN')\n        self.empty_list_check()\n        fields = ['state']\n        for lst in chunk_index_list(self.indices):\n            # This portion here is to ensure that we're not polling for data\n            # unless we must\n            needful = self.needs_data(lst, fields)\n            # Checking state is _always_ needful.\n            resp = self.client.cat.indices(\n                index=to_csv(needful), format='json', h='index,status'\n            )\n            for entry in resp:\n                try:\n                    self.index_info[entry['index']]['state'] = entry['status']\n                except KeyError:\n                    self.loggit.warning(\n                        'Index %s was not present at IndexList initialization, '\n                        'and may be behind an alias',\n                        entry['index'],\n                    )\n                    self.mitigate_alias(entry['index'])\n                    self.index_info[entry['index']]['state'] = entry['status']\n        # self.loggit.debug('Getting index state -- END')\n\n    def get_index_stats(self):\n        \"\"\"\n        Populate ``index_info`` with index ``size_in_bytes``,\n        ``primary_size_in_bytes`` and doc count information for each index.\n        \"\"\"\n        self.loggit.debug('Getting index stats -- BEGIN')\n        self.empty_list_check()\n        fields = ['size_in_bytes', 'docs', 'primary_size_in_bytes']\n        # This ensures that the index state is populated\n        self.get_index_state()\n        # Don't populate working_list until after the get_index state as it\n        # can and will remove missing indices\n        working_list = self.working_list()\n        for index in self.working_list():\n            if self.index_info[index]['state'] == 'close':\n                working_list.remove(index)\n        if working_list:\n            index_lists = chunk_index_list(working_list)\n            for lst in index_lists:\n                # This portion here is to ensure that we're not polling for data\n                # unless we must\n                needful = self.needs_data(lst, fields)\n                if not needful:\n                    # All indices are populated with some data, so we can skip\n                    # data collection\n                    continue\n                # Now we only need to run on the 'needful'\n                for sii, wli, index in self.data_getter(\n                    needful, self._get_indices_stats\n                ):\n                    try:\n                        size = wli['total']['store']['size_in_bytes']\n                        docs = wli['total']['docs']['count']\n                        primary_size = wli['primaries']['store']['size_in_bytes']\n                        msg = (\n                            f'Index: {index}  Size: {byte_size(size)}  Docs: {docs} '\n                            f'PrimarySize: {byte_size(primary_size)}'\n                        )\n                        self.loggit.debug(msg)\n                        sii['size_in_bytes'] = size\n                        sii['docs'] = docs\n                        sii['primary_size_in_bytes'] = primary_size\n                    except KeyError:\n                        msg = f'Index stats missing for \"{index}\" -- might be closed'\n                        self.loggit.warning(msg)\n        # self.loggit.debug('Getting index stats -- END')\n\n    def get_segment_counts(self):\n        \"\"\"\n        Populate ``index_info`` with segment information for each index.\n        \"\"\"\n        self.loggit.debug('Getting index segment counts')\n        self.empty_list_check()\n        for lst in chunk_index_list(self.indices):\n            for sii, wli, _ in self.data_getter(lst, self._get_indices_segments):\n                shards = wli['shards']\n                segmentcount = 0\n                for shardnum in shards:\n                    for shard in range(0, len(shards[shardnum])):\n                        segmentcount += shards[shardnum][shard]['num_search_segments']\n                sii['segments'] = segmentcount\n\n    def empty_list_check(self):\n        \"\"\"Raise :py:exc:`~.curator.exceptions.NoIndices` if ``indices`` is empty\"\"\"\n        self.loggit.debug('Checking for empty list')\n        if not self.indices:\n            raise NoIndices('index_list object is empty.')\n\n    def working_list(self):\n        \"\"\"\n        Return the current value of ``indices`` as copy-by-value to prevent list\n        stomping during iterations\n        \"\"\"\n        # Copy by value, rather than reference to prevent list stomping during\n        # iterations\n        self.loggit.debug('Generating working list of indices')\n        return self.indices[:]\n\n    def _get_name_based_ages(self, timestring):\n        \"\"\"\n        Add indices to ``index_info`` based on the age as indicated by the index\n        name pattern, if it matches ``timestring``\n\n        :param timestring: An :py:func:`time.strftime` pattern\n        \"\"\"\n        # Check for empty list before proceeding here to prevent non-iterable condition\n        self.loggit.debug('Getting ages of indices by \"name\"')\n        self.empty_list_check()\n        tstr = TimestringSearch(timestring)\n        for index in self.working_list():\n            epoch = tstr.get_epoch(index)\n            if isinstance(epoch, int):\n                self.index_info[index]['age']['name'] = epoch\n            else:\n                msg = (\n                    f'Timestring {timestring} was not found in index {index}. '\n                    f'Removing from actionable list'\n                )\n                self.loggit.debug(msg)\n                self.indices.remove(index)\n\n    def _get_field_stats_dates(self, field='@timestamp'):\n        \"\"\"\n        Add indices to ``index_info`` based on the values the queries return, as\n        determined by the min and max aggregated values of ``field``\n\n        :param field: The field with the date value.  The field must be mapped in\n            elasticsearch as a date datatype. Default: ``@timestamp``\n        \"\"\"\n        self.loggit.debug('Cannot query closed indices. Omitting any closed indices.')\n        self.filter_closed()\n        self.loggit.debug(\n            'Cannot use field_stats with empty indices. Omitting any empty indices.'\n        )\n        self.filter_empty()\n        self.loggit.debug(\n            'Getting index date by querying indices for min & max value of %s field',\n            field,\n        )\n        self.empty_list_check()\n        index_lists = chunk_index_list(self.indices)\n        for lst in index_lists:\n            for index in lst:\n                aggs = {\n                    'min': {'min': {'field': field}},\n                    'max': {'max': {'field': field}},\n                }\n                response = self.client.search(index=index, size=0, aggs=aggs)\n                self.loggit.debug('RESPONSE: %s', response)\n                if response:\n                    try:\n                        res = response['aggregations']\n                        self.loggit.debug('res: %s', res)\n                        data = self.index_info[index]['age']\n                        data['min_value'] = fix_epoch(res['min']['value'])\n                        data['max_value'] = fix_epoch(res['max']['value'])\n                        self.loggit.debug('data: %s', data)\n                    except KeyError as exc:\n                        raise ActionError(\n                            f'Field \"{field}\" not found in index \"{index}\"'\n                        ) from exc\n\n    def _calculate_ages(\n        self, source=None, timestring=None, field=None, stats_result=None\n    ):\n        \"\"\"\n        This method initiates index age calculation based on the given parameters.\n        Exceptions are raised when they are improperly configured.\n\n        Set instance variable ``age_keyfield`` for use later, if needed.\n\n        :param source: Source of index age. Can be: ``name``, ``creation_date``,\n            or ``field_stats``\n        :param timestring: An :py:func:`time.strftime` string to match the datestamp\n            in an index name. Only used for index filtering by ``name``.\n        :param field: A timestamp field name.  Only used for ``field_stats`` based\n            calculations.\n        :param stats_result: Either ``min_value`` or ``max_value``.  Only used\n            in conjunction with ``source=field_stats`` to choose whether to reference\n            the min or max result value.\n        \"\"\"\n        self.age_keyfield = source\n        if source == 'name':\n            if not timestring:\n                raise MissingArgument(\n                    'source \"name\" requires the \"timestring\" keyword argument'\n                )\n            self._get_name_based_ages(timestring)\n        elif source == 'creation_date':\n            # Nothing to do here as this comes from `get_settings` in __init__\n            pass\n        elif source == 'field_stats':\n            if not field:\n                raise MissingArgument(\n                    'source \"field_stats\" requires the \"field\" keyword argument'\n                )\n            if stats_result not in ['min_value', 'max_value']:\n                raise ValueError(f'Invalid value for \"stats_result\": {stats_result}')\n            self.age_keyfield = stats_result\n            self._get_field_stats_dates(field=field)\n        else:\n            raise ValueError(\n                f'Invalid source: {source}. Must be one of \"name\", \"creation_date\", '\n                f'\"field_stats\".'\n            )\n\n    def _sort_by_age(self, index_list, reverse=True):\n        \"\"\"\n        Take a list of indices and sort them by date.\n\n        By default, the youngest are first with ``reverse=True``, but the oldest\n        can be first by setting ``reverse=False``\n        \"\"\"\n        # Do the age-based sorting here.\n        # Build an temporary dictionary with just index and age as the key and\n        # value, respectively\n        temp = {}\n        for index in index_list:\n            try:\n                if self.index_info[index]['age'][self.age_keyfield]:\n                    temp[index] = self.index_info[index]['age'][self.age_keyfield]\n                else:\n                    msg = (\n                        f'No date for \"{index}\" in IndexList metadata. '\n                        f'Possible timestring mismatch. Excluding index \"{index}\".'\n                    )\n                    self.__excludify(True, True, index, msg)\n            except KeyError:\n                msg = (\n                    f'{index} does not have key \"{self.age_keyfield}\" in IndexList '\n                    f'metadata'\n                )\n                self.__excludify(True, True, index, msg)\n        # Sort alphabetically prior to age sort to keep sorting consistent\n        temp_tuple = sorted(temp.items(), key=lambda k: k[0], reverse=reverse)\n        # If reverse is True, this will sort so the youngest indices are first.\n        # However, if you want oldest first, set reverse to False.\n        # Effectively, this should set us up to act on everything older than\n        # meets the other set criteria. It starts as a tuple, but then becomes a list.\n        sorted_tuple = sorted(temp_tuple, key=lambda k: k[1], reverse=reverse)\n        return [x[0] for x in sorted_tuple]\n\n    def filter_by_regex(self, kind=None, value=None, exclude=False):\n        \"\"\"\n        Match indices by regular expression (pattern).\n\n        :param kind: Can be one of: ``suffix``, ``prefix``, ``regex``, or\n            ``timestring``. This option defines what ``kind`` of filter you will\n            be building.\n        :param value: Depends on ``kind``. It is the :py:func:`time.strftime` string if\n            ``kind`` is ``timestring``. It's used to build the regular expression\n            for other kinds.\n        :param exclude: If ``exclude=True``, this filter will remove matching\n            indices from ``indices``. If ``exclude=False``, then only matching\n            indices will be kept in ``indices``. Default is ``False``\n        \"\"\"\n        self.loggit.debug('Filtering indices by regex')\n        if kind not in ['regex', 'prefix', 'suffix', 'timestring']:\n            raise ValueError(f'{kind}: Invalid value for kind')\n        # Stop here if None or empty value, but zero is okay\n        if value == 0:\n            pass\n        elif not value:\n            raise ValueError(\n                'Invalid None value for \"value\". Cannot be \"None\" type, empty, or False'\n            )\n        if kind == 'timestring':\n            regex = settings.regex_map()[kind].format(get_date_regex(value))\n        else:\n            regex = settings.regex_map()[kind].format(value)\n        self.empty_list_check()\n        pattern = re.compile(regex)\n        for index in self.working_list():\n            self.loggit.debug('Filter by regex: Index: %s', index)\n            match = pattern.search(index)\n            if match:\n                self.__excludify(True, exclude, index)\n            else:\n                self.__excludify(False, exclude, index)\n\n    def filter_by_age(\n        self,\n        source='name',\n        direction=None,\n        timestring=None,\n        unit=None,\n        unit_count=None,\n        field=None,\n        stats_result='min_value',\n        epoch=None,\n        exclude=False,\n        unit_count_pattern=False,\n    ):\n        \"\"\"\n        Match indices by relative age calculations.\n\n        :param source: Source of index age. Can be one of ``name``, ``creation_date``,\n            or ``field_stats``\n        :param direction: Time to filter, either ``older`` or ``younger``\n        :param timestring: An :py:func:`time.strftime` string to match the datestamp\n            in an index name. Only used for index filtering by ``name``.\n        :param unit: One of ``seconds``, ``minutes``, ``hours``, ``days``, ``weeks``,\n            ``months``, or ``years``.\n        :param unit_count: The count of ``unit``. ``unit_count`` * ``unit`` will\n            be calculated out to the relative number of seconds.\n        :param unit_count_pattern: A regular expression whose capture group identifies\n            the value for ``unit_count``.\n        :param field: A timestamp field name.  Only used for ``field_stats`` based\n            calculations.\n        :param stats_result: Either ``min_value`` or ``max_value``.  Only used\n            in conjunction with ``source=field_stats`` to choose whether to reference\n            the minimum or maximum result value.\n        :param epoch: An epoch timestamp used in conjunction with ``unit`` and\n            ``unit_count`` to establish a point of reference for calculations.\n            If not provided, the current time will be used.\n        :param exclude: If ``exclude=True``, this filter will remove matching\n            indices from ``indices``. If ``exclude`` is `False`, then only matching\n            indices will be kept in ``indices``. Default is ``False``\n        \"\"\"\n        self.loggit.debug('Filtering indices by age')\n        # Get timestamp point of reference, por\n        por = get_point_of_reference(unit, unit_count, epoch)\n        if not direction:\n            raise MissingArgument('Must provide a value for \"direction\"')\n        if direction not in ['older', 'younger']:\n            raise ValueError(f'Invalid value for \"direction\": {direction}')\n        # This filter requires index settings.\n        self.get_index_settings()\n        self._calculate_ages(\n            source=source, timestring=timestring, field=field, stats_result=stats_result\n        )\n        if unit_count_pattern:\n            try:\n                unit_count_matcher = re.compile(unit_count_pattern)\n            # pylint: disable=broad-except\n            except Exception as exc:\n                # We got an illegal regex, so won't be able to match anything\n                self.loggit.error(\n                    'Regular expression failure. Will not match unit count. Error: %s',\n                    exc,\n                )\n                unit_count_matcher = None\n        for index in self.working_list():\n            try:\n                remove_this_index = False\n                age = int(self.index_info[index]['age'][self.age_keyfield])\n                # if age == 0:\n                #     msg = (\n                #         f'Evaluating {index} resulted in an epoch timestamp of '\n                #         f'0, meaning there is no associated date. Removing from '\n                #         f'the actionable list.'\n                #     )\n                #     self.loggit.debug(msg)\n                #     self.indices.remove(index)\n                #     continue\n                msg = (\n                    f'Index \"{index}\" age ({age}), direction: \"{direction}\", point of '\n                    f'reference, ({por})'\n                )\n                # Because time adds to epoch, smaller numbers are actually older\n                # timestamps.\n                if unit_count_pattern:\n                    msg = (\n                        f'unit_count_pattern is set, trying to match pattern to '\n                        f'index \"{index}\"'\n                    )\n                    self.loggit.debug(msg)\n                    unit_count_from_index = get_unit_count_from_name(\n                        index, unit_count_matcher\n                    )\n                    if unit_count_from_index:\n                        self.loggit.debug(\n                            'Pattern matched, applying unit_count of  \"%s\"',\n                            unit_count_from_index,\n                        )\n                        adjustedpor = get_point_of_reference(\n                            unit, unit_count_from_index, epoch\n                        )\n                        msg = (\n                            f'Adjusting point of reference from {por} to {adjustedpor} '\n                            f'based on unit_count of {unit_count_from_index} from '\n                            f'index name'\n                        )\n                        self.loggit.debug(msg)\n                    elif unit_count == -1:\n                        # Unable to match pattern and unit_count is -1, meaning no\n                        # fallback, so this index is removed from the list\n                        msg = (\n                            f'Unable to match pattern and no fallback value set. '\n                            f'Removing index \"{index}\" from actionable list'\n                        )\n                        self.loggit.debug(msg)\n                        remove_this_index = True\n                        adjustedpor = por\n                        # necessary to avoid exception if the first index is excluded\n                    else:\n                        # Unable to match the pattern and unit_count is set, so\n                        # fall back to using unit_count for determining whether\n                        # to keep this index in the list\n                        self.loggit.debug(\n                            'Unable to match pattern using fallback value of \"%s\"',\n                            unit_count,\n                        )\n                        adjustedpor = por\n                else:\n                    adjustedpor = por\n                if direction == 'older':\n                    agetest = age < adjustedpor\n                else:\n                    agetest = age > adjustedpor\n                self.__excludify(agetest and not remove_this_index, exclude, index, msg)\n            except KeyError:\n                msg = (\n                    f'Index \"{index}\" does not meet provided criteria. '\n                    f'Removing from list.'\n                )\n                self.loggit.debug(msg)\n                self.indices.remove(index)\n\n    def filter_by_space(\n        self,\n        disk_space=None,\n        reverse=True,\n        use_age=False,\n        source='creation_date',\n        timestring=None,\n        field=None,\n        stats_result='min_value',\n        exclude=False,\n        threshold_behavior='greater_than',\n    ):\n        \"\"\"\n        Remove indices from the actionable list based on space consumed, sorted\n        reverse-alphabetically by default.  If you set ``reverse`` to ``False``,\n        it will be sorted alphabetically.\n\n        The default is usually what you will want. If only one kind of index is\n        provided--for example, indices matching ``logstash-%Y.%m.%d`` --then\n        reverse alphabetical sorting will mean the oldest will remain in the list,\n        because lower numbers in the dates mean older indices.\n\n        By setting ``reverse`` to ``False``, then ``index3`` will be deleted before\n        ``index2``, which will be deleted before ``index1``\n\n        ``use_age`` allows ordering indices by age. Age is determined by the\n        index creation date by default, but you can specify an ``source`` of\n        ``name``, ``max_value``, or ``min_value``. The ``name`` ``source`` requires\n        the timestring argument.\n\n        ``threshold_behavior``, when set to ``greater_than`` (default), includes\n        if it the index tests to be larger than ``disk_space``. When set to\n        ``less_than``, it includes if the index is smaller than ``disk_space``\n\n        :param disk_space: Filter indices over *n* gigabytes\n        :param threshold_behavior: Size to filter, either ``greater_than`` or\n            ``less_than``. Defaults to ``greater_than`` to preserve backwards\n            compatability.\n        :param reverse: The filtering direction. (default: ``True``).  Ignored if\n            ``use_age`` is ``True``\n        :param use_age: Sort indices by age.  ``source`` is required in this case.\n        :param source: Source of index age. Can be one of ``name``, ``creation_date``,\n            or ``field_stats``. Default: ``creation_date``\n        :param timestring: An :py:func:`time.strftime` string to match the datestamp\n            in an index name. Only used if ``source=name`` is selected.\n        :param field: A timestamp field name.  Only used if ``source=field_stats``\n            is selected.\n        :param stats_result: Either ``min_value`` or ``max_value``.  Only used if\n            ``source=field_stats`` is selected. It determines whether to reference\n            the minimum or maximum value of `field` in each index.\n        :param exclude: If ``exclude=True``, this filter will remove matching\n            indices from ``indices``. If ``exclude=False``, then only matching\n            indices will be kept in ``indices``. Default is ``False``\n        \"\"\"\n        self.loggit.debug('Filtering indices by disk space')\n        # Ensure that disk_space is a float\n        if not disk_space:\n            raise MissingArgument('No value for \"disk_space\" provided')\n        if threshold_behavior not in ['greater_than', 'less_than']:\n            raise ValueError(\n                f'Invalid value for \"threshold_behavior\": {threshold_behavior}'\n            )\n        # This filter requires both index stats and index settings\n        self.get_index_stats()\n        self.get_index_settings()\n        disk_space = float(disk_space)\n        disk_usage = 0.0\n        disk_limit = disk_space * 2**30\n        msg = (\n            'Cannot get disk usage info from closed indices. Omitting any '\n            'closed indices.'\n        )\n        self.loggit.debug(msg)\n        self.filter_closed()\n        if use_age:\n            self._calculate_ages(\n                source=source,\n                timestring=timestring,\n                field=field,\n                stats_result=stats_result,\n            )\n            # Using default value of reverse=True in self._sort_by_age()\n            self.loggit.debug('SORTING BY AGE')\n            sorted_indices = self._sort_by_age(self.working_list())\n        else:\n            # Default to sorting by index name\n            sorted_indices = sorted(self.working_list(), reverse=reverse)\n        for index in sorted_indices:\n            disk_usage += self.index_info[index]['size_in_bytes']\n            msg = (\n                f'{index}, summed disk usage is {byte_size(disk_usage)} and disk limit '\n                f'is {byte_size(disk_limit)}.'\n            )\n            if threshold_behavior == 'greater_than':\n                self.__excludify((disk_usage > disk_limit), exclude, index, msg)\n            elif threshold_behavior == 'less_than':\n                self.__excludify((disk_usage < disk_limit), exclude, index, msg)\n\n    def filter_kibana(self, exclude=True):\n        \"\"\"\n        Match any index named ``.kibana*`` in ``indices``. Older releases addressed\n        index names that no longer exist.\n\n        :param exclude: If ``exclude=True``, this filter will remove matching\n            indices from ``indices``. If ``exclude=False``, then only matching\n            indices will be kept in ``indices``. Default is ``True``\n        \"\"\"\n        self.loggit.debug('Filtering kibana indices')\n        self.empty_list_check()\n        for index in self.working_list():\n            pattern = re.compile(r'^\\.kibana.*$')\n            if pattern.match(index):\n                self.__excludify(True, exclude, index)\n            else:\n                self.__excludify(False, exclude, index)\n\n    def filter_forceMerged(self, max_num_segments=None, exclude=True):\n        \"\"\"\n        Match any index which has ``max_num_segments`` per shard or fewer in the\n        actionable list.\n\n        :param max_num_segments: Cutoff number of segments per shard.\n        :param exclude: If ``exclude=True``, this filter will remove matching\n            indices from ``indices``. If ``exclude=False``, then only matching\n            indices will be kept in ``indices``. Default is ``True``\n        \"\"\"\n        self.loggit.debug('Filtering forceMerged indices')\n        if not max_num_segments:\n            raise MissingArgument('Missing value for \"max_num_segments\"')\n        self.loggit.debug(\n            'Cannot get segment count of closed indices. Omitting any closed indices.'\n        )\n        # This filter requires the index state (open/close), and index settings.\n        self.get_index_state()\n        self.get_index_settings()\n        self.filter_closed()\n        self.get_segment_counts()\n        for index in self.working_list():\n            # Do this to reduce long lines and make it more readable...\n            shards = int(self.index_info[index]['number_of_shards'])\n            replicas = int(self.index_info[index]['number_of_replicas'])\n            segments = int(self.index_info[index]['segments'])\n            msg = (\n                f'{index} has {shards} shard(s) + {replicas} replica(s) '\n                f'with a sum total of {segments} segments.'\n            )\n            expected_count = (shards + (shards * replicas)) * max_num_segments\n            self.__excludify((segments <= expected_count), exclude, index, msg)\n\n    def filter_closed(self, exclude=True):\n        \"\"\"\n        Filter out closed indices from ``indices``\n\n        :param exclude: If ``exclude=True``, this filter will remove matching\n            indices from ``indices``. If ``exclude=False``, then only matching\n            indices will be kept in ``indices``. Default is ``True``\n        \"\"\"\n        self.loggit.debug('Filtering closed indices')\n        # This filter requires index state (open/close)\n        self.get_index_state()\n        self.empty_list_check()\n        for index in self.working_list():\n            condition = self.index_info[index]['state'] == 'close'\n            self.loggit.debug(\n                'Index %s state: %s', index, self.index_info[index]['state']\n            )\n            self.__excludify(condition, exclude, index)\n\n    def filter_empty(self, exclude=True):\n        \"\"\"\n        Filter indices with a document count of zero. Indices that are closed\n        are automatically excluded from consideration due to closed indices reporting\n        a document count of zero.\n\n        :param exclude: If ``exclude=True``, this filter will remove matching indices\n            from ``indices``. If ``exclude=False``, then only matching indices\n            will be kept in ``indices``. Default is ``True``\n        \"\"\"\n        self.loggit.debug('Filtering empty indices')\n        # This index requires index state (open/close) and index stats\n        self.get_index_state()\n        self.get_index_stats()\n        self.filter_closed()\n        self.empty_list_check()\n        for index in self.working_list():\n            condition = self.index_info[index]['docs'] == 0\n            self.loggit.debug(\n                'Index %s doc count: %s', index, self.index_info[index]['docs']\n            )\n            self.__excludify(condition, exclude, index)\n\n    def filter_opened(self, exclude=True):\n        \"\"\"\n        Filter out opened indices from ``indices``\n\n        :param exclude: If ``exclude=True``, this filter will remove matching indices\n            from ``indices``. If ``exclude=False``, then only matching indices\n            will be kept in ``indices``. Default is ``True``\n        \"\"\"\n        self.loggit.debug('Filtering open indices')\n        # This filter requires index state (open/close)\n        self.get_index_state()\n        self.empty_list_check()\n        for index in self.working_list():\n            condition = self.index_info[index]['state'] == 'open'\n            self.loggit.debug(\n                'Index %s state: %s', index, self.index_info[index]['state']\n            )\n            self.__excludify(condition, exclude, index)\n\n    def filter_allocated(\n        self, key=None, value=None, allocation_type='require', exclude=True\n    ):\n        \"\"\"\n        Match indices that have the routing allocation rule of ``key=value`` from\n        ``indices``\n\n        :param key: The allocation attribute to check for\n        :param value: The value to check for\n        :param allocation_type: Type of allocation to apply\n        :param exclude: If ``exclude=True``, this filter will remove matching indices\n            from ``indices``. If ``exclude=False``, then only matching indices\n            will be kept in ``indices``. Default is `T`rue`\n        \"\"\"\n        self.loggit.debug('Filtering indices with shard routing allocation rules')\n        if not key:\n            raise MissingArgument('No value for \"key\" provided')\n        if not value:\n            raise MissingArgument('No value for \"value\" provided')\n        if allocation_type not in ['include', 'exclude', 'require']:\n            raise ValueError(f'Invalid \"allocation_type\": {allocation_type}')\n        # This filter requires index settings\n        self.get_index_settings()\n        self.get_index_state()\n        self.empty_list_check()\n        for lst in chunk_index_list(self.indices):\n            working_list = self._get_indices_settings(lst)\n            if working_list:\n                for index in list(working_list.keys()):\n                    try:\n                        has_routing = (\n                            working_list[index]['settings']['index']['routing'][\n                                'allocation'\n                            ][allocation_type][key]\n                            == value\n                        )\n                    except KeyError:\n                        has_routing = False\n                    # if has_routing:\n                    msg = (\n                        f'{index}: Routing (mis)match: '\n                        f'index.routing.allocation.{allocation_type}.{key}={value}.'\n                    )\n                    self.__excludify(has_routing, exclude, index, msg)\n\n    def filter_none(self):\n        \"\"\"The legendary NULL filter\"\"\"\n        self.loggit.debug('\"None\" filter selected.  No filtering will be done.')\n\n    def filter_by_alias(self, aliases=None, exclude=False):\n        \"\"\"\n        Match indices which are associated with the alias or list of aliases\n        identified by ``aliases``. Indices must appear in all aliases in list\n        ``aliases`` or a 404 error will result, leading to no indices being matched.\n\n        :param aliases: A list of alias names.\n        :type aliases: list\n        :param exclude: If ``exclude=True``, this filter will remove matching indices\n            from ``indices``. If ``exclude=False``, then only matching indices\n            will be kept in ``indices``. Default is ``False``\n        \"\"\"\n        self.loggit.debug('Filtering indices matching aliases: \"%s\"', aliases)\n        if not aliases:\n            raise MissingArgument('No value for \"aliases\" provided')\n        aliases = ensure_list(aliases)\n        self.empty_list_check()\n        for lst in chunk_index_list(self.indices):\n            try:\n                # get_alias will either return {} or a NotFoundError.\n                has_alias = list(\n                    self.client.indices.get_alias(\n                        index=to_csv(lst), name=to_csv(aliases)\n                    ).keys()\n                )\n                self.loggit.debug('has_alias: %s', has_alias)\n            except NotFoundError:\n                # if we see the NotFoundError, we need to set working_list to {}\n                has_alias = []\n            for index in lst:\n                if index in has_alias:\n                    isness = 'is'\n                    condition = True\n                else:\n                    isness = 'is not'\n                    condition = False\n                msg = f'{index} {isness} associated with aliases: {aliases}'\n                self.__excludify(condition, exclude, index, msg)\n\n    def filter_by_count(\n        self,\n        count=None,\n        reverse=True,\n        use_age=False,\n        pattern=None,\n        source='creation_date',\n        timestring=None,\n        field=None,\n        stats_result='min_value',\n        exclude=True,\n    ):\n        \"\"\"\n        Remove indices from the actionable list beyond the number ``count``, sorted\n        reverse-alphabetically by default.  If you set ``reverse=False``, it will\n        be sorted alphabetically.\n\n        The default is usually what you will want. If only one kind of index is\n        provided--for example, indices matching ``logstash-%Y.%m.%d`` -- then\n        reverse alphabetical sorting will mean the oldest will remain in the list,\n        because lower numbers in the dates mean older indices.\n\n        By setting ``reverse=False``, then ``index3`` will be deleted before\n        ``index2``, which will be deleted before ``index1``\n\n        ``use_age`` allows ordering indices by age. Age is determined by the index\n        creation date by default, but you can specify an ``source`` of ``name``,\n        ``max_value``, or ``min_value``. The ``name`` `source` requires the\n        timestring argument.\n\n        :param count: Filter indices beyond ``count``.\n        :param reverse: The filtering direction. (default: ``True``).\n        :param use_age: Sort indices by age.  ``source`` is required in this case.\n        :param pattern: Select indices to count from a regular expression pattern.\n            This pattern must have one and only one capture group. This can allow\n            a single ``count`` filter instance to operate against any number of\n            matching patterns, and keep ``count`` of each index in that group.\n            For example, given a ``pattern`` of ``'^(.*)-\\\\d{6}$'``, it will match\n            both ``rollover-000001`` and ``index-999990``, but not\n            ``logstash-2017.10.12``. Following the same example, if my cluster\n            also had ``rollover-000002`` through ``rollover-000010`` and\n            ``index-888888`` through ``index-999999``, it will process both groups\n            of indices, and include or exclude the ``count`` of each.\n        :param source: Source of index age. Can be one of ``name``,\n            ``creation_date``, or ``field_stats``. Default: ``creation_date``\n        :param timestring: An :py:func:`time.strftime` string to match the datestamp\n            in an index name. Only used if ``source=name``.\n        :param field: A timestamp field name.  Only used if ``source=field_stats``.\n        :param stats_result: Either ``min_value`` or ``max_value``.  Only used if\n            ``source=field_stats``. It determines whether to reference the minimum\n            or maximum value of ``field`` in each index.\n        :param exclude: If ``exclude=True``, this filter will remove matching indices\n            from ``indices``. If ``exclude=False``, then only matching indices\n            will be kept in ``indices``. Default is ``True``\n        \"\"\"\n        self.loggit.debug('Filtering indices by count')\n        if not count:\n            raise MissingArgument('No value for \"count\" provided')\n        # This filter requires index state (open/close) and index settings\n        self.get_index_state()\n        self.get_index_settings()\n        # Create a copy-by-value working list\n        working_list = self.working_list()\n        if pattern:\n            try:\n                regex = re.compile(pattern)\n                if regex.groups < 1:\n                    raise ConfigurationError(\n                        f'No regular expression group found in {pattern}'\n                    )\n                if regex.groups > 1:\n                    raise ConfigurationError(\n                        f'More than 1 regular expression group found in {pattern}'\n                    )\n                # Prune indices not matching the regular expression the object\n                # (And filtered_indices) We do not want to act on them by accident.\n                prune_these = list(\n                    filter(lambda x: regex.match(x) is None, working_list)\n                )\n                filtered_indices = working_list\n                for index in prune_these:\n                    msg = '{index} does not match regular expression {pattern}.'\n                    condition = True\n                    exclude = True\n                    self.__excludify(condition, exclude, index, msg)\n                    # also remove it from filtered_indices\n                    filtered_indices.remove(index)\n                # Presort these filtered_indices using the lambda\n                presorted = sorted(\n                    filtered_indices, key=lambda x: regex.match(x).group(1)\n                )\n            except Exception as exc:\n                raise ActionError(\n                    f'Unable to process pattern: \"{pattern}\". Error: {exc}'\n                ) from exc\n            # Initialize groups here\n            groups = []\n            # We have to pull keys k this way, but we don't need to keep them\n            # We only need g for groups\n            for _, g in itertools.groupby(\n                presorted, key=lambda x: regex.match(x).group(1)\n            ):\n                groups.append(list(g))\n        else:\n            # Since pattern will create a list of lists, and we iterate over that,\n            # we need to put our single list inside a list\n            groups = [working_list]\n        for group in groups:\n            if use_age:\n                if source != 'name':\n                    self.loggit.warning(\n                        'Cannot get age information from closed indices unless '\n                        'source=\"name\".  Omitting any closed indices.'\n                    )\n                    self.filter_closed()\n                self._calculate_ages(\n                    source=source,\n                    timestring=timestring,\n                    field=field,\n                    stats_result=stats_result,\n                )\n                # Using default value of reverse=True in self._sort_by_age()\n                sorted_indices = self._sort_by_age(group, reverse=reverse)\n            else:\n                # Default to sorting by index name\n                sorted_indices = sorted(group, reverse=reverse)\n            idx = 1\n            for index in sorted_indices:\n                msg = f'{index} is {idx} of specified count of {count}.'\n                condition = True if idx <= count else False\n                self.__excludify(condition, exclude, index, msg)\n                idx += 1\n\n    def filter_by_shards(\n        self, number_of_shards=None, shard_filter_behavior='greater_than', exclude=False\n    ):\n        \"\"\"\n        Match ``indices`` with a given shard count.\n\n        Selects all indices with a shard count ``greater_than`` ``number_of_shards``\n        by default. Use ``shard_filter_behavior`` to select indices with shard\n        count ``greater_than``, ``greater_than_or_equal``, ``less_than``,\n        ``less_than_or_equal``, or ``equal`` to ``number_of_shards``.\n\n        :param number_of_shards: shard threshold\n        :param shard_filter_behavior: Do you want to filter on ``greater_than``,\n            ``greater_than_or_equal``, ``less_than``, ``less_than_or_equal``,\n            or ``equal``?\n        :param exclude: If ``exclude=True``, this filter will remove matching indices\n            from ``indices``. If ``exclude=False``, then only matching indices\n            will be kept in ``indices``. Default is ``False``\n        \"\"\"\n        self.loggit.debug(\"Filtering indices by number of shards\")\n        if not number_of_shards:\n            raise MissingArgument('No value for \"number_of_shards\" provided')\n        if shard_filter_behavior not in [\n            'greater_than',\n            'less_than',\n            'greater_than_or_equal',\n            'less_than_or_equal',\n            'equal',\n        ]:\n            raise ValueError(\n                f'Invalid value for \"shard_filter_behavior\": {shard_filter_behavior}'\n            )\n        if number_of_shards < 1 or (\n            shard_filter_behavior == 'less_than' and number_of_shards == 1\n        ):\n            raise ValueError(\n                f'Unacceptable value: {number_of_shards} -- \"number_of_shards\" cannot '\n                f'be less than 1. A valid index will have at least one shard.'\n            )\n        # This filter requires index_settings to count shards\n        self.get_index_settings()\n        self.empty_list_check()\n        for index in self.working_list():\n            self.loggit.debug('Filter by number of shards: Index: %s', index)\n            if shard_filter_behavior == 'greater_than':\n                condition = (\n                    int(self.index_info[index]['number_of_shards']) > number_of_shards\n                )\n            elif shard_filter_behavior == 'less_than':\n                condition = (\n                    int(self.index_info[index]['number_of_shards']) < number_of_shards\n                )\n            elif shard_filter_behavior == 'greater_than_or_equal':\n                condition = (\n                    int(self.index_info[index]['number_of_shards']) >= number_of_shards\n                )\n            elif shard_filter_behavior == 'less_than_or_equal':\n                condition = (\n                    int(self.index_info[index]['number_of_shards']) <= number_of_shards\n                )\n            else:\n                condition = (\n                    int(self.index_info[index]['number_of_shards']) == number_of_shards\n                )\n            self.__excludify(condition, exclude, index)\n\n    def filter_period(\n        self,\n        period_type='relative',\n        source='name',\n        range_from=None,\n        range_to=None,\n        date_from=None,\n        date_to=None,\n        date_from_format=None,\n        date_to_format=None,\n        timestring=None,\n        unit=None,\n        field=None,\n        stats_result='min_value',\n        intersect=False,\n        week_starts_on='sunday',\n        epoch=None,\n        exclude=False,\n    ):\n        \"\"\"\n        Match ``indices`` with ages within a given period.\n\n        :param period_type: Can be either ``absolute`` or ``relative``.  Default\n            is ``relative``. ``date_from`` and ``date_to`` are required when using\n            ``period_type='absolute'``. ``range_from`` and ``range_to`` are required\n            with ``period_type='relative'``.\n        :param source: Source of index age. Can be ``name``, ``creation_date``,\n            or ``field_stats``\n        :param range_from: How many ``unit`` (s) in the past/future is the origin?\n        :param range_to: How many ``unit`` (s) in the past/future is the end point?\n        :param date_from: The simplified date for the start of the range\n        :param date_to: The simplified date for the end of the range.  If this\n            value is the same as ``date_from``, the full value of ``unit`` will be\n            extrapolated for the range.  For example, if ``unit=months``, and\n            ``date_from`` and ``date_to`` are both ``2017.01``, then the entire\n            month of January 2017 will be the absolute date range.\n        :param date_from_format: The :py:func:`time.strftime` string used to\n            parse ``date_from``\n        :param date_to_format: The :py:func:`time.strftime` string used to\n            parse ``date_to``\n        :param timestring: An :py:func:`time.strftime` string to match the datestamp\n            in an index name. Only used for index filtering by ``name``.\n        :param unit: One of ``hours``, ``days``, ``weeks``, ``months``, or ``years``.\n        :param field: A timestamp field name.  Only used for ``field_stats`` based\n            calculations.\n        :param stats_result: Either ``min_value`` or ``max_value``.  Only used in\n            conjunction with ``source='field_stats'`` to choose whether to reference\n            the min or max result value.\n        :param intersect: Only used when ``source='field_stats'``. If ``True``,\n            only indices where both ``min_value`` and ``max_value`` are within the\n            period will be selected. If ``False``, it will use whichever you specified.\n            Default is ``False`` to preserve expected behavior.\n        :param week_starts_on: Either ``sunday`` or ``monday``. Default is ``sunday``\n        :param epoch: An epoch timestamp used to establish a point of reference for\n            calculations. If not provided, the current time will be used.\n        :param exclude: If ``exclude=True``, this filter will remove matching indices\n            from ``indices``. If ``exclude=False``, then only matching indices\n            will be kept in ``indices``. Default is ``False``\n        \"\"\"\n        self.loggit.debug('Filtering indices by period')\n        if period_type not in ['absolute', 'relative']:\n            raise ValueError(\n                f'Unacceptable value: {period_type} -- \"period_type\" must be either '\n                f'\"absolute\" or \"relative\".'\n            )\n        if period_type == 'relative':\n            func = date_range\n            args = [unit, range_from, range_to, epoch]\n            kwgs = {'week_starts_on': week_starts_on}\n            if (not isinstance(range_from, int)) or (not isinstance(range_to, int)):\n                raise ConfigurationError(\n                    '\"range_from\" and \"range_to\" must be integer values'\n                )\n        else:\n            func = absolute_date_range\n            args = [unit, date_from, date_to]\n            kwgs = {\n                'date_from_format': date_from_format,\n                'date_to_format': date_to_format,\n            }\n            for reqd in [date_from, date_to, date_from_format, date_to_format]:\n                if not reqd:\n                    raise ConfigurationError(\n                        'Must provide \"date_from\", \"date_to\", \"date_from_format\", and '\n                        '\"date_to_format\" with absolute period_type'\n                    )\n        # This filter requires index settings\n        self.get_index_settings()\n        try:\n            start, end = func(*args, **kwgs)\n        # pylint: disable=broad-except\n        except Exception as exc:\n            report_failure(exc)\n        self._calculate_ages(\n            source=source, timestring=timestring, field=field, stats_result=stats_result\n        )\n        for index in self.working_list():\n            try:\n                if source == 'field_stats' and intersect:\n                    min_age = int(self.index_info[index]['age']['min_value'])\n                    max_age = int(self.index_info[index]['age']['max_value'])\n                    msg = (\n                        f'Index \"{index}\", timestamp field \"{field}\", min_value '\n                        f'({min_age}), max_value ({max_age}), period start: '\n                        f'\"{start}\", period end, \"{end}\"'\n                    )\n                    # Because time adds to epoch, smaller numbers are actually older\n                    # timestamps.\n                    inrange = (min_age >= start) and (max_age <= end)\n                else:\n                    age = int(self.index_info[index]['age'][self.age_keyfield])\n                    msg = (\n                        f'Index \"{index}\" age ({age}), period start: \"{start}\", period '\n                        f'end, \"{end}\"'\n                    )\n                    # Because time adds to epoch, smaller numbers are actually older\n                    # timestamps.\n                    inrange = (age >= start) and (age <= end)\n                self.__excludify(inrange, exclude, index, msg)\n            except KeyError:\n                self.loggit.debug(\n                    'Index \"%s\" does not meet provided criteria. Removing from list.',\n                    index,\n                )\n                self.indices.remove(index)\n\n    def filter_ilm(self, exclude=True):\n        \"\"\"\n        Match indices that have the setting ``index.lifecycle.name``\n\n        :param exclude: If ``exclude=True``, this filter will remove matching\n            ``indices``. If ``exclude=False``, then only matching indices will be\n            kept in ``indices``. Default is ``True``\n        \"\"\"\n        self.loggit.debug('Filtering indices with index.lifecycle.name')\n        index_lists = chunk_index_list(self.indices)\n        if index_lists == [['']]:\n            self.loggit.debug('Empty working list. No ILM indices to filter.')\n            return\n        for lst in index_lists:\n            working_list = self._get_indices_settings(lst)\n            if working_list:\n                for index in list(working_list.keys()):\n                    try:\n                        subvalue = working_list[index]['settings']['index']['lifecycle']\n                        has_ilm = 'name' in subvalue\n                        msg = f\"{index} has index.lifecycle.name {subvalue['name']}\"\n                    except KeyError:\n                        has_ilm = False\n                        msg = f'index.lifecycle.name is not set for index {index}'\n                    self.__excludify(has_ilm, exclude, index, msg)\n\n    def iterate_filters(self, filter_dict):\n        \"\"\"\n        Iterate over the filters defined in ``config`` and execute them.\n\n        :param filter_dict: The configuration dictionary\n\n        .. note:: ``filter_dict`` should be a dictionary with the following form:\n        .. code-block:: python\n\n                { 'filters' : [\n                        {\n                            'filtertype': 'the_filter_type',\n                            'key1' : 'value1',\n                            ...\n                            'keyN' : 'valueN'\n                        }\n                    ]\n                }\n\n        \"\"\"\n        self.loggit.debug('Iterating over a list of filters')\n        # Make sure we actually _have_ filters to act on\n        if 'filters' not in filter_dict or len(filter_dict['filters']) < 1:\n            self.loggit.info('No filters in config.  Returning unaltered object.')\n            return\n        self.loggit.debug('All filters: %s', filter_dict['filters'])\n        for fil in filter_dict['filters']:\n            self.loggit.debug('Top of the loop: %s', self.indices)\n            self.loggit.debug('Un-parsed filter args: %s', fil)\n            # Make sure we got at least this much in the configuration\n            chk = SchemaCheck(\n                fil, filterstructure(), 'filter', 'IndexList.iterate_filters'\n            ).result()\n            msg = f'Parsed filter args: {chk}'\n            self.loggit.debug(msg)\n            method = self.__map_method(fil['filtertype'])\n            del fil['filtertype']\n            # If it's a filtertype with arguments, update the defaults with the\n            # provided settings.\n            if fil:\n                self.loggit.debug('Filter args: %s', fil)\n                self.loggit.debug('Pre-instance: %s', self.indices)\n                method(**fil)\n                self.loggit.debug('Post-instance: %s', self.indices)\n            else:\n                # Otherwise, it's a settingless filter.\n                method()\n\n    def filter_by_size(\n        self,\n        size_threshold=None,\n        threshold_behavior='greater_than',\n        exclude=False,\n        size_behavior='primary',\n    ):\n        \"\"\"\n        Remove indices from the actionable list based on index size.\n\n        ``threshold_behavior``, when set to ``greater_than`` (default), includes\n        if it the index tests to be larger than ``size_threshold``. When set to\n        ``less_than``, it includes if the index is smaller than ``size_threshold``\n\n        :param size_threshold: Filter indices over *n* gigabytes\n        :param threshold_behavior: Size to filter, either ``greater_than`` or\n            ``less_than``. Defaults to ``greater_than`` to preserve backwards\n            compatability.\n        :param size_behavior: Size that used to filter, either ``primary`` or\n            ``total``. Defaults to ``primary``\n        :param exclude: If ``exclude=True``, this filter will remove matching indices\n            from ``indices``. If ``exclude=False``, then only matching indices\n            will be kept in ``indices``. Default is ``False``\n        \"\"\"\n        self.loggit.debug('Filtering indices by index size')\n        # Ensure that disk_space is a float\n        if not size_threshold:\n            raise MissingArgument('No value for \"size_threshold\" provided')\n        if size_behavior not in ['primary', 'total']:\n            raise ValueError(f'Invalid value for \"size_behavior\": {size_behavior}')\n        if threshold_behavior not in ['greater_than', 'less_than']:\n            raise ValueError(\n                f'Invalid value for \"threshold_behavior\": {threshold_behavior}'\n            )\n        index_size_limit = float(size_threshold) * 2**30\n        msg = (\n            'Cannot get disk usage info from closed indices. '\n            'Omitting any closed indices.'\n        )\n        self.loggit.debug(msg)\n        # This filter requires index state (open/close) and index stats\n        self.get_index_state()\n        self.get_index_stats()\n        self.filter_closed()\n        # Create a copy-by-value working list\n        working_list = self.working_list()\n        for index in working_list:\n            if size_behavior == 'primary':\n                index_size = self.index_info[index]['primary_size_in_bytes']\n            else:\n                index_size = self.index_info[index]['size_in_bytes']\n            msg = (\n                f'{index}, index size is {byte_size(index_size)} and '\n                f'size limit is {byte_size(index_size_limit)}.'\n            )\n            if threshold_behavior == 'greater_than':\n                self.__excludify((index_size > index_size_limit), exclude, index, msg)\n            elif threshold_behavior == 'less_than':\n                self.__excludify((index_size < index_size_limit), exclude, index, msg)\n"
  },
  "GT_src_dict": {
    "curator/helpers/getters.py": {
      "get_tier_preference": {
        "code": "def get_tier_preference(client, target_tier='data_frozen'):\n    \"\"\"Determine the preferred data tier(s) in an Elasticsearch cluster based on the specified target tier. The function constructs a string of data tiers in decreasing order of importance, from coldest to hottest, allowing for efficient data access and management.\n\n:param client: A client connection object for interacting with the Elasticsearch cluster.\n:type client: :py:class:`~.elasticsearch.Elasticsearch`\n:param target_tier: The target data tier for the preference (default is 'data_frozen').\n:type target_tier: str\n\n:returns: A comma-separated string of the preferred data tiers in order from coldest to hottest.\n:rtype: str\n\nThis function interacts with the `get_data_tiers` function to retrieve the current data tiers available in the cluster. Constants `tiermap` define the priority of each tier, facilitating the ordering process. If the specified `target_tier` is 'data_frozen' and the tier exists, it immediately returns 'data_frozen'. If no valid tiers are found, it defaults to 'data_content'.\"\"\"\n    'Do the tier preference thing in reverse order from coldest to hottest\\n    Based on the value of ``target_tier``, build out the list to use.\\n\\n    :param client: A client connection object\\n    :param target_tier: The target data tier, e.g. data_warm.\\n\\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\\n    :type target_tier: str\\n\\n    :returns: A suitable tier preference string in csv format\\n    :rtype: str\\n    '\n    tiermap = {'data_content': 0, 'data_hot': 1, 'data_warm': 2, 'data_cold': 3, 'data_frozen': 4}\n    tiers = get_data_tiers(client)\n    test_list = []\n    for tier in ['data_hot', 'data_warm', 'data_cold', 'data_frozen']:\n        if tier in tiers and tiermap[tier] <= tiermap[target_tier]:\n            test_list.insert(0, tier)\n    if target_tier == 'data_frozen':\n        if 'data_frozen' in tiers and tiers['data_frozen']:\n            return 'data_frozen'\n    preflist = []\n    for key in test_list:\n        if key in tiers and tiers[key]:\n            preflist.append(key)\n    if not preflist:\n        return 'data_content'\n    return ','.join(preflist)",
        "docstring": "Determine the preferred data tier(s) in an Elasticsearch cluster based on the specified target tier. The function constructs a string of data tiers in decreasing order of importance, from coldest to hottest, allowing for efficient data access and management.\n\n:param client: A client connection object for interacting with the Elasticsearch cluster.\n:type client: :py:class:`~.elasticsearch.Elasticsearch`\n:param target_tier: The target data tier for the preference (default is 'data_frozen').\n:type target_tier: str\n\n:returns: A comma-separated string of the preferred data tiers in order from coldest to hottest.\n:rtype: str\n\nThis function interacts with the `get_data_tiers` function to retrieve the current data tiers available in the cluster. Constants `tiermap` define the priority of each tier, facilitating the ordering process. If the specified `target_tier` is 'data_frozen' and the tier exists, it immediately returns 'data_frozen'. If no valid tiers are found, it defaults to 'data_content'.",
        "signature": "def get_tier_preference(client, target_tier='data_frozen'):",
        "type": "Function",
        "class_signature": null
      },
      "meta_getter": {
        "code": "def meta_getter(client, idx, get=None):\n    \"\"\"Retrieves metadata for an Elasticsearch index, either its settings or aliases, depending on the specified parameter.\n\n:param client: A client connection object to interact with Elasticsearch.\n:type client: :py:class:`~.elasticsearch.Elasticsearch`\n:param idx: The name of the Elasticsearch index for which metadata is requested.\n:type idx: str\n:param get: Specifies the type of metadata to retrieve; must be either 'settings' or 'alias'.\n:type get: str, optional\n\n:raises ConfigurationError: If the 'get' parameter is None or not one of the acceptable values ('settings', 'alias').\n:raises es8exc.NotFoundError: If the specified index is not found in Elasticsearch.\n:raises KeyError: If the expected key is not present in the response.\n:return: A dictionary containing the requested metadata from the index.\n:rtype: dict\n\nThis function utilizes the Elasticsearch client to call either `get_settings` or `get_alias` methods from the `IndicesClient`, depending on the value of the `get` parameter. It also employs a logger to capture potential errors and activities within the function.\"\"\"\n    'Meta Getter\\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.get_settings` or\\n    :py:meth:`~.elasticsearch.client.IndicesClient.get_alias`\\n\\n    :param client: A client connection object\\n    :param idx: An Elasticsearch index\\n    :param get: The kind of get to perform, e.g. settings or alias\\n\\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\\n    :type idx: str\\n    :type get: str\\n\\n    :returns: The settings from the get call to the named index\\n    :rtype: dict\\n    '\n    logger = logging.getLogger(__name__)\n    acceptable = ['settings', 'alias']\n    if not get:\n        raise ConfigurationError('\"get\" can not be a NoneType')\n    if get not in acceptable:\n        raise ConfigurationError(f'\"get\" must be one of {acceptable}')\n    retval = {}\n    try:\n        if get == 'settings':\n            retval = client.indices.get_settings(index=idx)[idx]['settings']['index']\n        elif get == 'alias':\n            retval = client.indices.get_alias(index=idx)[idx]['aliases']\n    except es8exc.NotFoundError as missing:\n        logger.error('Index %s was not found!', idx)\n        raise es8exc.NotFoundError from missing\n    except KeyError as err:\n        logger.error('Key not found: %s', err)\n        raise KeyError from err\n    except Exception as exc:\n        logger.error('Exception encountered: %s', exc)\n    return retval",
        "docstring": "Retrieves metadata for an Elasticsearch index, either its settings or aliases, depending on the specified parameter.\n\n:param client: A client connection object to interact with Elasticsearch.\n:type client: :py:class:`~.elasticsearch.Elasticsearch`\n:param idx: The name of the Elasticsearch index for which metadata is requested.\n:type idx: str\n:param get: Specifies the type of metadata to retrieve; must be either 'settings' or 'alias'.\n:type get: str, optional\n\n:raises ConfigurationError: If the 'get' parameter is None or not one of the acceptable values ('settings', 'alias').\n:raises es8exc.NotFoundError: If the specified index is not found in Elasticsearch.\n:raises KeyError: If the expected key is not present in the response.\n:return: A dictionary containing the requested metadata from the index.\n:rtype: dict\n\nThis function utilizes the Elasticsearch client to call either `get_settings` or `get_alias` methods from the `IndicesClient`, depending on the value of the `get` parameter. It also employs a logger to capture potential errors and activities within the function.",
        "signature": "def meta_getter(client, idx, get=None):",
        "type": "Function",
        "class_signature": null
      }
    },
    "curator/helpers/testers.py": {
      "has_lifecycle_name": {
        "code": "def has_lifecycle_name(idx_settings):\n    \"\"\"Checks whether an Elasticsearch index has a defined lifecycle name in its settings.\n\n:param idx_settings: A dictionary containing the settings of the index being examined.\n:type idx_settings: dict\n\n:returns: ``True`` if the index settings include a lifecycle name; otherwise, ``False``.\n:rtype: bool\n\nThis function operates by checking the presence of the 'lifecycle' key in the `idx_settings` dictionary, and subsequently checking for a 'name' key within the 'lifecycle'. If both keys are present, it confirms that the index has a lifecycle name defined, which is crucial for managing index workflows in Elasticsearch.\"\"\"\n    '\\n    :param idx_settings: The settings for an index being tested\\n    :type idx_settings: dict\\n\\n    :returns: ``True`` if a lifecycle name exists in settings, else ``False``\\n    :rtype: bool\\n    '\n    if 'lifecycle' in idx_settings:\n        if 'name' in idx_settings['lifecycle']:\n            return True\n    return False",
        "docstring": "Checks whether an Elasticsearch index has a defined lifecycle name in its settings.\n\n:param idx_settings: A dictionary containing the settings of the index being examined.\n:type idx_settings: dict\n\n:returns: ``True`` if the index settings include a lifecycle name; otherwise, ``False``.\n:rtype: bool\n\nThis function operates by checking the presence of the 'lifecycle' key in the `idx_settings` dictionary, and subsequently checking for a 'name' key within the 'lifecycle'. If both keys are present, it confirms that the index has a lifecycle name defined, which is crucial for managing index workflows in Elasticsearch.",
        "signature": "def has_lifecycle_name(idx_settings):",
        "type": "Function",
        "class_signature": null
      },
      "is_idx_partial": {
        "code": "def is_idx_partial(idx_settings):\n    \"\"\"Checks if an index's settings indicate it is a mounted searchable snapshot with partial storage enabled.\n\n:param idx_settings: A dictionary containing the settings of an Elasticsearch index.\n:type idx_settings: dict\n\n:returns: ``True`` if the index's settings contain `store.snapshot.partial` set to `True`, indicating that the index is a mounted searchable snapshot with partial storage. Returns ``False`` if `store.snapshot.partial` exists but is `False`, or if the relevant keys are missing (indicating a potentially cold tier mount). Raises `SearchableSnapshotException` if the index is not a mounted searchable snapshot.\n\n:raises SearchableSnapshotException: If the index is not a mounted searchable snapshot due to missing keys.\n\nThe function interacts with the Elasticsearch index settings structure, particularly focusing on the presence of specific keys: 'store', 'snapshot', and 'partial', which indicate the configuration of snapshot storage.\"\"\"\n    '\\n    :param idx_settings: The settings for an index being tested\\n    :type idx_settings: dict\\n\\n    :returns: ``True`` if store.snapshot.partial exists in settings, else ``False``\\n    :rtype: bool\\n    '\n    if 'store' in idx_settings:\n        if 'snapshot' in idx_settings['store']:\n            if 'partial' in idx_settings['store']['snapshot']:\n                if idx_settings['store']['snapshot']['partial']:\n                    return True\n                return False\n            return False\n        raise SearchableSnapshotException('Index not a mounted searchable snapshot')\n    raise SearchableSnapshotException('Index not a mounted searchable snapshot')",
        "docstring": "Checks if an index's settings indicate it is a mounted searchable snapshot with partial storage enabled.\n\n:param idx_settings: A dictionary containing the settings of an Elasticsearch index.\n:type idx_settings: dict\n\n:returns: ``True`` if the index's settings contain `store.snapshot.partial` set to `True`, indicating that the index is a mounted searchable snapshot with partial storage. Returns ``False`` if `store.snapshot.partial` exists but is `False`, or if the relevant keys are missing (indicating a potentially cold tier mount). Raises `SearchableSnapshotException` if the index is not a mounted searchable snapshot.\n\n:raises SearchableSnapshotException: If the index is not a mounted searchable snapshot due to missing keys.\n\nThe function interacts with the Elasticsearch index settings structure, particularly focusing on the presence of specific keys: 'store', 'snapshot', and 'partial', which indicate the configuration of snapshot storage.",
        "signature": "def is_idx_partial(idx_settings):",
        "type": "Function",
        "class_signature": null
      },
      "verify_client_object": {
        "code": "def verify_client_object(test):\n    \"\"\"Verify that the provided object is a valid Elasticsearch client instance.\n\nThis function checks if the input is an instance of the `Elasticsearch` client class from the `elasticsearch8` library. It handles mock objects used in testing by ignoring them. If the input is not a valid `Elasticsearch` client, it raises a `TypeError` with an appropriate error message.\n\n:param test: The variable or object to test.\n:type test: :py:class:`~.elasticsearch.Elasticsearch`\n\n:raises TypeError: If the input is not a valid Elasticsearch client object.\n\n:returns: ``True`` if the input is a valid client object.\n:rtype: bool\n\nDependencies:\n- `logging`: Used for logging error messages.\n- `elasticsearch8`: The library that defines the `Elasticsearch` class.\"\"\"\n    '\\n    :param test: The variable or object to test\\n\\n    :type test: :py:class:`~.elasticsearch.Elasticsearch`\\n\\n    :returns: ``True`` if ``test`` is a proper :py:class:`~.elasticsearch.Elasticsearch`\\n        client object and raise a :py:exc:`TypeError` exception if it is not.\\n    :rtype: bool\\n    '\n    logger = logging.getLogger(__name__)\n    if str(type(test)) == \"<class 'unittest.mock.Mock'>\":\n        pass\n    elif not isinstance(test, Elasticsearch):\n        msg = f'Not a valid client object. Type: {type(test)} was passed'\n        logger.error(msg)\n        raise TypeError(msg)",
        "docstring": "Verify that the provided object is a valid Elasticsearch client instance.\n\nThis function checks if the input is an instance of the `Elasticsearch` client class from the `elasticsearch8` library. It handles mock objects used in testing by ignoring them. If the input is not a valid `Elasticsearch` client, it raises a `TypeError` with an appropriate error message.\n\n:param test: The variable or object to test.\n:type test: :py:class:`~.elasticsearch.Elasticsearch`\n\n:raises TypeError: If the input is not a valid Elasticsearch client object.\n\n:returns: ``True`` if the input is a valid client object.\n:rtype: bool\n\nDependencies:\n- `logging`: Used for logging error messages.\n- `elasticsearch8`: The library that defines the `Elasticsearch` class.",
        "signature": "def verify_client_object(test):",
        "type": "Function",
        "class_signature": null
      },
      "verify_index_list": {
        "code": "def verify_index_list(test):\n    \"\"\"Verifies that the provided object is a valid instance of the `IndexList` class from the `curator.indexlist` module.\n\nThis function takes a single parameter, `test`, which is expected to be an object of type `IndexList`. If `test` is not an instance of `IndexList`, it raises a `TypeError`, logging an error message indicating the type of the object that was passed. The purpose of this verification is to ensure that the `test` variable holds the appropriate type before proceeding with operations that assume it is an `IndexList`.\n\nDependencies: The function imports `IndexList` from the `curator.indexlist` module locally to avoid circular import issues. It also utilizes Python's built-in logging module to log errors.\n\nParameters:\n- `test`: An object to be verified as an instance of `IndexList`.\n\nReturns:\n- None: If `test` is a valid `IndexList` object.\n\nRaises:\n- `TypeError`: If `test` is not an instance of `IndexList`.\"\"\"\n    '\\n    :param test: The variable or object to test\\n\\n    :type test: :py:class:`~.curator.IndexList`\\n\\n    :returns: ``None`` if ``test`` is a proper :py:class:`~.curator.indexlist.IndexList`\\n        object, else raise a :py:class:`TypeError` exception.\\n    :rtype: None\\n    '\n    from curator.indexlist import IndexList\n    logger = logging.getLogger(__name__)\n    if not isinstance(test, IndexList):\n        msg = f'Not a valid IndexList object. Type: {type(test)} was passed'\n        logger.error(msg)\n        raise TypeError(msg)",
        "docstring": "Verifies that the provided object is a valid instance of the `IndexList` class from the `curator.indexlist` module.\n\nThis function takes a single parameter, `test`, which is expected to be an object of type `IndexList`. If `test` is not an instance of `IndexList`, it raises a `TypeError`, logging an error message indicating the type of the object that was passed. The purpose of this verification is to ensure that the `test` variable holds the appropriate type before proceeding with operations that assume it is an `IndexList`.\n\nDependencies: The function imports `IndexList` from the `curator.indexlist` module locally to avoid circular import issues. It also utilizes Python's built-in logging module to log errors.\n\nParameters:\n- `test`: An object to be verified as an instance of `IndexList`.\n\nReturns:\n- None: If `test` is a valid `IndexList` object.\n\nRaises:\n- `TypeError`: If `test` is not an instance of `IndexList`.",
        "signature": "def verify_index_list(test):",
        "type": "Function",
        "class_signature": null
      }
    },
    "curator/actions/cold2frozen.py": {
      "Cold2Frozen.__init__": {
        "code": "    def __init__(self, ilo, **kwargs):\n        \"\"\"Initialize the Cold2Frozen action class for migrating searchable snapshots from the cold tier to the frozen tier in Elasticsearch.\n\n:param ilo: An IndexList Object that contains the indices to be migrated.\n:type ilo: :py:class:`~.curator.indexlist.IndexList`\n:param index_settings: (Optional) A dictionary of settings to be applied when the index is mounted. Defaults to None, which sets the `_tier_preference` based on available tiers.\n:type index_settings: dict\n:param ignore_index_settings: (Optional) A list of strings representing index settings that should be ignored during the mount process. Defaults to `['index.refresh_interval']`.\n:type ignore_index_settings: list\n:param wait_for_completion: (Optional) A boolean indicating whether to wait for the operation to complete prior to returning, defaulting to True.\n:type wait_for_completion: bool\n\nAttributes:\n- loggit: A logger instance for logging actions and errors.\n- index_list: An IndexList object containing the indices to manage.\n- client: An Elasticsearch client object associated with the index list.\n- indices: A representation of the indices in the IndexList.\n- index_settings: Settings to be applied to the index upon mounting.\n- ignore_index_settings: Settings to be ignored during the mount.\n- wait_for_completion: Indicates whether to wait for actions to complete.\n\nThis constructor verifies the provided IndexList and initializes necessary attributes, preparing the class for executing the migration actions defined in other methods.\"\"\"\n        '\\n        :param ilo: An IndexList Object\\n        :param index_settings: (Optional) Settings that should be added to the index when it is\\n            mounted. If not set, set the ``_tier_preference`` to the tiers available, coldest\\n            first.\\n        :param ignore_index_settings: (Optional, array of strings) Names of settings that should\\n            be removed from the index when it is mounted.\\n        :param wait_for_completion: Wait for completion before returning.\\n\\n        :type ilo: :py:class:`~.curator.indexlist.IndexList`\\n        :type index_settings: dict\\n        :type ignore_index_settings: list\\n        :type wait_for_completion: bool\\n        '\n        self.loggit = logging.getLogger('curator.actions.cold2frozen')\n        verify_index_list(ilo)\n        ilo.empty_list_check()\n        self.index_list = ilo\n        self.client = ilo.client\n        self.indices = ilo\n        self.index_settings = None\n        self.ignore_index_settings = None\n        self.wait_for_completion = None\n        self.assign_kwargs(**kwargs)",
        "docstring": "Initialize the Cold2Frozen action class for migrating searchable snapshots from the cold tier to the frozen tier in Elasticsearch.\n\n:param ilo: An IndexList Object that contains the indices to be migrated.\n:type ilo: :py:class:`~.curator.indexlist.IndexList`\n:param index_settings: (Optional) A dictionary of settings to be applied when the index is mounted. Defaults to None, which sets the `_tier_preference` based on available tiers.\n:type index_settings: dict\n:param ignore_index_settings: (Optional) A list of strings representing index settings that should be ignored during the mount process. Defaults to `['index.refresh_interval']`.\n:type ignore_index_settings: list\n:param wait_for_completion: (Optional) A boolean indicating whether to wait for the operation to complete prior to returning, defaulting to True.\n:type wait_for_completion: bool\n\nAttributes:\n- loggit: A logger instance for logging actions and errors.\n- index_list: An IndexList object containing the indices to manage.\n- client: An Elasticsearch client object associated with the index list.\n- indices: A representation of the indices in the IndexList.\n- index_settings: Settings to be applied to the index upon mounting.\n- ignore_index_settings: Settings to be ignored during the mount.\n- wait_for_completion: Indicates whether to wait for actions to complete.\n\nThis constructor verifies the provided IndexList and initializes necessary attributes, preparing the class for executing the migration actions defined in other methods.",
        "signature": "def __init__(self, ilo, **kwargs):",
        "type": "Method",
        "class_signature": "class Cold2Frozen:"
      },
      "Cold2Frozen.assign_kwargs": {
        "code": "    def assign_kwargs(self, **kwargs):\n        \"\"\"Assign the provided keyword arguments (kwargs) to the attributes of the Cold2Frozen class, matching them with the names defined in the DEFAULTS dictionary. If a specific key is not supplied in kwargs, the method will assign the default value from DEFAULTS to the corresponding attribute.\n\nParameters:\n    **kwargs: Arbitrary keyword arguments that may include 'index_settings', 'ignore_index_settings', and 'wait_for_completion'. The keys must correspond to attribute names in the class.\n\nReturns:\n    None - The method modifies the instance attributes directly based on the supplied arguments or defaults.\n\nSide Effects:\n    Updates instance attributes with values from kwargs or defaults, affecting the behavior of subsequent actions performed by the class.\n\nConstants:\n    DEFAULTS: A class-level dictionary that defines default values for certain attributes. It includes:\n        - 'index_settings': Default is None (no additional settings on index mount).\n        - 'ignore_index_settings': List of index settings to exclude during mount, defaults to ['index.refresh_interval'].\n        - 'wait_for_completion': Boolean flag, defaults to True, indicating whether to wait for the action to complete before returning.\"\"\"\n        '\\n        Assign the kwargs to the attribute of the same name with the passed value or the default\\n        from DEFAULTS\\n        '\n        for key, value in self.DEFAULTS.items():\n            if key in kwargs:\n                setattr(self, key, kwargs[key])\n            else:\n                setattr(self, key, value)",
        "docstring": "Assign the provided keyword arguments (kwargs) to the attributes of the Cold2Frozen class, matching them with the names defined in the DEFAULTS dictionary. If a specific key is not supplied in kwargs, the method will assign the default value from DEFAULTS to the corresponding attribute.\n\nParameters:\n    **kwargs: Arbitrary keyword arguments that may include 'index_settings', 'ignore_index_settings', and 'wait_for_completion'. The keys must correspond to attribute names in the class.\n\nReturns:\n    None - The method modifies the instance attributes directly based on the supplied arguments or defaults.\n\nSide Effects:\n    Updates instance attributes with values from kwargs or defaults, affecting the behavior of subsequent actions performed by the class.\n\nConstants:\n    DEFAULTS: A class-level dictionary that defines default values for certain attributes. It includes:\n        - 'index_settings': Default is None (no additional settings on index mount).\n        - 'ignore_index_settings': List of index settings to exclude during mount, defaults to ['index.refresh_interval'].\n        - 'wait_for_completion': Boolean flag, defaults to True, indicating whether to wait for the action to complete before returning.",
        "signature": "def assign_kwargs(self, **kwargs):",
        "type": "Method",
        "class_signature": "class Cold2Frozen:"
      },
      "Cold2Frozen.action_generator": {
        "code": "    def action_generator(self):\n        \"\"\"Yield a dictionary for each index in the index list to facilitate the migration of indices\n    from the cold tier to the frozen tier as searchable snapshots. This method checks the index \n    settings to ensure the index is not associated with an ILM policy and is not already in the \n    frozen tier. The generator constructs the necessary configurations, including snapshot and \n    repository details, index settings, and alias information, required for restoring the index \n    in the frozen tier.\n\n    :returns: A generator yielding dictionaries containing settings for migrating indices, which \n              includes 'repository', 'snapshot', 'index', 'renamed_index', 'index_settings', \n              'ignore_index_settings', 'storage', 'wait_for_completion', 'aliases', and \n              'current_idx'.\n    :rtype: generator\n\n    Dependencies:\n    - Uses the `meta_getter` function to retrieve index settings and aliases for each index.\n    - Utilizes `get_tier_preference` to determine the appropriate tier preferences for mounting.\n    \n    Constants:\n    - `self.index_settings`: Defines how indices should be allocated within the Elasticsearch \n      cluster, defaulting to tiers preference based on the client if not provided.\"\"\"\n        'Yield a dict for use in :py:meth:`do_action` and :py:meth:`do_dry_run`\\n\\n        :returns: A generator object containing the settings necessary to migrate indices from cold\\n            to frozen\\n        :rtype: dict\\n        '\n        for idx in self.index_list.indices:\n            idx_settings = meta_getter(self.client, idx, get='settings')\n            self.loggit.debug('Index %s has settings: %s', idx, idx_settings)\n            if has_lifecycle_name(idx_settings):\n                self.loggit.critical('Index %s is associated with an ILM policy and this action will never work on an index associated with an ILM policy', idx)\n                raise CuratorException(f'Index {idx} is associated with an ILM policy')\n            if is_idx_partial(idx_settings):\n                self.loggit.critical('Index %s is already in the frozen tier', idx)\n                raise SearchableSnapshotException('Index is already in frozen tier')\n            snap = idx_settings['store']['snapshot']['snapshot_name']\n            snap_idx = idx_settings['store']['snapshot']['index_name']\n            repo = idx_settings['store']['snapshot']['repository_name']\n            msg = f'Index {idx} Snapshot name: {snap}, Snapshot index: {snap_idx}, repo: {repo}'\n            self.loggit.debug(msg)\n            aliases = meta_getter(self.client, idx, get='alias')\n            renamed = f'partial-{idx}'\n            if not self.index_settings:\n                self.index_settings = {'routing': {'allocation': {'include': {'_tier_preference': get_tier_preference(self.client)}}}}\n            yield {'repository': repo, 'snapshot': snap, 'index': snap_idx, 'renamed_index': renamed, 'index_settings': self.index_settings, 'ignore_index_settings': self.ignore_index_settings, 'storage': 'shared_cache', 'wait_for_completion': self.wait_for_completion, 'aliases': aliases, 'current_idx': idx}",
        "docstring": "Yield a dictionary for each index in the index list to facilitate the migration of indices\nfrom the cold tier to the frozen tier as searchable snapshots. This method checks the index \nsettings to ensure the index is not associated with an ILM policy and is not already in the \nfrozen tier. The generator constructs the necessary configurations, including snapshot and \nrepository details, index settings, and alias information, required for restoring the index \nin the frozen tier.\n\n:returns: A generator yielding dictionaries containing settings for migrating indices, which \n          includes 'repository', 'snapshot', 'index', 'renamed_index', 'index_settings', \n          'ignore_index_settings', 'storage', 'wait_for_completion', 'aliases', and \n          'current_idx'.\n:rtype: generator\n\nDependencies:\n- Uses the `meta_getter` function to retrieve index settings and aliases for each index.\n- Utilizes `get_tier_preference` to determine the appropriate tier preferences for mounting.\n\nConstants:\n- `self.index_settings`: Defines how indices should be allocated within the Elasticsearch \n  cluster, defaulting to tiers preference based on the client if not provided.",
        "signature": "def action_generator(self):",
        "type": "Method",
        "class_signature": "class Cold2Frozen:"
      },
      "Cold2Frozen.do_dry_run": {
        "code": "    def do_dry_run(self):\n        \"\"\"Log the actions that would be performed to migrate indices from the cold tier to the frozen tier without executing them. This method outputs detailed information about the intended operations, including snapshot details, index settings, and aliases for each index defined in the `IndexList` object.\n\nParameters:\n- No parameters.\n\nReturns:\n- None.\n\nSide Effects:\n- Logs information to `loggit`, indicating actions that would be taken during a real migration without making any changes.\n\nDependencies:\n- Utilizes the `action_generator()` method to retrieve necessary parameters for log messages.\n- The `aliases` and `current_idx` variables are extracted from the generated kwargs. `aliases` represents index aliases to be transferred to the new index, and `current_idx` refers to the original index name being processed.\"\"\"\n        'Log what the output would be, but take no action.'\n        self.loggit.info('DRY-RUN MODE.  No changes will be made.')\n        for kwargs in self.action_generator():\n            aliases = kwargs.pop('aliases')\n            current_idx = kwargs.pop('current_idx')\n            msg = f'DRY-RUN: cold2frozen: from snapshot {kwargs['snapshot']} in repository {kwargs['repository']}, mount index {kwargs['index']} renamed as {kwargs['renamed_index']} with index settings: {kwargs['index_settings']} and ignoring settings: {kwargs['ignore_index_settings']}. wait_for_completion: {kwargs['wait_for_completion']}. Restore aliases: {aliases}. Current index name: {current_idx}'\n            self.loggit.info(msg)",
        "docstring": "Log the actions that would be performed to migrate indices from the cold tier to the frozen tier without executing them. This method outputs detailed information about the intended operations, including snapshot details, index settings, and aliases for each index defined in the `IndexList` object.\n\nParameters:\n- No parameters.\n\nReturns:\n- None.\n\nSide Effects:\n- Logs information to `loggit`, indicating actions that would be taken during a real migration without making any changes.\n\nDependencies:\n- Utilizes the `action_generator()` method to retrieve necessary parameters for log messages.\n- The `aliases` and `current_idx` variables are extracted from the generated kwargs. `aliases` represents index aliases to be transferred to the new index, and `current_idx` refers to the original index name being processed.",
        "signature": "def do_dry_run(self):",
        "type": "Method",
        "class_signature": "class Cold2Frozen:"
      }
    },
    "curator/exceptions.py": {},
    "curator/indexlist.py": {
      "IndexList.__init__": {
        "code": "    def __init__(self, client, search_pattern='_all'):\n        \"\"\"Initialize an `IndexList` object for managing Elasticsearch indices.\n\nThis constructor takes an Elasticsearch client and an optional search pattern to populate the index list. It verifies the client object, sets up logger for debugging, and initializes three key attributes: `index_info`, which holds metadata about the indices, `indices`, the current running list of indices, and `all_indices`, a complete list of indices at initialization. The method then calls `__get_indices` to populate these attributes based on the search pattern provided.\n\nParameters:\n- `client`: An instance of the Elasticsearch client, which is validated by the `verify_client_object` function.\n- `search_pattern`: A string pattern to match indices. Defaults to `'_all'`, which includes all indices.\n\nReturns:\n- None\n\nAttributes initialized:\n- `self.index_info`: A dictionary to store metadata extracted from indices, populated during initialization.\n- `self.indices`: A list to maintain the running list of actionable indices.\n- `self.all_indices`: A list to store all indices fetched from the cluster.\"\"\"\n        verify_client_object(client)\n        self.loggit = logging.getLogger('curator.indexlist')\n        self.client = client\n        self.index_info = {}\n        self.indices = []\n        self.all_indices = []\n        self.__get_indices(search_pattern)\n        self.age_keyfield = None",
        "docstring": "Initialize an `IndexList` object for managing Elasticsearch indices.\n\nThis constructor takes an Elasticsearch client and an optional search pattern to populate the index list. It verifies the client object, sets up logger for debugging, and initializes three key attributes: `index_info`, which holds metadata about the indices, `indices`, the current running list of indices, and `all_indices`, a complete list of indices at initialization. The method then calls `__get_indices` to populate these attributes based on the search pattern provided.\n\nParameters:\n- `client`: An instance of the Elasticsearch client, which is validated by the `verify_client_object` function.\n- `search_pattern`: A string pattern to match indices. Defaults to `'_all'`, which includes all indices.\n\nReturns:\n- None\n\nAttributes initialized:\n- `self.index_info`: A dictionary to store metadata extracted from indices, populated during initialization.\n- `self.indices`: A list to maintain the running list of actionable indices.\n- `self.all_indices`: A list to store all indices fetched from the cluster.",
        "signature": "def __init__(self, client, search_pattern='_all'):",
        "type": "Method",
        "class_signature": "class IndexList:"
      },
      "IndexList.__get_indices": {
        "code": "    def __get_indices(self, pattern):\n        \"\"\"Retrieve and populate all indices from the Elasticsearch client based on the given search pattern.\n\nParameters:\n- pattern (str): A search pattern used to fetch indices from the Elasticsearch client. Default is '_all', which retrieves all indices.\n\nThis method updates the `all_indices` attribute with all indices matching the `pattern` by calling `get_indices` from the `curator.helpers.getters` module. It then initializes the `indices` attribute as a copy of `all_indices`, making it available for further operations within the `IndexList` class. The `loggit` logger is used to document the activity, providing debugging information during the process of acquiring indices.\"\"\"\n        '\\n        Pull all indices into ``all_indices``, then populate ``indices`` and\\n        ``index_info``\\n        '\n        self.loggit.debug('Getting indices matching search_pattern: \"%s\"', pattern)\n        self.all_indices = get_indices(self.client, search_pattern=pattern)\n        self.indices = self.all_indices[:]",
        "docstring": "Retrieve and populate all indices from the Elasticsearch client based on the given search pattern.\n\nParameters:\n- pattern (str): A search pattern used to fetch indices from the Elasticsearch client. Default is '_all', which retrieves all indices.\n\nThis method updates the `all_indices` attribute with all indices matching the `pattern` by calling `get_indices` from the `curator.helpers.getters` module. It then initializes the `indices` attribute as a copy of `all_indices`, making it available for further operations within the `IndexList` class. The `loggit` logger is used to document the activity, providing debugging information during the process of acquiring indices.",
        "signature": "def __get_indices(self, pattern):",
        "type": "Method",
        "class_signature": "class IndexList:"
      },
      "IndexList.empty_list_check": {
        "code": "    def empty_list_check(self):\n        \"\"\"Check if the list of indices is empty and raise an exception if it is.\n\nThis method verifies the `indices` attribute of the `IndexList` instance. If `indices` is empty, it raises a `NoIndices` exception, indicating that no indices are available for processing. This is crucial for ensuring that subsequent operations on indices do not fail due to an empty list.\n\nRaises:\n    NoIndices: If the `indices` attribute is empty.\n\nDependencies:\n    - `self.indices`: A list attribute that holds the current indices. It is populated during the initialization of the `IndexList` instance and is manipulated by various filtering methods within the class.\"\"\"\n        'Raise :py:exc:`~.curator.exceptions.NoIndices` if ``indices`` is empty'\n        self.loggit.debug('Checking for empty list')\n        if not self.indices:\n            raise NoIndices('index_list object is empty.')",
        "docstring": "Check if the list of indices is empty and raise an exception if it is.\n\nThis method verifies the `indices` attribute of the `IndexList` instance. If `indices` is empty, it raises a `NoIndices` exception, indicating that no indices are available for processing. This is crucial for ensuring that subsequent operations on indices do not fail due to an empty list.\n\nRaises:\n    NoIndices: If the `indices` attribute is empty.\n\nDependencies:\n    - `self.indices`: A list attribute that holds the current indices. It is populated during the initialization of the `IndexList` instance and is manipulated by various filtering methods within the class.",
        "signature": "def empty_list_check(self):",
        "type": "Method",
        "class_signature": "class IndexList:"
      }
    }
  },
  "dependency_dict": {
    "curator/indexlist.py:IndexList:__init__": {},
    "curator/helpers/testers.py:verify_client_object": {},
    "curator/indexlist.py:IndexList:__get_indices": {
      "curator/helpers/getters.py": {
        "get_indices": {
          "code": "def get_indices(client, search_pattern='_all'):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.CatClient.indices`\n\n    :param client: A client connection object\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n\n    :returns: The current list of indices from the cluster\n    :rtype: list\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    indices = []\n    try:\n        resp = client.cat.indices(index=search_pattern, expand_wildcards='open,closed', h='index,status', format='json')\n    except Exception as err:\n        raise FailedExecution(f'Failed to get indices. Error: {err}') from err\n    if not resp:\n        return indices\n    for entry in resp:\n        indices.append(entry['index'])\n    logger.debug('All indices: %s', indices)\n    return indices",
          "docstring": "Calls :py:meth:`~.elasticsearch.client.CatClient.indices`\n\n:param client: A client connection object\n:type client: :py:class:`~.elasticsearch.Elasticsearch`\n\n:returns: The current list of indices from the cluster\n:rtype: list",
          "signature": "def get_indices(client, search_pattern='_all'):",
          "type": "Function",
          "class_signature": null
        }
      }
    },
    "curator/actions/cold2frozen.py:Cold2Frozen:__init__": {},
    "curator/helpers/testers.py:verify_index_list": {},
    "curator/indexlist.py:IndexList:empty_list_check": {},
    "curator/actions/cold2frozen.py:Cold2Frozen:assign_kwargs": {},
    "curator/actions/cold2frozen.py:Cold2Frozen:action_generator": {},
    "curator/helpers/getters.py:meta_getter": {},
    "curator/helpers/testers.py:has_lifecycle_name": {},
    "curator/helpers/testers.py:is_idx_partial": {},
    "curator/helpers/getters.py:get_tier_preference": {
      "curator/helpers/getters.py": {
        "get_data_tiers": {
          "code": "def get_data_tiers(client):\n    \"\"\"\n    Get all valid data tiers from the node roles of each node in the cluster by\n    polling each node\n\n    :param client: A client connection object\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n\n    :returns: The available data tiers in ``tier: bool`` form.\n    :rtype: dict\n    \"\"\"\n\n    def role_check(role, node_info):\n        if role in node_info['roles']:\n            return True\n        return False\n    info = client.nodes.info()['nodes']\n    retval = {'data_hot': False, 'data_warm': False, 'data_cold': False, 'data_frozen': False}\n    for node in info:\n        for role in ['data_hot', 'data_warm', 'data_cold', 'data_frozen']:\n            if role_check(role, info[node]):\n                retval[role] = True\n    return retval",
          "docstring": "Get all valid data tiers from the node roles of each node in the cluster by\npolling each node\n\n:param client: A client connection object\n:type client: :py:class:`~.elasticsearch.Elasticsearch`\n\n:returns: The available data tiers in ``tier: bool`` form.\n:rtype: dict",
          "signature": "def get_data_tiers(client):",
          "type": "Function",
          "class_signature": null
        }
      }
    },
    "curator/actions/cold2frozen.py:Cold2Frozen:do_dry_run": {}
  },
  "call_tree": {
    "tests/unit/test_action_cold2frozen.py:TestActionCold2Frozen:test_action_generator1": {
      "tests/unit/test_action_cold2frozen.py:TestActionCold2Frozen:builder": {
        "curator/indexlist.py:IndexList:__init__": {
          "curator/helpers/testers.py:verify_client_object": {},
          "curator/indexlist.py:IndexList:__get_indices": {
            "curator/helpers/getters.py:get_indices": {}
          }
        }
      },
      "curator/actions/cold2frozen.py:Cold2Frozen:__init__": {
        "curator/helpers/testers.py:verify_index_list": {
          "curator/indexlist.py:IndexList:IndexList": {}
        },
        "curator/indexlist.py:IndexList:empty_list_check": {},
        "curator/actions/cold2frozen.py:Cold2Frozen:assign_kwargs": {}
      },
      "curator/actions/cold2frozen.py:Cold2Frozen:action_generator": {
        "curator/helpers/getters.py:meta_getter": {},
        "curator/helpers/testers.py:has_lifecycle_name": {},
        "curator/helpers/testers.py:is_idx_partial": {},
        "curator/helpers/getters.py:get_tier_preference": {
          "curator/helpers/getters.py:get_data_tiers": {
            "curator/helpers/getters.py:role_check": {}
          }
        }
      },
      "curator/actions/cold2frozen.py:Cold2Frozen:do_dry_run": {
        "curator/actions/cold2frozen.py:Cold2Frozen:action_generator": {
          "curator/helpers/getters.py:meta_getter": {},
          "curator/helpers/testers.py:has_lifecycle_name": {},
          "curator/helpers/testers.py:is_idx_partial": {}
        }
      }
    },
    "tests/unit/test_action_cold2frozen.py:TestActionCold2Frozen:test_action_generator2": {
      "tests/unit/test_action_cold2frozen.py:TestActionCold2Frozen:builder": {
        "curator/indexlist.py:IndexList:__init__": {
          "curator/helpers/testers.py:verify_client_object": {},
          "curator/indexlist.py:IndexList:__get_indices": {
            "curator/helpers/getters.py:get_indices": {}
          }
        }
      },
      "curator/actions/cold2frozen.py:Cold2Frozen:__init__": {
        "curator/helpers/testers.py:verify_index_list": {},
        "curator/indexlist.py:IndexList:empty_list_check": {},
        "curator/actions/cold2frozen.py:Cold2Frozen:assign_kwargs": {}
      },
      "curator/actions/cold2frozen.py:Cold2Frozen:action_generator": {
        "curator/helpers/getters.py:meta_getter": {},
        "curator/helpers/testers.py:has_lifecycle_name": {}
      }
    },
    "tests/unit/test_action_cold2frozen.py:TestActionCold2Frozen:test_action_generator3": {
      "tests/unit/test_action_cold2frozen.py:TestActionCold2Frozen:builder": {
        "curator/indexlist.py:IndexList:__init__": {
          "curator/helpers/testers.py:verify_client_object": {},
          "curator/indexlist.py:IndexList:__get_indices": {
            "curator/helpers/getters.py:get_indices": {}
          }
        }
      },
      "curator/actions/cold2frozen.py:Cold2Frozen:__init__": {
        "curator/helpers/testers.py:verify_index_list": {},
        "curator/indexlist.py:IndexList:empty_list_check": {},
        "curator/actions/cold2frozen.py:Cold2Frozen:assign_kwargs": {}
      },
      "curator/actions/cold2frozen.py:Cold2Frozen:action_generator": {
        "curator/helpers/getters.py:meta_getter": {},
        "curator/helpers/testers.py:has_lifecycle_name": {},
        "curator/helpers/testers.py:is_idx_partial": {}
      }
    },
    "tests/unit/test_action_cold2frozen.py:TestActionCold2Frozen:test_init_add_kwargs": {
      "tests/unit/test_action_cold2frozen.py:TestActionCold2Frozen:builder": {
        "curator/indexlist.py:IndexList:__init__": {
          "curator/helpers/testers.py:verify_client_object": {},
          "curator/indexlist.py:IndexList:__get_indices": {
            "curator/helpers/getters.py:get_indices": {}
          }
        }
      },
      "curator/actions/cold2frozen.py:Cold2Frozen:__init__": {
        "curator/helpers/testers.py:verify_index_list": {},
        "curator/indexlist.py:IndexList:empty_list_check": {},
        "curator/actions/cold2frozen.py:Cold2Frozen:assign_kwargs": {}
      }
    },
    "tests/unit/test_action_cold2frozen.py:TestActionCold2Frozen:test_init_raise_bad_index_list": {
      "curator/actions/cold2frozen.py:Cold2Frozen:__init__": {
        "curator/helpers/testers.py:verify_index_list": {}
      }
    },
    "/mnt/sfs_turbo/yaxindu/tmp/elasticsearch_curator-image-test_action_cold2frozen/elasticsearch_curator-test_action_cold2frozen/tests/integration/test_cli.py:TestCLIMethods:test_action_is_none": {
      "curator/exceptions.py:ConfigurationError:ConfigurationError": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/elasticsearch_curator-image-test_action_cold2frozen/elasticsearch_curator-test_action_cold2frozen/tests/integration/test_cli.py:TestCLIMethods:test_no_action": {
      "curator/exceptions.py:ConfigurationError:ConfigurationError": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/elasticsearch_curator-image-test_action_cold2frozen/elasticsearch_curator-test_action_cold2frozen/tests/integration/test_integrations.py:TestFilters:test_filter_by_alias_bad_aliases": {
      "curator/exceptions.py:ConfigurationError:ConfigurationError": {}
    }
  },
  "PRD": "# PROJECT NAME: elasticsearch_curator-test_action_cold2frozen\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 curator/\n    \u251c\u2500\u2500 actions/\n    \u2502   \u2514\u2500\u2500 cold2frozen.py\n    \u2502       \u251c\u2500\u2500 Cold2Frozen.__init__\n    \u2502       \u251c\u2500\u2500 Cold2Frozen.action_generator\n    \u2502       \u251c\u2500\u2500 Cold2Frozen.assign_kwargs\n    \u2502       \u2514\u2500\u2500 Cold2Frozen.do_dry_run\n    \u251c\u2500\u2500 exceptions.py\n    \u2502   \u2514\u2500\u2500 ConfigurationError.ConfigurationError\n    \u251c\u2500\u2500 helpers/\n    \u2502   \u251c\u2500\u2500 getters.py\n    \u2502   \u2502   \u251c\u2500\u2500 get_tier_preference\n    \u2502   \u2502   \u2514\u2500\u2500 meta_getter\n    \u2502   \u2514\u2500\u2500 testers.py\n    \u2502       \u251c\u2500\u2500 has_lifecycle_name\n    \u2502       \u251c\u2500\u2500 is_idx_partial\n    \u2502       \u251c\u2500\u2500 verify_client_object\n    \u2502       \u2514\u2500\u2500 verify_index_list\n    \u2514\u2500\u2500 indexlist.py\n        \u251c\u2500\u2500 IndexList.__get_indices\n        \u251c\u2500\u2500 IndexList.__init__\n        \u2514\u2500\u2500 IndexList.empty_list_check\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThe `Cold2Frozen` module is designed to facilitate the transition of Elasticsearch indices from the cold to the frozen data tier in a managed and automated manner. It provides functionality to prepare, validate, and execute the transformation of cold indices into frozen indices, including applying tier-specific settings, managing snapshot-based storage configurations, and ensuring compliance with index lifecycle management (ILM) policies. By automating this process, the module reduces the manual effort required to optimize index storage and retrieval in cost-effective tiers while ensuring data availability and integrity. This functionality is particularly valuable for developers and administrators managing large-scale Elasticsearch clusters, enabling efficient tier-based data management and compliance with storage optimization strategies.\n\n## FILE 1: curator/helpers/getters.py\n\n- FUNCTION NAME: get_tier_preference\n  - SIGNATURE: def get_tier_preference(client, target_tier='data_frozen'):\n  - DOCSTRING: \n```python\n\"\"\"\nDetermine the preferred data tier(s) in an Elasticsearch cluster based on the specified target tier. The function constructs a string of data tiers in decreasing order of importance, from coldest to hottest, allowing for efficient data access and management.\n\n:param client: A client connection object for interacting with the Elasticsearch cluster.\n:type client: :py:class:`~.elasticsearch.Elasticsearch`\n:param target_tier: The target data tier for the preference (default is 'data_frozen').\n:type target_tier: str\n\n:returns: A comma-separated string of the preferred data tiers in order from coldest to hottest.\n:rtype: str\n\nThis function interacts with the `get_data_tiers` function to retrieve the current data tiers available in the cluster. Constants `tiermap` define the priority of each tier, facilitating the ordering process. If the specified `target_tier` is 'data_frozen' and the tier exists, it immediately returns 'data_frozen'. If no valid tiers are found, it defaults to 'data_content'.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - curator/actions/cold2frozen.py:Cold2Frozen:action_generator\n    - curator/helpers/getters.py:get_data_tiers\n\n- FUNCTION NAME: meta_getter\n  - SIGNATURE: def meta_getter(client, idx, get=None):\n  - DOCSTRING: \n```python\n\"\"\"\nRetrieves metadata for an Elasticsearch index, either its settings or aliases, depending on the specified parameter.\n\n:param client: A client connection object to interact with Elasticsearch.\n:type client: :py:class:`~.elasticsearch.Elasticsearch`\n:param idx: The name of the Elasticsearch index for which metadata is requested.\n:type idx: str\n:param get: Specifies the type of metadata to retrieve; must be either 'settings' or 'alias'.\n:type get: str, optional\n\n:raises ConfigurationError: If the 'get' parameter is None or not one of the acceptable values ('settings', 'alias').\n:raises es8exc.NotFoundError: If the specified index is not found in Elasticsearch.\n:raises KeyError: If the expected key is not present in the response.\n:return: A dictionary containing the requested metadata from the index.\n:rtype: dict\n\nThis function utilizes the Elasticsearch client to call either `get_settings` or `get_alias` methods from the `IndicesClient`, depending on the value of the `get` parameter. It also employs a logger to capture potential errors and activities within the function.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - curator/actions/cold2frozen.py:Cold2Frozen:action_generator\n\n## FILE 2: curator/helpers/testers.py\n\n- FUNCTION NAME: is_idx_partial\n  - SIGNATURE: def is_idx_partial(idx_settings):\n  - DOCSTRING: \n```python\n\"\"\"\nChecks if an index's settings indicate it is a mounted searchable snapshot with partial storage enabled.\n\n:param idx_settings: A dictionary containing the settings of an Elasticsearch index.\n:type idx_settings: dict\n\n:returns: ``True`` if the index's settings contain `store.snapshot.partial` set to `True`, indicating that the index is a mounted searchable snapshot with partial storage. Returns ``False`` if `store.snapshot.partial` exists but is `False`, or if the relevant keys are missing (indicating a potentially cold tier mount). Raises `SearchableSnapshotException` if the index is not a mounted searchable snapshot.\n\n:raises SearchableSnapshotException: If the index is not a mounted searchable snapshot due to missing keys.\n\nThe function interacts with the Elasticsearch index settings structure, particularly focusing on the presence of specific keys: 'store', 'snapshot', and 'partial', which indicate the configuration of snapshot storage.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - curator/actions/cold2frozen.py:Cold2Frozen:action_generator\n\n- FUNCTION NAME: verify_client_object\n  - SIGNATURE: def verify_client_object(test):\n  - DOCSTRING: \n```python\n\"\"\"\nVerify that the provided object is a valid Elasticsearch client instance.\n\nThis function checks if the input is an instance of the `Elasticsearch` client class from the `elasticsearch8` library. It handles mock objects used in testing by ignoring them. If the input is not a valid `Elasticsearch` client, it raises a `TypeError` with an appropriate error message.\n\n:param test: The variable or object to test.\n:type test: :py:class:`~.elasticsearch.Elasticsearch`\n\n:raises TypeError: If the input is not a valid Elasticsearch client object.\n\n:returns: ``True`` if the input is a valid client object.\n:rtype: bool\n\nDependencies:\n- `logging`: Used for logging error messages.\n- `elasticsearch8`: The library that defines the `Elasticsearch` class.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - curator/indexlist.py:IndexList:__init__\n\n- FUNCTION NAME: verify_index_list\n  - SIGNATURE: def verify_index_list(test):\n  - DOCSTRING: \n```python\n\"\"\"\nVerifies that the provided object is a valid instance of the `IndexList` class from the `curator.indexlist` module.\n\nThis function takes a single parameter, `test`, which is expected to be an object of type `IndexList`. If `test` is not an instance of `IndexList`, it raises a `TypeError`, logging an error message indicating the type of the object that was passed. The purpose of this verification is to ensure that the `test` variable holds the appropriate type before proceeding with operations that assume it is an `IndexList`.\n\nDependencies: The function imports `IndexList` from the `curator.indexlist` module locally to avoid circular import issues. It also utilizes Python's built-in logging module to log errors.\n\nParameters:\n- `test`: An object to be verified as an instance of `IndexList`.\n\nReturns:\n- None: If `test` is a valid `IndexList` object.\n\nRaises:\n- `TypeError`: If `test` is not an instance of `IndexList`.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - curator/indexlist.py:IndexList:IndexList\n    - curator/actions/cold2frozen.py:Cold2Frozen:__init__\n\n- FUNCTION NAME: has_lifecycle_name\n  - SIGNATURE: def has_lifecycle_name(idx_settings):\n  - DOCSTRING: \n```python\n\"\"\"\nChecks whether an Elasticsearch index has a defined lifecycle name in its settings.\n\n:param idx_settings: A dictionary containing the settings of the index being examined.\n:type idx_settings: dict\n\n:returns: ``True`` if the index settings include a lifecycle name; otherwise, ``False``.\n:rtype: bool\n\nThis function operates by checking the presence of the 'lifecycle' key in the `idx_settings` dictionary, and subsequently checking for a 'name' key within the 'lifecycle'. If both keys are present, it confirms that the index has a lifecycle name defined, which is crucial for managing index workflows in Elasticsearch.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - curator/actions/cold2frozen.py:Cold2Frozen:action_generator\n\n## FILE 3: curator/actions/cold2frozen.py\n\n- CLASS METHOD: Cold2Frozen.action_generator\n  - CLASS SIGNATURE: class Cold2Frozen:\n  - SIGNATURE: def action_generator(self):\n  - DOCSTRING: \n```python\n\"\"\"\nYield a dictionary for each index in the index list to facilitate the migration of indices\nfrom the cold tier to the frozen tier as searchable snapshots. This method checks the index \nsettings to ensure the index is not associated with an ILM policy and is not already in the \nfrozen tier. The generator constructs the necessary configurations, including snapshot and \nrepository details, index settings, and alias information, required for restoring the index \nin the frozen tier.\n\n:returns: A generator yielding dictionaries containing settings for migrating indices, which \n          includes 'repository', 'snapshot', 'index', 'renamed_index', 'index_settings', \n          'ignore_index_settings', 'storage', 'wait_for_completion', 'aliases', and \n          'current_idx'.\n:rtype: generator\n\nDependencies:\n- Uses the `meta_getter` function to retrieve index settings and aliases for each index.\n- Utilizes `get_tier_preference` to determine the appropriate tier preferences for mounting.\n\nConstants:\n- `self.index_settings`: Defines how indices should be allocated within the Elasticsearch \n  cluster, defaulting to tiers preference based on the client if not provided.\n\"\"\"\n```\n\n- CLASS METHOD: Cold2Frozen.do_dry_run\n  - CLASS SIGNATURE: class Cold2Frozen:\n  - SIGNATURE: def do_dry_run(self):\n  - DOCSTRING: \n```python\n\"\"\"\nLog the actions that would be performed to migrate indices from the cold tier to the frozen tier without executing them. This method outputs detailed information about the intended operations, including snapshot details, index settings, and aliases for each index defined in the `IndexList` object.\n\nParameters:\n- No parameters.\n\nReturns:\n- None.\n\nSide Effects:\n- Logs information to `loggit`, indicating actions that would be taken during a real migration without making any changes.\n\nDependencies:\n- Utilizes the `action_generator()` method to retrieve necessary parameters for log messages.\n- The `aliases` and `current_idx` variables are extracted from the generated kwargs. `aliases` represents index aliases to be transferred to the new index, and `current_idx` refers to the original index name being processed.\n\"\"\"\n```\n\n- CLASS METHOD: Cold2Frozen.assign_kwargs\n  - CLASS SIGNATURE: class Cold2Frozen:\n  - SIGNATURE: def assign_kwargs(self, **kwargs):\n  - DOCSTRING: \n```python\n\"\"\"\nAssign the provided keyword arguments (kwargs) to the attributes of the Cold2Frozen class, matching them with the names defined in the DEFAULTS dictionary. If a specific key is not supplied in kwargs, the method will assign the default value from DEFAULTS to the corresponding attribute.\n\nParameters:\n    **kwargs: Arbitrary keyword arguments that may include 'index_settings', 'ignore_index_settings', and 'wait_for_completion'. The keys must correspond to attribute names in the class.\n\nReturns:\n    None - The method modifies the instance attributes directly based on the supplied arguments or defaults.\n\nSide Effects:\n    Updates instance attributes with values from kwargs or defaults, affecting the behavior of subsequent actions performed by the class.\n\nConstants:\n    DEFAULTS: A class-level dictionary that defines default values for certain attributes. It includes:\n        - 'index_settings': Default is None (no additional settings on index mount).\n        - 'ignore_index_settings': List of index settings to exclude during mount, defaults to ['index.refresh_interval'].\n        - 'wait_for_completion': Boolean flag, defaults to True, indicating whether to wait for the action to complete before returning.\n\"\"\"\n```\n\n- CLASS METHOD: Cold2Frozen.__init__\n  - CLASS SIGNATURE: class Cold2Frozen:\n  - SIGNATURE: def __init__(self, ilo, **kwargs):\n  - DOCSTRING: \n```python\n\"\"\"\nInitialize the Cold2Frozen action class for migrating searchable snapshots from the cold tier to the frozen tier in Elasticsearch.\n\n:param ilo: An IndexList Object that contains the indices to be migrated.\n:type ilo: :py:class:`~.curator.indexlist.IndexList`\n:param index_settings: (Optional) A dictionary of settings to be applied when the index is mounted. Defaults to None, which sets the `_tier_preference` based on available tiers.\n:type index_settings: dict\n:param ignore_index_settings: (Optional) A list of strings representing index settings that should be ignored during the mount process. Defaults to `['index.refresh_interval']`.\n:type ignore_index_settings: list\n:param wait_for_completion: (Optional) A boolean indicating whether to wait for the operation to complete prior to returning, defaulting to True.\n:type wait_for_completion: bool\n\nAttributes:\n- loggit: A logger instance for logging actions and errors.\n- index_list: An IndexList object containing the indices to manage.\n- client: An Elasticsearch client object associated with the index list.\n- indices: A representation of the indices in the IndexList.\n- index_settings: Settings to be applied to the index upon mounting.\n- ignore_index_settings: Settings to be ignored during the mount.\n- wait_for_completion: Indicates whether to wait for actions to complete.\n\nThis constructor verifies the provided IndexList and initializes necessary attributes, preparing the class for executing the migration actions defined in other methods.\n\"\"\"\n```\n\n## FILE 4: curator/exceptions.py\n\n## FILE 5: curator/indexlist.py\n\n- CLASS METHOD: IndexList.__get_indices\n  - CLASS SIGNATURE: class IndexList:\n  - SIGNATURE: def __get_indices(self, pattern):\n  - DOCSTRING: \n```python\n\"\"\"\nRetrieve and populate all indices from the Elasticsearch client based on the given search pattern.\n\nParameters:\n- pattern (str): A search pattern used to fetch indices from the Elasticsearch client. Default is '_all', which retrieves all indices.\n\nThis method updates the `all_indices` attribute with all indices matching the `pattern` by calling `get_indices` from the `curator.helpers.getters` module. It then initializes the `indices` attribute as a copy of `all_indices`, making it available for further operations within the `IndexList` class. The `loggit` logger is used to document the activity, providing debugging information during the process of acquiring indices.\n\"\"\"\n```\n\n- CLASS METHOD: IndexList.__init__\n  - CLASS SIGNATURE: class IndexList:\n  - SIGNATURE: def __init__(self, client, search_pattern='_all'):\n  - DOCSTRING: \n```python\n\"\"\"\nInitialize an `IndexList` object for managing Elasticsearch indices.\n\nThis constructor takes an Elasticsearch client and an optional search pattern to populate the index list. It verifies the client object, sets up logger for debugging, and initializes three key attributes: `index_info`, which holds metadata about the indices, `indices`, the current running list of indices, and `all_indices`, a complete list of indices at initialization. The method then calls `__get_indices` to populate these attributes based on the search pattern provided.\n\nParameters:\n- `client`: An instance of the Elasticsearch client, which is validated by the `verify_client_object` function.\n- `search_pattern`: A string pattern to match indices. Defaults to `'_all'`, which includes all indices.\n\nReturns:\n- None\n\nAttributes initialized:\n- `self.index_info`: A dictionary to store metadata extracted from indices, populated during initialization.\n- `self.indices`: A list to maintain the running list of actionable indices.\n- `self.all_indices`: A list to store all indices fetched from the cluster.\n\"\"\"\n```\n\n- CLASS METHOD: IndexList.empty_list_check\n  - CLASS SIGNATURE: class IndexList:\n  - SIGNATURE: def empty_list_check(self):\n  - DOCSTRING: \n```python\n\"\"\"\nCheck if the list of indices is empty and raise an exception if it is.\n\nThis method verifies the `indices` attribute of the `IndexList` instance. If `indices` is empty, it raises a `NoIndices` exception, indicating that no indices are available for processing. This is crucial for ensuring that subsequent operations on indices do not fail due to an empty list.\n\nRaises:\n    NoIndices: If the `indices` attribute is empty.\n\nDependencies:\n    - `self.indices`: A list attribute that holds the current indices. It is populated during the initialization of the `IndexList` instance and is manipulated by various filtering methods within the class.\n\"\"\"\n```\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "curator/helpers/getters.py": "\"\"\"Utility functions that get things\"\"\"\nimport logging\nfrom elasticsearch8 import exceptions as es8exc\nfrom curator.exceptions import ConfigurationError, CuratorException, FailedExecution, MissingArgument\n\ndef byte_size(num, suffix='B'):\n    \"\"\"\n    :param num: The number of byte\n    :param suffix: An arbitrary suffix, like ``Bytes``\n\n    :type num: int\n    :type suffix: str\n\n    :returns: A formatted string indicating the size in bytes, with the proper unit,\n        e.g. KB, MB, GB, TB, etc.\n    :rtype: float\n    \"\"\"\n    for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:\n        if abs(num) < 1024.0:\n            return f'{num:3.1f}{unit}{suffix}'\n        num /= 1024.0\n    return f'{num:.1f}Y{suffix}'\n\ndef escape_dots(stringval):\n    \"\"\"\n    Escape any dots (periods) in ``stringval``.\n\n    Primarily used for ``filter_path`` where dots are indicators of path nesting\n\n    :param stringval: A string, ostensibly an index name\n\n    :type stringval: str\n\n    :returns: ``stringval``, but with any periods escaped with a backslash\n    :retval: str\n    \"\"\"\n    return stringval.replace('.', '\\\\.')\n\ndef get_alias_actions(oldidx, newidx, aliases):\n    \"\"\"\n    :param oldidx: The old index name\n    :param newidx: The new index name\n    :param aliases: The aliases\n\n    :type oldidx: str\n    :type newidx: str\n    :type aliases: dict\n\n    :returns: A list of actions suitable for\n        :py:meth:`~.elasticsearch.client.IndicesClient.update_aliases` ``actions``\n        kwarg.\n    :rtype: list\n    \"\"\"\n    actions = []\n    for alias in aliases.keys():\n        actions.append({'remove': {'index': oldidx, 'alias': alias}})\n        actions.append({'add': {'index': newidx, 'alias': alias}})\n    return actions\n\ndef get_data_tiers(client):\n    \"\"\"\n    Get all valid data tiers from the node roles of each node in the cluster by\n    polling each node\n\n    :param client: A client connection object\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n\n    :returns: The available data tiers in ``tier: bool`` form.\n    :rtype: dict\n    \"\"\"\n\n    def role_check(role, node_info):\n        if role in node_info['roles']:\n            return True\n        return False\n    info = client.nodes.info()['nodes']\n    retval = {'data_hot': False, 'data_warm': False, 'data_cold': False, 'data_frozen': False}\n    for node in info:\n        for role in ['data_hot', 'data_warm', 'data_cold', 'data_frozen']:\n            if role_check(role, info[node]):\n                retval[role] = True\n    return retval\n\ndef get_indices(client, search_pattern='_all'):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.CatClient.indices`\n\n    :param client: A client connection object\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n\n    :returns: The current list of indices from the cluster\n    :rtype: list\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    indices = []\n    try:\n        resp = client.cat.indices(index=search_pattern, expand_wildcards='open,closed', h='index,status', format='json')\n    except Exception as err:\n        raise FailedExecution(f'Failed to get indices. Error: {err}') from err\n    if not resp:\n        return indices\n    for entry in resp:\n        indices.append(entry['index'])\n    logger.debug('All indices: %s', indices)\n    return indices\n\ndef get_repository(client, repository=''):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get_repository`\n\n    :param client: A client connection object\n    :param repository: The Elasticsearch snapshot repository to use\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type repository: str\n\n    :returns: Configuration information for ``repository``.\n    :rtype: dict\n    \"\"\"\n    try:\n        return client.snapshot.get_repository(name=repository)\n    except (es8exc.TransportError, es8exc.NotFoundError) as err:\n        msg = f'Unable to get repository {repository}.  Error: {err} Check Elasticsearch logs for more information.'\n        raise CuratorException(msg) from err\n\ndef get_snapshot(client, repository=None, snapshot=''):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get`\n\n    :param client: A client connection object\n    :param repository: The Elasticsearch snapshot repository to use\n    :param snapshot: The snapshot name, or a comma-separated list of snapshots\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type repository: str\n    :type snapshot: str\n\n    :returns: Information about the provided ``snapshot``, a snapshot (or a\n        comma-separated list of snapshots). If no snapshot specified, it will\n        collect info for all snapshots.  If none exist, an empty :py:class:`dict`\n        will be returned.\n    :rtype: dict\n    \"\"\"\n    if not repository:\n        raise MissingArgument('No value for \"repository\" provided')\n    snapname = '*' if snapshot == '' else snapshot\n    try:\n        return client.snapshot.get(repository=repository, snapshot=snapshot)\n    except (es8exc.TransportError, es8exc.NotFoundError) as err:\n        msg = f'Unable to get information about snapshot {snapname} from repository: {repository}.  Error: {err}'\n        raise FailedExecution(msg) from err\n\ndef get_snapshot_data(client, repository=None):\n    \"\"\"\n    Get all snapshots from repository and return a list.\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get`\n\n    :param client: A client connection object\n    :param repository: The Elasticsearch snapshot repository to use\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type repository: str\n\n    :returns: The list of all snapshots from ``repository``\n    :rtype: list\n    \"\"\"\n    if not repository:\n        raise MissingArgument('No value for \"repository\" provided')\n    try:\n        return client.snapshot.get(repository=repository, snapshot='*')['snapshots']\n    except (es8exc.TransportError, es8exc.NotFoundError) as err:\n        msg = f'Unable to get snapshot information from repository: {repository}. Error: {err}'\n        raise FailedExecution(msg) from err\n\ndef get_write_index(client, alias):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.get_alias`\n\n    :param client: A client connection object\n    :param alias: An alias name\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type alias: str\n\n    :returns: The the index name associated with the alias that is designated\n        ``is_write_index``\n    :rtype: str\n    \"\"\"\n    try:\n        response = client.indices.get_alias(index=alias)\n    except Exception as exc:\n        raise CuratorException(f'Alias {alias} not found') from exc\n    retval = None\n    if len(list(response.keys())) > 1:\n        for index in list(response.keys()):\n            try:\n                if response[index]['aliases'][alias]['is_write_index']:\n                    retval = index\n            except KeyError as exc:\n                raise FailedExecution('Invalid alias: is_write_index not found in 1 to many alias') from exc\n    else:\n        retval = list(response.keys())[0]\n    return retval\n\ndef index_size(client, idx, value='total'):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.stats`\n\n    :param client: A client connection object\n    :param idx: An index name\n    :param value: One of either ``primaries`` or ``total``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type idx: str\n    :type value: str\n\n    :returns: The sum of either ``primaries`` or ``total`` shards for index ``idx``\n    :rtype: integer\n    \"\"\"\n    fpath = f'indices.{escape_dots(idx)}.{value}.store.size_in_bytes'\n    return client.indices.stats(index=idx, filter_path=fpath)['indices'][idx][value]['store']['size_in_bytes']\n\ndef name_to_node_id(client, name):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.NodesClient.info`\n\n    :param client: A client connection object\n    :param name: The node ``name``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type name: str\n\n    :returns: The node_id of the node identified by ``name``\n    :rtype: str\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    fpath = 'nodes'\n    info = client.nodes.info(filter_path=fpath)\n    for node in info['nodes']:\n        if info['nodes'][node]['name'] == name:\n            logger.debug('Found node_id \"%s\" for name \"%s\".', node, name)\n            return node\n    logger.error('No node_id found matching name: \"%s\"', name)\n    return None\n\ndef node_id_to_name(client, node_id):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.NodesClient.info`\n\n    :param client: A client connection object\n    :param node_id: The node ``node_id``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type node_id: str\n\n    :returns: The name of the node identified by ``node_id``\n    :rtype: str\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    fpath = f'nodes.{node_id}.name'\n    info = client.nodes.info(filter_path=fpath)\n    name = None\n    if node_id in info['nodes']:\n        name = info['nodes'][node_id]['name']\n    else:\n        logger.error('No node_id found matching: \"%s\"', node_id)\n    logger.debug('Name associated with node_id \"%s\": %s', node_id, name)\n    return name\n\ndef node_roles(client, node_id):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.NodesClient.info`\n\n    :param client: A client connection object\n    :param node_id: The node ``node_id``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type node_id: str\n\n    :returns: The list of roles assigned to the node identified by ``node_id``\n    :rtype: list\n    \"\"\"\n    fpath = f'nodes.{node_id}.roles'\n    return client.nodes.info(filter_path=fpath)['nodes'][node_id]['roles']\n\ndef single_data_path(client, node_id):\n    \"\"\"\n    In order for a shrink to work, it should be on a single filesystem, as shards\n    cannot span filesystems. Calls :py:meth:`~.elasticsearch.client.NodesClient.stats`\n\n    :param client: A client connection object\n    :param node_id: The node ``node_id``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type node_id: str\n\n    :returns: ``True`` if the node has a single filesystem, else ``False``\n    :rtype: bool\n    \"\"\"\n    fpath = f'nodes.{node_id}.fs.data'\n    response = client.nodes.stats(filter_path=fpath)\n    return len(response['nodes'][node_id]['fs']['data']) == 1",
    "curator/helpers/testers.py": "\"\"\"Utility functions that get things\"\"\"\nimport logging\nfrom voluptuous import Schema\nfrom elasticsearch8 import Elasticsearch\nfrom elasticsearch8.exceptions import NotFoundError\nfrom es_client.helpers.schemacheck import SchemaCheck\nfrom es_client.helpers.utils import prune_nones\nfrom curator.helpers.getters import get_repository, get_write_index\nfrom curator.exceptions import ConfigurationError, MissingArgument, RepositoryException, SearchableSnapshotException\nfrom curator.defaults.settings import index_filtertypes, snapshot_actions, snapshot_filtertypes\nfrom curator.validators import actions, options\nfrom curator.validators.filter_functions import validfilters\nfrom curator.helpers.utils import report_failure\n\ndef ilm_policy_check(client, alias):\n    \"\"\"Test if alias is associated with an ILM policy\n\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.get_settings`\n\n    :param client: A client connection object\n    :param alias: The alias name\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type alias: str\n    :rtype: bool\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    write_index = get_write_index(client, alias)\n    try:\n        idx_settings = client.indices.get_settings(index=write_index)\n        if 'name' in idx_settings[write_index]['settings']['index']['lifecycle']:\n            return True\n    except KeyError:\n        logger.debug('No ILM policies associated with %s', alias)\n    return False\n\ndef repository_exists(client, repository=None):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get_repository`\n\n    :param client: A client connection object\n    :param repository: The Elasticsearch snapshot repository to use\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type repository: str\n\n    :returns: ``True`` if ``repository`` exists, else ``False``\n    :rtype: bool\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    if not repository:\n        raise MissingArgument('No value for \"repository\" provided')\n    try:\n        test_result = get_repository(client, repository)\n        if repository in test_result:\n            logger.debug('Repository %s exists.', repository)\n            response = True\n        else:\n            logger.debug('Repository %s not found...', repository)\n            response = False\n    except Exception as err:\n        logger.debug('Unable to find repository \"%s\": Error: %s', repository, err)\n        response = False\n    return response\n\ndef rollable_alias(client, alias):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.get_alias`\n\n    :param client: A client connection object\n    :param alias: An Elasticsearch alias\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type alias: str\n\n\n    :returns: ``True`` or ``False`` depending on whether ``alias`` is an alias that\n        points to an index that can be used by the ``_rollover`` API.\n    :rtype: bool\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    try:\n        response = client.indices.get_alias(name=alias)\n    except NotFoundError:\n        logger.error('Alias \"%s\" not found.', alias)\n        return False\n    for idx in response:\n        if 'is_write_index' in response[idx]['aliases'][alias]:\n            if response[idx]['aliases'][alias]['is_write_index']:\n                return True\n    if len(response) > 1:\n        logger.error('\"alias\" must only reference one index, but points to %s', response)\n        return False\n    index = list(response.keys())[0]\n    rollable = False\n    if index[-2:][1].isdigit():\n        if index[-2:][0].isdigit():\n            rollable = True\n        elif index[-2:][0] == '-':\n            rollable = True\n    return rollable\n\ndef snapshot_running(client):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get_repository`\n\n    Return ``True`` if a snapshot is in progress, and ``False`` if not\n\n    :param client: A client connection object\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n\n    :rtype: bool\n    \"\"\"\n    try:\n        status = client.snapshot.status()['snapshots']\n    except Exception as exc:\n        report_failure(exc)\n    return False if not status else True\n\ndef validate_actions(data):\n    \"\"\"\n    Validate the ``actions`` configuration dictionary, as imported from actions.yml,\n    for example.\n\n    :param data: The configuration dictionary\n\n    :type data: dict\n\n    :returns: The validated and sanitized configuration dictionary.\n    :rtype: dict\n    \"\"\"\n    clean_config = {}\n    root = SchemaCheck(data, actions.root(), 'Actions File', 'root').result()\n    for action_id in root['actions']:\n        action_dict = root['actions'][action_id]\n        loc = f'Action ID \"{action_id}\"'\n        valid_structure = SchemaCheck(action_dict, actions.structure(action_dict, loc), 'structure', loc).result()\n        current_action = valid_structure['action']\n        loc = f'Action ID \"{action_id}\", action \"{current_action}\"'\n        clean_options = SchemaCheck(prune_nones(valid_structure['options']), options.get_schema(current_action), 'options', loc).result()\n        clean_config[action_id] = {'action': current_action, 'description': valid_structure['description'], 'options': clean_options}\n        if current_action == 'alias':\n            add_remove = {}\n            for k in ['add', 'remove']:\n                if k in valid_structure:\n                    current_filters = SchemaCheck(valid_structure[k]['filters'], Schema(validfilters(current_action, location=loc)), f'\"{k}\" filters', f'{loc}, \"filters\"').result()\n                    add_remove.update({k: {'filters': SchemaCheck(current_filters, Schema(validfilters(current_action, location=loc)), 'filters', f'{loc}, \"{k}\", \"filters\"').result()}})\n            clean_config[action_id].update(add_remove)\n        elif current_action in ['cluster_routing', 'create_index', 'rollover']:\n            pass\n        else:\n            valid_filters = SchemaCheck(valid_structure['filters'], Schema(validfilters(current_action, location=loc)), 'filters', f'{loc}, \"filters\"').result()\n            clean_filters = validate_filters(current_action, valid_filters)\n            clean_config[action_id].update({'filters': clean_filters})\n        if current_action == 'reindex':\n            if 'remote_filters' in valid_structure['options']:\n                valid_filters = SchemaCheck(valid_structure['options']['remote_filters'], Schema(validfilters(current_action, location=loc)), 'filters', f'{loc}, \"filters\"').result()\n                clean_remote_filters = validate_filters(current_action, valid_filters)\n                clean_config[action_id]['options'].update({'remote_filters': clean_remote_filters})\n    return {'actions': clean_config}\n\ndef validate_filters(action, myfilters):\n    \"\"\"\n    Validate that myfilters are appropriate for the action type, e.g. no\n    index filters applied to a snapshot list.\n\n    :param action: An action name\n    :param myfilters: A list of filters to test.\n\n    :type action: str\n    :type myfilters: list\n\n    :returns: Validated list of filters\n    :rtype: list\n    \"\"\"\n    if action in snapshot_actions():\n        filtertypes = snapshot_filtertypes()\n    else:\n        filtertypes = index_filtertypes()\n    for fil in myfilters:\n        if fil['filtertype'] not in filtertypes:\n            raise ConfigurationError(f'\"{fil['filtertype']}\" filtertype is not compatible with action \"{action}\"')\n    return myfilters\n\ndef verify_repository(client, repository=None):\n    \"\"\"\n    Do :py:meth:`~.elasticsearch.snapshot.verify_repository` call. If it fails, raise a\n    :py:exc:`~.curator.exceptions.RepositoryException`.\n\n    :param client: A client connection object\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :param repository: A repository name\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type repository: str\n\n    :rtype: None\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    try:\n        nodes = client.snapshot.verify_repository(name=repository)['nodes']\n        logger.debug('All nodes can write to the repository')\n        logger.debug('Nodes with verified repository access: %s', nodes)\n    except Exception as err:\n        try:\n            if err.status_code == 404:\n                msg = f'--- Repository \"{repository}\" not found. Error: {err.meta.status}, {err.error}'\n            else:\n                msg = f'--- Got a {err.meta.status} response from Elasticsearch.  Error message: {err.error}'\n        except AttributeError:\n            msg = f'--- Error message: {err}'.format()\n        report = f'Failed to verify all nodes have repository access: {msg}'\n        raise RepositoryException(report) from err\n\ndef verify_snapshot_list(test):\n    \"\"\"\n    :param test: The variable or object to test\n\n    :type test: :py:class:`~.curator.SnapshotList`\n\n    :returns: ``None`` if ``test`` is a proper\n        :py:class:`~.curator.snapshotlist.SnapshotList` object, else raise a\n        :py:class:`TypeError` exception.\n    :rtype: None\n    \"\"\"\n    from curator.snapshotlist import SnapshotList\n    logger = logging.getLogger(__name__)\n    if not isinstance(test, SnapshotList):\n        msg = f'Not a valid SnapshotList object. Type: {type(test)} was passed'\n        logger.error(msg)\n        raise TypeError(msg)",
    "curator/actions/cold2frozen.py": "\"\"\"Snapshot and Restore action classes\"\"\"\nimport logging\nfrom curator.helpers.getters import get_alias_actions, get_tier_preference, meta_getter\nfrom curator.helpers.testers import has_lifecycle_name, is_idx_partial, verify_index_list\nfrom curator.helpers.utils import report_failure\nfrom curator.exceptions import CuratorException, FailedExecution, SearchableSnapshotException\n\nclass Cold2Frozen:\n    \"\"\"Cold to Frozen Tier Searchable Snapshot Action Class\n\n    For manually migrating snapshots not associated with ILM from the cold tier to the frozen tier.\n    \"\"\"\n    DEFAULTS = {'index_settings': None, 'ignore_index_settings': ['index.refresh_interval'], 'wait_for_completion': True}\n\n    def mount_index(self, newidx, kwargs):\n        \"\"\"\n        Call :py:meth:`~.elasticsearch.client.SearchableSnapshotsClient.mount` to mount the indices\n        in :py:attr:`ilo` in the Frozen tier.\n        \"\"\"\n        try:\n            self.loggit.debug('Mounting new index %s in frozen tier...', newidx)\n            self.client.searchable_snapshots.mount(**kwargs)\n        except Exception as err:\n            report_failure(err)\n\n    def verify_mount(self, newidx):\n        \"\"\"\n        Verify that newidx is a mounted index\n        \"\"\"\n        self.loggit.debug('Verifying new index %s is mounted properly...', newidx)\n        idx_settings = self.client.indices.get(index=newidx)[newidx]\n        if is_idx_partial(idx_settings['settings']['index']):\n            self.loggit.info('Index %s is mounted for frozen tier', newidx)\n        else:\n            report_failure(SearchableSnapshotException(f'Index {newidx} not a mounted searchable snapshot'))\n\n    def update_aliases(self, current_idx, newidx, aliases):\n        \"\"\"\n        Call :py:meth:`~.elasticsearch.client.IndicesClient.update_aliases` to update each new\n        frozen index with the aliases from the old cold-tier index.\n\n        Verify aliases look good.\n        \"\"\"\n        alias_names = aliases.keys()\n        if not alias_names:\n            self.loggit.warning('No aliases associated with index %s', current_idx)\n        else:\n            self.loggit.debug('Transferring aliases to new index %s', newidx)\n            self.client.indices.update_aliases(actions=get_alias_actions(current_idx, newidx, aliases))\n            verify = self.client.indices.get(index=newidx)[newidx]['aliases'].keys()\n            if alias_names != verify:\n                self.loggit.error('Alias names do not match! %s does not match: %s', alias_names, verify)\n                report_failure(FailedExecution('Aliases failed to transfer to new index'))\n\n    def cleanup(self, current_idx, newidx):\n        \"\"\"\n        Call :py:meth:`~.elasticsearch.client.IndicesClient.delete` to delete the cold tier index.\n        \"\"\"\n        self.loggit.debug('Deleting old index: %s', current_idx)\n        try:\n            self.client.indices.delete(index=current_idx)\n        except Exception as err:\n            report_failure(err)\n        self.loggit.info('Successfully migrated %s to the frozen tier as %s', current_idx, newidx)\n\n    def do_action(self):\n        \"\"\"\n        Do the actions outlined:\n        Extract values from generated kwargs\n        Mount\n        Verify\n        Update Aliases\n        Cleanup\n        \"\"\"\n        for kwargs in self.action_generator():\n            aliases = kwargs.pop('aliases')\n            current_idx = kwargs.pop('current_idx')\n            newidx = kwargs['renamed_index']\n            self.mount_index(newidx, kwargs)\n            self.verify_mount(newidx)\n            self.update_aliases(current_idx, newidx, aliases)\n            self.cleanup(current_idx, newidx)",
    "curator/exceptions.py": "\"\"\"Curator Exceptions\"\"\"\n\nclass CuratorException(Exception):\n    \"\"\"\n    Base class for all exceptions raised by Curator which are not Elasticsearch\n    exceptions.\n    \"\"\"\n\nclass ConfigurationError(CuratorException):\n    \"\"\"\n    Exception raised when a misconfiguration is detected\n    \"\"\"\n\nclass MissingArgument(CuratorException):\n    \"\"\"\n    Exception raised when a needed argument is not passed.\n    \"\"\"\n\nclass NoIndices(CuratorException):\n    \"\"\"\n    Exception raised when an operation is attempted against an empty index_list\n    \"\"\"\n\nclass NoSnapshots(CuratorException):\n    \"\"\"\n    Exception raised when an operation is attempted against an empty snapshot_list\n    \"\"\"\n\nclass ActionError(CuratorException):\n    \"\"\"\n    Exception raised when an action (against an index_list or snapshot_list) cannot be taken.\n    \"\"\"\n\nclass FailedExecution(CuratorException):\n    \"\"\"\n    Exception raised when an action fails to execute for some reason.\n    \"\"\"\n\nclass SnapshotInProgress(ActionError):\n    \"\"\"\n    Exception raised when a snapshot is already in progress\n    \"\"\"\n\nclass ActionTimeout(CuratorException):\n    \"\"\"\n    Exception raised when an action fails to complete in the allotted time\n    \"\"\"\n\nclass FailedSnapshot(CuratorException):\n    \"\"\"\n    Exception raised when a snapshot does not complete with state SUCCESS\n    \"\"\"\n\nclass FailedRestore(CuratorException):\n    \"\"\"\n    Exception raised when a Snapshot Restore does not restore all selected indices\n    \"\"\"\n\nclass FailedReindex(CuratorException):\n    \"\"\"\n    Exception raised when failures are found in the reindex task response\n    \"\"\"\n\nclass ClientException(CuratorException):\n    \"\"\"\n    Exception raised when the Elasticsearch client and/or connection is the source of the problem.\n    \"\"\"\n\nclass LoggingException(CuratorException):\n    \"\"\"\n    Exception raised when Curator cannot either log or configure logging\n    \"\"\"\n\nclass RepositoryException(CuratorException):\n    \"\"\"\n    Exception raised when Curator cannot verify a snapshot repository\n    \"\"\"\n\nclass SearchableSnapshotException(CuratorException):\n    \"\"\"\n    Exception raised when Curator finds something out of order with a Searchable Snapshot\n    \"\"\"",
    "curator/indexlist.py": "\"\"\"Index List Class\"\"\"\nimport re\nimport itertools\nimport logging\nfrom elasticsearch8.exceptions import NotFoundError, TransportError\nfrom es_client.helpers.schemacheck import SchemaCheck\nfrom es_client.helpers.utils import ensure_list\nfrom curator.defaults import settings\nfrom curator.exceptions import ActionError, ConfigurationError, MissingArgument, NoIndices\nfrom curator.helpers.date_ops import absolute_date_range, date_range, fix_epoch, get_date_regex, get_point_of_reference, get_unit_count_from_name, TimestringSearch\nfrom curator.helpers.getters import byte_size, get_indices\nfrom curator.helpers.testers import verify_client_object\nfrom curator.helpers.utils import chunk_index_list, report_failure, to_csv\nfrom curator.validators.filter_functions import filterstructure\n\nclass IndexList:\n    \"\"\"IndexList class\"\"\"\n\n    def __actionable(self, idx):\n        self.loggit.debug('Index %s is actionable and remains in the list.', idx)\n\n    def __not_actionable(self, idx):\n        self.loggit.debug('Index %s is not actionable, removing from list.', idx)\n        self.indices.remove(idx)\n\n    def __excludify(self, condition, exclude, index, msg=None):\n        if condition is True:\n            if exclude:\n                text = 'Removed from actionable list'\n                self.__not_actionable(index)\n            else:\n                text = 'Remains in actionable list'\n                self.__actionable(index)\n        elif exclude:\n            text = 'Remains in actionable list'\n            self.__actionable(index)\n        else:\n            text = 'Removed from actionable list'\n            self.__not_actionable(index)\n        if msg:\n            self.loggit.debug('%s: %s', text, msg)\n\n    def __build_index_info(self, index):\n        \"\"\"\n        Ensure that ``index`` is a key in ``index_info``. If not, create a\n        sub-dictionary structure under that key.\n        \"\"\"\n        self.loggit.debug('Building preliminary index metadata for %s', index)\n        if index not in self.index_info:\n            self.index_info[index] = self.__zero_values()\n\n    def __map_method(self, ftype):\n        methods = {'alias': self.filter_by_alias, 'age': self.filter_by_age, 'allocated': self.filter_allocated, 'closed': self.filter_closed, 'count': self.filter_by_count, 'empty': self.filter_empty, 'forcemerged': self.filter_forceMerged, 'ilm': self.filter_ilm, 'kibana': self.filter_kibana, 'none': self.filter_none, 'opened': self.filter_opened, 'period': self.filter_period, 'pattern': self.filter_by_regex, 'space': self.filter_by_space, 'shards': self.filter_by_shards, 'size': self.filter_by_size}\n        return methods[ftype]\n\n    def __remove_missing(self, err):\n        \"\"\"\n        Remove missing index found in ``err`` from self.indices and return that name\n        \"\"\"\n        missing = err.info['error']['index']\n        self.loggit.warning('Index was initiallly present, but now is not: %s', missing)\n        self.loggit.debug('Removing %s from active IndexList', missing)\n        self.indices.remove(missing)\n        return missing\n\n    def __zero_values(self):\n        \"\"\"The default values for index metadata\"\"\"\n        return {'age': {'creation_date': 0, 'name': 0}, 'docs': 0, 'number_of_replicas': 0, 'number_of_shards': 0, 'primary_size_in_bytes': 0, 'routing': {}, 'segments': 0, 'size_in_bytes': 0, 'state': ''}\n\n    def _get_indices_segments(self, data):\n        return self.client.indices.segments(index=to_csv(data))['indices'].copy()\n\n    def _get_indices_settings(self, data):\n        return self.client.indices.get_settings(index=to_csv(data))\n\n    def _get_indices_stats(self, data):\n        return self.client.indices.stats(index=to_csv(data), metric='store,docs')['indices']\n\n    def _bulk_queries(self, data, exec_func):\n        slice_number = 10\n        query_result = {}\n        loop_number = round(len(data) / slice_number) if round(len(data) / slice_number) > 0 else 1\n        self.loggit.debug('Bulk Queries - number requests created: %s', loop_number)\n        for num in range(0, loop_number):\n            if num == loop_number - 1:\n                data_sliced = data[num * slice_number:]\n            else:\n                data_sliced = data[num * slice_number:(num + 1) * slice_number]\n            query_result.update(exec_func(data_sliced))\n        return query_result\n\n    def mitigate_alias(self, index):\n        \"\"\"\n        Mitigate when an alias is detected instead of an index name\n\n        :param index: The index name that is showing up *instead* of what was expected\n\n        :type index: str\n\n        :returns: No return value:\n        :rtype: None\n        \"\"\"\n        self.loggit.debug('BEGIN mitigate_alias')\n        self.loggit.debug('Correcting an instance where an alias name points to index \"%s\"', index)\n        data = self.client.indices.get(index=index)\n        aliases = list(data[index]['aliases'])\n        if aliases:\n            for alias in aliases:\n                if alias in self.indices:\n                    self.loggit.warning('Removing alias \"%s\" from IndexList.indices', alias)\n                    self.indices.remove(alias)\n                if alias in list(self.index_info):\n                    self.loggit.warning('Removing alias \"%s\" from IndexList.index_info', alias)\n                    del self.index_info[alias]\n        self.loggit.debug('Adding \"%s\" to IndexList.indices', index)\n        self.indices.append(index)\n        self.loggit.debug('Adding preliminary metadata for \"%s\" to IndexList.index_info', index)\n        self.__build_index_info(index)\n        self.loggit.debug('END mitigate_alias')\n\n    def alias_index_check(self, data):\n        \"\"\"\n        Check each index in data to see if it's an alias.\n        \"\"\"\n        working_list = data[:]\n        for entry in working_list:\n            if self.client.indices.exists_alias(name=entry):\n                index = list(self.client.indices.get_alias(name=entry).keys())[0]\n                self.loggit.warning('\"%s\" is actually an alias for index \"%s\"', entry, index)\n                self.mitigate_alias(index)\n                data.remove(entry)\n                data.append(index)\n        return data\n\n    def indices_exist(self, data, exec_func):\n        \"\"\"Check if indices exist. If one doesn't, remove it. Loop until all exist\"\"\"\n        self.loggit.debug('BEGIN indices_exist')\n        checking = True\n        working_list = {}\n        verified_data = self.alias_index_check(data)\n        while checking:\n            try:\n                working_list.update(exec_func(verified_data))\n            except NotFoundError as err:\n                data.remove(self.__remove_missing(err))\n                continue\n            except TransportError as err:\n                if '413' in err.errors:\n                    msg = 'Huge Payload 413 Err - Trying to get information via multiple requests'\n                    self.loggit.debug(msg)\n                    working_list.update(self._bulk_queries(verified_data, exec_func))\n            checking = False\n        return working_list\n\n    def data_getter(self, data, exec_func):\n        \"\"\"\n        Function that prevents unnecessary code repetition for different data\n        getter methods\n        \"\"\"\n        self.loggit.debug('BEGIN data_getter')\n        checking = True\n        while checking:\n            working_list = self.indices_exist(data, exec_func)\n            if working_list:\n                for index in list(working_list.keys()):\n                    try:\n                        sii = self.index_info[index]\n                    except KeyError:\n                        self.loggit.warning('Index %s was not present at IndexList initialization,  and may be behind an alias', index)\n                        self.mitigate_alias(index)\n                        sii = self.index_info[index]\n                        working_list = {}\n                        try:\n                            working_list.update(self._bulk_queries(data, exec_func))\n                        except NotFoundError as err:\n                            data.remove(self.__remove_missing(err))\n                            continue\n                    yield (sii, working_list[index], index)\n            checking = False\n\n    def population_check(self, index, key):\n        \"\"\"Verify that key is in self.index_info[index], and that it is populated\"\"\"\n        retval = True\n        if index not in self.index_info:\n            self.__build_index_info(index)\n        if key not in self.index_info[index]:\n            self.index_info[index][key] = self.__zero_values()[key]\n        if self.index_info[index][key] == self.__zero_values()[key]:\n            retval = False\n        return retval\n\n    def needs_data(self, indices, fields):\n        \"\"\"Check for data population in self.index_info\"\"\"\n        self.loggit.debug('Indices: %s, Fields: %s', indices, fields)\n        needful = []\n        working_list = self.indices_exist(indices, self._get_indices_settings)\n        for idx in working_list:\n            count = 0\n            for field in fields:\n                if self.population_check(idx, field):\n                    count += 1\n            if count == 0:\n                needful.append(idx)\n        if fields == ['state']:\n            self.loggit.debug('Always check open/close for all passed indices')\n            needful = list(working_list.keys())\n        self.loggit.debug('These indices need data in index_info: %s', needful)\n        return needful\n\n    def get_index_settings(self):\n        \"\"\"\n        For each index in self.indices, populate ``index_info`` with:\n            creation_date\n            number_of_replicas\n            number_of_shards\n            routing information (if present)\n        \"\"\"\n        self.loggit.debug('Getting index settings -- BEGIN')\n        self.empty_list_check()\n        fields = ['age', 'number_of_replicas', 'number_of_shards', 'routing']\n        for lst in chunk_index_list(self.indices):\n            needful = self.needs_data(lst, fields)\n            if not needful:\n                continue\n            for sii, wli, _ in self.data_getter(needful, self._get_indices_settings):\n                sii['age']['creation_date'] = fix_epoch(wli['settings']['index']['creation_date'])\n                sii['number_of_replicas'] = wli['settings']['index']['number_of_replicas']\n                sii['number_of_shards'] = wli['settings']['index']['number_of_shards']\n                if 'routing' in wli['settings']['index']:\n                    sii['routing'] = wli['settings']['index']['routing']\n        self.loggit.debug('Getting index settings -- END')\n\n    def get_index_state(self):\n        \"\"\"\n        For each index in self.indices, populate ``index_info`` with:\n\n            state (open or closed)\n\n        from the cat API\n        \"\"\"\n        self.loggit.debug('Getting index state -- BEGIN')\n        self.empty_list_check()\n        fields = ['state']\n        for lst in chunk_index_list(self.indices):\n            needful = self.needs_data(lst, fields)\n            resp = self.client.cat.indices(index=to_csv(needful), format='json', h='index,status')\n            for entry in resp:\n                try:\n                    self.index_info[entry['index']]['state'] = entry['status']\n                except KeyError:\n                    self.loggit.warning('Index %s was not present at IndexList initialization, and may be behind an alias', entry['index'])\n                    self.mitigate_alias(entry['index'])\n                    self.index_info[entry['index']]['state'] = entry['status']\n\n    def get_index_stats(self):\n        \"\"\"\n        Populate ``index_info`` with index ``size_in_bytes``,\n        ``primary_size_in_bytes`` and doc count information for each index.\n        \"\"\"\n        self.loggit.debug('Getting index stats -- BEGIN')\n        self.empty_list_check()\n        fields = ['size_in_bytes', 'docs', 'primary_size_in_bytes']\n        self.get_index_state()\n        working_list = self.working_list()\n        for index in self.working_list():\n            if self.index_info[index]['state'] == 'close':\n                working_list.remove(index)\n        if working_list:\n            index_lists = chunk_index_list(working_list)\n            for lst in index_lists:\n                needful = self.needs_data(lst, fields)\n                if not needful:\n                    continue\n                for sii, wli, index in self.data_getter(needful, self._get_indices_stats):\n                    try:\n                        size = wli['total']['store']['size_in_bytes']\n                        docs = wli['total']['docs']['count']\n                        primary_size = wli['primaries']['store']['size_in_bytes']\n                        msg = f'Index: {index}  Size: {byte_size(size)}  Docs: {docs} PrimarySize: {byte_size(primary_size)}'\n                        self.loggit.debug(msg)\n                        sii['size_in_bytes'] = size\n                        sii['docs'] = docs\n                        sii['primary_size_in_bytes'] = primary_size\n                    except KeyError:\n                        msg = f'Index stats missing for \"{index}\" -- might be closed'\n                        self.loggit.warning(msg)\n\n    def get_segment_counts(self):\n        \"\"\"\n        Populate ``index_info`` with segment information for each index.\n        \"\"\"\n        self.loggit.debug('Getting index segment counts')\n        self.empty_list_check()\n        for lst in chunk_index_list(self.indices):\n            for sii, wli, _ in self.data_getter(lst, self._get_indices_segments):\n                shards = wli['shards']\n                segmentcount = 0\n                for shardnum in shards:\n                    for shard in range(0, len(shards[shardnum])):\n                        segmentcount += shards[shardnum][shard]['num_search_segments']\n                sii['segments'] = segmentcount\n\n    def working_list(self):\n        \"\"\"\n        Return the current value of ``indices`` as copy-by-value to prevent list\n        stomping during iterations\n        \"\"\"\n        self.loggit.debug('Generating working list of indices')\n        return self.indices[:]\n\n    def _get_name_based_ages(self, timestring):\n        \"\"\"\n        Add indices to ``index_info`` based on the age as indicated by the index\n        name pattern, if it matches ``timestring``\n\n        :param timestring: An :py:func:`time.strftime` pattern\n        \"\"\"\n        self.loggit.debug('Getting ages of indices by \"name\"')\n        self.empty_list_check()\n        tstr = TimestringSearch(timestring)\n        for index in self.working_list():\n            epoch = tstr.get_epoch(index)\n            if isinstance(epoch, int):\n                self.index_info[index]['age']['name'] = epoch\n            else:\n                msg = f'Timestring {timestring} was not found in index {index}. Removing from actionable list'\n                self.loggit.debug(msg)\n                self.indices.remove(index)\n\n    def _get_field_stats_dates(self, field='@timestamp'):\n        \"\"\"\n        Add indices to ``index_info`` based on the values the queries return, as\n        determined by the min and max aggregated values of ``field``\n\n        :param field: The field with the date value.  The field must be mapped in\n            elasticsearch as a date datatype. Default: ``@timestamp``\n        \"\"\"\n        self.loggit.debug('Cannot query closed indices. Omitting any closed indices.')\n        self.filter_closed()\n        self.loggit.debug('Cannot use field_stats with empty indices. Omitting any empty indices.')\n        self.filter_empty()\n        self.loggit.debug('Getting index date by querying indices for min & max value of %s field', field)\n        self.empty_list_check()\n        index_lists = chunk_index_list(self.indices)\n        for lst in index_lists:\n            for index in lst:\n                aggs = {'min': {'min': {'field': field}}, 'max': {'max': {'field': field}}}\n                response = self.client.search(index=index, size=0, aggs=aggs)\n                self.loggit.debug('RESPONSE: %s', response)\n                if response:\n                    try:\n                        res = response['aggregations']\n                        self.loggit.debug('res: %s', res)\n                        data = self.index_info[index]['age']\n                        data['min_value'] = fix_epoch(res['min']['value'])\n                        data['max_value'] = fix_epoch(res['max']['value'])\n                        self.loggit.debug('data: %s', data)\n                    except KeyError as exc:\n                        raise ActionError(f'Field \"{field}\" not found in index \"{index}\"') from exc\n\n    def _calculate_ages(self, source=None, timestring=None, field=None, stats_result=None):\n        \"\"\"\n        This method initiates index age calculation based on the given parameters.\n        Exceptions are raised when they are improperly configured.\n\n        Set instance variable ``age_keyfield`` for use later, if needed.\n\n        :param source: Source of index age. Can be: ``name``, ``creation_date``,\n            or ``field_stats``\n        :param timestring: An :py:func:`time.strftime` string to match the datestamp\n            in an index name. Only used for index filtering by ``name``.\n        :param field: A timestamp field name.  Only used for ``field_stats`` based\n            calculations.\n        :param stats_result: Either ``min_value`` or ``max_value``.  Only used\n            in conjunction with ``source=field_stats`` to choose whether to reference\n            the min or max result value.\n        \"\"\"\n        self.age_keyfield = source\n        if source == 'name':\n            if not timestring:\n                raise MissingArgument('source \"name\" requires the \"timestring\" keyword argument')\n            self._get_name_based_ages(timestring)\n        elif source == 'creation_date':\n            pass\n        elif source == 'field_stats':\n            if not field:\n                raise MissingArgument('source \"field_stats\" requires the \"field\" keyword argument')\n            if stats_result not in ['min_value', 'max_value']:\n                raise ValueError(f'Invalid value for \"stats_result\": {stats_result}')\n            self.age_keyfield = stats_result\n            self._get_field_stats_dates(field=field)\n        else:\n            raise ValueError(f'Invalid source: {source}. Must be one of \"name\", \"creation_date\", \"field_stats\".')\n\n    def _sort_by_age(self, index_list, reverse=True):\n        \"\"\"\n        Take a list of indices and sort them by date.\n\n        By default, the youngest are first with ``reverse=True``, but the oldest\n        can be first by setting ``reverse=False``\n        \"\"\"\n        temp = {}\n        for index in index_list:\n            try:\n                if self.index_info[index]['age'][self.age_keyfield]:\n                    temp[index] = self.index_info[index]['age'][self.age_keyfield]\n                else:\n                    msg = f'No date for \"{index}\" in IndexList metadata. Possible timestring mismatch. Excluding index \"{index}\".'\n                    self.__excludify(True, True, index, msg)\n            except KeyError:\n                msg = f'{index} does not have key \"{self.age_keyfield}\" in IndexList metadata'\n                self.__excludify(True, True, index, msg)\n        temp_tuple = sorted(temp.items(), key=lambda k: k[0], reverse=reverse)\n        sorted_tuple = sorted(temp_tuple, key=lambda k: k[1], reverse=reverse)\n        return [x[0] for x in sorted_tuple]\n\n    def filter_by_regex(self, kind=None, value=None, exclude=False):\n        \"\"\"\n        Match indices by regular expression (pattern).\n\n        :param kind: Can be one of: ``suffix``, ``prefix``, ``regex``, or\n            ``timestring``. This option defines what ``kind`` of filter you will\n            be building.\n        :param value: Depends on ``kind``. It is the :py:func:`time.strftime` string if\n            ``kind`` is ``timestring``. It's used to build the regular expression\n            for other kinds.\n        :param exclude: If ``exclude=True``, this filter will remove matching\n            indices from ``indices``. If ``exclude=False``, then only matching\n            indices will be kept in ``indices``. Default is ``False``\n        \"\"\"\n        self.loggit.debug('Filtering indices by regex')\n        if kind not in ['regex', 'prefix', 'suffix', 'timestring']:\n            raise ValueError(f'{kind}: Invalid value for kind')\n        if value == 0:\n            pass\n        elif not value:\n            raise ValueError('Invalid None value for \"value\". Cannot be \"None\" type, empty, or False')\n        if kind == 'timestring':\n            regex = settings.regex_map()[kind].format(get_date_regex(value))\n        else:\n            regex = settings.regex_map()[kind].format(value)\n        self.empty_list_check()\n        pattern = re.compile(regex)\n        for index in self.working_list():\n            self.loggit.debug('Filter by regex: Index: %s', index)\n            match = pattern.search(index)\n            if match:\n                self.__excludify(True, exclude, index)\n            else:\n                self.__excludify(False, exclude, index)\n\n    def filter_by_age(self, source='name', direction=None, timestring=None, unit=None, unit_count=None, field=None, stats_result='min_value', epoch=None, exclude=False, unit_count_pattern=False):\n        \"\"\"\n        Match indices by relative age calculations.\n\n        :param source: Source of index age. Can be one of ``name``, ``creation_date``,\n            or ``field_stats``\n        :param direction: Time to filter, either ``older`` or ``younger``\n        :param timestring: An :py:func:`time.strftime` string to match the datestamp\n            in an index name. Only used for index filtering by ``name``.\n        :param unit: One of ``seconds``, ``minutes``, ``hours``, ``days``, ``weeks``,\n            ``months``, or ``years``.\n        :param unit_count: The count of ``unit``. ``unit_count`` * ``unit`` will\n            be calculated out to the relative number of seconds.\n        :param unit_count_pattern: A regular expression whose capture group identifies\n            the value for ``unit_count``.\n        :param field: A timestamp field name.  Only used for ``field_stats`` based\n            calculations.\n        :param stats_result: Either ``min_value`` or ``max_value``.  Only used\n            in conjunction with ``source=field_stats`` to choose whether to reference\n            the minimum or maximum result value.\n        :param epoch: An epoch timestamp used in conjunction with ``unit`` and\n            ``unit_count`` to establish a point of reference for calculations.\n            If not provided, the current time will be used.\n        :param exclude: If ``exclude=True``, this filter will remove matching\n            indices from ``indices``. If ``exclude`` is `False`, then only matching\n            indices will be kept in ``indices``. Default is ``False``\n        \"\"\"\n        self.loggit.debug('Filtering indices by age')\n        por = get_point_of_reference(unit, unit_count, epoch)\n        if not direction:\n            raise MissingArgument('Must provide a value for \"direction\"')\n        if direction not in ['older', 'younger']:\n            raise ValueError(f'Invalid value for \"direction\": {direction}')\n        self.get_index_settings()\n        self._calculate_ages(source=source, timestring=timestring, field=field, stats_result=stats_result)\n        if unit_count_pattern:\n            try:\n                unit_count_matcher = re.compile(unit_count_pattern)\n            except Exception as exc:\n                self.loggit.error('Regular expression failure. Will not match unit count. Error: %s', exc)\n                unit_count_matcher = None\n        for index in self.working_list():\n            try:\n                remove_this_index = False\n                age = int(self.index_info[index]['age'][self.age_keyfield])\n                msg = f'Index \"{index}\" age ({age}), direction: \"{direction}\", point of reference, ({por})'\n                if unit_count_pattern:\n                    msg = f'unit_count_pattern is set, trying to match pattern to index \"{index}\"'\n                    self.loggit.debug(msg)\n                    unit_count_from_index = get_unit_count_from_name(index, unit_count_matcher)\n                    if unit_count_from_index:\n                        self.loggit.debug('Pattern matched, applying unit_count of  \"%s\"', unit_count_from_index)\n                        adjustedpor = get_point_of_reference(unit, unit_count_from_index, epoch)\n                        msg = f'Adjusting point of reference from {por} to {adjustedpor} based on unit_count of {unit_count_from_index} from index name'\n                        self.loggit.debug(msg)\n                    elif unit_count == -1:\n                        msg = f'Unable to match pattern and no fallback value set. Removing index \"{index}\" from actionable list'\n                        self.loggit.debug(msg)\n                        remove_this_index = True\n                        adjustedpor = por\n                    else:\n                        self.loggit.debug('Unable to match pattern using fallback value of \"%s\"', unit_count)\n                        adjustedpor = por\n                else:\n                    adjustedpor = por\n                if direction == 'older':\n                    agetest = age < adjustedpor\n                else:\n                    agetest = age > adjustedpor\n                self.__excludify(agetest and (not remove_this_index), exclude, index, msg)\n            except KeyError:\n                msg = f'Index \"{index}\" does not meet provided criteria. Removing from list.'\n                self.loggit.debug(msg)\n                self.indices.remove(index)\n\n    def filter_by_space(self, disk_space=None, reverse=True, use_age=False, source='creation_date', timestring=None, field=None, stats_result='min_value', exclude=False, threshold_behavior='greater_than'):\n        \"\"\"\n        Remove indices from the actionable list based on space consumed, sorted\n        reverse-alphabetically by default.  If you set ``reverse`` to ``False``,\n        it will be sorted alphabetically.\n\n        The default is usually what you will want. If only one kind of index is\n        provided--for example, indices matching ``logstash-%Y.%m.%d`` --then\n        reverse alphabetical sorting will mean the oldest will remain in the list,\n        because lower numbers in the dates mean older indices.\n\n        By setting ``reverse`` to ``False``, then ``index3`` will be deleted before\n        ``index2``, which will be deleted before ``index1``\n\n        ``use_age`` allows ordering indices by age. Age is determined by the\n        index creation date by default, but you can specify an ``source`` of\n        ``name``, ``max_value``, or ``min_value``. The ``name`` ``source`` requires\n        the timestring argument.\n\n        ``threshold_behavior``, when set to ``greater_than`` (default), includes\n        if it the index tests to be larger than ``disk_space``. When set to\n        ``less_than``, it includes if the index is smaller than ``disk_space``\n\n        :param disk_space: Filter indices over *n* gigabytes\n        :param threshold_behavior: Size to filter, either ``greater_than`` or\n            ``less_than``. Defaults to ``greater_than`` to preserve backwards\n            compatability.\n        :param reverse: The filtering direction. (default: ``True``).  Ignored if\n            ``use_age`` is ``True``\n        :param use_age: Sort indices by age.  ``source`` is required in this case.\n        :param source: Source of index age. Can be one of ``name``, ``creation_date``,\n            or ``field_stats``. Default: ``creation_date``\n        :param timestring: An :py:func:`time.strftime` string to match the datestamp\n            in an index name. Only used if ``source=name`` is selected.\n        :param field: A timestamp field name.  Only used if ``source=field_stats``\n            is selected.\n        :param stats_result: Either ``min_value`` or ``max_value``.  Only used if\n            ``source=field_stats`` is selected. It determines whether to reference\n            the minimum or maximum value of `field` in each index.\n        :param exclude: If ``exclude=True``, this filter will remove matching\n            indices from ``indices``. If ``exclude=False``, then only matching\n            indices will be kept in ``indices``. Default is ``False``\n        \"\"\"\n        self.loggit.debug('Filtering indices by disk space')\n        if not disk_space:\n            raise MissingArgument('No value for \"disk_space\" provided')\n        if threshold_behavior not in ['greater_than', 'less_than']:\n            raise ValueError(f'Invalid value for \"threshold_behavior\": {threshold_behavior}')\n        self.get_index_stats()\n        self.get_index_settings()\n        disk_space = float(disk_space)\n        disk_usage = 0.0\n        disk_limit = disk_space * 2 ** 30\n        msg = 'Cannot get disk usage info from closed indices. Omitting any closed indices.'\n        self.loggit.debug(msg)\n        self.filter_closed()\n        if use_age:\n            self._calculate_ages(source=source, timestring=timestring, field=field, stats_result=stats_result)\n            self.loggit.debug('SORTING BY AGE')\n            sorted_indices = self._sort_by_age(self.working_list())\n        else:\n            sorted_indices = sorted(self.working_list(), reverse=reverse)\n        for index in sorted_indices:\n            disk_usage += self.index_info[index]['size_in_bytes']\n            msg = f'{index}, summed disk usage is {byte_size(disk_usage)} and disk limit is {byte_size(disk_limit)}.'\n            if threshold_behavior == 'greater_than':\n                self.__excludify(disk_usage > disk_limit, exclude, index, msg)\n            elif threshold_behavior == 'less_than':\n                self.__excludify(disk_usage < disk_limit, exclude, index, msg)\n\n    def filter_kibana(self, exclude=True):\n        \"\"\"\n        Match any index named ``.kibana*`` in ``indices``. Older releases addressed\n        index names that no longer exist.\n\n        :param exclude: If ``exclude=True``, this filter will remove matching\n            indices from ``indices``. If ``exclude=False``, then only matching\n            indices will be kept in ``indices``. Default is ``True``\n        \"\"\"\n        self.loggit.debug('Filtering kibana indices')\n        self.empty_list_check()\n        for index in self.working_list():\n            pattern = re.compile('^\\\\.kibana.*$')\n            if pattern.match(index):\n                self.__excludify(True, exclude, index)\n            else:\n                self.__excludify(False, exclude, index)\n\n    def filter_forceMerged(self, max_num_segments=None, exclude=True):\n        \"\"\"\n        Match any index which has ``max_num_segments`` per shard or fewer in the\n        actionable list.\n\n        :param max_num_segments: Cutoff number of segments per shard.\n        :param exclude: If ``exclude=True``, this filter will remove matching\n            indices from ``indices``. If ``exclude=False``, then only matching\n            indices will be kept in ``indices``. Default is ``True``\n        \"\"\"\n        self.loggit.debug('Filtering forceMerged indices')\n        if not max_num_segments:\n            raise MissingArgument('Missing value for \"max_num_segments\"')\n        self.loggit.debug('Cannot get segment count of closed indices. Omitting any closed indices.')\n        self.get_index_state()\n        self.get_index_settings()\n        self.filter_closed()\n        self.get_segment_counts()\n        for index in self.working_list():\n            shards = int(self.index_info[index]['number_of_shards'])\n            replicas = int(self.index_info[index]['number_of_replicas'])\n            segments = int(self.index_info[index]['segments'])\n            msg = f'{index} has {shards} shard(s) + {replicas} replica(s) with a sum total of {segments} segments.'\n            expected_count = (shards + shards * replicas) * max_num_segments\n            self.__excludify(segments <= expected_count, exclude, index, msg)\n\n    def filter_closed(self, exclude=True):\n        \"\"\"\n        Filter out closed indices from ``indices``\n\n        :param exclude: If ``exclude=True``, this filter will remove matching\n            indices from ``indices``. If ``exclude=False``, then only matching\n            indices will be kept in ``indices``. Default is ``True``\n        \"\"\"\n        self.loggit.debug('Filtering closed indices')\n        self.get_index_state()\n        self.empty_list_check()\n        for index in self.working_list():\n            condition = self.index_info[index]['state'] == 'close'\n            self.loggit.debug('Index %s state: %s', index, self.index_info[index]['state'])\n            self.__excludify(condition, exclude, index)\n\n    def filter_empty(self, exclude=True):\n        \"\"\"\n        Filter indices with a document count of zero. Indices that are closed\n        are automatically excluded from consideration due to closed indices reporting\n        a document count of zero.\n\n        :param exclude: If ``exclude=True``, this filter will remove matching indices\n            from ``indices``. If ``exclude=False``, then only matching indices\n            will be kept in ``indices``. Default is ``True``\n        \"\"\"\n        self.loggit.debug('Filtering empty indices')\n        self.get_index_state()\n        self.get_index_stats()\n        self.filter_closed()\n        self.empty_list_check()\n        for index in self.working_list():\n            condition = self.index_info[index]['docs'] == 0\n            self.loggit.debug('Index %s doc count: %s', index, self.index_info[index]['docs'])\n            self.__excludify(condition, exclude, index)\n\n    def filter_opened(self, exclude=True):\n        \"\"\"\n        Filter out opened indices from ``indices``\n\n        :param exclude: If ``exclude=True``, this filter will remove matching indices\n            from ``indices``. If ``exclude=False``, then only matching indices\n            will be kept in ``indices``. Default is ``True``\n        \"\"\"\n        self.loggit.debug('Filtering open indices')\n        self.get_index_state()\n        self.empty_list_check()\n        for index in self.working_list():\n            condition = self.index_info[index]['state'] == 'open'\n            self.loggit.debug('Index %s state: %s', index, self.index_info[index]['state'])\n            self.__excludify(condition, exclude, index)\n\n    def filter_allocated(self, key=None, value=None, allocation_type='require', exclude=True):\n        \"\"\"\n        Match indices that have the routing allocation rule of ``key=value`` from\n        ``indices``\n\n        :param key: The allocation attribute to check for\n        :param value: The value to check for\n        :param allocation_type: Type of allocation to apply\n        :param exclude: If ``exclude=True``, this filter will remove matching indices\n            from ``indices``. If ``exclude=False``, then only matching indices\n            will be kept in ``indices``. Default is `T`rue`\n        \"\"\"\n        self.loggit.debug('Filtering indices with shard routing allocation rules')\n        if not key:\n            raise MissingArgument('No value for \"key\" provided')\n        if not value:\n            raise MissingArgument('No value for \"value\" provided')\n        if allocation_type not in ['include', 'exclude', 'require']:\n            raise ValueError(f'Invalid \"allocation_type\": {allocation_type}')\n        self.get_index_settings()\n        self.get_index_state()\n        self.empty_list_check()\n        for lst in chunk_index_list(self.indices):\n            working_list = self._get_indices_settings(lst)\n            if working_list:\n                for index in list(working_list.keys()):\n                    try:\n                        has_routing = working_list[index]['settings']['index']['routing']['allocation'][allocation_type][key] == value\n                    except KeyError:\n                        has_routing = False\n                    msg = f'{index}: Routing (mis)match: index.routing.allocation.{allocation_type}.{key}={value}.'\n                    self.__excludify(has_routing, exclude, index, msg)\n\n    def filter_none(self):\n        \"\"\"The legendary NULL filter\"\"\"\n        self.loggit.debug('\"None\" filter selected.  No filtering will be done.')\n\n    def filter_by_alias(self, aliases=None, exclude=False):\n        \"\"\"\n        Match indices which are associated with the alias or list of aliases\n        identified by ``aliases``. Indices must appear in all aliases in list\n        ``aliases`` or a 404 error will result, leading to no indices being matched.\n\n        :param aliases: A list of alias names.\n        :type aliases: list\n        :param exclude: If ``exclude=True``, this filter will remove matching indices\n            from ``indices``. If ``exclude=False``, then only matching indices\n            will be kept in ``indices``. Default is ``False``\n        \"\"\"\n        self.loggit.debug('Filtering indices matching aliases: \"%s\"', aliases)\n        if not aliases:\n            raise MissingArgument('No value for \"aliases\" provided')\n        aliases = ensure_list(aliases)\n        self.empty_list_check()\n        for lst in chunk_index_list(self.indices):\n            try:\n                has_alias = list(self.client.indices.get_alias(index=to_csv(lst), name=to_csv(aliases)).keys())\n                self.loggit.debug('has_alias: %s', has_alias)\n            except NotFoundError:\n                has_alias = []\n            for index in lst:\n                if index in has_alias:\n                    isness = 'is'\n                    condition = True\n                else:\n                    isness = 'is not'\n                    condition = False\n                msg = f'{index} {isness} associated with aliases: {aliases}'\n                self.__excludify(condition, exclude, index, msg)\n\n    def filter_by_count(self, count=None, reverse=True, use_age=False, pattern=None, source='creation_date', timestring=None, field=None, stats_result='min_value', exclude=True):\n        \"\"\"\n        Remove indices from the actionable list beyond the number ``count``, sorted\n        reverse-alphabetically by default.  If you set ``reverse=False``, it will\n        be sorted alphabetically.\n\n        The default is usually what you will want. If only one kind of index is\n        provided--for example, indices matching ``logstash-%Y.%m.%d`` -- then\n        reverse alphabetical sorting will mean the oldest will remain in the list,\n        because lower numbers in the dates mean older indices.\n\n        By setting ``reverse=False``, then ``index3`` will be deleted before\n        ``index2``, which will be deleted before ``index1``\n\n        ``use_age`` allows ordering indices by age. Age is determined by the index\n        creation date by default, but you can specify an ``source`` of ``name``,\n        ``max_value``, or ``min_value``. The ``name`` `source` requires the\n        timestring argument.\n\n        :param count: Filter indices beyond ``count``.\n        :param reverse: The filtering direction. (default: ``True``).\n        :param use_age: Sort indices by age.  ``source`` is required in this case.\n        :param pattern: Select indices to count from a regular expression pattern.\n            This pattern must have one and only one capture group. This can allow\n            a single ``count`` filter instance to operate against any number of\n            matching patterns, and keep ``count`` of each index in that group.\n            For example, given a ``pattern`` of ``'^(.*)-\\\\d{6}$'``, it will match\n            both ``rollover-000001`` and ``index-999990``, but not\n            ``logstash-2017.10.12``. Following the same example, if my cluster\n            also had ``rollover-000002`` through ``rollover-000010`` and\n            ``index-888888`` through ``index-999999``, it will process both groups\n            of indices, and include or exclude the ``count`` of each.\n        :param source: Source of index age. Can be one of ``name``,\n            ``creation_date``, or ``field_stats``. Default: ``creation_date``\n        :param timestring: An :py:func:`time.strftime` string to match the datestamp\n            in an index name. Only used if ``source=name``.\n        :param field: A timestamp field name.  Only used if ``source=field_stats``.\n        :param stats_result: Either ``min_value`` or ``max_value``.  Only used if\n            ``source=field_stats``. It determines whether to reference the minimum\n            or maximum value of ``field`` in each index.\n        :param exclude: If ``exclude=True``, this filter will remove matching indices\n            from ``indices``. If ``exclude=False``, then only matching indices\n            will be kept in ``indices``. Default is ``True``\n        \"\"\"\n        self.loggit.debug('Filtering indices by count')\n        if not count:\n            raise MissingArgument('No value for \"count\" provided')\n        self.get_index_state()\n        self.get_index_settings()\n        working_list = self.working_list()\n        if pattern:\n            try:\n                regex = re.compile(pattern)\n                if regex.groups < 1:\n                    raise ConfigurationError(f'No regular expression group found in {pattern}')\n                if regex.groups > 1:\n                    raise ConfigurationError(f'More than 1 regular expression group found in {pattern}')\n                prune_these = list(filter(lambda x: regex.match(x) is None, working_list))\n                filtered_indices = working_list\n                for index in prune_these:\n                    msg = '{index} does not match regular expression {pattern}.'\n                    condition = True\n                    exclude = True\n                    self.__excludify(condition, exclude, index, msg)\n                    filtered_indices.remove(index)\n                presorted = sorted(filtered_indices, key=lambda x: regex.match(x).group(1))\n            except Exception as exc:\n                raise ActionError(f'Unable to process pattern: \"{pattern}\". Error: {exc}') from exc\n            groups = []\n            for _, g in itertools.groupby(presorted, key=lambda x: regex.match(x).group(1)):\n                groups.append(list(g))\n        else:\n            groups = [working_list]\n        for group in groups:\n            if use_age:\n                if source != 'name':\n                    self.loggit.warning('Cannot get age information from closed indices unless source=\"name\".  Omitting any closed indices.')\n                    self.filter_closed()\n                self._calculate_ages(source=source, timestring=timestring, field=field, stats_result=stats_result)\n                sorted_indices = self._sort_by_age(group, reverse=reverse)\n            else:\n                sorted_indices = sorted(group, reverse=reverse)\n            idx = 1\n            for index in sorted_indices:\n                msg = f'{index} is {idx} of specified count of {count}.'\n                condition = True if idx <= count else False\n                self.__excludify(condition, exclude, index, msg)\n                idx += 1\n\n    def filter_by_shards(self, number_of_shards=None, shard_filter_behavior='greater_than', exclude=False):\n        \"\"\"\n        Match ``indices`` with a given shard count.\n\n        Selects all indices with a shard count ``greater_than`` ``number_of_shards``\n        by default. Use ``shard_filter_behavior`` to select indices with shard\n        count ``greater_than``, ``greater_than_or_equal``, ``less_than``,\n        ``less_than_or_equal``, or ``equal`` to ``number_of_shards``.\n\n        :param number_of_shards: shard threshold\n        :param shard_filter_behavior: Do you want to filter on ``greater_than``,\n            ``greater_than_or_equal``, ``less_than``, ``less_than_or_equal``,\n            or ``equal``?\n        :param exclude: If ``exclude=True``, this filter will remove matching indices\n            from ``indices``. If ``exclude=False``, then only matching indices\n            will be kept in ``indices``. Default is ``False``\n        \"\"\"\n        self.loggit.debug('Filtering indices by number of shards')\n        if not number_of_shards:\n            raise MissingArgument('No value for \"number_of_shards\" provided')\n        if shard_filter_behavior not in ['greater_than', 'less_than', 'greater_than_or_equal', 'less_than_or_equal', 'equal']:\n            raise ValueError(f'Invalid value for \"shard_filter_behavior\": {shard_filter_behavior}')\n        if number_of_shards < 1 or (shard_filter_behavior == 'less_than' and number_of_shards == 1):\n            raise ValueError(f'Unacceptable value: {number_of_shards} -- \"number_of_shards\" cannot be less than 1. A valid index will have at least one shard.')\n        self.get_index_settings()\n        self.empty_list_check()\n        for index in self.working_list():\n            self.loggit.debug('Filter by number of shards: Index: %s', index)\n            if shard_filter_behavior == 'greater_than':\n                condition = int(self.index_info[index]['number_of_shards']) > number_of_shards\n            elif shard_filter_behavior == 'less_than':\n                condition = int(self.index_info[index]['number_of_shards']) < number_of_shards\n            elif shard_filter_behavior == 'greater_than_or_equal':\n                condition = int(self.index_info[index]['number_of_shards']) >= number_of_shards\n            elif shard_filter_behavior == 'less_than_or_equal':\n                condition = int(self.index_info[index]['number_of_shards']) <= number_of_shards\n            else:\n                condition = int(self.index_info[index]['number_of_shards']) == number_of_shards\n            self.__excludify(condition, exclude, index)\n\n    def filter_period(self, period_type='relative', source='name', range_from=None, range_to=None, date_from=None, date_to=None, date_from_format=None, date_to_format=None, timestring=None, unit=None, field=None, stats_result='min_value', intersect=False, week_starts_on='sunday', epoch=None, exclude=False):\n        \"\"\"\n        Match ``indices`` with ages within a given period.\n\n        :param period_type: Can be either ``absolute`` or ``relative``.  Default\n            is ``relative``. ``date_from`` and ``date_to`` are required when using\n            ``period_type='absolute'``. ``range_from`` and ``range_to`` are required\n            with ``period_type='relative'``.\n        :param source: Source of index age. Can be ``name``, ``creation_date``,\n            or ``field_stats``\n        :param range_from: How many ``unit`` (s) in the past/future is the origin?\n        :param range_to: How many ``unit`` (s) in the past/future is the end point?\n        :param date_from: The simplified date for the start of the range\n        :param date_to: The simplified date for the end of the range.  If this\n            value is the same as ``date_from``, the full value of ``unit`` will be\n            extrapolated for the range.  For example, if ``unit=months``, and\n            ``date_from`` and ``date_to`` are both ``2017.01``, then the entire\n            month of January 2017 will be the absolute date range.\n        :param date_from_format: The :py:func:`time.strftime` string used to\n            parse ``date_from``\n        :param date_to_format: The :py:func:`time.strftime` string used to\n            parse ``date_to``\n        :param timestring: An :py:func:`time.strftime` string to match the datestamp\n            in an index name. Only used for index filtering by ``name``.\n        :param unit: One of ``hours``, ``days``, ``weeks``, ``months``, or ``years``.\n        :param field: A timestamp field name.  Only used for ``field_stats`` based\n            calculations.\n        :param stats_result: Either ``min_value`` or ``max_value``.  Only used in\n            conjunction with ``source='field_stats'`` to choose whether to reference\n            the min or max result value.\n        :param intersect: Only used when ``source='field_stats'``. If ``True``,\n            only indices where both ``min_value`` and ``max_value`` are within the\n            period will be selected. If ``False``, it will use whichever you specified.\n            Default is ``False`` to preserve expected behavior.\n        :param week_starts_on: Either ``sunday`` or ``monday``. Default is ``sunday``\n        :param epoch: An epoch timestamp used to establish a point of reference for\n            calculations. If not provided, the current time will be used.\n        :param exclude: If ``exclude=True``, this filter will remove matching indices\n            from ``indices``. If ``exclude=False``, then only matching indices\n            will be kept in ``indices``. Default is ``False``\n        \"\"\"\n        self.loggit.debug('Filtering indices by period')\n        if period_type not in ['absolute', 'relative']:\n            raise ValueError(f'Unacceptable value: {period_type} -- \"period_type\" must be either \"absolute\" or \"relative\".')\n        if period_type == 'relative':\n            func = date_range\n            args = [unit, range_from, range_to, epoch]\n            kwgs = {'week_starts_on': week_starts_on}\n            if not isinstance(range_from, int) or not isinstance(range_to, int):\n                raise ConfigurationError('\"range_from\" and \"range_to\" must be integer values')\n        else:\n            func = absolute_date_range\n            args = [unit, date_from, date_to]\n            kwgs = {'date_from_format': date_from_format, 'date_to_format': date_to_format}\n            for reqd in [date_from, date_to, date_from_format, date_to_format]:\n                if not reqd:\n                    raise ConfigurationError('Must provide \"date_from\", \"date_to\", \"date_from_format\", and \"date_to_format\" with absolute period_type')\n        self.get_index_settings()\n        try:\n            start, end = func(*args, **kwgs)\n        except Exception as exc:\n            report_failure(exc)\n        self._calculate_ages(source=source, timestring=timestring, field=field, stats_result=stats_result)\n        for index in self.working_list():\n            try:\n                if source == 'field_stats' and intersect:\n                    min_age = int(self.index_info[index]['age']['min_value'])\n                    max_age = int(self.index_info[index]['age']['max_value'])\n                    msg = f'Index \"{index}\", timestamp field \"{field}\", min_value ({min_age}), max_value ({max_age}), period start: \"{start}\", period end, \"{end}\"'\n                    inrange = min_age >= start and max_age <= end\n                else:\n                    age = int(self.index_info[index]['age'][self.age_keyfield])\n                    msg = f'Index \"{index}\" age ({age}), period start: \"{start}\", period end, \"{end}\"'\n                    inrange = age >= start and age <= end\n                self.__excludify(inrange, exclude, index, msg)\n            except KeyError:\n                self.loggit.debug('Index \"%s\" does not meet provided criteria. Removing from list.', index)\n                self.indices.remove(index)\n\n    def filter_ilm(self, exclude=True):\n        \"\"\"\n        Match indices that have the setting ``index.lifecycle.name``\n\n        :param exclude: If ``exclude=True``, this filter will remove matching\n            ``indices``. If ``exclude=False``, then only matching indices will be\n            kept in ``indices``. Default is ``True``\n        \"\"\"\n        self.loggit.debug('Filtering indices with index.lifecycle.name')\n        index_lists = chunk_index_list(self.indices)\n        if index_lists == [['']]:\n            self.loggit.debug('Empty working list. No ILM indices to filter.')\n            return\n        for lst in index_lists:\n            working_list = self._get_indices_settings(lst)\n            if working_list:\n                for index in list(working_list.keys()):\n                    try:\n                        subvalue = working_list[index]['settings']['index']['lifecycle']\n                        has_ilm = 'name' in subvalue\n                        msg = f'{index} has index.lifecycle.name {subvalue['name']}'\n                    except KeyError:\n                        has_ilm = False\n                        msg = f'index.lifecycle.name is not set for index {index}'\n                    self.__excludify(has_ilm, exclude, index, msg)\n\n    def iterate_filters(self, filter_dict):\n        \"\"\"\n        Iterate over the filters defined in ``config`` and execute them.\n\n        :param filter_dict: The configuration dictionary\n\n        .. note:: ``filter_dict`` should be a dictionary with the following form:\n        .. code-block:: python\n\n                { 'filters' : [\n                        {\n                            'filtertype': 'the_filter_type',\n                            'key1' : 'value1',\n                            ...\n                            'keyN' : 'valueN'\n                        }\n                    ]\n                }\n\n        \"\"\"\n        self.loggit.debug('Iterating over a list of filters')\n        if 'filters' not in filter_dict or len(filter_dict['filters']) < 1:\n            self.loggit.info('No filters in config.  Returning unaltered object.')\n            return\n        self.loggit.debug('All filters: %s', filter_dict['filters'])\n        for fil in filter_dict['filters']:\n            self.loggit.debug('Top of the loop: %s', self.indices)\n            self.loggit.debug('Un-parsed filter args: %s', fil)\n            chk = SchemaCheck(fil, filterstructure(), 'filter', 'IndexList.iterate_filters').result()\n            msg = f'Parsed filter args: {chk}'\n            self.loggit.debug(msg)\n            method = self.__map_method(fil['filtertype'])\n            del fil['filtertype']\n            if fil:\n                self.loggit.debug('Filter args: %s', fil)\n                self.loggit.debug('Pre-instance: %s', self.indices)\n                method(**fil)\n                self.loggit.debug('Post-instance: %s', self.indices)\n            else:\n                method()\n\n    def filter_by_size(self, size_threshold=None, threshold_behavior='greater_than', exclude=False, size_behavior='primary'):\n        \"\"\"\n        Remove indices from the actionable list based on index size.\n\n        ``threshold_behavior``, when set to ``greater_than`` (default), includes\n        if it the index tests to be larger than ``size_threshold``. When set to\n        ``less_than``, it includes if the index is smaller than ``size_threshold``\n\n        :param size_threshold: Filter indices over *n* gigabytes\n        :param threshold_behavior: Size to filter, either ``greater_than`` or\n            ``less_than``. Defaults to ``greater_than`` to preserve backwards\n            compatability.\n        :param size_behavior: Size that used to filter, either ``primary`` or\n            ``total``. Defaults to ``primary``\n        :param exclude: If ``exclude=True``, this filter will remove matching indices\n            from ``indices``. If ``exclude=False``, then only matching indices\n            will be kept in ``indices``. Default is ``False``\n        \"\"\"\n        self.loggit.debug('Filtering indices by index size')\n        if not size_threshold:\n            raise MissingArgument('No value for \"size_threshold\" provided')\n        if size_behavior not in ['primary', 'total']:\n            raise ValueError(f'Invalid value for \"size_behavior\": {size_behavior}')\n        if threshold_behavior not in ['greater_than', 'less_than']:\n            raise ValueError(f'Invalid value for \"threshold_behavior\": {threshold_behavior}')\n        index_size_limit = float(size_threshold) * 2 ** 30\n        msg = 'Cannot get disk usage info from closed indices. Omitting any closed indices.'\n        self.loggit.debug(msg)\n        self.get_index_state()\n        self.get_index_stats()\n        self.filter_closed()\n        working_list = self.working_list()\n        for index in working_list:\n            if size_behavior == 'primary':\n                index_size = self.index_info[index]['primary_size_in_bytes']\n            else:\n                index_size = self.index_info[index]['size_in_bytes']\n            msg = f'{index}, index size is {byte_size(index_size)} and size limit is {byte_size(index_size_limit)}.'\n            if threshold_behavior == 'greater_than':\n                self.__excludify(index_size > index_size_limit, exclude, index, msg)\n            elif threshold_behavior == 'less_than':\n                self.__excludify(index_size < index_size_limit, exclude, index, msg)"
  }
}