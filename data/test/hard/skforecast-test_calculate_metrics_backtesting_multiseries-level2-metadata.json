{
  "dir_path": "/app/skforecast",
  "package_name": "skforecast",
  "sample_name": "skforecast-test_calculate_metrics_backtesting_multiseries",
  "src_dir": "skforecast/",
  "test_dir": "tests/",
  "test_file": "skforecast/model_selection/tests/tests_utils/test_calculate_metrics_backtesting_multiseries.py",
  "test_code": "# Unit test calculate_metrics_multiseries\n# ==============================================================================\nimport pandas as pd\nimport numpy as np\nimport pytest as pytest\nfrom skforecast.model_selection._utils import _calculate_metrics_backtesting_multiseries\nfrom skforecast.metrics import add_y_train_argument\nfrom sklearn.metrics import mean_absolute_error\nfrom skforecast.metrics import mean_absolute_scaled_error\n\n# Fixtures\ndata = pd.DataFrame(\n    data={\n        \"item_1\": [\n            8.253175, 22.777826, 27.549099, 25.895533, 21.379238, 21.106643,\n            20.533871, 20.069327, 20.006161, 21.620184, 21.717691, 21.751748,\n            21.758617, 20.784194, 18.976196, 20.228468, 26.636444, 29.245869,\n            24.772249, 24.018768, 22.503533, 20.794986, 23.981037, 28.018830,\n            28.747482, 23.908368, 21.423930, 24.786455, 24.615778, 27.388275,\n            25.724191, 22.825491, 23.066582, 23.788066, 23.360304, 23.119966,\n            21.763739, 23.008517, 22.861086, 22.807790, 23.424717, 22.208947,\n            19.558775, 20.788390, 23.619240, 25.061150, 27.646380, 25.609772,\n            22.504042, 20.838095\n        ],\n        \"item_2\": [\n            21.047727, 26.578125, 31.751042, 24.567708, 18.191667, 17.812500,\n            19.510417, 24.098958, 20.223958, 19.161458, 16.042708, 14.815625,\n            17.031250, 17.009375, 17.096875, 19.255208, 28.060417, 28.779167,\n            19.265625, 19.178125, 19.688542, 21.690625, 25.332292, 26.675000,\n            26.611458, 19.759375, 20.038542, 24.680208, 25.032292, 28.111458,\n            21.542708, 16.605208, 18.593750, 20.667708, 21.977083, 29.040625,\n            18.979167, 18.459375, 17.295833, 17.282292, 20.844792, 19.858333,\n            18.446875, 19.239583, 19.903125, 22.970833, 28.195833, 20.221875,\n            19.176042, 21.991667\n        ],\n        \"item_3\": [\n            19.429739, 28.009863, 32.078922, 27.252276, 20.357737, 19.879148,\n            18.043499, 26.287368, 16.315997, 21.772584, 18.729748, 12.552534,\n            18.996209, 18.534327, 15.418361, 16.304852, 30.076258, 28.886334,\n            20.286651, 21.367727, 20.248170, 19.799975, 25.931558, 27.698196,\n            30.725005, 19.573577, 23.310162, 24.959233, 24.399246, 29.094136,\n            22.639513, 18.372362, 21.256450, 22.430527, 19.575067, 31.767626,\n            20.086271, 21.380186, 17.553807, 17.369879, 21.829746, 16.208510,\n            25.067215, 21.863615, 17.887458, 23.005424, 25.013939, 22.142083,\n            23.673005, 25.238480\n        ],\n    },\n    index=pd.date_range(start=\"2012-01-01\", end=\"2012-02-19\"),\n)\n\npredictions = pd.DataFrame(\n    data={\n        \"item_1\": [\n            25.849411, 24.507137, 23.885447, 23.597504, 23.464140, 23.402371,\n            23.373762, 23.360511, 23.354374, 23.351532, 23.354278, 23.351487,\n            23.350195, 23.349596, 23.349319, 23.349190, 23.349131, 23.349103,\n            23.349090, 23.349084, 23.474207, 23.407034, 23.375922, 23.361512,\n            23.354837\n        ],\n        \"item_2\": [\n            24.561460, 23.611980, 23.172218, 22.968536, 22.874199, 22.830506,\n            22.810269, 22.800896, 22.796555, 22.794544, 22.414996, 22.617821,\n            22.711761, 22.755271, 22.775423, 22.784756, 22.789079, 22.791082,\n            22.792009, 22.792439, 21.454419, 22.172918, 22.505700, 22.659831,\n            22.731219\n        ],\n        \"item_3\": [\n            26.168069, 24.057472, 23.079925, 22.627163, 22.417461, 22.320335,\n            22.275350, 22.254515, 22.244865, 22.240395, 21.003848, 21.665604,\n            21.972104, 22.114063, 22.179813, 22.210266, 22.224370, 22.230903,\n            22.233929, 22.235330, 20.222212, 21.303581, 21.804429, 22.036402,\n            22.143843\n        ],\n    },\n    index=pd.date_range(start=\"2012-01-26\", periods=25)\n)\n\npredictions_missing_level = predictions.drop(columns=\"item_3\").copy()\n\npredictions_different_lenght = pd.DataFrame(\n    data={\n        \"item_1\": [\n            25.849411, 24.507137, 23.885447, 23.597504, 23.464140, 23.402371,\n            23.373762, 23.360511, 23.354374, 23.351532, 23.354278, 23.351487,\n            23.350195, 23.349596, 23.349319, 23.349190, 23.349131, 23.349103,\n            23.349090, 23.349084, 23.474207, 23.407034, 23.375922, 23.361512,\n            23.354837\n        ],\n        \"item_2\": [\n            24.561460, 23.611980, 23.172218, 22.968536, 22.874199, 22.830506,\n            22.810269, 22.800896, 22.796555, 22.794544, 22.414996, 22.617821,\n            22.711761, 22.755271, 22.775423, 22.784756, 22.789079, 22.791082,\n            22.792009, 22.792439, 21.454419, 22.172918, 22.505700, 22.659831,\n            22.731219\n        ],\n        \"item_3\": [\n            26.168069, 24.057472, 23.079925, 22.627163, 22.417461, 22.320335,\n            22.275350, 22.254515, 22.244865, 22.240395, 21.003848, 21.665604,\n            np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,\n            np.nan, np.nan, np.nan, np.nan, np.nan\n        ],\n    },\n    index=pd.date_range(start=\"2012-01-26\", periods=25)\n)\n\nspan_index = span_index = pd.date_range(start=\"2012-01-01\", end=\"2012-02-19\", freq=\"D\")\n\nfolds = [\n    [[0, 25], [24, 25], [25, 35], [25, 35], False],\n    [[0, 25], [34, 35], [35, 45], [35, 45], False],\n    [[0, 25], [44, 45], [45, 50], [45, 50], False],\n]\nwindow_size = 2\nlevels = [\"item_1\", \"item_2\", \"item_3\"]\n\n\ndef custom_metric(y_true, y_pred):  # pragma: no cover\n    \"\"\"\n    Calculate the mean absolute error excluding predictions between '2012-01-05'\n    and '2012-01-10'.\n    \"\"\"\n    mask = (y_true.index < '2012-01-05') | (y_true.index > '2012-01-10')\n    metric = mean_absolute_error(y_true[mask], y_pred[mask])\n    \n    return metric\n\n\ndef test_calculate_metrics_backtesting_multiseries_input_types():\n    \"\"\"\n    Check if function raises errors when input parameters have wrong types.\n    \"\"\"\n\n    # Mock inputs\n    series_df = pd.DataFrame(\n        {\"time\": pd.date_range(start=\"2020-01-01\", periods=3), \"value\": [1, 2, 3]}\n    )\n    predictions = pd.DataFrame(\n        {\n            \"time\": pd.date_range(start=\"2020-01-01\", periods=3),\n            \"predicted\": [1.5, 2.5, 3.5],\n        }\n    )\n    folds = [{\"train\": (0, 1), \"test\": (2, 3)}]\n    span_index = pd.date_range(start=\"2020-01-01\", periods=3)\n    window_size = 2\n    metrics = [\"mean_absolute_error\"]\n    levels = [\"level1\", \"level2\"]\n\n    # Test invalid type for series\n    msg = \"`series` must be a pandas DataFrame or a dictionary of pandas DataFrames.\"\n    with pytest.raises(TypeError, match=msg):\n        _calculate_metrics_backtesting_multiseries(\n            \"invalid_series_type\", predictions, folds, span_index, window_size, metrics, levels\n        )\n\n    # Test invalid type for predictions\n    msg = \"`predictions` must be a pandas DataFrame.\"\n    with pytest.raises(TypeError, match=msg):\n        _calculate_metrics_backtesting_multiseries(\n            series_df, \"invalid_predictions_type\", folds, span_index, window_size, metrics, levels\n        )\n\n    # Test invalid type for folds\n    msg = \"`folds` must be a list.\"\n    with pytest.raises(TypeError, match=msg):\n        _calculate_metrics_backtesting_multiseries(\n            series_df, predictions, \"invalid_folds_type\", span_index, window_size, metrics, levels\n        )\n\n    # Test invalid type for span_index\n    msg = \"`span_index` must be a pandas DatetimeIndex or pandas RangeIndex.\"\n    with pytest.raises(TypeError, match=msg):\n        _calculate_metrics_backtesting_multiseries(\n            series_df, predictions, folds, \"invalid_span_index_type\", window_size, metrics, levels\n        )\n\n    # Test invalid type for window_size\n    msg = \"`window_size` must be an integer.\"\n    with pytest.raises(TypeError, match=msg):\n        _calculate_metrics_backtesting_multiseries(\n            series_df, predictions, folds, span_index, \"invalid_window_size_type\", metrics, levels\n        )\n\n    # Test invalid type for metrics\n    msg = \"`metrics` must be a list.\"\n    with pytest.raises(TypeError, match=msg):\n        _calculate_metrics_backtesting_multiseries(\n            series_df, predictions, folds, span_index, window_size, \"invalid_metrics_type\", levels\n        )\n\n    # Test invalid type for levels\n    msg = \"`levels` must be a list.\"\n    with pytest.raises(TypeError, match=msg):\n        _calculate_metrics_backtesting_multiseries(\n            series_df, predictions, folds, span_index, window_size, metrics, \"invalid_levels_type\"\n        )\n\n    # Test invalid type for add_aggregated_metric\n    msg = \"`add_aggregated_metric` must be a boolean.\"\n    with pytest.raises(TypeError, match=msg):\n        _calculate_metrics_backtesting_multiseries(\n            series_df,\n            predictions,\n            folds,\n            span_index,\n            window_size,\n            metrics,\n            levels,\n            add_aggregated_metric=\"invalid_type\",\n        )\n\n\ndef test_calculate_metrics_backtesting_multiseries_output_when_no_aggregated_metric(\n    metrics=[mean_absolute_error, mean_absolute_scaled_error]\n):\n    \"\"\"\n    Test output of _calculate_metrics_backtesting_multiseries when add_aggregated_metric=False\n    \"\"\"\n\n    metrics = [add_y_train_argument(metric) for metric in metrics]\n    results = _calculate_metrics_backtesting_multiseries(\n        series=data,\n        predictions=predictions,\n        folds=folds,\n        span_index=span_index,\n        window_size=window_size,\n        metrics=metrics,\n        levels=levels,\n        add_aggregated_metric=False,\n    )\n\n    expected = pd.DataFrame(\n        data={\n            \"levels\": [\"item_1\", \"item_2\", \"item_3\"],\n            \"mean_absolute_error\": [1.477567, 3.480129, 2.942386],\n            \"mean_absolute_scaled_error\": [0.8388579569071319, 1.261808218733781, 0.6816085701001846],\n        }\n    )\n\n    pd.testing.assert_frame_equal(results, expected)\n\n\ndef test_calculate_metrics_backtesting_multiseries_output_when_aggregated_metric(\n    metrics=[mean_absolute_error, mean_absolute_scaled_error]\n):\n    \"\"\"\n    Test output of _calculate_metrics_backtesting_multiseries when add_aggregated_metric=True\n    \"\"\"\n\n    metrics = [add_y_train_argument(metric) for metric in metrics]\n    results = _calculate_metrics_backtesting_multiseries(\n        series=data,\n        predictions=predictions,\n        folds=folds,\n        span_index=span_index,\n        window_size=window_size,\n        metrics=metrics,\n        levels=levels,\n        add_aggregated_metric=True,\n    )\n\n    expected = pd.DataFrame(\n        data={\n            \"levels\": [\n                \"item_1\",\n                \"item_2\",\n                \"item_3\",\n                \"average\",\n                \"weighted_average\",\n                \"pooling\",\n            ],\n            \"mean_absolute_error\": [\n                1.477567,\n                3.480129,\n                2.942386,\n                2.633361,\n                2.633361,\n                2.633361,\n            ],\n            \"mean_absolute_scaled_error\": [\n                0.8388579569071319,\n                1.261808218733781,\n                0.6816085701001846,\n                0.9274249152470325,\n                0.9274249152470325,\n                0.8940507968656655,\n            ],\n        }\n    )\n\n    pd.testing.assert_frame_equal(results, expected)\n\n\ndef test_calculate_metrics_backtesting_multiseries_output_when_aggregated_metric_and_customer_metric(\n    metrics=[custom_metric],\n):\n    \"\"\"\n    Test output of _calculate_metrics_backtesting_multiseries when add_aggregated_metric=True\n    \"\"\"\n\n    metrics = [add_y_train_argument(metric) for metric in metrics]\n    results = _calculate_metrics_backtesting_multiseries(\n        series=data,\n        predictions=predictions,\n        folds=folds,\n        span_index=span_index,\n        window_size=window_size,\n        metrics=metrics,\n        levels=levels,\n        add_aggregated_metric=True,\n    )\n\n    expected = pd.DataFrame(\n        {\n            \"levels\": {\n                0: \"item_1\",\n                1: \"item_2\",\n                2: \"item_3\",\n                3: \"average\",\n                4: \"weighted_average\",\n                5: \"pooling\",\n            },\n            \"custom_metric\": {\n                0: 1.47756696,\n                1: 3.48012924,\n                2: 2.9423860000000004,\n                3: 2.6333607333333333,\n                4: 2.6333607333333333,\n                5: 2.6333607333333338,\n            },\n        }\n    )\n\n    pd.testing.assert_frame_equal(results, expected)\n\n\ndef test_calculate_metrics_backtesting_multiseries_output_when_aggregated_metric_and_predictions_have_different_length(\n    metrics=[mean_absolute_error, mean_absolute_scaled_error]\n):\n    \"\"\"\n    \"\"\"\n    metrics = [add_y_train_argument(metric) for metric in metrics]\n    results = _calculate_metrics_backtesting_multiseries(\n        series=data,\n        predictions=predictions_different_lenght,\n        folds=folds,\n        span_index=span_index,\n        window_size=window_size,\n        metrics=metrics,\n        levels=levels,\n        add_aggregated_metric=True,\n    )\n\n    expected = pd.DataFrame(\n        data={\n            \"levels\": [\n                \"item_1\",\n                \"item_2\",\n                \"item_3\",\n                \"average\",\n                \"weighted_average\",\n                \"pooling\",\n            ],\n            \"mean_absolute_error\": [\n                1.477567,\n                3.480129,\n                3.173683,\n                2.710460,\n                2.613332,\n                2.613332,\n            ],\n            \"mean_absolute_scaled_error\": [\n                0.8388579569071319,\n                1.261808218733781,\n                0.7351889788709302,\n                0.9452850515039476,\n                0.9893374538302255,\n                0.8872509680588639,\n            ],\n        }\n    )\n\n    pd.testing.assert_frame_equal(results, expected)\n\n\ndef test_calculate_metrics_backtesting_multiseries_output_when_aggregated_metric_and_one_level_is_not_predicted(\n    metrics=[mean_absolute_error, mean_absolute_scaled_error]\n):\n    \"\"\"\n    \n    \"\"\"\n    metrics = [add_y_train_argument(metric) for metric in metrics]\n    results = _calculate_metrics_backtesting_multiseries(\n        series=data,\n        predictions=predictions_missing_level,\n        folds=folds,\n        span_index=span_index,\n        window_size=window_size,\n        metrics=metrics,\n        levels=levels,\n        add_aggregated_metric=True,\n    )\n\n    expected = pd.DataFrame(\n        {\n            \"levels\": [\n                \"item_1\",\n                \"item_2\",\n                \"item_3\",\n                \"average\",\n                \"weighted_average\",\n                \"pooling\",\n            ],\n            \"mean_absolute_error\": [\n                1.47756696,\n                3.48012924,\n                np.nan,\n                2.4788481,\n                2.4788481,\n                2.4788481,\n            ],\n            \"mean_absolute_scaled_error\": [\n                0.8388579569071319,\n                1.261808218733781,\n                np.nan,\n                1.0503330878204564,\n                1.0503330878204564,\n                1.096968360536767,\n            ],\n        }\n    )\n\n    pd.testing.assert_frame_equal(results, expected)\n",
  "GT_file_code": {
    "skforecast/metrics/metrics.py": "################################################################################\n#                                metrics                                       #\n#                                                                              #\n# This work by skforecast team is licensed under the BSD 3-Clause License.     #\n################################################################################\n# coding=utf-8\n\nfrom typing import Union, Callable\nimport numpy as np\nimport pandas as pd\nimport inspect\nfrom functools import wraps\nfrom sklearn.metrics import (\n    mean_squared_error,\n    mean_absolute_error,\n    mean_absolute_percentage_error,\n    mean_squared_log_error,\n    median_absolute_error,\n)\n\n\ndef _get_metric(metric: str) -> Callable:\n    \"\"\"\n    Get the corresponding scikit-learn function to calculate the metric.\n\n    Parameters\n    ----------\n    metric : str\n        Metric used to quantify the goodness of fit of the model.\n\n    Returns\n    -------\n    metric : Callable\n        scikit-learn function to calculate the desired metric.\n\n    \"\"\"\n\n    allowed_metrics = [\n        \"mean_squared_error\",\n        \"mean_absolute_error\",\n        \"mean_absolute_percentage_error\",\n        \"mean_squared_log_error\",\n        \"mean_absolute_scaled_error\",\n        \"root_mean_squared_scaled_error\",\n        \"median_absolute_error\",\n    ]\n\n    if metric not in allowed_metrics:\n        raise ValueError((f\"Allowed metrics are: {allowed_metrics}. Got {metric}.\"))\n\n    metrics = {\n        \"mean_squared_error\": mean_squared_error,\n        \"mean_absolute_error\": mean_absolute_error,\n        \"mean_absolute_percentage_error\": mean_absolute_percentage_error,\n        \"mean_squared_log_error\": mean_squared_log_error,\n        \"mean_absolute_scaled_error\": mean_absolute_scaled_error,\n        \"root_mean_squared_scaled_error\": root_mean_squared_scaled_error,\n        \"median_absolute_error\": median_absolute_error,\n    }\n\n    metric = add_y_train_argument(metrics[metric])\n\n    return metric\n\n\ndef add_y_train_argument(func: Callable) -> Callable:\n    \"\"\"\n    Add `y_train` argument to a function if it is not already present.\n\n    Parameters\n    ----------\n    func : callable\n        Function to which the argument is added.\n\n    Returns\n    -------\n    wrapper : callable\n        Function with `y_train` argument added.\n    \n    \"\"\"\n\n    sig = inspect.signature(func)\n    \n    if \"y_train\" in sig.parameters:\n        return func\n\n    new_params = list(sig.parameters.values()) + [\n        inspect.Parameter(\"y_train\", inspect.Parameter.KEYWORD_ONLY, default=None)\n    ]\n    new_sig = sig.replace(parameters=new_params)\n\n    @wraps(func)\n    def wrapper(*args, y_train=None, **kwargs):\n        return func(*args, **kwargs)\n    \n    wrapper.__signature__ = new_sig\n    \n    return wrapper\n\n\ndef mean_absolute_scaled_error(\n    y_true: Union[pd.Series, np.ndarray],\n    y_pred: Union[pd.Series, np.ndarray],\n    y_train: Union[list, pd.Series, np.ndarray],\n) -> float:\n    \"\"\"\n    Mean Absolute Scaled Error (MASE)\n\n    MASE is a scale-independent error metric that measures the accuracy of\n    a forecast. It is the mean absolute error of the forecast divided by the\n    mean absolute error of a naive forecast in the training set. The naive\n    forecast is the one obtained by shifting the time series by one period.\n    If y_train is a list of numpy arrays or pandas Series, it is considered\n    that each element is the true value of the target variable in the training\n    set for each time series. In this case, the naive forecast is calculated\n    for each time series separately.\n\n    Parameters\n    ----------\n    y_true : pandas Series, numpy ndarray\n        True values of the target variable.\n    y_pred : pandas Series, numpy ndarray\n        Predicted values of the target variable.\n    y_train : list, pandas Series, numpy ndarray\n        True values of the target variable in the training set. If `list`, it\n        is consider that each element is the true value of the target variable\n        in the training set for each time series.\n\n    Returns\n    -------\n    mase : float\n        MASE value.\n    \n    \"\"\"\n\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_true` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_pred` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_train, (list, pd.Series, np.ndarray)):\n        raise TypeError(\"`y_train` must be a list, pandas Series or numpy ndarray.\")\n    if isinstance(y_train, list):\n        for x in y_train:\n            if not isinstance(x, (pd.Series, np.ndarray)):\n                raise TypeError(\n                    (\"When `y_train` is a list, each element must be a pandas Series \"\n                     \"or numpy ndarray.\")\n                )\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"`y_true` and `y_pred` must have the same length.\")\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError(\"`y_true` and `y_pred` must have at least one element.\")\n\n    if isinstance(y_train, list):\n        naive_forecast = np.concatenate([np.diff(x) for x in y_train])\n    else:\n        naive_forecast = np.diff(y_train)\n\n    mase = np.mean(np.abs(y_true - y_pred)) / np.nanmean(np.abs(naive_forecast))\n\n    return mase\n\n\ndef root_mean_squared_scaled_error(\n    y_true: Union[pd.Series, np.ndarray],\n    y_pred: Union[pd.Series, np.ndarray],\n    y_train: Union[list, pd.Series, np.ndarray],\n) -> float:\n    \"\"\"\n    Root Mean Squared Scaled Error (RMSSE)\n\n    RMSSE is a scale-independent error metric that measures the accuracy of\n    a forecast. It is the root mean squared error of the forecast divided by\n    the root mean squared error of a naive forecast in the training set. The\n    naive forecast is the one obtained by shifting the time series by one period.\n    If y_train is a list of numpy arrays or pandas Series, it is considered\n    that each element is the true value of the target variable in the training\n    set for each time series. In this case, the naive forecast is calculated\n    for each time series separately.\n\n    Parameters\n    ----------\n    y_true : pandas Series, numpy ndarray\n        True values of the target variable.\n    y_pred : pandas Series, numpy ndarray\n        Predicted values of the target variable.\n    y_train : list, pandas Series, numpy ndarray\n        True values of the target variable in the training set. If list, it\n        is consider that each element is the true value of the target variable\n        in the training set for each time series.\n\n    Returns\n    -------\n    rmsse : float\n        RMSSE value.\n    \n    \"\"\"\n\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_true` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_pred` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_train, (list, pd.Series, np.ndarray)):\n        raise TypeError(\"`y_train` must be a list, pandas Series or numpy ndarray.\")\n    if isinstance(y_train, list):\n        for x in y_train:\n            if not isinstance(x, (pd.Series, np.ndarray)):\n                raise TypeError(\n                    (\"When `y_train` is a list, each element must be a pandas Series \"\n                     \"or numpy ndarray.\")\n                )\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"`y_true` and `y_pred` must have the same length.\")\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError(\"`y_true` and `y_pred` must have at least one element.\")\n\n    if isinstance(y_train, list):\n        naive_forecast = np.concatenate([np.diff(x) for x in y_train])\n    else:\n        naive_forecast = np.diff(y_train)\n    \n    rmsse = np.sqrt(np.mean((y_true - y_pred) ** 2)) / np.sqrt(np.nanmean(naive_forecast ** 2))\n    \n    return rmsse\n",
    "skforecast/model_selection/_utils.py": "################################################################################\n#                     skforecast.model_selection._utils                        #\n#                                                                              #\n# This work by skforecast team is licensed under the BSD 3-Clause License.     #\n################################################################################\n# coding=utf-8\n\nfrom typing import Union, Tuple, Optional, Callable, Generator\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom joblib import cpu_count\nfrom tqdm.auto import tqdm\nfrom sklearn.pipeline import Pipeline\nimport sklearn.linear_model\nfrom sklearn.exceptions import NotFittedError\n\nfrom ..exceptions import IgnoredArgumentWarning\nfrom ..metrics import add_y_train_argument, _get_metric\nfrom ..utils import check_interval\n\n\ndef initialize_lags_grid(\n    forecaster: object, \n    lags_grid: Optional[Union[list, dict]] = None\n) -> Tuple[dict, str]:\n    \"\"\"\n    Initialize lags grid and lags label for model selection. \n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model. ForecasterRecursive, ForecasterDirect, \n        ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate.\n    lags_grid : list, dict, default `None`\n        Lists of lags to try, containing int, lists, numpy ndarray, or range \n        objects. If `dict`, the keys are used as labels in the `results` \n        DataFrame, and the values are used as the lists of lags to try.\n\n    Returns\n    -------\n    lags_grid : dict\n        Dictionary with lags configuration for each iteration.\n    lags_label : str\n        Label for lags representation in the results object.\n\n    \"\"\"\n\n    if not isinstance(lags_grid, (list, dict, type(None))):\n        raise TypeError(\n            (f\"`lags_grid` argument must be a list, dict or None. \"\n             f\"Got {type(lags_grid)}.\")\n        )\n\n    lags_label = 'values'\n    if isinstance(lags_grid, list):\n        lags_grid = {f'{lags}': lags for lags in lags_grid}\n    elif lags_grid is None:\n        lags = [int(lag) for lag in forecaster.lags]  # Required since numpy 2.0\n        lags_grid = {f'{lags}': lags}\n    else:\n        lags_label = 'keys'\n\n    return lags_grid, lags_label\n\n\ndef check_backtesting_input(\n    forecaster: object,\n    cv: object,\n    metric: Union[str, Callable, list],\n    add_aggregated_metric: bool = True,\n    y: Optional[pd.Series] = None,\n    series: Optional[Union[pd.DataFrame, dict]] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    interval: Optional[list] = None,\n    alpha: Optional[float] = None,\n    n_boot: int = 250,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = False,\n    n_jobs: Union[int, str] = 'auto',\n    show_progress: bool = True,\n    suppress_warnings: bool = False,\n    suppress_warnings_fit: bool = False\n) -> None:\n    \"\"\"\n    This is a helper function to check most inputs of backtesting functions in \n    modules `model_selection`.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model.\n    cv : TimeSeriesFold\n        TimeSeriesFold object with the information needed to split the data into folds.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n    add_aggregated_metric : bool, default `True`\n        If `True`, the aggregated metrics (average, weighted average and pooling)\n        over all levels are also returned (only multiseries).\n    y : pandas Series, default `None`\n        Training time series for uni-series forecasters.\n    series : pandas DataFrame, dict, default `None`\n        Training time series for multi-series forecasters.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive.\n    alpha : float, default `None`\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %. \n    n_boot : int, default `250`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of prediction \n        error to create prediction intervals.  If `False`, out_sample_residuals \n        are used if they are already stored inside the forecaster.\n    use_binned_residuals : bool, default `False`\n        If `True`, residuals used in each bootstrapping iteration are selected\n        conditioning on the predicted values. If `False`, residuals are selected\n        randomly without conditioning on the predicted values.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the fuction\n        skforecast.utils.select_n_jobs_fit_forecaster.\n        **New in version 0.9.0**\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n    suppress_warnings: bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the backtesting \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    suppress_warnings_fit : bool, default `False`\n        If `True`, warnings generated during fitting will be ignored. Only \n        `ForecasterSarimax`.\n\n    Returns\n    -------\n    None\n    \n    \"\"\"\n\n    forecaster_name = type(forecaster).__name__\n    cv_name = type(cv).__name__\n\n    if cv_name != \"TimeSeriesFold\":\n        raise TypeError(f\"`cv` must be a TimeSeriesFold object. Got {cv_name}.\")\n\n    steps = cv.steps\n    initial_train_size = cv.initial_train_size\n    gap = cv.gap\n    allow_incomplete_fold = cv.allow_incomplete_fold\n    refit = cv.refit\n\n    forecasters_uni = [\n        \"ForecasterRecursive\",\n        \"ForecasterDirect\",\n        \"ForecasterSarimax\",\n        \"ForecasterEquivalentDate\",\n    ]\n    forecasters_multi = [\n        \"ForecasterDirectMultiVariate\",\n        \"ForecasterRnn\",\n    ]\n    forecasters_multi_dict = [\n        \"ForecasterRecursiveMultiSeries\"\n    ]\n\n    if forecaster_name in forecasters_uni:\n        if not isinstance(y, pd.Series):\n            raise TypeError(\"`y` must be a pandas Series.\")\n        data_name = 'y'\n        data_length = len(y)\n\n    elif forecaster_name in forecasters_multi:\n        if not isinstance(series, pd.DataFrame):\n            raise TypeError(\"`series` must be a pandas DataFrame.\")\n        data_name = 'series'\n        data_length = len(series)\n    \n    elif forecaster_name in forecasters_multi_dict:\n        if not isinstance(series, (pd.DataFrame, dict)):\n            raise TypeError(\n                f\"`series` must be a pandas DataFrame or a dict of DataFrames or Series. \"\n                f\"Got {type(series)}.\"\n            )\n        \n        data_name = 'series'\n        if isinstance(series, dict):\n            not_valid_series = [\n                k \n                for k, v in series.items()\n                if not isinstance(v, (pd.Series, pd.DataFrame))\n            ]\n            if not_valid_series:\n                raise TypeError(\n                    f\"If `series` is a dictionary, all series must be a named \"\n                    f\"pandas Series or a pandas DataFrame with a single column. \"\n                    f\"Review series: {not_valid_series}\"\n                )\n            not_valid_index = [\n                k \n                for k, v in series.items()\n                if not isinstance(v.index, pd.DatetimeIndex)\n            ]\n            if not_valid_index:\n                raise ValueError(\n                    f\"If `series` is a dictionary, all series must have a Pandas \"\n                    f\"DatetimeIndex as index with the same frequency. \"\n                    f\"Review series: {not_valid_index}\"\n                )\n\n            indexes_freq = [f'{v.index.freq}' for v in series.values()]\n            indexes_freq = sorted(set(indexes_freq))\n            if not len(indexes_freq) == 1:\n                raise ValueError(\n                    f\"If `series` is a dictionary, all series must have a Pandas \"\n                    f\"DatetimeIndex as index with the same frequency. \"\n                    f\"Found frequencies: {indexes_freq}\"\n                )\n            data_length = max([len(series[serie]) for serie in series])\n        else:\n            data_length = len(series)\n\n    if exog is not None:\n        if forecaster_name in forecasters_multi_dict:\n            if not isinstance(exog, (pd.Series, pd.DataFrame, dict)):\n                raise TypeError(\n                    f\"`exog` must be a pandas Series, DataFrame, dictionary of pandas \"\n                    f\"Series/DataFrames or None. Got {type(exog)}.\"\n                )\n            if isinstance(exog, dict):\n                not_valid_exog = [\n                    k \n                    for k, v in exog.items()\n                    if not isinstance(v, (pd.Series, pd.DataFrame, type(None)))\n                ]\n                if not_valid_exog:\n                    raise TypeError(\n                        f\"If `exog` is a dictionary, All exog must be a named pandas \"\n                        f\"Series, a pandas DataFrame or None. Review exog: {not_valid_exog}\"\n                    )\n        else:\n            if not isinstance(exog, (pd.Series, pd.DataFrame)):\n                raise TypeError(\n                    f\"`exog` must be a pandas Series, DataFrame or None. Got {type(exog)}.\"\n                )\n\n    if hasattr(forecaster, 'differentiation'):\n        if forecaster.differentiation != cv.differentiation:\n            raise ValueError(\n                f\"The differentiation included in the forecaster \"\n                f\"({forecaster.differentiation}) differs from the differentiation \"\n                f\"included in the cv ({cv.differentiation}). Set the same value \"\n                f\"for both using the `differentiation` argument.\"\n            )\n\n    if not isinstance(metric, (str, Callable, list)):\n        raise TypeError(\n            f\"`metric` must be a string, a callable function, or a list containing \"\n            f\"multiple strings and/or callables. Got {type(metric)}.\"\n        )\n\n    if forecaster_name == \"ForecasterEquivalentDate\" and isinstance(\n        forecaster.offset, pd.tseries.offsets.DateOffset\n    ):\n        if initial_train_size is None:\n            raise ValueError(\n                f\"`initial_train_size` must be an integer greater than \"\n                f\"the `window_size` of the forecaster ({forecaster.window_size}) \"\n                f\"and smaller than the length of `{data_name}` ({data_length}).\"\n            )\n    elif initial_train_size is not None:\n        if initial_train_size < forecaster.window_size or initial_train_size >= data_length:\n            raise ValueError(\n                f\"If used, `initial_train_size` must be an integer greater than \"\n                f\"the `window_size` of the forecaster ({forecaster.window_size}) \"\n                f\"and smaller than the length of `{data_name}` ({data_length}).\"\n            )\n        if initial_train_size + gap >= data_length:\n            raise ValueError(\n                f\"The combination of initial_train_size {initial_train_size} and \"\n                f\"gap {gap} cannot be greater than the length of `{data_name}` \"\n                f\"({data_length}).\"\n            )\n    else:\n        if forecaster_name in ['ForecasterSarimax', 'ForecasterEquivalentDate']:\n            raise ValueError(\n                f\"`initial_train_size` must be an integer smaller than the \"\n                f\"length of `{data_name}` ({data_length}).\"\n            )\n        else:\n            if not forecaster.is_fitted:\n                raise NotFittedError(\n                    \"`forecaster` must be already trained if no `initial_train_size` \"\n                    \"is provided.\"\n                )\n            if refit:\n                raise ValueError(\n                    \"`refit` is only allowed when `initial_train_size` is not `None`.\"\n                )\n\n    if forecaster_name == 'ForecasterSarimax' and cv.skip_folds is not None:\n        raise ValueError(\n            \"`skip_folds` is not allowed for ForecasterSarimax. Set it to `None`.\"\n        )\n\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError(\"`add_aggregated_metric` must be a boolean: `True`, `False`.\")\n    if not isinstance(n_boot, (int, np.integer)) or n_boot < 0:\n        raise TypeError(f\"`n_boot` must be an integer greater than 0. Got {n_boot}.\")\n    if not isinstance(random_state, (int, np.integer)) or random_state < 0:\n        raise TypeError(f\"`random_state` must be an integer greater than 0. Got {random_state}.\")\n    if not isinstance(use_in_sample_residuals, bool):\n        raise TypeError(\"`use_in_sample_residuals` must be a boolean: `True`, `False`.\")\n    if not isinstance(use_binned_residuals, bool):\n        raise TypeError(\"`use_binned_residuals` must be a boolean: `True`, `False`.\")\n    if not isinstance(n_jobs, int) and n_jobs != 'auto':\n        raise TypeError(f\"`n_jobs` must be an integer or `'auto'`. Got {n_jobs}.\")\n    if not isinstance(show_progress, bool):\n        raise TypeError(\"`show_progress` must be a boolean: `True`, `False`.\")\n    if not isinstance(suppress_warnings, bool):\n        raise TypeError(\"`suppress_warnings` must be a boolean: `True`, `False`.\")\n    if not isinstance(suppress_warnings_fit, bool):\n        raise TypeError(\"`suppress_warnings_fit` must be a boolean: `True`, `False`.\")\n\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n\n    if not allow_incomplete_fold and data_length - (initial_train_size + gap) < steps:\n        raise ValueError(\n            f\"There is not enough data to evaluate {steps} steps in a single \"\n            f\"fold. Set `allow_incomplete_fold` to `True` to allow incomplete folds.\\n\"\n            f\"    Data available for test : {data_length - (initial_train_size + gap)}\\n\"\n            f\"    Steps                   : {steps}\"\n        )\n\n\ndef select_n_jobs_backtesting(\n    forecaster: object,\n    refit: Union[bool, int]\n) -> int:\n    \"\"\"\n    Select the optimal number of jobs to use in the backtesting process. This\n    selection is based on heuristics and is not guaranteed to be optimal.\n\n    The number of jobs is chosen as follows:\n\n    - If `refit` is an integer, then `n_jobs = 1`. This is because parallelization doesn't \n    work with intermittent refit.\n    - If forecaster is 'ForecasterRecursive' and regressor is a linear regressor, \n    then `n_jobs = 1`.\n    - If forecaster is 'ForecasterRecursive' and regressor is not a linear \n    regressor then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and `refit = True`, then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and `refit = False`, then `n_jobs = 1`.\n    - If forecaster is 'ForecasterRecursiveMultiSeries', then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterSarimax' or 'ForecasterEquivalentDate', \n    then `n_jobs = 1`.\n    - If regressor is a `LGBMRegressor(n_jobs=1)`, then `n_jobs = cpu_count() - 1`.\n    - If regressor is a `LGBMRegressor` with internal n_jobs != 1, then `n_jobs = 1`.\n    This is because `lightgbm` is highly optimized for gradient boosting and\n    parallelizes operations at a very fine-grained level, making additional\n    parallelization unnecessary and potentially harmful due to resource contention.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model.\n    refit : bool, int\n        If the forecaster is refitted during the backtesting process.\n\n    Returns\n    -------\n    n_jobs : int\n        The number of jobs to run in parallel.\n    \n    \"\"\"\n\n    forecaster_name = type(forecaster).__name__\n\n    if isinstance(forecaster.regressor, Pipeline):\n        regressor = forecaster.regressor[-1]\n        regressor_name = type(regressor).__name__\n    else:\n        regressor = forecaster.regressor\n        regressor_name = type(regressor).__name__\n\n    linear_regressors = [\n        regressor_name\n        for regressor_name in dir(sklearn.linear_model)\n        if not regressor_name.startswith('_')\n    ]\n\n    refit = False if refit == 0 else refit\n    if not isinstance(refit, bool) and refit != 1:\n        n_jobs = 1\n    else:\n        if forecaster_name in ['ForecasterRecursive']:\n            if regressor_name in linear_regressors:\n                n_jobs = 1\n            elif regressor_name == 'LGBMRegressor':\n                n_jobs = cpu_count() - 1 if regressor.n_jobs == 1 else 1\n            else:\n                n_jobs = cpu_count() - 1\n        elif forecaster_name in ['ForecasterDirect', 'ForecasterDirectMultiVariate']:\n            # Parallelization is applied during the fitting process.\n            n_jobs = 1\n        elif forecaster_name in ['ForecasterRecursiveMultiSeries']:\n            if regressor_name == 'LGBMRegressor':\n                n_jobs = cpu_count() - 1 if regressor.n_jobs == 1 else 1\n            else:\n                n_jobs = cpu_count() - 1\n        elif forecaster_name in ['ForecasterSarimax', 'ForecasterEquivalentDate']:\n            n_jobs = 1\n        else:\n            n_jobs = 1\n\n    return n_jobs\n\n\ndef _calculate_metrics_one_step_ahead(\n    forecaster: object,\n    y: pd.Series,\n    metrics: list,\n    X_train: pd.DataFrame,\n    y_train: Union[pd.Series, dict],\n    X_test: pd.DataFrame,\n    y_test: Union[pd.Series, dict]\n) -> list:\n    \"\"\"\n    Calculate metrics when predictions are one-step-ahead. When forecaster is\n    of type ForecasterDirect only the regressor for step 1 is used.\n\n    Parameters\n    ----------\n    forecaster : object\n        Forecaster model.\n    y : pandas Series\n        Time series data used to train and test the model.\n    metrics : list\n        List of metrics.\n    X_train : pandas DataFrame\n        Predictor values used to train the model.\n    y_train : pandas Series\n        Target values related to each row of `X_train`.\n    X_test : pandas DataFrame\n        Predictor values used to test the model.\n    y_test : pandas Series\n        Target values related to each row of `X_test`.\n\n    Returns\n    -------\n    metric_values : list\n        List with metric values.\n    \n    \"\"\"\n\n    if type(forecaster).__name__ == 'ForecasterDirect':\n\n        step = 1  # Only the model for step 1 is optimized.\n        X_train, y_train = forecaster.filter_train_X_y_for_step(\n                               step    = step,\n                               X_train = X_train,\n                               y_train = y_train\n                           )\n        X_test, y_test = forecaster.filter_train_X_y_for_step(\n                             step    = step,  \n                             X_train = X_test,\n                             y_train = y_test\n                         )\n        forecaster.regressors_[step].fit(X_train, y_train)\n        y_pred = forecaster.regressors_[step].predict(X_test)\n\n    else:\n        forecaster.regressor.fit(X_train, y_train)\n        y_pred = forecaster.regressor.predict(X_test)\n\n    y_true = y_test.to_numpy()\n    y_pred = y_pred.ravel()\n    y_train = y_train.to_numpy()\n\n    if forecaster.differentiation is not None:\n        y_true = forecaster.differentiator.inverse_transform_next_window(y_true)\n        y_pred = forecaster.differentiator.inverse_transform_next_window(y_pred)\n        y_train = forecaster.differentiator.inverse_transform_training(y_train)\n\n    if forecaster.transformer_y is not None:\n        y_true = forecaster.transformer_y.inverse_transform(y_true.reshape(-1, 1))\n        y_pred = forecaster.transformer_y.inverse_transform(y_pred.reshape(-1, 1))\n        y_train = forecaster.transformer_y.inverse_transform(y_train.reshape(-1, 1))\n\n    metric_values = []\n    for m in metrics:\n        metric_values.append(\n            m(y_true=y_true.ravel(), y_pred=y_pred.ravel(), y_train=y_train.ravel())\n        )\n\n    return metric_values\n\n\ndef _initialize_levels_model_selection_multiseries(\n    forecaster: object, \n    series: Union[pd.DataFrame, dict],\n    levels: Optional[Union[str, list]] = None\n) -> list:\n    \"\"\"\n    Initialize levels for model_selection multi-series functions.\n\n    Parameters\n    ----------\n    forecaster : ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate, ForecasterRnn\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Training time series.\n    levels : str, list, default `None`\n        level (`str`) or levels (`list`) at which the forecaster is optimized. \n        If `None`, all levels are taken into account. The resulting metric will be\n        the average of the optimization of all levels.\n\n    Returns\n    -------\n    levels : list\n        List of levels to be used in model_selection multi-series functions.\n    \n    \"\"\"\n\n    multi_series_forecasters_with_levels = [\n        'ForecasterRecursiveMultiSeries', \n        'ForecasterRnn'\n    ]\n\n    if type(forecaster).__name__ in multi_series_forecasters_with_levels  \\\n        and not isinstance(levels, (str, list, type(None))):\n        raise TypeError(\n            (f\"`levels` must be a `list` of column names, a `str` of a column \"\n             f\"name or `None` when using a forecaster of type \"\n             f\"{multi_series_forecasters_with_levels}. If the forecaster is of \"\n             f\"type `ForecasterDirectMultiVariate`, this argument is ignored.\")\n        )\n\n    if type(forecaster).__name__ == 'ForecasterDirectMultiVariate':\n        if levels and levels != forecaster.level and levels != [forecaster.level]:\n            warnings.warn(\n                (f\"`levels` argument have no use when the forecaster is of type \"\n                 f\"`ForecasterDirectMultiVariate`. The level of this forecaster \"\n                 f\"is '{forecaster.level}', to predict another level, change \"\n                 f\"the `level` argument when initializing the forecaster. \\n\"),\n                 IgnoredArgumentWarning\n            )\n        levels = [forecaster.level]\n    else:\n        if levels is None:\n            # Forecaster could be untrained, so self.series_col_names cannot be used.\n            if isinstance(series, pd.DataFrame):\n                levels = list(series.columns)\n            else:\n                levels = list(series.keys())\n        elif isinstance(levels, str):\n            levels = [levels]\n\n    return levels\n\n\ndef _extract_data_folds_multiseries(\n    series: Union[pd.Series, pd.DataFrame, dict],\n    folds: list,\n    span_index: Union[pd.DatetimeIndex, pd.RangeIndex],\n    window_size: int,\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    dropna_last_window: bool = False,\n    externally_fitted: bool = False\n) -> Generator[\n        Tuple[\n            Union[pd.Series, pd.DataFrame, dict],\n            pd.DataFrame,\n            list,\n            Optional[Union[pd.Series, pd.DataFrame, dict]],\n            Optional[Union[pd.Series, pd.DataFrame, dict]],\n            list\n        ],\n        None,\n        None\n    ]:\n    \"\"\"\n    Select the data from series and exog that corresponds to each fold created using the\n    skforecast.model_selection._create_backtesting_folds function.\n\n    Parameters\n    ----------\n    series : pandas Series, pandas DataFrame, dict\n        Time series.\n    folds : list\n        Folds created using the skforecast.model_selection._create_backtesting_folds\n        function.\n    span_index : pandas DatetimeIndex, pandas RangeIndex\n        Full index from the minimum to the maximum index among all series.\n    window_size : int\n        Size of the window needed to create the predictors.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    dropna_last_window : bool, default `False`\n        If `True`, drop the columns of the last window that have NaN values.\n    externally_fitted : bool, default `False`\n        Flag indicating whether the forecaster is already trained. Only used when \n        `initial_train_size` is None and `refit` is False.\n\n    Yield\n    -----\n    series_train : pandas Series, pandas DataFrame, dict\n        Time series corresponding to the training set of the fold.\n    series_last_window: pandas DataFrame\n        Time series corresponding to the last window of the fold.\n    levels_last_window: list\n        Levels of the time series present in the last window of the fold.\n    exog_train: pandas Series, pandas DataFrame, dict, None\n        Exogenous variable corresponding to the training set of the fold.\n    exog_test: pandas Series, pandas DataFrame, dict, None\n        Exogenous variable corresponding to the test set of the fold.\n    fold: list\n        Fold created using the skforecast.model_selection._create_backtesting_folds\n\n    \"\"\"\n\n    for fold in folds:\n        train_iloc_start       = fold[0][0]\n        train_iloc_end         = fold[0][1]\n        last_window_iloc_start = fold[1][0]\n        last_window_iloc_end   = fold[1][1]\n        test_iloc_start        = fold[2][0]\n        test_iloc_end          = fold[2][1]\n\n        if isinstance(series, dict) or isinstance(exog, dict):\n            # Substract 1 to the iloc indexes to get the loc indexes\n            train_loc_start       = span_index[train_iloc_start]\n            train_loc_end         = span_index[train_iloc_end - 1]\n            last_window_loc_start = span_index[last_window_iloc_start]\n            last_window_loc_end   = span_index[last_window_iloc_end - 1]\n            test_loc_start        = span_index[test_iloc_start]\n            test_loc_end          = span_index[test_iloc_end - 1]\n\n        if isinstance(series, pd.DataFrame):\n            series_train = series.iloc[train_iloc_start:train_iloc_end, ]\n\n            series_to_drop = []\n            for col in series_train.columns:\n                if series_train[col].isna().all():\n                    series_to_drop.append(col)\n                else:\n                    first_valid_index = series_train[col].first_valid_index()\n                    last_valid_index = series_train[col].last_valid_index()\n                    if (\n                        len(series_train[col].loc[first_valid_index:last_valid_index])\n                        < window_size\n                    ):\n                        series_to_drop.append(col)\n\n            series_last_window = series.iloc[\n                last_window_iloc_start:last_window_iloc_end,\n            ]\n            \n            series_train = series_train.drop(columns=series_to_drop)\n            if not externally_fitted:\n                series_last_window = series_last_window.drop(columns=series_to_drop)\n        else:\n            series_train = {}\n            for k in series.keys():\n                v = series[k].loc[train_loc_start:train_loc_end]\n                if not v.isna().all():\n                    first_valid_index = v.first_valid_index()\n                    last_valid_index  = v.last_valid_index()\n                    if first_valid_index is not None and last_valid_index is not None:\n                        v = v.loc[first_valid_index : last_valid_index]\n                        if len(v) >= window_size:\n                            series_train[k] = v\n\n            series_last_window = {}\n            for k, v in series.items():\n                v = series[k].loc[last_window_loc_start:last_window_loc_end]\n                if ((externally_fitted or k in series_train) and len(v) >= window_size):\n                    series_last_window[k] = v\n\n            series_last_window = pd.DataFrame(series_last_window)\n\n        if dropna_last_window:\n            series_last_window = series_last_window.dropna(axis=1, how=\"any\")\n            # TODO: add the option to drop the series without minimum non NaN values.\n            # Similar to how pandas does in the rolling window function.\n        \n        levels_last_window = list(series_last_window.columns)\n\n        if exog is not None:\n            if isinstance(exog, (pd.Series, pd.DataFrame)):\n                exog_train = exog.iloc[train_iloc_start:train_iloc_end, ]\n                exog_test = exog.iloc[test_iloc_start:test_iloc_end, ]\n            else:\n                exog_train = {\n                    k: v.loc[train_loc_start:train_loc_end] \n                    for k, v in exog.items()\n                }\n                exog_train = {k: v for k, v in exog_train.items() if len(v) > 0}\n\n                exog_test = {\n                    k: v.loc[test_loc_start:test_loc_end]\n                    for k, v in exog.items()\n                    if externally_fitted or k in exog_train\n                }\n\n                exog_test = {k: v for k, v in exog_test.items() if len(v) > 0}\n        else:\n            exog_train = None\n            exog_test = None\n\n        yield series_train, series_last_window, levels_last_window, exog_train, exog_test, fold\n\n\ndef _calculate_metrics_backtesting_multiseries(\n    series: Union[pd.DataFrame, dict],\n    predictions: pd.DataFrame,\n    folds: Union[list, tqdm],\n    span_index: Union[pd.DatetimeIndex, pd.RangeIndex],\n    window_size: int,\n    metrics: list,\n    levels: list,\n    add_aggregated_metric: bool = True\n) -> pd.DataFrame:\n    \"\"\"   \n    Calculate metrics for each level and also for all levels aggregated using\n    average, weighted average or pooling.\n\n    - 'average': the average (arithmetic mean) of all levels.\n    - 'weighted_average': the average of the metrics weighted by the number of\n    predicted values of each level.\n    - 'pooling': the values of all levels are pooled and then the metric is\n    calculated.\n\n    Parameters\n    ----------\n    series : pandas DataFrame, dict\n        Series data used for backtesting.\n    predictions : pandas DataFrame\n        Predictions generated during the backtesting process.\n    folds : list, tqdm\n        Folds created during the backtesting process.\n    span_index : pandas DatetimeIndex, pandas RangeIndex\n        Full index from the minimum to the maximum index among all series.\n    window_size : int\n        Size of the window used by the forecaster to create the predictors.\n        This is used remove the first `window_size` (differentiation included) \n        values from y_train since they are not part of the training matrix.\n    metrics : list\n        List of metrics to calculate.\n    levels : list\n        Levels to calculate the metrics.\n    add_aggregated_metric : bool, default `True`\n        If `True`, and multiple series (`levels`) are predicted, the aggregated\n        metrics (average, weighted average and pooled) are also returned.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s).\n    \n    \"\"\"\n\n    if not isinstance(series, (pd.DataFrame, dict)):\n        raise TypeError(\n            (\"`series` must be a pandas DataFrame or a dictionary of pandas \"\n             \"DataFrames.\")\n        )\n    if not isinstance(predictions, pd.DataFrame):\n        raise TypeError(\"`predictions` must be a pandas DataFrame.\")\n    if not isinstance(folds, (list, tqdm)):\n        raise TypeError(\"`folds` must be a list or a tqdm object.\")\n    if not isinstance(span_index, (pd.DatetimeIndex, pd.RangeIndex)):\n        raise TypeError(\"`span_index` must be a pandas DatetimeIndex or pandas RangeIndex.\")\n    if not isinstance(window_size, (int, np.integer)):\n        raise TypeError(\"`window_size` must be an integer.\")\n    if not isinstance(metrics, list):\n        raise TypeError(\"`metrics` must be a list.\")\n    if not isinstance(levels, list):\n        raise TypeError(\"`levels` must be a list.\")\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError(\"`add_aggregated_metric` must be a boolean.\")\n    \n    metric_names = [(m if isinstance(m, str) else m.__name__) for m in metrics]\n\n    y_true_pred_levels = []\n    y_train_levels = []\n    for level in levels:\n        y_true_pred_level = None\n        y_train = None\n        if level in predictions.columns:\n            # TODO: avoid merges inside the loop, instead merge outside and then filter\n            y_true_pred_level = pd.merge(\n                series[level],\n                predictions[level],\n                left_index  = True,\n                right_index = True,\n                how         = \"inner\",\n            ).dropna(axis=0, how=\"any\")\n            y_true_pred_level.columns = ['y_true', 'y_pred']\n\n            train_indexes = []\n            for i, fold in enumerate(folds):\n                fit_fold = fold[-1]\n                if i == 0 or fit_fold:\n                    train_iloc_start = fold[0][0]\n                    train_iloc_end = fold[0][1]\n                    train_indexes.append(np.arange(train_iloc_start, train_iloc_end))\n            train_indexes = np.unique(np.concatenate(train_indexes))\n            train_indexes = span_index[train_indexes]\n            y_train = series[level].loc[series[level].index.intersection(train_indexes)]\n\n        y_true_pred_levels.append(y_true_pred_level)\n        y_train_levels.append(y_train)\n            \n    metrics_levels = []\n    for i, level in enumerate(levels):\n        if y_true_pred_levels[i] is not None and not y_true_pred_levels[i].empty:\n            metrics_level = [\n                m(\n                    y_true = y_true_pred_levels[i].iloc[:, 0],\n                    y_pred = y_true_pred_levels[i].iloc[:, 1],\n                    y_train = y_train_levels[i].iloc[window_size:]  # Exclude observations used to create predictors\n                )\n                for m in metrics\n            ]\n            metrics_levels.append(metrics_level)\n        else:\n            metrics_levels.append([None for _ in metrics])\n\n    metrics_levels = pd.DataFrame(\n                         data    = metrics_levels,\n                         columns = [m if isinstance(m, str) else m.__name__\n                                    for m in metrics]\n                     )\n    metrics_levels.insert(0, 'levels', levels)\n\n    if len(levels) < 2:\n        add_aggregated_metric = False\n    \n    if add_aggregated_metric:\n\n        # aggragation: average\n        average = metrics_levels.drop(columns='levels').mean(skipna=True)\n        average = average.to_frame().transpose()\n        average['levels'] = 'average'\n\n        # aggregation: weighted_average\n        weighted_averages = {}\n        n_predictions_levels = (\n            predictions\n            .notna()\n            .sum()\n            .to_frame(name='n_predictions')\n            .reset_index(names='levels')\n        )\n        metrics_levels_no_missing = (\n            metrics_levels.merge(n_predictions_levels, on='levels', how='inner')\n        )\n        for col in metric_names:\n            weighted_averages[col] = np.average(\n                metrics_levels_no_missing[col],\n                weights=metrics_levels_no_missing['n_predictions']\n            )\n        weighted_average = pd.DataFrame(weighted_averages, index=[0])\n        weighted_average['levels'] = 'weighted_average'\n\n        # aggregation: pooling\n        y_true_pred_levels, y_train_levels = zip(\n            *[\n                (a, b.iloc[window_size:])  # Exclude observations used to create predictors\n                for a, b in zip(y_true_pred_levels, y_train_levels)\n                if a is not None\n            ]\n        )\n        y_train_levels = list(y_train_levels)\n        y_true_pred_levels = pd.concat(y_true_pred_levels)\n        y_train_levels_concat = pd.concat(y_train_levels)\n\n        pooled = []\n        for m, m_name in zip(metrics, metric_names):\n            if m_name in ['mean_absolute_scaled_error', 'root_mean_squared_scaled_error']:\n                pooled.append(\n                    m(\n                        y_true = y_true_pred_levels.loc[:, 'y_true'],\n                        y_pred = y_true_pred_levels.loc[:, 'y_pred'],\n                        y_train = y_train_levels\n                    )\n                )\n            else:\n                pooled.append(\n                    m(\n                        y_true = y_true_pred_levels.loc[:, 'y_true'],\n                        y_pred = y_true_pred_levels.loc[:, 'y_pred'],\n                        y_train = y_train_levels_concat\n                    )\n                )\n        pooled = pd.DataFrame([pooled], columns=metric_names)\n        pooled['levels'] = 'pooling'\n\n        metrics_levels = pd.concat(\n            [metrics_levels, average, weighted_average, pooled],\n            axis=0,\n            ignore_index=True\n        )\n\n    return metrics_levels\n\n\ndef _predict_and_calculate_metrics_one_step_ahead_multiseries(\n    forecaster: object,\n    series: Union[pd.DataFrame, dict],\n    X_train: pd.DataFrame,\n    y_train: Union[pd.Series, dict],\n    X_test: pd.DataFrame,\n    y_test: Union[pd.Series, dict],\n    X_train_encoding: pd.Series,\n    X_test_encoding: pd.Series,\n    levels: list,\n    metrics: list,\n    add_aggregated_metric: bool = True\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"   \n    One-step-ahead predictions and metrics for each level and also for all levels\n    aggregated using average, weighted average or pooling.\n    Input matrices (X_train, y_train, X_train_encoding, X_test, y_test, X_test_encoding)\n    should have been generated using the forecaster._train_test_split_one_step_ahead().\n\n    - 'average': the average (arithmetic mean) of all levels.\n    - 'weighted_average': the average of the metrics weighted by the number of\n    predicted values of each level.\n    - 'pooling': the values of all levels are pooled and then the metric is\n    calculated.\n\n    Parameters\n    ----------\n    forecaster : object\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Series data used to train and test the forecaster.\n    X_train : pandas DataFrame\n        Training matrix.\n    y_train : pandas Series, dict\n        Target values of the training set.\n    X_test : pandas DataFrame\n        Test matrix.\n    y_test : pandas Series, dict\n        Target values of the test set.\n    X_train_encoding : pandas Series\n        Series identifiers for each row of `X_train`.\n    X_test_encoding : pandas Series\n        Series identifiers for each row of `X_test`.\n    levels : list\n        Levels to calculate the metrics.\n    metrics : list\n        List of metrics to calculate.\n    add_aggregated_metric : bool, default `True`\n        If `True`, and multiple series (`levels`) are predicted, the aggregated\n        metrics (average, weighted average and pooled) are also returned.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s).\n    predictions : pandas DataFrame\n        Value of predictions for each level.\n    \n    \"\"\"\n\n    if not isinstance(series, (pd.DataFrame, dict)):\n        raise TypeError(\n            \"`series` must be a pandas DataFrame or a dictionary of pandas \"\n            \"DataFrames.\"\n        )\n    if not isinstance(X_train, pd.DataFrame):\n        raise TypeError(f\"`X_train` must be a pandas DataFrame. Got: {type(X_train)}\")\n    if not isinstance(y_train, (pd.Series, dict)):\n        raise TypeError(\n            f\"`y_train` must be a pandas Series or a dictionary of pandas Series. \"\n            f\"Got: {type(y_train)}\"\n        )        \n    if not isinstance(X_test, pd.DataFrame):\n        raise TypeError(f\"`X_test` must be a pandas DataFrame. Got: {type(X_test)}\")\n    if not isinstance(y_test, (pd.Series, dict)):\n        raise TypeError(\n            f\"`y_test` must be a pandas Series or a dictionary of pandas Series. \"\n            f\"Got: {type(y_test)}\"\n        )\n    if not isinstance(X_train_encoding, pd.Series):\n        raise TypeError(\n            f\"`X_train_encoding` must be a pandas Series. Got: {type(X_train_encoding)}\"\n        )\n    if not isinstance(X_test_encoding, pd.Series):\n        raise TypeError(\n            f\"`X_test_encoding` must be a pandas Series. Got: {type(X_test_encoding)}\"\n        )\n    if not isinstance(levels, list):\n        raise TypeError(f\"`levels` must be a list. Got: {type(levels)}\")\n    if not isinstance(metrics, list):\n        raise TypeError(f\"`metrics` must be a list. Got: {type(metrics)}\")\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError(\n            f\"`add_aggregated_metric` must be a boolean. Got: {type(add_aggregated_metric)}\"\n        )\n    \n    metrics = [\n        _get_metric(metric=m)\n        if isinstance(m, str)\n        else add_y_train_argument(m) \n        for m in metrics\n    ]\n    metric_names = [(m if isinstance(m, str) else m.__name__) for m in metrics]\n\n    if isinstance(series[levels[0]].index, pd.DatetimeIndex):\n        freq = series[levels[0]].index.freq\n    else:\n        freq = series[levels[0]].index.step\n\n    if type(forecaster).__name__ == 'ForecasterDirectMultiVariate':\n        step = 1\n        X_train, y_train = forecaster.filter_train_X_y_for_step(\n                               step    = step,\n                               X_train = X_train,\n                               y_train = y_train\n                           )\n        X_test, y_test = forecaster.filter_train_X_y_for_step(\n                             step    = step,  \n                             X_train = X_test,\n                             y_train = y_test\n                         )                 \n        forecaster.regressors_[step].fit(X_train, y_train)\n        pred = forecaster.regressors_[step].predict(X_test)\n    else:\n        forecaster.regressor.fit(X_train, y_train)\n        pred = forecaster.regressor.predict(X_test)\n\n    predictions_per_level = pd.DataFrame(\n        {\n            'y_true': y_test,\n            'y_pred': pred,\n            '_level_skforecast': X_test_encoding,\n        },\n        index=y_test.index,\n    ).groupby('_level_skforecast')\n    predictions_per_level = {key: group for key, group in predictions_per_level}\n\n    y_train_per_level = pd.DataFrame(\n        {\"y_train\": y_train, \"_level_skforecast\": X_train_encoding},\n        index=y_train.index,\n    ).groupby(\"_level_skforecast\")\n    # Interleaved Nan values were excluded fom y_train. They are reestored\n    y_train_per_level = {key: group.asfreq(freq) for key, group in y_train_per_level}\n\n    if forecaster.differentiation is not None:\n        for level in predictions_per_level:\n            predictions_per_level[level][\"y_true\"] = (\n                forecaster.differentiator_[level].inverse_transform_next_window(\n                    predictions_per_level[level][\"y_true\"].to_numpy()\n                )\n            )\n            predictions_per_level[level][\"y_pred\"] = (\n                forecaster.differentiator_[level].inverse_transform_next_window(\n                    predictions_per_level[level][\"y_pred\"].to_numpy()\n                )   \n            )\n            y_train_per_level[level][\"y_train\"] = (\n                forecaster.differentiator_[level].inverse_transform_training(\n                    y_train_per_level[level][\"y_train\"].to_numpy()\n                )\n            )\n\n    if forecaster.transformer_series is not None:\n        for level in predictions_per_level:\n            transformer = forecaster.transformer_series_[level]\n            predictions_per_level[level][\"y_true\"] = transformer.inverse_transform(\n                predictions_per_level[level][[\"y_true\"]]\n            )\n            predictions_per_level[level][\"y_pred\"] = transformer.inverse_transform(\n                predictions_per_level[level][[\"y_pred\"]]\n            )\n            y_train_per_level[level][\"y_train\"] = transformer.inverse_transform(\n                y_train_per_level[level][[\"y_train\"]]\n            )\n    \n    metrics_levels = []\n    for level in levels:\n        if level in predictions_per_level:\n            metrics_level = [\n                m(\n                    y_true  = predictions_per_level[level].loc[:, 'y_true'],\n                    y_pred  = predictions_per_level[level].loc[:, 'y_pred'],\n                    y_train = y_train_per_level[level].loc[:, 'y_train']\n                )\n                for m in metrics\n            ]\n            metrics_levels.append(metrics_level)\n        else:\n            metrics_levels.append([None for _ in metrics])\n\n    metrics_levels = pd.DataFrame(\n                         data    = metrics_levels,\n                         columns = [m if isinstance(m, str) else m.__name__\n                                    for m in metrics]\n                     )\n    metrics_levels.insert(0, 'levels', levels)\n\n    if len(levels) < 2:\n        add_aggregated_metric = False\n\n    if add_aggregated_metric:\n\n        # aggragation: average\n        average = metrics_levels.drop(columns='levels').mean(skipna=True)\n        average = average.to_frame().transpose()\n        average['levels'] = 'average'\n\n        # aggregation: weighted_average\n        weighted_averages = {}\n        n_predictions_levels = {\n            k: v['y_pred'].notna().sum()\n            for k, v in predictions_per_level.items()\n        }\n        n_predictions_levels = pd.DataFrame(\n            n_predictions_levels.items(),\n            columns=['levels', 'n_predictions']\n        )\n        metrics_levels_no_missing = (\n            metrics_levels.merge(n_predictions_levels, on='levels', how='inner')\n        )\n        for col in metric_names:\n            weighted_averages[col] = np.average(\n                metrics_levels_no_missing[col],\n                weights=metrics_levels_no_missing['n_predictions']\n            )\n        weighted_average = pd.DataFrame(weighted_averages, index=[0])\n        weighted_average['levels'] = 'weighted_average'\n\n        # aggregation: pooling\n        list_y_train_by_level = [\n            v['y_train'].to_numpy()\n            for k, v in y_train_per_level.items()\n            if k in predictions_per_level\n        ]\n        predictions_pooled = pd.concat(predictions_per_level.values())\n        y_train_pooled = pd.concat(\n            [v for k, v in y_train_per_level.items() if k in predictions_per_level]\n        )\n        pooled = []\n        for m, m_name in zip(metrics, metric_names):\n            if m_name in ['mean_absolute_scaled_error', 'root_mean_squared_scaled_error']:\n                pooled.append(\n                    m(\n                        y_true  = predictions_pooled['y_true'],\n                        y_pred  = predictions_pooled['y_pred'],\n                        y_train = list_y_train_by_level\n                    )\n                )\n            else:\n                pooled.append(\n                    m(\n                        y_true  = predictions_pooled['y_true'],\n                        y_pred  = predictions_pooled['y_pred'],\n                        y_train = y_train_pooled['y_train']\n                    )\n                )\n        pooled = pd.DataFrame([pooled], columns=metric_names)\n        pooled['levels'] = 'pooling'\n\n        metrics_levels = pd.concat(\n            [metrics_levels, average, weighted_average, pooled],\n            axis=0,\n            ignore_index=True\n        )\n\n    predictions = (\n        pd.concat(predictions_per_level.values())\n        .loc[:, [\"y_pred\", \"_level_skforecast\"]]\n        .pivot(columns=\"_level_skforecast\", values=\"y_pred\")\n        .rename_axis(columns=None, index=None)\n    )\n    predictions = predictions.asfreq(X_test.index.freq)\n\n    return metrics_levels, predictions\n"
  },
  "GT_src_dict": {
    "skforecast/metrics/metrics.py": {
      "add_y_train_argument": {
        "code": "def add_y_train_argument(func: Callable) -> Callable:\n    \"\"\"Add `y_train` as a keyword-only argument to a given callable function if it is not already present in the function's signature. This allows for greater flexibility in statistical forecasting functions that require access to training data for performance metrics, such as Mean Absolute Scaled Error (MASE) or Root Mean Squared Scaled Error (RMSSE).\n\nParameters\n----------\nfunc : Callable\n    The original function to which the `y_train` argument will be added. \n\nReturns\n-------\nwrapper : Callable\n    The modified function with the `y_train` argument included.\n\nNotes\n-----\nThis function uses the `inspect` module to analyze the function's signature and modify it accordingly. The `wraps` decorator is used to preserve the original function\u2019s metadata in the wrapper function. The added argument `y_train` is intended for use in functions related to forecasting error metrics and is set as a keyword-only parameter with a default value of `None`. If `y_train` is already part of the original function's signature, the original function is returned unmodified, ensuring compatibility with existing implementations.\"\"\"\n    '\\n    Add `y_train` argument to a function if it is not already present.\\n\\n    Parameters\\n    ----------\\n    func : callable\\n        Function to which the argument is added.\\n\\n    Returns\\n    -------\\n    wrapper : callable\\n        Function with `y_train` argument added.\\n    \\n    '\n    sig = inspect.signature(func)\n    if 'y_train' in sig.parameters:\n        return func\n    new_params = list(sig.parameters.values()) + [inspect.Parameter('y_train', inspect.Parameter.KEYWORD_ONLY, default=None)]\n    new_sig = sig.replace(parameters=new_params)\n\n    @wraps(func)\n    def wrapper(*args, y_train=None, **kwargs):\n        \"\"\"Wrapper function that adds a `y_train` argument to a given function if it does not already exist. This is useful for metrics functions that require access to training data for calculations. The wrapper maintains the original function's parameters and signature.\n\nParameters\n----------\n*args : positional arguments\n    Arguments to be passed to the wrapped function.\ny_train : optional\n    Training data used for calculations, provided as a keyword-only argument.\n**kwargs : keyword arguments\n    Additional keyword arguments to be passed to the wrapped function.\n\nReturns\n-------\ncallable\n    The original function with the new `y_train` parameter.\n\nNotes\n-----\nThis wrapper leverages the `inspect` module to modify the function signature and `functools.wraps` to preserve the metadata of the original function. It is used in conjunction with the `_get_metric` function that retrieves specific metrics from the scikit-learn library.\"\"\"\n        return func(*args, **kwargs)\n    wrapper.__signature__ = new_sig\n    return wrapper",
        "docstring": "Add `y_train` as a keyword-only argument to a given callable function if it is not already present in the function's signature. This allows for greater flexibility in statistical forecasting functions that require access to training data for performance metrics, such as Mean Absolute Scaled Error (MASE) or Root Mean Squared Scaled Error (RMSSE).\n\nParameters\n----------\nfunc : Callable\n    The original function to which the `y_train` argument will be added. \n\nReturns\n-------\nwrapper : Callable\n    The modified function with the `y_train` argument included.\n\nNotes\n-----\nThis function uses the `inspect` module to analyze the function's signature and modify it accordingly. The `wraps` decorator is used to preserve the original function\u2019s metadata in the wrapper function. The added argument `y_train` is intended for use in functions related to forecasting error metrics and is set as a keyword-only parameter with a default value of `None`. If `y_train` is already part of the original function's signature, the original function is returned unmodified, ensuring compatibility with existing implementations.",
        "signature": "def add_y_train_argument(func: Callable) -> Callable:",
        "type": "Function",
        "class_signature": null
      },
      "mean_absolute_scaled_error": {
        "code": "def mean_absolute_scaled_error(y_true: Union[pd.Series, np.ndarray], y_pred: Union[pd.Series, np.ndarray], y_train: Union[list, pd.Series, np.ndarray]) -> float:\n    \"\"\"Mean Absolute Scaled Error (MASE)\n\nMASE is a scale-independent metric that evaluates the accuracy of a forecast by comparing the mean absolute error of the forecast against the mean absolute error of a naive forecast derived from the training set. It accommodates single time series or multiple time series forecasts by considering the naive forecast individually for each.\n\nParameters\n----------\ny_true : Union[pd.Series, np.ndarray]\n    True values of the target variable, either as a pandas Series or a numpy ndarray.\ny_pred : Union[pd.Series, np.ndarray]\n    Predicted values of the target variable, either as a pandas Series or a numpy ndarray.\ny_train : Union[list, pd.Series, np.ndarray]\n    True values from the training set, which may be provided as a list of pandas Series or numpy arrays, a single Series, or a numpy array. If a list, each element represents the true values for separate time series.\n\nReturns\n-------\nfloat\n    The calculated MASE value.\n\nRaises\n------\nTypeError\n    If `y_true`, `y_pred`, or `y_train` are of unexpected types.\nValueError\n    If `y_true` and `y_pred` do not match in length or if they are empty.\n\nDependencies\n------------\nThis function is dependent on NumPy for numerical operations and on pandas for handling time series data structures.\"\"\"\n    '\\n    Mean Absolute Scaled Error (MASE)\\n\\n    MASE is a scale-independent error metric that measures the accuracy of\\n    a forecast. It is the mean absolute error of the forecast divided by the\\n    mean absolute error of a naive forecast in the training set. The naive\\n    forecast is the one obtained by shifting the time series by one period.\\n    If y_train is a list of numpy arrays or pandas Series, it is considered\\n    that each element is the true value of the target variable in the training\\n    set for each time series. In this case, the naive forecast is calculated\\n    for each time series separately.\\n\\n    Parameters\\n    ----------\\n    y_true : pandas Series, numpy ndarray\\n        True values of the target variable.\\n    y_pred : pandas Series, numpy ndarray\\n        Predicted values of the target variable.\\n    y_train : list, pandas Series, numpy ndarray\\n        True values of the target variable in the training set. If `list`, it\\n        is consider that each element is the true value of the target variable\\n        in the training set for each time series.\\n\\n    Returns\\n    -------\\n    mase : float\\n        MASE value.\\n    \\n    '\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError('`y_true` must be a pandas Series or numpy ndarray.')\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError('`y_pred` must be a pandas Series or numpy ndarray.')\n    if not isinstance(y_train, (list, pd.Series, np.ndarray)):\n        raise TypeError('`y_train` must be a list, pandas Series or numpy ndarray.')\n    if isinstance(y_train, list):\n        for x in y_train:\n            if not isinstance(x, (pd.Series, np.ndarray)):\n                raise TypeError('When `y_train` is a list, each element must be a pandas Series or numpy ndarray.')\n    if len(y_true) != len(y_pred):\n        raise ValueError('`y_true` and `y_pred` must have the same length.')\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError('`y_true` and `y_pred` must have at least one element.')\n    if isinstance(y_train, list):\n        naive_forecast = np.concatenate([np.diff(x) for x in y_train])\n    else:\n        naive_forecast = np.diff(y_train)\n    mase = np.mean(np.abs(y_true - y_pred)) / np.nanmean(np.abs(naive_forecast))\n    return mase",
        "docstring": "Mean Absolute Scaled Error (MASE)\n\nMASE is a scale-independent metric that evaluates the accuracy of a forecast by comparing the mean absolute error of the forecast against the mean absolute error of a naive forecast derived from the training set. It accommodates single time series or multiple time series forecasts by considering the naive forecast individually for each.\n\nParameters\n----------\ny_true : Union[pd.Series, np.ndarray]\n    True values of the target variable, either as a pandas Series or a numpy ndarray.\ny_pred : Union[pd.Series, np.ndarray]\n    Predicted values of the target variable, either as a pandas Series or a numpy ndarray.\ny_train : Union[list, pd.Series, np.ndarray]\n    True values from the training set, which may be provided as a list of pandas Series or numpy arrays, a single Series, or a numpy array. If a list, each element represents the true values for separate time series.\n\nReturns\n-------\nfloat\n    The calculated MASE value.\n\nRaises\n------\nTypeError\n    If `y_true`, `y_pred`, or `y_train` are of unexpected types.\nValueError\n    If `y_true` and `y_pred` do not match in length or if they are empty.\n\nDependencies\n------------\nThis function is dependent on NumPy for numerical operations and on pandas for handling time series data structures.",
        "signature": "def mean_absolute_scaled_error(y_true: Union[pd.Series, np.ndarray], y_pred: Union[pd.Series, np.ndarray], y_train: Union[list, pd.Series, np.ndarray]) -> float:",
        "type": "Function",
        "class_signature": null
      },
      "wrapper": {
        "code": "    def wrapper(*args, y_train=None, **kwargs):\n        \"\"\"Wrapper function that adds a `y_train` argument to a given function if it does not already exist. This is useful for metrics functions that require access to training data for calculations. The wrapper maintains the original function's parameters and signature.\n\nParameters\n----------\n*args : positional arguments\n    Arguments to be passed to the wrapped function.\ny_train : optional\n    Training data used for calculations, provided as a keyword-only argument.\n**kwargs : keyword arguments\n    Additional keyword arguments to be passed to the wrapped function.\n\nReturns\n-------\ncallable\n    The original function with the new `y_train` parameter.\n\nNotes\n-----\nThis wrapper leverages the `inspect` module to modify the function signature and `functools.wraps` to preserve the metadata of the original function. It is used in conjunction with the `_get_metric` function that retrieves specific metrics from the scikit-learn library.\"\"\"\n        return func(*args, **kwargs)",
        "docstring": "Wrapper function that adds a `y_train` argument to a given function if it does not already exist. This is useful for metrics functions that require access to training data for calculations. The wrapper maintains the original function's parameters and signature.\n\nParameters\n----------\n*args : positional arguments\n    Arguments to be passed to the wrapped function.\ny_train : optional\n    Training data used for calculations, provided as a keyword-only argument.\n**kwargs : keyword arguments\n    Additional keyword arguments to be passed to the wrapped function.\n\nReturns\n-------\ncallable\n    The original function with the new `y_train` parameter.\n\nNotes\n-----\nThis wrapper leverages the `inspect` module to modify the function signature and `functools.wraps` to preserve the metadata of the original function. It is used in conjunction with the `_get_metric` function that retrieves specific metrics from the scikit-learn library.",
        "signature": "def wrapper(*args, y_train=None, **kwargs):",
        "type": "Function",
        "class_signature": null
      }
    },
    "skforecast/model_selection/_utils.py": {
      "_calculate_metrics_backtesting_multiseries": {
        "code": "def _calculate_metrics_backtesting_multiseries(series: Union[pd.DataFrame, dict], predictions: pd.DataFrame, folds: Union[list, tqdm], span_index: Union[pd.DatetimeIndex, pd.RangeIndex], window_size: int, metrics: list, levels: list, add_aggregated_metric: bool=True) -> pd.DataFrame:\n    \"\"\"Calculate evaluation metrics for each forecast level in a multi-series context and aggregate them across levels. The function supports various metrics and calculates average, weighted average, and pooled metrics over the predictions compared to the true values.\n\nParameters\n----------\nseries : Union[pd.DataFrame, dict]\n    The true values for the time series used during backtesting. This can be structured as a pandas DataFrame or a dictionary of DataFrames.\npredictions : pd.DataFrame\n    The predicted values generated during the backtesting process.\nfolds : Union[list, tqdm]\n    The folds created for backtesting, facilitating cross-validation.\nspan_index : Union[pd.DatetimeIndex, pd.RangeIndex]\n    The complete index covering the range of all series from minimum to maximum.\nwindow_size : int\n    The length of the window utilized to generate predictors, critical for determining the portion of data to exclude from training metrics.\nmetrics : list\n    A list of callable metrics to evaluate model performance.\nlevels : list\n    The specific levels for which metrics will be calculated.\nadd_aggregated_metric : bool, default=True\n    If True, includes aggregated metrics (average, weighted average, pooling) in the output.\n\nReturns\n-------\nmetrics_levels : pd.DataFrame\n    A DataFrame containing calculated metrics for each level, along with any specified aggregated metrics.\n\nThe function utilizes various constants and methods from the provided code, such as DataFrame merging, metric calculation functions, and error handling for data types. It assumes a structured input for series and predictions, ensuring validation checks are in place for all parameters.\"\"\"\n    \"   \\n    Calculate metrics for each level and also for all levels aggregated using\\n    average, weighted average or pooling.\\n\\n    - 'average': the average (arithmetic mean) of all levels.\\n    - 'weighted_average': the average of the metrics weighted by the number of\\n    predicted values of each level.\\n    - 'pooling': the values of all levels are pooled and then the metric is\\n    calculated.\\n\\n    Parameters\\n    ----------\\n    series : pandas DataFrame, dict\\n        Series data used for backtesting.\\n    predictions : pandas DataFrame\\n        Predictions generated during the backtesting process.\\n    folds : list, tqdm\\n        Folds created during the backtesting process.\\n    span_index : pandas DatetimeIndex, pandas RangeIndex\\n        Full index from the minimum to the maximum index among all series.\\n    window_size : int\\n        Size of the window used by the forecaster to create the predictors.\\n        This is used remove the first `window_size` (differentiation included) \\n        values from y_train since they are not part of the training matrix.\\n    metrics : list\\n        List of metrics to calculate.\\n    levels : list\\n        Levels to calculate the metrics.\\n    add_aggregated_metric : bool, default `True`\\n        If `True`, and multiple series (`levels`) are predicted, the aggregated\\n        metrics (average, weighted average and pooled) are also returned.\\n\\n        - 'average': the average (arithmetic mean) of all levels.\\n        - 'weighted_average': the average of the metrics weighted by the number of\\n        predicted values of each level.\\n        - 'pooling': the values of all levels are pooled and then the metric is\\n        calculated.\\n\\n    Returns\\n    -------\\n    metrics_levels : pandas DataFrame\\n        Value(s) of the metric(s).\\n    \\n    \"\n    if not isinstance(series, (pd.DataFrame, dict)):\n        raise TypeError('`series` must be a pandas DataFrame or a dictionary of pandas DataFrames.')\n    if not isinstance(predictions, pd.DataFrame):\n        raise TypeError('`predictions` must be a pandas DataFrame.')\n    if not isinstance(folds, (list, tqdm)):\n        raise TypeError('`folds` must be a list or a tqdm object.')\n    if not isinstance(span_index, (pd.DatetimeIndex, pd.RangeIndex)):\n        raise TypeError('`span_index` must be a pandas DatetimeIndex or pandas RangeIndex.')\n    if not isinstance(window_size, (int, np.integer)):\n        raise TypeError('`window_size` must be an integer.')\n    if not isinstance(metrics, list):\n        raise TypeError('`metrics` must be a list.')\n    if not isinstance(levels, list):\n        raise TypeError('`levels` must be a list.')\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError('`add_aggregated_metric` must be a boolean.')\n    metric_names = [m if isinstance(m, str) else m.__name__ for m in metrics]\n    y_true_pred_levels = []\n    y_train_levels = []\n    for level in levels:\n        y_true_pred_level = None\n        y_train = None\n        if level in predictions.columns:\n            y_true_pred_level = pd.merge(series[level], predictions[level], left_index=True, right_index=True, how='inner').dropna(axis=0, how='any')\n            y_true_pred_level.columns = ['y_true', 'y_pred']\n            train_indexes = []\n            for i, fold in enumerate(folds):\n                fit_fold = fold[-1]\n                if i == 0 or fit_fold:\n                    train_iloc_start = fold[0][0]\n                    train_iloc_end = fold[0][1]\n                    train_indexes.append(np.arange(train_iloc_start, train_iloc_end))\n            train_indexes = np.unique(np.concatenate(train_indexes))\n            train_indexes = span_index[train_indexes]\n            y_train = series[level].loc[series[level].index.intersection(train_indexes)]\n        y_true_pred_levels.append(y_true_pred_level)\n        y_train_levels.append(y_train)\n    metrics_levels = []\n    for i, level in enumerate(levels):\n        if y_true_pred_levels[i] is not None and (not y_true_pred_levels[i].empty):\n            metrics_level = [m(y_true=y_true_pred_levels[i].iloc[:, 0], y_pred=y_true_pred_levels[i].iloc[:, 1], y_train=y_train_levels[i].iloc[window_size:]) for m in metrics]\n            metrics_levels.append(metrics_level)\n        else:\n            metrics_levels.append([None for _ in metrics])\n    metrics_levels = pd.DataFrame(data=metrics_levels, columns=[m if isinstance(m, str) else m.__name__ for m in metrics])\n    metrics_levels.insert(0, 'levels', levels)\n    if len(levels) < 2:\n        add_aggregated_metric = False\n    if add_aggregated_metric:\n        average = metrics_levels.drop(columns='levels').mean(skipna=True)\n        average = average.to_frame().transpose()\n        average['levels'] = 'average'\n        weighted_averages = {}\n        n_predictions_levels = predictions.notna().sum().to_frame(name='n_predictions').reset_index(names='levels')\n        metrics_levels_no_missing = metrics_levels.merge(n_predictions_levels, on='levels', how='inner')\n        for col in metric_names:\n            weighted_averages[col] = np.average(metrics_levels_no_missing[col], weights=metrics_levels_no_missing['n_predictions'])\n        weighted_average = pd.DataFrame(weighted_averages, index=[0])\n        weighted_average['levels'] = 'weighted_average'\n        y_true_pred_levels, y_train_levels = zip(*[(a, b.iloc[window_size:]) for a, b in zip(y_true_pred_levels, y_train_levels) if a is not None])\n        y_train_levels = list(y_train_levels)\n        y_true_pred_levels = pd.concat(y_true_pred_levels)\n        y_train_levels_concat = pd.concat(y_train_levels)\n        pooled = []\n        for m, m_name in zip(metrics, metric_names):\n            if m_name in ['mean_absolute_scaled_error', 'root_mean_squared_scaled_error']:\n                pooled.append(m(y_true=y_true_pred_levels.loc[:, 'y_true'], y_pred=y_true_pred_levels.loc[:, 'y_pred'], y_train=y_train_levels))\n            else:\n                pooled.append(m(y_true=y_true_pred_levels.loc[:, 'y_true'], y_pred=y_true_pred_levels.loc[:, 'y_pred'], y_train=y_train_levels_concat))\n        pooled = pd.DataFrame([pooled], columns=metric_names)\n        pooled['levels'] = 'pooling'\n        metrics_levels = pd.concat([metrics_levels, average, weighted_average, pooled], axis=0, ignore_index=True)\n    return metrics_levels",
        "docstring": "Calculate evaluation metrics for each forecast level in a multi-series context and aggregate them across levels. The function supports various metrics and calculates average, weighted average, and pooled metrics over the predictions compared to the true values.\n\nParameters\n----------\nseries : Union[pd.DataFrame, dict]\n    The true values for the time series used during backtesting. This can be structured as a pandas DataFrame or a dictionary of DataFrames.\npredictions : pd.DataFrame\n    The predicted values generated during the backtesting process.\nfolds : Union[list, tqdm]\n    The folds created for backtesting, facilitating cross-validation.\nspan_index : Union[pd.DatetimeIndex, pd.RangeIndex]\n    The complete index covering the range of all series from minimum to maximum.\nwindow_size : int\n    The length of the window utilized to generate predictors, critical for determining the portion of data to exclude from training metrics.\nmetrics : list\n    A list of callable metrics to evaluate model performance.\nlevels : list\n    The specific levels for which metrics will be calculated.\nadd_aggregated_metric : bool, default=True\n    If True, includes aggregated metrics (average, weighted average, pooling) in the output.\n\nReturns\n-------\nmetrics_levels : pd.DataFrame\n    A DataFrame containing calculated metrics for each level, along with any specified aggregated metrics.\n\nThe function utilizes various constants and methods from the provided code, such as DataFrame merging, metric calculation functions, and error handling for data types. It assumes a structured input for series and predictions, ensuring validation checks are in place for all parameters.",
        "signature": "def _calculate_metrics_backtesting_multiseries(series: Union[pd.DataFrame, dict], predictions: pd.DataFrame, folds: Union[list, tqdm], span_index: Union[pd.DatetimeIndex, pd.RangeIndex], window_size: int, metrics: list, levels: list, add_aggregated_metric: bool=True) -> pd.DataFrame:",
        "type": "Function",
        "class_signature": null
      }
    }
  },
  "dependency_dict": {
    "skforecast/model_selection/_utils.py:_calculate_metrics_backtesting_multiseries": {},
    "skforecast/metrics/metrics.py:wrapper": {
      "skforecast/model_selection/tests/tests_utils/test_calculate_metrics_backtesting_multiseries.py": {
        "custom_metric": {
          "code": "def custom_metric(y_true, y_pred):  # pragma: no cover\n    \"\"\"\n    Calculate the mean absolute error excluding predictions between '2012-01-05'\n    and '2012-01-10'.\n    \"\"\"\n    mask = (y_true.index < '2012-01-05') | (y_true.index > '2012-01-10')\n    metric = mean_absolute_error(y_true[mask], y_pred[mask])\n    \n    return metric",
          "docstring": "Calculate the mean absolute error excluding predictions between '2012-01-05'\nand '2012-01-10'.",
          "signature": "def custom_metric(y_true, y_pred):",
          "type": "Function",
          "class_signature": null
        }
      }
    },
    "skforecast/metrics/metrics.py:mean_absolute_scaled_error": {}
  },
  "PRD": "# PROJECT NAME: skforecast-test_calculate_metrics_backtesting_multiseries\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 skforecast/\n    \u251c\u2500\u2500 metrics/\n    \u2502   \u2514\u2500\u2500 metrics.py\n    \u2502       \u251c\u2500\u2500 add_y_train_argument\n    \u2502       \u251c\u2500\u2500 mean_absolute_scaled_error\n    \u2502       \u2514\u2500\u2500 wrapper\n    \u2514\u2500\u2500 model_selection/\n        \u2514\u2500\u2500 _utils.py\n            \u2514\u2500\u2500 _calculate_metrics_backtesting_multiseries\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThe module facilitates comprehensive evaluation of multivariate time series forecasting by implementing backtesting metrics calculation across multiple series. It supports input validation, error handling, and flexible analysis by allowing users to define custom metrics and evaluate aggregated or individual metrics for different forecasted levels or items. By providing metrics such as mean absolute error and mean absolute scaled error, along with aggregation methods like averages and pooled metrics, it helps users objectively assess the accuracy of forecasting models. This enables developers and analysts to identify strengths and weaknesses in their predictions and optimize model performance more effectively.\n\n## FILE 1: skforecast/metrics/metrics.py\n\n- FUNCTION NAME: add_y_train_argument\n  - SIGNATURE: def add_y_train_argument(func: Callable) -> Callable:\n  - DOCSTRING: \n```python\n\"\"\"\nAdd `y_train` as a keyword-only argument to a given callable function if it is not already present in the function's signature. This allows for greater flexibility in statistical forecasting functions that require access to training data for performance metrics, such as Mean Absolute Scaled Error (MASE) or Root Mean Squared Scaled Error (RMSSE).\n\nParameters\n----------\nfunc : Callable\n    The original function to which the `y_train` argument will be added. \n\nReturns\n-------\nwrapper : Callable\n    The modified function with the `y_train` argument included.\n\nNotes\n-----\nThis function uses the `inspect` module to analyze the function's signature and modify it accordingly. The `wraps` decorator is used to preserve the original function\u2019s metadata in the wrapper function. The added argument `y_train` is intended for use in functions related to forecasting error metrics and is set as a keyword-only parameter with a default value of `None`. If `y_train` is already part of the original function's signature, the original function is returned unmodified, ensuring compatibility with existing implementations.\n\"\"\"\n```\n\n- FUNCTION NAME: wrapper\n  - SIGNATURE: def wrapper(*args, y_train=None, **kwargs):\n  - DOCSTRING: \n```python\n\"\"\"\nWrapper function that adds a `y_train` argument to a given function if it does not already exist. This is useful for metrics functions that require access to training data for calculations. The wrapper maintains the original function's parameters and signature.\n\nParameters\n----------\n*args : positional arguments\n    Arguments to be passed to the wrapped function.\ny_train : optional\n    Training data used for calculations, provided as a keyword-only argument.\n**kwargs : keyword arguments\n    Additional keyword arguments to be passed to the wrapped function.\n\nReturns\n-------\ncallable\n    The original function with the new `y_train` parameter.\n\nNotes\n-----\nThis wrapper leverages the `inspect` module to modify the function signature and `functools.wraps` to preserve the metadata of the original function. It is used in conjunction with the `_get_metric` function that retrieves specific metrics from the scikit-learn library.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - skforecast/model_selection/tests/tests_utils/test_calculate_metrics_backtesting_multiseries.py:custom_metric\n    - skforecast/model_selection/_utils.py:_calculate_metrics_backtesting_multiseries\n\n- FUNCTION NAME: mean_absolute_scaled_error\n  - SIGNATURE: def mean_absolute_scaled_error(y_true: Union[pd.Series, np.ndarray], y_pred: Union[pd.Series, np.ndarray], y_train: Union[list, pd.Series, np.ndarray]) -> float:\n  - DOCSTRING: \n```python\n\"\"\"\nMean Absolute Scaled Error (MASE)\n\nMASE is a scale-independent metric that evaluates the accuracy of a forecast by comparing the mean absolute error of the forecast against the mean absolute error of a naive forecast derived from the training set. It accommodates single time series or multiple time series forecasts by considering the naive forecast individually for each.\n\nParameters\n----------\ny_true : Union[pd.Series, np.ndarray]\n    True values of the target variable, either as a pandas Series or a numpy ndarray.\ny_pred : Union[pd.Series, np.ndarray]\n    Predicted values of the target variable, either as a pandas Series or a numpy ndarray.\ny_train : Union[list, pd.Series, np.ndarray]\n    True values from the training set, which may be provided as a list of pandas Series or numpy arrays, a single Series, or a numpy array. If a list, each element represents the true values for separate time series.\n\nReturns\n-------\nfloat\n    The calculated MASE value.\n\nRaises\n------\nTypeError\n    If `y_true`, `y_pred`, or `y_train` are of unexpected types.\nValueError\n    If `y_true` and `y_pred` do not match in length or if they are empty.\n\nDependencies\n------------\nThis function is dependent on NumPy for numerical operations and on pandas for handling time series data structures.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - skforecast/model_selection/_utils.py:_calculate_metrics_backtesting_multiseries\n\n## FILE 2: skforecast/model_selection/_utils.py\n\n- FUNCTION NAME: _calculate_metrics_backtesting_multiseries\n  - SIGNATURE: def _calculate_metrics_backtesting_multiseries(series: Union[pd.DataFrame, dict], predictions: pd.DataFrame, folds: Union[list, tqdm], span_index: Union[pd.DatetimeIndex, pd.RangeIndex], window_size: int, metrics: list, levels: list, add_aggregated_metric: bool=True) -> pd.DataFrame:\n  - DOCSTRING: \n```python\n\"\"\"\nCalculate evaluation metrics for each forecast level in a multi-series context and aggregate them across levels. The function supports various metrics and calculates average, weighted average, and pooled metrics over the predictions compared to the true values.\n\nParameters\n----------\nseries : Union[pd.DataFrame, dict]\n    The true values for the time series used during backtesting. This can be structured as a pandas DataFrame or a dictionary of DataFrames.\npredictions : pd.DataFrame\n    The predicted values generated during the backtesting process.\nfolds : Union[list, tqdm]\n    The folds created for backtesting, facilitating cross-validation.\nspan_index : Union[pd.DatetimeIndex, pd.RangeIndex]\n    The complete index covering the range of all series from minimum to maximum.\nwindow_size : int\n    The length of the window utilized to generate predictors, critical for determining the portion of data to exclude from training metrics.\nmetrics : list\n    A list of callable metrics to evaluate model performance.\nlevels : list\n    The specific levels for which metrics will be calculated.\nadd_aggregated_metric : bool, default=True\n    If True, includes aggregated metrics (average, weighted average, pooling) in the output.\n\nReturns\n-------\nmetrics_levels : pd.DataFrame\n    A DataFrame containing calculated metrics for each level, along with any specified aggregated metrics.\n\nThe function utilizes various constants and methods from the provided code, such as DataFrame merging, metric calculation functions, and error handling for data types. It assumes a structured input for series and predictions, ensuring validation checks are in place for all parameters.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - skforecast/metrics/metrics.py:wrapper\n    - skforecast/metrics/metrics.py:mean_absolute_scaled_error\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "skforecast/metrics/metrics.py": "from typing import Union, Callable\nimport numpy as np\nimport pandas as pd\nimport inspect\nfrom functools import wraps\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, mean_squared_log_error, median_absolute_error\n\ndef _get_metric(metric: str) -> Callable:\n    \"\"\"\n    Get the corresponding scikit-learn function to calculate the metric.\n\n    Parameters\n    ----------\n    metric : str\n        Metric used to quantify the goodness of fit of the model.\n\n    Returns\n    -------\n    metric : Callable\n        scikit-learn function to calculate the desired metric.\n\n    \"\"\"\n    allowed_metrics = ['mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error', 'median_absolute_error']\n    if metric not in allowed_metrics:\n        raise ValueError(f'Allowed metrics are: {allowed_metrics}. Got {metric}.')\n    metrics = {'mean_squared_error': mean_squared_error, 'mean_absolute_error': mean_absolute_error, 'mean_absolute_percentage_error': mean_absolute_percentage_error, 'mean_squared_log_error': mean_squared_log_error, 'mean_absolute_scaled_error': mean_absolute_scaled_error, 'root_mean_squared_scaled_error': root_mean_squared_scaled_error, 'median_absolute_error': median_absolute_error}\n    metric = add_y_train_argument(metrics[metric])\n    return metric\n\ndef root_mean_squared_scaled_error(y_true: Union[pd.Series, np.ndarray], y_pred: Union[pd.Series, np.ndarray], y_train: Union[list, pd.Series, np.ndarray]) -> float:\n    \"\"\"\n    Root Mean Squared Scaled Error (RMSSE)\n\n    RMSSE is a scale-independent error metric that measures the accuracy of\n    a forecast. It is the root mean squared error of the forecast divided by\n    the root mean squared error of a naive forecast in the training set. The\n    naive forecast is the one obtained by shifting the time series by one period.\n    If y_train is a list of numpy arrays or pandas Series, it is considered\n    that each element is the true value of the target variable in the training\n    set for each time series. In this case, the naive forecast is calculated\n    for each time series separately.\n\n    Parameters\n    ----------\n    y_true : pandas Series, numpy ndarray\n        True values of the target variable.\n    y_pred : pandas Series, numpy ndarray\n        Predicted values of the target variable.\n    y_train : list, pandas Series, numpy ndarray\n        True values of the target variable in the training set. If list, it\n        is consider that each element is the true value of the target variable\n        in the training set for each time series.\n\n    Returns\n    -------\n    rmsse : float\n        RMSSE value.\n    \n    \"\"\"\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError('`y_true` must be a pandas Series or numpy ndarray.')\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError('`y_pred` must be a pandas Series or numpy ndarray.')\n    if not isinstance(y_train, (list, pd.Series, np.ndarray)):\n        raise TypeError('`y_train` must be a list, pandas Series or numpy ndarray.')\n    if isinstance(y_train, list):\n        for x in y_train:\n            if not isinstance(x, (pd.Series, np.ndarray)):\n                raise TypeError('When `y_train` is a list, each element must be a pandas Series or numpy ndarray.')\n    if len(y_true) != len(y_pred):\n        raise ValueError('`y_true` and `y_pred` must have the same length.')\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError('`y_true` and `y_pred` must have at least one element.')\n    if isinstance(y_train, list):\n        naive_forecast = np.concatenate([np.diff(x) for x in y_train])\n    else:\n        naive_forecast = np.diff(y_train)\n    rmsse = np.sqrt(np.mean((y_true - y_pred) ** 2)) / np.sqrt(np.nanmean(naive_forecast ** 2))\n    return rmsse",
    "skforecast/model_selection/_utils.py": "from typing import Union, Tuple, Optional, Callable, Generator\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom joblib import cpu_count\nfrom tqdm.auto import tqdm\nfrom sklearn.pipeline import Pipeline\nimport sklearn.linear_model\nfrom sklearn.exceptions import NotFittedError\nfrom ..exceptions import IgnoredArgumentWarning\nfrom ..metrics import add_y_train_argument, _get_metric\nfrom ..utils import check_interval\n\ndef initialize_lags_grid(forecaster: object, lags_grid: Optional[Union[list, dict]]=None) -> Tuple[dict, str]:\n    \"\"\"\n    Initialize lags grid and lags label for model selection. \n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model. ForecasterRecursive, ForecasterDirect, \n        ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate.\n    lags_grid : list, dict, default `None`\n        Lists of lags to try, containing int, lists, numpy ndarray, or range \n        objects. If `dict`, the keys are used as labels in the `results` \n        DataFrame, and the values are used as the lists of lags to try.\n\n    Returns\n    -------\n    lags_grid : dict\n        Dictionary with lags configuration for each iteration.\n    lags_label : str\n        Label for lags representation in the results object.\n\n    \"\"\"\n    if not isinstance(lags_grid, (list, dict, type(None))):\n        raise TypeError(f'`lags_grid` argument must be a list, dict or None. Got {type(lags_grid)}.')\n    lags_label = 'values'\n    if isinstance(lags_grid, list):\n        lags_grid = {f'{lags}': lags for lags in lags_grid}\n    elif lags_grid is None:\n        lags = [int(lag) for lag in forecaster.lags]\n        lags_grid = {f'{lags}': lags}\n    else:\n        lags_label = 'keys'\n    return (lags_grid, lags_label)\n\ndef check_backtesting_input(forecaster: object, cv: object, metric: Union[str, Callable, list], add_aggregated_metric: bool=True, y: Optional[pd.Series]=None, series: Optional[Union[pd.DataFrame, dict]]=None, exog: Optional[Union[pd.Series, pd.DataFrame, dict]]=None, interval: Optional[list]=None, alpha: Optional[float]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False, n_jobs: Union[int, str]='auto', show_progress: bool=True, suppress_warnings: bool=False, suppress_warnings_fit: bool=False) -> None:\n    \"\"\"\n    This is a helper function to check most inputs of backtesting functions in \n    modules `model_selection`.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model.\n    cv : TimeSeriesFold\n        TimeSeriesFold object with the information needed to split the data into folds.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n    add_aggregated_metric : bool, default `True`\n        If `True`, the aggregated metrics (average, weighted average and pooling)\n        over all levels are also returned (only multiseries).\n    y : pandas Series, default `None`\n        Training time series for uni-series forecasters.\n    series : pandas DataFrame, dict, default `None`\n        Training time series for multi-series forecasters.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive.\n    alpha : float, default `None`\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %. \n    n_boot : int, default `250`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of prediction \n        error to create prediction intervals.  If `False`, out_sample_residuals \n        are used if they are already stored inside the forecaster.\n    use_binned_residuals : bool, default `False`\n        If `True`, residuals used in each bootstrapping iteration are selected\n        conditioning on the predicted values. If `False`, residuals are selected\n        randomly without conditioning on the predicted values.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the fuction\n        skforecast.utils.select_n_jobs_fit_forecaster.\n        **New in version 0.9.0**\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n    suppress_warnings: bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the backtesting \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    suppress_warnings_fit : bool, default `False`\n        If `True`, warnings generated during fitting will be ignored. Only \n        `ForecasterSarimax`.\n\n    Returns\n    -------\n    None\n    \n    \"\"\"\n    forecaster_name = type(forecaster).__name__\n    cv_name = type(cv).__name__\n    if cv_name != 'TimeSeriesFold':\n        raise TypeError(f'`cv` must be a TimeSeriesFold object. Got {cv_name}.')\n    steps = cv.steps\n    initial_train_size = cv.initial_train_size\n    gap = cv.gap\n    allow_incomplete_fold = cv.allow_incomplete_fold\n    refit = cv.refit\n    forecasters_uni = ['ForecasterRecursive', 'ForecasterDirect', 'ForecasterSarimax', 'ForecasterEquivalentDate']\n    forecasters_multi = ['ForecasterDirectMultiVariate', 'ForecasterRnn']\n    forecasters_multi_dict = ['ForecasterRecursiveMultiSeries']\n    if forecaster_name in forecasters_uni:\n        if not isinstance(y, pd.Series):\n            raise TypeError('`y` must be a pandas Series.')\n        data_name = 'y'\n        data_length = len(y)\n    elif forecaster_name in forecasters_multi:\n        if not isinstance(series, pd.DataFrame):\n            raise TypeError('`series` must be a pandas DataFrame.')\n        data_name = 'series'\n        data_length = len(series)\n    elif forecaster_name in forecasters_multi_dict:\n        if not isinstance(series, (pd.DataFrame, dict)):\n            raise TypeError(f'`series` must be a pandas DataFrame or a dict of DataFrames or Series. Got {type(series)}.')\n        data_name = 'series'\n        if isinstance(series, dict):\n            not_valid_series = [k for k, v in series.items() if not isinstance(v, (pd.Series, pd.DataFrame))]\n            if not_valid_series:\n                raise TypeError(f'If `series` is a dictionary, all series must be a named pandas Series or a pandas DataFrame with a single column. Review series: {not_valid_series}')\n            not_valid_index = [k for k, v in series.items() if not isinstance(v.index, pd.DatetimeIndex)]\n            if not_valid_index:\n                raise ValueError(f'If `series` is a dictionary, all series must have a Pandas DatetimeIndex as index with the same frequency. Review series: {not_valid_index}')\n            indexes_freq = [f'{v.index.freq}' for v in series.values()]\n            indexes_freq = sorted(set(indexes_freq))\n            if not len(indexes_freq) == 1:\n                raise ValueError(f'If `series` is a dictionary, all series must have a Pandas DatetimeIndex as index with the same frequency. Found frequencies: {indexes_freq}')\n            data_length = max([len(series[serie]) for serie in series])\n        else:\n            data_length = len(series)\n    if exog is not None:\n        if forecaster_name in forecasters_multi_dict:\n            if not isinstance(exog, (pd.Series, pd.DataFrame, dict)):\n                raise TypeError(f'`exog` must be a pandas Series, DataFrame, dictionary of pandas Series/DataFrames or None. Got {type(exog)}.')\n            if isinstance(exog, dict):\n                not_valid_exog = [k for k, v in exog.items() if not isinstance(v, (pd.Series, pd.DataFrame, type(None)))]\n                if not_valid_exog:\n                    raise TypeError(f'If `exog` is a dictionary, All exog must be a named pandas Series, a pandas DataFrame or None. Review exog: {not_valid_exog}')\n        elif not isinstance(exog, (pd.Series, pd.DataFrame)):\n            raise TypeError(f'`exog` must be a pandas Series, DataFrame or None. Got {type(exog)}.')\n    if hasattr(forecaster, 'differentiation'):\n        if forecaster.differentiation != cv.differentiation:\n            raise ValueError(f'The differentiation included in the forecaster ({forecaster.differentiation}) differs from the differentiation included in the cv ({cv.differentiation}). Set the same value for both using the `differentiation` argument.')\n    if not isinstance(metric, (str, Callable, list)):\n        raise TypeError(f'`metric` must be a string, a callable function, or a list containing multiple strings and/or callables. Got {type(metric)}.')\n    if forecaster_name == 'ForecasterEquivalentDate' and isinstance(forecaster.offset, pd.tseries.offsets.DateOffset):\n        if initial_train_size is None:\n            raise ValueError(f'`initial_train_size` must be an integer greater than the `window_size` of the forecaster ({forecaster.window_size}) and smaller than the length of `{data_name}` ({data_length}).')\n    elif initial_train_size is not None:\n        if initial_train_size < forecaster.window_size or initial_train_size >= data_length:\n            raise ValueError(f'If used, `initial_train_size` must be an integer greater than the `window_size` of the forecaster ({forecaster.window_size}) and smaller than the length of `{data_name}` ({data_length}).')\n        if initial_train_size + gap >= data_length:\n            raise ValueError(f'The combination of initial_train_size {initial_train_size} and gap {gap} cannot be greater than the length of `{data_name}` ({data_length}).')\n    elif forecaster_name in ['ForecasterSarimax', 'ForecasterEquivalentDate']:\n        raise ValueError(f'`initial_train_size` must be an integer smaller than the length of `{data_name}` ({data_length}).')\n    else:\n        if not forecaster.is_fitted:\n            raise NotFittedError('`forecaster` must be already trained if no `initial_train_size` is provided.')\n        if refit:\n            raise ValueError('`refit` is only allowed when `initial_train_size` is not `None`.')\n    if forecaster_name == 'ForecasterSarimax' and cv.skip_folds is not None:\n        raise ValueError('`skip_folds` is not allowed for ForecasterSarimax. Set it to `None`.')\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError('`add_aggregated_metric` must be a boolean: `True`, `False`.')\n    if not isinstance(n_boot, (int, np.integer)) or n_boot < 0:\n        raise TypeError(f'`n_boot` must be an integer greater than 0. Got {n_boot}.')\n    if not isinstance(random_state, (int, np.integer)) or random_state < 0:\n        raise TypeError(f'`random_state` must be an integer greater than 0. Got {random_state}.')\n    if not isinstance(use_in_sample_residuals, bool):\n        raise TypeError('`use_in_sample_residuals` must be a boolean: `True`, `False`.')\n    if not isinstance(use_binned_residuals, bool):\n        raise TypeError('`use_binned_residuals` must be a boolean: `True`, `False`.')\n    if not isinstance(n_jobs, int) and n_jobs != 'auto':\n        raise TypeError(f\"`n_jobs` must be an integer or `'auto'`. Got {n_jobs}.\")\n    if not isinstance(show_progress, bool):\n        raise TypeError('`show_progress` must be a boolean: `True`, `False`.')\n    if not isinstance(suppress_warnings, bool):\n        raise TypeError('`suppress_warnings` must be a boolean: `True`, `False`.')\n    if not isinstance(suppress_warnings_fit, bool):\n        raise TypeError('`suppress_warnings_fit` must be a boolean: `True`, `False`.')\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n    if not allow_incomplete_fold and data_length - (initial_train_size + gap) < steps:\n        raise ValueError(f'There is not enough data to evaluate {steps} steps in a single fold. Set `allow_incomplete_fold` to `True` to allow incomplete folds.\\n    Data available for test : {data_length - (initial_train_size + gap)}\\n    Steps                   : {steps}')\n\ndef select_n_jobs_backtesting(forecaster: object, refit: Union[bool, int]) -> int:\n    \"\"\"\n    Select the optimal number of jobs to use in the backtesting process. This\n    selection is based on heuristics and is not guaranteed to be optimal.\n\n    The number of jobs is chosen as follows:\n\n    - If `refit` is an integer, then `n_jobs = 1`. This is because parallelization doesn't \n    work with intermittent refit.\n    - If forecaster is 'ForecasterRecursive' and regressor is a linear regressor, \n    then `n_jobs = 1`.\n    - If forecaster is 'ForecasterRecursive' and regressor is not a linear \n    regressor then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and `refit = True`, then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and `refit = False`, then `n_jobs = 1`.\n    - If forecaster is 'ForecasterRecursiveMultiSeries', then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterSarimax' or 'ForecasterEquivalentDate', \n    then `n_jobs = 1`.\n    - If regressor is a `LGBMRegressor(n_jobs=1)`, then `n_jobs = cpu_count() - 1`.\n    - If regressor is a `LGBMRegressor` with internal n_jobs != 1, then `n_jobs = 1`.\n    This is because `lightgbm` is highly optimized for gradient boosting and\n    parallelizes operations at a very fine-grained level, making additional\n    parallelization unnecessary and potentially harmful due to resource contention.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model.\n    refit : bool, int\n        If the forecaster is refitted during the backtesting process.\n\n    Returns\n    -------\n    n_jobs : int\n        The number of jobs to run in parallel.\n    \n    \"\"\"\n    forecaster_name = type(forecaster).__name__\n    if isinstance(forecaster.regressor, Pipeline):\n        regressor = forecaster.regressor[-1]\n        regressor_name = type(regressor).__name__\n    else:\n        regressor = forecaster.regressor\n        regressor_name = type(regressor).__name__\n    linear_regressors = [regressor_name for regressor_name in dir(sklearn.linear_model) if not regressor_name.startswith('_')]\n    refit = False if refit == 0 else refit\n    if not isinstance(refit, bool) and refit != 1:\n        n_jobs = 1\n    elif forecaster_name in ['ForecasterRecursive']:\n        if regressor_name in linear_regressors:\n            n_jobs = 1\n        elif regressor_name == 'LGBMRegressor':\n            n_jobs = cpu_count() - 1 if regressor.n_jobs == 1 else 1\n        else:\n            n_jobs = cpu_count() - 1\n    elif forecaster_name in ['ForecasterDirect', 'ForecasterDirectMultiVariate']:\n        n_jobs = 1\n    elif forecaster_name in ['ForecasterRecursiveMultiSeries']:\n        if regressor_name == 'LGBMRegressor':\n            n_jobs = cpu_count() - 1 if regressor.n_jobs == 1 else 1\n        else:\n            n_jobs = cpu_count() - 1\n    elif forecaster_name in ['ForecasterSarimax', 'ForecasterEquivalentDate']:\n        n_jobs = 1\n    else:\n        n_jobs = 1\n    return n_jobs\n\ndef _calculate_metrics_one_step_ahead(forecaster: object, y: pd.Series, metrics: list, X_train: pd.DataFrame, y_train: Union[pd.Series, dict], X_test: pd.DataFrame, y_test: Union[pd.Series, dict]) -> list:\n    \"\"\"\n    Calculate metrics when predictions are one-step-ahead. When forecaster is\n    of type ForecasterDirect only the regressor for step 1 is used.\n\n    Parameters\n    ----------\n    forecaster : object\n        Forecaster model.\n    y : pandas Series\n        Time series data used to train and test the model.\n    metrics : list\n        List of metrics.\n    X_train : pandas DataFrame\n        Predictor values used to train the model.\n    y_train : pandas Series\n        Target values related to each row of `X_train`.\n    X_test : pandas DataFrame\n        Predictor values used to test the model.\n    y_test : pandas Series\n        Target values related to each row of `X_test`.\n\n    Returns\n    -------\n    metric_values : list\n        List with metric values.\n    \n    \"\"\"\n    if type(forecaster).__name__ == 'ForecasterDirect':\n        step = 1\n        X_train, y_train = forecaster.filter_train_X_y_for_step(step=step, X_train=X_train, y_train=y_train)\n        X_test, y_test = forecaster.filter_train_X_y_for_step(step=step, X_train=X_test, y_train=y_test)\n        forecaster.regressors_[step].fit(X_train, y_train)\n        y_pred = forecaster.regressors_[step].predict(X_test)\n    else:\n        forecaster.regressor.fit(X_train, y_train)\n        y_pred = forecaster.regressor.predict(X_test)\n    y_true = y_test.to_numpy()\n    y_pred = y_pred.ravel()\n    y_train = y_train.to_numpy()\n    if forecaster.differentiation is not None:\n        y_true = forecaster.differentiator.inverse_transform_next_window(y_true)\n        y_pred = forecaster.differentiator.inverse_transform_next_window(y_pred)\n        y_train = forecaster.differentiator.inverse_transform_training(y_train)\n    if forecaster.transformer_y is not None:\n        y_true = forecaster.transformer_y.inverse_transform(y_true.reshape(-1, 1))\n        y_pred = forecaster.transformer_y.inverse_transform(y_pred.reshape(-1, 1))\n        y_train = forecaster.transformer_y.inverse_transform(y_train.reshape(-1, 1))\n    metric_values = []\n    for m in metrics:\n        metric_values.append(m(y_true=y_true.ravel(), y_pred=y_pred.ravel(), y_train=y_train.ravel()))\n    return metric_values\n\ndef _initialize_levels_model_selection_multiseries(forecaster: object, series: Union[pd.DataFrame, dict], levels: Optional[Union[str, list]]=None) -> list:\n    \"\"\"\n    Initialize levels for model_selection multi-series functions.\n\n    Parameters\n    ----------\n    forecaster : ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate, ForecasterRnn\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Training time series.\n    levels : str, list, default `None`\n        level (`str`) or levels (`list`) at which the forecaster is optimized. \n        If `None`, all levels are taken into account. The resulting metric will be\n        the average of the optimization of all levels.\n\n    Returns\n    -------\n    levels : list\n        List of levels to be used in model_selection multi-series functions.\n    \n    \"\"\"\n    multi_series_forecasters_with_levels = ['ForecasterRecursiveMultiSeries', 'ForecasterRnn']\n    if type(forecaster).__name__ in multi_series_forecasters_with_levels and (not isinstance(levels, (str, list, type(None)))):\n        raise TypeError(f'`levels` must be a `list` of column names, a `str` of a column name or `None` when using a forecaster of type {multi_series_forecasters_with_levels}. If the forecaster is of type `ForecasterDirectMultiVariate`, this argument is ignored.')\n    if type(forecaster).__name__ == 'ForecasterDirectMultiVariate':\n        if levels and levels != forecaster.level and (levels != [forecaster.level]):\n            warnings.warn(f\"`levels` argument have no use when the forecaster is of type `ForecasterDirectMultiVariate`. The level of this forecaster is '{forecaster.level}', to predict another level, change the `level` argument when initializing the forecaster. \\n\", IgnoredArgumentWarning)\n        levels = [forecaster.level]\n    elif levels is None:\n        if isinstance(series, pd.DataFrame):\n            levels = list(series.columns)\n        else:\n            levels = list(series.keys())\n    elif isinstance(levels, str):\n        levels = [levels]\n    return levels\n\ndef _extract_data_folds_multiseries(series: Union[pd.Series, pd.DataFrame, dict], folds: list, span_index: Union[pd.DatetimeIndex, pd.RangeIndex], window_size: int, exog: Optional[Union[pd.Series, pd.DataFrame, dict]]=None, dropna_last_window: bool=False, externally_fitted: bool=False) -> Generator[Tuple[Union[pd.Series, pd.DataFrame, dict], pd.DataFrame, list, Optional[Union[pd.Series, pd.DataFrame, dict]], Optional[Union[pd.Series, pd.DataFrame, dict]], list], None, None]:\n    \"\"\"\n    Select the data from series and exog that corresponds to each fold created using the\n    skforecast.model_selection._create_backtesting_folds function.\n\n    Parameters\n    ----------\n    series : pandas Series, pandas DataFrame, dict\n        Time series.\n    folds : list\n        Folds created using the skforecast.model_selection._create_backtesting_folds\n        function.\n    span_index : pandas DatetimeIndex, pandas RangeIndex\n        Full index from the minimum to the maximum index among all series.\n    window_size : int\n        Size of the window needed to create the predictors.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    dropna_last_window : bool, default `False`\n        If `True`, drop the columns of the last window that have NaN values.\n    externally_fitted : bool, default `False`\n        Flag indicating whether the forecaster is already trained. Only used when \n        `initial_train_size` is None and `refit` is False.\n\n    Yield\n    -----\n    series_train : pandas Series, pandas DataFrame, dict\n        Time series corresponding to the training set of the fold.\n    series_last_window: pandas DataFrame\n        Time series corresponding to the last window of the fold.\n    levels_last_window: list\n        Levels of the time series present in the last window of the fold.\n    exog_train: pandas Series, pandas DataFrame, dict, None\n        Exogenous variable corresponding to the training set of the fold.\n    exog_test: pandas Series, pandas DataFrame, dict, None\n        Exogenous variable corresponding to the test set of the fold.\n    fold: list\n        Fold created using the skforecast.model_selection._create_backtesting_folds\n\n    \"\"\"\n    for fold in folds:\n        train_iloc_start = fold[0][0]\n        train_iloc_end = fold[0][1]\n        last_window_iloc_start = fold[1][0]\n        last_window_iloc_end = fold[1][1]\n        test_iloc_start = fold[2][0]\n        test_iloc_end = fold[2][1]\n        if isinstance(series, dict) or isinstance(exog, dict):\n            train_loc_start = span_index[train_iloc_start]\n            train_loc_end = span_index[train_iloc_end - 1]\n            last_window_loc_start = span_index[last_window_iloc_start]\n            last_window_loc_end = span_index[last_window_iloc_end - 1]\n            test_loc_start = span_index[test_iloc_start]\n            test_loc_end = span_index[test_iloc_end - 1]\n        if isinstance(series, pd.DataFrame):\n            series_train = series.iloc[train_iloc_start:train_iloc_end,]\n            series_to_drop = []\n            for col in series_train.columns:\n                if series_train[col].isna().all():\n                    series_to_drop.append(col)\n                else:\n                    first_valid_index = series_train[col].first_valid_index()\n                    last_valid_index = series_train[col].last_valid_index()\n                    if len(series_train[col].loc[first_valid_index:last_valid_index]) < window_size:\n                        series_to_drop.append(col)\n            series_last_window = series.iloc[last_window_iloc_start:last_window_iloc_end,]\n            series_train = series_train.drop(columns=series_to_drop)\n            if not externally_fitted:\n                series_last_window = series_last_window.drop(columns=series_to_drop)\n        else:\n            series_train = {}\n            for k in series.keys():\n                v = series[k].loc[train_loc_start:train_loc_end]\n                if not v.isna().all():\n                    first_valid_index = v.first_valid_index()\n                    last_valid_index = v.last_valid_index()\n                    if first_valid_index is not None and last_valid_index is not None:\n                        v = v.loc[first_valid_index:last_valid_index]\n                        if len(v) >= window_size:\n                            series_train[k] = v\n            series_last_window = {}\n            for k, v in series.items():\n                v = series[k].loc[last_window_loc_start:last_window_loc_end]\n                if (externally_fitted or k in series_train) and len(v) >= window_size:\n                    series_last_window[k] = v\n            series_last_window = pd.DataFrame(series_last_window)\n        if dropna_last_window:\n            series_last_window = series_last_window.dropna(axis=1, how='any')\n        levels_last_window = list(series_last_window.columns)\n        if exog is not None:\n            if isinstance(exog, (pd.Series, pd.DataFrame)):\n                exog_train = exog.iloc[train_iloc_start:train_iloc_end,]\n                exog_test = exog.iloc[test_iloc_start:test_iloc_end,]\n            else:\n                exog_train = {k: v.loc[train_loc_start:train_loc_end] for k, v in exog.items()}\n                exog_train = {k: v for k, v in exog_train.items() if len(v) > 0}\n                exog_test = {k: v.loc[test_loc_start:test_loc_end] for k, v in exog.items() if externally_fitted or k in exog_train}\n                exog_test = {k: v for k, v in exog_test.items() if len(v) > 0}\n        else:\n            exog_train = None\n            exog_test = None\n        yield (series_train, series_last_window, levels_last_window, exog_train, exog_test, fold)\n\ndef _predict_and_calculate_metrics_one_step_ahead_multiseries(forecaster: object, series: Union[pd.DataFrame, dict], X_train: pd.DataFrame, y_train: Union[pd.Series, dict], X_test: pd.DataFrame, y_test: Union[pd.Series, dict], X_train_encoding: pd.Series, X_test_encoding: pd.Series, levels: list, metrics: list, add_aggregated_metric: bool=True) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"   \n    One-step-ahead predictions and metrics for each level and also for all levels\n    aggregated using average, weighted average or pooling.\n    Input matrices (X_train, y_train, X_train_encoding, X_test, y_test, X_test_encoding)\n    should have been generated using the forecaster._train_test_split_one_step_ahead().\n\n    - 'average': the average (arithmetic mean) of all levels.\n    - 'weighted_average': the average of the metrics weighted by the number of\n    predicted values of each level.\n    - 'pooling': the values of all levels are pooled and then the metric is\n    calculated.\n\n    Parameters\n    ----------\n    forecaster : object\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Series data used to train and test the forecaster.\n    X_train : pandas DataFrame\n        Training matrix.\n    y_train : pandas Series, dict\n        Target values of the training set.\n    X_test : pandas DataFrame\n        Test matrix.\n    y_test : pandas Series, dict\n        Target values of the test set.\n    X_train_encoding : pandas Series\n        Series identifiers for each row of `X_train`.\n    X_test_encoding : pandas Series\n        Series identifiers for each row of `X_test`.\n    levels : list\n        Levels to calculate the metrics.\n    metrics : list\n        List of metrics to calculate.\n    add_aggregated_metric : bool, default `True`\n        If `True`, and multiple series (`levels`) are predicted, the aggregated\n        metrics (average, weighted average and pooled) are also returned.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s).\n    predictions : pandas DataFrame\n        Value of predictions for each level.\n    \n    \"\"\"\n    if not isinstance(series, (pd.DataFrame, dict)):\n        raise TypeError('`series` must be a pandas DataFrame or a dictionary of pandas DataFrames.')\n    if not isinstance(X_train, pd.DataFrame):\n        raise TypeError(f'`X_train` must be a pandas DataFrame. Got: {type(X_train)}')\n    if not isinstance(y_train, (pd.Series, dict)):\n        raise TypeError(f'`y_train` must be a pandas Series or a dictionary of pandas Series. Got: {type(y_train)}')\n    if not isinstance(X_test, pd.DataFrame):\n        raise TypeError(f'`X_test` must be a pandas DataFrame. Got: {type(X_test)}')\n    if not isinstance(y_test, (pd.Series, dict)):\n        raise TypeError(f'`y_test` must be a pandas Series or a dictionary of pandas Series. Got: {type(y_test)}')\n    if not isinstance(X_train_encoding, pd.Series):\n        raise TypeError(f'`X_train_encoding` must be a pandas Series. Got: {type(X_train_encoding)}')\n    if not isinstance(X_test_encoding, pd.Series):\n        raise TypeError(f'`X_test_encoding` must be a pandas Series. Got: {type(X_test_encoding)}')\n    if not isinstance(levels, list):\n        raise TypeError(f'`levels` must be a list. Got: {type(levels)}')\n    if not isinstance(metrics, list):\n        raise TypeError(f'`metrics` must be a list. Got: {type(metrics)}')\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError(f'`add_aggregated_metric` must be a boolean. Got: {type(add_aggregated_metric)}')\n    metrics = [_get_metric(metric=m) if isinstance(m, str) else add_y_train_argument(m) for m in metrics]\n    metric_names = [m if isinstance(m, str) else m.__name__ for m in metrics]\n    if isinstance(series[levels[0]].index, pd.DatetimeIndex):\n        freq = series[levels[0]].index.freq\n    else:\n        freq = series[levels[0]].index.step\n    if type(forecaster).__name__ == 'ForecasterDirectMultiVariate':\n        step = 1\n        X_train, y_train = forecaster.filter_train_X_y_for_step(step=step, X_train=X_train, y_train=y_train)\n        X_test, y_test = forecaster.filter_train_X_y_for_step(step=step, X_train=X_test, y_train=y_test)\n        forecaster.regressors_[step].fit(X_train, y_train)\n        pred = forecaster.regressors_[step].predict(X_test)\n    else:\n        forecaster.regressor.fit(X_train, y_train)\n        pred = forecaster.regressor.predict(X_test)\n    predictions_per_level = pd.DataFrame({'y_true': y_test, 'y_pred': pred, '_level_skforecast': X_test_encoding}, index=y_test.index).groupby('_level_skforecast')\n    predictions_per_level = {key: group for key, group in predictions_per_level}\n    y_train_per_level = pd.DataFrame({'y_train': y_train, '_level_skforecast': X_train_encoding}, index=y_train.index).groupby('_level_skforecast')\n    y_train_per_level = {key: group.asfreq(freq) for key, group in y_train_per_level}\n    if forecaster.differentiation is not None:\n        for level in predictions_per_level:\n            predictions_per_level[level]['y_true'] = forecaster.differentiator_[level].inverse_transform_next_window(predictions_per_level[level]['y_true'].to_numpy())\n            predictions_per_level[level]['y_pred'] = forecaster.differentiator_[level].inverse_transform_next_window(predictions_per_level[level]['y_pred'].to_numpy())\n            y_train_per_level[level]['y_train'] = forecaster.differentiator_[level].inverse_transform_training(y_train_per_level[level]['y_train'].to_numpy())\n    if forecaster.transformer_series is not None:\n        for level in predictions_per_level:\n            transformer = forecaster.transformer_series_[level]\n            predictions_per_level[level]['y_true'] = transformer.inverse_transform(predictions_per_level[level][['y_true']])\n            predictions_per_level[level]['y_pred'] = transformer.inverse_transform(predictions_per_level[level][['y_pred']])\n            y_train_per_level[level]['y_train'] = transformer.inverse_transform(y_train_per_level[level][['y_train']])\n    metrics_levels = []\n    for level in levels:\n        if level in predictions_per_level:\n            metrics_level = [m(y_true=predictions_per_level[level].loc[:, 'y_true'], y_pred=predictions_per_level[level].loc[:, 'y_pred'], y_train=y_train_per_level[level].loc[:, 'y_train']) for m in metrics]\n            metrics_levels.append(metrics_level)\n        else:\n            metrics_levels.append([None for _ in metrics])\n    metrics_levels = pd.DataFrame(data=metrics_levels, columns=[m if isinstance(m, str) else m.__name__ for m in metrics])\n    metrics_levels.insert(0, 'levels', levels)\n    if len(levels) < 2:\n        add_aggregated_metric = False\n    if add_aggregated_metric:\n        average = metrics_levels.drop(columns='levels').mean(skipna=True)\n        average = average.to_frame().transpose()\n        average['levels'] = 'average'\n        weighted_averages = {}\n        n_predictions_levels = {k: v['y_pred'].notna().sum() for k, v in predictions_per_level.items()}\n        n_predictions_levels = pd.DataFrame(n_predictions_levels.items(), columns=['levels', 'n_predictions'])\n        metrics_levels_no_missing = metrics_levels.merge(n_predictions_levels, on='levels', how='inner')\n        for col in metric_names:\n            weighted_averages[col] = np.average(metrics_levels_no_missing[col], weights=metrics_levels_no_missing['n_predictions'])\n        weighted_average = pd.DataFrame(weighted_averages, index=[0])\n        weighted_average['levels'] = 'weighted_average'\n        list_y_train_by_level = [v['y_train'].to_numpy() for k, v in y_train_per_level.items() if k in predictions_per_level]\n        predictions_pooled = pd.concat(predictions_per_level.values())\n        y_train_pooled = pd.concat([v for k, v in y_train_per_level.items() if k in predictions_per_level])\n        pooled = []\n        for m, m_name in zip(metrics, metric_names):\n            if m_name in ['mean_absolute_scaled_error', 'root_mean_squared_scaled_error']:\n                pooled.append(m(y_true=predictions_pooled['y_true'], y_pred=predictions_pooled['y_pred'], y_train=list_y_train_by_level))\n            else:\n                pooled.append(m(y_true=predictions_pooled['y_true'], y_pred=predictions_pooled['y_pred'], y_train=y_train_pooled['y_train']))\n        pooled = pd.DataFrame([pooled], columns=metric_names)\n        pooled['levels'] = 'pooling'\n        metrics_levels = pd.concat([metrics_levels, average, weighted_average, pooled], axis=0, ignore_index=True)\n    predictions = pd.concat(predictions_per_level.values()).loc[:, ['y_pred', '_level_skforecast']].pivot(columns='_level_skforecast', values='y_pred').rename_axis(columns=None, index=None)\n    predictions = predictions.asfreq(X_test.index.freq)\n    return (metrics_levels, predictions)"
  },
  "call_tree": {
    "skforecast/model_selection/tests/tests_utils/test_calculate_metrics_backtesting_multiseries.py:test_calculate_metrics_backtesting_multiseries_input_types": {
      "skforecast/model_selection/_utils.py:_calculate_metrics_backtesting_multiseries": {}
    },
    "skforecast/model_selection/tests/tests_utils/test_calculate_metrics_backtesting_multiseries.py:test_calculate_metrics_backtesting_multiseries_output_when_no_aggregated_metric": {
      "skforecast/metrics/metrics.py:add_y_train_argument": {},
      "skforecast/model_selection/_utils.py:_calculate_metrics_backtesting_multiseries": {
        "skforecast/metrics/metrics.py:wrapper": {},
        "skforecast/metrics/metrics.py:mean_absolute_scaled_error": {}
      }
    },
    "skforecast/model_selection/tests/tests_utils/test_calculate_metrics_backtesting_multiseries.py:test_calculate_metrics_backtesting_multiseries_output_when_aggregated_metric": {
      "skforecast/metrics/metrics.py:add_y_train_argument": {},
      "skforecast/model_selection/_utils.py:_calculate_metrics_backtesting_multiseries": {
        "skforecast/metrics/metrics.py:wrapper": {},
        "skforecast/metrics/metrics.py:mean_absolute_scaled_error": {}
      }
    },
    "skforecast/model_selection/tests/tests_utils/test_calculate_metrics_backtesting_multiseries.py:test_calculate_metrics_backtesting_multiseries_output_when_aggregated_metric_and_customer_metric": {
      "skforecast/metrics/metrics.py:add_y_train_argument": {},
      "skforecast/model_selection/_utils.py:_calculate_metrics_backtesting_multiseries": {
        "skforecast/metrics/metrics.py:wrapper": {
          "skforecast/model_selection/tests/tests_utils/test_calculate_metrics_backtesting_multiseries.py:custom_metric": {}
        }
      }
    },
    "skforecast/model_selection/tests/tests_utils/test_calculate_metrics_backtesting_multiseries.py:test_calculate_metrics_backtesting_multiseries_output_when_aggregated_metric_and_predictions_have_different_length": {
      "skforecast/metrics/metrics.py:add_y_train_argument": {},
      "skforecast/model_selection/_utils.py:_calculate_metrics_backtesting_multiseries": {
        "skforecast/metrics/metrics.py:wrapper": {},
        "skforecast/metrics/metrics.py:mean_absolute_scaled_error": {}
      }
    },
    "skforecast/model_selection/tests/tests_utils/test_calculate_metrics_backtesting_multiseries.py:test_calculate_metrics_backtesting_multiseries_output_when_aggregated_metric_and_one_level_is_not_predicted": {
      "skforecast/metrics/metrics.py:add_y_train_argument": {},
      "skforecast/model_selection/_utils.py:_calculate_metrics_backtesting_multiseries": {
        "skforecast/metrics/metrics.py:wrapper": {},
        "skforecast/metrics/metrics.py:mean_absolute_scaled_error": {}
      }
    }
  }
}