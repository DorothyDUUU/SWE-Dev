{
  "dir_path": "/app/scylla_driver",
  "package_name": "scylla_driver",
  "sample_name": "scylla_driver-test_time_util",
  "src_dir": "cassandra/",
  "test_dir": "tests/",
  "test_file": "tests/unit/test_time_util.py",
  "test_code": "# Copyright DataStax, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nfrom cassandra import marshal\nfrom cassandra import util\nimport calendar\nimport datetime\nimport time\nimport uuid\n\n\nclass TimeUtilTest(unittest.TestCase):\n    def test_datetime_from_timestamp(self):\n        self.assertEqual(util.datetime_from_timestamp(0), datetime.datetime(1970, 1, 1))\n        # large negative; test PYTHON-110 workaround for windows\n        self.assertEqual(util.datetime_from_timestamp(-62135596800), datetime.datetime(1, 1, 1))\n        self.assertEqual(util.datetime_from_timestamp(-62135596199), datetime.datetime(1, 1, 1, 0, 10, 1))\n\n        self.assertEqual(util.datetime_from_timestamp(253402300799), datetime.datetime(9999, 12, 31, 23, 59, 59))\n\n        self.assertEqual(util.datetime_from_timestamp(0.123456), datetime.datetime(1970, 1, 1, 0, 0, 0, 123456))\n\n        self.assertEqual(util.datetime_from_timestamp(2177403010.123456), datetime.datetime(2038, 12, 31, 10, 10, 10, 123456))\n\n    def test_times_from_uuid1(self):\n        node = uuid.getnode()\n        now = time.time()\n        u = uuid.uuid1(node, 0)\n\n        t = util.unix_time_from_uuid1(u)\n        self.assertAlmostEqual(now, t, 2)\n\n        dt = util.datetime_from_uuid1(u)\n        t = calendar.timegm(dt.timetuple()) + dt.microsecond / 1e6\n        self.assertAlmostEqual(now, t, 2)\n\n    def test_uuid_from_time(self):\n        t = time.time()\n        seq = 0x2aa5\n        node = uuid.getnode()\n        u = util.uuid_from_time(t, node, seq)\n        # using AlmostEqual because time precision is different for\n        # some platforms\n        self.assertAlmostEqual(util.unix_time_from_uuid1(u), t, 4)\n        self.assertEqual(u.node, node)\n        self.assertEqual(u.clock_seq, seq)\n\n        # random node\n        u1 = util.uuid_from_time(t, clock_seq=seq)\n        u2 = util.uuid_from_time(t, clock_seq=seq)\n        self.assertAlmostEqual(util.unix_time_from_uuid1(u1), t, 4)\n        self.assertAlmostEqual(util.unix_time_from_uuid1(u2), t, 4)\n        self.assertEqual(u.clock_seq, seq)\n        # not impossible, but we shouldn't get the same value twice\n        self.assertNotEqual(u1.node, u2.node)\n\n        # random seq\n        u1 = util.uuid_from_time(t, node=node)\n        u2 = util.uuid_from_time(t, node=node)\n        self.assertAlmostEqual(util.unix_time_from_uuid1(u1), t, 4)\n        self.assertAlmostEqual(util.unix_time_from_uuid1(u2), t, 4)\n        self.assertEqual(u.node, node)\n        # not impossible, but we shouldn't get the same value twice\n        self.assertNotEqual(u1.clock_seq, u2.clock_seq)\n\n        # node too large\n        with self.assertRaises(ValueError):\n            u = util.uuid_from_time(t, node=2 ** 48)\n\n        # clock_seq too large\n        with self.assertRaises(ValueError):\n            u = util.uuid_from_time(t, clock_seq=0x4000)\n\n        # construct from datetime\n        dt = util.datetime_from_timestamp(t)\n        u = util.uuid_from_time(dt, node, seq)\n        self.assertAlmostEqual(util.unix_time_from_uuid1(u), t, 4)\n        self.assertEqual(u.node, node)\n        self.assertEqual(u.clock_seq, seq)\n\n#   0                   1                   2                   3\n#    0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n#   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n#   |                          time_low                             |\n#   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n#   |       time_mid                |         time_hi_and_version   |\n#   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n#   |clk_seq_hi_res |  clk_seq_low  |         node (0-1)            |\n#   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n#   |                         node (2-5)                            |\n#   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n    def test_min_uuid(self):\n        u = util.min_uuid_from_time(0)\n        # cassandra does a signed comparison of the remaining bytes\n        for i in range(8, 16):\n            self.assertEqual(marshal.int8_unpack(u.bytes[i:i + 1]), -128)\n\n    def test_max_uuid(self):\n        u = util.max_uuid_from_time(0)\n        # cassandra does a signed comparison of the remaining bytes\n        # the first non-time byte has the variant in it\n        # This byte is always negative, but should be the smallest negative\n        # number with high-order bits '10'\n        self.assertEqual(marshal.int8_unpack(u.bytes[8:9]), -65)\n        for i in range(9, 16):\n            self.assertEqual(marshal.int8_unpack(u.bytes[i:i + 1]), 127)\n",
  "GT_file_code": {
    "cassandra/util.py": "# Copyright DataStax, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import with_statement\nfrom _weakref import ref\nimport calendar\nfrom collections import OrderedDict\nfrom collections.abc import Mapping\nimport datetime\nfrom functools import total_ordering\nfrom itertools import chain\nimport keyword\nimport logging\nimport pickle\nimport random\nimport re\nimport socket\nimport sys\nimport time\nimport uuid\nfrom typing import Optional\n\n_HAS_GEOMET = True\ntry:\n    from geomet import wkt\nexcept:\n    _HAS_GEOMET = False\n\n\nfrom cassandra import DriverException\n\nDATETIME_EPOC = datetime.datetime(1970, 1, 1)\nUTC_DATETIME_EPOC = datetime.datetime.utcfromtimestamp(0)\n\n_nan = float('nan')\n\nlog = logging.getLogger(__name__)\n\nassert sys.byteorder in ('little', 'big')\nis_little_endian = sys.byteorder == 'little'\n\n\ndef datetime_from_timestamp(timestamp):\n    \"\"\"\n    Creates a timezone-agnostic datetime from timestamp (in seconds) in a consistent manner.\n    Works around a Windows issue with large negative timestamps (PYTHON-119),\n    and rounding differences in Python 3.4 (PYTHON-340).\n\n    :param timestamp: a unix timestamp, in seconds\n    \"\"\"\n    dt = DATETIME_EPOC + datetime.timedelta(seconds=timestamp)\n    return dt\n\n\ndef utc_datetime_from_ms_timestamp(timestamp):\n    \"\"\"\n    Creates a UTC datetime from a timestamp in milliseconds. See\n    :meth:`datetime_from_timestamp`.\n\n    Raises an `OverflowError` if the timestamp is out of range for\n    :class:`~datetime.datetime`.\n\n    :param timestamp: timestamp, in milliseconds\n    \"\"\"\n    return UTC_DATETIME_EPOC + datetime.timedelta(milliseconds=timestamp)\n\n\ndef ms_timestamp_from_datetime(dt):\n    \"\"\"\n    Converts a datetime to a timestamp expressed in milliseconds.\n\n    :param dt: a :class:`datetime.datetime`\n    \"\"\"\n    return int(round((dt - UTC_DATETIME_EPOC).total_seconds() * 1000))\n\n\ndef unix_time_from_uuid1(uuid_arg):\n    \"\"\"\n    Converts a version 1 :class:`uuid.UUID` to a timestamp with the same precision\n    as :meth:`time.time()` returns.  This is useful for examining the\n    results of queries returning a v1 :class:`~uuid.UUID`.\n\n    :param uuid_arg: a version 1 :class:`~uuid.UUID`\n    \"\"\"\n    return (uuid_arg.time - 0x01B21DD213814000) / 1e7\n\n\ndef datetime_from_uuid1(uuid_arg):\n    \"\"\"\n    Creates a timezone-agnostic datetime from the timestamp in the\n    specified type-1 UUID.\n\n    :param uuid_arg: a version 1 :class:`~uuid.UUID`\n    \"\"\"\n    return datetime_from_timestamp(unix_time_from_uuid1(uuid_arg))\n\n\ndef min_uuid_from_time(timestamp):\n    \"\"\"\n    Generates the minimum TimeUUID (type 1) for a given timestamp, as compared by Cassandra.\n\n    See :func:`uuid_from_time` for argument and return types.\n    \"\"\"\n    return uuid_from_time(timestamp, 0x808080808080, 0x80)  # Cassandra does byte-wise comparison; fill with min signed bytes (0x80 = -128)\n\n\ndef max_uuid_from_time(timestamp):\n    \"\"\"\n    Generates the maximum TimeUUID (type 1) for a given timestamp, as compared by Cassandra.\n\n    See :func:`uuid_from_time` for argument and return types.\n    \"\"\"\n    return uuid_from_time(timestamp, 0x7f7f7f7f7f7f, 0x3f7f)  # Max signed bytes (0x7f = 127)\n\n\ndef uuid_from_time(time_arg, node=None, clock_seq=None):\n    \"\"\"\n    Converts a datetime or timestamp to a type 1 :class:`uuid.UUID`.\n\n    :param time_arg:\n      The time to use for the timestamp portion of the UUID.\n      This can either be a :class:`datetime` object or a timestamp\n      in seconds (as returned from :meth:`time.time()`).\n    :type datetime: :class:`datetime` or timestamp\n\n    :param node:\n      None integer for the UUID (up to 48 bits). If not specified, this\n      field is randomized.\n    :type node: long\n\n    :param clock_seq:\n      Clock sequence field for the UUID (up to 14 bits). If not specified,\n      a random sequence is generated.\n    :type clock_seq: int\n\n    :rtype: :class:`uuid.UUID`\n\n    \"\"\"\n    if hasattr(time_arg, 'utctimetuple'):\n        seconds = int(calendar.timegm(time_arg.utctimetuple()))\n        microseconds = (seconds * 1e6) + time_arg.time().microsecond\n    else:\n        microseconds = int(time_arg * 1e6)\n\n    # 0x01b21dd213814000 is the number of 100-ns intervals between the\n    # UUID epoch 1582-10-15 00:00:00 and the Unix epoch 1970-01-01 00:00:00.\n    intervals = int(microseconds * 10) + 0x01b21dd213814000\n\n    time_low = intervals & 0xffffffff\n    time_mid = (intervals >> 32) & 0xffff\n    time_hi_version = (intervals >> 48) & 0x0fff\n\n    if clock_seq is None:\n        clock_seq = random.getrandbits(14)\n    else:\n        if clock_seq > 0x3fff:\n            raise ValueError('clock_seq is out of range (need a 14-bit value)')\n\n    clock_seq_low = clock_seq & 0xff\n    clock_seq_hi_variant = 0x80 | ((clock_seq >> 8) & 0x3f)\n\n    if node is None:\n        node = random.getrandbits(48)\n\n    return uuid.UUID(fields=(time_low, time_mid, time_hi_version,\n                             clock_seq_hi_variant, clock_seq_low, node), version=1)\n\nLOWEST_TIME_UUID = uuid.UUID('00000000-0000-1000-8080-808080808080')\n\"\"\" The lowest possible TimeUUID, as sorted by Cassandra. \"\"\"\n\nHIGHEST_TIME_UUID = uuid.UUID('ffffffff-ffff-1fff-bf7f-7f7f7f7f7f7f')\n\"\"\" The highest possible TimeUUID, as sorted by Cassandra. \"\"\"\n\n\ndef _addrinfo_or_none(contact_point, port):\n    \"\"\"\n    A helper function that wraps socket.getaddrinfo and returns None\n    when it fails to, e.g. resolve one of the hostnames. Used to address\n    PYTHON-895.\n    \"\"\"\n    try:\n        value = socket.getaddrinfo(contact_point, port,\n                                  socket.AF_UNSPEC, socket.SOCK_STREAM)\n        return value\n    except socket.gaierror:\n        log.debug('Could not resolve hostname \"{}\" '\n                  'with port {}'.format(contact_point, port))\n        return None\n\n\ndef _addrinfo_to_ip_strings(addrinfo):\n    \"\"\"\n    Helper function that consumes the data output by socket.getaddrinfo and\n    extracts the IP address from the sockaddr portion of the result.\n\n    Since this is meant to be used in conjunction with _addrinfo_or_none,\n    this will pass None and EndPoint instances through unaffected.\n    \"\"\"\n    if addrinfo is None:\n        return None\n    return [(entry[4][0], entry[4][1]) for entry in addrinfo]\n\n\ndef _resolve_contact_points_to_string_map(contact_points):\n    return OrderedDict(\n        ('{cp}:{port}'.format(cp=cp, port=port), _addrinfo_to_ip_strings(_addrinfo_or_none(cp, port)))\n        for cp, port in contact_points\n    )\n\n\nclass _IterationGuard(object):\n    # This context manager registers itself in the current iterators of the\n    # weak container, such as to delay all removals until the context manager\n    # exits.\n    # This technique should be relatively thread-safe (since sets are).\n\n    def __init__(self, weakcontainer):\n        # Don't create cycles\n        self.weakcontainer = ref(weakcontainer)\n\n    def __enter__(self):\n        w = self.weakcontainer()\n        if w is not None:\n            w._iterating.add(self)\n        return self\n\n    def __exit__(self, e, t, b):\n        w = self.weakcontainer()\n        if w is not None:\n            s = w._iterating\n            s.remove(self)\n            if not s:\n                w._commit_removals()\n\n\nclass WeakSet(object):\n    def __init__(self, data=None):\n        self.data = set()\n\n        def _remove(item, selfref=ref(self)):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(item)\n                else:\n                    self.data.discard(item)\n\n        self._remove = _remove\n        # A list of keys to be removed\n        self._pending_removals = []\n        self._iterating = set()\n        if data is not None:\n            self.update(data)\n\n    def _commit_removals(self):\n        l = self._pending_removals\n        discard = self.data.discard\n        while l:\n            discard(l.pop())\n\n    def __iter__(self):\n        with _IterationGuard(self):\n            for itemref in self.data:\n                item = itemref()\n                if item is not None:\n                    yield item\n\n    def __len__(self):\n        return sum(x() is not None for x in self.data)\n\n    def __contains__(self, item):\n        return ref(item) in self.data\n\n    def __reduce__(self):\n        return (self.__class__, (list(self),),\n                getattr(self, '__dict__', None))\n\n    __hash__ = None\n\n    def add(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.add(ref(item, self._remove))\n\n    def clear(self):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.clear()\n\n    def copy(self):\n        return self.__class__(self)\n\n    def pop(self):\n        if self._pending_removals:\n            self._commit_removals()\n        while True:\n            try:\n                itemref = self.data.pop()\n            except KeyError:\n                raise KeyError('pop from empty WeakSet')\n            item = itemref()\n            if item is not None:\n                return item\n\n    def remove(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.remove(ref(item))\n\n    def discard(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.discard(ref(item))\n\n    def update(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if isinstance(other, self.__class__):\n            self.data.update(other.data)\n        else:\n            for element in other:\n                self.add(element)\n\n    def __ior__(self, other):\n        self.update(other)\n        return self\n\n    # Helper functions for simple delegating methods.\n    def _apply(self, other, method):\n        if not isinstance(other, self.__class__):\n            other = self.__class__(other)\n        newdata = method(other.data)\n        newset = self.__class__()\n        newset.data = newdata\n        return newset\n\n    def difference(self, other):\n        return self._apply(other, self.data.difference)\n    __sub__ = difference\n\n    def difference_update(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.difference_update(ref(item) for item in other)\n\n    def __isub__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.difference_update(ref(item) for item in other)\n        return self\n\n    def intersection(self, other):\n        return self._apply(other, self.data.intersection)\n    __and__ = intersection\n\n    def intersection_update(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.intersection_update(ref(item) for item in other)\n\n    def __iand__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.intersection_update(ref(item) for item in other)\n        return self\n\n    def issubset(self, other):\n        return self.data.issubset(ref(item) for item in other)\n    __lt__ = issubset\n\n    def __le__(self, other):\n        return self.data <= set(ref(item) for item in other)\n\n    def issuperset(self, other):\n        return self.data.issuperset(ref(item) for item in other)\n    __gt__ = issuperset\n\n    def __ge__(self, other):\n        return self.data >= set(ref(item) for item in other)\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n        return self.data == set(ref(item) for item in other)\n\n    def symmetric_difference(self, other):\n        return self._apply(other, self.data.symmetric_difference)\n    __xor__ = symmetric_difference\n\n    def symmetric_difference_update(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.symmetric_difference_update(ref(item) for item in other)\n\n    def __ixor__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.symmetric_difference_update(ref(item) for item in other)\n        return self\n\n    def union(self, other):\n        return self._apply(other, self.data.union)\n    __or__ = union\n\n    def isdisjoint(self, other):\n        return len(self.intersection(other)) == 0\n\n\nclass SortedSet(object):\n    '''\n    A sorted set based on sorted list\n\n    A sorted set implementation is used in this case because it does not\n    require its elements to be immutable/hashable.\n\n    #Not implemented: update functions, inplace operators\n    '''\n\n    def __init__(self, iterable=()):\n        self._items = []\n        self.update(iterable)\n\n    def __len__(self):\n        return len(self._items)\n\n    def __getitem__(self, i):\n        return self._items[i]\n\n    def __iter__(self):\n        return iter(self._items)\n\n    def __reversed__(self):\n        return reversed(self._items)\n\n    def __repr__(self):\n        return '%s(%r)' % (\n            self.__class__.__name__,\n            self._items)\n\n    def __reduce__(self):\n        return self.__class__, (self._items,)\n\n    def __eq__(self, other):\n        if isinstance(other, self.__class__):\n            return self._items == other._items\n        else:\n            try:\n                return len(other) == len(self._items) and all(item in self for item in other)\n            except TypeError:\n                return NotImplemented\n\n    def __ne__(self, other):\n        if isinstance(other, self.__class__):\n            return self._items != other._items\n        else:\n            try:\n                return len(other) != len(self._items) or any(item not in self for item in other)\n            except TypeError:\n                return NotImplemented\n\n    def __le__(self, other):\n        return self.issubset(other)\n\n    def __lt__(self, other):\n        return len(other) > len(self._items) and self.issubset(other)\n\n    def __ge__(self, other):\n        return self.issuperset(other)\n\n    def __gt__(self, other):\n        return len(self._items) > len(other) and self.issuperset(other)\n\n    def __and__(self, other):\n        return self._intersect(other)\n    __rand__ = __and__\n\n    def __iand__(self, other):\n        isect = self._intersect(other)\n        self._items = isect._items\n        return self\n\n    def __or__(self, other):\n        return self.union(other)\n    __ror__ = __or__\n\n    def __ior__(self, other):\n        union = self.union(other)\n        self._items = union._items\n        return self\n\n    def __sub__(self, other):\n        return self._diff(other)\n\n    def __rsub__(self, other):\n        return sortedset(other) - self\n\n    def __isub__(self, other):\n        diff = self._diff(other)\n        self._items = diff._items\n        return self\n\n    def __xor__(self, other):\n        return self.symmetric_difference(other)\n    __rxor__ = __xor__\n\n    def __ixor__(self, other):\n        sym_diff = self.symmetric_difference(other)\n        self._items = sym_diff._items\n        return self\n\n    def __contains__(self, item):\n        i = self._find_insertion(item)\n        return i < len(self._items) and self._items[i] == item\n\n    def __delitem__(self, i):\n        del self._items[i]\n\n    def __delslice__(self, i, j):\n        del self._items[i:j]\n\n    def add(self, item):\n        i = self._find_insertion(item)\n        if i < len(self._items):\n            if self._items[i] != item:\n                self._items.insert(i, item)\n        else:\n            self._items.append(item)\n\n    def update(self, iterable):\n        for i in iterable:\n            self.add(i)\n\n    def clear(self):\n        del self._items[:]\n\n    def copy(self):\n        new = sortedset()\n        new._items = list(self._items)\n        return new\n\n    def isdisjoint(self, other):\n        return len(self._intersect(other)) == 0\n\n    def issubset(self, other):\n        return len(self._intersect(other)) == len(self._items)\n\n    def issuperset(self, other):\n        return len(self._intersect(other)) == len(other)\n\n    def pop(self):\n        if not self._items:\n            raise KeyError(\"pop from empty set\")\n        return self._items.pop()\n\n    def remove(self, item):\n        i = self._find_insertion(item)\n        if i < len(self._items):\n            if self._items[i] == item:\n                self._items.pop(i)\n                return\n        raise KeyError('%r' % item)\n\n    def union(self, *others):\n        union = sortedset()\n        union._items = list(self._items)\n        for other in others:\n            for item in other:\n                union.add(item)\n        return union\n\n    def intersection(self, *others):\n        isect = self.copy()\n        for other in others:\n            isect = isect._intersect(other)\n            if not isect:\n                break\n        return isect\n\n    def difference(self, *others):\n        diff = self.copy()\n        for other in others:\n            diff = diff._diff(other)\n            if not diff:\n                break\n        return diff\n\n    def symmetric_difference(self, other):\n        diff_self_other = self._diff(other)\n        diff_other_self = other.difference(self)\n        return diff_self_other.union(diff_other_self)\n\n    def _diff(self, other):\n        diff = sortedset()\n        for item in self._items:\n            if item not in other:\n                diff.add(item)\n        return diff\n\n    def _intersect(self, other):\n        isect = sortedset()\n        for item in self._items:\n            if item in other:\n                isect.add(item)\n        return isect\n\n    def _find_insertion(self, x):\n        # this uses bisect_left algorithm unless it has elements it can't compare,\n        # in which case it defaults to grouping non-comparable items at the beginning or end,\n        # and scanning sequentially to find an insertion point\n        a = self._items\n        lo = 0\n        hi = len(a)\n        try:\n            while lo < hi:\n                mid = (lo + hi) // 2\n                if a[mid] < x: lo = mid + 1\n                else: hi = mid\n        except TypeError:\n            # could not compare a[mid] with x\n            # start scanning to find insertion point while swallowing type errors\n            lo = 0\n            compared_one = False  # flag is used to determine whether uncomparables are grouped at the front or back\n            while lo < hi:\n                try:\n                    if a[lo] == x or a[lo] >= x: break\n                    compared_one = True\n                except TypeError:\n                    if compared_one: break\n                lo += 1\n        return lo\n\nsortedset = SortedSet  # backwards-compatibility\n\n\nclass OrderedMap(Mapping):\n    '''\n    An ordered map that accepts non-hashable types for keys. It also maintains the\n    insertion order of items, behaving as OrderedDict in that regard. These maps\n    are constructed and read just as normal mapping types, except that they may\n    contain arbitrary collections and other non-hashable items as keys::\n\n        >>> od = OrderedMap([({'one': 1, 'two': 2}, 'value'),\n        ...                  ({'three': 3, 'four': 4}, 'value2')])\n        >>> list(od.keys())\n        [{'two': 2, 'one': 1}, {'three': 3, 'four': 4}]\n        >>> list(od.values())\n        ['value', 'value2']\n\n    These constructs are needed to support nested collections in Cassandra 2.1.3+,\n    where frozen collections can be specified as parameters to others::\n\n        CREATE TABLE example (\n            ...\n            value map<frozen<map<int, int>>, double>\n            ...\n        )\n\n    This class derives from the (immutable) Mapping API. Objects in these maps\n    are not intended be modified.\n    '''\n\n    def __init__(self, *args, **kwargs):\n        if len(args) > 1:\n            raise TypeError('expected at most 1 arguments, got %d' % len(args))\n\n        self._items = []\n        self._index = {}\n        if args:\n            e = args[0]\n            if callable(getattr(e, 'keys', None)):\n                for k in e.keys():\n                    self._insert(k, e[k])\n            else:\n                for k, v in e:\n                    self._insert(k, v)\n\n        for k, v in kwargs.items():\n            self._insert(k, v)\n\n    def _insert(self, key, value):\n        flat_key = self._serialize_key(key)\n        i = self._index.get(flat_key, -1)\n        if i >= 0:\n            self._items[i] = (key, value)\n        else:\n            self._items.append((key, value))\n            self._index[flat_key] = len(self._items) - 1\n\n    __setitem__ = _insert\n\n    def __getitem__(self, key):\n        try:\n            index = self._index[self._serialize_key(key)]\n            return self._items[index][1]\n        except KeyError:\n            raise KeyError(str(key))\n\n    def __delitem__(self, key):\n        # not efficient -- for convenience only\n        try:\n            index = self._index.pop(self._serialize_key(key))\n            self._index = dict((k, i if i < index else i - 1) for k, i in self._index.items())\n            self._items.pop(index)\n        except KeyError:\n            raise KeyError(str(key))\n\n    def __iter__(self):\n        for i in self._items:\n            yield i[0]\n\n    def __len__(self):\n        return len(self._items)\n\n    def __eq__(self, other):\n        if isinstance(other, OrderedMap):\n            return self._items == other._items\n        try:\n            d = dict(other)\n            return len(d) == len(self._items) and all(i[1] == d[i[0]] for i in self._items)\n        except KeyError:\n            return False\n        except TypeError:\n            pass\n        return NotImplemented\n\n    def __repr__(self):\n        return '%s([%s])' % (\n            self.__class__.__name__,\n            ', '.join(\"(%r, %r)\" % (k, v) for k, v in self._items))\n\n    def __str__(self):\n        return '{%s}' % ', '.join(\"%r: %r\" % (k, v) for k, v in self._items)\n\n    def popitem(self):\n        try:\n            kv = self._items.pop()\n            del self._index[self._serialize_key(kv[0])]\n            return kv\n        except IndexError:\n            raise KeyError()\n\n    def _serialize_key(self, key):\n        return pickle.dumps(key)\n\n\nclass OrderedMapSerializedKey(OrderedMap):\n\n    def __init__(self, cass_type, protocol_version):\n        super(OrderedMapSerializedKey, self).__init__()\n        self.cass_key_type = cass_type\n        self.protocol_version = protocol_version\n\n    def _insert_unchecked(self, key, flat_key, value):\n        self._items.append((key, value))\n        self._index[flat_key] = len(self._items) - 1\n\n    def _serialize_key(self, key):\n        return self.cass_key_type.serialize(key, self.protocol_version)\n\n\n@total_ordering\nclass Time(object):\n    '''\n    Idealized time, independent of day.\n\n    Up to nanosecond resolution\n    '''\n\n    MICRO = 1000\n    MILLI = 1000 * MICRO\n    SECOND = 1000 * MILLI\n    MINUTE = 60 * SECOND\n    HOUR = 60 * MINUTE\n    DAY = 24 * HOUR\n\n    nanosecond_time = 0\n\n    def __init__(self, value):\n        \"\"\"\n        Initializer value can be:\n\n        - integer_type: absolute nanoseconds in the day\n        - datetime.time: built-in time\n        - string_type: a string time of the form \"HH:MM:SS[.mmmuuunnn]\"\n        \"\"\"\n        if isinstance(value, int):\n            self._from_timestamp(value)\n        elif isinstance(value, datetime.time):\n            self._from_time(value)\n        elif isinstance(value, str):\n            self._from_timestring(value)\n        else:\n            raise TypeError('Time arguments must be a whole number, datetime.time, or string')\n\n    @property\n    def hour(self):\n        \"\"\"\n        The hour component of this time (0-23)\n        \"\"\"\n        return self.nanosecond_time // Time.HOUR\n\n    @property\n    def minute(self):\n        \"\"\"\n        The minute component of this time (0-59)\n        \"\"\"\n        minutes = self.nanosecond_time // Time.MINUTE\n        return minutes % 60\n\n    @property\n    def second(self):\n        \"\"\"\n        The second component of this time (0-59)\n        \"\"\"\n        seconds = self.nanosecond_time // Time.SECOND\n        return seconds % 60\n\n    @property\n    def nanosecond(self):\n        \"\"\"\n        The fractional seconds component of the time, in nanoseconds\n        \"\"\"\n        return self.nanosecond_time % Time.SECOND\n\n    def time(self):\n        \"\"\"\n        Return a built-in datetime.time (nanosecond precision truncated to micros).\n        \"\"\"\n        return datetime.time(hour=self.hour, minute=self.minute, second=self.second,\n                             microsecond=self.nanosecond // Time.MICRO)\n\n    def _from_timestamp(self, t):\n        if t >= Time.DAY:\n            raise ValueError(\"value must be less than number of nanoseconds in a day (%d)\" % Time.DAY)\n        self.nanosecond_time = t\n\n    def _from_timestring(self, s):\n        try:\n            parts = s.split('.')\n            base_time = time.strptime(parts[0], \"%H:%M:%S\")\n            self.nanosecond_time = (base_time.tm_hour * Time.HOUR +\n                                    base_time.tm_min * Time.MINUTE +\n                                    base_time.tm_sec * Time.SECOND)\n\n            if len(parts) > 1:\n                # right pad to 9 digits\n                nano_time_str = parts[1] + \"0\" * (9 - len(parts[1]))\n                self.nanosecond_time += int(nano_time_str)\n\n        except ValueError:\n            raise ValueError(\"can't interpret %r as a time\" % (s,))\n\n    def _from_time(self, t):\n        self.nanosecond_time = (t.hour * Time.HOUR +\n                                t.minute * Time.MINUTE +\n                                t.second * Time.SECOND +\n                                t.microsecond * Time.MICRO)\n\n    def __hash__(self):\n        return self.nanosecond_time\n\n    def __eq__(self, other):\n        if isinstance(other, Time):\n            return self.nanosecond_time == other.nanosecond_time\n\n        if isinstance(other, int):\n            return self.nanosecond_time == other\n\n        return self.nanosecond_time % Time.MICRO == 0 and \\\n            datetime.time(hour=self.hour, minute=self.minute, second=self.second,\n                          microsecond=self.nanosecond // Time.MICRO) == other\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __lt__(self, other):\n        if not isinstance(other, Time):\n            return NotImplemented\n        return self.nanosecond_time < other.nanosecond_time\n\n    def __repr__(self):\n        return \"Time(%s)\" % self.nanosecond_time\n\n    def __str__(self):\n        return \"%02d:%02d:%02d.%09d\" % (self.hour, self.minute,\n                                        self.second, self.nanosecond)\n\n\n@total_ordering\nclass Date(object):\n    '''\n    Idealized date: year, month, day\n\n    Offers wider year range than datetime.date. For Dates that cannot be represented\n    as a datetime.date (because datetime.MINYEAR, datetime.MAXYEAR), this type falls back\n    to printing days_from_epoch offset.\n    '''\n\n    MINUTE = 60\n    HOUR = 60 * MINUTE\n    DAY = 24 * HOUR\n\n    date_format = \"%Y-%m-%d\"\n\n    days_from_epoch = 0\n\n    def __init__(self, value):\n        \"\"\"\n        Initializer value can be:\n\n        - integer_type: absolute days from epoch (1970, 1, 1). Can be negative.\n        - datetime.date: built-in date\n        - string_type: a string time of the form \"yyyy-mm-dd\"\n        \"\"\"\n        if isinstance(value, int):\n            self.days_from_epoch = value\n        elif isinstance(value, (datetime.date, datetime.datetime)):\n            self._from_timetuple(value.timetuple())\n        elif isinstance(value, str):\n            self._from_datestring(value)\n        else:\n            raise TypeError('Date arguments must be a whole number, datetime.date, or string')\n\n    @property\n    def seconds(self):\n        \"\"\"\n        Absolute seconds from epoch (can be negative)\n        \"\"\"\n        return self.days_from_epoch * Date.DAY\n\n    def date(self):\n        \"\"\"\n        Return a built-in datetime.date for Dates falling in the years [datetime.MINYEAR, datetime.MAXYEAR]\n\n        ValueError is raised for Dates outside this range.\n        \"\"\"\n        try:\n            dt = datetime_from_timestamp(self.seconds)\n            return datetime.date(dt.year, dt.month, dt.day)\n        except Exception:\n            raise ValueError(\"%r exceeds ranges for built-in datetime.date\" % self)\n\n    def _from_timetuple(self, t):\n        self.days_from_epoch = calendar.timegm(t) // Date.DAY\n\n    def _from_datestring(self, s):\n        if s[0] == '+':\n            s = s[1:]\n        dt = datetime.datetime.strptime(s, self.date_format)\n        self._from_timetuple(dt.timetuple())\n\n    def __hash__(self):\n        return self.days_from_epoch\n\n    def __eq__(self, other):\n        if isinstance(other, Date):\n            return self.days_from_epoch == other.days_from_epoch\n\n        if isinstance(other, int):\n            return self.days_from_epoch == other\n\n        try:\n            return self.date() == other\n        except Exception:\n            return False\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __lt__(self, other):\n        if not isinstance(other, Date):\n            return NotImplemented\n        return self.days_from_epoch < other.days_from_epoch\n\n    def __repr__(self):\n        return \"Date(%s)\" % self.days_from_epoch\n\n    def __str__(self):\n        try:\n            dt = datetime_from_timestamp(self.seconds)\n            return \"%04d-%02d-%02d\" % (dt.year, dt.month, dt.day)\n        except:\n            # If we overflow datetime.[MIN|MAX]\n            return str(self.days_from_epoch)\n\n\ninet_pton = socket.inet_pton\ninet_ntop = socket.inet_ntop\n\n\n# similar to collections.namedtuple, reproduced here because Python 2.6 did not have the rename logic\ndef _positional_rename_invalid_identifiers(field_names):\n    names_out = list(field_names)\n    for index, name in enumerate(field_names):\n        if (not all(c.isalnum() or c == '_' for c in name)\n            or keyword.iskeyword(name)\n            or not name\n            or name[0].isdigit()\n            or name.startswith('_')):\n            names_out[index] = 'field_%d_' % index\n    return names_out\n\n\ndef _sanitize_identifiers(field_names):\n    names_out = _positional_rename_invalid_identifiers(field_names)\n    if len(names_out) != len(set(names_out)):\n        observed_names = set()\n        for index, name in enumerate(names_out):\n            while names_out[index] in observed_names:\n                names_out[index] = \"%s_\" % (names_out[index],)\n            observed_names.add(names_out[index])\n    return names_out\n\n\ndef list_contents_to_tuple(to_convert):\n    if isinstance(to_convert, list):\n        for n, i in enumerate(to_convert):\n            if isinstance(to_convert[n], list):\n                to_convert[n] = tuple(to_convert[n])\n        return tuple(to_convert)\n    else:\n        return to_convert\n\n\nclass Point(object):\n    \"\"\"\n    Represents a point geometry for DSE\n    \"\"\"\n\n    x = None\n    \"\"\"\n    x coordinate of the point\n    \"\"\"\n\n    y = None\n    \"\"\"\n    y coordinate of the point\n    \"\"\"\n\n    def __init__(self, x=_nan, y=_nan):\n        self.x = x\n        self.y = y\n\n    def __eq__(self, other):\n        return isinstance(other, Point) and self.x == other.x and self.y == other.y\n\n    def __hash__(self):\n        return hash((self.x, self.y))\n\n    def __str__(self):\n        \"\"\"\n        Well-known text representation of the point\n        \"\"\"\n        return \"POINT (%r %r)\" % (self.x, self.y)\n\n    def __repr__(self):\n        return \"%s(%r, %r)\" % (self.__class__.__name__, self.x, self.y)\n\n    @staticmethod\n    def from_wkt(s):\n        \"\"\"\n        Parse a Point geometry from a wkt string and return a new Point object.\n        \"\"\"\n        if not _HAS_GEOMET:\n            raise DriverException(\"Geomet is required to deserialize a wkt geometry.\")\n\n        try:\n            geom = wkt.loads(s)\n        except ValueError:\n            raise ValueError(\"Invalid WKT geometry: '{0}'\".format(s))\n\n        if geom['type'] != 'Point':\n            raise ValueError(\"Invalid WKT geometry type. Expected 'Point', got '{0}': '{1}'\".format(geom['type'], s))\n\n        coords = geom['coordinates']\n        if len(coords) < 2:\n            x = y = _nan\n        else:\n            x = coords[0]\n            y = coords[1]\n\n        return Point(x=x, y=y)\n\n\nclass LineString(object):\n    \"\"\"\n    Represents a linestring geometry for DSE\n    \"\"\"\n\n    coords = None\n    \"\"\"\n    Tuple of (x, y) coordinates in the linestring\n    \"\"\"\n    def __init__(self, coords=tuple()):\n        \"\"\"\n        'coords`: a sequence of (x, y) coordinates of points in the linestring\n        \"\"\"\n        self.coords = tuple(coords)\n\n    def __eq__(self, other):\n        return isinstance(other, LineString) and self.coords == other.coords\n\n    def __hash__(self):\n        return hash(self.coords)\n\n    def __str__(self):\n        \"\"\"\n        Well-known text representation of the LineString\n        \"\"\"\n        if not self.coords:\n            return \"LINESTRING EMPTY\"\n        return \"LINESTRING (%s)\" % ', '.join(\"%r %r\" % (x, y) for x, y in self.coords)\n\n    def __repr__(self):\n        return \"%s(%r)\" % (self.__class__.__name__, self.coords)\n\n    @staticmethod\n    def from_wkt(s):\n        \"\"\"\n        Parse a LineString geometry from a wkt string and return a new LineString object.\n        \"\"\"\n        if not _HAS_GEOMET:\n            raise DriverException(\"Geomet is required to deserialize a wkt geometry.\")\n\n        try:\n            geom = wkt.loads(s)\n        except ValueError:\n            raise ValueError(\"Invalid WKT geometry: '{0}'\".format(s))\n\n        if geom['type'] != 'LineString':\n            raise ValueError(\"Invalid WKT geometry type. Expected 'LineString', got '{0}': '{1}'\".format(geom['type'], s))\n\n        geom['coordinates'] = list_contents_to_tuple(geom['coordinates'])\n\n        return LineString(coords=geom['coordinates'])\n\n\nclass _LinearRing(object):\n    # no validation, no implicit closing; just used for poly composition, to\n    # mimic that of shapely.geometry.Polygon\n    def __init__(self, coords=tuple()):\n        self.coords = list_contents_to_tuple(coords)\n\n    def __eq__(self, other):\n        return isinstance(other, _LinearRing) and self.coords == other.coords\n\n    def __hash__(self):\n        return hash(self.coords)\n\n    def __str__(self):\n        if not self.coords:\n            return \"LINEARRING EMPTY\"\n        return \"LINEARRING (%s)\" % ', '.join(\"%r %r\" % (x, y) for x, y in self.coords)\n\n    def __repr__(self):\n        return \"%s(%r)\" % (self.__class__.__name__, self.coords)\n\n\nclass Polygon(object):\n    \"\"\"\n    Represents a polygon geometry for DSE\n    \"\"\"\n\n    exterior = None\n    \"\"\"\n    _LinearRing representing the exterior of the polygon\n    \"\"\"\n\n    interiors = None\n    \"\"\"\n    Tuple of _LinearRings representing interior holes in the polygon\n    \"\"\"\n\n    def __init__(self, exterior=tuple(), interiors=None):\n        \"\"\"\n        'exterior`: a sequence of (x, y) coordinates of points in the linestring\n        `interiors`: None, or a sequence of sequences or (x, y) coordinates of points describing interior linear rings\n        \"\"\"\n        self.exterior = _LinearRing(exterior)\n        self.interiors = tuple(_LinearRing(e) for e in interiors) if interiors else tuple()\n\n    def __eq__(self, other):\n        return isinstance(other, Polygon) and self.exterior == other.exterior and self.interiors == other.interiors\n\n    def __hash__(self):\n        return hash((self.exterior, self.interiors))\n\n    def __str__(self):\n        \"\"\"\n        Well-known text representation of the polygon\n        \"\"\"\n        if not self.exterior.coords:\n            return \"POLYGON EMPTY\"\n        rings = [ring.coords for ring in chain((self.exterior,), self.interiors)]\n        rings = [\"(%s)\" % ', '.join(\"%r %r\" % (x, y) for x, y in ring) for ring in rings]\n        return \"POLYGON (%s)\" % ', '.join(rings)\n\n    def __repr__(self):\n        return \"%s(%r, %r)\" % (self.__class__.__name__, self.exterior.coords, [ring.coords for ring in self.interiors])\n\n    @staticmethod\n    def from_wkt(s):\n        \"\"\"\n        Parse a Polygon geometry from a wkt string and return a new Polygon object.\n        \"\"\"\n        if not _HAS_GEOMET:\n            raise DriverException(\"Geomet is required to deserialize a wkt geometry.\")\n\n        try:\n            geom = wkt.loads(s)\n        except ValueError:\n            raise ValueError(\"Invalid WKT geometry: '{0}'\".format(s))\n\n        if geom['type'] != 'Polygon':\n            raise ValueError(\"Invalid WKT geometry type. Expected 'Polygon', got '{0}': '{1}'\".format(geom['type'], s))\n\n        coords = geom['coordinates']\n        exterior = coords[0] if len(coords) > 0 else tuple()\n        interiors = coords[1:] if len(coords) > 1 else None\n\n        return Polygon(exterior=exterior, interiors=interiors)\n\n\n_distance_wkt_pattern = re.compile(\"distance *\\\\( *\\\\( *([\\\\d\\\\.-]+) *([\\\\d+\\\\.-]+) *\\\\) *([\\\\d+\\\\.-]+) *\\\\) *$\", re.IGNORECASE)\n\n\nclass Distance(object):\n    \"\"\"\n    Represents a Distance geometry for DSE\n    \"\"\"\n\n    x = None\n    \"\"\"\n    x coordinate of the center point\n    \"\"\"\n\n    y = None\n    \"\"\"\n    y coordinate of the center point\n    \"\"\"\n\n    radius = None\n    \"\"\"\n    radius to represent the distance from the center point\n    \"\"\"\n\n    def __init__(self, x=_nan, y=_nan, radius=_nan):\n        self.x = x\n        self.y = y\n        self.radius = radius\n\n    def __eq__(self, other):\n        return isinstance(other, Distance) and self.x == other.x and self.y == other.y and self.radius == other.radius\n\n    def __hash__(self):\n        return hash((self.x, self.y, self.radius))\n\n    def __str__(self):\n        \"\"\"\n        Well-known text representation of the point\n        \"\"\"\n        return \"DISTANCE ((%r %r) %r)\" % (self.x, self.y, self.radius)\n\n    def __repr__(self):\n        return \"%s(%r, %r, %r)\" % (self.__class__.__name__, self.x, self.y, self.radius)\n\n    @staticmethod\n    def from_wkt(s):\n        \"\"\"\n        Parse a Distance geometry from a wkt string and return a new Distance object.\n        \"\"\"\n\n        distance_match = _distance_wkt_pattern.match(s)\n\n        if distance_match is None:\n            raise ValueError(\"Invalid WKT geometry: '{0}'\".format(s))\n\n        x, y, radius = distance_match.groups()\n        return Distance(x, y, radius)\n\n\nclass Duration(object):\n    \"\"\"\n    Cassandra Duration Type\n    \"\"\"\n\n    months = 0\n    \"\"\n    days = 0\n    \"\"\n    nanoseconds = 0\n    \"\"\n\n    def __init__(self, months=0, days=0, nanoseconds=0):\n        self.months = months\n        self.days = days\n        self.nanoseconds = nanoseconds\n\n    def __eq__(self, other):\n        return isinstance(other, self.__class__) and self.months == other.months and self.days == other.days and self.nanoseconds == other.nanoseconds\n\n    def __repr__(self):\n        return \"Duration({0}, {1}, {2})\".format(self.months, self.days, self.nanoseconds)\n\n    def __str__(self):\n        has_negative_values = self.months < 0 or self.days < 0 or self.nanoseconds < 0\n        return '%s%dmo%dd%dns' % (\n            '-' if has_negative_values else '',\n            abs(self.months),\n            abs(self.days),\n            abs(self.nanoseconds)\n        )\n\n\nclass DateRangePrecision(object):\n    \"\"\"\n    An \"enum\" representing the valid values for :attr:`DateRange.precision`.\n    \"\"\"\n    YEAR = 'YEAR'\n    \"\"\"\n    \"\"\"\n\n    MONTH = 'MONTH'\n    \"\"\"\n    \"\"\"\n\n    DAY = 'DAY'\n    \"\"\"\n    \"\"\"\n\n    HOUR = 'HOUR'\n    \"\"\"\n    \"\"\"\n\n    MINUTE = 'MINUTE'\n    \"\"\"\n    \"\"\"\n\n    SECOND = 'SECOND'\n    \"\"\"\n    \"\"\"\n\n    MILLISECOND = 'MILLISECOND'\n    \"\"\"\n    \"\"\"\n\n    PRECISIONS = (YEAR, MONTH, DAY, HOUR,\n                  MINUTE, SECOND, MILLISECOND)\n    \"\"\"\n    \"\"\"\n\n    @classmethod\n    def _to_int(cls, precision):\n        return cls.PRECISIONS.index(precision.upper())\n\n    @classmethod\n    def _round_to_precision(cls, ms, precision, default_dt):\n        try:\n            dt = utc_datetime_from_ms_timestamp(ms)\n        except OverflowError:\n            return ms\n        precision_idx = cls._to_int(precision)\n        replace_kwargs = {}\n        if precision_idx <= cls._to_int(DateRangePrecision.YEAR):\n            replace_kwargs['month'] = default_dt.month\n        if precision_idx <= cls._to_int(DateRangePrecision.MONTH):\n            replace_kwargs['day'] = default_dt.day\n        if precision_idx <= cls._to_int(DateRangePrecision.DAY):\n            replace_kwargs['hour'] = default_dt.hour\n        if precision_idx <= cls._to_int(DateRangePrecision.HOUR):\n            replace_kwargs['minute'] = default_dt.minute\n        if precision_idx <= cls._to_int(DateRangePrecision.MINUTE):\n            replace_kwargs['second'] = default_dt.second\n        if precision_idx <= cls._to_int(DateRangePrecision.SECOND):\n            # truncate to nearest 1000 so we deal in ms, not us\n            replace_kwargs['microsecond'] = (default_dt.microsecond // 1000) * 1000\n        if precision_idx == cls._to_int(DateRangePrecision.MILLISECOND):\n            replace_kwargs['microsecond'] = int(round(dt.microsecond, -3))\n        return ms_timestamp_from_datetime(dt.replace(**replace_kwargs))\n\n    @classmethod\n    def round_up_to_precision(cls, ms, precision):\n        # PYTHON-912: this is the only case in which we can't take as upper bound\n        # datetime.datetime.max because the month from ms may be February and we'd\n        # be setting 31 as the month day\n        if precision == cls.MONTH:\n            date_ms = utc_datetime_from_ms_timestamp(ms)\n            upper_date = datetime.datetime.max.replace(year=date_ms.year, month=date_ms.month,\n                                                       day=calendar.monthrange(date_ms.year, date_ms.month)[1])\n        else:\n            upper_date = datetime.datetime.max\n        return cls._round_to_precision(ms, precision, upper_date)\n\n    @classmethod\n    def round_down_to_precision(cls, ms, precision):\n        return cls._round_to_precision(ms, precision, datetime.datetime.min)\n\n\n@total_ordering\nclass DateRangeBound(object):\n    \"\"\"DateRangeBound(value, precision)\n    Represents a single date value and its precision for :class:`DateRange`.\n\n    .. attribute:: milliseconds\n\n        Integer representing milliseconds since the UNIX epoch. May be negative.\n\n    .. attribute:: precision\n\n        String representing the precision of a bound. Must be a valid\n        :class:`DateRangePrecision` member.\n\n    :class:`DateRangeBound` uses a millisecond offset from the UNIX epoch to\n    allow :class:`DateRange` to represent values `datetime.datetime` cannot.\n    For such values, string representions will show this offset rather than the\n    CQL representation.\n    \"\"\"\n    milliseconds = None\n    precision = None\n\n    def __init__(self, value, precision):\n        \"\"\"\n        :param value: a value representing ms since the epoch. Accepts an\n            integer or a datetime.\n        :param precision: a string representing precision\n        \"\"\"\n        if precision is not None:\n            try:\n                self.precision = precision.upper()\n            except AttributeError:\n                raise TypeError('precision must be a string; got %r' % precision)\n\n        if value is None:\n            milliseconds = None\n        elif isinstance(value, int):\n            milliseconds = value\n        elif isinstance(value, datetime.datetime):\n            value = value.replace(\n                microsecond=int(round(value.microsecond, -3))\n            )\n            milliseconds = ms_timestamp_from_datetime(value)\n        else:\n            raise ValueError('%r is not a valid value for DateRangeBound' % value)\n\n        self.milliseconds = milliseconds\n        self.validate()\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n        return (self.milliseconds == other.milliseconds and\n                self.precision == other.precision)\n\n    def __lt__(self, other):\n        return ((str(self.milliseconds), str(self.precision)) <\n                (str(other.milliseconds), str(other.precision)))\n\n    def datetime(self):\n        \"\"\"\n        Return :attr:`milliseconds` as a :class:`datetime.datetime` if possible.\n        Raises an `OverflowError` if the value is out of range.\n        \"\"\"\n        return utc_datetime_from_ms_timestamp(self.milliseconds)\n\n    def validate(self):\n        attrs = self.milliseconds, self.precision\n        if attrs == (None, None):\n            return\n        if None in attrs:\n            raise TypeError(\n                (\"%s.datetime and %s.precision must not be None unless both \"\n                 \"are None; Got: %r\") % (self.__class__.__name__,\n                                         self.__class__.__name__,\n                                         self)\n            )\n        if self.precision not in DateRangePrecision.PRECISIONS:\n            raise ValueError(\n                \"%s.precision: expected value in %r; got %r\" % (\n                    self.__class__.__name__,\n                    DateRangePrecision.PRECISIONS,\n                    self.precision\n                )\n            )\n\n    @classmethod\n    def from_value(cls, value):\n        \"\"\"\n        Construct a new :class:`DateRangeBound` from a given value. If\n        possible, use the `value['milliseconds']` and `value['precision']` keys\n        of the argument. Otherwise, use the argument as a `(milliseconds,\n        precision)` iterable.\n\n        :param value: a dictlike or iterable object\n        \"\"\"\n        if isinstance(value, cls):\n            return value\n\n        # if possible, use as a mapping\n        try:\n            milliseconds, precision = value.get('milliseconds'), value.get('precision')\n        except AttributeError:\n            milliseconds = precision = None\n        if milliseconds is not None and precision is not None:\n            return DateRangeBound(value=milliseconds, precision=precision)\n\n        # otherwise, use as an iterable\n        return DateRangeBound(*value)\n\n    def round_up(self):\n        if self.milliseconds is None or self.precision is None:\n            return self\n        self.milliseconds = DateRangePrecision.round_up_to_precision(\n            self.milliseconds, self.precision\n        )\n        return self\n\n    def round_down(self):\n        if self.milliseconds is None or self.precision is None:\n            return self\n        self.milliseconds = DateRangePrecision.round_down_to_precision(\n            self.milliseconds, self.precision\n        )\n        return self\n\n    _formatter_map = {\n        DateRangePrecision.YEAR: '%Y',\n        DateRangePrecision.MONTH: '%Y-%m',\n        DateRangePrecision.DAY: '%Y-%m-%d',\n        DateRangePrecision.HOUR: '%Y-%m-%dT%HZ',\n        DateRangePrecision.MINUTE: '%Y-%m-%dT%H:%MZ',\n        DateRangePrecision.SECOND: '%Y-%m-%dT%H:%M:%SZ',\n        DateRangePrecision.MILLISECOND: '%Y-%m-%dT%H:%M:%S',\n    }\n\n    def __str__(self):\n        if self == OPEN_BOUND:\n            return '*'\n\n        try:\n            dt = self.datetime()\n        except OverflowError:\n            return '%sms' % (self.milliseconds,)\n\n        formatted = dt.strftime(self._formatter_map[self.precision])\n\n        if self.precision == DateRangePrecision.MILLISECOND:\n            # we'd like to just format with '%Y-%m-%dT%H:%M:%S.%fZ', but %f\n            # gives us more precision than we want, so we strftime up to %S and\n            # do the rest ourselves\n            return '%s.%03dZ' % (formatted, dt.microsecond / 1000)\n\n        return formatted\n\n    def __repr__(self):\n        return '%s(milliseconds=%r, precision=%r)' % (\n            self.__class__.__name__, self.milliseconds, self.precision\n        )\n\n\nOPEN_BOUND = DateRangeBound(value=None, precision=None)\n\"\"\"\nRepresents `*`, an open value or bound for :class:`DateRange`.\n\"\"\"\n\n\n@total_ordering\nclass DateRange(object):\n    \"\"\"DateRange(lower_bound=None, upper_bound=None, value=None)\n    DSE DateRange Type\n\n    .. attribute:: lower_bound\n\n        :class:`~DateRangeBound` representing the lower bound of a bounded range.\n\n    .. attribute:: upper_bound\n\n        :class:`~DateRangeBound` representing the upper bound of a bounded range.\n\n    .. attribute:: value\n\n        :class:`~DateRangeBound` representing the value of a single-value range.\n\n    As noted in its documentation, :class:`DateRangeBound` uses a millisecond\n    offset from the UNIX epoch to allow :class:`DateRange` to represent values\n    `datetime.datetime` cannot. For such values, string representions will show\n    this offset rather than the CQL representation.\n    \"\"\"\n    lower_bound = None\n    upper_bound = None\n    value = None\n\n    def __init__(self, lower_bound=None, upper_bound=None, value=None):\n        \"\"\"\n        :param lower_bound: a :class:`DateRangeBound` or object accepted by\n            :meth:`DateRangeBound.from_value` to be used as a\n            :attr:`lower_bound`. Mutually exclusive with `value`. If\n            `upper_bound` is specified and this is not, the :attr:`lower_bound`\n            will be open.\n        :param upper_bound: a :class:`DateRangeBound` or object accepted by\n            :meth:`DateRangeBound.from_value` to be used as a\n            :attr:`upper_bound`. Mutually exclusive with `value`. If\n            `lower_bound` is specified and this is not, the :attr:`upper_bound`\n            will be open.\n        :param value: a :class:`DateRangeBound` or object accepted by\n            :meth:`DateRangeBound.from_value` to be used as :attr:`value`. Mutually\n            exclusive with `lower_bound` and `lower_bound`.\n        \"\"\"\n\n        # if necessary, transform non-None args to DateRangeBounds\n        lower_bound = (DateRangeBound.from_value(lower_bound).round_down()\n                       if lower_bound else lower_bound)\n        upper_bound = (DateRangeBound.from_value(upper_bound).round_up()\n                       if upper_bound else upper_bound)\n        value = (DateRangeBound.from_value(value).round_down()\n                 if value else value)\n\n        # if we're using a 2-ended range but one bound isn't specified, specify\n        # an open bound\n        if lower_bound is None and upper_bound is not None:\n            lower_bound = OPEN_BOUND\n        if upper_bound is None and lower_bound is not None:\n            upper_bound = OPEN_BOUND\n\n        self.lower_bound, self.upper_bound, self.value = (\n            lower_bound, upper_bound, value\n        )\n        self.validate()\n\n    def validate(self):\n        if self.value is None:\n            if self.lower_bound is None or self.upper_bound is None:\n                raise ValueError(\n                    '%s instances where value attribute is None must set '\n                    'lower_bound or upper_bound; got %r' % (\n                        self.__class__.__name__,\n                        self\n                    )\n                )\n        else:  # self.value is not None\n            if self.lower_bound is not None or self.upper_bound is not None:\n                raise ValueError(\n                    '%s instances where value attribute is not None must not '\n                    'set lower_bound or upper_bound; got %r' % (\n                        self.__class__.__name__,\n                        self\n                    )\n                )\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n        return (self.lower_bound == other.lower_bound and\n                self.upper_bound == other.upper_bound and\n                self.value == other.value)\n\n    def __lt__(self, other):\n        return ((str(self.lower_bound), str(self.upper_bound), str(self.value)) <\n                (str(other.lower_bound), str(other.upper_bound), str(other.value)))\n\n    def __str__(self):\n        if self.value:\n            return str(self.value)\n        else:\n            return '[%s TO %s]' % (self.lower_bound, self.upper_bound)\n\n    def __repr__(self):\n        return '%s(lower_bound=%r, upper_bound=%r, value=%r)' % (\n            self.__class__.__name__,\n            self.lower_bound, self.upper_bound, self.value\n        )\n\n\n@total_ordering\nclass Version(object):\n    \"\"\"\n    Internal minimalist class to compare versions.\n    A valid version is: <int>.<int>.<int>.<int or str>.\n\n    TODO: when python2 support is removed, use packaging.version.\n    \"\"\"\n\n    _version = None\n    major = None\n    minor = 0\n    patch = 0\n    build = 0\n    prerelease = 0\n\n    def __init__(self, version):\n        self._version = version\n        if '-' in version:\n            version_without_prerelease, self.prerelease = version.split('-', 1)\n        else:\n            version_without_prerelease = version\n        parts = list(reversed(version_without_prerelease.split('.')))\n        if len(parts) > 4:\n            prerelease_string = \"-{}\".format(self.prerelease) if self.prerelease else \"\"\n            log.warning(\"Unrecognized version: {}. Only 4 components plus prerelease are supported. \"\n                        \"Assuming version as {}{}\".format(version, '.'.join(parts[:-5:-1]), prerelease_string))\n\n        try:\n            self.major = int(parts.pop())\n        except ValueError as e:\n            raise ValueError(\n                \"Couldn't parse version {}. Version should start with a number\".format(version))\\\n                .with_traceback(e.__traceback__)\n        try:\n            self.minor = int(parts.pop()) if parts else 0\n            self.patch = int(parts.pop()) if parts else 0\n\n            if parts:  # we have a build version\n                build = parts.pop()\n                try:\n                    self.build = int(build)\n                except ValueError:\n                    self.build = build\n        except ValueError:\n            assumed_version = \"{}.{}.{}.{}-{}\".format(self.major, self.minor, self.patch, self.build, self.prerelease)\n            log.warning(\"Unrecognized version {}. Assuming version as {}\".format(version, assumed_version))\n\n    def __hash__(self):\n        return self._version\n\n    def __repr__(self):\n        version_string = \"Version({0}, {1}, {2}\".format(self.major, self.minor, self.patch)\n        if self.build:\n            version_string += \", {}\".format(self.build)\n        if self.prerelease:\n            version_string += \", {}\".format(self.prerelease)\n        version_string += \")\"\n\n        return version_string\n\n    def __str__(self):\n        return self._version\n\n    @staticmethod\n    def _compare_version_part(version, other_version, cmp):\n        if not (isinstance(version, int) and\n                isinstance(other_version, int)):\n            version = str(version)\n            other_version = str(other_version)\n\n        return cmp(version, other_version)\n\n    def __eq__(self, other):\n        if not isinstance(other, Version):\n            return NotImplemented\n\n        return (self.major == other.major and\n                self.minor == other.minor and\n                self.patch == other.patch and\n                self._compare_version_part(self.build, other.build, lambda s, o: s == o) and\n                self._compare_version_part(self.prerelease, other.prerelease, lambda s, o: s == o)\n                )\n\n    def __gt__(self, other):\n        if not isinstance(other, Version):\n            return NotImplemented\n\n        is_major_ge = self.major >= other.major\n        is_minor_ge = self.minor >= other.minor\n        is_patch_ge = self.patch >= other.patch\n        is_build_gt = self._compare_version_part(self.build, other.build, lambda s, o: s > o)\n        is_build_ge = self._compare_version_part(self.build, other.build, lambda s, o: s >= o)\n\n        # By definition, a prerelease comes BEFORE the actual release, so if a version\n        # doesn't have a prerelease, it's automatically greater than anything that does\n        if self.prerelease and not other.prerelease:\n            is_prerelease_gt = False\n        elif other.prerelease and not self.prerelease:\n            is_prerelease_gt = True\n        else:\n            is_prerelease_gt = self._compare_version_part(self.prerelease, other.prerelease, lambda s, o: s > o) \\\n\n        return (self.major > other.major or\n                (is_major_ge and self.minor > other.minor) or\n                (is_major_ge and is_minor_ge and self.patch > other.patch) or\n                (is_major_ge and is_minor_ge and is_patch_ge and is_build_gt) or\n                (is_major_ge and is_minor_ge and is_patch_ge and is_build_ge and is_prerelease_gt)\n                )\n\n\ndef maybe_add_timeout_to_query(stmt: str, metadata_request_timeout: Optional[datetime.timedelta]) -> str:\n    if metadata_request_timeout is None:\n        return stmt\n    ms = int(metadata_request_timeout / datetime.timedelta(milliseconds=1))\n    if ms == 0:\n        return stmt\n    return f\"{stmt} USING TIMEOUT {ms}ms\"\n",
    "cassandra/policies.py": "# Copyright DataStax, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport random\n\nfrom collections import namedtuple\nfrom functools import lru_cache\nfrom itertools import islice, cycle, groupby, repeat\nimport logging\nfrom random import randint, shuffle\nfrom threading import Lock\nimport socket\nimport warnings\n\nlog = logging.getLogger(__name__)\n\nfrom cassandra import WriteType as WT\n\n\n# This is done this way because WriteType was originally\n# defined here and in order not to break the API.\n# It may removed in the next mayor.\nWriteType = WT\n\nfrom cassandra import ConsistencyLevel, OperationTimedOut\n\nclass HostDistance(object):\n    \"\"\"\n    A measure of how \"distant\" a node is from the client, which\n    may influence how the load balancer distributes requests\n    and how many connections are opened to the node.\n    \"\"\"\n\n    IGNORED = -1\n    \"\"\"\n    A node with this distance should never be queried or have\n    connections opened to it.\n    \"\"\"\n\n    LOCAL_RACK = 0\n    \"\"\"\n    Nodes with ``LOCAL_RACK`` distance will be preferred for operations\n    under some load balancing policies (such as :class:`.RackAwareRoundRobinPolicy`)\n    and will have a greater number of connections opened against\n    them by default.\n\n    This distance is typically used for nodes within the same\n    datacenter and the same rack as the client.\n    \"\"\"\n\n    LOCAL = 1\n    \"\"\"\n    Nodes with ``LOCAL`` distance will be preferred for operations\n    under some load balancing policies (such as :class:`.DCAwareRoundRobinPolicy`)\n    and will have a greater number of connections opened against\n    them by default.\n\n    This distance is typically used for nodes within the same\n    datacenter as the client.\n    \"\"\"\n\n    REMOTE = 2\n    \"\"\"\n    Nodes with ``REMOTE`` distance will be treated as a last resort\n    by some load balancing policies (such as :class:`.DCAwareRoundRobinPolicy`\n    and :class:`.RackAwareRoundRobinPolicy`)and will have a smaller number of\n    connections opened against them by default.\n\n    This distance is typically used for nodes outside of the\n    datacenter that the client is running in.\n    \"\"\"\n\n\nclass HostStateListener(object):\n\n    def on_up(self, host):\n        \"\"\" Called when a node is marked up. \"\"\"\n        raise NotImplementedError()\n\n    def on_down(self, host):\n        \"\"\" Called when a node is marked down. \"\"\"\n        raise NotImplementedError()\n\n    def on_add(self, host):\n        \"\"\"\n        Called when a node is added to the cluster.  The newly added node\n        should be considered up.\n        \"\"\"\n        raise NotImplementedError()\n\n    def on_remove(self, host):\n        \"\"\" Called when a node is removed from the cluster. \"\"\"\n        raise NotImplementedError()\n\n\nclass LoadBalancingPolicy(HostStateListener):\n    \"\"\"\n    Load balancing policies are used to decide how to distribute\n    requests among all possible coordinator nodes in the cluster.\n\n    In particular, they may focus on querying \"near\" nodes (those\n    in a local datacenter) or on querying nodes who happen to\n    be replicas for the requested data.\n\n    You may also use subclasses of :class:`.LoadBalancingPolicy` for\n    custom behavior.\n\n    You should always use immutable collections (e.g., tuples or\n    frozensets) to store information about hosts to prevent accidental\n    modification. When there are changes to the hosts (e.g., a host is\n    down or up), the old collection should be replaced with a new one.\n    \"\"\"\n\n    _hosts_lock = None\n\n    def __init__(self):\n        self._hosts_lock = Lock()\n\n    def distance(self, host):\n        \"\"\"\n        Returns a measure of how remote a :class:`~.pool.Host` is in\n        terms of the :class:`.HostDistance` enums.\n        \"\"\"\n        raise NotImplementedError()\n\n    def populate(self, cluster, hosts):\n        \"\"\"\n        This method is called to initialize the load balancing\n        policy with a set of :class:`.Host` instances before its\n        first use.  The `cluster` parameter is an instance of\n        :class:`.Cluster`.\n        \"\"\"\n        raise NotImplementedError()\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        \"\"\"\n        Given a :class:`~.query.Statement` instance, return a iterable\n        of :class:`.Host` instances which should be queried in that\n        order.  A generator may work well for custom implementations\n        of this method.\n\n        Note that the `query` argument may be :const:`None` when preparing\n        statements.\n\n        `working_keyspace` should be the string name of the current keyspace,\n        as set through :meth:`.Session.set_keyspace()` or with a ``USE``\n        statement.\n        \"\"\"\n        raise NotImplementedError()\n\n    def check_supported(self):\n        \"\"\"\n        This will be called after the cluster Metadata has been initialized.\n        If the load balancing policy implementation cannot be supported for\n        some reason (such as a missing C extension), this is the point at\n        which it should raise an exception.\n        \"\"\"\n        pass\n\n\nclass RoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    A subclass of :class:`.LoadBalancingPolicy` which evenly\n    distributes queries across all nodes in the cluster,\n    regardless of what datacenter the nodes may be in.\n    \"\"\"\n    _live_hosts = frozenset(())\n    _position = 0\n\n    def populate(self, cluster, hosts):\n        self._live_hosts = frozenset(hosts)\n        if len(hosts) > 1:\n            self._position = randint(0, len(hosts) - 1)\n\n    def distance(self, host):\n        return HostDistance.LOCAL\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        # not thread-safe, but we don't care much about lost increments\n        # for the purposes of load balancing\n        pos = self._position\n        self._position += 1\n\n        hosts = self._live_hosts\n        length = len(hosts)\n        if length:\n            pos %= length\n            return islice(cycle(hosts), pos, pos + length)\n        else:\n            return []\n\n    def on_up(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.union((host, ))\n\n    def on_down(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.difference((host, ))\n\n    def on_add(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.union((host, ))\n\n    def on_remove(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.difference((host, ))\n\n\nclass DCAwareRoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    Similar to :class:`.RoundRobinPolicy`, but prefers hosts\n    in the local datacenter and only uses nodes in remote\n    datacenters as a last resort.\n    \"\"\"\n\n    local_dc = None\n    used_hosts_per_remote_dc = 0\n\n    def __init__(self, local_dc='', used_hosts_per_remote_dc=0):\n        \"\"\"\n        The `local_dc` parameter should be the name of the datacenter\n        (such as is reported by ``nodetool ring``) that should\n        be considered local. If not specified, the driver will choose\n        a local_dc based on the first host among :attr:`.Cluster.contact_points`\n        having a valid DC. If relying on this mechanism, all specified\n        contact points should be nodes in a single, local DC.\n\n        `used_hosts_per_remote_dc` controls how many nodes in\n        each remote datacenter will have connections opened\n        against them. In other words, `used_hosts_per_remote_dc` hosts\n        will be considered :attr:`~.HostDistance.REMOTE` and the\n        rest will be considered :attr:`~.HostDistance.IGNORED`.\n        By default, all remote hosts are ignored.\n        \"\"\"\n        self.local_dc = local_dc\n        self.used_hosts_per_remote_dc = used_hosts_per_remote_dc\n        self._dc_live_hosts = {}\n        self._position = 0\n        self._endpoints = []\n        LoadBalancingPolicy.__init__(self)\n\n    def _dc(self, host):\n        return host.datacenter or self.local_dc\n\n    def populate(self, cluster, hosts):\n        for dc, dc_hosts in groupby(hosts, lambda h: self._dc(h)):\n            self._dc_live_hosts[dc] = tuple(set(dc_hosts))\n\n        if not self.local_dc:\n            self._endpoints = [\n                endpoint\n                for endpoint in cluster.endpoints_resolved]\n\n        self._position = randint(0, len(hosts) - 1) if hosts else 0\n\n    def distance(self, host):\n        dc = self._dc(host)\n        if dc == self.local_dc:\n            return HostDistance.LOCAL\n\n        if not self.used_hosts_per_remote_dc:\n            return HostDistance.IGNORED\n        else:\n            dc_hosts = self._dc_live_hosts.get(dc)\n            if not dc_hosts:\n                return HostDistance.IGNORED\n\n            if host in list(dc_hosts)[:self.used_hosts_per_remote_dc]:\n                return HostDistance.REMOTE\n            else:\n                return HostDistance.IGNORED\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        # not thread-safe, but we don't care much about lost increments\n        # for the purposes of load balancing\n        pos = self._position\n        self._position += 1\n\n        local_live = self._dc_live_hosts.get(self.local_dc, ())\n        pos = (pos % len(local_live)) if local_live else 0\n        for host in islice(cycle(local_live), pos, pos + len(local_live)):\n            yield host\n\n        # the dict can change, so get candidate DCs iterating over keys of a copy\n        other_dcs = [dc for dc in self._dc_live_hosts.copy().keys() if dc != self.local_dc]\n        for dc in other_dcs:\n            remote_live = self._dc_live_hosts.get(dc, ())\n            for host in remote_live[:self.used_hosts_per_remote_dc]:\n                yield host\n\n    def on_up(self, host):\n        # not worrying about threads because this will happen during\n        # control connection startup/refresh\n        if not self.local_dc and host.datacenter:\n            if host.endpoint in self._endpoints:\n                self.local_dc = host.datacenter\n                log.info(\"Using datacenter '%s' for DCAwareRoundRobinPolicy (via host '%s'); \"\n                         \"if incorrect, please specify a local_dc to the constructor, \"\n                         \"or limit contact points to local cluster nodes\" %\n                         (self.local_dc, host.endpoint))\n                del self._endpoints\n\n        dc = self._dc(host)\n        with self._hosts_lock:\n            current_hosts = self._dc_live_hosts.get(dc, ())\n            if host not in current_hosts:\n                self._dc_live_hosts[dc] = current_hosts + (host, )\n\n    def on_down(self, host):\n        dc = self._dc(host)\n        with self._hosts_lock:\n            current_hosts = self._dc_live_hosts.get(dc, ())\n            if host in current_hosts:\n                hosts = tuple(h for h in current_hosts if h != host)\n                if hosts:\n                    self._dc_live_hosts[dc] = hosts\n                else:\n                    del self._dc_live_hosts[dc]\n\n    def on_add(self, host):\n        self.on_up(host)\n\n    def on_remove(self, host):\n        self.on_down(host)\n\nclass RackAwareRoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    Similar to :class:`.DCAwareRoundRobinPolicy`, but prefers hosts\n    in the local rack, before hosts in the local datacenter but a\n    different rack, before hosts in all other datercentres\n    \"\"\"\n\n    local_dc = None\n    local_rack = None\n    used_hosts_per_remote_dc = 0\n\n    def __init__(self, local_dc, local_rack, used_hosts_per_remote_dc=0):\n        \"\"\"\n        The `local_dc` and `local_rack` parameters should be the name of the\n        datacenter and rack (such as is reported by ``nodetool ring``) that\n        should be considered local.\n\n        `used_hosts_per_remote_dc` controls how many nodes in\n        each remote datacenter will have connections opened\n        against them. In other words, `used_hosts_per_remote_dc` hosts\n        will be considered :attr:`~.HostDistance.REMOTE` and the\n        rest will be considered :attr:`~.HostDistance.IGNORED`.\n        By default, all remote hosts are ignored.\n        \"\"\"\n        self.local_rack = local_rack\n        self.local_dc = local_dc\n        self.used_hosts_per_remote_dc = used_hosts_per_remote_dc\n        self._live_hosts = {}\n        self._dc_live_hosts = {}\n        self._endpoints = []\n        self._position = 0\n        LoadBalancingPolicy.__init__(self)\n\n    def _rack(self, host):\n        return host.rack or self.local_rack\n\n    def _dc(self, host):\n        return host.datacenter or self.local_dc\n\n    def populate(self, cluster, hosts):\n        for (dc, rack), rack_hosts in groupby(hosts, lambda host: (self._dc(host), self._rack(host))):\n            self._live_hosts[(dc, rack)] = tuple(set(rack_hosts))\n        for dc, dc_hosts in groupby(hosts, lambda host: self._dc(host)):\n            self._dc_live_hosts[dc] = tuple(set(dc_hosts))\n\n        self._position = randint(0, len(hosts) - 1) if hosts else 0\n\n    def distance(self, host):\n        rack = self._rack(host)\n        dc = self._dc(host)\n        if rack == self.local_rack and dc == self.local_dc:\n            return HostDistance.LOCAL_RACK\n\n        if dc == self.local_dc:\n            return HostDistance.LOCAL\n\n        if not self.used_hosts_per_remote_dc:\n            return HostDistance.IGNORED\n\n        dc_hosts = self._dc_live_hosts.get(dc, ())\n        if not dc_hosts:\n            return HostDistance.IGNORED\n        if host in dc_hosts and dc_hosts.index(host) < self.used_hosts_per_remote_dc:\n            return HostDistance.REMOTE\n        else:\n            return HostDistance.IGNORED\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        pos = self._position\n        self._position += 1\n\n        local_rack_live = self._live_hosts.get((self.local_dc, self.local_rack), ())\n        pos = (pos % len(local_rack_live)) if local_rack_live else 0\n        # Slice the cyclic iterator to start from pos and include the next len(local_live) elements\n        # This ensures we get exactly one full cycle starting from pos\n        for host in islice(cycle(local_rack_live), pos, pos + len(local_rack_live)):\n            yield host\n\n        local_live = [host for host in self._dc_live_hosts.get(self.local_dc, ()) if host.rack != self.local_rack]\n        pos = (pos % len(local_live)) if local_live else 0\n        for host in islice(cycle(local_live), pos, pos + len(local_live)):\n            yield host\n\n        # the dict can change, so get candidate DCs iterating over keys of a copy\n        for dc, remote_live in self._dc_live_hosts.copy().items():\n            if dc != self.local_dc:\n                for host in remote_live[:self.used_hosts_per_remote_dc]:\n                    yield host\n\n    def on_up(self, host):\n        dc = self._dc(host)\n        rack = self._rack(host)\n        with self._hosts_lock:\n            current_rack_hosts = self._live_hosts.get((dc, rack), ())\n            if host not in current_rack_hosts:\n                self._live_hosts[(dc, rack)] = current_rack_hosts + (host, )\n            current_dc_hosts = self._dc_live_hosts.get(dc, ())\n            if host not in current_dc_hosts:\n                self._dc_live_hosts[dc] = current_dc_hosts + (host, )\n\n    def on_down(self, host):\n        dc = self._dc(host)\n        rack = self._rack(host)\n        with self._hosts_lock:\n            current_rack_hosts = self._live_hosts.get((dc, rack), ())\n            if host in current_rack_hosts:\n                hosts = tuple(h for h in current_rack_hosts if h != host)\n                if hosts:\n                    self._live_hosts[(dc, rack)] = hosts\n                else:\n                    del self._live_hosts[(dc, rack)]\n            current_dc_hosts = self._dc_live_hosts.get(dc, ())\n            if host in current_dc_hosts:\n                hosts = tuple(h for h in current_dc_hosts if h != host)\n                if hosts:\n                    self._dc_live_hosts[dc] = hosts\n                else:\n                    del self._dc_live_hosts[dc]\n\n    def on_add(self, host):\n        self.on_up(host)\n\n    def on_remove(self, host):\n        self.on_down(host)\n\nclass TokenAwarePolicy(LoadBalancingPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` wrapper that adds token awareness to\n    a child policy.\n\n    This alters the child policy's behavior so that it first attempts to\n    send queries to :attr:`~.HostDistance.LOCAL` replicas (as determined\n    by the child policy) based on the :class:`.Statement`'s\n    :attr:`~.Statement.routing_key`. If :attr:`.shuffle_replicas` is\n    truthy, these replicas will be yielded in a random order. Once those\n    hosts are exhausted, the remaining hosts in the child policy's query\n    plan will be used in the order provided by the child policy.\n\n    If no :attr:`~.Statement.routing_key` is set on the query, the child\n    policy's query plan will be used as is.\n    \"\"\"\n\n    _child_policy = None\n    _cluster_metadata = None\n    _tablets_routing_v1 = False\n    shuffle_replicas = False\n    \"\"\"\n    Yield local replicas in a random order.\n    \"\"\"\n\n    def __init__(self, child_policy, shuffle_replicas=False):\n        self._child_policy = child_policy\n        self.shuffle_replicas = shuffle_replicas\n\n    def populate(self, cluster, hosts):\n        self._cluster_metadata = cluster.metadata\n        self._tablets_routing_v1 = cluster.control_connection._tablets_routing_v1\n        self._child_policy.populate(cluster, hosts)\n\n    def check_supported(self):\n        if not self._cluster_metadata.can_support_partitioner():\n            raise RuntimeError(\n                '%s cannot be used with the cluster partitioner (%s) because '\n                'the relevant C extension for this driver was not compiled. '\n                'See the installation instructions for details on building '\n                'and installing the C extensions.' %\n                (self.__class__.__name__, self._cluster_metadata.partitioner))\n\n    def distance(self, *args, **kwargs):\n        return self._child_policy.distance(*args, **kwargs)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        keyspace = query.keyspace if query and query.keyspace else working_keyspace\n\n        child = self._child_policy\n        if query is None or query.routing_key is None or keyspace is None:\n            for host in child.make_query_plan(keyspace, query):\n                yield host\n            return\n\n        replicas = []\n        if self._tablets_routing_v1:\n            tablet = self._cluster_metadata._tablets.get_tablet_for_key(\n                keyspace, query.table, self._cluster_metadata.token_map.token_class.from_key(query.routing_key))\n\n            if tablet is not None:\n                replicas_mapped = set(map(lambda r: r[0], tablet.replicas))\n                child_plan = child.make_query_plan(keyspace, query)\n\n                replicas = [host for host in child_plan if host.host_id in replicas_mapped]\n\n        if not replicas:\n            replicas = self._cluster_metadata.get_replicas(keyspace, query.routing_key)\n\n        if self.shuffle_replicas:\n            shuffle(replicas)\n\n        for replica in replicas:\n            if replica.is_up and child.distance(replica) in [HostDistance.LOCAL, HostDistance.LOCAL_RACK]:\n                yield replica\n\n        for host in child.make_query_plan(keyspace, query):\n            # skip if we've already listed this host\n            if host not in replicas or child.distance(host) == HostDistance.REMOTE:\n                yield host\n\n    def on_up(self, *args, **kwargs):\n        return self._child_policy.on_up(*args, **kwargs)\n\n    def on_down(self, *args, **kwargs):\n        return self._child_policy.on_down(*args, **kwargs)\n\n    def on_add(self, *args, **kwargs):\n        return self._child_policy.on_add(*args, **kwargs)\n\n    def on_remove(self, *args, **kwargs):\n        return self._child_policy.on_remove(*args, **kwargs)\n\n\nclass WhiteListRoundRobinPolicy(RoundRobinPolicy):\n    \"\"\"\n    A subclass of :class:`.RoundRobinPolicy` which evenly\n    distributes queries across all nodes in the cluster,\n    regardless of what datacenter the nodes may be in, but\n    only if that node exists in the list of allowed nodes\n\n    This policy is addresses the issue described in\n    https://datastax-oss.atlassian.net/browse/JAVA-145\n    Where connection errors occur when connection\n    attempts are made to private IP addresses remotely\n    \"\"\"\n\n    def __init__(self, hosts):\n        \"\"\"\n        The `hosts` parameter should be a sequence of hosts to permit\n        connections to.\n        \"\"\"\n        self._allowed_hosts = tuple(hosts)\n        self._allowed_hosts_resolved = []\n        for h in self._allowed_hosts:\n            unix_socket_path = getattr(h, \"_unix_socket_path\", None)\n            if unix_socket_path:\n                self._allowed_hosts_resolved.append(unix_socket_path)\n            else:\n                self._allowed_hosts_resolved.extend([endpoint[4][0]\n                                        for endpoint in socket.getaddrinfo(h, None, socket.AF_UNSPEC, socket.SOCK_STREAM)])\n\n        RoundRobinPolicy.__init__(self)\n\n    def populate(self, cluster, hosts):\n        self._live_hosts = frozenset(h for h in hosts if h.address in self._allowed_hosts_resolved)\n\n        if len(hosts) <= 1:\n            self._position = 0\n        else:\n            self._position = randint(0, len(hosts) - 1)\n\n    def distance(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            return HostDistance.LOCAL\n        else:\n            return HostDistance.IGNORED\n\n    def on_up(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            RoundRobinPolicy.on_up(self, host)\n\n    def on_add(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            RoundRobinPolicy.on_add(self, host)\n\n\nclass HostFilterPolicy(LoadBalancingPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` subclass configured with a child policy,\n    and a single-argument predicate. This policy defers to the child policy for\n    hosts where ``predicate(host)`` is truthy. Hosts for which\n    ``predicate(host)`` is falsy will be considered :attr:`.IGNORED`, and will\n    not be used in a query plan.\n\n    This can be used in the cases where you need a whitelist or blacklist\n    policy, e.g. to prepare for decommissioning nodes or for testing:\n\n    .. code-block:: python\n\n        def address_is_ignored(host):\n            return host.address in [ignored_address0, ignored_address1]\n\n        blacklist_filter_policy = HostFilterPolicy(\n            child_policy=RoundRobinPolicy(),\n            predicate=address_is_ignored\n        )\n\n        cluster = Cluster(\n            primary_host,\n            load_balancing_policy=blacklist_filter_policy,\n        )\n\n    See the note in the :meth:`.make_query_plan` documentation for a caveat on\n    how wrapping ordering polices (e.g. :class:`.RoundRobinPolicy`) may break\n    desirable properties of the wrapped policy.\n\n    Please note that whitelist and blacklist policies are not recommended for\n    general, day-to-day use. You probably want something like\n    :class:`.DCAwareRoundRobinPolicy`, which prefers a local DC but has\n    fallbacks, over a brute-force method like whitelisting or blacklisting.\n    \"\"\"\n\n    def __init__(self, child_policy, predicate):\n        \"\"\"\n        :param child_policy: an instantiated :class:`.LoadBalancingPolicy`\n                             that this one will defer to.\n        :param predicate: a one-parameter function that takes a :class:`.Host`.\n                          If it returns a falsy value, the :class:`.Host` will\n                          be :attr:`.IGNORED` and not returned in query plans.\n        \"\"\"\n        super(HostFilterPolicy, self).__init__()\n        self._child_policy = child_policy\n        self._predicate = predicate\n\n    def on_up(self, host, *args, **kwargs):\n        return self._child_policy.on_up(host, *args, **kwargs)\n\n    def on_down(self, host, *args, **kwargs):\n        return self._child_policy.on_down(host, *args, **kwargs)\n\n    def on_add(self, host, *args, **kwargs):\n        return self._child_policy.on_add(host, *args, **kwargs)\n\n    def on_remove(self, host, *args, **kwargs):\n        return self._child_policy.on_remove(host, *args, **kwargs)\n\n    @property\n    def predicate(self):\n        \"\"\"\n        A predicate, set on object initialization, that takes a :class:`.Host`\n        and returns a value. If the value is falsy, the :class:`.Host` is\n        :class:`~HostDistance.IGNORED`. If the value is truthy,\n        :class:`.HostFilterPolicy` defers to the child policy to determine the\n        host's distance.\n\n        This is a read-only value set in ``__init__``, implemented as a\n        ``property``.\n        \"\"\"\n        return self._predicate\n\n    def distance(self, host):\n        \"\"\"\n        Checks if ``predicate(host)``, then returns\n        :attr:`~HostDistance.IGNORED` if falsy, and defers to the child policy\n        otherwise.\n        \"\"\"\n        if self.predicate(host):\n            return self._child_policy.distance(host)\n        else:\n            return HostDistance.IGNORED\n\n    def populate(self, cluster, hosts):\n        self._child_policy.populate(cluster=cluster, hosts=hosts)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        \"\"\"\n        Defers to the child policy's\n        :meth:`.LoadBalancingPolicy.make_query_plan` and filters the results.\n\n        Note that this filtering may break desirable properties of the wrapped\n        policy in some cases. For instance, imagine if you configure this\n        policy to filter out ``host2``, and to wrap a round-robin policy that\n        rotates through three hosts in the order ``host1, host2, host3``,\n        ``host2, host3, host1``, ``host3, host1, host2``, repeating. This\n        policy will yield ``host1, host3``, ``host3, host1``, ``host3, host1``,\n        disproportionately favoring ``host3``.\n        \"\"\"\n        child_qp = self._child_policy.make_query_plan(\n            working_keyspace=working_keyspace, query=query\n        )\n        for host in child_qp:\n            if self.predicate(host):\n                yield host\n\n    def check_supported(self):\n        return self._child_policy.check_supported()\n\n\nclass ConvictionPolicy(object):\n    \"\"\"\n    A policy which decides when hosts should be considered down\n    based on the types of failures and the number of failures.\n\n    If custom behavior is needed, this class may be subclassed.\n    \"\"\"\n\n    def __init__(self, host):\n        \"\"\"\n        `host` is an instance of :class:`.Host`.\n        \"\"\"\n        self.host = host\n\n    def add_failure(self, connection_exc):\n        \"\"\"\n        Implementations should return :const:`True` if the host should be\n        convicted, :const:`False` otherwise.\n        \"\"\"\n        raise NotImplementedError()\n\n    def reset(self):\n        \"\"\"\n        Implementations should clear out any convictions or state regarding\n        the host.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass SimpleConvictionPolicy(ConvictionPolicy):\n    \"\"\"\n    The default implementation of :class:`ConvictionPolicy`,\n    which simply marks a host as down after the first failure\n    of any kind.\n    \"\"\"\n\n    def add_failure(self, connection_exc):\n        return not isinstance(connection_exc, OperationTimedOut)\n\n    def reset(self):\n        pass\n\n\nclass ReconnectionPolicy(object):\n    \"\"\"\n    This class and its subclasses govern how frequently an attempt is made\n    to reconnect to nodes that are marked as dead.\n\n    If custom behavior is needed, this class may be subclassed.\n    \"\"\"\n\n    def new_schedule(self):\n        \"\"\"\n        This should return a finite or infinite iterable of delays (each as a\n        floating point number of seconds) in-between each failed reconnection\n        attempt.  Note that if the iterable is finite, reconnection attempts\n        will cease once the iterable is exhausted.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass ConstantReconnectionPolicy(ReconnectionPolicy):\n    \"\"\"\n    A :class:`.ReconnectionPolicy` subclass which sleeps for a fixed delay\n    in-between each reconnection attempt.\n    \"\"\"\n\n    def __init__(self, delay, max_attempts=64):\n        \"\"\"\n        `delay` should be a floating point number of seconds to wait in-between\n        each attempt.\n\n        `max_attempts` should be a total number of attempts to be made before\n        giving up, or :const:`None` to continue reconnection attempts forever.\n        The default is 64.\n        \"\"\"\n        if delay < 0:\n            raise ValueError(\"delay must not be negative\")\n        if max_attempts is not None and max_attempts < 0:\n            raise ValueError(\"max_attempts must not be negative\")\n\n        self.delay = delay\n        self.max_attempts = max_attempts\n\n    def new_schedule(self):\n        if self.max_attempts:\n            return repeat(self.delay, self.max_attempts)\n        return repeat(self.delay)\n\n\nclass ExponentialReconnectionPolicy(ReconnectionPolicy):\n    \"\"\"\n    A :class:`.ReconnectionPolicy` subclass which exponentially increases\n    the length of the delay in-between each reconnection attempt up to\n    a set maximum delay.\n\n    A random amount of jitter (+/- 15%) will be added to the pure exponential\n    delay value to avoid the situations where many reconnection handlers are\n    trying to reconnect at exactly the same time.\n    \"\"\"\n\n    # TODO: max_attempts is 64 to preserve legacy default behavior\n    # consider changing to None in major release to prevent the policy\n    # giving up forever\n    def __init__(self, base_delay, max_delay, max_attempts=64):\n        \"\"\"\n        `base_delay` and `max_delay` should be in floating point units of\n        seconds.\n\n        `max_attempts` should be a total number of attempts to be made before\n        giving up, or :const:`None` to continue reconnection attempts forever.\n        The default is 64.\n        \"\"\"\n        if base_delay < 0 or max_delay < 0:\n            raise ValueError(\"Delays may not be negative\")\n\n        if max_delay < base_delay:\n            raise ValueError(\"Max delay must be greater than base delay\")\n\n        if max_attempts is not None and max_attempts < 0:\n            raise ValueError(\"max_attempts must not be negative\")\n\n        self.base_delay = base_delay\n        self.max_delay = max_delay\n        self.max_attempts = max_attempts\n\n    def new_schedule(self):\n        i, overflowed = 0, False\n        while self.max_attempts is None or i < self.max_attempts:\n            if overflowed:\n                yield self.max_delay\n            else:\n                try:\n                    yield self._add_jitter(min(self.base_delay * (2 ** i), self.max_delay))\n                except OverflowError:\n                    overflowed = True\n                    yield self.max_delay\n\n            i += 1\n\n    # Adds -+ 15% to the delay provided\n    def _add_jitter(self, value):\n        jitter = randint(85, 115)\n        delay = (jitter * value) / 100\n        return min(max(self.base_delay, delay), self.max_delay)\n\n\nclass RetryPolicy(object):\n    \"\"\"\n    A policy that describes whether to retry, rethrow, or ignore coordinator\n    timeout and unavailable failures. These are failures reported from the\n    server side. Timeouts are configured by\n    `settings in cassandra.yaml <https://github.com/apache/cassandra/blob/cassandra-2.1.4/conf/cassandra.yaml#L568-L584>`_.\n    Unavailable failures occur when the coordinator cannot achieve the consistency\n    level for a request. For further information see the method descriptions\n    below.\n\n    To specify a default retry policy, set the\n    :attr:`.Cluster.default_retry_policy` attribute to an instance of this\n    class or one of its subclasses.\n\n    To specify a retry policy per query, set the :attr:`.Statement.retry_policy`\n    attribute to an instance of this class or one of its subclasses.\n\n    If custom behavior is needed for retrying certain operations,\n    this class may be subclassed.\n    \"\"\"\n\n    RETRY = 0\n    \"\"\"\n    This should be returned from the below methods if the operation\n    should be retried on the same connection.\n    \"\"\"\n\n    RETHROW = 1\n    \"\"\"\n    This should be returned from the below methods if the failure\n    should be propagated and no more retries attempted.\n    \"\"\"\n\n    IGNORE = 2\n    \"\"\"\n    This should be returned from the below methods if the failure\n    should be ignored but no more retries should be attempted.\n    \"\"\"\n\n    RETRY_NEXT_HOST = 3\n    \"\"\"\n    This should be returned from the below methods if the operation\n    should be retried on another connection.\n    \"\"\"\n\n    def on_read_timeout(self, query, consistency, required_responses,\n                        received_responses, data_retrieved, retry_num):\n        \"\"\"\n        This is called when a read operation times out from the coordinator's\n        perspective (i.e. a replica did not respond to the coordinator in time).\n        It should return a tuple with two items: one of the class enums (such\n        as :attr:`.RETRY`) and a :class:`.ConsistencyLevel` to retry the\n        operation at or :const:`None` to keep the same consistency level.\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        The `required_responses` and `received_responses` parameters describe\n        how many replicas needed to respond to meet the requested consistency\n        level and how many actually did respond before the coordinator timed\n        out the request. `data_retrieved` is a boolean indicating whether\n        any of those responses contained data (as opposed to just a digest).\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, operations will be retried at most once, and only if\n        a sufficient number of replicas responded (with data digests).\n        \"\"\"\n        if retry_num != 0:\n            return self.RETHROW, None\n        elif received_responses >= required_responses and not data_retrieved:\n            return self.RETRY, consistency\n        else:\n            return self.RETHROW, None\n\n    def on_write_timeout(self, query, consistency, write_type,\n                         required_responses, received_responses, retry_num):\n        \"\"\"\n        This is called when a write operation times out from the coordinator's\n        perspective (i.e. a replica did not respond to the coordinator in time).\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `write_type` is one of the :class:`.WriteType` enums describing the\n        type of write operation.\n\n        The `required_responses` and `received_responses` parameters describe\n        how many replicas needed to acknowledge the write to meet the requested\n        consistency level and how many replicas actually did acknowledge the\n        write before the coordinator timed out the request.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, failed write operations will retried at most once, and\n        they will only be retried if the `write_type` was\n        :attr:`~.WriteType.BATCH_LOG`.\n        \"\"\"\n        if retry_num != 0:\n            return self.RETHROW, None\n        elif write_type == WriteType.BATCH_LOG:\n            return self.RETRY, consistency\n        else:\n            return self.RETHROW, None\n\n    def on_unavailable(self, query, consistency, required_replicas, alive_replicas, retry_num):\n        \"\"\"\n        This is called when the coordinator node determines that a read or\n        write operation cannot be successful because the number of live\n        replicas are too low to meet the requested :class:`.ConsistencyLevel`.\n        This means that the read or write operation was never forwarded to\n        any replicas.\n\n        `query` is the :class:`.Statement` that failed.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `required_replicas` is the number of replicas that would have needed to\n        acknowledge the operation to meet the requested consistency level.\n        `alive_replicas` is the number of replicas that the coordinator\n        considered alive at the time of the request.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, if this is the first retry, it triggers a retry on the next\n        host in the query plan with the same consistency level. If this is not the\n        first retry, no retries will be attempted and the error will be re-raised.\n        \"\"\"\n        return (self.RETRY_NEXT_HOST, None) if retry_num == 0 else (self.RETHROW, None)\n\n    def on_request_error(self, query, consistency, error, retry_num):\n        \"\"\"\n        This is called when an unexpected error happens. This can be in the\n        following situations:\n\n        * On a connection error\n        * On server errors: overloaded, isBootstrapping, serverError, etc.\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `error` the instance of the exception.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, it triggers a retry on the next host in the query plan\n        with the same consistency level.\n        \"\"\"\n        # TODO revisit this for the next major\n        # To preserve the same behavior than before, we don't take retry_num into account\n        return self.RETRY_NEXT_HOST, None\n\n\nclass FallthroughRetryPolicy(RetryPolicy):\n    \"\"\"\n    A retry policy that never retries and always propagates failures to\n    the application.\n    \"\"\"\n\n    def on_read_timeout(self, *args, **kwargs):\n        return self.RETHROW, None\n\n    def on_write_timeout(self, *args, **kwargs):\n        return self.RETHROW, None\n\n    def on_unavailable(self, *args, **kwargs):\n        return self.RETHROW, None\n\n    def on_request_error(self, *args, **kwargs):\n        return self.RETHROW, None\n\n\nclass DowngradingConsistencyRetryPolicy(RetryPolicy):\n    \"\"\"\n    *Deprecated:* This retry policy will be removed in the next major release.\n\n    A retry policy that sometimes retries with a lower consistency level than\n    the one initially requested.\n\n    **BEWARE**: This policy may retry queries using a lower consistency\n    level than the one initially requested. By doing so, it may break\n    consistency guarantees. In other words, if you use this retry policy,\n    there are cases (documented below) where a read at :attr:`~.QUORUM`\n    *may not* see a preceding write at :attr:`~.QUORUM`. Do not use this\n    policy unless you have understood the cases where this can happen and\n    are ok with that. It is also recommended to subclass this class so\n    that queries that required a consistency level downgrade can be\n    recorded (so that repairs can be made later, etc).\n\n    This policy implements the same retries as :class:`.RetryPolicy`,\n    but on top of that, it also retries in the following cases:\n\n    * On a read timeout: if the number of replicas that responded is\n      greater than one but lower than is required by the requested\n      consistency level, the operation is retried at a lower consistency\n      level.\n    * On a write timeout: if the operation is an :attr:`~.UNLOGGED_BATCH`\n      and at least one replica acknowledged the write, the operation is\n      retried at a lower consistency level.  Furthermore, for other\n      write types, if at least one replica acknowledged the write, the\n      timeout is ignored.\n    * On an unavailable exception: if at least one replica is alive, the\n      operation is retried at a lower consistency level.\n\n    The reasoning behind this retry policy is as follows: if, based\n    on the information the Cassandra coordinator node returns, retrying the\n    operation with the initially requested consistency has a chance to\n    succeed, do it. Otherwise, if based on that information we know the\n    initially requested consistency level cannot be achieved currently, then:\n\n    * For writes, ignore the exception (thus silently failing the\n      consistency requirement) if we know the write has been persisted on at\n      least one replica.\n    * For reads, try reading at a lower consistency level (thus silently\n      failing the consistency requirement).\n\n    In other words, this policy implements the idea that if the requested\n    consistency level cannot be achieved, the next best thing for writes is\n    to make sure the data is persisted, and that reading something is better\n    than reading nothing, even if there is a risk of reading stale data.\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(DowngradingConsistencyRetryPolicy, self).__init__(*args, **kwargs)\n        warnings.warn('DowngradingConsistencyRetryPolicy is deprecated '\n                      'and will be removed in the next major release.',\n                      DeprecationWarning)\n\n    def _pick_consistency(self, num_responses):\n        if num_responses >= 3:\n            return self.RETRY, ConsistencyLevel.THREE\n        elif num_responses >= 2:\n            return self.RETRY, ConsistencyLevel.TWO\n        elif num_responses >= 1:\n            return self.RETRY, ConsistencyLevel.ONE\n        else:\n            return self.RETHROW, None\n\n    def on_read_timeout(self, query, consistency, required_responses,\n                        received_responses, data_retrieved, retry_num):\n        if retry_num != 0:\n            return self.RETHROW, None\n        elif ConsistencyLevel.is_serial(consistency):\n            # Downgrading does not make sense for a CAS read query\n            return self.RETHROW, None\n        elif received_responses < required_responses:\n            return self._pick_consistency(received_responses)\n        elif not data_retrieved:\n            return self.RETRY, consistency\n        else:\n            return self.RETHROW, None\n\n    def on_write_timeout(self, query, consistency, write_type,\n                         required_responses, received_responses, retry_num):\n        if retry_num != 0:\n            return self.RETHROW, None\n\n        if write_type in (WriteType.SIMPLE, WriteType.BATCH, WriteType.COUNTER):\n            if received_responses > 0:\n                # persisted on at least one replica\n                return self.IGNORE, None\n            else:\n                return self.RETHROW, None\n        elif write_type == WriteType.UNLOGGED_BATCH:\n            return self._pick_consistency(received_responses)\n        elif write_type == WriteType.BATCH_LOG:\n            return self.RETRY, consistency\n\n        return self.RETHROW, None\n\n    def on_unavailable(self, query, consistency, required_replicas, alive_replicas, retry_num):\n        if retry_num != 0:\n            return self.RETHROW, None\n        elif ConsistencyLevel.is_serial(consistency):\n            # failed at the paxos phase of a LWT, retry on the next host\n            return self.RETRY_NEXT_HOST, None\n        else:\n            return self._pick_consistency(alive_replicas)\n\n\nclass ExponentialBackoffRetryPolicy(RetryPolicy):\n    \"\"\"\n    A policy that do retries with exponential backoff\n    \"\"\"\n\n    def __init__(self, max_num_retries: float, min_interval: float = 0.1, max_interval: float = 10.0,\n                 *args, **kwargs):\n        \"\"\"\n        `max_num_retries` counts how many times the operation would be retried,\n        `min_interval` is the initial time in seconds to wait before first retry\n        `max_interval` is the maximum time to wait between retries\n        \"\"\"\n        self.min_interval = min_interval\n        self.max_num_retries = max_num_retries\n        self.max_interval = max_interval\n        super(ExponentialBackoffRetryPolicy).__init__(*args, **kwargs)\n\n    def _calculate_backoff(self, attempt: int):\n        delay = min(self.max_interval, self.min_interval * 2 ** attempt)\n        # add some jitter\n        delay += random.random() * self.min_interval - (self.min_interval / 2)\n        return delay\n\n    def on_read_timeout(self, query, consistency, required_responses,\n                        received_responses, data_retrieved, retry_num):\n        if retry_num < self.max_num_retries and received_responses >= required_responses and not data_retrieved:\n            return self.RETRY, consistency, self._calculate_backoff(retry_num)\n        else:\n            return self.RETHROW, None, None\n\n    def on_write_timeout(self, query, consistency, write_type,\n                         required_responses, received_responses, retry_num):\n        if retry_num < self.max_num_retries and write_type == WriteType.BATCH_LOG:\n            return self.RETRY, consistency, self._calculate_backoff(retry_num)\n        else:\n            return self.RETHROW, None, None\n\n    def on_unavailable(self, query, consistency, required_replicas,\n                       alive_replicas, retry_num):\n        if retry_num < self.max_num_retries:\n            return self.RETRY_NEXT_HOST, None, self._calculate_backoff(retry_num)\n        else:\n            return self.RETHROW, None, None\n\n    def on_request_error(self, query, consistency, error, retry_num):\n        if retry_num < self.max_num_retries:\n            return self.RETRY_NEXT_HOST, None, self._calculate_backoff(retry_num)\n        else:\n            return self.RETHROW, None, None\n\n\nclass AddressTranslator(object):\n    \"\"\"\n    Interface for translating cluster-defined endpoints.\n\n    The driver discovers nodes using server metadata and topology change events. Normally,\n    the endpoint defined by the server is the right way to connect to a node. In some environments,\n    these addresses may not be reachable, or not preferred (public vs. private IPs in cloud environments,\n    suboptimal routing, etc). This interface allows for translating from server defined endpoints to\n    preferred addresses for driver connections.\n\n    *Note:* :attr:`~Cluster.contact_points` provided while creating the :class:`~.Cluster` instance are not\n    translated using this mechanism -- only addresses received from Cassandra nodes are.\n    \"\"\"\n    def translate(self, addr):\n        \"\"\"\n        Accepts the node ip address, and returns a translated address to be used connecting to this node.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass IdentityTranslator(AddressTranslator):\n    \"\"\"\n    Returns the endpoint with no translation\n    \"\"\"\n    def translate(self, addr):\n        return addr\n\n\nclass EC2MultiRegionTranslator(AddressTranslator):\n    \"\"\"\n    Resolves private ips of the hosts in the same datacenter as the client, and public ips of hosts in other datacenters.\n    \"\"\"\n    def translate(self, addr):\n        \"\"\"\n        Reverse DNS the public broadcast_address, then lookup that hostname to get the AWS-resolved IP, which\n        will point to the private IP address within the same datacenter.\n        \"\"\"\n        # get family of this address so we translate to the same\n        family = socket.getaddrinfo(addr, 0, socket.AF_UNSPEC, socket.SOCK_STREAM)[0][0]\n        host = socket.getfqdn(addr)\n        for a in socket.getaddrinfo(host, 0, family, socket.SOCK_STREAM):\n            try:\n                return a[4][0]\n            except Exception:\n                pass\n        return addr\n\n\nclass SpeculativeExecutionPolicy(object):\n    \"\"\"\n    Interface for specifying speculative execution plans\n    \"\"\"\n\n    def new_plan(self, keyspace, statement):\n        \"\"\"\n        Returns\n\n        :param keyspace:\n        :param statement:\n        :return:\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass SpeculativeExecutionPlan(object):\n    def next_execution(self, host):\n        raise NotImplementedError()\n\n\nclass NoSpeculativeExecutionPlan(SpeculativeExecutionPlan):\n    def next_execution(self, host):\n        return -1\n\n\nclass NoSpeculativeExecutionPolicy(SpeculativeExecutionPolicy):\n\n    def new_plan(self, keyspace, statement):\n        return NoSpeculativeExecutionPlan()\n\n\nclass ConstantSpeculativeExecutionPolicy(SpeculativeExecutionPolicy):\n    \"\"\"\n    A speculative execution policy that sends a new query every X seconds (**delay**) for a maximum of Y attempts (**max_attempts**).\n    \"\"\"\n\n    def __init__(self, delay, max_attempts):\n        self.delay = delay\n        self.max_attempts = max_attempts\n\n    class ConstantSpeculativeExecutionPlan(SpeculativeExecutionPlan):\n        def __init__(self, delay, max_attempts):\n            self.delay = delay\n            self.remaining = max_attempts\n\n        def next_execution(self, host):\n            if self.remaining > 0:\n                self.remaining -= 1\n                return self.delay\n            else:\n                return -1\n\n    def new_plan(self, keyspace, statement):\n        return self.ConstantSpeculativeExecutionPlan(self.delay, self.max_attempts)\n\n\nclass WrapperPolicy(LoadBalancingPolicy):\n\n    def __init__(self, child_policy):\n        self._child_policy = child_policy\n\n    def distance(self, *args, **kwargs):\n        return self._child_policy.distance(*args, **kwargs)\n\n    def populate(self, cluster, hosts):\n        self._child_policy.populate(cluster, hosts)\n\n    def on_up(self, *args, **kwargs):\n        return self._child_policy.on_up(*args, **kwargs)\n\n    def on_down(self, *args, **kwargs):\n        return self._child_policy.on_down(*args, **kwargs)\n\n    def on_add(self, *args, **kwargs):\n        return self._child_policy.on_add(*args, **kwargs)\n\n    def on_remove(self, *args, **kwargs):\n        return self._child_policy.on_remove(*args, **kwargs)\n\n\nclass DefaultLoadBalancingPolicy(WrapperPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` wrapper that adds the ability to target a specific host first.\n\n    If no host is set on the query, the child policy's query plan will be used as is.\n    \"\"\"\n\n    _cluster_metadata = None\n\n    def populate(self, cluster, hosts):\n        self._cluster_metadata = cluster.metadata\n        self._child_policy.populate(cluster, hosts)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        if query and query.keyspace:\n            keyspace = query.keyspace\n        else:\n            keyspace = working_keyspace\n\n        # TODO remove next major since execute(..., host=XXX) is now available\n        addr = getattr(query, 'target_host', None) if query else None\n        target_host = self._cluster_metadata.get_host(addr)\n\n        child = self._child_policy\n        if target_host and target_host.is_up:\n            yield target_host\n            for h in child.make_query_plan(keyspace, query):\n                if h != target_host:\n                    yield h\n        else:\n            for h in child.make_query_plan(keyspace, query):\n                yield h\n\n\n# TODO for backward compatibility, remove in next major\nclass DSELoadBalancingPolicy(DefaultLoadBalancingPolicy):\n    \"\"\"\n    *Deprecated:* This will be removed in the next major release,\n    consider using :class:`.DefaultLoadBalancingPolicy`.\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(DSELoadBalancingPolicy, self).__init__(*args, **kwargs)\n        warnings.warn(\"DSELoadBalancingPolicy will be removed in 4.0. Consider using \"\n                      \"DefaultLoadBalancingPolicy.\", DeprecationWarning)\n\n\nclass NeverRetryPolicy(RetryPolicy):\n    def _rethrow(self, *args, **kwargs):\n        return self.RETHROW, None\n\n    on_read_timeout = _rethrow\n    on_write_timeout = _rethrow\n    on_unavailable = _rethrow\n\n\nColDesc = namedtuple('ColDesc', ['ks', 'table', 'col'])\n\nclass ColumnEncryptionPolicy(object):\n    \"\"\"\n    A policy enabling (mostly) transparent encryption and decryption of data before it is\n    sent to the cluster.\n\n    Key materials and other configurations are specified on a per-column basis.  This policy can\n    then be used by driver structures which are aware of the underlying columns involved in their\n    work.  In practice this includes the following cases:\n\n    * Prepared statements - data for columns specified by the cluster's policy will be transparently\n      encrypted before they are sent\n    * Rows returned from any query - data for columns specified by the cluster's policy will be\n      transparently decrypted before they are returned to the user\n\n    To enable this functionality, create an instance of this class (or more likely a subclass)\n    before creating a cluster.  This policy should then be configured and supplied to the Cluster\n    at creation time via the :attr:`.Cluster.column_encryption_policy` attribute.\n    \"\"\"\n\n    def encrypt(self, coldesc, obj_bytes):\n        \"\"\"\n        Encrypt the specified bytes using the cryptography materials for the specified column.\n        Largely used internally, although this could also be used to encrypt values supplied\n        to non-prepared statements in a way that is consistent with this policy.\n        \"\"\"\n        raise NotImplementedError()\n\n    def decrypt(self, coldesc, encrypted_bytes):\n        \"\"\"\n        Decrypt the specified (encrypted) bytes using the cryptography materials for the\n        specified column.  Used internally; could be used externally as well but there's\n        not currently an obvious use case.\n        \"\"\"\n        raise NotImplementedError()\n\n    def add_column(self, coldesc, key):\n        \"\"\"\n        Provide cryptography materials to be used when encrypted and/or decrypting data\n        for the specified column.\n        \"\"\"\n        raise NotImplementedError()\n\n    def contains_column(self, coldesc):\n        \"\"\"\n        Predicate to determine if a specific column is supported by this policy.\n        Currently only used internally.\n        \"\"\"\n        raise NotImplementedError()\n\n    def encode_and_encrypt(self, coldesc, obj):\n        \"\"\"\n        Helper function to enable use of this policy on simple (i.e. non-prepared)\n        statements.\n        \"\"\"\n        raise NotImplementedError()\n",
    "cassandra/__init__.py": "# Copyright DataStax, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom enum import Enum\nimport logging\n\n\nclass NullHandler(logging.Handler):\n\n    def emit(self, record):\n        pass\n\nlogging.getLogger('cassandra').addHandler(NullHandler())\n\n__version_info__ = (3, 28, 0)\n__version__ = '.'.join(map(str, __version_info__))\n\n\nclass ConsistencyLevel(object):\n    \"\"\"\n    Spcifies how many replicas must respond for an operation to be considered\n    a success.  By default, ``ONE`` is used for all operations.\n    \"\"\"\n\n    ANY = 0\n    \"\"\"\n    Only requires that one replica receives the write *or* the coordinator\n    stores a hint to replay later. Valid only for writes.\n    \"\"\"\n\n    ONE = 1\n    \"\"\"\n    Only one replica needs to respond to consider the operation a success\n    \"\"\"\n\n    TWO = 2\n    \"\"\"\n    Two replicas must respond to consider the operation a success\n    \"\"\"\n\n    THREE = 3\n    \"\"\"\n    Three replicas must respond to consider the operation a success\n    \"\"\"\n\n    QUORUM = 4\n    \"\"\"\n    ``ceil(RF/2) + 1`` replicas must respond to consider the operation a success\n    \"\"\"\n\n    ALL = 5\n    \"\"\"\n    All replicas must respond to consider the operation a success\n    \"\"\"\n\n    LOCAL_QUORUM = 6\n    \"\"\"\n    Requires a quorum of replicas in the local datacenter\n    \"\"\"\n\n    EACH_QUORUM = 7\n    \"\"\"\n    Requires a quorum of replicas in each datacenter\n    \"\"\"\n\n    SERIAL = 8\n    \"\"\"\n    For conditional inserts/updates that utilize Cassandra's lightweight\n    transactions, this requires consensus among all replicas for the\n    modified data.\n    \"\"\"\n\n    LOCAL_SERIAL = 9\n    \"\"\"\n    Like :attr:`~ConsistencyLevel.SERIAL`, but only requires consensus\n    among replicas in the local datacenter.\n    \"\"\"\n\n    LOCAL_ONE = 10\n    \"\"\"\n    Sends a request only to replicas in the local datacenter and waits for\n    one response.\n    \"\"\"\n\n    @staticmethod\n    def is_serial(cl):\n        return cl == ConsistencyLevel.SERIAL or cl == ConsistencyLevel.LOCAL_SERIAL\n\n\nConsistencyLevel.value_to_name = {\n    ConsistencyLevel.ANY: 'ANY',\n    ConsistencyLevel.ONE: 'ONE',\n    ConsistencyLevel.TWO: 'TWO',\n    ConsistencyLevel.THREE: 'THREE',\n    ConsistencyLevel.QUORUM: 'QUORUM',\n    ConsistencyLevel.ALL: 'ALL',\n    ConsistencyLevel.LOCAL_QUORUM: 'LOCAL_QUORUM',\n    ConsistencyLevel.EACH_QUORUM: 'EACH_QUORUM',\n    ConsistencyLevel.SERIAL: 'SERIAL',\n    ConsistencyLevel.LOCAL_SERIAL: 'LOCAL_SERIAL',\n    ConsistencyLevel.LOCAL_ONE: 'LOCAL_ONE'\n}\n\nConsistencyLevel.name_to_value = {\n    'ANY': ConsistencyLevel.ANY,\n    'ONE': ConsistencyLevel.ONE,\n    'TWO': ConsistencyLevel.TWO,\n    'THREE': ConsistencyLevel.THREE,\n    'QUORUM': ConsistencyLevel.QUORUM,\n    'ALL': ConsistencyLevel.ALL,\n    'LOCAL_QUORUM': ConsistencyLevel.LOCAL_QUORUM,\n    'EACH_QUORUM': ConsistencyLevel.EACH_QUORUM,\n    'SERIAL': ConsistencyLevel.SERIAL,\n    'LOCAL_SERIAL': ConsistencyLevel.LOCAL_SERIAL,\n    'LOCAL_ONE': ConsistencyLevel.LOCAL_ONE\n}\n\n\ndef consistency_value_to_name(value):\n    return ConsistencyLevel.value_to_name[value] if value is not None else \"Not Set\"\n\n\nclass ProtocolVersion(object):\n    \"\"\"\n    Defines native protocol versions supported by this driver.\n    \"\"\"\n    V1 = 1\n    \"\"\"\n    v1, supported in Cassandra 1.2-->2.2\n    \"\"\"\n\n    V2 = 2\n    \"\"\"\n    v2, supported in Cassandra 2.0-->2.2;\n    added support for lightweight transactions, batch operations, and automatic query paging.\n    \"\"\"\n\n    V3 = 3\n    \"\"\"\n    v3, supported in Cassandra 2.1-->3.x+;\n    added support for protocol-level client-side timestamps (see :attr:`.Session.use_client_timestamp`),\n    serial consistency levels for :class:`~.BatchStatement`, and an improved connection pool.\n    \"\"\"\n\n    V4 = 4\n    \"\"\"\n    v4, supported in Cassandra 2.2-->3.x+;\n    added a number of new types, server warnings, new failure messages, and custom payloads. Details in the\n    `project docs <https://github.com/apache/cassandra/blob/trunk/doc/native_protocol_v4.spec>`_\n    \"\"\"\n\n    V5 = 5\n    \"\"\"\n    v5, in beta from 3.x+. Finalised in 4.0-beta5\n    \"\"\"\n\n    V6 = 6\n    \"\"\"\n    v6, in beta from 4.0-beta5\n    \"\"\"\n\n    DSE_V1 = 0x41\n    \"\"\"\n    DSE private protocol v1, supported in DSE 5.1+\n    \"\"\"\n\n    DSE_V2 = 0x42\n    \"\"\"\n    DSE private protocol v2, supported in DSE 6.0+\n    \"\"\"\n\n    SUPPORTED_VERSIONS = (DSE_V2, DSE_V1, V6, V5, V4, V3, V2, V1)\n    \"\"\"\n    A tuple of all supported protocol versions\n    \"\"\"\n\n    BETA_VERSIONS = (V6,)\n    \"\"\"\n    A tuple of all beta protocol versions\n    \"\"\"\n\n    MIN_SUPPORTED = min(SUPPORTED_VERSIONS)\n    \"\"\"\n    Minimum protocol version supported by this driver.\n    \"\"\"\n\n    MAX_SUPPORTED = max(SUPPORTED_VERSIONS)\n    \"\"\"\n    Maximum protocol version supported by this driver.\n    \"\"\"\n\n    @classmethod\n    def get_lower_supported(cls, previous_version):\n        \"\"\"\n        Return the lower supported protocol version. Beta versions are omitted.\n        \"\"\"\n        try:\n            version = next(v for v in sorted(ProtocolVersion.SUPPORTED_VERSIONS, reverse=True) if\n                           v not in ProtocolVersion.BETA_VERSIONS and v < previous_version)\n        except StopIteration:\n            version = 0\n\n        return version\n\n    @classmethod\n    def uses_int_query_flags(cls, version):\n        return version >= cls.V5\n\n    @classmethod\n    def uses_prepare_flags(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def uses_prepared_metadata(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def uses_error_code_map(cls, version):\n        return version >= cls.V5\n\n    @classmethod\n    def uses_keyspace_flag(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def has_continuous_paging_support(cls, version):\n        return version >= cls.DSE_V1\n\n    @classmethod\n    def has_continuous_paging_next_pages(cls, version):\n        return version >= cls.DSE_V2\n\n    @classmethod\n    def has_checksumming_support(cls, version):\n        return cls.V5 <= version < cls.DSE_V1\n\n\nclass WriteType(object):\n    \"\"\"\n    For usage with :class:`.RetryPolicy`, this describe a type\n    of write operation.\n    \"\"\"\n\n    SIMPLE = 0\n    \"\"\"\n    A write to a single partition key. Such writes are guaranteed to be atomic\n    and isolated.\n    \"\"\"\n\n    BATCH = 1\n    \"\"\"\n    A write to multiple partition keys that used the distributed batch log to\n    ensure atomicity.\n    \"\"\"\n\n    UNLOGGED_BATCH = 2\n    \"\"\"\n    A write to multiple partition keys that did not use the distributed batch\n    log. Atomicity for such writes is not guaranteed.\n    \"\"\"\n\n    COUNTER = 3\n    \"\"\"\n    A counter write (for one or multiple partition keys). Such writes should\n    not be replayed in order to avoid overcount.\n    \"\"\"\n\n    BATCH_LOG = 4\n    \"\"\"\n    The initial write to the distributed batch log that Cassandra performs\n    internally before a BATCH write.\n    \"\"\"\n\n    CAS = 5\n    \"\"\"\n    A lighweight-transaction write, such as \"DELETE ... IF EXISTS\".\n    \"\"\"\n\n    VIEW = 6\n    \"\"\"\n    This WriteType is only seen in results for requests that were unable to\n    complete MV operations.\n    \"\"\"\n\n    CDC = 7\n    \"\"\"\n    This WriteType is only seen in results for requests that were unable to\n    complete CDC operations.\n    \"\"\"\n\n\nWriteType.name_to_value = {\n    'SIMPLE': WriteType.SIMPLE,\n    'BATCH': WriteType.BATCH,\n    'UNLOGGED_BATCH': WriteType.UNLOGGED_BATCH,\n    'COUNTER': WriteType.COUNTER,\n    'BATCH_LOG': WriteType.BATCH_LOG,\n    'CAS': WriteType.CAS,\n    'VIEW': WriteType.VIEW,\n    'CDC': WriteType.CDC\n}\n\n\nWriteType.value_to_name = {v: k for k, v in WriteType.name_to_value.items()}\n\n\nclass SchemaChangeType(object):\n    DROPPED = 'DROPPED'\n    CREATED = 'CREATED'\n    UPDATED = 'UPDATED'\n\n\nclass SchemaTargetType(object):\n    KEYSPACE = 'KEYSPACE'\n    TABLE = 'TABLE'\n    TYPE = 'TYPE'\n    FUNCTION = 'FUNCTION'\n    AGGREGATE = 'AGGREGATE'\n\n\nclass SignatureDescriptor(object):\n\n    def __init__(self, name, argument_types):\n        self.name = name\n        self.argument_types = argument_types\n\n    @property\n    def signature(self):\n        \"\"\"\n        function signature string in the form 'name([type0[,type1[...]]])'\n\n        can be used to uniquely identify overloaded function names within a keyspace\n        \"\"\"\n        return self.format_signature(self.name, self.argument_types)\n\n    @staticmethod\n    def format_signature(name, argument_types):\n        return \"%s(%s)\" % (name, ','.join(t for t in argument_types))\n\n    def __repr__(self):\n        return \"%s(%s, %s)\" % (self.__class__.__name__, self.name, self.argument_types)\n\n\nclass UserFunctionDescriptor(SignatureDescriptor):\n    \"\"\"\n    Describes a User function by name and argument signature\n    \"\"\"\n\n    name = None\n    \"\"\"\n    name of the function\n    \"\"\"\n\n    argument_types = None\n    \"\"\"\n    Ordered list of CQL argument type names comprising the type signature\n    \"\"\"\n\n\nclass UserAggregateDescriptor(SignatureDescriptor):\n    \"\"\"\n    Describes a User aggregate function by name and argument signature\n    \"\"\"\n\n    name = None\n    \"\"\"\n    name of the aggregate\n    \"\"\"\n\n    argument_types = None\n    \"\"\"\n    Ordered list of CQL argument type names comprising the type signature\n    \"\"\"\n\n\nclass DriverException(Exception):\n    \"\"\"\n    Base for all exceptions explicitly raised by the driver.\n    \"\"\"\n    pass\n\n\nclass RequestExecutionException(DriverException):\n    \"\"\"\n    Base for request execution exceptions returned from the server.\n    \"\"\"\n    pass\n\n\nclass Unavailable(RequestExecutionException):\n    \"\"\"\n    There were not enough live replicas to satisfy the requested consistency\n    level, so the coordinator node immediately failed the request without\n    forwarding it to any replicas.\n    \"\"\"\n\n    consistency = None\n    \"\"\" The requested :class:`ConsistencyLevel` \"\"\"\n\n    required_replicas = None\n    \"\"\" The number of replicas that needed to be live to complete the operation \"\"\"\n\n    alive_replicas = None\n    \"\"\" The number of replicas that were actually alive \"\"\"\n\n    def __init__(self, summary_message, consistency=None, required_replicas=None, alive_replicas=None):\n        self.consistency = consistency\n        self.required_replicas = required_replicas\n        self.alive_replicas = alive_replicas\n        Exception.__init__(self, summary_message + ' info=' +\n                           repr({'consistency': consistency_value_to_name(consistency),\n                                 'required_replicas': required_replicas,\n                                 'alive_replicas': alive_replicas}))\n\n\nclass Timeout(RequestExecutionException):\n    \"\"\"\n    Replicas failed to respond to the coordinator node before timing out.\n    \"\"\"\n\n    consistency = None\n    \"\"\" The requested :class:`ConsistencyLevel` \"\"\"\n\n    required_responses = None\n    \"\"\" The number of required replica responses \"\"\"\n\n    received_responses = None\n    \"\"\"\n    The number of replicas that responded before the coordinator timed out\n    the operation\n    \"\"\"\n\n    def __init__(self, summary_message, consistency=None, required_responses=None,\n                 received_responses=None, **kwargs):\n        self.consistency = consistency\n        self.required_responses = required_responses\n        self.received_responses = received_responses\n\n        if \"write_type\" in kwargs:\n            kwargs[\"write_type\"] = WriteType.value_to_name[kwargs[\"write_type\"]]\n\n        info = {'consistency': consistency_value_to_name(consistency),\n                'required_responses': required_responses,\n                'received_responses': received_responses}\n        info.update(kwargs)\n\n        Exception.__init__(self, summary_message + ' info=' + repr(info))\n\n\nclass ReadTimeout(Timeout):\n    \"\"\"\n    A subclass of :exc:`Timeout` for read operations.\n\n    This indicates that the replicas failed to respond to the coordinator\n    node before the configured timeout. This timeout is configured in\n    ``cassandra.yaml`` with the ``read_request_timeout_in_ms``\n    and ``range_request_timeout_in_ms`` options.\n    \"\"\"\n\n    data_retrieved = None\n    \"\"\"\n    A boolean indicating whether the requested data was retrieved\n    by the coordinator from any replicas before it timed out the\n    operation\n    \"\"\"\n\n    def __init__(self, message, data_retrieved=None, **kwargs):\n        Timeout.__init__(self, message, **kwargs)\n        self.data_retrieved = data_retrieved\n\n\nclass WriteTimeout(Timeout):\n    \"\"\"\n    A subclass of :exc:`Timeout` for write operations.\n\n    This indicates that the replicas failed to respond to the coordinator\n    node before the configured timeout. This timeout is configured in\n    ``cassandra.yaml`` with the ``write_request_timeout_in_ms``\n    option.\n    \"\"\"\n\n    write_type = None\n    \"\"\"\n    The type of write operation, enum on :class:`~cassandra.policies.WriteType`\n    \"\"\"\n\n    def __init__(self, message, write_type=None, **kwargs):\n        kwargs[\"write_type\"] = write_type\n        Timeout.__init__(self, message, **kwargs)\n        self.write_type = write_type\n\n\nclass CDCWriteFailure(RequestExecutionException):\n    \"\"\"\n    Hit limit on data in CDC folder, writes are rejected\n    \"\"\"\n    def __init__(self, message):\n        Exception.__init__(self, message)\n\n\nclass CoordinationFailure(RequestExecutionException):\n    \"\"\"\n    Replicas sent a failure to the coordinator.\n    \"\"\"\n\n    consistency = None\n    \"\"\" The requested :class:`ConsistencyLevel` \"\"\"\n\n    required_responses = None\n    \"\"\" The number of required replica responses \"\"\"\n\n    received_responses = None\n    \"\"\"\n    The number of replicas that responded before the coordinator timed out\n    the operation\n    \"\"\"\n\n    failures = None\n    \"\"\"\n    The number of replicas that sent a failure message\n    \"\"\"\n\n    error_code_map = None\n    \"\"\"\n    A map of inet addresses to error codes representing replicas that sent\n    a failure message.  Only set when `protocol_version` is 5 or higher.\n    \"\"\"\n\n    def __init__(self, summary_message, consistency=None, required_responses=None,\n                 received_responses=None, failures=None, error_code_map=None):\n        self.consistency = consistency\n        self.required_responses = required_responses\n        self.received_responses = received_responses\n        self.failures = failures\n        self.error_code_map = error_code_map\n\n        info_dict = {\n            'consistency': consistency_value_to_name(consistency),\n            'required_responses': required_responses,\n            'received_responses': received_responses,\n            'failures': failures\n        }\n\n        if error_code_map is not None:\n            # make error codes look like \"0x002a\"\n            formatted_map = dict((addr, '0x%04x' % err_code)\n                                 for (addr, err_code) in error_code_map.items())\n            info_dict['error_code_map'] = formatted_map\n\n        Exception.__init__(self, summary_message + ' info=' + repr(info_dict))\n\n\nclass ReadFailure(CoordinationFailure):\n    \"\"\"\n    A subclass of :exc:`CoordinationFailure` for read operations.\n\n    This indicates that the replicas sent a failure message to the coordinator.\n    \"\"\"\n\n    data_retrieved = None\n    \"\"\"\n    A boolean indicating whether the requested data was retrieved\n    by the coordinator from any replicas before it timed out the\n    operation\n    \"\"\"\n\n    def __init__(self, message, data_retrieved=None, **kwargs):\n        CoordinationFailure.__init__(self, message, **kwargs)\n        self.data_retrieved = data_retrieved\n\n\nclass WriteFailure(CoordinationFailure):\n    \"\"\"\n    A subclass of :exc:`CoordinationFailure` for write operations.\n\n    This indicates that the replicas sent a failure message to the coordinator.\n    \"\"\"\n\n    write_type = None\n    \"\"\"\n    The type of write operation, enum on :class:`~cassandra.policies.WriteType`\n    \"\"\"\n\n    def __init__(self, message, write_type=None, **kwargs):\n        CoordinationFailure.__init__(self, message, **kwargs)\n        self.write_type = write_type\n\n\nclass FunctionFailure(RequestExecutionException):\n    \"\"\"\n    User Defined Function failed during execution\n    \"\"\"\n\n    keyspace = None\n    \"\"\"\n    Keyspace of the function\n    \"\"\"\n\n    function = None\n    \"\"\"\n    Name of the function\n    \"\"\"\n\n    arg_types = None\n    \"\"\"\n    List of argument type names of the function\n    \"\"\"\n\n    def __init__(self, summary_message, keyspace, function, arg_types):\n        self.keyspace = keyspace\n        self.function = function\n        self.arg_types = arg_types\n        Exception.__init__(self, summary_message)\n\n\nclass RequestValidationException(DriverException):\n    \"\"\"\n    Server request validation failed\n    \"\"\"\n    pass\n\n\nclass ConfigurationException(RequestValidationException):\n    \"\"\"\n    Server indicated request errro due to current configuration\n    \"\"\"\n    pass\n\n\nclass AlreadyExists(ConfigurationException):\n    \"\"\"\n    An attempt was made to create a keyspace or table that already exists.\n    \"\"\"\n\n    keyspace = None\n    \"\"\"\n    The name of the keyspace that already exists, or, if an attempt was\n    made to create a new table, the keyspace that the table is in.\n    \"\"\"\n\n    table = None\n    \"\"\"\n    The name of the table that already exists, or, if an attempt was\n    make to create a keyspace, :const:`None`.\n    \"\"\"\n\n    def __init__(self, keyspace=None, table=None):\n        if table:\n            message = \"Table '%s.%s' already exists\" % (keyspace, table)\n        else:\n            message = \"Keyspace '%s' already exists\" % (keyspace,)\n\n        Exception.__init__(self, message)\n        self.keyspace = keyspace\n        self.table = table\n\n\nclass InvalidRequest(RequestValidationException):\n    \"\"\"\n    A query was made that was invalid for some reason, such as trying to set\n    the keyspace for a connection to a nonexistent keyspace.\n    \"\"\"\n    pass\n\n\nclass Unauthorized(RequestValidationException):\n    \"\"\"\n    The current user is not authorized to perform the requested operation.\n    \"\"\"\n    pass\n\n\nclass AuthenticationFailed(DriverException):\n    \"\"\"\n    Failed to authenticate.\n    \"\"\"\n    pass\n\n\nclass OperationTimedOut(DriverException):\n    \"\"\"\n    The operation took longer than the specified (client-side) timeout\n    to complete.  This is not an error generated by Cassandra, only\n    the driver.\n    \"\"\"\n\n    errors = None\n    \"\"\"\n    A dict of errors keyed by the :class:`~.Host` against which they occurred.\n    \"\"\"\n\n    last_host = None\n    \"\"\"\n    The last :class:`~.Host` this operation was attempted against.\n    \"\"\"\n\n    def __init__(self, errors=None, last_host=None):\n        self.errors = errors\n        self.last_host = last_host\n        message = \"errors=%s, last_host=%s\" % (self.errors, self.last_host)\n        Exception.__init__(self, message)\n\n\nclass UnsupportedOperation(DriverException):\n    \"\"\"\n    An attempt was made to use a feature that is not supported by the\n    selected protocol version.  See :attr:`Cluster.protocol_version`\n    for more details.\n    \"\"\"\n    pass\n\n\nclass UnresolvableContactPoints(DriverException):\n    \"\"\"\n    The driver was unable to resolve any provided hostnames.\n\n    Note that this is *not* raised when a :class:`.Cluster` is created with no\n    contact points, only when lookup fails for all hosts\n    \"\"\"\n    pass\n\n\nclass OperationType(Enum):\n    Read = 0\n    Write = 1\n\nclass RateLimitReached(ConfigurationException):\n    '''\n    Rate limit was exceeded for a partition affected by the request.\n    '''\n    op_type = None\n    rejected_by_coordinator = False\n\n    def __init__(self, op_type=None, rejected_by_coordinator=False):\n        self.op_type = op_type\n        self.rejected_by_coordinator = rejected_by_coordinator\n        message = f\"[request_error_rate_limit_reached OpType={op_type.name} RejectedByCoordinator={rejected_by_coordinator}]\"\n        Exception.__init__(self, message)\n"
  },
  "GT_src_dict": {
    "cassandra/util.py": {
      "datetime_from_timestamp": {
        "code": "def datetime_from_timestamp(timestamp):\n    \"\"\"Creates a timezone-agnostic `datetime` object from a given Unix timestamp in seconds. This function addresses specific issues related to large negative timestamps on Windows and rounding differences in older Python versions.\n\n:param timestamp: A Unix timestamp represented in seconds, which is the number of seconds since the Unix epoch (January 1, 1970).\n:return: A `datetime.datetime` object representing the corresponding date and time.\n\nThe function utilizes the constant `DATETIME_EPOC`, which is defined as `datetime.datetime(1970, 1, 1)`, to calculate the result by adding a `datetime.timedelta` to it. This ensures compatibility across different systems when handling Unix timestamps.\"\"\"\n    '\\n    Creates a timezone-agnostic datetime from timestamp (in seconds) in a consistent manner.\\n    Works around a Windows issue with large negative timestamps (PYTHON-119),\\n    and rounding differences in Python 3.4 (PYTHON-340).\\n\\n    :param timestamp: a unix timestamp, in seconds\\n    '\n    dt = DATETIME_EPOC + datetime.timedelta(seconds=timestamp)\n    return dt",
        "docstring": "Creates a timezone-agnostic `datetime` object from a given Unix timestamp in seconds. This function addresses specific issues related to large negative timestamps on Windows and rounding differences in older Python versions.\n\n:param timestamp: A Unix timestamp represented in seconds, which is the number of seconds since the Unix epoch (January 1, 1970).\n:return: A `datetime.datetime` object representing the corresponding date and time.\n\nThe function utilizes the constant `DATETIME_EPOC`, which is defined as `datetime.datetime(1970, 1, 1)`, to calculate the result by adding a `datetime.timedelta` to it. This ensures compatibility across different systems when handling Unix timestamps.",
        "signature": "def datetime_from_timestamp(timestamp):",
        "type": "Function",
        "class_signature": null
      },
      "unix_time_from_uuid1": {
        "code": "def unix_time_from_uuid1(uuid_arg):\n    \"\"\"Converts a version 1 `uuid.UUID` to a Unix timestamp with the same precision as the output of `time.time()`. This transformation is essential for interpreting the timestamps embedded within version 1 UUIDs, which are based on time.\n\nParameters:\n- uuid_arg (uuid.UUID): A version 1 UUID from which the timestamp will be extracted.\n\nReturns:\n- float: The Unix timestamp in seconds, representing the time stored in the provided UUID.\n\nThis function utilizes the constant `0x01B21DD213814000`, which represents the number of 100-nanosecond intervals between the UUID epoch (October 15, 1582) and the Unix epoch (January 1, 1970). The calculation converts the UUID time to seconds, making it compatible with typical Unix timestamps.\"\"\"\n    '\\n    Converts a version 1 :class:`uuid.UUID` to a timestamp with the same precision\\n    as :meth:`time.time()` returns.  This is useful for examining the\\n    results of queries returning a v1 :class:`~uuid.UUID`.\\n\\n    :param uuid_arg: a version 1 :class:`~uuid.UUID`\\n    '\n    return (uuid_arg.time - 122192928000000000) / 10000000.0",
        "docstring": "Converts a version 1 `uuid.UUID` to a Unix timestamp with the same precision as the output of `time.time()`. This transformation is essential for interpreting the timestamps embedded within version 1 UUIDs, which are based on time.\n\nParameters:\n- uuid_arg (uuid.UUID): A version 1 UUID from which the timestamp will be extracted.\n\nReturns:\n- float: The Unix timestamp in seconds, representing the time stored in the provided UUID.\n\nThis function utilizes the constant `0x01B21DD213814000`, which represents the number of 100-nanosecond intervals between the UUID epoch (October 15, 1582) and the Unix epoch (January 1, 1970). The calculation converts the UUID time to seconds, making it compatible with typical Unix timestamps.",
        "signature": "def unix_time_from_uuid1(uuid_arg):",
        "type": "Function",
        "class_signature": null
      },
      "datetime_from_uuid1": {
        "code": "def datetime_from_uuid1(uuid_arg):\n    \"\"\"Creates a timezone-agnostic datetime from the timestamp embedded in a version 1 UUID.\n\n:param uuid_arg: A version 1 :class:`~uuid.UUID` object. This UUID type contains a timestamp that can be extracted to generate a corresponding datetime.\n:return: Returns a :class:`datetime.datetime` object representing the time associated with the provided UUID. The datetime is timezone-agnostic and represents the moment in time when the UUID was generated.\n\nThis function depends on the `unix_time_from_uuid1()` internal helper, which calculates a UNIX timestamp from the given UUID. It also utilizes the `datetime_from_timestamp()` function to convert the UNIX timestamp into a datetime object. Both helper functions are defined above and provide the necessary conversions for handling UUID timestamps.\"\"\"\n    '\\n    Creates a timezone-agnostic datetime from the timestamp in the\\n    specified type-1 UUID.\\n\\n    :param uuid_arg: a version 1 :class:`~uuid.UUID`\\n    '\n    return datetime_from_timestamp(unix_time_from_uuid1(uuid_arg))",
        "docstring": "Creates a timezone-agnostic datetime from the timestamp embedded in a version 1 UUID.\n\n:param uuid_arg: A version 1 :class:`~uuid.UUID` object. This UUID type contains a timestamp that can be extracted to generate a corresponding datetime.\n:return: Returns a :class:`datetime.datetime` object representing the time associated with the provided UUID. The datetime is timezone-agnostic and represents the moment in time when the UUID was generated.\n\nThis function depends on the `unix_time_from_uuid1()` internal helper, which calculates a UNIX timestamp from the given UUID. It also utilizes the `datetime_from_timestamp()` function to convert the UNIX timestamp into a datetime object. Both helper functions are defined above and provide the necessary conversions for handling UUID timestamps.",
        "signature": "def datetime_from_uuid1(uuid_arg):",
        "type": "Function",
        "class_signature": null
      },
      "min_uuid_from_time": {
        "code": "def min_uuid_from_time(timestamp):\n    \"\"\"Generates the minimum TimeUUID (type 1) for a given timestamp based on Cassandra's byte-wise comparison rules.\n\n:param timestamp: A timestamp in seconds, which can be either a datetime object or a Unix timestamp.\n:return: A type 1 :class:`uuid.UUID` representing the minimum TimeUUID for the specified timestamp.\n\nThis function utilizes `uuid_from_time`, passing the timestamp along with predefined constants to generate the UUID. The constant `0x808080808080` is used to fill in the node portion of the UUID with minimum signed bytes (0x80 = -128), ensuring that the generated UUID is the lowest possible for that specific timestamp.\"\"\"\n    '\\n    Generates the minimum TimeUUID (type 1) for a given timestamp, as compared by Cassandra.\\n\\n    See :func:`uuid_from_time` for argument and return types.\\n    '\n    return uuid_from_time(timestamp, 141289400074368, 128)",
        "docstring": "Generates the minimum TimeUUID (type 1) for a given timestamp based on Cassandra's byte-wise comparison rules.\n\n:param timestamp: A timestamp in seconds, which can be either a datetime object or a Unix timestamp.\n:return: A type 1 :class:`uuid.UUID` representing the minimum TimeUUID for the specified timestamp.\n\nThis function utilizes `uuid_from_time`, passing the timestamp along with predefined constants to generate the UUID. The constant `0x808080808080` is used to fill in the node portion of the UUID with minimum signed bytes (0x80 = -128), ensuring that the generated UUID is the lowest possible for that specific timestamp.",
        "signature": "def min_uuid_from_time(timestamp):",
        "type": "Function",
        "class_signature": null
      },
      "max_uuid_from_time": {
        "code": "def max_uuid_from_time(timestamp):\n    \"\"\"Generates the maximum TimeUUID (type 1) for a specified timestamp, suitable for use with Cassandra. \n\nThis function leverages the `uuid_from_time` method to create a TimeUUID based on the provided timestamp. The maximum TimeUUID is defined by using specific maximum signed byte values: `0x7f7f7f7f7f7f` for the node identifier (which represents the maximum value for 48 bits) and `0x3f7f` for the clock sequence (the maximum value for 14 bits). \n\nParameters:\n- timestamp (float): A Unix timestamp in seconds that represents the time for which to generate the maximum TimeUUID.\n\nReturns:\n- uuid.UUID: The maximum TimeUUID corresponding to the specified timestamp.\n\nConstants utilized:\n- `uuid_from_time`: A function defined earlier in the code that constructs type 1 UUIDs from timestamps, nodes, and clock sequences.\"\"\"\n    '\\n    Generates the maximum TimeUUID (type 1) for a given timestamp, as compared by Cassandra.\\n\\n    See :func:`uuid_from_time` for argument and return types.\\n    '\n    return uuid_from_time(timestamp, 140185576636287, 16255)",
        "docstring": "Generates the maximum TimeUUID (type 1) for a specified timestamp, suitable for use with Cassandra. \n\nThis function leverages the `uuid_from_time` method to create a TimeUUID based on the provided timestamp. The maximum TimeUUID is defined by using specific maximum signed byte values: `0x7f7f7f7f7f7f` for the node identifier (which represents the maximum value for 48 bits) and `0x3f7f` for the clock sequence (the maximum value for 14 bits). \n\nParameters:\n- timestamp (float): A Unix timestamp in seconds that represents the time for which to generate the maximum TimeUUID.\n\nReturns:\n- uuid.UUID: The maximum TimeUUID corresponding to the specified timestamp.\n\nConstants utilized:\n- `uuid_from_time`: A function defined earlier in the code that constructs type 1 UUIDs from timestamps, nodes, and clock sequences.",
        "signature": "def max_uuid_from_time(timestamp):",
        "type": "Function",
        "class_signature": null
      },
      "uuid_from_time": {
        "code": "def uuid_from_time(time_arg, node=None, clock_seq=None):\n    \"\"\"Converts a datetime or timestamp into a Type 1 UUID, which is based on time. This UUID version incorporates a timestamp to ensure uniqueness across different time intervals.\n\nParameters:\n- time_arg: Either a `datetime` object or a timestamp in seconds (as returned by `time.time()`) to be used for generating the UUID's timestamp component.\n- node: An optional integer for the UUID node (up to 48 bits). If not provided, a random node value is generated.\n- clock_seq: An optional integer for the clock sequence field (up to 14 bits). If not provided, a random sequence is generated. Raises a ValueError if it exceeds the valid range.\n\nReturns:\n- A `uuid.UUID` object representing the generated Type 1 UUID.\n\nNotes:\n- The function uses the constant `0x01b21dd213814000`, which is the number of 100-nanosecond intervals between the UUID epoch (October 15, 1582) and the Unix epoch (January 1, 1970), to compute the appropriate timestamp for the UUID.\n- If the timestamp is derived from a `datetime` object, it is converted to seconds and microseconds before being transformed into the UUID format. If it's a timestamp in seconds, it is directly converted.\"\"\"\n    '\\n    Converts a datetime or timestamp to a type 1 :class:`uuid.UUID`.\\n\\n    :param time_arg:\\n      The time to use for the timestamp portion of the UUID.\\n      This can either be a :class:`datetime` object or a timestamp\\n      in seconds (as returned from :meth:`time.time()`).\\n    :type datetime: :class:`datetime` or timestamp\\n\\n    :param node:\\n      None integer for the UUID (up to 48 bits). If not specified, this\\n      field is randomized.\\n    :type node: long\\n\\n    :param clock_seq:\\n      Clock sequence field for the UUID (up to 14 bits). If not specified,\\n      a random sequence is generated.\\n    :type clock_seq: int\\n\\n    :rtype: :class:`uuid.UUID`\\n\\n    '\n    if hasattr(time_arg, 'utctimetuple'):\n        seconds = int(calendar.timegm(time_arg.utctimetuple()))\n        microseconds = seconds * 1000000.0 + time_arg.time().microsecond\n    else:\n        microseconds = int(time_arg * 1000000.0)\n    intervals = int(microseconds * 10) + 122192928000000000\n    time_low = intervals & 4294967295\n    time_mid = intervals >> 32 & 65535\n    time_hi_version = intervals >> 48 & 4095\n    if clock_seq is None:\n        clock_seq = random.getrandbits(14)\n    elif clock_seq > 16383:\n        raise ValueError('clock_seq is out of range (need a 14-bit value)')\n    clock_seq_low = clock_seq & 255\n    clock_seq_hi_variant = 128 | clock_seq >> 8 & 63\n    if node is None:\n        node = random.getrandbits(48)\n    return uuid.UUID(fields=(time_low, time_mid, time_hi_version, clock_seq_hi_variant, clock_seq_low, node), version=1)",
        "docstring": "Converts a datetime or timestamp into a Type 1 UUID, which is based on time. This UUID version incorporates a timestamp to ensure uniqueness across different time intervals.\n\nParameters:\n- time_arg: Either a `datetime` object or a timestamp in seconds (as returned by `time.time()`) to be used for generating the UUID's timestamp component.\n- node: An optional integer for the UUID node (up to 48 bits). If not provided, a random node value is generated.\n- clock_seq: An optional integer for the clock sequence field (up to 14 bits). If not provided, a random sequence is generated. Raises a ValueError if it exceeds the valid range.\n\nReturns:\n- A `uuid.UUID` object representing the generated Type 1 UUID.\n\nNotes:\n- The function uses the constant `0x01b21dd213814000`, which is the number of 100-nanosecond intervals between the UUID epoch (October 15, 1582) and the Unix epoch (January 1, 1970), to compute the appropriate timestamp for the UUID.\n- If the timestamp is derived from a `datetime` object, it is converted to seconds and microseconds before being transformed into the UUID format. If it's a timestamp in seconds, it is directly converted.",
        "signature": "def uuid_from_time(time_arg, node=None, clock_seq=None):",
        "type": "Function",
        "class_signature": null
      }
    },
    "cassandra/policies.py": {},
    "cassandra/__init__.py": {}
  },
  "dependency_dict": {
    "cassandra/util.py:max_uuid_from_time": {},
    "cassandra/util.py:min_uuid_from_time": {},
    "cassandra/util.py:datetime_from_uuid1": {}
  },
  "call_tree": {
    "tests/unit/test_time_util.py:TimeUtilTest:test_datetime_from_timestamp": {
      "cassandra/util.py:datetime_from_timestamp": {}
    },
    "tests/unit/test_time_util.py:TimeUtilTest:test_max_uuid": {
      "cassandra/util.py:max_uuid_from_time": {
        "cassandra/util.py:uuid_from_time": {}
      }
    },
    "tests/unit/test_time_util.py:TimeUtilTest:test_min_uuid": {
      "cassandra/util.py:min_uuid_from_time": {
        "cassandra/util.py:uuid_from_time": {}
      }
    },
    "tests/unit/test_time_util.py:TimeUtilTest:test_times_from_uuid1": {
      "cassandra/util.py:unix_time_from_uuid1": {},
      "cassandra/util.py:datetime_from_uuid1": {
        "cassandra/util.py:unix_time_from_uuid1": {},
        "cassandra/util.py:datetime_from_timestamp": {}
      }
    },
    "tests/unit/test_time_util.py:TimeUtilTest:test_uuid_from_time": {
      "cassandra/util.py:uuid_from_time": {},
      "cassandra/util.py:unix_time_from_uuid1": {},
      "cassandra/util.py:datetime_from_timestamp": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/scylla_driver-image-test_time_util/scylla_driver-test_time_util/tests/unit/test_policies.py:TestRackOrDCAwareRoundRobinPolicy:test_with_remotes": {
      "cassandra/policies.py:DCAwareRoundRobinPolicy:DCAwareRoundRobinPolicy": {},
      "cassandra/policies.py:RackAwareRoundRobinPolicy:RackAwareRoundRobinPolicy": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/scylla_driver-image-test_time_util/scylla_driver-test_time_util/tests/unit/test_policies.py:TestRackOrDCAwareRoundRobinPolicy:test_get_distance": {
      "cassandra/policies.py:DCAwareRoundRobinPolicy:DCAwareRoundRobinPolicy": {},
      "cassandra/policies.py:RackAwareRoundRobinPolicy:RackAwareRoundRobinPolicy": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/scylla_driver-image-test_time_util/scylla_driver-test_time_util/tests/integration/long/test_loadbalancingpolicies.py:LoadBalancingPolicyTests:test_token_aware_is_used_by_default": {
      "cassandra/policies.py:TokenAwarePolicy:TokenAwarePolicy": {},
      "cassandra/policies.py:DCAwareRoundRobinPolicy:DCAwareRoundRobinPolicy": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/scylla_driver-image-test_time_util/scylla_driver-test_time_util/tests/integration/advanced/graph/test_graph.py:GraphTimeoutTests:test_server_timeout_less_then_request": {
      "cassandra/__init__.py:InvalidRequest:InvalidRequest": {},
      "cassandra/__init__.py:OperationTimedOut:OperationTimedOut": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/scylla_driver-image-test_time_util/scylla_driver-test_time_util/tests/integration/advanced/graph/test_graph.py:GraphProfileTests:test_graph_profile": {
      "cassandra/__init__.py:InvalidRequest:InvalidRequest": {},
      "cassandra/__init__.py:OperationTimedOut:OperationTimedOut": {}
    }
  },
  "PRD": "# PROJECT NAME: scylla_driver-test_time_util\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 cassandra/\n    \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 InvalidRequest.InvalidRequest\n    \u2502   \u2514\u2500\u2500 OperationTimedOut.OperationTimedOut\n    \u251c\u2500\u2500 policies.py\n    \u2502   \u251c\u2500\u2500 DCAwareRoundRobinPolicy.DCAwareRoundRobinPolicy\n    \u2502   \u251c\u2500\u2500 RackAwareRoundRobinPolicy.RackAwareRoundRobinPolicy\n    \u2502   \u2514\u2500\u2500 TokenAwarePolicy.TokenAwarePolicy\n    \u2514\u2500\u2500 util.py\n        \u251c\u2500\u2500 datetime_from_timestamp\n        \u251c\u2500\u2500 datetime_from_uuid1\n        \u251c\u2500\u2500 max_uuid_from_time\n        \u251c\u2500\u2500 min_uuid_from_time\n        \u251c\u2500\u2500 unix_time_from_uuid1\n        \u2514\u2500\u2500 uuid_from_time\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThe module is designed to provide functionality for handling time and UUID-related operations, specifically tailored for systems interacting with Cassandra or similar databases. It facilitates conversion between timestamps, UUIDs, and datetime objects, enabling precise and consistent time representation across various formats. Additionally, the module supports the generation and manipulation of version-1 UUIDs derived from time and system node information, including options to create minimum and maximum UUIDs for specific timestamps. By streamlining time-to-UUID conversions and related utilities, this module simplifies the management of temporal data for developers, ensuring interoperability and precision in time-sensitive applications.\n\n## FILE 1: cassandra/util.py\n\n- FUNCTION NAME: datetime_from_uuid1\n  - SIGNATURE: def datetime_from_uuid1(uuid_arg):\n  - DOCSTRING: \n```python\n\"\"\"\nCreates a timezone-agnostic datetime from the timestamp embedded in a version 1 UUID.\n\n:param uuid_arg: A version 1 :class:`~uuid.UUID` object. This UUID type contains a timestamp that can be extracted to generate a corresponding datetime.\n:return: Returns a :class:`datetime.datetime` object representing the time associated with the provided UUID. The datetime is timezone-agnostic and represents the moment in time when the UUID was generated.\n\nThis function depends on the `unix_time_from_uuid1()` internal helper, which calculates a UNIX timestamp from the given UUID. It also utilizes the `datetime_from_timestamp()` function to convert the UNIX timestamp into a datetime object. Both helper functions are defined above and provide the necessary conversions for handling UUID timestamps.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - cassandra/util.py:datetime_from_timestamp\n    - cassandra/util.py:unix_time_from_uuid1\n\n- FUNCTION NAME: datetime_from_timestamp\n  - SIGNATURE: def datetime_from_timestamp(timestamp):\n  - DOCSTRING: \n```python\n\"\"\"\nCreates a timezone-agnostic `datetime` object from a given Unix timestamp in seconds. This function addresses specific issues related to large negative timestamps on Windows and rounding differences in older Python versions.\n\n:param timestamp: A Unix timestamp represented in seconds, which is the number of seconds since the Unix epoch (January 1, 1970).\n:return: A `datetime.datetime` object representing the corresponding date and time.\n\nThe function utilizes the constant `DATETIME_EPOC`, which is defined as `datetime.datetime(1970, 1, 1)`, to calculate the result by adding a `datetime.timedelta` to it. This ensures compatibility across different systems when handling Unix timestamps.\n\"\"\"\n```\n\n- FUNCTION NAME: uuid_from_time\n  - SIGNATURE: def uuid_from_time(time_arg, node=None, clock_seq=None):\n  - DOCSTRING: \n```python\n\"\"\"\nConverts a datetime or timestamp into a Type 1 UUID, which is based on time. This UUID version incorporates a timestamp to ensure uniqueness across different time intervals.\n\nParameters:\n- time_arg: Either a `datetime` object or a timestamp in seconds (as returned by `time.time()`) to be used for generating the UUID's timestamp component.\n- node: An optional integer for the UUID node (up to 48 bits). If not provided, a random node value is generated.\n- clock_seq: An optional integer for the clock sequence field (up to 14 bits). If not provided, a random sequence is generated. Raises a ValueError if it exceeds the valid range.\n\nReturns:\n- A `uuid.UUID` object representing the generated Type 1 UUID.\n\nNotes:\n- The function uses the constant `0x01b21dd213814000`, which is the number of 100-nanosecond intervals between the UUID epoch (October 15, 1582) and the Unix epoch (January 1, 1970), to compute the appropriate timestamp for the UUID.\n- If the timestamp is derived from a `datetime` object, it is converted to seconds and microseconds before being transformed into the UUID format. If it's a timestamp in seconds, it is directly converted.\n\"\"\"\n```\n\n- FUNCTION NAME: unix_time_from_uuid1\n  - SIGNATURE: def unix_time_from_uuid1(uuid_arg):\n  - DOCSTRING: \n```python\n\"\"\"\nConverts a version 1 `uuid.UUID` to a Unix timestamp with the same precision as the output of `time.time()`. This transformation is essential for interpreting the timestamps embedded within version 1 UUIDs, which are based on time.\n\nParameters:\n- uuid_arg (uuid.UUID): A version 1 UUID from which the timestamp will be extracted.\n\nReturns:\n- float: The Unix timestamp in seconds, representing the time stored in the provided UUID.\n\nThis function utilizes the constant `0x01B21DD213814000`, which represents the number of 100-nanosecond intervals between the UUID epoch (October 15, 1582) and the Unix epoch (January 1, 1970). The calculation converts the UUID time to seconds, making it compatible with typical Unix timestamps.\n\"\"\"\n```\n\n- FUNCTION NAME: max_uuid_from_time\n  - SIGNATURE: def max_uuid_from_time(timestamp):\n  - DOCSTRING: \n```python\n\"\"\"\nGenerates the maximum TimeUUID (type 1) for a specified timestamp, suitable for use with Cassandra. \n\nThis function leverages the `uuid_from_time` method to create a TimeUUID based on the provided timestamp. The maximum TimeUUID is defined by using specific maximum signed byte values: `0x7f7f7f7f7f7f` for the node identifier (which represents the maximum value for 48 bits) and `0x3f7f` for the clock sequence (the maximum value for 14 bits). \n\nParameters:\n- timestamp (float): A Unix timestamp in seconds that represents the time for which to generate the maximum TimeUUID.\n\nReturns:\n- uuid.UUID: The maximum TimeUUID corresponding to the specified timestamp.\n\nConstants utilized:\n- `uuid_from_time`: A function defined earlier in the code that constructs type 1 UUIDs from timestamps, nodes, and clock sequences.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - cassandra/util.py:uuid_from_time\n\n- FUNCTION NAME: min_uuid_from_time\n  - SIGNATURE: def min_uuid_from_time(timestamp):\n  - DOCSTRING: \n```python\n\"\"\"\nGenerates the minimum TimeUUID (type 1) for a given timestamp based on Cassandra's byte-wise comparison rules.\n\n:param timestamp: A timestamp in seconds, which can be either a datetime object or a Unix timestamp.\n:return: A type 1 :class:`uuid.UUID` representing the minimum TimeUUID for the specified timestamp.\n\nThis function utilizes `uuid_from_time`, passing the timestamp along with predefined constants to generate the UUID. The constant `0x808080808080` is used to fill in the node portion of the UUID with minimum signed bytes (0x80 = -128), ensuring that the generated UUID is the lowest possible for that specific timestamp.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - cassandra/util.py:uuid_from_time\n\n## FILE 2: cassandra/policies.py\n\n## FILE 3: cassandra/__init__.py\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "cassandra/util.py": "from __future__ import with_statement\nfrom _weakref import ref\nimport calendar\nfrom collections import OrderedDict\nfrom collections.abc import Mapping\nimport datetime\nfrom functools import total_ordering\nfrom itertools import chain\nimport keyword\nimport logging\nimport pickle\nimport random\nimport re\nimport socket\nimport sys\nimport time\nimport uuid\nfrom typing import Optional\n_HAS_GEOMET = True\ntry:\n    from geomet import wkt\nexcept:\n    _HAS_GEOMET = False\nfrom cassandra import DriverException\nDATETIME_EPOC = datetime.datetime(1970, 1, 1)\nUTC_DATETIME_EPOC = datetime.datetime.utcfromtimestamp(0)\n_nan = float('nan')\nlog = logging.getLogger(__name__)\nassert sys.byteorder in ('little', 'big')\nis_little_endian = sys.byteorder == 'little'\n\ndef utc_datetime_from_ms_timestamp(timestamp):\n    \"\"\"\n    Creates a UTC datetime from a timestamp in milliseconds. See\n    :meth:`datetime_from_timestamp`.\n\n    Raises an `OverflowError` if the timestamp is out of range for\n    :class:`~datetime.datetime`.\n\n    :param timestamp: timestamp, in milliseconds\n    \"\"\"\n    return UTC_DATETIME_EPOC + datetime.timedelta(milliseconds=timestamp)\n\ndef ms_timestamp_from_datetime(dt):\n    \"\"\"\n    Converts a datetime to a timestamp expressed in milliseconds.\n\n    :param dt: a :class:`datetime.datetime`\n    \"\"\"\n    return int(round((dt - UTC_DATETIME_EPOC).total_seconds() * 1000))\nLOWEST_TIME_UUID = uuid.UUID('00000000-0000-1000-8080-808080808080')\n' The lowest possible TimeUUID, as sorted by Cassandra. '\nHIGHEST_TIME_UUID = uuid.UUID('ffffffff-ffff-1fff-bf7f-7f7f7f7f7f7f')\n' The highest possible TimeUUID, as sorted by Cassandra. '\n\ndef _addrinfo_or_none(contact_point, port):\n    \"\"\"\n    A helper function that wraps socket.getaddrinfo and returns None\n    when it fails to, e.g. resolve one of the hostnames. Used to address\n    PYTHON-895.\n    \"\"\"\n    try:\n        value = socket.getaddrinfo(contact_point, port, socket.AF_UNSPEC, socket.SOCK_STREAM)\n        return value\n    except socket.gaierror:\n        log.debug('Could not resolve hostname \"{}\" with port {}'.format(contact_point, port))\n        return None\n\ndef _addrinfo_to_ip_strings(addrinfo):\n    \"\"\"\n    Helper function that consumes the data output by socket.getaddrinfo and\n    extracts the IP address from the sockaddr portion of the result.\n\n    Since this is meant to be used in conjunction with _addrinfo_or_none,\n    this will pass None and EndPoint instances through unaffected.\n    \"\"\"\n    if addrinfo is None:\n        return None\n    return [(entry[4][0], entry[4][1]) for entry in addrinfo]\n\ndef _resolve_contact_points_to_string_map(contact_points):\n    return OrderedDict((('{cp}:{port}'.format(cp=cp, port=port), _addrinfo_to_ip_strings(_addrinfo_or_none(cp, port))) for cp, port in contact_points))\n\nclass _IterationGuard(object):\n\n    def __init__(self, weakcontainer):\n        self.weakcontainer = ref(weakcontainer)\n\n    def __enter__(self):\n        w = self.weakcontainer()\n        if w is not None:\n            w._iterating.add(self)\n        return self\n\n    def __exit__(self, e, t, b):\n        w = self.weakcontainer()\n        if w is not None:\n            s = w._iterating\n            s.remove(self)\n            if not s:\n                w._commit_removals()\n\nclass WeakSet(object):\n\n    def __init__(self, data=None):\n        self.data = set()\n\n        def _remove(item, selfref=ref(self)):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(item)\n                else:\n                    self.data.discard(item)\n        self._remove = _remove\n        self._pending_removals = []\n        self._iterating = set()\n        if data is not None:\n            self.update(data)\n\n    def _commit_removals(self):\n        l = self._pending_removals\n        discard = self.data.discard\n        while l:\n            discard(l.pop())\n\n    def __iter__(self):\n        with _IterationGuard(self):\n            for itemref in self.data:\n                item = itemref()\n                if item is not None:\n                    yield item\n\n    def __len__(self):\n        return sum((x() is not None for x in self.data))\n\n    def __contains__(self, item):\n        return ref(item) in self.data\n\n    def __reduce__(self):\n        return (self.__class__, (list(self),), getattr(self, '__dict__', None))\n    __hash__ = None\n\n    def add(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.add(ref(item, self._remove))\n\n    def clear(self):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.clear()\n\n    def copy(self):\n        return self.__class__(self)\n\n    def pop(self):\n        if self._pending_removals:\n            self._commit_removals()\n        while True:\n            try:\n                itemref = self.data.pop()\n            except KeyError:\n                raise KeyError('pop from empty WeakSet')\n            item = itemref()\n            if item is not None:\n                return item\n\n    def remove(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.remove(ref(item))\n\n    def discard(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.discard(ref(item))\n\n    def update(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if isinstance(other, self.__class__):\n            self.data.update(other.data)\n        else:\n            for element in other:\n                self.add(element)\n\n    def __ior__(self, other):\n        self.update(other)\n        return self\n\n    def _apply(self, other, method):\n        if not isinstance(other, self.__class__):\n            other = self.__class__(other)\n        newdata = method(other.data)\n        newset = self.__class__()\n        newset.data = newdata\n        return newset\n\n    def difference(self, other):\n        return self._apply(other, self.data.difference)\n    __sub__ = difference\n\n    def difference_update(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.difference_update((ref(item) for item in other))\n\n    def __isub__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.difference_update((ref(item) for item in other))\n        return self\n\n    def intersection(self, other):\n        return self._apply(other, self.data.intersection)\n    __and__ = intersection\n\n    def intersection_update(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.intersection_update((ref(item) for item in other))\n\n    def __iand__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.intersection_update((ref(item) for item in other))\n        return self\n\n    def issubset(self, other):\n        return self.data.issubset((ref(item) for item in other))\n    __lt__ = issubset\n\n    def __le__(self, other):\n        return self.data <= set((ref(item) for item in other))\n\n    def issuperset(self, other):\n        return self.data.issuperset((ref(item) for item in other))\n    __gt__ = issuperset\n\n    def __ge__(self, other):\n        return self.data >= set((ref(item) for item in other))\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n        return self.data == set((ref(item) for item in other))\n\n    def symmetric_difference(self, other):\n        return self._apply(other, self.data.symmetric_difference)\n    __xor__ = symmetric_difference\n\n    def symmetric_difference_update(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.symmetric_difference_update((ref(item) for item in other))\n\n    def __ixor__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.symmetric_difference_update((ref(item) for item in other))\n        return self\n\n    def union(self, other):\n        return self._apply(other, self.data.union)\n    __or__ = union\n\n    def isdisjoint(self, other):\n        return len(self.intersection(other)) == 0\n\nclass SortedSet(object):\n    \"\"\"\n    A sorted set based on sorted list\n\n    A sorted set implementation is used in this case because it does not\n    require its elements to be immutable/hashable.\n\n    #Not implemented: update functions, inplace operators\n    \"\"\"\n\n    def __init__(self, iterable=()):\n        self._items = []\n        self.update(iterable)\n\n    def __len__(self):\n        return len(self._items)\n\n    def __getitem__(self, i):\n        return self._items[i]\n\n    def __iter__(self):\n        return iter(self._items)\n\n    def __reversed__(self):\n        return reversed(self._items)\n\n    def __repr__(self):\n        return '%s(%r)' % (self.__class__.__name__, self._items)\n\n    def __reduce__(self):\n        return (self.__class__, (self._items,))\n\n    def __eq__(self, other):\n        if isinstance(other, self.__class__):\n            return self._items == other._items\n        else:\n            try:\n                return len(other) == len(self._items) and all((item in self for item in other))\n            except TypeError:\n                return NotImplemented\n\n    def __ne__(self, other):\n        if isinstance(other, self.__class__):\n            return self._items != other._items\n        else:\n            try:\n                return len(other) != len(self._items) or any((item not in self for item in other))\n            except TypeError:\n                return NotImplemented\n\n    def __le__(self, other):\n        return self.issubset(other)\n\n    def __lt__(self, other):\n        return len(other) > len(self._items) and self.issubset(other)\n\n    def __ge__(self, other):\n        return self.issuperset(other)\n\n    def __gt__(self, other):\n        return len(self._items) > len(other) and self.issuperset(other)\n\n    def __and__(self, other):\n        return self._intersect(other)\n    __rand__ = __and__\n\n    def __iand__(self, other):\n        isect = self._intersect(other)\n        self._items = isect._items\n        return self\n\n    def __or__(self, other):\n        return self.union(other)\n    __ror__ = __or__\n\n    def __ior__(self, other):\n        union = self.union(other)\n        self._items = union._items\n        return self\n\n    def __sub__(self, other):\n        return self._diff(other)\n\n    def __rsub__(self, other):\n        return sortedset(other) - self\n\n    def __isub__(self, other):\n        diff = self._diff(other)\n        self._items = diff._items\n        return self\n\n    def __xor__(self, other):\n        return self.symmetric_difference(other)\n    __rxor__ = __xor__\n\n    def __ixor__(self, other):\n        sym_diff = self.symmetric_difference(other)\n        self._items = sym_diff._items\n        return self\n\n    def __contains__(self, item):\n        i = self._find_insertion(item)\n        return i < len(self._items) and self._items[i] == item\n\n    def __delitem__(self, i):\n        del self._items[i]\n\n    def __delslice__(self, i, j):\n        del self._items[i:j]\n\n    def add(self, item):\n        i = self._find_insertion(item)\n        if i < len(self._items):\n            if self._items[i] != item:\n                self._items.insert(i, item)\n        else:\n            self._items.append(item)\n\n    def update(self, iterable):\n        for i in iterable:\n            self.add(i)\n\n    def clear(self):\n        del self._items[:]\n\n    def copy(self):\n        new = sortedset()\n        new._items = list(self._items)\n        return new\n\n    def isdisjoint(self, other):\n        return len(self._intersect(other)) == 0\n\n    def issubset(self, other):\n        return len(self._intersect(other)) == len(self._items)\n\n    def issuperset(self, other):\n        return len(self._intersect(other)) == len(other)\n\n    def pop(self):\n        if not self._items:\n            raise KeyError('pop from empty set')\n        return self._items.pop()\n\n    def remove(self, item):\n        i = self._find_insertion(item)\n        if i < len(self._items):\n            if self._items[i] == item:\n                self._items.pop(i)\n                return\n        raise KeyError('%r' % item)\n\n    def union(self, *others):\n        union = sortedset()\n        union._items = list(self._items)\n        for other in others:\n            for item in other:\n                union.add(item)\n        return union\n\n    def intersection(self, *others):\n        isect = self.copy()\n        for other in others:\n            isect = isect._intersect(other)\n            if not isect:\n                break\n        return isect\n\n    def difference(self, *others):\n        diff = self.copy()\n        for other in others:\n            diff = diff._diff(other)\n            if not diff:\n                break\n        return diff\n\n    def symmetric_difference(self, other):\n        diff_self_other = self._diff(other)\n        diff_other_self = other.difference(self)\n        return diff_self_other.union(diff_other_self)\n\n    def _diff(self, other):\n        diff = sortedset()\n        for item in self._items:\n            if item not in other:\n                diff.add(item)\n        return diff\n\n    def _intersect(self, other):\n        isect = sortedset()\n        for item in self._items:\n            if item in other:\n                isect.add(item)\n        return isect\n\n    def _find_insertion(self, x):\n        a = self._items\n        lo = 0\n        hi = len(a)\n        try:\n            while lo < hi:\n                mid = (lo + hi) // 2\n                if a[mid] < x:\n                    lo = mid + 1\n                else:\n                    hi = mid\n        except TypeError:\n            lo = 0\n            compared_one = False\n            while lo < hi:\n                try:\n                    if a[lo] == x or a[lo] >= x:\n                        break\n                    compared_one = True\n                except TypeError:\n                    if compared_one:\n                        break\n                lo += 1\n        return lo\nsortedset = SortedSet\n\nclass OrderedMap(Mapping):\n    \"\"\"\n    An ordered map that accepts non-hashable types for keys. It also maintains the\n    insertion order of items, behaving as OrderedDict in that regard. These maps\n    are constructed and read just as normal mapping types, except that they may\n    contain arbitrary collections and other non-hashable items as keys::\n\n        >>> od = OrderedMap([({'one': 1, 'two': 2}, 'value'),\n        ...                  ({'three': 3, 'four': 4}, 'value2')])\n        >>> list(od.keys())\n        [{'two': 2, 'one': 1}, {'three': 3, 'four': 4}]\n        >>> list(od.values())\n        ['value', 'value2']\n\n    These constructs are needed to support nested collections in Cassandra 2.1.3+,\n    where frozen collections can be specified as parameters to others::\n\n        CREATE TABLE example (\n            ...\n            value map<frozen<map<int, int>>, double>\n            ...\n        )\n\n    This class derives from the (immutable) Mapping API. Objects in these maps\n    are not intended be modified.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        if len(args) > 1:\n            raise TypeError('expected at most 1 arguments, got %d' % len(args))\n        self._items = []\n        self._index = {}\n        if args:\n            e = args[0]\n            if callable(getattr(e, 'keys', None)):\n                for k in e.keys():\n                    self._insert(k, e[k])\n            else:\n                for k, v in e:\n                    self._insert(k, v)\n        for k, v in kwargs.items():\n            self._insert(k, v)\n\n    def _insert(self, key, value):\n        flat_key = self._serialize_key(key)\n        i = self._index.get(flat_key, -1)\n        if i >= 0:\n            self._items[i] = (key, value)\n        else:\n            self._items.append((key, value))\n            self._index[flat_key] = len(self._items) - 1\n    __setitem__ = _insert\n\n    def __getitem__(self, key):\n        try:\n            index = self._index[self._serialize_key(key)]\n            return self._items[index][1]\n        except KeyError:\n            raise KeyError(str(key))\n\n    def __delitem__(self, key):\n        try:\n            index = self._index.pop(self._serialize_key(key))\n            self._index = dict(((k, i if i < index else i - 1) for k, i in self._index.items()))\n            self._items.pop(index)\n        except KeyError:\n            raise KeyError(str(key))\n\n    def __iter__(self):\n        for i in self._items:\n            yield i[0]\n\n    def __len__(self):\n        return len(self._items)\n\n    def __eq__(self, other):\n        if isinstance(other, OrderedMap):\n            return self._items == other._items\n        try:\n            d = dict(other)\n            return len(d) == len(self._items) and all((i[1] == d[i[0]] for i in self._items))\n        except KeyError:\n            return False\n        except TypeError:\n            pass\n        return NotImplemented\n\n    def __repr__(self):\n        return '%s([%s])' % (self.__class__.__name__, ', '.join(('(%r, %r)' % (k, v) for k, v in self._items)))\n\n    def __str__(self):\n        return '{%s}' % ', '.join(('%r: %r' % (k, v) for k, v in self._items))\n\n    def popitem(self):\n        try:\n            kv = self._items.pop()\n            del self._index[self._serialize_key(kv[0])]\n            return kv\n        except IndexError:\n            raise KeyError()\n\n    def _serialize_key(self, key):\n        return pickle.dumps(key)\n\nclass OrderedMapSerializedKey(OrderedMap):\n\n    def __init__(self, cass_type, protocol_version):\n        super(OrderedMapSerializedKey, self).__init__()\n        self.cass_key_type = cass_type\n        self.protocol_version = protocol_version\n\n    def _insert_unchecked(self, key, flat_key, value):\n        self._items.append((key, value))\n        self._index[flat_key] = len(self._items) - 1\n\n    def _serialize_key(self, key):\n        return self.cass_key_type.serialize(key, self.protocol_version)\n\n@total_ordering\nclass Time(object):\n    \"\"\"\n    Idealized time, independent of day.\n\n    Up to nanosecond resolution\n    \"\"\"\n    MICRO = 1000\n    MILLI = 1000 * MICRO\n    SECOND = 1000 * MILLI\n    MINUTE = 60 * SECOND\n    HOUR = 60 * MINUTE\n    DAY = 24 * HOUR\n    nanosecond_time = 0\n\n    def __init__(self, value):\n        \"\"\"\n        Initializer value can be:\n\n        - integer_type: absolute nanoseconds in the day\n        - datetime.time: built-in time\n        - string_type: a string time of the form \"HH:MM:SS[.mmmuuunnn]\"\n        \"\"\"\n        if isinstance(value, int):\n            self._from_timestamp(value)\n        elif isinstance(value, datetime.time):\n            self._from_time(value)\n        elif isinstance(value, str):\n            self._from_timestring(value)\n        else:\n            raise TypeError('Time arguments must be a whole number, datetime.time, or string')\n\n    @property\n    def hour(self):\n        \"\"\"\n        The hour component of this time (0-23)\n        \"\"\"\n        return self.nanosecond_time // Time.HOUR\n\n    @property\n    def minute(self):\n        \"\"\"\n        The minute component of this time (0-59)\n        \"\"\"\n        minutes = self.nanosecond_time // Time.MINUTE\n        return minutes % 60\n\n    @property\n    def second(self):\n        \"\"\"\n        The second component of this time (0-59)\n        \"\"\"\n        seconds = self.nanosecond_time // Time.SECOND\n        return seconds % 60\n\n    @property\n    def nanosecond(self):\n        \"\"\"\n        The fractional seconds component of the time, in nanoseconds\n        \"\"\"\n        return self.nanosecond_time % Time.SECOND\n\n    def time(self):\n        \"\"\"\n        Return a built-in datetime.time (nanosecond precision truncated to micros).\n        \"\"\"\n        return datetime.time(hour=self.hour, minute=self.minute, second=self.second, microsecond=self.nanosecond // Time.MICRO)\n\n    def _from_timestamp(self, t):\n        if t >= Time.DAY:\n            raise ValueError('value must be less than number of nanoseconds in a day (%d)' % Time.DAY)\n        self.nanosecond_time = t\n\n    def _from_timestring(self, s):\n        try:\n            parts = s.split('.')\n            base_time = time.strptime(parts[0], '%H:%M:%S')\n            self.nanosecond_time = base_time.tm_hour * Time.HOUR + base_time.tm_min * Time.MINUTE + base_time.tm_sec * Time.SECOND\n            if len(parts) > 1:\n                nano_time_str = parts[1] + '0' * (9 - len(parts[1]))\n                self.nanosecond_time += int(nano_time_str)\n        except ValueError:\n            raise ValueError(\"can't interpret %r as a time\" % (s,))\n\n    def _from_time(self, t):\n        self.nanosecond_time = t.hour * Time.HOUR + t.minute * Time.MINUTE + t.second * Time.SECOND + t.microsecond * Time.MICRO\n\n    def __hash__(self):\n        return self.nanosecond_time\n\n    def __eq__(self, other):\n        if isinstance(other, Time):\n            return self.nanosecond_time == other.nanosecond_time\n        if isinstance(other, int):\n            return self.nanosecond_time == other\n        return self.nanosecond_time % Time.MICRO == 0 and datetime.time(hour=self.hour, minute=self.minute, second=self.second, microsecond=self.nanosecond // Time.MICRO) == other\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __lt__(self, other):\n        if not isinstance(other, Time):\n            return NotImplemented\n        return self.nanosecond_time < other.nanosecond_time\n\n    def __repr__(self):\n        return 'Time(%s)' % self.nanosecond_time\n\n    def __str__(self):\n        return '%02d:%02d:%02d.%09d' % (self.hour, self.minute, self.second, self.nanosecond)\n\n@total_ordering\nclass Date(object):\n    \"\"\"\n    Idealized date: year, month, day\n\n    Offers wider year range than datetime.date. For Dates that cannot be represented\n    as a datetime.date (because datetime.MINYEAR, datetime.MAXYEAR), this type falls back\n    to printing days_from_epoch offset.\n    \"\"\"\n    MINUTE = 60\n    HOUR = 60 * MINUTE\n    DAY = 24 * HOUR\n    date_format = '%Y-%m-%d'\n    days_from_epoch = 0\n\n    def __init__(self, value):\n        \"\"\"\n        Initializer value can be:\n\n        - integer_type: absolute days from epoch (1970, 1, 1). Can be negative.\n        - datetime.date: built-in date\n        - string_type: a string time of the form \"yyyy-mm-dd\"\n        \"\"\"\n        if isinstance(value, int):\n            self.days_from_epoch = value\n        elif isinstance(value, (datetime.date, datetime.datetime)):\n            self._from_timetuple(value.timetuple())\n        elif isinstance(value, str):\n            self._from_datestring(value)\n        else:\n            raise TypeError('Date arguments must be a whole number, datetime.date, or string')\n\n    @property\n    def seconds(self):\n        \"\"\"\n        Absolute seconds from epoch (can be negative)\n        \"\"\"\n        return self.days_from_epoch * Date.DAY\n\n    def date(self):\n        \"\"\"\n        Return a built-in datetime.date for Dates falling in the years [datetime.MINYEAR, datetime.MAXYEAR]\n\n        ValueError is raised for Dates outside this range.\n        \"\"\"\n        try:\n            dt = datetime_from_timestamp(self.seconds)\n            return datetime.date(dt.year, dt.month, dt.day)\n        except Exception:\n            raise ValueError('%r exceeds ranges for built-in datetime.date' % self)\n\n    def _from_timetuple(self, t):\n        self.days_from_epoch = calendar.timegm(t) // Date.DAY\n\n    def _from_datestring(self, s):\n        if s[0] == '+':\n            s = s[1:]\n        dt = datetime.datetime.strptime(s, self.date_format)\n        self._from_timetuple(dt.timetuple())\n\n    def __hash__(self):\n        return self.days_from_epoch\n\n    def __eq__(self, other):\n        if isinstance(other, Date):\n            return self.days_from_epoch == other.days_from_epoch\n        if isinstance(other, int):\n            return self.days_from_epoch == other\n        try:\n            return self.date() == other\n        except Exception:\n            return False\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __lt__(self, other):\n        if not isinstance(other, Date):\n            return NotImplemented\n        return self.days_from_epoch < other.days_from_epoch\n\n    def __repr__(self):\n        return 'Date(%s)' % self.days_from_epoch\n\n    def __str__(self):\n        try:\n            dt = datetime_from_timestamp(self.seconds)\n            return '%04d-%02d-%02d' % (dt.year, dt.month, dt.day)\n        except:\n            return str(self.days_from_epoch)\ninet_pton = socket.inet_pton\ninet_ntop = socket.inet_ntop\n\ndef _positional_rename_invalid_identifiers(field_names):\n    names_out = list(field_names)\n    for index, name in enumerate(field_names):\n        if not all((c.isalnum() or c == '_' for c in name)) or keyword.iskeyword(name) or (not name) or name[0].isdigit() or name.startswith('_'):\n            names_out[index] = 'field_%d_' % index\n    return names_out\n\ndef _sanitize_identifiers(field_names):\n    names_out = _positional_rename_invalid_identifiers(field_names)\n    if len(names_out) != len(set(names_out)):\n        observed_names = set()\n        for index, name in enumerate(names_out):\n            while names_out[index] in observed_names:\n                names_out[index] = '%s_' % (names_out[index],)\n            observed_names.add(names_out[index])\n    return names_out\n\ndef list_contents_to_tuple(to_convert):\n    if isinstance(to_convert, list):\n        for n, i in enumerate(to_convert):\n            if isinstance(to_convert[n], list):\n                to_convert[n] = tuple(to_convert[n])\n        return tuple(to_convert)\n    else:\n        return to_convert\n\nclass Point(object):\n    \"\"\"\n    Represents a point geometry for DSE\n    \"\"\"\n    x = None\n    '\\n    x coordinate of the point\\n    '\n    y = None\n    '\\n    y coordinate of the point\\n    '\n\n    def __init__(self, x=_nan, y=_nan):\n        self.x = x\n        self.y = y\n\n    def __eq__(self, other):\n        return isinstance(other, Point) and self.x == other.x and (self.y == other.y)\n\n    def __hash__(self):\n        return hash((self.x, self.y))\n\n    def __str__(self):\n        \"\"\"\n        Well-known text representation of the point\n        \"\"\"\n        return 'POINT (%r %r)' % (self.x, self.y)\n\n    def __repr__(self):\n        return '%s(%r, %r)' % (self.__class__.__name__, self.x, self.y)\n\n    @staticmethod\n    def from_wkt(s):\n        \"\"\"\n        Parse a Point geometry from a wkt string and return a new Point object.\n        \"\"\"\n        if not _HAS_GEOMET:\n            raise DriverException('Geomet is required to deserialize a wkt geometry.')\n        try:\n            geom = wkt.loads(s)\n        except ValueError:\n            raise ValueError(\"Invalid WKT geometry: '{0}'\".format(s))\n        if geom['type'] != 'Point':\n            raise ValueError(\"Invalid WKT geometry type. Expected 'Point', got '{0}': '{1}'\".format(geom['type'], s))\n        coords = geom['coordinates']\n        if len(coords) < 2:\n            x = y = _nan\n        else:\n            x = coords[0]\n            y = coords[1]\n        return Point(x=x, y=y)\n\nclass LineString(object):\n    \"\"\"\n    Represents a linestring geometry for DSE\n    \"\"\"\n    coords = None\n    '\\n    Tuple of (x, y) coordinates in the linestring\\n    '\n\n    def __init__(self, coords=tuple()):\n        \"\"\"\n        'coords`: a sequence of (x, y) coordinates of points in the linestring\n        \"\"\"\n        self.coords = tuple(coords)\n\n    def __eq__(self, other):\n        return isinstance(other, LineString) and self.coords == other.coords\n\n    def __hash__(self):\n        return hash(self.coords)\n\n    def __str__(self):\n        \"\"\"\n        Well-known text representation of the LineString\n        \"\"\"\n        if not self.coords:\n            return 'LINESTRING EMPTY'\n        return 'LINESTRING (%s)' % ', '.join(('%r %r' % (x, y) for x, y in self.coords))\n\n    def __repr__(self):\n        return '%s(%r)' % (self.__class__.__name__, self.coords)\n\n    @staticmethod\n    def from_wkt(s):\n        \"\"\"\n        Parse a LineString geometry from a wkt string and return a new LineString object.\n        \"\"\"\n        if not _HAS_GEOMET:\n            raise DriverException('Geomet is required to deserialize a wkt geometry.')\n        try:\n            geom = wkt.loads(s)\n        except ValueError:\n            raise ValueError(\"Invalid WKT geometry: '{0}'\".format(s))\n        if geom['type'] != 'LineString':\n            raise ValueError(\"Invalid WKT geometry type. Expected 'LineString', got '{0}': '{1}'\".format(geom['type'], s))\n        geom['coordinates'] = list_contents_to_tuple(geom['coordinates'])\n        return LineString(coords=geom['coordinates'])\n\nclass _LinearRing(object):\n\n    def __init__(self, coords=tuple()):\n        self.coords = list_contents_to_tuple(coords)\n\n    def __eq__(self, other):\n        return isinstance(other, _LinearRing) and self.coords == other.coords\n\n    def __hash__(self):\n        return hash(self.coords)\n\n    def __str__(self):\n        if not self.coords:\n            return 'LINEARRING EMPTY'\n        return 'LINEARRING (%s)' % ', '.join(('%r %r' % (x, y) for x, y in self.coords))\n\n    def __repr__(self):\n        return '%s(%r)' % (self.__class__.__name__, self.coords)\n\nclass Polygon(object):\n    \"\"\"\n    Represents a polygon geometry for DSE\n    \"\"\"\n    exterior = None\n    '\\n    _LinearRing representing the exterior of the polygon\\n    '\n    interiors = None\n    '\\n    Tuple of _LinearRings representing interior holes in the polygon\\n    '\n\n    def __init__(self, exterior=tuple(), interiors=None):\n        \"\"\"\n        'exterior`: a sequence of (x, y) coordinates of points in the linestring\n        `interiors`: None, or a sequence of sequences or (x, y) coordinates of points describing interior linear rings\n        \"\"\"\n        self.exterior = _LinearRing(exterior)\n        self.interiors = tuple((_LinearRing(e) for e in interiors)) if interiors else tuple()\n\n    def __eq__(self, other):\n        return isinstance(other, Polygon) and self.exterior == other.exterior and (self.interiors == other.interiors)\n\n    def __hash__(self):\n        return hash((self.exterior, self.interiors))\n\n    def __str__(self):\n        \"\"\"\n        Well-known text representation of the polygon\n        \"\"\"\n        if not self.exterior.coords:\n            return 'POLYGON EMPTY'\n        rings = [ring.coords for ring in chain((self.exterior,), self.interiors)]\n        rings = ['(%s)' % ', '.join(('%r %r' % (x, y) for x, y in ring)) for ring in rings]\n        return 'POLYGON (%s)' % ', '.join(rings)\n\n    def __repr__(self):\n        return '%s(%r, %r)' % (self.__class__.__name__, self.exterior.coords, [ring.coords for ring in self.interiors])\n\n    @staticmethod\n    def from_wkt(s):\n        \"\"\"\n        Parse a Polygon geometry from a wkt string and return a new Polygon object.\n        \"\"\"\n        if not _HAS_GEOMET:\n            raise DriverException('Geomet is required to deserialize a wkt geometry.')\n        try:\n            geom = wkt.loads(s)\n        except ValueError:\n            raise ValueError(\"Invalid WKT geometry: '{0}'\".format(s))\n        if geom['type'] != 'Polygon':\n            raise ValueError(\"Invalid WKT geometry type. Expected 'Polygon', got '{0}': '{1}'\".format(geom['type'], s))\n        coords = geom['coordinates']\n        exterior = coords[0] if len(coords) > 0 else tuple()\n        interiors = coords[1:] if len(coords) > 1 else None\n        return Polygon(exterior=exterior, interiors=interiors)\n_distance_wkt_pattern = re.compile('distance *\\\\( *\\\\( *([\\\\d\\\\.-]+) *([\\\\d+\\\\.-]+) *\\\\) *([\\\\d+\\\\.-]+) *\\\\) *$', re.IGNORECASE)\n\nclass Distance(object):\n    \"\"\"\n    Represents a Distance geometry for DSE\n    \"\"\"\n    x = None\n    '\\n    x coordinate of the center point\\n    '\n    y = None\n    '\\n    y coordinate of the center point\\n    '\n    radius = None\n    '\\n    radius to represent the distance from the center point\\n    '\n\n    def __init__(self, x=_nan, y=_nan, radius=_nan):\n        self.x = x\n        self.y = y\n        self.radius = radius\n\n    def __eq__(self, other):\n        return isinstance(other, Distance) and self.x == other.x and (self.y == other.y) and (self.radius == other.radius)\n\n    def __hash__(self):\n        return hash((self.x, self.y, self.radius))\n\n    def __str__(self):\n        \"\"\"\n        Well-known text representation of the point\n        \"\"\"\n        return 'DISTANCE ((%r %r) %r)' % (self.x, self.y, self.radius)\n\n    def __repr__(self):\n        return '%s(%r, %r, %r)' % (self.__class__.__name__, self.x, self.y, self.radius)\n\n    @staticmethod\n    def from_wkt(s):\n        \"\"\"\n        Parse a Distance geometry from a wkt string and return a new Distance object.\n        \"\"\"\n        distance_match = _distance_wkt_pattern.match(s)\n        if distance_match is None:\n            raise ValueError(\"Invalid WKT geometry: '{0}'\".format(s))\n        x, y, radius = distance_match.groups()\n        return Distance(x, y, radius)\n\nclass Duration(object):\n    \"\"\"\n    Cassandra Duration Type\n    \"\"\"\n    months = 0\n    ''\n    days = 0\n    ''\n    nanoseconds = 0\n    ''\n\n    def __init__(self, months=0, days=0, nanoseconds=0):\n        self.months = months\n        self.days = days\n        self.nanoseconds = nanoseconds\n\n    def __eq__(self, other):\n        return isinstance(other, self.__class__) and self.months == other.months and (self.days == other.days) and (self.nanoseconds == other.nanoseconds)\n\n    def __repr__(self):\n        return 'Duration({0}, {1}, {2})'.format(self.months, self.days, self.nanoseconds)\n\n    def __str__(self):\n        has_negative_values = self.months < 0 or self.days < 0 or self.nanoseconds < 0\n        return '%s%dmo%dd%dns' % ('-' if has_negative_values else '', abs(self.months), abs(self.days), abs(self.nanoseconds))\n\nclass DateRangePrecision(object):\n    \"\"\"\n    An \"enum\" representing the valid values for :attr:`DateRange.precision`.\n    \"\"\"\n    YEAR = 'YEAR'\n    '\\n    '\n    MONTH = 'MONTH'\n    '\\n    '\n    DAY = 'DAY'\n    '\\n    '\n    HOUR = 'HOUR'\n    '\\n    '\n    MINUTE = 'MINUTE'\n    '\\n    '\n    SECOND = 'SECOND'\n    '\\n    '\n    MILLISECOND = 'MILLISECOND'\n    '\\n    '\n    PRECISIONS = (YEAR, MONTH, DAY, HOUR, MINUTE, SECOND, MILLISECOND)\n    '\\n    '\n\n    @classmethod\n    def _to_int(cls, precision):\n        return cls.PRECISIONS.index(precision.upper())\n\n    @classmethod\n    def _round_to_precision(cls, ms, precision, default_dt):\n        try:\n            dt = utc_datetime_from_ms_timestamp(ms)\n        except OverflowError:\n            return ms\n        precision_idx = cls._to_int(precision)\n        replace_kwargs = {}\n        if precision_idx <= cls._to_int(DateRangePrecision.YEAR):\n            replace_kwargs['month'] = default_dt.month\n        if precision_idx <= cls._to_int(DateRangePrecision.MONTH):\n            replace_kwargs['day'] = default_dt.day\n        if precision_idx <= cls._to_int(DateRangePrecision.DAY):\n            replace_kwargs['hour'] = default_dt.hour\n        if precision_idx <= cls._to_int(DateRangePrecision.HOUR):\n            replace_kwargs['minute'] = default_dt.minute\n        if precision_idx <= cls._to_int(DateRangePrecision.MINUTE):\n            replace_kwargs['second'] = default_dt.second\n        if precision_idx <= cls._to_int(DateRangePrecision.SECOND):\n            replace_kwargs['microsecond'] = default_dt.microsecond // 1000 * 1000\n        if precision_idx == cls._to_int(DateRangePrecision.MILLISECOND):\n            replace_kwargs['microsecond'] = int(round(dt.microsecond, -3))\n        return ms_timestamp_from_datetime(dt.replace(**replace_kwargs))\n\n    @classmethod\n    def round_up_to_precision(cls, ms, precision):\n        if precision == cls.MONTH:\n            date_ms = utc_datetime_from_ms_timestamp(ms)\n            upper_date = datetime.datetime.max.replace(year=date_ms.year, month=date_ms.month, day=calendar.monthrange(date_ms.year, date_ms.month)[1])\n        else:\n            upper_date = datetime.datetime.max\n        return cls._round_to_precision(ms, precision, upper_date)\n\n    @classmethod\n    def round_down_to_precision(cls, ms, precision):\n        return cls._round_to_precision(ms, precision, datetime.datetime.min)\n\n@total_ordering\nclass DateRangeBound(object):\n    \"\"\"DateRangeBound(value, precision)\n    Represents a single date value and its precision for :class:`DateRange`.\n\n    .. attribute:: milliseconds\n\n        Integer representing milliseconds since the UNIX epoch. May be negative.\n\n    .. attribute:: precision\n\n        String representing the precision of a bound. Must be a valid\n        :class:`DateRangePrecision` member.\n\n    :class:`DateRangeBound` uses a millisecond offset from the UNIX epoch to\n    allow :class:`DateRange` to represent values `datetime.datetime` cannot.\n    For such values, string representions will show this offset rather than the\n    CQL representation.\n    \"\"\"\n    milliseconds = None\n    precision = None\n\n    def __init__(self, value, precision):\n        \"\"\"\n        :param value: a value representing ms since the epoch. Accepts an\n            integer or a datetime.\n        :param precision: a string representing precision\n        \"\"\"\n        if precision is not None:\n            try:\n                self.precision = precision.upper()\n            except AttributeError:\n                raise TypeError('precision must be a string; got %r' % precision)\n        if value is None:\n            milliseconds = None\n        elif isinstance(value, int):\n            milliseconds = value\n        elif isinstance(value, datetime.datetime):\n            value = value.replace(microsecond=int(round(value.microsecond, -3)))\n            milliseconds = ms_timestamp_from_datetime(value)\n        else:\n            raise ValueError('%r is not a valid value for DateRangeBound' % value)\n        self.milliseconds = milliseconds\n        self.validate()\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n        return self.milliseconds == other.milliseconds and self.precision == other.precision\n\n    def __lt__(self, other):\n        return (str(self.milliseconds), str(self.precision)) < (str(other.milliseconds), str(other.precision))\n\n    def datetime(self):\n        \"\"\"\n        Return :attr:`milliseconds` as a :class:`datetime.datetime` if possible.\n        Raises an `OverflowError` if the value is out of range.\n        \"\"\"\n        return utc_datetime_from_ms_timestamp(self.milliseconds)\n\n    def validate(self):\n        attrs = (self.milliseconds, self.precision)\n        if attrs == (None, None):\n            return\n        if None in attrs:\n            raise TypeError('%s.datetime and %s.precision must not be None unless both are None; Got: %r' % (self.__class__.__name__, self.__class__.__name__, self))\n        if self.precision not in DateRangePrecision.PRECISIONS:\n            raise ValueError('%s.precision: expected value in %r; got %r' % (self.__class__.__name__, DateRangePrecision.PRECISIONS, self.precision))\n\n    @classmethod\n    def from_value(cls, value):\n        \"\"\"\n        Construct a new :class:`DateRangeBound` from a given value. If\n        possible, use the `value['milliseconds']` and `value['precision']` keys\n        of the argument. Otherwise, use the argument as a `(milliseconds,\n        precision)` iterable.\n\n        :param value: a dictlike or iterable object\n        \"\"\"\n        if isinstance(value, cls):\n            return value\n        try:\n            milliseconds, precision = (value.get('milliseconds'), value.get('precision'))\n        except AttributeError:\n            milliseconds = precision = None\n        if milliseconds is not None and precision is not None:\n            return DateRangeBound(value=milliseconds, precision=precision)\n        return DateRangeBound(*value)\n\n    def round_up(self):\n        if self.milliseconds is None or self.precision is None:\n            return self\n        self.milliseconds = DateRangePrecision.round_up_to_precision(self.milliseconds, self.precision)\n        return self\n\n    def round_down(self):\n        if self.milliseconds is None or self.precision is None:\n            return self\n        self.milliseconds = DateRangePrecision.round_down_to_precision(self.milliseconds, self.precision)\n        return self\n    _formatter_map = {DateRangePrecision.YEAR: '%Y', DateRangePrecision.MONTH: '%Y-%m', DateRangePrecision.DAY: '%Y-%m-%d', DateRangePrecision.HOUR: '%Y-%m-%dT%HZ', DateRangePrecision.MINUTE: '%Y-%m-%dT%H:%MZ', DateRangePrecision.SECOND: '%Y-%m-%dT%H:%M:%SZ', DateRangePrecision.MILLISECOND: '%Y-%m-%dT%H:%M:%S'}\n\n    def __str__(self):\n        if self == OPEN_BOUND:\n            return '*'\n        try:\n            dt = self.datetime()\n        except OverflowError:\n            return '%sms' % (self.milliseconds,)\n        formatted = dt.strftime(self._formatter_map[self.precision])\n        if self.precision == DateRangePrecision.MILLISECOND:\n            return '%s.%03dZ' % (formatted, dt.microsecond / 1000)\n        return formatted\n\n    def __repr__(self):\n        return '%s(milliseconds=%r, precision=%r)' % (self.__class__.__name__, self.milliseconds, self.precision)\nOPEN_BOUND = DateRangeBound(value=None, precision=None)\n'\\nRepresents `*`, an open value or bound for :class:`DateRange`.\\n'\n\n@total_ordering\nclass DateRange(object):\n    \"\"\"DateRange(lower_bound=None, upper_bound=None, value=None)\n    DSE DateRange Type\n\n    .. attribute:: lower_bound\n\n        :class:`~DateRangeBound` representing the lower bound of a bounded range.\n\n    .. attribute:: upper_bound\n\n        :class:`~DateRangeBound` representing the upper bound of a bounded range.\n\n    .. attribute:: value\n\n        :class:`~DateRangeBound` representing the value of a single-value range.\n\n    As noted in its documentation, :class:`DateRangeBound` uses a millisecond\n    offset from the UNIX epoch to allow :class:`DateRange` to represent values\n    `datetime.datetime` cannot. For such values, string representions will show\n    this offset rather than the CQL representation.\n    \"\"\"\n    lower_bound = None\n    upper_bound = None\n    value = None\n\n    def __init__(self, lower_bound=None, upper_bound=None, value=None):\n        \"\"\"\n        :param lower_bound: a :class:`DateRangeBound` or object accepted by\n            :meth:`DateRangeBound.from_value` to be used as a\n            :attr:`lower_bound`. Mutually exclusive with `value`. If\n            `upper_bound` is specified and this is not, the :attr:`lower_bound`\n            will be open.\n        :param upper_bound: a :class:`DateRangeBound` or object accepted by\n            :meth:`DateRangeBound.from_value` to be used as a\n            :attr:`upper_bound`. Mutually exclusive with `value`. If\n            `lower_bound` is specified and this is not, the :attr:`upper_bound`\n            will be open.\n        :param value: a :class:`DateRangeBound` or object accepted by\n            :meth:`DateRangeBound.from_value` to be used as :attr:`value`. Mutually\n            exclusive with `lower_bound` and `lower_bound`.\n        \"\"\"\n        lower_bound = DateRangeBound.from_value(lower_bound).round_down() if lower_bound else lower_bound\n        upper_bound = DateRangeBound.from_value(upper_bound).round_up() if upper_bound else upper_bound\n        value = DateRangeBound.from_value(value).round_down() if value else value\n        if lower_bound is None and upper_bound is not None:\n            lower_bound = OPEN_BOUND\n        if upper_bound is None and lower_bound is not None:\n            upper_bound = OPEN_BOUND\n        self.lower_bound, self.upper_bound, self.value = (lower_bound, upper_bound, value)\n        self.validate()\n\n    def validate(self):\n        if self.value is None:\n            if self.lower_bound is None or self.upper_bound is None:\n                raise ValueError('%s instances where value attribute is None must set lower_bound or upper_bound; got %r' % (self.__class__.__name__, self))\n        elif self.lower_bound is not None or self.upper_bound is not None:\n            raise ValueError('%s instances where value attribute is not None must not set lower_bound or upper_bound; got %r' % (self.__class__.__name__, self))\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n        return self.lower_bound == other.lower_bound and self.upper_bound == other.upper_bound and (self.value == other.value)\n\n    def __lt__(self, other):\n        return (str(self.lower_bound), str(self.upper_bound), str(self.value)) < (str(other.lower_bound), str(other.upper_bound), str(other.value))\n\n    def __str__(self):\n        if self.value:\n            return str(self.value)\n        else:\n            return '[%s TO %s]' % (self.lower_bound, self.upper_bound)\n\n    def __repr__(self):\n        return '%s(lower_bound=%r, upper_bound=%r, value=%r)' % (self.__class__.__name__, self.lower_bound, self.upper_bound, self.value)\n\n@total_ordering\nclass Version(object):\n    \"\"\"\n    Internal minimalist class to compare versions.\n    A valid version is: <int>.<int>.<int>.<int or str>.\n\n    TODO: when python2 support is removed, use packaging.version.\n    \"\"\"\n    _version = None\n    major = None\n    minor = 0\n    patch = 0\n    build = 0\n    prerelease = 0\n\n    def __init__(self, version):\n        self._version = version\n        if '-' in version:\n            version_without_prerelease, self.prerelease = version.split('-', 1)\n        else:\n            version_without_prerelease = version\n        parts = list(reversed(version_without_prerelease.split('.')))\n        if len(parts) > 4:\n            prerelease_string = '-{}'.format(self.prerelease) if self.prerelease else ''\n            log.warning('Unrecognized version: {}. Only 4 components plus prerelease are supported. Assuming version as {}{}'.format(version, '.'.join(parts[:-5:-1]), prerelease_string))\n        try:\n            self.major = int(parts.pop())\n        except ValueError as e:\n            raise ValueError(\"Couldn't parse version {}. Version should start with a number\".format(version)).with_traceback(e.__traceback__)\n        try:\n            self.minor = int(parts.pop()) if parts else 0\n            self.patch = int(parts.pop()) if parts else 0\n            if parts:\n                build = parts.pop()\n                try:\n                    self.build = int(build)\n                except ValueError:\n                    self.build = build\n        except ValueError:\n            assumed_version = '{}.{}.{}.{}-{}'.format(self.major, self.minor, self.patch, self.build, self.prerelease)\n            log.warning('Unrecognized version {}. Assuming version as {}'.format(version, assumed_version))\n\n    def __hash__(self):\n        return self._version\n\n    def __repr__(self):\n        version_string = 'Version({0}, {1}, {2}'.format(self.major, self.minor, self.patch)\n        if self.build:\n            version_string += ', {}'.format(self.build)\n        if self.prerelease:\n            version_string += ', {}'.format(self.prerelease)\n        version_string += ')'\n        return version_string\n\n    def __str__(self):\n        return self._version\n\n    @staticmethod\n    def _compare_version_part(version, other_version, cmp):\n        if not (isinstance(version, int) and isinstance(other_version, int)):\n            version = str(version)\n            other_version = str(other_version)\n        return cmp(version, other_version)\n\n    def __eq__(self, other):\n        if not isinstance(other, Version):\n            return NotImplemented\n        return self.major == other.major and self.minor == other.minor and (self.patch == other.patch) and self._compare_version_part(self.build, other.build, lambda s, o: s == o) and self._compare_version_part(self.prerelease, other.prerelease, lambda s, o: s == o)\n\n    def __gt__(self, other):\n        if not isinstance(other, Version):\n            return NotImplemented\n        is_major_ge = self.major >= other.major\n        is_minor_ge = self.minor >= other.minor\n        is_patch_ge = self.patch >= other.patch\n        is_build_gt = self._compare_version_part(self.build, other.build, lambda s, o: s > o)\n        is_build_ge = self._compare_version_part(self.build, other.build, lambda s, o: s >= o)\n        if self.prerelease and (not other.prerelease):\n            is_prerelease_gt = False\n        elif other.prerelease and (not self.prerelease):\n            is_prerelease_gt = True\n        else:\n            is_prerelease_gt = self._compare_version_part(self.prerelease, other.prerelease, lambda s, o: s > o)\n        return self.major > other.major or (is_major_ge and self.minor > other.minor) or (is_major_ge and is_minor_ge and (self.patch > other.patch)) or (is_major_ge and is_minor_ge and is_patch_ge and is_build_gt) or (is_major_ge and is_minor_ge and is_patch_ge and is_build_ge and is_prerelease_gt)\n\ndef maybe_add_timeout_to_query(stmt: str, metadata_request_timeout: Optional[datetime.timedelta]) -> str:\n    if metadata_request_timeout is None:\n        return stmt\n    ms = int(metadata_request_timeout / datetime.timedelta(milliseconds=1))\n    if ms == 0:\n        return stmt\n    return f'{stmt} USING TIMEOUT {ms}ms'",
    "cassandra/policies.py": "import random\nfrom collections import namedtuple\nfrom functools import lru_cache\nfrom itertools import islice, cycle, groupby, repeat\nimport logging\nfrom random import randint, shuffle\nfrom threading import Lock\nimport socket\nimport warnings\nlog = logging.getLogger(__name__)\nfrom cassandra import WriteType as WT\nWriteType = WT\nfrom cassandra import ConsistencyLevel, OperationTimedOut\n\nclass HostDistance(object):\n    \"\"\"\n    A measure of how \"distant\" a node is from the client, which\n    may influence how the load balancer distributes requests\n    and how many connections are opened to the node.\n    \"\"\"\n    IGNORED = -1\n    '\\n    A node with this distance should never be queried or have\\n    connections opened to it.\\n    '\n    LOCAL_RACK = 0\n    '\\n    Nodes with ``LOCAL_RACK`` distance will be preferred for operations\\n    under some load balancing policies (such as :class:`.RackAwareRoundRobinPolicy`)\\n    and will have a greater number of connections opened against\\n    them by default.\\n\\n    This distance is typically used for nodes within the same\\n    datacenter and the same rack as the client.\\n    '\n    LOCAL = 1\n    '\\n    Nodes with ``LOCAL`` distance will be preferred for operations\\n    under some load balancing policies (such as :class:`.DCAwareRoundRobinPolicy`)\\n    and will have a greater number of connections opened against\\n    them by default.\\n\\n    This distance is typically used for nodes within the same\\n    datacenter as the client.\\n    '\n    REMOTE = 2\n    '\\n    Nodes with ``REMOTE`` distance will be treated as a last resort\\n    by some load balancing policies (such as :class:`.DCAwareRoundRobinPolicy`\\n    and :class:`.RackAwareRoundRobinPolicy`)and will have a smaller number of\\n    connections opened against them by default.\\n\\n    This distance is typically used for nodes outside of the\\n    datacenter that the client is running in.\\n    '\n\nclass HostStateListener(object):\n\n    def on_up(self, host):\n        \"\"\" Called when a node is marked up. \"\"\"\n        raise NotImplementedError()\n\n    def on_down(self, host):\n        \"\"\" Called when a node is marked down. \"\"\"\n        raise NotImplementedError()\n\n    def on_add(self, host):\n        \"\"\"\n        Called when a node is added to the cluster.  The newly added node\n        should be considered up.\n        \"\"\"\n        raise NotImplementedError()\n\n    def on_remove(self, host):\n        \"\"\" Called when a node is removed from the cluster. \"\"\"\n        raise NotImplementedError()\n\nclass LoadBalancingPolicy(HostStateListener):\n    \"\"\"\n    Load balancing policies are used to decide how to distribute\n    requests among all possible coordinator nodes in the cluster.\n\n    In particular, they may focus on querying \"near\" nodes (those\n    in a local datacenter) or on querying nodes who happen to\n    be replicas for the requested data.\n\n    You may also use subclasses of :class:`.LoadBalancingPolicy` for\n    custom behavior.\n\n    You should always use immutable collections (e.g., tuples or\n    frozensets) to store information about hosts to prevent accidental\n    modification. When there are changes to the hosts (e.g., a host is\n    down or up), the old collection should be replaced with a new one.\n    \"\"\"\n    _hosts_lock = None\n\n    def __init__(self):\n        self._hosts_lock = Lock()\n\n    def distance(self, host):\n        \"\"\"\n        Returns a measure of how remote a :class:`~.pool.Host` is in\n        terms of the :class:`.HostDistance` enums.\n        \"\"\"\n        raise NotImplementedError()\n\n    def populate(self, cluster, hosts):\n        \"\"\"\n        This method is called to initialize the load balancing\n        policy with a set of :class:`.Host` instances before its\n        first use.  The `cluster` parameter is an instance of\n        :class:`.Cluster`.\n        \"\"\"\n        raise NotImplementedError()\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        \"\"\"\n        Given a :class:`~.query.Statement` instance, return a iterable\n        of :class:`.Host` instances which should be queried in that\n        order.  A generator may work well for custom implementations\n        of this method.\n\n        Note that the `query` argument may be :const:`None` when preparing\n        statements.\n\n        `working_keyspace` should be the string name of the current keyspace,\n        as set through :meth:`.Session.set_keyspace()` or with a ``USE``\n        statement.\n        \"\"\"\n        raise NotImplementedError()\n\n    def check_supported(self):\n        \"\"\"\n        This will be called after the cluster Metadata has been initialized.\n        If the load balancing policy implementation cannot be supported for\n        some reason (such as a missing C extension), this is the point at\n        which it should raise an exception.\n        \"\"\"\n        pass\n\nclass RoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    A subclass of :class:`.LoadBalancingPolicy` which evenly\n    distributes queries across all nodes in the cluster,\n    regardless of what datacenter the nodes may be in.\n    \"\"\"\n    _live_hosts = frozenset(())\n    _position = 0\n\n    def populate(self, cluster, hosts):\n        self._live_hosts = frozenset(hosts)\n        if len(hosts) > 1:\n            self._position = randint(0, len(hosts) - 1)\n\n    def distance(self, host):\n        return HostDistance.LOCAL\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        pos = self._position\n        self._position += 1\n        hosts = self._live_hosts\n        length = len(hosts)\n        if length:\n            pos %= length\n            return islice(cycle(hosts), pos, pos + length)\n        else:\n            return []\n\n    def on_up(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.union((host,))\n\n    def on_down(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.difference((host,))\n\n    def on_add(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.union((host,))\n\n    def on_remove(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.difference((host,))\n\nclass DCAwareRoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    Similar to :class:`.RoundRobinPolicy`, but prefers hosts\n    in the local datacenter and only uses nodes in remote\n    datacenters as a last resort.\n    \"\"\"\n    local_dc = None\n    used_hosts_per_remote_dc = 0\n\n    def __init__(self, local_dc='', used_hosts_per_remote_dc=0):\n        \"\"\"\n        The `local_dc` parameter should be the name of the datacenter\n        (such as is reported by ``nodetool ring``) that should\n        be considered local. If not specified, the driver will choose\n        a local_dc based on the first host among :attr:`.Cluster.contact_points`\n        having a valid DC. If relying on this mechanism, all specified\n        contact points should be nodes in a single, local DC.\n\n        `used_hosts_per_remote_dc` controls how many nodes in\n        each remote datacenter will have connections opened\n        against them. In other words, `used_hosts_per_remote_dc` hosts\n        will be considered :attr:`~.HostDistance.REMOTE` and the\n        rest will be considered :attr:`~.HostDistance.IGNORED`.\n        By default, all remote hosts are ignored.\n        \"\"\"\n        self.local_dc = local_dc\n        self.used_hosts_per_remote_dc = used_hosts_per_remote_dc\n        self._dc_live_hosts = {}\n        self._position = 0\n        self._endpoints = []\n        LoadBalancingPolicy.__init__(self)\n\n    def _dc(self, host):\n        return host.datacenter or self.local_dc\n\n    def populate(self, cluster, hosts):\n        for dc, dc_hosts in groupby(hosts, lambda h: self._dc(h)):\n            self._dc_live_hosts[dc] = tuple(set(dc_hosts))\n        if not self.local_dc:\n            self._endpoints = [endpoint for endpoint in cluster.endpoints_resolved]\n        self._position = randint(0, len(hosts) - 1) if hosts else 0\n\n    def distance(self, host):\n        dc = self._dc(host)\n        if dc == self.local_dc:\n            return HostDistance.LOCAL\n        if not self.used_hosts_per_remote_dc:\n            return HostDistance.IGNORED\n        else:\n            dc_hosts = self._dc_live_hosts.get(dc)\n            if not dc_hosts:\n                return HostDistance.IGNORED\n            if host in list(dc_hosts)[:self.used_hosts_per_remote_dc]:\n                return HostDistance.REMOTE\n            else:\n                return HostDistance.IGNORED\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        pos = self._position\n        self._position += 1\n        local_live = self._dc_live_hosts.get(self.local_dc, ())\n        pos = pos % len(local_live) if local_live else 0\n        for host in islice(cycle(local_live), pos, pos + len(local_live)):\n            yield host\n        other_dcs = [dc for dc in self._dc_live_hosts.copy().keys() if dc != self.local_dc]\n        for dc in other_dcs:\n            remote_live = self._dc_live_hosts.get(dc, ())\n            for host in remote_live[:self.used_hosts_per_remote_dc]:\n                yield host\n\n    def on_up(self, host):\n        if not self.local_dc and host.datacenter:\n            if host.endpoint in self._endpoints:\n                self.local_dc = host.datacenter\n                log.info(\"Using datacenter '%s' for DCAwareRoundRobinPolicy (via host '%s'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\" % (self.local_dc, host.endpoint))\n                del self._endpoints\n        dc = self._dc(host)\n        with self._hosts_lock:\n            current_hosts = self._dc_live_hosts.get(dc, ())\n            if host not in current_hosts:\n                self._dc_live_hosts[dc] = current_hosts + (host,)\n\n    def on_down(self, host):\n        dc = self._dc(host)\n        with self._hosts_lock:\n            current_hosts = self._dc_live_hosts.get(dc, ())\n            if host in current_hosts:\n                hosts = tuple((h for h in current_hosts if h != host))\n                if hosts:\n                    self._dc_live_hosts[dc] = hosts\n                else:\n                    del self._dc_live_hosts[dc]\n\n    def on_add(self, host):\n        self.on_up(host)\n\n    def on_remove(self, host):\n        self.on_down(host)\n\nclass RackAwareRoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    Similar to :class:`.DCAwareRoundRobinPolicy`, but prefers hosts\n    in the local rack, before hosts in the local datacenter but a\n    different rack, before hosts in all other datercentres\n    \"\"\"\n    local_dc = None\n    local_rack = None\n    used_hosts_per_remote_dc = 0\n\n    def __init__(self, local_dc, local_rack, used_hosts_per_remote_dc=0):\n        \"\"\"\n        The `local_dc` and `local_rack` parameters should be the name of the\n        datacenter and rack (such as is reported by ``nodetool ring``) that\n        should be considered local.\n\n        `used_hosts_per_remote_dc` controls how many nodes in\n        each remote datacenter will have connections opened\n        against them. In other words, `used_hosts_per_remote_dc` hosts\n        will be considered :attr:`~.HostDistance.REMOTE` and the\n        rest will be considered :attr:`~.HostDistance.IGNORED`.\n        By default, all remote hosts are ignored.\n        \"\"\"\n        self.local_rack = local_rack\n        self.local_dc = local_dc\n        self.used_hosts_per_remote_dc = used_hosts_per_remote_dc\n        self._live_hosts = {}\n        self._dc_live_hosts = {}\n        self._endpoints = []\n        self._position = 0\n        LoadBalancingPolicy.__init__(self)\n\n    def _rack(self, host):\n        return host.rack or self.local_rack\n\n    def _dc(self, host):\n        return host.datacenter or self.local_dc\n\n    def populate(self, cluster, hosts):\n        for (dc, rack), rack_hosts in groupby(hosts, lambda host: (self._dc(host), self._rack(host))):\n            self._live_hosts[dc, rack] = tuple(set(rack_hosts))\n        for dc, dc_hosts in groupby(hosts, lambda host: self._dc(host)):\n            self._dc_live_hosts[dc] = tuple(set(dc_hosts))\n        self._position = randint(0, len(hosts) - 1) if hosts else 0\n\n    def distance(self, host):\n        rack = self._rack(host)\n        dc = self._dc(host)\n        if rack == self.local_rack and dc == self.local_dc:\n            return HostDistance.LOCAL_RACK\n        if dc == self.local_dc:\n            return HostDistance.LOCAL\n        if not self.used_hosts_per_remote_dc:\n            return HostDistance.IGNORED\n        dc_hosts = self._dc_live_hosts.get(dc, ())\n        if not dc_hosts:\n            return HostDistance.IGNORED\n        if host in dc_hosts and dc_hosts.index(host) < self.used_hosts_per_remote_dc:\n            return HostDistance.REMOTE\n        else:\n            return HostDistance.IGNORED\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        pos = self._position\n        self._position += 1\n        local_rack_live = self._live_hosts.get((self.local_dc, self.local_rack), ())\n        pos = pos % len(local_rack_live) if local_rack_live else 0\n        for host in islice(cycle(local_rack_live), pos, pos + len(local_rack_live)):\n            yield host\n        local_live = [host for host in self._dc_live_hosts.get(self.local_dc, ()) if host.rack != self.local_rack]\n        pos = pos % len(local_live) if local_live else 0\n        for host in islice(cycle(local_live), pos, pos + len(local_live)):\n            yield host\n        for dc, remote_live in self._dc_live_hosts.copy().items():\n            if dc != self.local_dc:\n                for host in remote_live[:self.used_hosts_per_remote_dc]:\n                    yield host\n\n    def on_up(self, host):\n        dc = self._dc(host)\n        rack = self._rack(host)\n        with self._hosts_lock:\n            current_rack_hosts = self._live_hosts.get((dc, rack), ())\n            if host not in current_rack_hosts:\n                self._live_hosts[dc, rack] = current_rack_hosts + (host,)\n            current_dc_hosts = self._dc_live_hosts.get(dc, ())\n            if host not in current_dc_hosts:\n                self._dc_live_hosts[dc] = current_dc_hosts + (host,)\n\n    def on_down(self, host):\n        dc = self._dc(host)\n        rack = self._rack(host)\n        with self._hosts_lock:\n            current_rack_hosts = self._live_hosts.get((dc, rack), ())\n            if host in current_rack_hosts:\n                hosts = tuple((h for h in current_rack_hosts if h != host))\n                if hosts:\n                    self._live_hosts[dc, rack] = hosts\n                else:\n                    del self._live_hosts[dc, rack]\n            current_dc_hosts = self._dc_live_hosts.get(dc, ())\n            if host in current_dc_hosts:\n                hosts = tuple((h for h in current_dc_hosts if h != host))\n                if hosts:\n                    self._dc_live_hosts[dc] = hosts\n                else:\n                    del self._dc_live_hosts[dc]\n\n    def on_add(self, host):\n        self.on_up(host)\n\n    def on_remove(self, host):\n        self.on_down(host)\n\nclass TokenAwarePolicy(LoadBalancingPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` wrapper that adds token awareness to\n    a child policy.\n\n    This alters the child policy's behavior so that it first attempts to\n    send queries to :attr:`~.HostDistance.LOCAL` replicas (as determined\n    by the child policy) based on the :class:`.Statement`'s\n    :attr:`~.Statement.routing_key`. If :attr:`.shuffle_replicas` is\n    truthy, these replicas will be yielded in a random order. Once those\n    hosts are exhausted, the remaining hosts in the child policy's query\n    plan will be used in the order provided by the child policy.\n\n    If no :attr:`~.Statement.routing_key` is set on the query, the child\n    policy's query plan will be used as is.\n    \"\"\"\n    _child_policy = None\n    _cluster_metadata = None\n    _tablets_routing_v1 = False\n    shuffle_replicas = False\n    '\\n    Yield local replicas in a random order.\\n    '\n\n    def __init__(self, child_policy, shuffle_replicas=False):\n        self._child_policy = child_policy\n        self.shuffle_replicas = shuffle_replicas\n\n    def populate(self, cluster, hosts):\n        self._cluster_metadata = cluster.metadata\n        self._tablets_routing_v1 = cluster.control_connection._tablets_routing_v1\n        self._child_policy.populate(cluster, hosts)\n\n    def check_supported(self):\n        if not self._cluster_metadata.can_support_partitioner():\n            raise RuntimeError('%s cannot be used with the cluster partitioner (%s) because the relevant C extension for this driver was not compiled. See the installation instructions for details on building and installing the C extensions.' % (self.__class__.__name__, self._cluster_metadata.partitioner))\n\n    def distance(self, *args, **kwargs):\n        return self._child_policy.distance(*args, **kwargs)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        keyspace = query.keyspace if query and query.keyspace else working_keyspace\n        child = self._child_policy\n        if query is None or query.routing_key is None or keyspace is None:\n            for host in child.make_query_plan(keyspace, query):\n                yield host\n            return\n        replicas = []\n        if self._tablets_routing_v1:\n            tablet = self._cluster_metadata._tablets.get_tablet_for_key(keyspace, query.table, self._cluster_metadata.token_map.token_class.from_key(query.routing_key))\n            if tablet is not None:\n                replicas_mapped = set(map(lambda r: r[0], tablet.replicas))\n                child_plan = child.make_query_plan(keyspace, query)\n                replicas = [host for host in child_plan if host.host_id in replicas_mapped]\n        if not replicas:\n            replicas = self._cluster_metadata.get_replicas(keyspace, query.routing_key)\n        if self.shuffle_replicas:\n            shuffle(replicas)\n        for replica in replicas:\n            if replica.is_up and child.distance(replica) in [HostDistance.LOCAL, HostDistance.LOCAL_RACK]:\n                yield replica\n        for host in child.make_query_plan(keyspace, query):\n            if host not in replicas or child.distance(host) == HostDistance.REMOTE:\n                yield host\n\n    def on_up(self, *args, **kwargs):\n        return self._child_policy.on_up(*args, **kwargs)\n\n    def on_down(self, *args, **kwargs):\n        return self._child_policy.on_down(*args, **kwargs)\n\n    def on_add(self, *args, **kwargs):\n        return self._child_policy.on_add(*args, **kwargs)\n\n    def on_remove(self, *args, **kwargs):\n        return self._child_policy.on_remove(*args, **kwargs)\n\nclass WhiteListRoundRobinPolicy(RoundRobinPolicy):\n    \"\"\"\n    A subclass of :class:`.RoundRobinPolicy` which evenly\n    distributes queries across all nodes in the cluster,\n    regardless of what datacenter the nodes may be in, but\n    only if that node exists in the list of allowed nodes\n\n    This policy is addresses the issue described in\n    https://datastax-oss.atlassian.net/browse/JAVA-145\n    Where connection errors occur when connection\n    attempts are made to private IP addresses remotely\n    \"\"\"\n\n    def __init__(self, hosts):\n        \"\"\"\n        The `hosts` parameter should be a sequence of hosts to permit\n        connections to.\n        \"\"\"\n        self._allowed_hosts = tuple(hosts)\n        self._allowed_hosts_resolved = []\n        for h in self._allowed_hosts:\n            unix_socket_path = getattr(h, '_unix_socket_path', None)\n            if unix_socket_path:\n                self._allowed_hosts_resolved.append(unix_socket_path)\n            else:\n                self._allowed_hosts_resolved.extend([endpoint[4][0] for endpoint in socket.getaddrinfo(h, None, socket.AF_UNSPEC, socket.SOCK_STREAM)])\n        RoundRobinPolicy.__init__(self)\n\n    def populate(self, cluster, hosts):\n        self._live_hosts = frozenset((h for h in hosts if h.address in self._allowed_hosts_resolved))\n        if len(hosts) <= 1:\n            self._position = 0\n        else:\n            self._position = randint(0, len(hosts) - 1)\n\n    def distance(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            return HostDistance.LOCAL\n        else:\n            return HostDistance.IGNORED\n\n    def on_up(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            RoundRobinPolicy.on_up(self, host)\n\n    def on_add(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            RoundRobinPolicy.on_add(self, host)\n\nclass HostFilterPolicy(LoadBalancingPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` subclass configured with a child policy,\n    and a single-argument predicate. This policy defers to the child policy for\n    hosts where ``predicate(host)`` is truthy. Hosts for which\n    ``predicate(host)`` is falsy will be considered :attr:`.IGNORED`, and will\n    not be used in a query plan.\n\n    This can be used in the cases where you need a whitelist or blacklist\n    policy, e.g. to prepare for decommissioning nodes or for testing:\n\n    .. code-block:: python\n\n        def address_is_ignored(host):\n            return host.address in [ignored_address0, ignored_address1]\n\n        blacklist_filter_policy = HostFilterPolicy(\n            child_policy=RoundRobinPolicy(),\n            predicate=address_is_ignored\n        )\n\n        cluster = Cluster(\n            primary_host,\n            load_balancing_policy=blacklist_filter_policy,\n        )\n\n    See the note in the :meth:`.make_query_plan` documentation for a caveat on\n    how wrapping ordering polices (e.g. :class:`.RoundRobinPolicy`) may break\n    desirable properties of the wrapped policy.\n\n    Please note that whitelist and blacklist policies are not recommended for\n    general, day-to-day use. You probably want something like\n    :class:`.DCAwareRoundRobinPolicy`, which prefers a local DC but has\n    fallbacks, over a brute-force method like whitelisting or blacklisting.\n    \"\"\"\n\n    def __init__(self, child_policy, predicate):\n        \"\"\"\n        :param child_policy: an instantiated :class:`.LoadBalancingPolicy`\n                             that this one will defer to.\n        :param predicate: a one-parameter function that takes a :class:`.Host`.\n                          If it returns a falsy value, the :class:`.Host` will\n                          be :attr:`.IGNORED` and not returned in query plans.\n        \"\"\"\n        super(HostFilterPolicy, self).__init__()\n        self._child_policy = child_policy\n        self._predicate = predicate\n\n    def on_up(self, host, *args, **kwargs):\n        return self._child_policy.on_up(host, *args, **kwargs)\n\n    def on_down(self, host, *args, **kwargs):\n        return self._child_policy.on_down(host, *args, **kwargs)\n\n    def on_add(self, host, *args, **kwargs):\n        return self._child_policy.on_add(host, *args, **kwargs)\n\n    def on_remove(self, host, *args, **kwargs):\n        return self._child_policy.on_remove(host, *args, **kwargs)\n\n    @property\n    def predicate(self):\n        \"\"\"\n        A predicate, set on object initialization, that takes a :class:`.Host`\n        and returns a value. If the value is falsy, the :class:`.Host` is\n        :class:`~HostDistance.IGNORED`. If the value is truthy,\n        :class:`.HostFilterPolicy` defers to the child policy to determine the\n        host's distance.\n\n        This is a read-only value set in ``__init__``, implemented as a\n        ``property``.\n        \"\"\"\n        return self._predicate\n\n    def distance(self, host):\n        \"\"\"\n        Checks if ``predicate(host)``, then returns\n        :attr:`~HostDistance.IGNORED` if falsy, and defers to the child policy\n        otherwise.\n        \"\"\"\n        if self.predicate(host):\n            return self._child_policy.distance(host)\n        else:\n            return HostDistance.IGNORED\n\n    def populate(self, cluster, hosts):\n        self._child_policy.populate(cluster=cluster, hosts=hosts)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        \"\"\"\n        Defers to the child policy's\n        :meth:`.LoadBalancingPolicy.make_query_plan` and filters the results.\n\n        Note that this filtering may break desirable properties of the wrapped\n        policy in some cases. For instance, imagine if you configure this\n        policy to filter out ``host2``, and to wrap a round-robin policy that\n        rotates through three hosts in the order ``host1, host2, host3``,\n        ``host2, host3, host1``, ``host3, host1, host2``, repeating. This\n        policy will yield ``host1, host3``, ``host3, host1``, ``host3, host1``,\n        disproportionately favoring ``host3``.\n        \"\"\"\n        child_qp = self._child_policy.make_query_plan(working_keyspace=working_keyspace, query=query)\n        for host in child_qp:\n            if self.predicate(host):\n                yield host\n\n    def check_supported(self):\n        return self._child_policy.check_supported()\n\nclass ConvictionPolicy(object):\n    \"\"\"\n    A policy which decides when hosts should be considered down\n    based on the types of failures and the number of failures.\n\n    If custom behavior is needed, this class may be subclassed.\n    \"\"\"\n\n    def __init__(self, host):\n        \"\"\"\n        `host` is an instance of :class:`.Host`.\n        \"\"\"\n        self.host = host\n\n    def add_failure(self, connection_exc):\n        \"\"\"\n        Implementations should return :const:`True` if the host should be\n        convicted, :const:`False` otherwise.\n        \"\"\"\n        raise NotImplementedError()\n\n    def reset(self):\n        \"\"\"\n        Implementations should clear out any convictions or state regarding\n        the host.\n        \"\"\"\n        raise NotImplementedError()\n\nclass SimpleConvictionPolicy(ConvictionPolicy):\n    \"\"\"\n    The default implementation of :class:`ConvictionPolicy`,\n    which simply marks a host as down after the first failure\n    of any kind.\n    \"\"\"\n\n    def add_failure(self, connection_exc):\n        return not isinstance(connection_exc, OperationTimedOut)\n\n    def reset(self):\n        pass\n\nclass ReconnectionPolicy(object):\n    \"\"\"\n    This class and its subclasses govern how frequently an attempt is made\n    to reconnect to nodes that are marked as dead.\n\n    If custom behavior is needed, this class may be subclassed.\n    \"\"\"\n\n    def new_schedule(self):\n        \"\"\"\n        This should return a finite or infinite iterable of delays (each as a\n        floating point number of seconds) in-between each failed reconnection\n        attempt.  Note that if the iterable is finite, reconnection attempts\n        will cease once the iterable is exhausted.\n        \"\"\"\n        raise NotImplementedError()\n\nclass ConstantReconnectionPolicy(ReconnectionPolicy):\n    \"\"\"\n    A :class:`.ReconnectionPolicy` subclass which sleeps for a fixed delay\n    in-between each reconnection attempt.\n    \"\"\"\n\n    def __init__(self, delay, max_attempts=64):\n        \"\"\"\n        `delay` should be a floating point number of seconds to wait in-between\n        each attempt.\n\n        `max_attempts` should be a total number of attempts to be made before\n        giving up, or :const:`None` to continue reconnection attempts forever.\n        The default is 64.\n        \"\"\"\n        if delay < 0:\n            raise ValueError('delay must not be negative')\n        if max_attempts is not None and max_attempts < 0:\n            raise ValueError('max_attempts must not be negative')\n        self.delay = delay\n        self.max_attempts = max_attempts\n\n    def new_schedule(self):\n        if self.max_attempts:\n            return repeat(self.delay, self.max_attempts)\n        return repeat(self.delay)\n\nclass ExponentialReconnectionPolicy(ReconnectionPolicy):\n    \"\"\"\n    A :class:`.ReconnectionPolicy` subclass which exponentially increases\n    the length of the delay in-between each reconnection attempt up to\n    a set maximum delay.\n\n    A random amount of jitter (+/- 15%) will be added to the pure exponential\n    delay value to avoid the situations where many reconnection handlers are\n    trying to reconnect at exactly the same time.\n    \"\"\"\n\n    def __init__(self, base_delay, max_delay, max_attempts=64):\n        \"\"\"\n        `base_delay` and `max_delay` should be in floating point units of\n        seconds.\n\n        `max_attempts` should be a total number of attempts to be made before\n        giving up, or :const:`None` to continue reconnection attempts forever.\n        The default is 64.\n        \"\"\"\n        if base_delay < 0 or max_delay < 0:\n            raise ValueError('Delays may not be negative')\n        if max_delay < base_delay:\n            raise ValueError('Max delay must be greater than base delay')\n        if max_attempts is not None and max_attempts < 0:\n            raise ValueError('max_attempts must not be negative')\n        self.base_delay = base_delay\n        self.max_delay = max_delay\n        self.max_attempts = max_attempts\n\n    def new_schedule(self):\n        i, overflowed = (0, False)\n        while self.max_attempts is None or i < self.max_attempts:\n            if overflowed:\n                yield self.max_delay\n            else:\n                try:\n                    yield self._add_jitter(min(self.base_delay * 2 ** i, self.max_delay))\n                except OverflowError:\n                    overflowed = True\n                    yield self.max_delay\n            i += 1\n\n    def _add_jitter(self, value):\n        jitter = randint(85, 115)\n        delay = jitter * value / 100\n        return min(max(self.base_delay, delay), self.max_delay)\n\nclass RetryPolicy(object):\n    \"\"\"\n    A policy that describes whether to retry, rethrow, or ignore coordinator\n    timeout and unavailable failures. These are failures reported from the\n    server side. Timeouts are configured by\n    `settings in cassandra.yaml <https://github.com/apache/cassandra/blob/cassandra-2.1.4/conf/cassandra.yaml#L568-L584>`_.\n    Unavailable failures occur when the coordinator cannot achieve the consistency\n    level for a request. For further information see the method descriptions\n    below.\n\n    To specify a default retry policy, set the\n    :attr:`.Cluster.default_retry_policy` attribute to an instance of this\n    class or one of its subclasses.\n\n    To specify a retry policy per query, set the :attr:`.Statement.retry_policy`\n    attribute to an instance of this class or one of its subclasses.\n\n    If custom behavior is needed for retrying certain operations,\n    this class may be subclassed.\n    \"\"\"\n    RETRY = 0\n    '\\n    This should be returned from the below methods if the operation\\n    should be retried on the same connection.\\n    '\n    RETHROW = 1\n    '\\n    This should be returned from the below methods if the failure\\n    should be propagated and no more retries attempted.\\n    '\n    IGNORE = 2\n    '\\n    This should be returned from the below methods if the failure\\n    should be ignored but no more retries should be attempted.\\n    '\n    RETRY_NEXT_HOST = 3\n    '\\n    This should be returned from the below methods if the operation\\n    should be retried on another connection.\\n    '\n\n    def on_read_timeout(self, query, consistency, required_responses, received_responses, data_retrieved, retry_num):\n        \"\"\"\n        This is called when a read operation times out from the coordinator's\n        perspective (i.e. a replica did not respond to the coordinator in time).\n        It should return a tuple with two items: one of the class enums (such\n        as :attr:`.RETRY`) and a :class:`.ConsistencyLevel` to retry the\n        operation at or :const:`None` to keep the same consistency level.\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        The `required_responses` and `received_responses` parameters describe\n        how many replicas needed to respond to meet the requested consistency\n        level and how many actually did respond before the coordinator timed\n        out the request. `data_retrieved` is a boolean indicating whether\n        any of those responses contained data (as opposed to just a digest).\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, operations will be retried at most once, and only if\n        a sufficient number of replicas responded (with data digests).\n        \"\"\"\n        if retry_num != 0:\n            return (self.RETHROW, None)\n        elif received_responses >= required_responses and (not data_retrieved):\n            return (self.RETRY, consistency)\n        else:\n            return (self.RETHROW, None)\n\n    def on_write_timeout(self, query, consistency, write_type, required_responses, received_responses, retry_num):\n        \"\"\"\n        This is called when a write operation times out from the coordinator's\n        perspective (i.e. a replica did not respond to the coordinator in time).\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `write_type` is one of the :class:`.WriteType` enums describing the\n        type of write operation.\n\n        The `required_responses` and `received_responses` parameters describe\n        how many replicas needed to acknowledge the write to meet the requested\n        consistency level and how many replicas actually did acknowledge the\n        write before the coordinator timed out the request.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, failed write operations will retried at most once, and\n        they will only be retried if the `write_type` was\n        :attr:`~.WriteType.BATCH_LOG`.\n        \"\"\"\n        if retry_num != 0:\n            return (self.RETHROW, None)\n        elif write_type == WriteType.BATCH_LOG:\n            return (self.RETRY, consistency)\n        else:\n            return (self.RETHROW, None)\n\n    def on_unavailable(self, query, consistency, required_replicas, alive_replicas, retry_num):\n        \"\"\"\n        This is called when the coordinator node determines that a read or\n        write operation cannot be successful because the number of live\n        replicas are too low to meet the requested :class:`.ConsistencyLevel`.\n        This means that the read or write operation was never forwarded to\n        any replicas.\n\n        `query` is the :class:`.Statement` that failed.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `required_replicas` is the number of replicas that would have needed to\n        acknowledge the operation to meet the requested consistency level.\n        `alive_replicas` is the number of replicas that the coordinator\n        considered alive at the time of the request.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, if this is the first retry, it triggers a retry on the next\n        host in the query plan with the same consistency level. If this is not the\n        first retry, no retries will be attempted and the error will be re-raised.\n        \"\"\"\n        return (self.RETRY_NEXT_HOST, None) if retry_num == 0 else (self.RETHROW, None)\n\n    def on_request_error(self, query, consistency, error, retry_num):\n        \"\"\"\n        This is called when an unexpected error happens. This can be in the\n        following situations:\n\n        * On a connection error\n        * On server errors: overloaded, isBootstrapping, serverError, etc.\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `error` the instance of the exception.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, it triggers a retry on the next host in the query plan\n        with the same consistency level.\n        \"\"\"\n        return (self.RETRY_NEXT_HOST, None)\n\nclass FallthroughRetryPolicy(RetryPolicy):\n    \"\"\"\n    A retry policy that never retries and always propagates failures to\n    the application.\n    \"\"\"\n\n    def on_read_timeout(self, *args, **kwargs):\n        return (self.RETHROW, None)\n\n    def on_write_timeout(self, *args, **kwargs):\n        return (self.RETHROW, None)\n\n    def on_unavailable(self, *args, **kwargs):\n        return (self.RETHROW, None)\n\n    def on_request_error(self, *args, **kwargs):\n        return (self.RETHROW, None)\n\nclass DowngradingConsistencyRetryPolicy(RetryPolicy):\n    \"\"\"\n    *Deprecated:* This retry policy will be removed in the next major release.\n\n    A retry policy that sometimes retries with a lower consistency level than\n    the one initially requested.\n\n    **BEWARE**: This policy may retry queries using a lower consistency\n    level than the one initially requested. By doing so, it may break\n    consistency guarantees. In other words, if you use this retry policy,\n    there are cases (documented below) where a read at :attr:`~.QUORUM`\n    *may not* see a preceding write at :attr:`~.QUORUM`. Do not use this\n    policy unless you have understood the cases where this can happen and\n    are ok with that. It is also recommended to subclass this class so\n    that queries that required a consistency level downgrade can be\n    recorded (so that repairs can be made later, etc).\n\n    This policy implements the same retries as :class:`.RetryPolicy`,\n    but on top of that, it also retries in the following cases:\n\n    * On a read timeout: if the number of replicas that responded is\n      greater than one but lower than is required by the requested\n      consistency level, the operation is retried at a lower consistency\n      level.\n    * On a write timeout: if the operation is an :attr:`~.UNLOGGED_BATCH`\n      and at least one replica acknowledged the write, the operation is\n      retried at a lower consistency level.  Furthermore, for other\n      write types, if at least one replica acknowledged the write, the\n      timeout is ignored.\n    * On an unavailable exception: if at least one replica is alive, the\n      operation is retried at a lower consistency level.\n\n    The reasoning behind this retry policy is as follows: if, based\n    on the information the Cassandra coordinator node returns, retrying the\n    operation with the initially requested consistency has a chance to\n    succeed, do it. Otherwise, if based on that information we know the\n    initially requested consistency level cannot be achieved currently, then:\n\n    * For writes, ignore the exception (thus silently failing the\n      consistency requirement) if we know the write has been persisted on at\n      least one replica.\n    * For reads, try reading at a lower consistency level (thus silently\n      failing the consistency requirement).\n\n    In other words, this policy implements the idea that if the requested\n    consistency level cannot be achieved, the next best thing for writes is\n    to make sure the data is persisted, and that reading something is better\n    than reading nothing, even if there is a risk of reading stale data.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(DowngradingConsistencyRetryPolicy, self).__init__(*args, **kwargs)\n        warnings.warn('DowngradingConsistencyRetryPolicy is deprecated and will be removed in the next major release.', DeprecationWarning)\n\n    def _pick_consistency(self, num_responses):\n        if num_responses >= 3:\n            return (self.RETRY, ConsistencyLevel.THREE)\n        elif num_responses >= 2:\n            return (self.RETRY, ConsistencyLevel.TWO)\n        elif num_responses >= 1:\n            return (self.RETRY, ConsistencyLevel.ONE)\n        else:\n            return (self.RETHROW, None)\n\n    def on_read_timeout(self, query, consistency, required_responses, received_responses, data_retrieved, retry_num):\n        if retry_num != 0:\n            return (self.RETHROW, None)\n        elif ConsistencyLevel.is_serial(consistency):\n            return (self.RETHROW, None)\n        elif received_responses < required_responses:\n            return self._pick_consistency(received_responses)\n        elif not data_retrieved:\n            return (self.RETRY, consistency)\n        else:\n            return (self.RETHROW, None)\n\n    def on_write_timeout(self, query, consistency, write_type, required_responses, received_responses, retry_num):\n        if retry_num != 0:\n            return (self.RETHROW, None)\n        if write_type in (WriteType.SIMPLE, WriteType.BATCH, WriteType.COUNTER):\n            if received_responses > 0:\n                return (self.IGNORE, None)\n            else:\n                return (self.RETHROW, None)\n        elif write_type == WriteType.UNLOGGED_BATCH:\n            return self._pick_consistency(received_responses)\n        elif write_type == WriteType.BATCH_LOG:\n            return (self.RETRY, consistency)\n        return (self.RETHROW, None)\n\n    def on_unavailable(self, query, consistency, required_replicas, alive_replicas, retry_num):\n        if retry_num != 0:\n            return (self.RETHROW, None)\n        elif ConsistencyLevel.is_serial(consistency):\n            return (self.RETRY_NEXT_HOST, None)\n        else:\n            return self._pick_consistency(alive_replicas)\n\nclass ExponentialBackoffRetryPolicy(RetryPolicy):\n    \"\"\"\n    A policy that do retries with exponential backoff\n    \"\"\"\n\n    def __init__(self, max_num_retries: float, min_interval: float=0.1, max_interval: float=10.0, *args, **kwargs):\n        \"\"\"\n        `max_num_retries` counts how many times the operation would be retried,\n        `min_interval` is the initial time in seconds to wait before first retry\n        `max_interval` is the maximum time to wait between retries\n        \"\"\"\n        self.min_interval = min_interval\n        self.max_num_retries = max_num_retries\n        self.max_interval = max_interval\n        super(ExponentialBackoffRetryPolicy).__init__(*args, **kwargs)\n\n    def _calculate_backoff(self, attempt: int):\n        delay = min(self.max_interval, self.min_interval * 2 ** attempt)\n        delay += random.random() * self.min_interval - self.min_interval / 2\n        return delay\n\n    def on_read_timeout(self, query, consistency, required_responses, received_responses, data_retrieved, retry_num):\n        if retry_num < self.max_num_retries and received_responses >= required_responses and (not data_retrieved):\n            return (self.RETRY, consistency, self._calculate_backoff(retry_num))\n        else:\n            return (self.RETHROW, None, None)\n\n    def on_write_timeout(self, query, consistency, write_type, required_responses, received_responses, retry_num):\n        if retry_num < self.max_num_retries and write_type == WriteType.BATCH_LOG:\n            return (self.RETRY, consistency, self._calculate_backoff(retry_num))\n        else:\n            return (self.RETHROW, None, None)\n\n    def on_unavailable(self, query, consistency, required_replicas, alive_replicas, retry_num):\n        if retry_num < self.max_num_retries:\n            return (self.RETRY_NEXT_HOST, None, self._calculate_backoff(retry_num))\n        else:\n            return (self.RETHROW, None, None)\n\n    def on_request_error(self, query, consistency, error, retry_num):\n        if retry_num < self.max_num_retries:\n            return (self.RETRY_NEXT_HOST, None, self._calculate_backoff(retry_num))\n        else:\n            return (self.RETHROW, None, None)\n\nclass AddressTranslator(object):\n    \"\"\"\n    Interface for translating cluster-defined endpoints.\n\n    The driver discovers nodes using server metadata and topology change events. Normally,\n    the endpoint defined by the server is the right way to connect to a node. In some environments,\n    these addresses may not be reachable, or not preferred (public vs. private IPs in cloud environments,\n    suboptimal routing, etc). This interface allows for translating from server defined endpoints to\n    preferred addresses for driver connections.\n\n    *Note:* :attr:`~Cluster.contact_points` provided while creating the :class:`~.Cluster` instance are not\n    translated using this mechanism -- only addresses received from Cassandra nodes are.\n    \"\"\"\n\n    def translate(self, addr):\n        \"\"\"\n        Accepts the node ip address, and returns a translated address to be used connecting to this node.\n        \"\"\"\n        raise NotImplementedError()\n\nclass IdentityTranslator(AddressTranslator):\n    \"\"\"\n    Returns the endpoint with no translation\n    \"\"\"\n\n    def translate(self, addr):\n        return addr\n\nclass EC2MultiRegionTranslator(AddressTranslator):\n    \"\"\"\n    Resolves private ips of the hosts in the same datacenter as the client, and public ips of hosts in other datacenters.\n    \"\"\"\n\n    def translate(self, addr):\n        \"\"\"\n        Reverse DNS the public broadcast_address, then lookup that hostname to get the AWS-resolved IP, which\n        will point to the private IP address within the same datacenter.\n        \"\"\"\n        family = socket.getaddrinfo(addr, 0, socket.AF_UNSPEC, socket.SOCK_STREAM)[0][0]\n        host = socket.getfqdn(addr)\n        for a in socket.getaddrinfo(host, 0, family, socket.SOCK_STREAM):\n            try:\n                return a[4][0]\n            except Exception:\n                pass\n        return addr\n\nclass SpeculativeExecutionPolicy(object):\n    \"\"\"\n    Interface for specifying speculative execution plans\n    \"\"\"\n\n    def new_plan(self, keyspace, statement):\n        \"\"\"\n        Returns\n\n        :param keyspace:\n        :param statement:\n        :return:\n        \"\"\"\n        raise NotImplementedError()\n\nclass SpeculativeExecutionPlan(object):\n\n    def next_execution(self, host):\n        raise NotImplementedError()\n\nclass NoSpeculativeExecutionPlan(SpeculativeExecutionPlan):\n\n    def next_execution(self, host):\n        return -1\n\nclass NoSpeculativeExecutionPolicy(SpeculativeExecutionPolicy):\n\n    def new_plan(self, keyspace, statement):\n        return NoSpeculativeExecutionPlan()\n\nclass ConstantSpeculativeExecutionPolicy(SpeculativeExecutionPolicy):\n    \"\"\"\n    A speculative execution policy that sends a new query every X seconds (**delay**) for a maximum of Y attempts (**max_attempts**).\n    \"\"\"\n\n    def __init__(self, delay, max_attempts):\n        self.delay = delay\n        self.max_attempts = max_attempts\n\n    class ConstantSpeculativeExecutionPlan(SpeculativeExecutionPlan):\n\n        def __init__(self, delay, max_attempts):\n            self.delay = delay\n            self.remaining = max_attempts\n\n        def next_execution(self, host):\n            if self.remaining > 0:\n                self.remaining -= 1\n                return self.delay\n            else:\n                return -1\n\n    def new_plan(self, keyspace, statement):\n        return self.ConstantSpeculativeExecutionPlan(self.delay, self.max_attempts)\n\nclass WrapperPolicy(LoadBalancingPolicy):\n\n    def __init__(self, child_policy):\n        self._child_policy = child_policy\n\n    def distance(self, *args, **kwargs):\n        return self._child_policy.distance(*args, **kwargs)\n\n    def populate(self, cluster, hosts):\n        self._child_policy.populate(cluster, hosts)\n\n    def on_up(self, *args, **kwargs):\n        return self._child_policy.on_up(*args, **kwargs)\n\n    def on_down(self, *args, **kwargs):\n        return self._child_policy.on_down(*args, **kwargs)\n\n    def on_add(self, *args, **kwargs):\n        return self._child_policy.on_add(*args, **kwargs)\n\n    def on_remove(self, *args, **kwargs):\n        return self._child_policy.on_remove(*args, **kwargs)\n\nclass DefaultLoadBalancingPolicy(WrapperPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` wrapper that adds the ability to target a specific host first.\n\n    If no host is set on the query, the child policy's query plan will be used as is.\n    \"\"\"\n    _cluster_metadata = None\n\n    def populate(self, cluster, hosts):\n        self._cluster_metadata = cluster.metadata\n        self._child_policy.populate(cluster, hosts)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        if query and query.keyspace:\n            keyspace = query.keyspace\n        else:\n            keyspace = working_keyspace\n        addr = getattr(query, 'target_host', None) if query else None\n        target_host = self._cluster_metadata.get_host(addr)\n        child = self._child_policy\n        if target_host and target_host.is_up:\n            yield target_host\n            for h in child.make_query_plan(keyspace, query):\n                if h != target_host:\n                    yield h\n        else:\n            for h in child.make_query_plan(keyspace, query):\n                yield h\n\nclass DSELoadBalancingPolicy(DefaultLoadBalancingPolicy):\n    \"\"\"\n    *Deprecated:* This will be removed in the next major release,\n    consider using :class:`.DefaultLoadBalancingPolicy`.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(DSELoadBalancingPolicy, self).__init__(*args, **kwargs)\n        warnings.warn('DSELoadBalancingPolicy will be removed in 4.0. Consider using DefaultLoadBalancingPolicy.', DeprecationWarning)\n\nclass NeverRetryPolicy(RetryPolicy):\n\n    def _rethrow(self, *args, **kwargs):\n        return (self.RETHROW, None)\n    on_read_timeout = _rethrow\n    on_write_timeout = _rethrow\n    on_unavailable = _rethrow\nColDesc = namedtuple('ColDesc', ['ks', 'table', 'col'])\n\nclass ColumnEncryptionPolicy(object):\n    \"\"\"\n    A policy enabling (mostly) transparent encryption and decryption of data before it is\n    sent to the cluster.\n\n    Key materials and other configurations are specified on a per-column basis.  This policy can\n    then be used by driver structures which are aware of the underlying columns involved in their\n    work.  In practice this includes the following cases:\n\n    * Prepared statements - data for columns specified by the cluster's policy will be transparently\n      encrypted before they are sent\n    * Rows returned from any query - data for columns specified by the cluster's policy will be\n      transparently decrypted before they are returned to the user\n\n    To enable this functionality, create an instance of this class (or more likely a subclass)\n    before creating a cluster.  This policy should then be configured and supplied to the Cluster\n    at creation time via the :attr:`.Cluster.column_encryption_policy` attribute.\n    \"\"\"\n\n    def encrypt(self, coldesc, obj_bytes):\n        \"\"\"\n        Encrypt the specified bytes using the cryptography materials for the specified column.\n        Largely used internally, although this could also be used to encrypt values supplied\n        to non-prepared statements in a way that is consistent with this policy.\n        \"\"\"\n        raise NotImplementedError()\n\n    def decrypt(self, coldesc, encrypted_bytes):\n        \"\"\"\n        Decrypt the specified (encrypted) bytes using the cryptography materials for the\n        specified column.  Used internally; could be used externally as well but there's\n        not currently an obvious use case.\n        \"\"\"\n        raise NotImplementedError()\n\n    def add_column(self, coldesc, key):\n        \"\"\"\n        Provide cryptography materials to be used when encrypted and/or decrypting data\n        for the specified column.\n        \"\"\"\n        raise NotImplementedError()\n\n    def contains_column(self, coldesc):\n        \"\"\"\n        Predicate to determine if a specific column is supported by this policy.\n        Currently only used internally.\n        \"\"\"\n        raise NotImplementedError()\n\n    def encode_and_encrypt(self, coldesc, obj):\n        \"\"\"\n        Helper function to enable use of this policy on simple (i.e. non-prepared)\n        statements.\n        \"\"\"\n        raise NotImplementedError()",
    "cassandra/__init__.py": "from enum import Enum\nimport logging\n\nclass NullHandler(logging.Handler):\n\n    def emit(self, record):\n        pass\nlogging.getLogger('cassandra').addHandler(NullHandler())\n__version_info__ = (3, 28, 0)\n__version__ = '.'.join(map(str, __version_info__))\n\nclass ConsistencyLevel(object):\n    \"\"\"\n    Spcifies how many replicas must respond for an operation to be considered\n    a success.  By default, ``ONE`` is used for all operations.\n    \"\"\"\n    ANY = 0\n    '\\n    Only requires that one replica receives the write *or* the coordinator\\n    stores a hint to replay later. Valid only for writes.\\n    '\n    ONE = 1\n    '\\n    Only one replica needs to respond to consider the operation a success\\n    '\n    TWO = 2\n    '\\n    Two replicas must respond to consider the operation a success\\n    '\n    THREE = 3\n    '\\n    Three replicas must respond to consider the operation a success\\n    '\n    QUORUM = 4\n    '\\n    ``ceil(RF/2) + 1`` replicas must respond to consider the operation a success\\n    '\n    ALL = 5\n    '\\n    All replicas must respond to consider the operation a success\\n    '\n    LOCAL_QUORUM = 6\n    '\\n    Requires a quorum of replicas in the local datacenter\\n    '\n    EACH_QUORUM = 7\n    '\\n    Requires a quorum of replicas in each datacenter\\n    '\n    SERIAL = 8\n    \"\\n    For conditional inserts/updates that utilize Cassandra's lightweight\\n    transactions, this requires consensus among all replicas for the\\n    modified data.\\n    \"\n    LOCAL_SERIAL = 9\n    '\\n    Like :attr:`~ConsistencyLevel.SERIAL`, but only requires consensus\\n    among replicas in the local datacenter.\\n    '\n    LOCAL_ONE = 10\n    '\\n    Sends a request only to replicas in the local datacenter and waits for\\n    one response.\\n    '\n\n    @staticmethod\n    def is_serial(cl):\n        return cl == ConsistencyLevel.SERIAL or cl == ConsistencyLevel.LOCAL_SERIAL\nConsistencyLevel.value_to_name = {ConsistencyLevel.ANY: 'ANY', ConsistencyLevel.ONE: 'ONE', ConsistencyLevel.TWO: 'TWO', ConsistencyLevel.THREE: 'THREE', ConsistencyLevel.QUORUM: 'QUORUM', ConsistencyLevel.ALL: 'ALL', ConsistencyLevel.LOCAL_QUORUM: 'LOCAL_QUORUM', ConsistencyLevel.EACH_QUORUM: 'EACH_QUORUM', ConsistencyLevel.SERIAL: 'SERIAL', ConsistencyLevel.LOCAL_SERIAL: 'LOCAL_SERIAL', ConsistencyLevel.LOCAL_ONE: 'LOCAL_ONE'}\nConsistencyLevel.name_to_value = {'ANY': ConsistencyLevel.ANY, 'ONE': ConsistencyLevel.ONE, 'TWO': ConsistencyLevel.TWO, 'THREE': ConsistencyLevel.THREE, 'QUORUM': ConsistencyLevel.QUORUM, 'ALL': ConsistencyLevel.ALL, 'LOCAL_QUORUM': ConsistencyLevel.LOCAL_QUORUM, 'EACH_QUORUM': ConsistencyLevel.EACH_QUORUM, 'SERIAL': ConsistencyLevel.SERIAL, 'LOCAL_SERIAL': ConsistencyLevel.LOCAL_SERIAL, 'LOCAL_ONE': ConsistencyLevel.LOCAL_ONE}\n\ndef consistency_value_to_name(value):\n    return ConsistencyLevel.value_to_name[value] if value is not None else 'Not Set'\n\nclass ProtocolVersion(object):\n    \"\"\"\n    Defines native protocol versions supported by this driver.\n    \"\"\"\n    V1 = 1\n    '\\n    v1, supported in Cassandra 1.2-->2.2\\n    '\n    V2 = 2\n    '\\n    v2, supported in Cassandra 2.0-->2.2;\\n    added support for lightweight transactions, batch operations, and automatic query paging.\\n    '\n    V3 = 3\n    '\\n    v3, supported in Cassandra 2.1-->3.x+;\\n    added support for protocol-level client-side timestamps (see :attr:`.Session.use_client_timestamp`),\\n    serial consistency levels for :class:`~.BatchStatement`, and an improved connection pool.\\n    '\n    V4 = 4\n    '\\n    v4, supported in Cassandra 2.2-->3.x+;\\n    added a number of new types, server warnings, new failure messages, and custom payloads. Details in the\\n    `project docs <https://github.com/apache/cassandra/blob/trunk/doc/native_protocol_v4.spec>`_\\n    '\n    V5 = 5\n    '\\n    v5, in beta from 3.x+. Finalised in 4.0-beta5\\n    '\n    V6 = 6\n    '\\n    v6, in beta from 4.0-beta5\\n    '\n    DSE_V1 = 65\n    '\\n    DSE private protocol v1, supported in DSE 5.1+\\n    '\n    DSE_V2 = 66\n    '\\n    DSE private protocol v2, supported in DSE 6.0+\\n    '\n    SUPPORTED_VERSIONS = (DSE_V2, DSE_V1, V6, V5, V4, V3, V2, V1)\n    '\\n    A tuple of all supported protocol versions\\n    '\n    BETA_VERSIONS = (V6,)\n    '\\n    A tuple of all beta protocol versions\\n    '\n    MIN_SUPPORTED = min(SUPPORTED_VERSIONS)\n    '\\n    Minimum protocol version supported by this driver.\\n    '\n    MAX_SUPPORTED = max(SUPPORTED_VERSIONS)\n    '\\n    Maximum protocol version supported by this driver.\\n    '\n\n    @classmethod\n    def get_lower_supported(cls, previous_version):\n        \"\"\"\n        Return the lower supported protocol version. Beta versions are omitted.\n        \"\"\"\n        try:\n            version = next((v for v in sorted(ProtocolVersion.SUPPORTED_VERSIONS, reverse=True) if v not in ProtocolVersion.BETA_VERSIONS and v < previous_version))\n        except StopIteration:\n            version = 0\n        return version\n\n    @classmethod\n    def uses_int_query_flags(cls, version):\n        return version >= cls.V5\n\n    @classmethod\n    def uses_prepare_flags(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def uses_prepared_metadata(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def uses_error_code_map(cls, version):\n        return version >= cls.V5\n\n    @classmethod\n    def uses_keyspace_flag(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def has_continuous_paging_support(cls, version):\n        return version >= cls.DSE_V1\n\n    @classmethod\n    def has_continuous_paging_next_pages(cls, version):\n        return version >= cls.DSE_V2\n\n    @classmethod\n    def has_checksumming_support(cls, version):\n        return cls.V5 <= version < cls.DSE_V1\n\nclass WriteType(object):\n    \"\"\"\n    For usage with :class:`.RetryPolicy`, this describe a type\n    of write operation.\n    \"\"\"\n    SIMPLE = 0\n    '\\n    A write to a single partition key. Such writes are guaranteed to be atomic\\n    and isolated.\\n    '\n    BATCH = 1\n    '\\n    A write to multiple partition keys that used the distributed batch log to\\n    ensure atomicity.\\n    '\n    UNLOGGED_BATCH = 2\n    '\\n    A write to multiple partition keys that did not use the distributed batch\\n    log. Atomicity for such writes is not guaranteed.\\n    '\n    COUNTER = 3\n    '\\n    A counter write (for one or multiple partition keys). Such writes should\\n    not be replayed in order to avoid overcount.\\n    '\n    BATCH_LOG = 4\n    '\\n    The initial write to the distributed batch log that Cassandra performs\\n    internally before a BATCH write.\\n    '\n    CAS = 5\n    '\\n    A lighweight-transaction write, such as \"DELETE ... IF EXISTS\".\\n    '\n    VIEW = 6\n    '\\n    This WriteType is only seen in results for requests that were unable to\\n    complete MV operations.\\n    '\n    CDC = 7\n    '\\n    This WriteType is only seen in results for requests that were unable to\\n    complete CDC operations.\\n    '\nWriteType.name_to_value = {'SIMPLE': WriteType.SIMPLE, 'BATCH': WriteType.BATCH, 'UNLOGGED_BATCH': WriteType.UNLOGGED_BATCH, 'COUNTER': WriteType.COUNTER, 'BATCH_LOG': WriteType.BATCH_LOG, 'CAS': WriteType.CAS, 'VIEW': WriteType.VIEW, 'CDC': WriteType.CDC}\nWriteType.value_to_name = {v: k for k, v in WriteType.name_to_value.items()}\n\nclass SchemaChangeType(object):\n    DROPPED = 'DROPPED'\n    CREATED = 'CREATED'\n    UPDATED = 'UPDATED'\n\nclass SchemaTargetType(object):\n    KEYSPACE = 'KEYSPACE'\n    TABLE = 'TABLE'\n    TYPE = 'TYPE'\n    FUNCTION = 'FUNCTION'\n    AGGREGATE = 'AGGREGATE'\n\nclass SignatureDescriptor(object):\n\n    def __init__(self, name, argument_types):\n        self.name = name\n        self.argument_types = argument_types\n\n    @property\n    def signature(self):\n        \"\"\"\n        function signature string in the form 'name([type0[,type1[...]]])'\n\n        can be used to uniquely identify overloaded function names within a keyspace\n        \"\"\"\n        return self.format_signature(self.name, self.argument_types)\n\n    @staticmethod\n    def format_signature(name, argument_types):\n        return '%s(%s)' % (name, ','.join((t for t in argument_types)))\n\n    def __repr__(self):\n        return '%s(%s, %s)' % (self.__class__.__name__, self.name, self.argument_types)\n\nclass UserFunctionDescriptor(SignatureDescriptor):\n    \"\"\"\n    Describes a User function by name and argument signature\n    \"\"\"\n    name = None\n    '\\n    name of the function\\n    '\n    argument_types = None\n    '\\n    Ordered list of CQL argument type names comprising the type signature\\n    '\n\nclass UserAggregateDescriptor(SignatureDescriptor):\n    \"\"\"\n    Describes a User aggregate function by name and argument signature\n    \"\"\"\n    name = None\n    '\\n    name of the aggregate\\n    '\n    argument_types = None\n    '\\n    Ordered list of CQL argument type names comprising the type signature\\n    '\n\nclass DriverException(Exception):\n    \"\"\"\n    Base for all exceptions explicitly raised by the driver.\n    \"\"\"\n    pass\n\nclass RequestExecutionException(DriverException):\n    \"\"\"\n    Base for request execution exceptions returned from the server.\n    \"\"\"\n    pass\n\nclass Unavailable(RequestExecutionException):\n    \"\"\"\n    There were not enough live replicas to satisfy the requested consistency\n    level, so the coordinator node immediately failed the request without\n    forwarding it to any replicas.\n    \"\"\"\n    consistency = None\n    ' The requested :class:`ConsistencyLevel` '\n    required_replicas = None\n    ' The number of replicas that needed to be live to complete the operation '\n    alive_replicas = None\n    ' The number of replicas that were actually alive '\n\n    def __init__(self, summary_message, consistency=None, required_replicas=None, alive_replicas=None):\n        self.consistency = consistency\n        self.required_replicas = required_replicas\n        self.alive_replicas = alive_replicas\n        Exception.__init__(self, summary_message + ' info=' + repr({'consistency': consistency_value_to_name(consistency), 'required_replicas': required_replicas, 'alive_replicas': alive_replicas}))\n\nclass Timeout(RequestExecutionException):\n    \"\"\"\n    Replicas failed to respond to the coordinator node before timing out.\n    \"\"\"\n    consistency = None\n    ' The requested :class:`ConsistencyLevel` '\n    required_responses = None\n    ' The number of required replica responses '\n    received_responses = None\n    '\\n    The number of replicas that responded before the coordinator timed out\\n    the operation\\n    '\n\n    def __init__(self, summary_message, consistency=None, required_responses=None, received_responses=None, **kwargs):\n        self.consistency = consistency\n        self.required_responses = required_responses\n        self.received_responses = received_responses\n        if 'write_type' in kwargs:\n            kwargs['write_type'] = WriteType.value_to_name[kwargs['write_type']]\n        info = {'consistency': consistency_value_to_name(consistency), 'required_responses': required_responses, 'received_responses': received_responses}\n        info.update(kwargs)\n        Exception.__init__(self, summary_message + ' info=' + repr(info))\n\nclass ReadTimeout(Timeout):\n    \"\"\"\n    A subclass of :exc:`Timeout` for read operations.\n\n    This indicates that the replicas failed to respond to the coordinator\n    node before the configured timeout. This timeout is configured in\n    ``cassandra.yaml`` with the ``read_request_timeout_in_ms``\n    and ``range_request_timeout_in_ms`` options.\n    \"\"\"\n    data_retrieved = None\n    '\\n    A boolean indicating whether the requested data was retrieved\\n    by the coordinator from any replicas before it timed out the\\n    operation\\n    '\n\n    def __init__(self, message, data_retrieved=None, **kwargs):\n        Timeout.__init__(self, message, **kwargs)\n        self.data_retrieved = data_retrieved\n\nclass WriteTimeout(Timeout):\n    \"\"\"\n    A subclass of :exc:`Timeout` for write operations.\n\n    This indicates that the replicas failed to respond to the coordinator\n    node before the configured timeout. This timeout is configured in\n    ``cassandra.yaml`` with the ``write_request_timeout_in_ms``\n    option.\n    \"\"\"\n    write_type = None\n    '\\n    The type of write operation, enum on :class:`~cassandra.policies.WriteType`\\n    '\n\n    def __init__(self, message, write_type=None, **kwargs):\n        kwargs['write_type'] = write_type\n        Timeout.__init__(self, message, **kwargs)\n        self.write_type = write_type\n\nclass CDCWriteFailure(RequestExecutionException):\n    \"\"\"\n    Hit limit on data in CDC folder, writes are rejected\n    \"\"\"\n\n    def __init__(self, message):\n        Exception.__init__(self, message)\n\nclass CoordinationFailure(RequestExecutionException):\n    \"\"\"\n    Replicas sent a failure to the coordinator.\n    \"\"\"\n    consistency = None\n    ' The requested :class:`ConsistencyLevel` '\n    required_responses = None\n    ' The number of required replica responses '\n    received_responses = None\n    '\\n    The number of replicas that responded before the coordinator timed out\\n    the operation\\n    '\n    failures = None\n    '\\n    The number of replicas that sent a failure message\\n    '\n    error_code_map = None\n    '\\n    A map of inet addresses to error codes representing replicas that sent\\n    a failure message.  Only set when `protocol_version` is 5 or higher.\\n    '\n\n    def __init__(self, summary_message, consistency=None, required_responses=None, received_responses=None, failures=None, error_code_map=None):\n        self.consistency = consistency\n        self.required_responses = required_responses\n        self.received_responses = received_responses\n        self.failures = failures\n        self.error_code_map = error_code_map\n        info_dict = {'consistency': consistency_value_to_name(consistency), 'required_responses': required_responses, 'received_responses': received_responses, 'failures': failures}\n        if error_code_map is not None:\n            formatted_map = dict(((addr, '0x%04x' % err_code) for addr, err_code in error_code_map.items()))\n            info_dict['error_code_map'] = formatted_map\n        Exception.__init__(self, summary_message + ' info=' + repr(info_dict))\n\nclass ReadFailure(CoordinationFailure):\n    \"\"\"\n    A subclass of :exc:`CoordinationFailure` for read operations.\n\n    This indicates that the replicas sent a failure message to the coordinator.\n    \"\"\"\n    data_retrieved = None\n    '\\n    A boolean indicating whether the requested data was retrieved\\n    by the coordinator from any replicas before it timed out the\\n    operation\\n    '\n\n    def __init__(self, message, data_retrieved=None, **kwargs):\n        CoordinationFailure.__init__(self, message, **kwargs)\n        self.data_retrieved = data_retrieved\n\nclass WriteFailure(CoordinationFailure):\n    \"\"\"\n    A subclass of :exc:`CoordinationFailure` for write operations.\n\n    This indicates that the replicas sent a failure message to the coordinator.\n    \"\"\"\n    write_type = None\n    '\\n    The type of write operation, enum on :class:`~cassandra.policies.WriteType`\\n    '\n\n    def __init__(self, message, write_type=None, **kwargs):\n        CoordinationFailure.__init__(self, message, **kwargs)\n        self.write_type = write_type\n\nclass FunctionFailure(RequestExecutionException):\n    \"\"\"\n    User Defined Function failed during execution\n    \"\"\"\n    keyspace = None\n    '\\n    Keyspace of the function\\n    '\n    function = None\n    '\\n    Name of the function\\n    '\n    arg_types = None\n    '\\n    List of argument type names of the function\\n    '\n\n    def __init__(self, summary_message, keyspace, function, arg_types):\n        self.keyspace = keyspace\n        self.function = function\n        self.arg_types = arg_types\n        Exception.__init__(self, summary_message)\n\nclass RequestValidationException(DriverException):\n    \"\"\"\n    Server request validation failed\n    \"\"\"\n    pass\n\nclass ConfigurationException(RequestValidationException):\n    \"\"\"\n    Server indicated request errro due to current configuration\n    \"\"\"\n    pass\n\nclass AlreadyExists(ConfigurationException):\n    \"\"\"\n    An attempt was made to create a keyspace or table that already exists.\n    \"\"\"\n    keyspace = None\n    '\\n    The name of the keyspace that already exists, or, if an attempt was\\n    made to create a new table, the keyspace that the table is in.\\n    '\n    table = None\n    '\\n    The name of the table that already exists, or, if an attempt was\\n    make to create a keyspace, :const:`None`.\\n    '\n\n    def __init__(self, keyspace=None, table=None):\n        if table:\n            message = \"Table '%s.%s' already exists\" % (keyspace, table)\n        else:\n            message = \"Keyspace '%s' already exists\" % (keyspace,)\n        Exception.__init__(self, message)\n        self.keyspace = keyspace\n        self.table = table\n\nclass InvalidRequest(RequestValidationException):\n    \"\"\"\n    A query was made that was invalid for some reason, such as trying to set\n    the keyspace for a connection to a nonexistent keyspace.\n    \"\"\"\n    pass\n\nclass Unauthorized(RequestValidationException):\n    \"\"\"\n    The current user is not authorized to perform the requested operation.\n    \"\"\"\n    pass\n\nclass AuthenticationFailed(DriverException):\n    \"\"\"\n    Failed to authenticate.\n    \"\"\"\n    pass\n\nclass OperationTimedOut(DriverException):\n    \"\"\"\n    The operation took longer than the specified (client-side) timeout\n    to complete.  This is not an error generated by Cassandra, only\n    the driver.\n    \"\"\"\n    errors = None\n    '\\n    A dict of errors keyed by the :class:`~.Host` against which they occurred.\\n    '\n    last_host = None\n    '\\n    The last :class:`~.Host` this operation was attempted against.\\n    '\n\n    def __init__(self, errors=None, last_host=None):\n        self.errors = errors\n        self.last_host = last_host\n        message = 'errors=%s, last_host=%s' % (self.errors, self.last_host)\n        Exception.__init__(self, message)\n\nclass UnsupportedOperation(DriverException):\n    \"\"\"\n    An attempt was made to use a feature that is not supported by the\n    selected protocol version.  See :attr:`Cluster.protocol_version`\n    for more details.\n    \"\"\"\n    pass\n\nclass UnresolvableContactPoints(DriverException):\n    \"\"\"\n    The driver was unable to resolve any provided hostnames.\n\n    Note that this is *not* raised when a :class:`.Cluster` is created with no\n    contact points, only when lookup fails for all hosts\n    \"\"\"\n    pass\n\nclass OperationType(Enum):\n    Read = 0\n    Write = 1\n\nclass RateLimitReached(ConfigurationException):\n    \"\"\"\n    Rate limit was exceeded for a partition affected by the request.\n    \"\"\"\n    op_type = None\n    rejected_by_coordinator = False\n\n    def __init__(self, op_type=None, rejected_by_coordinator=False):\n        self.op_type = op_type\n        self.rejected_by_coordinator = rejected_by_coordinator\n        message = f'[request_error_rate_limit_reached OpType={op_type.name} RejectedByCoordinator={rejected_by_coordinator}]'\n        Exception.__init__(self, message)"
  }
}