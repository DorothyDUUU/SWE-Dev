{
  "dir_path": "/app/skforecast",
  "package_name": "skforecast",
  "sample_name": "skforecast-test_binning_in_sample_residuals",
  "src_dir": "skforecast/",
  "test_dir": "tests/",
  "test_file": "skforecast/recursive/tests/tests_forecaster_recursive/test_binning_in_sample_residuals.py",
  "test_code": "# Unit test _binning_in_sample_residuals ForecasterRecursive\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom skforecast.recursive import ForecasterRecursive\n\n\ndef test_binning_in_sample_residuals_output():\n    \"\"\"\n    Test that _binning_in_sample_residuals returns the expected output.\n    \"\"\"\n\n    forecaster = ForecasterRecursive(\n        regressor=object(),\n        lags = 5,\n        binner_kwargs={'n_bins': 3}\n    )\n\n    rng = np.random.default_rng(12345)\n    y_pred = rng.normal(100, 15, 20)\n    y_true = rng.normal(100, 10, 20)\n\n    forecaster._binning_in_sample_residuals(\n        y_pred=y_pred,\n        y_true=y_true,\n    )\n\n    expected_1 = np.array([\n                     34.58035615,  22.08911948,  15.6081091 ,   7.0808798 ,\n                     55.47454021,   1.80092458,  12.2624188 , -12.32822882,\n                     -0.451743  ,  11.83152763,  -7.11862253,   6.32581108,\n                     -20.92461257, -21.95291202, -10.55026794, -27.43753138,\n                     -6.24112163, -25.62685698,  -4.31327121, -18.40910724\n                 ])\n\n    expected_2 = {\n        0: np.array([\n               34.58035615, 22.08911948, 15.6081091,  7.0808798, 55.47454021,\n               1.80092458, 12.2624188\n           ]),\n        1: np.array([\n               -12.32822882,  -0.45174300,  11.83152763,  -7.11862253,\n               6.32581108, -20.92461257\n           ]),\n        2: np.array([\n               -21.95291202, -10.55026794, -27.43753138,  -6.24112163,\n               -25.62685698,  -4.31327121, -18.40910724\n           ])\n    }\n\n    expected_3 = {\n        0: (70.70705405481715, 90.25638761254116),\n        1: (90.25638761254116, 109.36821559391004),\n        2: (109.36821559391004, 135.2111448156828)\n    }\n\n    np.testing.assert_almost_equal(forecaster.in_sample_residuals_, expected_1)\n    for k in expected_2.keys():\n        np.testing.assert_almost_equal(forecaster.in_sample_residuals_by_bin_[k], expected_2[k])\n    assert forecaster.binner_intervals_ == expected_3\n\n\ndef test_binning_in_sample_residuals_stores_maximum_10000_residuals():\n    \"\"\"\n    Test that maximum 10000 residuals are stored.\n    \"\"\"\n\n    n = 15000\n    y = pd.Series(\n            data = np.random.normal(loc=10, scale=1, size=n),\n            index = pd.date_range(start='01-01-2000', periods=n, freq='h')\n        )\n    forecaster = ForecasterRecursive(\n                    regressor=LinearRegression(),\n                    lags = 5,\n                    binner_kwargs={'n_bins': 2}\n                )\n    forecaster.fit(y)\n    max_residuals_per_bin = int(10_000 // forecaster.binner.n_bins_)\n\n    for v in forecaster.in_sample_residuals_by_bin_.values():\n        assert len(v) == max_residuals_per_bin\n        assert len(v) > 0\n\n    np.testing.assert_array_almost_equal(\n        forecaster.in_sample_residuals_,\n        np.concatenate(list(forecaster.in_sample_residuals_by_bin_.values()))\n    )\n\n    assert len(forecaster.in_sample_residuals_) == 10_000\n",
  "GT_file_code": {
    "skforecast/utils/utils.py": "################################################################################\n#                               skforecast.utils                               #\n#                                                                              #\n# This work by skforecast team is licensed under the BSD 3-Clause License.     #\n################################################################################\n# coding=utf-8\n\nimport importlib\nimport inspect\nimport warnings\nfrom copy import deepcopy\nfrom typing import Any, Callable, Optional, Tuple, Union\nfrom pathlib import Path\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport sklearn.linear_model\nfrom sklearn.base import clone\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.exceptions import NotFittedError\nimport skforecast\nfrom ..exceptions import warn_skforecast_categories\nfrom ..exceptions import (\n    MissingValuesWarning,\n    MissingExogWarning,\n    DataTypeWarning,\n    UnknownLevelWarning,\n    IgnoredArgumentWarning,\n    SaveLoadSkforecastWarning,\n    SkforecastVersionWarning\n)\n\noptional_dependencies = {\n    'sarimax': [\n        'statsmodels>=0.12, <0.15'\n    ],\n    'deeplearning': [\n        'matplotlib>=3.3, <3.10',\n        'keras>=2.6, <4.0',\n    ],\n    'plotting': [\n        'matplotlib>=3.3, <3.10', \n        'seaborn>=0.11, <0.14', \n        'statsmodels>=0.12, <0.15'\n    ]\n}\n\n\ndef initialize_lags(\n    forecaster_name: str,\n    lags: Any\n) -> Union[Optional[np.ndarray], Optional[list], Optional[int]]:\n    \"\"\"\n    Check lags argument input and generate the corresponding numpy ndarray.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    lags : Any\n        Lags used as predictors.\n\n    Returns\n    -------\n    lags : numpy ndarray, None\n        Lags used as predictors.\n    lags_names : list, None\n        Names of the lags used as predictors.\n    max_lag : int, None\n        Maximum value of the lags.\n    \n    \"\"\"\n\n    lags_names = None\n    max_lag = None\n    if lags is not None:\n        if isinstance(lags, int):\n            if lags < 1:\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n            lags = np.arange(1, lags + 1)\n\n        if isinstance(lags, (list, tuple, range)):\n            lags = np.array(lags)\n        \n        if isinstance(lags, np.ndarray):\n            if lags.size == 0:\n                return None, None, None\n            if lags.ndim != 1:\n                raise ValueError(\"`lags` must be a 1-dimensional array.\")\n            if not np.issubdtype(lags.dtype, np.integer):\n                raise TypeError(\"All values in `lags` must be integers.\")\n            if np.any(lags < 1):\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n        else:\n            if forecaster_name != 'ForecasterDirectMultiVariate':\n                raise TypeError(\n                    (f\"`lags` argument must be an int, 1d numpy ndarray, range, \"\n                     f\"tuple or list. Got {type(lags)}.\")\n                )\n            else:\n                raise TypeError(\n                    (f\"`lags` argument must be a dict, int, 1d numpy ndarray, range, \"\n                     f\"tuple or list. Got {type(lags)}.\")\n                )\n        \n        lags_names = [f'lag_{i}' for i in lags]\n        max_lag = max(lags)\n\n    return lags, lags_names, max_lag\n\n\ndef initialize_window_features(\n    window_features: Any\n) -> Union[Optional[list], Optional[list], Optional[int]]:\n    \"\"\"\n    Check window_features argument input and generate the corresponding list.\n\n    Parameters\n    ----------\n    window_features : Any\n        Classes used to create window features.\n\n    Returns\n    -------\n    window_features : list, None\n        List of classes used to create window features.\n    window_features_names : list, None\n        List with all the features names of the window features.\n    max_size_window_features : int, None\n        Maximum value of the `window_sizes` attribute of all classes.\n    \n    \"\"\"\n\n    needed_atts = ['window_sizes', 'features_names']\n    needed_methods = ['transform_batch', 'transform']\n\n    max_window_sizes = None\n    window_features_names = None\n    max_size_window_features = None\n    if window_features is not None:\n        if isinstance(window_features, list) and len(window_features) < 1:\n            raise ValueError(\n                \"Argument `window_features` must contain at least one element.\"\n            )\n        if not isinstance(window_features, list):\n            window_features = [window_features]\n\n        link_to_docs = (\n            \"\\nVisit the documentation for more information about how to create \"\n            \"custom window features:\\n\"\n            \"https://skforecast.org/latest/user_guides/window-features-and-custom-features.html#create-your-custom-window-features\"\n        )\n        \n        max_window_sizes = []\n        window_features_names = []\n        for wf in window_features:\n            wf_name = type(wf).__name__\n            atts_methods = set([a for a in dir(wf)])\n            if not set(needed_atts).issubset(atts_methods):\n                raise ValueError(\n                    f\"{wf_name} must have the attributes: {needed_atts}.\" + link_to_docs\n                )\n            if not set(needed_methods).issubset(atts_methods):\n                raise ValueError(\n                    f\"{wf_name} must have the methods: {needed_methods}.\" + link_to_docs\n                )\n            \n            window_sizes = wf.window_sizes\n            if not isinstance(window_sizes, (int, list)):\n                raise TypeError(\n                    f\"Attribute `window_sizes` of {wf_name} must be an int or a list \"\n                    f\"of ints. Got {type(window_sizes)}.\" + link_to_docs\n                )\n            \n            if isinstance(window_sizes, int):\n                if window_sizes < 1:\n                    raise ValueError(\n                        f\"If argument `window_sizes` is an integer, it must be equal to or \"\n                        f\"greater than 1. Got {window_sizes} from {wf_name}.\" + link_to_docs\n                    )\n                max_window_sizes.append(window_sizes)\n            else:\n                if not all(isinstance(ws, int) for ws in window_sizes) or not all(\n                    ws >= 1 for ws in window_sizes\n                ):                    \n                    raise ValueError(\n                        f\"If argument `window_sizes` is a list, all elements must be integers \"\n                        f\"equal to or greater than 1. Got {window_sizes} from {wf_name}.\" + link_to_docs\n                    )\n                max_window_sizes.append(max(window_sizes))\n\n            features_names = wf.features_names\n            if not isinstance(features_names, (str, list)):\n                raise TypeError(\n                    f\"Attribute `features_names` of {wf_name} must be a str or \"\n                    f\"a list of strings. Got {type(features_names)}.\" + link_to_docs\n                )\n            if isinstance(features_names, str):\n                window_features_names.append(features_names)\n            else:\n                if not all(isinstance(fn, str) for fn in features_names):\n                    raise TypeError(\n                        f\"If argument `features_names` is a list, all elements \"\n                        f\"must be strings. Got {features_names} from {wf_name}.\" + link_to_docs\n                    )\n                window_features_names.extend(features_names)\n\n        max_size_window_features = max(max_window_sizes)\n        if len(set(window_features_names)) != len(window_features_names):\n            raise ValueError(\n                f\"All window features names must be unique. Got {window_features_names}.\"\n            )\n\n    return window_features, window_features_names, max_size_window_features\n\n\ndef initialize_weights(\n    forecaster_name: str,\n    regressor: object,\n    weight_func: Union[Callable, dict],\n    series_weights: dict\n) -> Tuple[Union[Callable, dict], Union[str, dict], dict]:\n    \"\"\"\n    Check weights arguments, `weight_func` and `series_weights` for the different \n    forecasters. Create `source_code_weight_func`, source code of the custom \n    function(s) used to create weights.\n    \n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        Regressor of the forecaster.\n    weight_func : Callable, dict\n        Argument `weight_func` of the forecaster.\n    series_weights : dict\n        Argument `series_weights` of the forecaster.\n\n    Returns\n    -------\n    weight_func : Callable, dict\n        Argument `weight_func` of the forecaster.\n    source_code_weight_func : str, dict\n        Argument `source_code_weight_func` of the forecaster.\n    series_weights : dict\n        Argument `series_weights` of the forecaster.\n    \n    \"\"\"\n\n    source_code_weight_func = None\n\n    if weight_func is not None:\n\n        if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n            if not isinstance(weight_func, (Callable, dict)):\n                raise TypeError(\n                    (f\"Argument `weight_func` must be a Callable or a dict of \"\n                     f\"Callables. Got {type(weight_func)}.\")\n                )\n        elif not isinstance(weight_func, Callable):\n            raise TypeError(\n                f\"Argument `weight_func` must be a Callable. Got {type(weight_func)}.\"\n            )\n        \n        if isinstance(weight_func, dict):\n            source_code_weight_func = {}\n            for key in weight_func:\n                source_code_weight_func[key] = inspect.getsource(weight_func[key])\n        else:\n            source_code_weight_func = inspect.getsource(weight_func)\n\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(\n                (f\"Argument `weight_func` is ignored since regressor {regressor} \"\n                 f\"does not accept `sample_weight` in its `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n            weight_func = None\n            source_code_weight_func = None\n\n    if series_weights is not None:\n        if not isinstance(series_weights, dict):\n            raise TypeError(\n                (f\"Argument `series_weights` must be a dict of floats or ints.\"\n                 f\"Got {type(series_weights)}.\")\n            )\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(\n                (f\"Argument `series_weights` is ignored since regressor {regressor} \"\n                 f\"does not accept `sample_weight` in its `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n            series_weights = None\n\n    return weight_func, source_code_weight_func, series_weights\n\n\ndef initialize_transformer_series(\n    forecaster_name: str,\n    series_names_in_: list,\n    encoding: Optional[str] = None,\n    transformer_series: Optional[Union[object, dict]] = None\n) -> dict:\n    \"\"\"\n    Initialize `transformer_series_` attribute for the Forecasters Multiseries.\n\n    - If `transformer_series` is `None`, no transformation is applied.\n    - If `transformer_series` is a scikit-learn transformer (object), the same \n    transformer is applied to all series (`series_names_in_`).\n    - If `transformer_series` is a `dict`, a different transformer can be\n    applied to each series. The keys of the dictionary must be the same as the\n    names of the series in `series_names_in_`.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    series_names_in_ : list\n        Names of the series (levels) used during training.\n    encoding : str, default `None`\n        Encoding used to identify the different series (`ForecasterRecursiveMultiSeries`).\n    transformer_series : object, dict, default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and \n        inverse_transform. \n\n    Returns\n    -------\n    transformer_series_ : dict\n        Dictionary with the transformer for each series. It is created cloning the \n        objects in `transformer_series` and is used internally to avoid overwriting.\n    \n    \"\"\"\n\n    multiseries_forecasters = [\n        'ForecasterRecursiveMultiSeries'\n    ]\n\n    if forecaster_name in multiseries_forecasters:\n        if encoding is None:\n            series_names_in_ = ['_unknown_level']\n        else:\n            series_names_in_ = series_names_in_ + ['_unknown_level']\n\n    if transformer_series is None:\n        transformer_series_ = {serie: None for serie in series_names_in_}\n    elif not isinstance(transformer_series, dict):\n        transformer_series_ = {serie: clone(transformer_series) \n                               for serie in series_names_in_}\n    else:\n        transformer_series_ = {serie: None for serie in series_names_in_}\n        # Only elements already present in transformer_series_ are updated\n        transformer_series_.update(\n            (k, v) for k, v in deepcopy(transformer_series).items() \n            if k in transformer_series_\n        )\n\n        series_not_in_transformer_series = (\n            set(series_names_in_) - set(transformer_series.keys())\n        ) - {'_unknown_level'}\n        if series_not_in_transformer_series:\n            warnings.warn(\n                (f\"{series_not_in_transformer_series} not present in `transformer_series`.\"\n                f\" No transformation is applied to these series.\"),\n                IgnoredArgumentWarning\n            )\n\n    return transformer_series_\n\n\ndef check_select_fit_kwargs(\n    regressor: object,\n    fit_kwargs: Optional[dict] = None\n) -> dict:\n    \"\"\"\n    Check if `fit_kwargs` is a dict and select only the keys that are used by\n    the `fit` method of the regressor.\n\n    Parameters\n    ----------\n    regressor : object\n        Regressor object.\n    fit_kwargs : dict, default `None`\n        Dictionary with the arguments to pass to the `fit' method of the forecaster.\n\n    Returns\n    -------\n    fit_kwargs : dict\n        Dictionary with the arguments to be passed to the `fit` method of the \n        regressor after removing the unused keys.\n    \n    \"\"\"\n\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    else:\n        if not isinstance(fit_kwargs, dict):\n            raise TypeError(\n                f\"Argument `fit_kwargs` must be a dict. Got {type(fit_kwargs)}.\"\n            )\n\n        # Non used keys\n        non_used_keys = [k for k in fit_kwargs.keys()\n                         if k not in inspect.signature(regressor.fit).parameters]\n        if non_used_keys:\n            warnings.warn(\n                (f\"Argument/s {non_used_keys} ignored since they are not used by the \"\n                 f\"regressor's `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n\n        if 'sample_weight' in fit_kwargs.keys():\n            warnings.warn(\n                (\"The `sample_weight` argument is ignored. Use `weight_func` to pass \"\n                 \"a function that defines the individual weights for each sample \"\n                 \"based on its index.\"),\n                 IgnoredArgumentWarning\n            )\n            del fit_kwargs['sample_weight']\n\n        # Select only the keyword arguments allowed by the regressor's `fit` method.\n        fit_kwargs = {k: v for k, v in fit_kwargs.items()\n                      if k in inspect.signature(regressor.fit).parameters}\n\n    return fit_kwargs\n\n\ndef check_y(\n    y: Any,\n    series_id: str = \"`y`\"\n) -> None:\n    \"\"\"\n    Raise Exception if `y` is not pandas Series or if it has missing values.\n    \n    Parameters\n    ----------\n    y : Any\n        Time series values.\n    series_id : str, default '`y`'\n        Identifier of the series used in the warning message.\n    \n    Returns\n    -------\n    None\n    \n    \"\"\"\n    \n    if not isinstance(y, pd.Series):\n        raise TypeError(f\"{series_id} must be a pandas Series.\")\n        \n    if y.isnull().any():\n        raise ValueError(f\"{series_id} has missing values.\")\n    \n    return\n\n\ndef check_exog(\n    exog: Union[pd.Series, pd.DataFrame],\n    allow_nan: bool = True,\n    series_id: str = \"`exog`\"\n) -> None:\n    \"\"\"\n    Raise Exception if `exog` is not pandas Series or pandas DataFrame.\n    If `allow_nan = True`, issue a warning if `exog` contains NaN values.\n    \n    Parameters\n    ----------\n    exog : pandas DataFrame, pandas Series\n        Exogenous variable/s included as predictor/s.\n    allow_nan : bool, default `True`\n        If True, allows the presence of NaN values in `exog`. If False (default),\n        issue a warning if `exog` contains NaN values.\n    series_id : str, default '`exog`'\n        Identifier of the series for which the exogenous variable/s are used\n        in the warning message.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    \n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(\n            f\"{series_id} must be a pandas Series or DataFrame. Got {type(exog)}.\"\n        )\n    \n    if isinstance(exog, pd.Series) and exog.name is None:\n        raise ValueError(f\"When {series_id} is a pandas Series, it must have a name.\")\n\n    if not allow_nan:\n        if exog.isnull().any().any():\n            warnings.warn(\n                (f\"{series_id} has missing values. Most machine learning models \"\n                 f\"do not allow missing values. Fitting the forecaster may fail.\"), \n                 MissingValuesWarning\n            )\n    \n    return\n\n\ndef get_exog_dtypes(\n    exog: Union[pd.DataFrame, pd.Series]\n) -> dict:\n    \"\"\"\n    Store dtypes of `exog`.\n\n    Parameters\n    ----------\n    exog : pandas DataFrame, pandas Series\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    exog_dtypes : dict\n        Dictionary with the dtypes in `exog`.\n    \n    \"\"\"\n\n    if isinstance(exog, pd.Series):\n        exog_dtypes = {exog.name: exog.dtypes}\n    else:\n        exog_dtypes = exog.dtypes.to_dict()\n    \n    return exog_dtypes\n\n\ndef check_exog_dtypes(\n    exog: Union[pd.DataFrame, pd.Series],\n    call_check_exog: bool = True,\n    series_id: str = \"`exog`\"\n) -> None:\n    \"\"\"\n    Raise Exception if `exog` has categorical columns with non integer values.\n    This is needed when using machine learning regressors that allow categorical\n    features.\n    Issue a Warning if `exog` has columns that are not `init`, `float`, or `category`.\n    \n    Parameters\n    ----------\n    exog : pandas DataFrame, pandas Series\n        Exogenous variable/s included as predictor/s.\n    call_check_exog : bool, default `True`\n        If `True`, call `check_exog` function.\n    series_id : str, default '`exog`'\n        Identifier of the series for which the exogenous variable/s are used\n        in the warning message.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if call_check_exog:\n        check_exog(exog=exog, allow_nan=False, series_id=series_id)\n\n    if isinstance(exog, pd.DataFrame):\n        if not exog.select_dtypes(exclude=[np.number, 'category']).columns.empty:\n            warnings.warn(\n                (f\"{series_id} may contain only `int`, `float` or `category` dtypes. \"\n                 f\"Most machine learning models do not allow other types of values. \"\n                 f\"Fitting the forecaster may fail.\"), \n                 DataTypeWarning\n            )\n        for col in exog.select_dtypes(include='category'):\n            if exog[col].cat.categories.dtype not in [int, np.int32, np.int64]:\n                raise TypeError(\n                    (\"Categorical dtypes in exog must contain only integer values. \"\n                     \"See skforecast docs for more info about how to include \"\n                     \"categorical features https://skforecast.org/\"\n                     \"latest/user_guides/categorical-features.html\")\n                )\n    else:\n        if exog.dtype.name not in ['int', 'int8', 'int16', 'int32', 'int64', 'float', \n        'float16', 'float32', 'float64', 'uint8', 'uint16', 'uint32', 'uint64', 'category']:\n            warnings.warn(\n                (f\"{series_id} may contain only `int`, `float` or `category` dtypes. Most \"\n                 f\"machine learning models do not allow other types of values. \"\n                 f\"Fitting the forecaster may fail.\"), \n                 DataTypeWarning\n            )\n        if exog.dtype.name == 'category' and exog.cat.categories.dtype not in [int,\n        np.int32, np.int64]:\n            raise TypeError(\n                (\"Categorical dtypes in exog must contain only integer values. \"\n                 \"See skforecast docs for more info about how to include \"\n                 \"categorical features https://skforecast.org/\"\n                 \"latest/user_guides/categorical-features.html\")\n            )\n         \n    return\n\n\ndef check_interval(\n    interval: list = None,\n    quantiles: float = None,\n    alpha: float = None\n) -> None:\n    \"\"\"\n    Check provided confidence interval sequence is valid.\n\n    Parameters\n    ----------\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. For example, \n        interval of 95% should be as `interval = [2.5, 97.5]`.\n    quantiles : list, default `None`\n        Sequence of quantiles to compute, which must be between 0 and 1 \n        inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n        `quantiles = [0.05, 0.5, 0.95]`.\n    alpha : float, default `None`\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %.\n\n    Returns\n    -------\n    None\n    \n    \"\"\"\n\n    if interval is not None:\n        if not isinstance(interval, list):\n            raise TypeError(\n                (\"`interval` must be a `list`. For example, interval of 95% \"\n                 \"should be as `interval = [2.5, 97.5]`.\")\n            )\n\n        if len(interval) != 2:\n            raise ValueError(\n                (\"`interval` must contain exactly 2 values, respectively the \"\n                 \"lower and upper interval bounds. For example, interval of 95% \"\n                 \"should be as `interval = [2.5, 97.5]`.\")\n            )\n\n        if (interval[0] < 0.) or (interval[0] >= 100.):\n            raise ValueError(\n                f\"Lower interval bound ({interval[0]}) must be >= 0 and < 100.\"\n            )\n\n        if (interval[1] <= 0.) or (interval[1] > 100.):\n            raise ValueError(\n                f\"Upper interval bound ({interval[1]}) must be > 0 and <= 100.\"\n            )\n\n        if interval[0] >= interval[1]:\n            raise ValueError(\n                (f\"Lower interval bound ({interval[0]}) must be less than the \"\n                 f\"upper interval bound ({interval[1]}).\")\n            )\n        \n    if quantiles is not None:\n        if not isinstance(quantiles, list):\n            raise TypeError(\n                (\"`quantiles` must be a `list`. For example, quantiles 0.05, \"\n                 \"0.5, and 0.95 should be as `quantiles = [0.05, 0.5, 0.95]`.\")\n            )\n        \n        for q in quantiles:\n            if (q < 0.) or (q > 1.):\n                raise ValueError(\n                    (\"All elements in `quantiles` must be >= 0 and <= 1.\")\n                )\n    \n    if alpha is not None:\n        if not isinstance(alpha, float):\n            raise TypeError(\n                (\"`alpha` must be a `float`. For example, interval of 95% \"\n                 \"should be as `alpha = 0.05`.\")\n            )\n\n        if (alpha <= 0.) or (alpha >= 1):\n            raise ValueError(\n                f\"`alpha` must have a value between 0 and 1. Got {alpha}.\"\n            )\n\n    return\n\n\ndef check_predict_input(\n    forecaster_name: str,\n    steps: Union[int, list],\n    is_fitted: bool,\n    exog_in_: bool,\n    index_type_: type,\n    index_freq_: str,\n    window_size: int,\n    last_window: Union[pd.Series, pd.DataFrame, None],\n    last_window_exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    exog_type_in_: Optional[type] = None,\n    exog_names_in_: Optional[list] = None,\n    interval: Optional[list] = None,\n    alpha: Optional[float] = None,\n    max_steps: Optional[int] = None,\n    levels: Optional[Union[str, list]] = None,\n    levels_forecaster: Optional[Union[str, list]] = None,\n    series_names_in_: Optional[list] = None,\n    encoding: Optional[str] = None\n) -> None:\n    \"\"\"\n    Check all inputs of predict method. This is a helper function to validate\n    that inputs used in predict method match attributes of a forecaster already\n    trained.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    steps : int, list\n        Number of future steps predicted.\n    is_fitted: bool\n        Tag to identify if the regressor has been fitted (trained).\n    exog_in_ : bool\n        If the forecaster has been trained using exogenous variable/s.\n    index_type_ : type\n        Type of index of the input used in training.\n    index_freq_ : str\n        Frequency of Index of the input used in training.\n    window_size: int\n        Size of the window needed to create the predictors. It is equal to \n        `max_lag`.\n    last_window : pandas Series, pandas DataFrame, None\n        Values of the series used to create the predictors (lags) need in the \n        first iteration of prediction (t + 1).\n    last_window_exog : pandas Series, pandas DataFrame, default `None`\n        Values of the exogenous variables aligned with `last_window` in \n        ForecasterSarimax predictions.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    exog_type_in_ : type, default `None`\n        Type of exogenous variable/s used in training.\n    exog_names_in_ : list, default `None`\n        Names of the exogenous variables used during training.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. For example, \n        interval of 95% should be as `interval = [2.5, 97.5]`.\n    alpha : float, default `None`\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %.\n    max_steps: int, default `None`\n        Maximum number of steps allowed (`ForecasterDirect` and \n        `ForecasterDirectMultiVariate`).\n    levels : str, list, default `None`\n        Time series to be predicted (`ForecasterRecursiveMultiSeries`\n        and `ForecasterRnn).\n    levels_forecaster : str, list, default `None`\n        Time series used as output data of a multiseries problem in a RNN problem\n        (`ForecasterRnn`).\n    series_names_in_ : list, default `None`\n        Names of the columns used during fit (`ForecasterRecursiveMultiSeries`, \n        `ForecasterDirectMultiVariate` and `ForecasterRnn`).\n    encoding : str, default `None`\n        Encoding used to identify the different series (`ForecasterRecursiveMultiSeries`).\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n\n    if not is_fitted:\n        raise NotFittedError(\n            (\"This Forecaster instance is not fitted yet. Call `fit` with \"\n             \"appropriate arguments before using predict.\")\n        )\n\n    if isinstance(steps, (int, np.integer)) and steps < 1:\n        raise ValueError(\n            f\"`steps` must be an integer greater than or equal to 1. Got {steps}.\"\n        )\n\n    if isinstance(steps, list) and min(steps) < 1:\n        raise ValueError(\n           (f\"The minimum value of `steps` must be equal to or greater than 1. \"\n            f\"Got {min(steps)}.\")\n        )\n\n    if max_steps is not None:\n        if max(steps) > max_steps:\n            raise ValueError(\n                (f\"The maximum value of `steps` must be less than or equal to \"\n                 f\"the value of steps defined when initializing the forecaster. \"\n                 f\"Got {max(steps)}, but the maximum is {max_steps}.\")\n            )\n\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n\n    if forecaster_name in ['ForecasterRecursiveMultiSeries', \n                           'ForecasterRnn']:\n        if not isinstance(levels, (type(None), str, list)):\n            raise TypeError(\n                (\"`levels` must be a `list` of column names, a `str` of a \"\n                 \"column name or `None`.\")\n            )\n\n        levels_to_check = (\n            levels_forecaster if forecaster_name == 'ForecasterRnn'\n            else series_names_in_\n        )\n        unknown_levels = set(levels) - set(levels_to_check)\n        if forecaster_name == 'ForecasterRnn':\n            if len(unknown_levels) != 0:\n                raise ValueError(\n                    (f\"`levels` names must be included in the series used during fit \"\n                     f\"({levels_to_check}). Got {levels}.\")\n                )\n        else:\n            if len(unknown_levels) != 0 and last_window is not None and encoding is not None:\n                if encoding == 'onehot':\n                    warnings.warn(\n                        (f\"`levels` {unknown_levels} were not included in training. The resulting \"\n                         f\"one-hot encoded columns for this feature will be all zeros.\"),\n                         UnknownLevelWarning\n                    )\n                else:\n                    warnings.warn(\n                        (f\"`levels` {unknown_levels} were not included in training. \"\n                         f\"Unknown levels are encoded as NaN, which may cause the \"\n                         f\"prediction to fail if the regressor does not accept NaN values.\"),\n                         UnknownLevelWarning\n                    )\n\n    if exog is None and exog_in_:\n        raise ValueError(\n            (\"Forecaster trained with exogenous variable/s. \"\n             \"Same variable/s must be provided when predicting.\")\n        )\n\n    if exog is not None and not exog_in_:\n        raise ValueError(\n            (\"Forecaster trained without exogenous variable/s. \"\n             \"`exog` must be `None` when predicting.\")\n        )\n\n    # Checks last_window\n    # Check last_window type (pd.Series or pd.DataFrame according to forecaster)\n    if isinstance(last_window, type(None)) and forecaster_name not in [\n        'ForecasterRecursiveMultiSeries', \n        'ForecasterRnn'\n    ]:\n        raise ValueError(\n            (\"`last_window` was not stored during training. If you don't want \"\n             \"to retrain the Forecaster, provide `last_window` as argument.\")\n        )\n\n    if forecaster_name in ['ForecasterRecursiveMultiSeries', \n                           'ForecasterDirectMultiVariate',\n                           'ForecasterRnn']:\n        if not isinstance(last_window, pd.DataFrame):\n            raise TypeError(\n                f\"`last_window` must be a pandas DataFrame. Got {type(last_window)}.\"\n            )\n\n        last_window_cols = last_window.columns.to_list()\n\n        if forecaster_name in ['ForecasterRecursiveMultiSeries', \n                               'ForecasterRnn'] and \\\n            len(set(levels) - set(last_window_cols)) != 0:\n            raise ValueError(\n                (f\"`last_window` must contain a column(s) named as the level(s) \"\n                 f\"to be predicted.\\n\"\n                 f\"    `levels` : {levels}\\n\"\n                 f\"    `last_window` columns : {last_window_cols}\")\n            )\n\n        if forecaster_name == 'ForecasterDirectMultiVariate':\n            if len(set(series_names_in_) - set(last_window_cols)) > 0:\n                raise ValueError(\n                    (f\"`last_window` columns must be the same as the `series` \"\n                     f\"column names used to create the X_train matrix.\\n\"\n                     f\"    `last_window` columns    : {last_window_cols}\\n\"\n                     f\"    `series` columns X train : {series_names_in_}\")\n                )\n    else:\n        if not isinstance(last_window, (pd.Series, pd.DataFrame)):\n            raise TypeError(\n                f\"`last_window` must be a pandas Series or DataFrame. \"\n                f\"Got {type(last_window)}.\"\n            )\n\n    # Check last_window len, nulls and index (type and freq)\n    if len(last_window) < window_size:\n        raise ValueError(\n            (f\"`last_window` must have as many values as needed to \"\n             f\"generate the predictors. For this forecaster it is {window_size}.\")\n        )\n    if last_window.isnull().any().all():\n        warnings.warn(\n            (\"`last_window` has missing values. Most of machine learning models do \"\n             \"not allow missing values. Prediction method may fail.\"), \n             MissingValuesWarning\n        )\n    _, last_window_index = preprocess_last_window(\n                               last_window   = last_window.iloc[:0],\n                               return_values = False\n                           ) \n    if not isinstance(last_window_index, index_type_):\n        raise TypeError(\n            (f\"Expected index of type {index_type_} for `last_window`. \"\n             f\"Got {type(last_window_index)}.\")\n        )\n    if isinstance(last_window_index, pd.DatetimeIndex):\n        if not last_window_index.freqstr == index_freq_:\n            raise TypeError(\n                (f\"Expected frequency of type {index_freq_} for `last_window`. \"\n                 f\"Got {last_window_index.freqstr}.\")\n            )\n\n    # Checks exog\n    if exog is not None:\n\n        # Check type, nulls and expected type\n        if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n            if not isinstance(exog, (pd.Series, pd.DataFrame, dict)):\n                raise TypeError(\n                    f\"`exog` must be a pandas Series, DataFrame or dict. Got {type(exog)}.\"\n                )\n            if exog_type_in_ == dict and not isinstance(exog, dict):\n                raise TypeError(\n                    f\"Expected type for `exog`: {exog_type_in_}. Got {type(exog)}.\"\n                )\n        else:\n            if not isinstance(exog, (pd.Series, pd.DataFrame)):\n                raise TypeError(\n                    f\"`exog` must be a pandas Series or DataFrame. Got {type(exog)}.\"\n                )\n\n        if isinstance(exog, dict):\n            no_exog_levels = set(levels) - set(exog.keys())\n            if no_exog_levels:\n                warnings.warn(\n                    (f\"`exog` does not contain keys for levels {no_exog_levels}. \"\n                     f\"Missing levels are filled with NaN. Most of machine learning \"\n                     f\"models do not allow missing values. Prediction method may fail.\"),\n                     MissingExogWarning\n                )\n            exogs_to_check = [\n                (f\"`exog` for series '{k}'\", v) \n                for k, v in exog.items() \n                if v is not None and k in levels\n            ]\n        else:\n            exogs_to_check = [('`exog`', exog)]\n\n        for exog_name, exog_to_check in exogs_to_check:\n\n            if not isinstance(exog_to_check, (pd.Series, pd.DataFrame)):\n                raise TypeError(\n                    f\"{exog_name} must be a pandas Series or DataFrame. Got {type(exog_to_check)}\"\n                )\n\n            if exog_to_check.isnull().any().any():\n                warnings.warn(\n                    (f\"{exog_name} has missing values. Most of machine learning models \"\n                     f\"do not allow missing values. Prediction method may fail.\"), \n                     MissingValuesWarning\n                )\n\n            # Check exog has many values as distance to max step predicted\n            last_step = max(steps) if isinstance(steps, list) else steps\n            if len(exog_to_check) < last_step:\n                if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n                    warnings.warn(\n                        (f\"{exog_name} doesn't have as many values as steps \"\n                         f\"predicted, {last_step}. Missing values are filled \"\n                         f\"with NaN. Most of machine learning models do not \"\n                         f\"allow missing values. Prediction method may fail.\"),\n                         MissingValuesWarning\n                    )\n                else: \n                    raise ValueError(\n                        (f\"{exog_name} must have at least as many values as \"\n                         f\"steps predicted, {last_step}.\")\n                    )\n\n            # Check name/columns are in exog_names_in_\n            if isinstance(exog_to_check, pd.DataFrame):\n                col_missing = set(exog_names_in_).difference(set(exog_to_check.columns))\n                if col_missing:\n                    if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n                        warnings.warn(\n                            (f\"{col_missing} not present in {exog_name}. All \"\n                             f\"values will be NaN.\"),\n                             MissingExogWarning\n                        ) \n                    else:\n                        raise ValueError(\n                            (f\"Missing columns in {exog_name}. Expected {exog_names_in_}. \"\n                             f\"Got {exog_to_check.columns.to_list()}.\")\n                        )\n            else:\n                if exog_to_check.name is None:\n                    raise ValueError(\n                        (f\"When {exog_name} is a pandas Series, it must have a name. Got None.\")\n                    )\n\n                if exog_to_check.name not in exog_names_in_:\n                    if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n                        warnings.warn(\n                            (f\"'{exog_to_check.name}' was not observed during training. \"\n                             f\"{exog_name} is ignored. Exogenous variables must be one \"\n                             f\"of: {exog_names_in_}.\"),\n                             IgnoredArgumentWarning\n                        )\n                    else:\n                        raise ValueError(\n                            (f\"'{exog_to_check.name}' was not observed during training. \"\n                             f\"Exogenous variables must be: {exog_names_in_}.\")\n                        )\n\n            # Check index dtype and freq\n            _, exog_index = preprocess_exog(\n                                exog          = exog_to_check.iloc[:0, ],\n                                return_values = False\n                            )\n            if not isinstance(exog_index, index_type_):\n                raise TypeError(\n                    (f\"Expected index of type {index_type_} for {exog_name}. \"\n                     f\"Got {type(exog_index)}.\")\n                )\n            if forecaster_name not in ['ForecasterRecursiveMultiSeries']:\n                if isinstance(exog_index, pd.DatetimeIndex):\n                    if not exog_index.freqstr == index_freq_:\n                        raise TypeError(\n                            (f\"Expected frequency of type {index_freq_} for {exog_name}. \"\n                             f\"Got {exog_index.freqstr}.\")\n                        )\n\n            # Check exog starts one step ahead of last_window end.\n            expected_index = expand_index(last_window.index, 1)[0]\n            if expected_index != exog_to_check.index[0]:\n                if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n                    warnings.warn(\n                        (f\"To make predictions {exog_name} must start one step \"\n                         f\"ahead of `last_window`. Missing values are filled \"\n                         f\"with NaN.\\n\"\n                         f\"    `last_window` ends at : {last_window.index[-1]}.\\n\"\n                         f\"    {exog_name} starts at : {exog_to_check.index[0]}.\\n\"\n                         f\"     Expected index       : {expected_index}.\"),\n                         MissingValuesWarning\n                    )  \n                else:\n                    raise ValueError(\n                        (f\"To make predictions {exog_name} must start one step \"\n                         f\"ahead of `last_window`.\\n\"\n                         f\"    `last_window` ends at : {last_window.index[-1]}.\\n\"\n                         f\"    {exog_name} starts at : {exog_to_check.index[0]}.\\n\"\n                         f\"     Expected index : {expected_index}.\")\n                    )\n\n    # Checks ForecasterSarimax\n    if forecaster_name == 'ForecasterSarimax':\n        # Check last_window_exog type, len, nulls and index (type and freq)\n        if last_window_exog is not None:\n            if not exog_in_:\n                raise ValueError(\n                    (\"Forecaster trained without exogenous variable/s. \"\n                     \"`last_window_exog` must be `None` when predicting.\")\n                )\n\n            if not isinstance(last_window_exog, (pd.Series, pd.DataFrame)):\n                raise TypeError(\n                    (f\"`last_window_exog` must be a pandas Series or a \"\n                     f\"pandas DataFrame. Got {type(last_window_exog)}.\")\n                )\n            if len(last_window_exog) < window_size:\n                raise ValueError(\n                    (f\"`last_window_exog` must have as many values as needed to \"\n                     f\"generate the predictors. For this forecaster it is {window_size}.\")\n                )\n            if last_window_exog.isnull().any().all():\n                warnings.warn(\n                    (\"`last_window_exog` has missing values. Most of machine learning \"\n                     \"models do not allow missing values. Prediction method may fail.\"),\n                     MissingValuesWarning\n            )\n            _, last_window_exog_index = preprocess_last_window(\n                                            last_window   = last_window_exog.iloc[:0],\n                                            return_values = False\n                                        ) \n            if not isinstance(last_window_exog_index, index_type_):\n                raise TypeError(\n                    (f\"Expected index of type {index_type_} for `last_window_exog`. \"\n                     f\"Got {type(last_window_exog_index)}.\")\n                )\n            if isinstance(last_window_exog_index, pd.DatetimeIndex):\n                if not last_window_exog_index.freqstr == index_freq_:\n                    raise TypeError(\n                        (f\"Expected frequency of type {index_freq_} for \"\n                         f\"`last_window_exog`. Got {last_window_exog_index.freqstr}.\")\n                    )\n\n            # Check all columns are in the pd.DataFrame, last_window_exog\n            if isinstance(last_window_exog, pd.DataFrame):\n                col_missing = set(exog_names_in_).difference(set(last_window_exog.columns))\n                if col_missing:\n                    raise ValueError(\n                        (f\"Missing columns in `last_window_exog`. Expected {exog_names_in_}. \"\n                         f\"Got {last_window_exog.columns.to_list()}.\") \n                    )\n            else:\n                if last_window_exog.name is None:\n                    raise ValueError(\n                        (\"When `last_window_exog` is a pandas Series, it must have a \"\n                         \"name. Got None.\")\n                    )\n\n                if last_window_exog.name not in exog_names_in_:\n                    raise ValueError(\n                        (f\"'{last_window_exog.name}' was not observed during training. \"\n                         f\"Exogenous variables must be: {exog_names_in_}.\")\n                    )\n\n    return\n\n\ndef preprocess_y(\n    y: Union[pd.Series, pd.DataFrame],\n    return_values: bool = True\n) -> Tuple[Union[None, np.ndarray], pd.Index]:\n    \"\"\"\n    Return values and index of series separately. Index is overwritten \n    according to the next rules:\n    \n    - If index is of type `DatetimeIndex` and has frequency, nothing is \n    changed.\n    - If index is of type `RangeIndex`, nothing is changed.\n    - If index is of type `DatetimeIndex` but has no frequency, a \n    `RangeIndex` is created.\n    - If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\n    \n    Parameters\n    ----------\n    y : pandas Series, pandas DataFrame\n        Time series.\n    return_values : bool, default `True`\n        If `True` return the values of `y` as numpy ndarray. This option is \n        intended to avoid copying data when it is not necessary.\n\n    Returns\n    -------\n    y_values : None, numpy ndarray\n        Numpy array with values of `y`.\n    y_index : pandas Index\n        Index of `y` modified according to the rules.\n    \n    \"\"\"\n    \n    if isinstance(y.index, pd.DatetimeIndex) and y.index.freq is not None:\n        y_index = y.index\n    elif isinstance(y.index, pd.RangeIndex):\n        y_index = y.index\n    elif isinstance(y.index, pd.DatetimeIndex) and y.index.freq is None:\n        warnings.warn(\n            (\"Series has DatetimeIndex index but no frequency. \"\n             \"Index is overwritten with a RangeIndex of step 1.\")\n        )\n        y_index = pd.RangeIndex(\n                      start = 0,\n                      stop  = len(y),\n                      step  = 1\n                  )\n    else:\n        warnings.warn(\n            (\"Series has no DatetimeIndex nor RangeIndex index. \"\n             \"Index is overwritten with a RangeIndex.\")\n        )\n        y_index = pd.RangeIndex(\n                      start = 0,\n                      stop  = len(y),\n                      step  = 1\n                  )\n\n    y_values = y.to_numpy(copy=True).ravel() if return_values else None\n\n    return y_values, y_index\n\n\ndef preprocess_last_window(\n    last_window: Union[pd.Series, pd.DataFrame],\n    return_values: bool = True\n ) -> Tuple[np.ndarray, pd.Index]:\n    \"\"\"\n    Return values and index of series separately. Index is overwritten \n    according to the next rules:\n    \n    - If index is of type `DatetimeIndex` and has frequency, nothing is \n    changed.\n    - If index is of type `RangeIndex`, nothing is changed.\n    - If index is of type `DatetimeIndex` but has no frequency, a \n    `RangeIndex` is created.\n    - If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\n    \n    Parameters\n    ----------\n    last_window : pandas Series, pandas DataFrame\n        Time series values.\n    return_values : bool, default `True`\n        If `True` return the values of `last_window` as numpy ndarray. This option \n        is intended to avoid copying data when it is not necessary.\n\n    Returns\n    -------\n    last_window_values : numpy ndarray\n        Numpy array with values of `last_window`.\n    last_window_index : pandas Index\n        Index of `last_window` modified according to the rules.\n    \n    \"\"\"\n    \n    if isinstance(last_window.index, pd.DatetimeIndex) and last_window.index.freq is not None:\n        last_window_index = last_window.index\n    elif isinstance(last_window.index, pd.RangeIndex):\n        last_window_index = last_window.index\n    elif isinstance(last_window.index, pd.DatetimeIndex) and last_window.index.freq is None:\n        warnings.warn(\n            (\"`last_window` has DatetimeIndex index but no frequency. \"\n             \"Index is overwritten with a RangeIndex of step 1.\")\n        )\n        last_window_index = pd.RangeIndex(\n                                start = 0,\n                                stop  = len(last_window),\n                                step  = 1\n                            )\n    else:\n        warnings.warn(\n            (\"`last_window` has no DatetimeIndex nor RangeIndex index. \"\n             \"Index is overwritten with a RangeIndex.\")\n        )\n        last_window_index = pd.RangeIndex(\n                                start = 0,\n                                stop  = len(last_window),\n                                step  = 1\n                            )\n\n    last_window_values = last_window.to_numpy(copy=True).ravel() if return_values else None\n\n    return last_window_values, last_window_index\n\n\ndef preprocess_exog(\n    exog: Union[pd.Series, pd.DataFrame],\n    return_values: bool = True\n) -> Tuple[Union[None, np.ndarray], pd.Index]:\n    \"\"\"\n    Return values and index of series or data frame separately. Index is\n    overwritten  according to the next rules:\n    \n    - If index is of type `DatetimeIndex` and has frequency, nothing is \n    changed.\n    - If index is of type `RangeIndex`, nothing is changed.\n    - If index is of type `DatetimeIndex` but has no frequency, a \n    `RangeIndex` is created.\n    - If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\n\n    Parameters\n    ----------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables.\n    return_values : bool, default `True`\n        If `True` return the values of `exog` as numpy ndarray. This option is \n        intended to avoid copying data when it is not necessary.\n\n    Returns\n    -------\n    exog_values : None, numpy ndarray\n        Numpy array with values of `exog`.\n    exog_index : pandas Index\n        Index of `exog` modified according to the rules.\n    \n    \"\"\"\n    \n    if isinstance(exog.index, pd.DatetimeIndex) and exog.index.freq is not None:\n        exog_index = exog.index\n    elif isinstance(exog.index, pd.RangeIndex):\n        exog_index = exog.index\n    elif isinstance(exog.index, pd.DatetimeIndex) and exog.index.freq is None:\n        warnings.warn(\n            (\"`exog` has DatetimeIndex index but no frequency. \"\n             \"Index is overwritten with a RangeIndex of step 1.\")\n        )\n        exog_index = pd.RangeIndex(\n                         start = 0,\n                         stop  = len(exog),\n                         step  = 1\n                     )\n\n    else:\n        warnings.warn(\n            (\"`exog` has no DatetimeIndex nor RangeIndex index. \"\n             \"Index is overwritten with a RangeIndex.\")\n        )\n        exog_index = pd.RangeIndex(\n                         start = 0,\n                         stop  = len(exog),\n                         step  = 1\n                     )\n\n    exog_values = exog.to_numpy(copy=True) if return_values else None\n\n    return exog_values, exog_index\n\n\ndef input_to_frame(\n    data: Union[pd.Series, pd.DataFrame],\n    input_name: str\n) -> pd.DataFrame:\n    \"\"\"\n    Convert data to a pandas DataFrame. If data is a pandas Series, it is \n    converted to a DataFrame with a single column. If data is a DataFrame, \n    it is returned as is.\n\n    Parameters\n    ----------\n    data : pandas Series, pandas DataFrame\n        Input data.\n    input_name : str\n        Name of the input data. Accepted values are 'y', 'last_window' and 'exog'.\n\n    Returns\n    -------\n    data : pandas DataFrame\n        Input data as a DataFrame.\n\n    \"\"\"\n\n    output_col_name = {\n        'y': 'y',\n        'last_window': 'y',\n        'exog': 'exog'\n    }\n\n    if isinstance(data, pd.Series):\n        data = data.to_frame(\n            name=data.name if data.name is not None else output_col_name[input_name]\n        )\n\n    return data\n\n\ndef cast_exog_dtypes(\n    exog: Union[pd.Series, pd.DataFrame],\n    exog_dtypes: dict,\n) -> Union[pd.Series, pd.DataFrame]:  # pragma: no cover\n    \"\"\"\n    Cast `exog` to a specified types. This is done because, for a forecaster to \n    accept a categorical exog, it must contain only integer values. Due to the \n    internal modifications of numpy, the values may be casted to `float`, so \n    they have to be re-converted to `int`.\n\n    - If `exog` is a pandas Series, `exog_dtypes` must be a dict with a \n    single value.\n    - If `exog_dtypes` is `category` but the current type of `exog` is `float`, \n    then the type is cast to `int` and then to `category`. \n\n    Parameters\n    ----------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables.\n    exog_dtypes: dict\n        Dictionary with name and type of the series or data frame columns.\n\n    Returns\n    -------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables casted to the indicated dtypes.\n\n    \"\"\"\n\n    # Remove keys from exog_dtypes not in exog.columns\n    exog_dtypes = {k: v for k, v in exog_dtypes.items() if k in exog.columns}\n    \n    if isinstance(exog, pd.Series) and exog.dtypes != list(exog_dtypes.values())[0]:\n        exog = exog.astype(list(exog_dtypes.values())[0])\n    elif isinstance(exog, pd.DataFrame):\n        for col, initial_dtype in exog_dtypes.items():\n            if exog[col].dtypes != initial_dtype:\n                if initial_dtype == \"category\" and exog[col].dtypes == float:\n                    exog[col] = exog[col].astype(int).astype(\"category\")\n                else:\n                    exog[col] = exog[col].astype(initial_dtype)\n\n    return exog\n\n\ndef exog_to_direct(\n    exog: Union[pd.Series, pd.DataFrame],\n    steps: int\n) -> Union[pd.DataFrame, list]:\n    \"\"\"\n    Transforms `exog` to a pandas DataFrame with the shape needed for Direct\n    forecasting.\n    \n    Parameters\n    ----------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables.\n    steps : int\n        Number of steps that will be predicted using exog.\n\n    Returns\n    -------\n    exog_direct : pandas DataFrame\n        Exogenous variables transformed.\n    exog_direct_names : list\n        Names of the columns of the exogenous variables transformed. Only \n        created if `exog` is a pandas Series or DataFrame.\n    \n    \"\"\"\n\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(f\"`exog` must be a pandas Series or DataFrame. Got {type(exog)}.\")\n\n    if isinstance(exog, pd.Series):\n        exog = exog.to_frame()\n\n    n_rows = len(exog)\n    exog_idx = exog.index\n    exog_cols = exog.columns\n    exog_direct = []\n    for i in range(steps):\n        exog_step = exog.iloc[i : n_rows - (steps - 1 - i), ]\n        exog_step.index = pd.RangeIndex(len(exog_step))\n        exog_step.columns = [f\"{col}_step_{i + 1}\" for col in exog_cols]\n        exog_direct.append(exog_step)\n\n    if len(exog_direct) > 1:\n        exog_direct = pd.concat(exog_direct, axis=1, copy=False)\n    else:\n        exog_direct = exog_direct[0]\n\n    exog_direct_names = exog_direct.columns.to_list()\n    exog_direct.index = exog_idx[-len(exog_direct):]\n    \n    return exog_direct, exog_direct_names\n\n\ndef exog_to_direct_numpy(\n    exog: Union[np.ndarray, pd.Series, pd.DataFrame],\n    steps: int\n) -> Tuple[np.ndarray, Optional[list]]:\n    \"\"\"\n    Transforms `exog` to numpy ndarray with the shape needed for Direct\n    forecasting.\n    \n    Parameters\n    ----------\n    exog : numpy ndarray, pandas Series, pandas DataFrame\n        Exogenous variables, shape(samples,). If exog is a pandas format, the \n        direct exog names are created.\n    steps : int\n        Number of steps that will be predicted using exog.\n\n    Returns\n    -------\n    exog_direct : numpy ndarray\n        Exogenous variables transformed.\n    exog_direct_names : list, None\n        Names of the columns of the exogenous variables transformed. Only \n        created if `exog` is a pandas Series or DataFrame.\n\n    \"\"\"\n\n    if isinstance(exog, (pd.Series, pd.DataFrame)):\n        exog_cols = exog.columns if isinstance(exog, pd.DataFrame) else [exog.name]\n        exog_direct_names = [\n            f\"{col}_step_{i + 1}\" for i in range(steps) for col in exog_cols\n        ]\n        exog = exog.to_numpy()\n    else:\n        exog_direct_names = None\n        if not isinstance(exog, np.ndarray):\n            raise TypeError(\n                f\"`exog` must be a numpy ndarray, pandas Series or DataFrame. \"\n                f\"Got {type(exog)}.\"\n            )\n\n    if exog.ndim == 1:\n        exog = np.expand_dims(exog, axis=1)\n\n    n_rows = len(exog)\n    exog_direct = []\n    for i in range(steps):\n        exog_step = exog[i : n_rows - (steps - 1 - i)]\n        exog_direct.append(exog_step)\n\n    if len(exog_direct) > 1:\n        exog_direct = np.concatenate(exog_direct, axis=1)\n    else:\n        exog_direct = exog_direct[0]\n    \n    return exog_direct, exog_direct_names\n\n\ndef date_to_index_position(\n    index: pd.Index,\n    date_input: Union[int, str, pd.Timestamp],\n    date_literal: str = 'steps',\n    kwargs_pd_to_datetime: dict = {}\n) -> int:\n    \"\"\"\n    Transform a datetime string or pandas Timestamp to an integer. The integer\n    represents the position of the datetime in the index.\n    \n    Parameters\n    ----------\n    index : pandas Index\n        Original datetime index (must be a pandas DatetimeIndex if `date_input` \n        is not an int).\n    date_input : int, str, pandas Timestamp\n        Datetime to transform to integer.\n        \n        + If int, returns the same integer.\n        + If str or pandas Timestamp, it is converted and expanded into the index.\n    date_literal : str, default 'steps'\n        Variable name used in error messages.\n    kwargs_pd_to_datetime : dict, default {}\n        Additional keyword arguments to pass to `pd.to_datetime()`.\n    \n    Returns\n    -------\n    date_position : int\n        Integer representing the position of the datetime in the index.\n    \n    \"\"\"\n    \n    if isinstance(date_input, (str, pd.Timestamp)):\n        if not isinstance(index, pd.DatetimeIndex):\n            raise TypeError(\n                f\"Index must be a pandas DatetimeIndex when `{date_literal}` is \"\n                f\"not an integer. Check input series or last window.\"\n            )\n        \n        target_date = pd.to_datetime(date_input, **kwargs_pd_to_datetime)\n        last_date = pd.to_datetime(index[-1])\n        if target_date <= last_date:\n            raise ValueError(\n                \"The provided date must be later than the last date in the index.\"\n            )\n        \n        steps_diff = pd.date_range(start=last_date, end=target_date, freq=index.freq)\n        date_position = len(steps_diff) - 1\n    \n    elif isinstance(date_input, (int, np.integer)):\n        date_position = date_input\n    else:\n        raise TypeError(\n            f\"`{date_literal}` must be an integer, string, or pandas Timestamp.\"\n        )\n    \n    return date_position\n\n\ndef expand_index(\n    index: Union[pd.Index, None], \n    steps: int\n) -> pd.Index:\n    \"\"\"\n    Create a new index of length `steps` starting at the end of the index.\n    \n    Parameters\n    ----------\n    index : pandas Index, None\n        Original index.\n    steps : int\n        Number of steps to expand.\n\n    Returns\n    -------\n    new_index : pandas Index\n        New index.\n\n    \"\"\"\n\n    if not isinstance(steps, (int, np.integer)):\n        raise TypeError(f\"`steps` must be an integer. Got {type(steps)}.\")\n\n    if isinstance(index, pd.Index):\n        \n        if isinstance(index, pd.DatetimeIndex):\n            new_index = pd.date_range(\n                            start   = index[-1] + index.freq,\n                            periods = steps,\n                            freq    = index.freq\n                        )\n        elif isinstance(index, pd.RangeIndex):\n            new_index = pd.RangeIndex(\n                            start = index[-1] + 1,\n                            stop  = index[-1] + 1 + steps\n                        )\n        else:\n            raise TypeError(\n                \"Argument `index` must be a pandas DatetimeIndex or RangeIndex.\"\n            )\n    else:\n        new_index = pd.RangeIndex(\n                        start = 0,\n                        stop  = steps\n                    )\n    \n    return new_index\n\n\ndef transform_numpy(\n    array: np.ndarray,\n    transformer,\n    fit: bool = False,\n    inverse_transform: bool = False\n) -> np.ndarray:\n    \"\"\"\n    Transform raw values of a numpy ndarray with a scikit-learn alike \n    transformer, preprocessor or ColumnTransformer. The transformer used must \n    have the following methods: fit, transform, fit_transform and \n    inverse_transform. ColumnTransformers are not allowed since they do not \n    have inverse_transform method.\n\n    Parameters\n    ----------\n    array : numpy ndarray\n        Array to be transformed.\n    transformer : scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n        Scikit-learn alike transformer (preprocessor) with methods: fit, transform,\n        fit_transform and inverse_transform.\n    fit : bool, default `False`\n        Train the transformer before applying it.\n    inverse_transform : bool, default `False`\n        Transform back the data to the original representation. This is not available\n        when using transformers of class scikit-learn ColumnTransformers.\n\n    Returns\n    -------\n    array_transformed : numpy ndarray\n        Transformed array.\n\n    \"\"\"\n    \n    if not isinstance(array, np.ndarray):\n        raise TypeError(\n            f\"`array` argument must be a numpy ndarray. Got {type(array)}\"\n        )\n\n    if transformer is None:\n        return array\n    \n    array_ndim = array.ndim\n    if array_ndim == 1:\n        array = array.reshape(-1, 1)\n\n    if inverse_transform and isinstance(transformer, ColumnTransformer):\n        raise ValueError(\n            \"`inverse_transform` is not available when using ColumnTransformers.\"\n        )\n\n    if not inverse_transform:\n        if fit:\n            array_transformed = transformer.fit_transform(array)\n        else:\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\n                    \"ignore\", \n                    message=\"X does not have valid feature names\", \n                    category=UserWarning\n                )\n                array_transformed = transformer.transform(array)\n    else:\n        array_transformed = transformer.inverse_transform(array)\n\n    if hasattr(array_transformed, 'toarray'):\n        # If the returned values are in sparse matrix format, it is converted to dense\n        array_transformed = array_transformed.toarray()\n\n    if array_ndim == 1:\n        array_transformed = array_transformed.ravel()\n\n    return array_transformed\n\n\ndef transform_series(\n    series: pd.Series,\n    transformer,\n    fit: bool = False,\n    inverse_transform: bool = False\n) -> Union[pd.Series, pd.DataFrame]:\n    \"\"\"\n    Transform raw values of pandas Series with a scikit-learn alike \n    transformer, preprocessor or ColumnTransformer. The transformer used must \n    have the following methods: fit, transform, fit_transform and \n    inverse_transform. ColumnTransformers are not allowed since they do not \n    have inverse_transform method.\n\n    Parameters\n    ----------\n    series : pandas Series\n        Series to be transformed.\n    transformer : scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n        Scikit-learn alike transformer (preprocessor) with methods: fit, transform,\n        fit_transform and inverse_transform.\n    fit : bool, default `False`\n        Train the transformer before applying it.\n    inverse_transform : bool, default `False`\n        Transform back the data to the original representation. This is not available\n        when using transformers of class scikit-learn ColumnTransformers.\n\n    Returns\n    -------\n    series_transformed : pandas Series, pandas DataFrame\n        Transformed Series. Depending on the transformer used, the output may \n        be a Series or a DataFrame.\n\n    \"\"\"\n    \n    if not isinstance(series, pd.Series):\n        raise TypeError(\n            (f\"`series` argument must be a pandas Series. Got {type(series)}.\")\n        )\n        \n    if transformer is None:\n        return series\n\n    if series.name is None:\n        series.name = 'no_name'\n        \n    data = series.to_frame()\n\n    if fit and hasattr(transformer, 'fit'):\n        transformer.fit(data)\n\n    # If argument feature_names_in_ exits, is overwritten to allow using the \n    # transformer on other series than those that were passed during fit.\n    if hasattr(transformer, 'feature_names_in_') and transformer.feature_names_in_[0] != data.columns[0]:\n        transformer = deepcopy(transformer)\n        transformer.feature_names_in_ = np.array([data.columns[0]], dtype=object)\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=UserWarning)\n        if inverse_transform:\n            values_transformed = transformer.inverse_transform(data)\n        else:\n            values_transformed = transformer.transform(data)   \n\n    if hasattr(values_transformed, 'toarray'):\n        # If the returned values are in sparse matrix format, it is converted to dense array.\n        values_transformed = values_transformed.toarray()\n    \n    if isinstance(values_transformed, np.ndarray) and values_transformed.shape[1] == 1:\n        series_transformed = pd.Series(\n                                 data  = values_transformed.ravel(),\n                                 index = data.index,\n                                 name  = data.columns[0]\n                             )\n    elif isinstance(values_transformed, pd.DataFrame) and values_transformed.shape[1] == 1:\n        series_transformed = values_transformed.squeeze()\n    else:\n        series_transformed = pd.DataFrame(\n                                 data    = values_transformed,\n                                 index   = data.index,\n                                 columns = transformer.get_feature_names_out()\n                             )\n\n    return series_transformed\n\n\ndef transform_dataframe(\n    df: pd.DataFrame,\n    transformer,\n    fit: bool = False,\n    inverse_transform: bool = False\n) -> pd.DataFrame:\n    \"\"\"\n    Transform raw values of pandas DataFrame with a scikit-learn alike \n    transformer, preprocessor or ColumnTransformer. The transformer used must \n    have the following methods: fit, transform, fit_transform and \n    inverse_transform. ColumnTransformers are not allowed since they do not \n    have inverse_transform method.\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        DataFrame to be transformed.\n    transformer : scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n        Scikit-learn alike transformer (preprocessor) with methods: fit, transform,\n        fit_transform and inverse_transform.\n    fit : bool, default `False`\n        Train the transformer before applying it.\n    inverse_transform : bool, default `False`\n        Transform back the data to the original representation. This is not available\n        when using transformers of class scikit-learn ColumnTransformers.\n\n    Returns\n    -------\n    df_transformed : pandas DataFrame\n        Transformed DataFrame.\n\n    \"\"\"\n    \n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(\n            f\"`df` argument must be a pandas DataFrame. Got {type(df)}\"\n        )\n\n    if transformer is None:\n        return df\n\n    if inverse_transform and isinstance(transformer, ColumnTransformer):\n        raise ValueError(\n            \"`inverse_transform` is not available when using ColumnTransformers.\"\n        )\n \n    if not inverse_transform:\n        if fit:\n            values_transformed = transformer.fit_transform(df)\n        else:\n            values_transformed = transformer.transform(df)\n    else:\n        values_transformed = transformer.inverse_transform(df)\n\n    if hasattr(values_transformed, 'toarray'):\n        # If the returned values are in sparse matrix format, it is converted to dense\n        values_transformed = values_transformed.toarray()\n\n    if hasattr(transformer, 'get_feature_names_out'):\n        feature_names_out = transformer.get_feature_names_out()\n    elif hasattr(transformer, 'categories_'):   \n        feature_names_out = transformer.categories_\n    else:\n        feature_names_out = df.columns\n\n    df_transformed = pd.DataFrame(\n                         data    = values_transformed,\n                         index   = df.index,\n                         columns = feature_names_out\n                     )\n\n    return df_transformed\n\n\ndef save_forecaster(\n    forecaster: object, \n    file_name: str,\n    save_custom_functions: bool = True, \n    verbose: bool = True\n) -> None:\n    \"\"\"\n    Save forecaster model using joblib. If custom functions are used to create\n    weights, they are saved as .py files.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster created with skforecast library.\n    file_name : str\n        File name given to the object. The save extension will be .joblib.\n    save_custom_functions : bool, default True\n        If True, save custom functions used in the forecaster (weight_func) as \n        .py files. Custom functions need to be available in the environment \n        where the forecaster is going to be loaded.\n    verbose : bool, default True\n        Print summary about the forecaster saved.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    \n    file_name = Path(file_name).with_suffix('.joblib')\n\n    # Save forecaster\n    joblib.dump(forecaster, filename=file_name)\n\n    if save_custom_functions:\n        # Save custom functions to create weights\n        if hasattr(forecaster, 'weight_func') and forecaster.weight_func is not None:\n            if isinstance(forecaster.weight_func, dict):\n                for fun in set(forecaster.weight_func.values()):\n                    file_name = fun.__name__ + '.py'\n                    with open(file_name, 'w') as file:\n                        file.write(inspect.getsource(fun))\n            else:\n                file_name = forecaster.weight_func.__name__ + '.py'\n                with open(file_name, 'w') as file:\n                    file.write(inspect.getsource(forecaster.weight_func))\n    else:\n        if hasattr(forecaster, 'weight_func') and forecaster.weight_func is not None:\n            warnings.warn(\n                \"Custom function(s) used to create weights are not saved. To save them, \"\n                \"set `save_custom_functions` to `True`.\",\n                SaveLoadSkforecastWarning\n            )\n\n    if hasattr(forecaster, 'window_features') and forecaster.window_features is not None:\n        skforecast_classes = {'RollingFeatures'}\n        custom_classes = set(forecaster.window_features_class_names) - skforecast_classes\n        if custom_classes:\n            warnings.warn(\n                \"The Forecaster includes custom user-defined classes in the \"\n                \"`window_features` argument. These classes are not saved automatically \"\n                \"when saving the Forecaster. Please ensure you save these classes \"\n                \"manually and import them before loading the Forecaster.\\n\"\n                \"    Custom classes: \" + ', '.join(custom_classes) + \"\\n\"\n                \"Visit the documentation for more information: \"\n                \"https://skforecast.org/latest/user_guides/save-load-forecaster.html#saving-and-loading-a-forecaster-model-with-custom-features\",\n                SaveLoadSkforecastWarning\n            )\n\n    if verbose:\n        forecaster.summary()\n\n\ndef load_forecaster(\n    file_name: str,\n    verbose: bool = True\n) -> object:\n    \"\"\"\n    Load forecaster model using joblib. If the forecaster was saved with \n    custom user-defined classes as as window features or custom\n    functions to create weights, these objects must be available\n    in the environment where the forecaster is going to be loaded.\n\n    Parameters\n    ----------\n    file_name: str\n        Object file name.\n    verbose: bool, default `True`\n        Print summary about the forecaster loaded.\n\n    Returns\n    -------\n    forecaster: Forecaster\n        Forecaster created with skforecast library.\n    \n    \"\"\"\n\n    forecaster = joblib.load(filename=Path(file_name))\n\n    skforecast_v = skforecast.__version__\n    forecaster_v = forecaster.skforecast_version\n\n    if forecaster_v != skforecast_v:\n        warnings.warn(\n            f\"The skforecast version installed in the environment differs \"\n            f\"from the version used to create the forecaster.\\n\"\n            f\"    Installed Version  : {skforecast_v}\\n\"\n            f\"    Forecaster Version : {forecaster_v}\\n\"\n            f\"This may create incompatibilities when using the library.\",\n             SkforecastVersionWarning\n        )\n\n    if verbose:\n        forecaster.summary()\n\n    return forecaster\n\n\ndef _find_optional_dependency(\n    package_name: str, \n    optional_dependencies: dict = optional_dependencies\n) -> Tuple[str, str]:\n    \"\"\"\n    Find if a package is an optional dependency. If True, find the version and \n    the extension it belongs to.\n\n    Parameters\n    ----------\n    package_name : str\n        Name of the package to check.\n    optional_dependencies : dict, default `optional_dependencies`\n        Skforecast optional dependencies.\n\n    Returns\n    -------\n    extra: str\n        Name of the extra extension where the optional dependency is needed.\n    package_version: srt\n        Name and versions of the dependency.\n\n    \"\"\"\n\n    for extra, packages in optional_dependencies.items():\n        package_version = [package for package in packages if package_name in package]\n        if package_version:\n            return extra, package_version[0]\n\n\ndef check_optional_dependency(\n    package_name: str\n) -> None:\n    \"\"\"\n    Check if an optional dependency is installed, if not raise an ImportError  \n    with installation instructions.\n\n    Parameters\n    ----------\n    package_name : str\n        Name of the package to check.\n\n    Returns\n    -------\n    None\n    \n    \"\"\"\n\n    if importlib.util.find_spec(package_name) is None:\n        try:\n            extra, package_version = _find_optional_dependency(package_name=package_name)\n            msg = (\n                f\"\\n'{package_name}' is an optional dependency not included in the default \"\n                f\"skforecast installation. Please run: `pip install \\\"{package_version}\\\"` to install it.\"\n                f\"\\n\\nAlternately, you can install it by running `pip install skforecast[{extra}]`\"\n            )\n        except:\n            msg = f\"\\n'{package_name}' is needed but not installed. Please install it.\"\n        \n        raise ImportError(msg)\n\n\ndef multivariate_time_series_corr(\n    time_series: pd.Series,\n    other: pd.DataFrame,\n    lags: Union[int, list, np.array],\n    method: str = 'pearson'\n) -> pd.DataFrame:\n    \"\"\"\n    Compute correlation between a time_series and the lagged values of other \n    time series. \n\n    Parameters\n    ----------\n    time_series : pandas Series\n        Target time series.\n    other : pandas DataFrame\n        Time series whose lagged values are correlated to `time_series`.\n    lags : int, list, numpy ndarray\n        Lags to be included in the correlation analysis.\n    method : str, default 'pearson'\n        - 'pearson': standard correlation coefficient.\n        - 'kendall': Kendall Tau correlation coefficient.\n        - 'spearman': Spearman rank correlation.\n\n    Returns\n    -------\n    corr : pandas DataFrame\n        Correlation values.\n\n    \"\"\"\n\n    if not len(time_series) == len(other):\n        raise ValueError(\"`time_series` and `other` must have the same length.\")\n\n    if not (time_series.index == other.index).all():\n        raise ValueError(\"`time_series` and `other` must have the same index.\")\n\n    if isinstance(lags, int):\n        lags = range(lags)\n\n    corr = {}\n    for col in other.columns:\n        lag_values = {}\n        for lag in lags:\n            lag_values[lag] = other[col].shift(lag)\n\n        lag_values = pd.DataFrame(lag_values)\n        lag_values.insert(0, None, time_series)\n        corr[col] = lag_values.corr(method=method).iloc[1:, 0]\n\n    corr = pd.DataFrame(corr)\n    corr.index = corr.index.astype('int64')\n    corr.index.name = \"lag\"\n    \n    return corr\n\n\ndef select_n_jobs_fit_forecaster(\n    forecaster_name: str,\n    regressor: object,\n) -> int:\n    \"\"\"\n    Select the optimal number of jobs to use in the fitting process. This\n    selection is based on heuristics and is not guaranteed to be optimal. \n    \n    The number of jobs is chosen as follows:\n    \n    - If forecaster_name is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and regressor_name is a linear regressor then `n_jobs = 1`, \n    otherwise `n_jobs = cpu_count() - 1`.\n    - If regressor is a `LGBMRegressor(n_jobs=1)`, then `n_jobs = cpu_count() - 1`.\n    - If regressor is a `LGBMRegressor` with internal n_jobs != 1, then `n_jobs = 1`.\n    This is because `lightgbm` is highly optimized for gradient boosting and\n    parallelizes operations at a very fine-grained level, making additional\n    parallelization unnecessary and potentially harmful due to resource contention.\n    \n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n\n    Returns\n    -------\n    n_jobs : int\n        The number of jobs to run in parallel.\n    \n    \"\"\"\n\n    if isinstance(regressor, Pipeline):\n        regressor = regressor[-1]\n        regressor_name = type(regressor).__name__\n    else:\n        regressor_name = type(regressor).__name__\n\n    linear_regressors = [\n        regressor_name\n        for regressor_name in dir(sklearn.linear_model)\n        if not regressor_name.startswith('_')\n    ]\n\n    if forecaster_name in ['ForecasterDirect', \n                           'ForecasterDirectMultiVariate']:\n        if regressor_name in linear_regressors:\n            n_jobs = 1\n        elif regressor_name == 'LGBMRegressor':\n            n_jobs = joblib.cpu_count() - 1 if regressor.n_jobs == 1 else 1\n        else:\n            n_jobs = joblib.cpu_count() - 1\n    else:\n        n_jobs = 1\n\n    return n_jobs\n\n\ndef check_preprocess_series(\n    series: Union[pd.DataFrame, dict],\n) -> Tuple[dict, pd.Index]:\n    \"\"\"\n    Check and preprocess `series` argument in `ForecasterRecursiveMultiSeries` class.\n\n    - If `series` is a pandas DataFrame, it is converted to a dict of pandas \n    Series and index is overwritten according to the rules of preprocess_y.\n    - If `series` is a dict, all values are converted to pandas Series. Checks\n    if all index are pandas DatetimeIndex and, at least, one Series has a non-null\n    frequency. No multiple frequency is allowed.\n\n    Parameters\n    ----------\n    series : pandas DataFrame, dict\n        Training time series.\n\n    Returns\n    -------\n    series_dict : dict\n        Dictionary with the series used during training.\n    series_indexes : dict\n        Dictionary with the index of each series.\n    \n    \"\"\"\n\n    if isinstance(series, pd.DataFrame):\n\n        _, series_index = preprocess_y(y=series, return_values=False)\n        series = series.copy()\n        series.index = series_index\n        series_dict = series.to_dict(\"series\")\n\n    elif isinstance(series, dict):\n\n        not_valid_series = [\n            k \n            for k, v in series.items()\n            if not isinstance(v, (pd.Series, pd.DataFrame))\n        ]\n        if not_valid_series:\n            raise TypeError(\n                (f\"If `series` is a dictionary, all series must be a named \"\n                 f\"pandas Series or a pandas DataFrame with a single column. \"\n                 f\"Review series: {not_valid_series}\")\n            )\n\n        series_dict = {\n            k: v.copy()\n            for k, v in series.items()\n        }\n\n        for k, v in series_dict.items():\n            if isinstance(v, pd.DataFrame):\n                if v.shape[1] != 1:\n                    raise ValueError(\n                        (f\"If `series` is a dictionary, all series must be a named \"\n                         f\"pandas Series or a pandas DataFrame with a single column. \"\n                         f\"Review series: '{k}'\")\n                    )\n                series_dict[k] = v.iloc[:, 0]\n\n            series_dict[k].name = k\n\n        not_valid_index = [\n            k \n            for k, v in series_dict.items()\n            if not isinstance(v.index, pd.DatetimeIndex)\n        ]\n        if not_valid_index:\n            raise TypeError(\n                (f\"If `series` is a dictionary, all series must have a Pandas \"\n                 f\"DatetimeIndex as index with the same frequency. \"\n                 f\"Review series: {not_valid_index}\")\n            )\n\n        indexes_freq = [f\"{v.index.freq}\" for v in series_dict.values()]\n        indexes_freq = sorted(set(indexes_freq))\n        if not len(indexes_freq) == 1:\n            raise ValueError(\n                (f\"If `series` is a dictionary, all series must have a Pandas \"\n                 f\"DatetimeIndex as index with the same frequency. \"\n                 f\"Found frequencies: {indexes_freq}\")\n            )\n    else:\n        raise TypeError(\n            (f\"`series` must be a pandas DataFrame or a dict of DataFrames or Series. \"\n             f\"Got {type(series)}.\")\n        )\n\n    for k, v in series_dict.items():\n        if np.isnan(v).all():\n            raise ValueError(f\"All values of series '{k}' are NaN.\")\n\n    series_indexes = {\n        k: v.index\n        for k, v in series_dict.items()\n    }\n\n    return series_dict, series_indexes\n\n\ndef check_preprocess_exog_multiseries(\n    input_series_is_dict: bool,\n    series_indexes: dict,\n    series_names_in_: list,\n    exog: Union[pd.Series, pd.DataFrame, dict],\n    exog_dict: dict,\n) -> Tuple[dict, list]:\n    \"\"\"\n    Check and preprocess `exog` argument in `ForecasterRecursiveMultiSeries` class.\n\n    - If input series is a pandas DataFrame (input_series_is_dict = False),  \n    checks that input exog (pandas Series, DataFrame or dict) has the same index \n    (type, length and frequency). Index is overwritten according to the rules \n    of preprocess_exog. Create a dict of exog with the same keys as series.\n    - If input series is a dict (input_series_is_dict = True), then input \n    exog must be a dict. Check exog has a pandas DatetimeIndex and convert all\n    values to pandas DataFrames.\n\n    Parameters\n    ----------\n    input_series_is_dict : bool\n        Indicates if input series argument is a dict.\n    series_indexes : dict\n        Dictionary with the index of each series.\n    series_names_in_ : list\n        Names of the series (levels) used during training.\n    exog : pandas Series, pandas DataFrame, dict\n        Exogenous variable/s used during training.\n    exog_dict : dict\n        Dictionary with the exogenous variable/s used during training.\n\n    Returns\n    -------\n    exog_dict : dict\n        Dictionary with the exogenous variable/s used during training.\n    exog_names_in_ : list\n        Names of the exogenous variables used during training.\n    \n    \"\"\"\n\n    if not isinstance(exog, (pd.Series, pd.DataFrame, dict)):\n        raise TypeError(\n            (f\"`exog` must be a pandas Series, DataFrame, dictionary of pandas \"\n             f\"Series/DataFrames or None. Got {type(exog)}.\")\n        )\n\n    if not input_series_is_dict:\n        # If input series is a pandas DataFrame, all index are the same.\n        # Select the first index to check exog\n        series_index = series_indexes[series_names_in_[0]]\n\n    if isinstance(exog, (pd.Series, pd.DataFrame)): \n\n        if input_series_is_dict:\n            raise TypeError(\n                (f\"`exog` must be a dict of DataFrames or Series if \"\n                 f\"`series` is a dict. Got {type(exog)}.\")\n            )\n\n        _, exog_index = preprocess_exog(exog=exog, return_values=False)\n        exog = exog.copy().to_frame() if isinstance(exog, pd.Series) else exog.copy()\n        exog.index = exog_index\n\n        if len(exog) != len(series_index):\n            raise ValueError(\n                (f\"`exog` must have same number of samples as `series`. \"\n                 f\"length `exog`: ({len(exog)}), length `series`: ({len(series_index)})\")\n            )\n\n        if not (exog_index == series_index).all():\n            raise ValueError(\n                (\"Different index for `series` and `exog`. They must be equal \"\n                 \"to ensure the correct alignment of values.\")\n            )\n\n        exog_dict = {serie: exog for serie in series_names_in_}\n\n    else:\n\n        not_valid_exog = [\n            k \n            for k, v in exog.items()\n            if not isinstance(v, (pd.Series, pd.DataFrame, type(None)))\n        ]\n        if not_valid_exog:\n            raise TypeError(\n                (f\"If `exog` is a dictionary, all exog must be a named pandas \"\n                 f\"Series, a pandas DataFrame or None. Review exog: {not_valid_exog}\")\n            )\n\n        # Only elements already present in exog_dict are updated\n        exog_dict.update(\n            (k, v.copy())\n            for k, v in exog.items() \n            if k in exog_dict and v is not None\n        )\n\n        series_not_in_exog = set(series_names_in_) - set(exog.keys())\n        if series_not_in_exog:\n            warnings.warn(\n                (f\"{series_not_in_exog} not present in `exog`. All values \"\n                 f\"of the exogenous variables for these series will be NaN.\"),\n                 MissingExogWarning\n            )\n\n        for k, v in exog_dict.items():\n            if v is not None:\n                check_exog(exog=v, allow_nan=True)\n                if isinstance(v, pd.Series):\n                    v = v.to_frame()\n                exog_dict[k] = v\n\n        if not input_series_is_dict:\n            for k, v in exog_dict.items():\n                if v is not None:\n                    if len(v) != len(series_index):\n                        raise ValueError(\n                            (f\"`exog` for series '{k}' must have same number of \"\n                             f\"samples as `series`. length `exog`: ({len(v)}), \"\n                             f\"length `series`: ({len(series_index)})\")\n                        )\n\n                    _, v_index = preprocess_exog(exog=v, return_values=False)\n                    exog_dict[k].index = v_index\n                    if not (exog_dict[k].index == series_index).all():\n                        raise ValueError(\n                            (f\"Different index for series '{k}' and its exog. \"\n                             f\"When `series` is a pandas DataFrame, they must be \"\n                             f\"equal to ensure the correct alignment of values.\")\n                        )\n        else:\n            not_valid_index = [\n                k\n                for k, v in exog_dict.items()\n                if v is not None and not isinstance(v.index, pd.DatetimeIndex)\n            ]\n            if not_valid_index:\n                raise TypeError(\n                    (f\"All exog must have a Pandas DatetimeIndex as index with the \"\n                     f\"same frequency. Check exog for series: {not_valid_index}\")\n                )\n            \n        # Check that all exog have the same dtypes for common columns\n        exog_dtypes_buffer = [df.dtypes for df in exog_dict.values() if df is not None]\n        exog_dtypes_buffer = pd.concat(exog_dtypes_buffer, axis=1)\n        exog_dtypes_nunique = exog_dtypes_buffer.nunique(axis=1).eq(1)\n        if not exog_dtypes_nunique.all():\n            non_unique_dtyeps_exogs = exog_dtypes_nunique[exog_dtypes_nunique != 1].index.to_list()\n            raise TypeError(f\"Exog/s: {non_unique_dtyeps_exogs} have different dtypes in different series.\")\n\n    exog_names_in_ = list(\n        set(\n            column\n            for df in exog_dict.values()\n            if df is not None\n            for column in df.columns.to_list()\n        )\n    )\n\n    if len(set(exog_names_in_) - set(series_names_in_)) != len(exog_names_in_):\n        raise ValueError(\n            (f\"`exog` cannot contain a column named the same as one of the series.\\n\"\n             f\"    `series` columns : {series_names_in_}.\\n\"\n             f\"    `exog`   columns : {exog_names_in_}.\")\n        )\n\n    return exog_dict, exog_names_in_\n\n\ndef align_series_and_exog_multiseries(\n    series_dict: dict,\n    input_series_is_dict: bool,\n    exog_dict: dict = None\n) -> Tuple[Union[pd.Series, pd.DataFrame], Union[pd.Series, pd.DataFrame]]:\n    \"\"\"\n    Align series and exog according to their index. If needed, reindexing is\n    applied. Heading and trailing NaNs are removed from all series in \n    `series_dict`.\n\n    - If input series is a pandas DataFrame (input_series_is_dict = False),  \n    input exog (pandas Series, DataFrame or dict) must have the same index \n    (type, length and frequency). Reindexing is not applied.\n    - If input series is a dict (input_series_is_dict = True), then input \n    exog must be a dict. Both must have a pandas DatetimeIndex, but can have \n    different lengths. Reindexing is applied.\n\n    Parameters\n    ----------\n    series_dict : dict\n        Dictionary with the series used during training.\n    input_series_is_dict : bool\n        Indicates if input series argument is a dict.\n    exog_dict : dict, default `None`\n        Dictionary with the exogenous variable/s used during training.\n\n    Returns\n    -------\n    series_dict : dict\n        Dictionary with the series used during training.\n    exog_dict : dict\n        Dictionary with the exogenous variable/s used during training.\n    \n    \"\"\"\n\n    for k in series_dict.keys():\n\n        first_valid_index = series_dict[k].first_valid_index()\n        last_valid_index = series_dict[k].last_valid_index()\n\n        series_dict[k] = series_dict[k].loc[first_valid_index : last_valid_index]\n\n        if exog_dict[k] is not None:\n            if input_series_is_dict:\n                index_intersection = (\n                    series_dict[k].index.intersection(exog_dict[k].index)\n                )\n                if len(index_intersection) == 0:\n                    warnings.warn(\n                        (f\"Series '{k}' and its `exog` do not have the same index. \"\n                         f\"All exog values will be NaN for the period of the series.\"),\n                         MissingValuesWarning\n                    )\n                elif len(index_intersection) != len(series_dict[k]):\n                    warnings.warn(\n                        (f\"Series '{k}' and its `exog` do not have the same length. \"\n                         f\"Exog values will be NaN for the not matched period of the series.\"),\n                         MissingValuesWarning\n                    )  \n                exog_dict[k] = exog_dict[k].loc[index_intersection]\n                if len(index_intersection) != len(series_dict[k]):\n                    exog_dict[k] = exog_dict[k].reindex(\n                                       series_dict[k].index, \n                                       fill_value = np.nan\n                                   )\n            else:\n                exog_dict[k] = exog_dict[k].loc[first_valid_index : last_valid_index]\n\n    return series_dict, exog_dict\n\n\ndef prepare_levels_multiseries(\n    X_train_series_names_in_: list,\n    levels: Optional[Union[str, list]] = None\n) -> Tuple[list, bool]:\n    \"\"\"\n    Prepare list of levels to be predicted in multiseries Forecasters.\n\n    Parameters\n    ----------\n    X_train_series_names_in_ : list\n        Names of the series (levels) included in the matrix `X_train`.\n    levels : str, list, default `None`\n        Names of the series (levels) to be predicted.\n\n    Returns\n    -------\n    levels : list\n        Names of the series (levels) to be predicted.\n\n    \"\"\"\n\n    input_levels_is_list = False\n    if levels is None:\n        levels = X_train_series_names_in_\n    elif isinstance(levels, str):\n        levels = [levels]\n    else:\n        input_levels_is_list = True\n\n    return levels, input_levels_is_list\n\n\ndef preprocess_levels_self_last_window_multiseries(\n    levels: list,\n    input_levels_is_list: bool,\n    last_window_: dict\n) -> Tuple[list, pd.DataFrame]:\n    \"\"\"\n    Preprocess `levels` and `last_window` (when using self.last_window_) arguments \n    in multiseries Forecasters when predicting. Only levels whose last window \n    ends at the same datetime index will be predicted together.\n\n    Parameters\n    ----------\n    levels : list\n        Names of the series (levels) to be predicted.\n    input_levels_is_list : bool\n        Indicates if input levels argument is a list.\n    last_window_ : dict\n        Dictionary with the last window of each series (self.last_window_).\n\n    Returns\n    -------\n    levels : list\n        Names of the series (levels) to be predicted.\n    last_window : pandas DataFrame\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n\n    \"\"\"\n\n    available_last_windows = set() if last_window_ is None else set(last_window_.keys())\n    not_available_last_window = set(levels) - available_last_windows\n    if not_available_last_window:\n        levels = [level for level in levels \n                  if level not in not_available_last_window]\n        if not levels:\n            raise ValueError(\n                (f\"No series to predict. None of the series {not_available_last_window} \"\n                 f\"are present in `last_window_` attribute. Provide `last_window` \"\n                 f\"as argument in predict method.\")\n            )\n        else:\n            warnings.warn(\n                (f\"Levels {not_available_last_window} are excluded from \"\n                 f\"prediction since they were not stored in `last_window_` \"\n                 f\"attribute during training. If you don't want to retrain \"\n                 f\"the Forecaster, provide `last_window` as argument.\"),\n                 IgnoredArgumentWarning\n            )\n\n    last_index_levels = [\n        v.index[-1] \n        for k, v in last_window_.items()\n        if k in levels\n    ]\n    if len(set(last_index_levels)) > 1:\n        max_index_levels = max(last_index_levels)\n        selected_levels = [\n            k\n            for k, v in last_window_.items()\n            if k in levels and v.index[-1] == max_index_levels\n        ]\n\n        series_excluded_from_last_window = set(levels) - set(selected_levels)\n        levels = selected_levels\n\n        if input_levels_is_list and series_excluded_from_last_window:\n            warnings.warn(\n                (f\"Only series whose last window ends at the same index \"\n                 f\"can be predicted together. Series that do not reach \"\n                 f\"the maximum index, '{max_index_levels}', are excluded \"\n                 f\"from prediction: {series_excluded_from_last_window}.\"),\n                IgnoredArgumentWarning\n            )\n\n    last_window = pd.DataFrame(\n        {k: v \n         for k, v in last_window_.items() \n         if k in levels}\n    )\n\n    return levels, last_window\n\n\ndef prepare_residuals_multiseries(\n    levels: list,\n    use_in_sample_residuals: bool,\n    encoding: Optional[str] = None,\n    in_sample_residuals_: Optional[dict] = None,\n    out_sample_residuals_: Optional[dict] = None\n) -> Tuple[list, bool]:\n    \"\"\"\n    Prepare residuals for bootstrapping prediction in multiseries Forecasters.\n\n    Parameters\n    ----------\n    levels : list\n        Names of the series (levels) to be predicted.\n    use_in_sample_residuals : bool\n        Indicates if `forecaster.in_sample_residuals_` are used.\n    encoding : str, default `None`\n        Encoding used to identify the different series (`ForecasterRecursiveMultiSeries`).\n    in_sample_residuals_ : dict, default `None`\n        Residuals of the model when predicting training data. Only stored up to\n        1000 values in the form `{level: residuals}`. If `transformer_series` \n        is not `None`, residuals are stored in the transformed scale.\n    out_sample_residuals_ : dict, default `None`\n        Residuals of the model when predicting non-training data. Only stored\n        up to 1000 values in the form `{level: residuals}`. If `transformer_series` \n        is not `None`, residuals are assumed to be in the transformed scale. Use \n        `set_out_sample_residuals()` method to set values.\n\n    Returns\n    -------\n    levels : list\n        Names of the series (levels) to be predicted.\n    residuals : dict\n        Residuals of the model for each level to use in bootstrapping prediction.\n\n    \"\"\"\n\n    if use_in_sample_residuals:\n        unknown_levels = set(levels) - set(in_sample_residuals_.keys())\n        if unknown_levels and encoding is not None:\n            warnings.warn(\n                (f\"`levels` {unknown_levels} are not present in `forecaster.in_sample_residuals_`, \"\n                 f\"most likely because they were not present in the training data. \"\n                 f\"A random sample of the residuals from other levels will be used. \"\n                 f\"This can lead to inaccurate intervals for the unknown levels.\"),\n                 UnknownLevelWarning\n            )\n        residuals = in_sample_residuals_.copy()\n    else:\n        if out_sample_residuals_ is None:\n            raise ValueError(\n                (\"`forecaster.out_sample_residuals_` is `None`. Use \"\n                 \"`use_in_sample_residuals=True` or the \"\n                 \"`set_out_sample_residuals()` method before predicting.\")\n            )\n        else:\n            unknown_levels = set(levels) - set(out_sample_residuals_.keys())\n            if unknown_levels and encoding is not None:\n                warnings.warn(\n                    (f\"`levels` {unknown_levels} are not present in `forecaster.out_sample_residuals_`. \"\n                     f\"A random sample of the residuals from other levels will be used. \"\n                     f\"This can lead to inaccurate intervals for the unknown levels. \"\n                     f\"Otherwise, Use the `set_out_sample_residuals()` method before \"\n                     f\"predicting to set the residuals for these levels.\"),\n                     UnknownLevelWarning\n                )\n            residuals = out_sample_residuals_.copy()\n\n    check_residuals = (\n        \"forecaster.in_sample_residuals_\" if use_in_sample_residuals\n        else \"forecaster.out_sample_residuals_\"\n    )\n    for level in levels:\n        if level in unknown_levels:\n            residuals[level] = residuals['_unknown_level']\n        if residuals[level] is None or len(residuals[level]) == 0:\n            raise ValueError(\n                (f\"Not available residuals for level '{level}'. \"\n                 f\"Check `{check_residuals}`.\")\n            )\n        elif (any(element is None for element in residuals[level]) or\n              np.any(np.isnan(residuals[level]))):\n            raise ValueError(\n                (f\"forecaster residuals for level '{level}' contains `None` \"\n                 f\"or `NaNs` values. Check `{check_residuals}`.\")\n            )\n        \n    return residuals\n\n\ndef prepare_steps_direct(\n    max_step: int,\n    steps: Optional[Union[int, list]] = None\n) -> list:\n    \"\"\"\n    Prepare list of steps to be predicted in Direct Forecasters.\n\n    Parameters\n    ----------\n    max_step : int\n        Maximum number of future steps the forecaster will predict \n        when using method `predict()`.\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n    \n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n\n    Returns\n    -------\n    steps : list\n        Steps to be predicted.\n\n    \"\"\"\n\n    if isinstance(steps, int):\n        steps = list(np.arange(steps) + 1)\n    elif steps is None:\n        steps = list(np.arange(max_step) + 1)\n    elif isinstance(steps, list):\n        steps = list(np.array(steps))\n    \n    for step in steps:\n        if not isinstance(step, (int, np.int64, np.int32)):\n            raise TypeError(\n                (f\"`steps` argument must be an int, a list of ints or `None`. \"\n                 f\"Got {type(steps)}.\")\n            )\n    # Required since numpy 2.0\n    steps = [int(step) for step in steps if step is not None]\n\n    return steps\n\n\ndef set_skforecast_warnings(\n    suppress_warnings: bool,\n    action: str = 'default'\n) -> None:\n    \"\"\"\n    Set skforecast warnings action.\n\n    Parameters\n    ----------\n    suppress_warnings : bool\n        If `True`, skforecast warnings will be suppressed. If `False`, skforecast\n        warnings will be shown as default. See \n        skforecast.exceptions.warn_skforecast_categories for more information.\n    action : str, default `'default'`\n        Action to be taken when a warning is raised. See the warnings module\n        for more information.\n\n    Returns\n    -------\n    None\n    \n    \"\"\"\n\n    if suppress_warnings:\n        for category in warn_skforecast_categories:\n            warnings.filterwarnings(action, category=category)\n",
    "skforecast/recursive/_forecaster_recursive.py": "################################################################################\n#                           ForecasterRecursive                                #\n#                                                                              #\n# This work by skforecast team is licensed under the BSD 3-Clause License.     #\n################################################################################\n# coding=utf-8\n\nfrom typing import Union, Tuple, Optional, Callable\nimport warnings\nimport sys\nimport numpy as np\nimport pandas as pd\nimport inspect\nfrom copy import copy\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import clone\n\nimport skforecast\nfrom ..base import ForecasterBase\nfrom ..exceptions import DataTransformationWarning\nfrom ..utils import (\n    initialize_lags,\n    initialize_window_features,\n    initialize_weights,\n    check_select_fit_kwargs,\n    check_y,\n    check_exog,\n    get_exog_dtypes,\n    check_exog_dtypes,\n    check_predict_input,\n    check_interval,\n    preprocess_y,\n    preprocess_last_window,\n    preprocess_exog,\n    input_to_frame,\n    date_to_index_position,\n    expand_index,\n    transform_numpy,\n    transform_dataframe,\n)\nfrom ..preprocessing import TimeSeriesDifferentiator\nfrom ..preprocessing import QuantileBinner\n\n\nclass ForecasterRecursive(ForecasterBase):\n    \"\"\"\n    This class turns any regressor compatible with the scikit-learn API into a\n    recursive autoregressive (multi-step) forecaster.\n    \n    Parameters\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n    lags : int, list, numpy ndarray, range, default `None`\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n    \n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n        - `None`: no lags are included as predictors. \n    window_features : object, list, default `None`\n        Instance or list of instances used to create window features. Window features\n        are created from the original time series and are included as predictors.\n    transformer_y : object transformer (preprocessor), default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and inverse_transform.\n        ColumnTransformers are not allowed since they do not have inverse_transform method.\n        The transformation is applied to `y` before training the forecaster. \n    transformer_exog : object transformer (preprocessor), default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable, default `None`\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\n        method. The resulting `sample_weight` cannot have negative values.\n    differentiation : int, default `None`\n        Order of differencing applied to the time series before training the forecaster.\n        If `None`, no differencing is applied. The order of differentiation is the number\n        of times the differencing operation is applied to a time series. Differencing\n        involves computing the differences between consecutive data points in the series.\n        Differentiation is reversed in the output of `predict()` and `predict_interval()`.\n        **WARNING: This argument is newly introduced and requires special attention. It\n        is still experimental and may undergo changes.**\n    fit_kwargs : dict, default `None`\n        Additional arguments to be passed to the `fit` method of the regressor.\n    binner_kwargs : dict, default `None`\n        Additional arguments to pass to the `QuantileBinner` used to discretize \n        the residuals into k bins according to the predicted values associated \n        with each residual. Available arguments are: `n_bins`, `method`, `subsample`,\n        `random_state` and `dtype`. Argument `method` is passed internally to the\n        fucntion `numpy.percentile`.\n        **New in version 0.14.0**\n    forecaster_id : str, int, default `None`\n        Name used as an identifier of the forecaster.\n    \n    Attributes\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n    lags : numpy ndarray\n        Lags used as predictors.\n    lags_names : list\n        Names of the lags used as predictors.\n    max_lag : int\n        Maximum lag included in `lags`.\n    window_features : list\n        Class or list of classes used to create window features.\n    window_features_names : list\n        Names of the window features to be included in the `X_train` matrix.\n    window_features_class_names : list\n        Names of the classes used to create the window features.\n    max_size_window_features : int\n        Maximum window size required by the window features.\n    window_size : int\n        The window size needed to create the predictors. It is calculated as the \n        maximum value between `max_lag` and `max_size_window_features`. If \n        differentiation is used, `window_size` is increased by n units equal to \n        the order of differentiation so that predictors can be generated correctly.\n    transformer_y : object transformer (preprocessor)\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and inverse_transform.\n        ColumnTransformers are not allowed since they do not have inverse_transform method.\n        The transformation is applied to `y` before training the forecaster.\n    transformer_exog : object transformer (preprocessor)\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\n        method. The resulting `sample_weight` cannot have negative values.\n    differentiation : int\n        Order of differencing applied to the time series before training the forecaster.\n        If `None`, no differencing is applied. The order of differentiation is the number\n        of times the differencing operation is applied to a time series. Differencing\n        involves computing the differences between consecutive data points in the series.\n        Differentiation is reversed in the output of `predict()` and `predict_interval()`.\n        **WARNING: This argument is newly introduced and requires special attention. It\n        is still experimental and may undergo changes.**\n        **New in version 0.10.0**\n    binner : sklearn.preprocessing.KBinsDiscretizer\n        `KBinsDiscretizer` used to discretize residuals into k bins according \n        to the predicted values associated with each residual.\n        **New in version 0.12.0**\n    binner_intervals_ : dict\n        Intervals used to discretize residuals into k bins according to the predicted\n        values associated with each residual.\n        **New in version 0.12.0**\n    binner_kwargs : dict\n        Additional arguments to pass to the `QuantileBinner` used to discretize \n        the residuals into k bins according to the predicted values associated \n        with each residual. Available arguments are: `n_bins`, `method`, `subsample`,\n        `random_state` and `dtype`. Argument `method` is passed internally to the\n        fucntion `numpy.percentile`.\n        **New in version 0.14.0**\n    source_code_weight_func : str\n        Source code of the custom function used to create weights.\n    differentiation : int\n        Order of differencing applied to the time series before training the \n        forecaster.\n    differentiator : TimeSeriesDifferentiator\n        Skforecast object used to differentiate the time series.\n    last_window_ : pandas DataFrame\n        This window represents the most recent data observed by the predictor\n        during its training phase. It contains the values needed to predict the\n        next step immediately after the training data. These values are stored\n        in the original scale of the time series before undergoing any transformations\n        or differentiation. When `differentiation` parameter is specified, the\n        dimensions of the `last_window_` are expanded as many values as the order\n        of differentiation. For example, if `lags` = 7 and `differentiation` = 1,\n        `last_window_` will have 8 values.\n    index_type_ : type\n        Type of index of the input used in training.\n    index_freq_ : str\n        Frequency of Index of the input used in training.\n    training_range_ : pandas Index\n        First and last values of index of the data used during training.\n    exog_in_ : bool\n        If the forecaster has been trained using exogenous variable/s.\n    exog_names_in_ : list\n        Names of the exogenous variables used during training.\n    exog_type_in_ : type\n        Type of exogenous data (pandas Series or DataFrame) used in training.\n    exog_dtypes_in_ : dict\n        Type of each exogenous variable/s used in training. If `transformer_exog` \n        is used, the dtypes are calculated before the transformation.\n    X_train_window_features_names_out_ : list\n        Names of the window features included in the matrix `X_train` created\n        internally for training.\n    X_train_exog_names_out_ : list\n        Names of the exogenous variables included in the matrix `X_train` created\n        internally for training. It can be different from `exog_names_in_` if\n        some exogenous variables are transformed during the training process.\n    X_train_features_names_out_ : list\n        Names of columns of the matrix created internally for training.\n    fit_kwargs : dict\n        Additional arguments to be passed to the `fit` method of the regressor.\n    in_sample_residuals_ : numpy ndarray\n        Residuals of the model when predicting training data. Only stored up to\n        10_000 values. If `transformer_y` is not `None`, residuals are stored in\n        the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation.\n    in_sample_residuals_by_bin_ : dict\n        In sample residuals binned according to the predicted value each residual\n        is associated with. If `transformer_y` is not `None`, residuals are stored\n        in the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n        **New in version 0.14.0**\n    out_sample_residuals_ : numpy ndarray\n        Residuals of the model when predicting non training data. Only stored up to\n        10_000 values. If `transformer_y` is not `None`, residuals are stored in\n        the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation.\n    out_sample_residuals_by_bin_ : dict\n        Out of sample residuals binned according to the predicted value each residual\n        is associated with. If `transformer_y` is not `None`, residuals are stored\n        in the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n        **New in version 0.12.0**\n    creation_date : str\n        Date of creation.\n    is_fitted : bool\n        Tag to identify if the regressor has been fitted (trained).\n    fit_date : str\n        Date of last fit.\n    skforecast_version : str\n        Version of skforecast library used to create the forecaster.\n    python_version : str\n        Version of python used to create the forecaster.\n    forecaster_id : str, int\n        Name used as an identifier of the forecaster.\n    \n    \"\"\"\n\n    def __init__(\n        self,\n        regressor: object,\n        lags: Optional[Union[int, list, np.ndarray, range]] = None,\n        window_features: Optional[Union[object, list]] = None,\n        transformer_y: Optional[object] = None,\n        transformer_exog: Optional[object] = None,\n        weight_func: Optional[Callable] = None,\n        differentiation: Optional[int] = None,\n        fit_kwargs: Optional[dict] = None,\n        binner_kwargs: Optional[dict] = None,\n        forecaster_id: Optional[Union[str, int]] = None\n    ) -> None:\n        \n        self.regressor                          = copy(regressor)\n        self.transformer_y                      = transformer_y\n        self.transformer_exog                   = transformer_exog\n        self.weight_func                        = weight_func\n        self.source_code_weight_func            = None\n        self.differentiation                    = differentiation\n        self.differentiator                     = None\n        self.last_window_                       = None\n        self.index_type_                        = None\n        self.index_freq_                        = None\n        self.training_range_                    = None\n        self.exog_in_                           = False\n        self.exog_names_in_                     = None\n        self.exog_type_in_                      = None\n        self.exog_dtypes_in_                    = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_            = None\n        self.X_train_features_names_out_        = None\n        self.in_sample_residuals_               = None\n        self.out_sample_residuals_              = None\n        self.in_sample_residuals_by_bin_        = None\n        self.out_sample_residuals_by_bin_       = None\n        self.creation_date                      = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.is_fitted                          = False\n        self.fit_date                           = None\n        self.skforecast_version                 = skforecast.__version__\n        self.python_version                     = sys.version.split(\" \")[0]\n        self.forecaster_id                      = forecaster_id\n\n        self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\n        self.window_features, self.window_features_names, self.max_size_window_features = (\n            initialize_window_features(window_features)\n        )\n        if self.window_features is None and self.lags is None:\n            raise ValueError(\n                \"At least one of the arguments `lags` or `window_features` \"\n                \"must be different from None. This is required to create the \"\n                \"predictors used in training the forecaster.\"\n            )\n        \n        self.window_size = max(\n            [ws for ws in [self.max_lag, self.max_size_window_features] \n             if ws is not None]\n        )\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [\n                type(wf).__name__ for wf in self.window_features\n            ] \n\n        self.binner_kwargs = binner_kwargs\n        if binner_kwargs is None:\n            self.binner_kwargs = {\n                'n_bins': 10, 'method': 'linear', 'subsample': 200000,\n                'random_state': 789654, 'dtype': np.float64\n            }\n        self.binner = QuantileBinner(**self.binner_kwargs)\n        self.binner_intervals_ = None\n\n        if self.differentiation is not None:\n            if not isinstance(differentiation, int) or differentiation < 1:\n                raise ValueError(\n                    f\"Argument `differentiation` must be an integer equal to or \"\n                    f\"greater than 1. Got {differentiation}.\"\n                )\n            self.window_size += self.differentiation\n            self.differentiator = TimeSeriesDifferentiator(\n                order=self.differentiation, window_size=self.window_size\n            )\n\n        self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n            forecaster_name = type(self).__name__, \n            regressor       = regressor, \n            weight_func     = weight_func, \n            series_weights  = None\n        )\n\n        self.fit_kwargs = check_select_fit_kwargs(\n                              regressor  = regressor,\n                              fit_kwargs = fit_kwargs\n                          )\n\n    def __repr__(\n        self\n    ) -> str:\n        \"\"\"\n        Information displayed when a ForecasterRecursive object is printed.\n        \"\"\"\n\n        (\n            params,\n            _,\n            _,\n            exog_names_in_,\n            _,\n        ) = self._preprocess_repr(\n                regressor      = self.regressor,\n                exog_names_in_ = self.exog_names_in_\n            )\n        \n        params = self._format_text_repr(params)\n        exog_names_in_ = self._format_text_repr(exog_names_in_)\n\n        info = (\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"{type(self).__name__} \\n\"\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"Regressor: {type(self.regressor).__name__} \\n\"\n            f\"Lags: {self.lags} \\n\"\n            f\"Window features: {self.window_features_names} \\n\"\n            f\"Window size: {self.window_size} \\n\"\n            f\"Exogenous included: {self.exog_in_} \\n\"\n            f\"Exogenous names: {exog_names_in_} \\n\"\n            f\"Transformer for y: {self.transformer_y} \\n\"\n            f\"Transformer for exog: {self.transformer_exog} \\n\"\n            f\"Weight function included: {True if self.weight_func is not None else False} \\n\"\n            f\"Differentiation order: {self.differentiation} \\n\"\n            f\"Training range: {self.training_range_.to_list() if self.is_fitted else None} \\n\"\n            f\"Training index type: {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None} \\n\"\n            f\"Training index frequency: {self.index_freq_ if self.is_fitted else None} \\n\"\n            f\"Regressor parameters: {params} \\n\"\n            f\"fit_kwargs: {self.fit_kwargs} \\n\"\n            f\"Creation date: {self.creation_date} \\n\"\n            f\"Last fit date: {self.fit_date} \\n\"\n            f\"Skforecast version: {self.skforecast_version} \\n\"\n            f\"Python version: {self.python_version} \\n\"\n            f\"Forecaster id: {self.forecaster_id} \\n\"\n        )\n\n        return info\n\n\n    def _repr_html_(self):\n        \"\"\"\n        HTML representation of the object.\n        The \"General Information\" section is expanded by default.\n        \"\"\"\n\n        (\n            params,\n            _,\n            _,\n            exog_names_in_,\n            _,\n        ) = self._preprocess_repr(\n                regressor      = self.regressor,\n                exog_names_in_ = self.exog_names_in_\n            )\n\n        style, unique_id = self._get_style_repr_html(self.is_fitted)\n        \n        content = f\"\"\"\n        <div class=\"container-{unique_id}\">\n            <h2>{type(self).__name__}</h2>\n            <details open>\n                <summary>General Information</summary>\n                <ul>\n                    <li><strong>Regressor:</strong> {type(self.regressor).__name__}</li>\n                    <li><strong>Lags:</strong> {self.lags}</li>\n                    <li><strong>Window features:</strong> {self.window_features_names}</li>\n                    <li><strong>Window size:</strong> {self.window_size}</li>\n                    <li><strong>Exogenous included:</strong> {self.exog_in_}</li>\n                    <li><strong>Weight function included:</strong> {self.weight_func is not None}</li>\n                    <li><strong>Differentiation order:</strong> {self.differentiation}</li>\n                    <li><strong>Creation date:</strong> {self.creation_date}</li>\n                    <li><strong>Last fit date:</strong> {self.fit_date}</li>\n                    <li><strong>Skforecast version:</strong> {self.skforecast_version}</li>\n                    <li><strong>Python version:</strong> {self.python_version}</li>\n                    <li><strong>Forecaster id:</strong> {self.forecaster_id}</li>\n                </ul>\n            </details>\n            <details>\n                <summary>Exogenous Variables</summary>\n                <ul>\n                    {exog_names_in_}\n                </ul>\n            </details>\n            <details>\n                <summary>Data Transformations</summary>\n                <ul>\n                    <li><strong>Transformer for y:</strong> {self.transformer_y}</li>\n                    <li><strong>Transformer for exog:</strong> {self.transformer_exog}</li>\n                </ul>\n            </details>\n            <details>\n                <summary>Training Information</summary>\n                <ul>\n                    <li><strong>Training range:</strong> {self.training_range_.to_list() if self.is_fitted else 'Not fitted'}</li>\n                    <li><strong>Training index type:</strong> {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted'}</li>\n                    <li><strong>Training index frequency:</strong> {self.index_freq_ if self.is_fitted else 'Not fitted'}</li>\n                </ul>\n            </details>\n            <details>\n                <summary>Regressor Parameters</summary>\n                <ul>\n                    {params}\n                </ul>\n            </details>\n            <details>\n                <summary>Fit Kwargs</summary>\n                <ul>\n                    {self.fit_kwargs}\n                </ul>\n            </details>\n            <p>\n                <a href=\"https://skforecast.org/{skforecast.__version__}/api/forecasterrecursive.html\">&#128712 <strong>API Reference</strong></a>\n                &nbsp;&nbsp;\n                <a href=\"https://skforecast.org/{skforecast.__version__}/user_guides/autoregresive-forecaster.html\">&#128462 <strong>User Guide</strong></a>\n            </p>\n        </div>\n        \"\"\"\n\n        # Return the combined style and content\n        return style + content\n\n\n    def _create_lags(\n        self,\n        y: np.ndarray,\n        X_as_pandas: bool = False,\n        train_index: Optional[pd.Index] = None\n    ) -> Tuple[Optional[Union[np.ndarray, pd.DataFrame]], np.ndarray]:\n        \"\"\"\n        Create the lagged values and their target variable from a time series.\n        \n        Note that the returned matrix `X_data` contains the lag 1 in the first \n        column, the lag 2 in the in the second column and so on.\n        \n        Parameters\n        ----------\n        y : numpy ndarray\n            Training time series values.\n        X_as_pandas : bool, default `False`\n            If `True`, the returned matrix `X_data` is a pandas DataFrame.\n        train_index : pandas Index, default `None`\n            Index of the training data. It is used to create the pandas DataFrame\n            `X_data` when `X_as_pandas` is `True`.\n\n        Returns\n        -------\n        X_data : numpy ndarray, pandas DataFrame, None\n            Lagged values (predictors).\n        y_data : numpy ndarray\n            Values of the time series related to each row of `X_data`.\n        \n        \"\"\"\n\n        X_data = None\n        if self.lags is not None:\n            n_rows = len(y) - self.window_size\n            X_data = np.full(\n                shape=(n_rows, len(self.lags)), fill_value=np.nan, order='F', dtype=float\n            )\n            for i, lag in enumerate(self.lags):\n                X_data[:, i] = y[self.window_size - lag: -lag]\n\n            if X_as_pandas:\n                X_data = pd.DataFrame(\n                             data    = X_data,\n                             columns = self.lags_names,\n                             index   = train_index\n                         )\n\n        y_data = y[self.window_size:]\n\n        return X_data, y_data\n\n\n    def _create_window_features(\n        self, \n        y: pd.Series,\n        train_index: pd.Index,\n        X_as_pandas: bool = False,\n    ) -> Tuple[list, list]:\n        \"\"\"\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        train_index : pandas Index\n            Index of the training data. It is used to create the pandas DataFrame\n            `X_train_window_features` when `X_as_pandas` is `True`.\n        X_as_pandas : bool, default `False`\n            If `True`, the returned matrix `X_train_window_features` is a \n            pandas DataFrame.\n\n        Returns\n        -------\n        X_train_window_features : list\n            List of numpy ndarrays or pandas DataFrames with the window features.\n        X_train_window_features_names_out_ : list\n            Names of the window features.\n        \n        \"\"\"\n\n        len_train_index = len(train_index)\n        X_train_window_features = []\n        X_train_window_features_names_out_ = []\n        for wf in self.window_features:\n            X_train_wf = wf.transform_batch(y)\n            if not isinstance(X_train_wf, pd.DataFrame):\n                raise TypeError(\n                    (f\"The method `transform_batch` of {type(wf).__name__} \"\n                     f\"must return a pandas DataFrame.\")\n                )\n            X_train_wf = X_train_wf.iloc[-len_train_index:]\n            if not len(X_train_wf) == len_train_index:\n                raise ValueError(\n                    (f\"The method `transform_batch` of {type(wf).__name__} \"\n                     f\"must return a DataFrame with the same number of rows as \"\n                     f\"the input time series - `window_size`: {len_train_index}.\")\n                )\n            if not (X_train_wf.index == train_index).all():\n                raise ValueError(\n                    (f\"The method `transform_batch` of {type(wf).__name__} \"\n                     f\"must return a DataFrame with the same index as \"\n                     f\"the input time series - `window_size`.\")\n                )\n            \n            X_train_window_features_names_out_.extend(X_train_wf.columns)\n            if not X_as_pandas:\n                X_train_wf = X_train_wf.to_numpy()     \n            X_train_window_features.append(X_train_wf)\n\n        return X_train_window_features, X_train_window_features_names_out_\n\n\n    def _create_train_X_y(\n        self,\n        y: pd.Series,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n    ) -> Tuple[pd.DataFrame, pd.Series, list, list, list, list, dict]:\n        \"\"\"\n        Create training matrices from univariate time series and exogenous\n        variables.\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors).\n        y_train : pandas Series\n            Values of the time series related to each row of `X_train`.\n        exog_names_in_ : list\n            Names of the exogenous variables used during training.\n        X_train_window_features_names_out_ : list\n            Names of the window features included in the matrix `X_train` created\n            internally for training.\n        X_train_exog_names_out_ : list\n            Names of the exogenous variables included in the matrix `X_train` created\n            internally for training. It can be different from `exog_names_in_` if\n            some exogenous variables are transformed during the training process.\n        X_train_features_names_out_ : list\n            Names of the columns of the matrix created internally for training.\n        exog_dtypes_in_ : dict\n            Type of each exogenous variable/s used in training. If `transformer_exog` \n            is used, the dtypes are calculated before the transformation.\n        \n        \"\"\"\n\n        check_y(y=y)\n        y = input_to_frame(data=y, input_name='y')\n\n        if len(y) <= self.window_size:\n            raise ValueError(\n                (f\"Length of `y` must be greater than the maximum window size \"\n                 f\"needed by the forecaster.\\n\"\n                 f\"    Length `y`: {len(y)}.\\n\"\n                 f\"    Max window size: {self.window_size}.\\n\"\n                 f\"    Lags window size: {self.max_lag}.\\n\"\n                 f\"    Window features window size: {self.max_size_window_features}.\")\n            )\n\n        fit_transformer = False if self.is_fitted else True\n        y = transform_dataframe(\n                df                = y, \n                transformer       = self.transformer_y,\n                fit               = fit_transformer,\n                inverse_transform = False,\n            )\n        y_values, y_index = preprocess_y(y=y)\n        train_index = y_index[self.window_size:]\n\n        if self.differentiation is not None:\n            if not self.is_fitted:\n                y_values = self.differentiator.fit_transform(y_values)\n            else:\n                differentiator = copy(self.differentiator)\n                y_values = differentiator.fit_transform(y_values)\n\n        exog_names_in_ = None\n        exog_dtypes_in_ = None\n        categorical_features = False\n        if exog is not None:\n            check_exog(exog=exog, allow_nan=True)\n            exog = input_to_frame(data=exog, input_name='exog')\n\n            len_y = len(y_values)\n            len_train_index = len(train_index)\n            len_exog = len(exog)\n            if not len_exog == len_y and not len_exog == len_train_index:\n                raise ValueError(\n                    f\"Length of `exog` must be equal to the length of `y` (if index is \"\n                    f\"fully aligned) or length of `y` - `window_size` (if `exog` \"\n                    f\"starts after the first `window_size` values).\\n\"\n                    f\"    `exog`              : ({exog.index[0]} -- {exog.index[-1]})  (n={len_exog})\\n\"\n                    f\"    `y`                 : ({y.index[0]} -- {y.index[-1]})  (n={len_y})\\n\"\n                    f\"    `y` - `window_size` : ({train_index[0]} -- {train_index[-1]})  (n={len_train_index})\"\n                )\n\n            exog_names_in_ = exog.columns.to_list()\n            exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = fit_transformer,\n                       inverse_transform = False\n                   )\n\n            check_exog_dtypes(exog, call_check_exog=True)\n            categorical_features = (\n                exog.select_dtypes(include=np.number).shape[1] != exog.shape[1]\n            )\n\n            _, exog_index = preprocess_exog(exog=exog, return_values=False)\n            if len_exog == len_y:\n                if not (exog_index == y_index).all():\n                    raise ValueError(\n                        \"When `exog` has the same length as `y`, the index of \"\n                        \"`exog` must be aligned with the index of `y` \"\n                        \"to ensure the correct alignment of values.\"\n                    )\n                # The first `self.window_size` positions have to be removed from \n                # exog since they are not in X_train.\n                exog = exog.iloc[self.window_size:, ]\n            else:\n                if not (exog_index == train_index).all():\n                    raise ValueError(\n                        \"When `exog` doesn't contain the first `window_size` observations, \"\n                        \"the index of `exog` must be aligned with the index of `y` minus \"\n                        \"the first `window_size` observations to ensure the correct \"\n                        \"alignment of values.\"\n                    )\n            \n        X_train = []\n        X_train_features_names_out_ = []\n        X_as_pandas = True if categorical_features else False\n\n        X_train_lags, y_train = self._create_lags(\n            y=y_values, X_as_pandas=X_as_pandas, train_index=train_index\n        )\n        if X_train_lags is not None:\n            X_train.append(X_train_lags)\n            X_train_features_names_out_.extend(self.lags_names)\n        \n        X_train_window_features_names_out_ = None\n        if self.window_features is not None:\n            n_diff = 0 if self.differentiation is None else self.differentiation\n            y_window_features = pd.Series(y_values[n_diff:], index=y_index[n_diff:])\n            X_train_window_features, X_train_window_features_names_out_ = (\n                self._create_window_features(\n                    y=y_window_features, X_as_pandas=X_as_pandas, train_index=train_index\n                )\n            )\n            X_train.extend(X_train_window_features)\n            X_train_features_names_out_.extend(X_train_window_features_names_out_)\n\n        X_train_exog_names_out_ = None\n        if exog is not None:\n            X_train_exog_names_out_ = exog.columns.to_list()  \n            if not X_as_pandas:\n                exog = exog.to_numpy()     \n            X_train_features_names_out_.extend(X_train_exog_names_out_)\n            X_train.append(exog)\n        \n        if len(X_train) == 1:\n            X_train = X_train[0]\n        else:\n            if X_as_pandas:\n                X_train = pd.concat(X_train, axis=1)\n            else:\n                X_train = np.concatenate(X_train, axis=1)\n                \n        if X_as_pandas:\n            X_train.index = train_index\n        else:\n            X_train = pd.DataFrame(\n                          data    = X_train,\n                          index   = train_index,\n                          columns = X_train_features_names_out_\n                      )\n        \n        y_train = pd.Series(\n                      data  = y_train,\n                      index = train_index,\n                      name  = 'y'\n                  )\n\n        return (\n            X_train,\n            y_train,\n            exog_names_in_,\n            X_train_window_features_names_out_,\n            X_train_exog_names_out_,\n            X_train_features_names_out_,\n            exog_dtypes_in_\n        )\n\n\n    def create_train_X_y(\n        self,\n        y: pd.Series,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n    ) -> Tuple[pd.DataFrame, pd.Series]:\n        \"\"\"\n        Create training matrices from univariate time series and exogenous\n        variables.\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors).\n        y_train : pandas Series\n            Values of the time series related to each row of `X_data`.\n        \n        \"\"\"\n\n        output = self._create_train_X_y(y=y, exog=exog)\n\n        X_train = output[0]\n        y_train = output[1]\n\n        return X_train, y_train\n\n\n    def _train_test_split_one_step_ahead(\n        self,\n        y: pd.Series,\n        initial_train_size: int,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n    ) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n        \"\"\"\n        Create matrices needed to train and test the forecaster for one-step-ahead\n        predictions.\n\n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        initial_train_size : int\n            Initial size of the training set. It is the number of observations used\n            to train the forecaster before making the first prediction.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n        \n        Returns\n        -------\n        X_train : pandas DataFrame\n            Predictor values used to train the model.\n        y_train : pandas Series\n            Target values related to each row of `X_train`.\n        X_test : pandas DataFrame\n            Predictor values used to test the model.\n        y_test : pandas Series\n            Target values related to each row of `X_test`.\n        \n        \"\"\"\n\n        is_fitted = self.is_fitted\n        self.is_fitted = False\n        X_train, y_train, *_ = self._create_train_X_y(\n            y    = y.iloc[: initial_train_size],\n            exog = exog.iloc[: initial_train_size] if exog is not None else None\n        )\n\n        test_init = initial_train_size - self.window_size\n        self.is_fitted = True\n        X_test, y_test, *_ = self._create_train_X_y(\n            y    = y.iloc[test_init:],\n            exog = exog.iloc[test_init:] if exog is not None else None\n        )\n\n        self.is_fitted = is_fitted\n\n        return X_train, y_train, X_test, y_test\n\n\n    def create_sample_weights(\n        self,\n        X_train: pd.DataFrame,\n    ) -> np.ndarray:\n        \"\"\"\n        Crate weights for each observation according to the forecaster's attribute\n        `weight_func`.\n\n        Parameters\n        ----------\n        X_train : pandas DataFrame\n            Dataframe created with the `create_train_X_y` method, first return.\n\n        Returns\n        -------\n        sample_weight : numpy ndarray\n            Weights to use in `fit` method.\n\n        \"\"\"\n\n        sample_weight = None\n\n        if self.weight_func is not None:\n            sample_weight = self.weight_func(X_train.index)\n\n        if sample_weight is not None:\n            if np.isnan(sample_weight).any():\n                raise ValueError(\n                    \"The resulting `sample_weight` cannot have NaN values.\"\n                )\n            if np.any(sample_weight < 0):\n                raise ValueError(\n                    \"The resulting `sample_weight` cannot have negative values.\"\n                )\n            if np.sum(sample_weight) == 0:\n                raise ValueError(\n                    (\"The resulting `sample_weight` cannot be normalized because \"\n                     \"the sum of the weights is zero.\")\n                )\n\n        return sample_weight\n\n\n    def fit(\n        self,\n        y: pd.Series,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        store_last_window: bool = True,\n        store_in_sample_residuals: bool = True,\n        random_state: int = 123\n    ) -> None:\n        \"\"\"\n        Training Forecaster.\n\n        Additional arguments to be passed to the `fit` method of the regressor \n        can be added with the `fit_kwargs` argument when initializing the forecaster.\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned so\n            that y[i] is regressed on exog[i].\n        store_last_window : bool, default `True`\n            Whether or not to store the last window (`last_window_`) of training data.\n        store_in_sample_residuals : bool, default `True`\n            If `True`, in-sample residuals will be stored in the forecaster object\n            after fitting (`in_sample_residuals_` attribute).\n        random_state : int, default `123`\n            Set a seed for the random generator so that the stored sample \n            residuals are always deterministic.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        # Reset values in case the forecaster has already been fitted.\n        self.last_window_                       = None\n        self.index_type_                        = None\n        self.index_freq_                        = None\n        self.training_range_                    = None\n        self.exog_in_                           = False\n        self.exog_names_in_                     = None\n        self.exog_type_in_                      = None\n        self.exog_dtypes_in_                    = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_            = None\n        self.X_train_features_names_out_        = None\n        self.in_sample_residuals_               = None\n        self.is_fitted                          = False\n        self.fit_date                           = None\n\n        (\n            X_train,\n            y_train,\n            exog_names_in_,\n            X_train_window_features_names_out_,\n            X_train_exog_names_out_,\n            X_train_features_names_out_,\n            exog_dtypes_in_\n        ) = self._create_train_X_y(y=y, exog=exog)\n        sample_weight = self.create_sample_weights(X_train=X_train)\n\n        if sample_weight is not None:\n            self.regressor.fit(\n                X             = X_train,\n                y             = y_train,\n                sample_weight = sample_weight,\n                **self.fit_kwargs\n            )\n        else:\n            self.regressor.fit(X=X_train, y=y_train, **self.fit_kwargs)\n\n        self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n        self.X_train_features_names_out_ = X_train_features_names_out_\n\n        self.is_fitted = True\n        self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.training_range_ = preprocess_y(y=y, return_values=False)[1][[0, -1]]\n        self.index_type_ = type(X_train.index)\n        if isinstance(X_train.index, pd.DatetimeIndex):\n            self.index_freq_ = X_train.index.freqstr\n        else: \n            self.index_freq_ = X_train.index.step\n\n        if exog is not None:\n            self.exog_in_ = True\n            self.exog_type_in_ = type(exog)\n            self.exog_names_in_ = exog_names_in_\n            self.exog_dtypes_in_ = exog_dtypes_in_\n            self.X_train_exog_names_out_ = X_train_exog_names_out_\n\n        # This is done to save time during fit in functions such as backtesting()\n        if store_in_sample_residuals:\n            self._binning_in_sample_residuals(\n                y_true       = y_train.to_numpy(),\n                y_pred       = self.regressor.predict(X_train).ravel(),\n                random_state = random_state\n            )\n\n        # The last time window of training data is stored so that lags needed as\n        # predictors in the first iteration of `predict()` can be calculated. It\n        # also includes the values need to calculate the diferenctiation.\n        if store_last_window:\n            self.last_window_ = (\n                y.iloc[-self.window_size:]\n                .copy()\n                .to_frame(name=y.name if y.name is not None else 'y')\n            )\n\n\n    def _binning_in_sample_residuals(\n        self,\n        y_true: np.ndarray,\n        y_pred: np.ndarray,\n        random_state: int = 123\n    ) -> None:\n        \"\"\"\n        Binning residuals according to the predicted value each residual is\n        associated with. First a skforecast.preprocessing.QuantileBinner object\n        is fitted to the predicted values. Then, residuals are binned according\n        to the predicted value each residual is associated with. Residuals are\n        stored in the forecaster object as `in_sample_residuals_` and\n        `in_sample_residuals_by_bin_`.\n        If `transformer_y` is not `None`, `y_true` and `y_pred` are transformed\n        before calculating residuals. If `differentiation` is not `None`, `y_true`\n        and `y_pred` are differentiated before calculating residuals. If both,\n        `transformer_y` and `differentiation` are not `None`, transformation is\n        done before differentiation. The number of residuals stored per bin is\n        limited to  `10_000 // self.binner.n_bins_`. The total number of residuals\n        stored is `10_000`.\n        **New in version 0.14.0**\n\n        Parameters\n        ----------\n        y_true : numpy ndarray\n            True values of the time series.\n        y_pred : numpy ndarray\n            Predicted values of the time series.\n        random_state : int, default `123`\n            Set a seed for the random generator so that the stored sample \n            residuals are always deterministic.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        data = pd.DataFrame({'prediction': y_pred, 'residuals': (y_true - y_pred)})\n        data['bin'] = self.binner.fit_transform(y_pred).astype(int)\n        self.in_sample_residuals_by_bin_ = (\n            data.groupby('bin')['residuals'].apply(np.array).to_dict()\n        )\n\n        rng = np.random.default_rng(seed=random_state)\n        max_sample = 10_000 // self.binner.n_bins_\n        for k, v in self.in_sample_residuals_by_bin_.items():\n            \n            if len(v) > max_sample:\n                sample = v[rng.integers(low=0, high=len(v), size=max_sample)]\n                self.in_sample_residuals_by_bin_[k] = sample\n\n        self.in_sample_residuals_ = np.concatenate(list(\n            self.in_sample_residuals_by_bin_.values()\n        ))\n\n        self.binner_intervals_ = self.binner.intervals_\n\n\n    def _create_predict_inputs(\n        self,\n        steps: Union[int, str, pd.Timestamp], \n        last_window: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        predict_boot: bool = False,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = False,\n        check_inputs: bool = True,\n    ) -> Tuple[np.ndarray, Optional[np.ndarray], pd.Index, int]:\n        \"\"\"\n        Create the inputs needed for the first iteration of the prediction \n        process. As this is a recursive process, the last window is updated at \n        each iteration of the prediction process.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        predict_boot : bool, default `False`\n            If `True`, residuals are returned to generate bootstrapping predictions.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n\n        Returns\n        -------\n        last_window_values : numpy ndarray\n            Series values used to create the predictors needed in the first \n            iteration of the prediction (t + 1).\n        exog_values : numpy ndarray, None\n            Exogenous variable/s included as predictor/s.\n        prediction_index : pandas Index\n            Index of the predictions.\n        steps: int\n            Number of future steps predicted.\n        \n        \"\"\"\n\n        if last_window is None:\n            last_window = self.last_window_\n\n        if self.is_fitted:\n            steps = date_to_index_position(\n                        index        = last_window.index,\n                        date_input   = steps,\n                        date_literal = 'steps'\n                    )\n\n        if check_inputs:\n            check_predict_input(\n                forecaster_name  = type(self).__name__,\n                steps            = steps,\n                is_fitted        = self.is_fitted,\n                exog_in_         = self.exog_in_,\n                index_type_      = self.index_type_,\n                index_freq_      = self.index_freq_,\n                window_size      = self.window_size,\n                last_window      = last_window,\n                exog             = exog,\n                exog_type_in_    = self.exog_type_in_,\n                exog_names_in_   = self.exog_names_in_,\n                interval         = None\n            )\n        \n            if predict_boot and not use_in_sample_residuals:\n                if not use_binned_residuals and self.out_sample_residuals_ is None:\n                    raise ValueError(\n                        \"`forecaster.out_sample_residuals_` is `None`. Use \"\n                        \"`use_in_sample_residuals=True` or the \"\n                        \"`set_out_sample_residuals()` method before predicting.\"\n                    )\n                if use_binned_residuals and self.out_sample_residuals_by_bin_ is None:\n                    raise ValueError(\n                        \"`forecaster.out_sample_residuals_by_bin_` is `None`. Use \"\n                        \"`use_in_sample_residuals=True` or the \"\n                        \"`set_out_sample_residuals()` method before predicting.\"\n                    )\n\n        last_window = last_window.iloc[-self.window_size:].copy()\n        last_window_values, last_window_index = preprocess_last_window(\n                                                    last_window = last_window\n                                                )\n\n        last_window_values = transform_numpy(\n                                 array             = last_window_values,\n                                 transformer       = self.transformer_y,\n                                 fit               = False,\n                                 inverse_transform = False\n                             )\n        if self.differentiation is not None:\n            last_window_values = self.differentiator.fit_transform(last_window_values)\n\n        if exog is not None:\n            exog = input_to_frame(data=exog, input_name='exog')\n            exog = exog.loc[:, self.exog_names_in_]\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n            check_exog_dtypes(exog=exog)\n            exog_values = exog.to_numpy()[:steps]\n        else:\n            exog_values = None\n\n        prediction_index = expand_index(\n                               index = last_window_index,\n                               steps = steps,\n                           )\n\n        return last_window_values, exog_values, prediction_index, steps\n\n\n    def _recursive_predict(\n        self,\n        steps: int,\n        last_window_values: np.ndarray,\n        exog_values: Optional[np.ndarray] = None,\n        residuals: Optional[Union[np.ndarray, dict]] = None,\n        use_binned_residuals: bool = False,\n    ) -> np.ndarray:\n        \"\"\"\n        Predict n steps ahead. It is an iterative process in which, each prediction,\n        is used as a predictor for the next step.\n        \n        Parameters\n        ----------\n        steps : int\n            Number of future steps predicted.\n        last_window_values : numpy ndarray\n            Series values used to create the predictors needed in the first \n            iteration of the prediction (t + 1).\n        exog_values : numpy ndarray, default `None`\n            Exogenous variable/s included as predictor/s.\n        residuals : numpy ndarray, dict, default `None`\n            Residuals used to generate bootstrapping predictions.\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : numpy ndarray\n            Predicted values.\n        \n        \"\"\"\n\n        n_lags = len(self.lags) if self.lags is not None else 0\n        n_window_features = (\n            len(self.X_train_window_features_names_out_)\n            if self.window_features is not None\n            else 0\n        )\n        n_exog = exog_values.shape[1] if exog_values is not None else 0\n\n        X = np.full(\n            shape=(n_lags + n_window_features + n_exog), fill_value=np.nan, dtype=float\n        )\n        predictions = np.full(shape=steps, fill_value=np.nan, dtype=float)\n        last_window = np.concatenate((last_window_values, predictions))\n\n        for i in range(steps):\n\n            if self.lags is not None:\n                X[:n_lags] = last_window[-self.lags - (steps - i)]\n            if self.window_features is not None:\n                X[n_lags : n_lags + n_window_features] = np.concatenate(\n                    [\n                        wf.transform(last_window[i : -(steps - i)])\n                        for wf in self.window_features\n                    ]\n                )\n            if exog_values is not None:\n                X[n_lags + n_window_features:] = exog_values[i]\n        \n            pred = self.regressor.predict(X.reshape(1, -1)).ravel()\n            \n            if residuals is not None:\n                if use_binned_residuals:\n                    predicted_bin = (\n                        self.binner.transform(pred).item()\n                    )\n                    step_residual = residuals[predicted_bin][i]\n                else:\n                    step_residual = residuals[i]\n                \n                pred += step_residual\n            \n            predictions[i] = pred[0]\n\n            # Update `last_window` values. The first position is discarded and \n            # the new prediction is added at the end.\n            last_window[-(steps - i)] = pred[0]\n\n        return predictions\n\n\n    def create_predict_X(\n        self,\n        steps: int,\n        last_window: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        Create the predictors needed to predict `steps` ahead. As it is a recursive\n        process, the predictors are created at each iteration of the prediction \n        process.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n\n        Returns\n        -------\n        X_predict : pandas DataFrame\n            Pandas DataFrame with the predictors for each step. The index \n            is the same as the prediction index.\n        \n        \"\"\"\n\n        last_window_values, exog_values, prediction_index, steps = (\n            self._create_predict_inputs(steps=steps, last_window=last_window, exog=exog)\n        )\n        \n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\", \n                message=\"X does not have valid feature names\", \n                category=UserWarning\n            )\n            predictions = self._recursive_predict(\n                              steps              = steps,\n                              last_window_values = last_window_values,\n                              exog_values        = exog_values\n                          )\n\n        X_predict = []\n        full_predictors = np.concatenate((last_window_values, predictions))\n\n        if self.lags is not None:\n            idx = np.arange(-steps, 0)[:, None] - self.lags\n            X_lags = full_predictors[idx + len(full_predictors)]\n            X_predict.append(X_lags)\n\n        if self.window_features is not None:\n            X_window_features = np.full(\n                shape      = (steps, len(self.X_train_window_features_names_out_)), \n                fill_value = np.nan, \n                order      = 'C',\n                dtype      = float\n            )\n            for i in range(steps):\n                X_window_features[i, :] = np.concatenate(\n                    [wf.transform(full_predictors[i:-(steps - i)]) \n                     for wf in self.window_features]\n                )\n            X_predict.append(X_window_features)\n\n        if exog is not None:\n            X_predict.append(exog_values)\n\n        X_predict = pd.DataFrame(\n                        data    = np.concatenate(X_predict, axis=1),\n                        columns = self.X_train_features_names_out_,\n                        index   = prediction_index\n                    )\n        \n        if self.transformer_y is not None or self.differentiation is not None:\n            warnings.warn(\n                \"The output matrix is in the transformed scale due to the \"\n                \"inclusion of transformations or differentiation in the Forecaster. \"\n                \"As a result, any predictions generated using this matrix will also \"\n                \"be in the transformed scale. Please refer to the documentation \"\n                \"for more details: \"\n                \"https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html\",\n                DataTransformationWarning\n            )\n\n        return X_predict\n\n\n    def predict(\n        self,\n        steps: Union[int, str, pd.Timestamp],\n        last_window: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        check_inputs: bool = True\n    ) -> pd.Series:\n        \"\"\"\n        Predict n steps ahead. It is an recursive process in which, each prediction,\n        is used as a predictor for the next step.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n\n        Returns\n        -------\n        predictions : pandas Series\n            Predicted values.\n        \n        \"\"\"\n\n        last_window_values, exog_values, prediction_index, steps = (\n            self._create_predict_inputs(\n                steps=steps,\n                last_window=last_window,\n                exog=exog,\n                check_inputs=check_inputs,\n            )\n        )\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\", \n                message=\"X does not have valid feature names\", \n                category=UserWarning\n            )\n            predictions = self._recursive_predict(\n                              steps              = steps,\n                              last_window_values = last_window_values,\n                              exog_values        = exog_values\n                          )\n\n        if self.differentiation is not None:\n            predictions = self.differentiator.inverse_transform_next_window(predictions)\n\n        predictions = transform_numpy(\n                          array             = predictions,\n                          transformer       = self.transformer_y,\n                          fit               = False,\n                          inverse_transform = True\n                      )\n\n        predictions = pd.Series(\n                          data  = predictions,\n                          index = prediction_index,\n                          name  = 'pred'\n                      )\n\n        return predictions\n\n\n    def predict_bootstrapping(\n        self,\n        steps: Union[int, str, pd.Timestamp],\n        last_window: Optional[pd.Series] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = False\n    ) -> pd.DataFrame:\n        \"\"\"\n        Generate multiple forecasting predictions using a bootstrapping process. \n        By sampling from a collection of past observed errors (the residuals),\n        each iteration of bootstrapping generates a different set of predictions. \n        See the Notes section for more information. \n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        boot_predictions : pandas DataFrame\n            Predictions generated by bootstrapping.\n            Shape: (steps, n_boot)\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n        Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n        \"\"\"\n\n        (\n            last_window_values,\n            exog_values,\n            prediction_index,\n            steps\n        ) = self._create_predict_inputs(\n            steps                   = steps, \n            last_window             = last_window, \n            exog                    = exog,\n            predict_boot            = True, \n            use_in_sample_residuals = use_in_sample_residuals,\n            use_binned_residuals    = use_binned_residuals\n        )\n\n        if use_in_sample_residuals:\n            residuals = self.in_sample_residuals_\n            residuals_by_bin = self.in_sample_residuals_by_bin_\n        else:\n            residuals = self.out_sample_residuals_\n            residuals_by_bin = self.out_sample_residuals_by_bin_\n\n        rng = np.random.default_rng(seed=random_state)\n        if use_binned_residuals:\n            sampled_residuals = {\n                k: v[rng.integers(low=0, high=len(v), size=(steps, n_boot))]\n                for k, v in residuals_by_bin.items()\n            }\n        else:\n            sampled_residuals = residuals[\n                rng.integers(low=0, high=len(residuals), size=(steps, n_boot))\n            ]\n        \n        boot_columns = []\n        boot_predictions = np.full(\n                               shape      = (steps, n_boot),\n                               fill_value = np.nan,\n                               order      = 'F',\n                               dtype      = float\n                           )\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\", \n                message=\"X does not have valid feature names\", \n                category=UserWarning\n            )\n            for i in range(n_boot):\n\n                if use_binned_residuals:\n                    boot_sampled_residuals = {\n                        k: v[:, i]\n                        for k, v in sampled_residuals.items()\n                    }\n                else:\n                    boot_sampled_residuals = sampled_residuals[:, i]\n\n                boot_columns.append(f\"pred_boot_{i}\")\n                boot_predictions[:, i] = self._recursive_predict(\n                    steps                = steps,\n                    last_window_values   = last_window_values,\n                    exog_values          = exog_values,\n                    residuals            = boot_sampled_residuals,\n                    use_binned_residuals = use_binned_residuals,\n                )\n\n        if self.differentiation is not None:\n            boot_predictions = (\n                self.differentiator.inverse_transform_next_window(boot_predictions)\n            )\n        \n        if self.transformer_y:\n            boot_predictions = np.apply_along_axis(\n                                   func1d            = transform_numpy,\n                                   axis              = 0,\n                                   arr               = boot_predictions,\n                                   transformer       = self.transformer_y,\n                                   fit               = False,\n                                   inverse_transform = True\n                               )\n\n        boot_predictions = pd.DataFrame(\n                               data    = boot_predictions,\n                               index   = prediction_index,\n                               columns = boot_columns\n                           )\n\n        return boot_predictions\n\n\n    def predict_interval(\n        self,\n        steps: Union[int, str, pd.Timestamp],\n        last_window: Optional[pd.Series] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        interval: list = [5, 95],\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = False\n    ) -> pd.DataFrame:\n        \"\"\"\n        Iterative process in which each prediction is used as a predictor\n        for the next step, and bootstrapping is used to estimate prediction\n        intervals. Both predictions and intervals are returned.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        interval : list, default `[5, 95]`\n            Confidence of the prediction interval estimated. Sequence of \n            percentiles to compute, which must be between 0 and 100 inclusive. \n            For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Values predicted by the forecaster and their estimated interval.\n\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n\n        check_interval(interval=interval)\n\n        boot_predictions = self.predict_bootstrapping(\n                               steps                   = steps,\n                               last_window             = last_window,\n                               exog                    = exog,\n                               n_boot                  = n_boot,\n                               random_state            = random_state,\n                               use_in_sample_residuals = use_in_sample_residuals,\n                               use_binned_residuals    = use_binned_residuals\n                           )\n\n        predictions = self.predict(\n                          steps        = steps,\n                          last_window  = last_window,\n                          exog         = exog,\n                          check_inputs = False\n                      )\n\n        interval = np.array(interval) / 100\n        predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n        predictions_interval.columns = ['lower_bound', 'upper_bound']\n        predictions = pd.concat((predictions, predictions_interval), axis=1)\n\n        return predictions\n\n\n    def predict_quantiles(\n        self,\n        steps: Union[int, str, pd.Timestamp],\n        last_window: Optional[pd.Series] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        quantiles: list = [0.05, 0.5, 0.95],\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = False\n    ) -> pd.DataFrame:\n        \"\"\"\n        Calculate the specified quantiles for each step. After generating \n        multiple forecasting predictions through a bootstrapping process, each \n        quantile is calculated for each step.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        quantiles : list, default `[0.05, 0.5, 0.95]`\n            Sequence of quantiles to compute, which must be between 0 and 1 \n            inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n            `quantiles = [0.05, 0.5, 0.95]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate quantiles.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot quantiles are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create prediction quantiles. If `False`, out of\n            sample residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Quantiles predicted by the forecaster.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n\n        check_interval(quantiles=quantiles)\n\n        boot_predictions = self.predict_bootstrapping(\n                               steps                   = steps,\n                               last_window             = last_window,\n                               exog                    = exog,\n                               n_boot                  = n_boot,\n                               random_state            = random_state,\n                               use_in_sample_residuals = use_in_sample_residuals,\n                               use_binned_residuals    = use_binned_residuals\n                           )\n\n        predictions = boot_predictions.quantile(q=quantiles, axis=1).transpose()\n        predictions.columns = [f'q_{q}' for q in quantiles]\n\n        return predictions\n\n\n    def predict_dist(\n        self,\n        steps: Union[int, str, pd.Timestamp],\n        distribution: object,\n        last_window: Optional[pd.Series] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = False\n    ) -> pd.DataFrame:\n        \"\"\"\n        Fit a given probability distribution for each step. After generating \n        multiple forecasting predictions through a bootstrapping process, each \n        step is fitted to the given distribution.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        distribution : Object\n            A distribution object from scipy.stats.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).  \n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Distribution parameters estimated for each step.\n\n        \"\"\"\n\n        boot_samples = self.predict_bootstrapping(\n                           steps                   = steps,\n                           last_window             = last_window,\n                           exog                    = exog,\n                           n_boot                  = n_boot,\n                           random_state            = random_state,\n                           use_in_sample_residuals = use_in_sample_residuals,\n                           use_binned_residuals    = use_binned_residuals\n                       )       \n\n        param_names = [p for p in inspect.signature(distribution._pdf).parameters\n                       if not p == 'x'] + [\"loc\", \"scale\"]\n        param_values = np.apply_along_axis(\n                           lambda x: distribution.fit(x),\n                           axis = 1,\n                           arr  = boot_samples\n                       )\n        predictions = pd.DataFrame(\n                          data    = param_values,\n                          columns = param_names,\n                          index   = boot_samples.index\n                      )\n\n        return predictions\n\n    def set_params(\n        self, \n        params: dict\n    ) -> None:\n        \"\"\"\n        Set new values to the parameters of the scikit learn model stored in the\n        forecaster.\n        \n        Parameters\n        ----------\n        params : dict\n            Parameters values.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        self.regressor = clone(self.regressor)\n        self.regressor.set_params(**params)\n\n    def set_fit_kwargs(\n        self, \n        fit_kwargs: dict\n    ) -> None:\n        \"\"\"\n        Set new values for the additional keyword arguments passed to the `fit` \n        method of the regressor.\n        \n        Parameters\n        ----------\n        fit_kwargs : dict\n            Dict of the form {\"argument\": new_value}.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n\n    def set_lags(\n        self, \n        lags: Optional[Union[int, list, np.ndarray, range]] = None\n    ) -> None:\n        \"\"\"\n        Set new value to the attribute `lags`. Attributes `lags_names`, \n        `max_lag` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        lags : int, list, numpy ndarray, range, default `None`\n            Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. \n        \n            - `int`: include lags from 1 to `lags` (included).\n            - `list`, `1d numpy ndarray` or `range`: include only lags present in \n            `lags`, all elements must be int.\n            - `None`: no lags are included as predictors. \n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        if self.window_features is None and lags is None:\n            raise ValueError(\n                \"At least one of the arguments `lags` or `window_features` \"\n                \"must be different from None. This is required to create the \"\n                \"predictors used in training the forecaster.\"\n            )\n        \n        self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\n        self.window_size = max(\n            [ws for ws in [self.max_lag, self.max_size_window_features] \n             if ws is not None]\n        )\n        if self.differentiation is not None:\n            self.window_size += self.differentiation\n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_window_features(\n        self, \n        window_features: Optional[Union[object, list]] = None\n    ) -> None:\n        \"\"\"\n        Set new value to the attribute `window_features`. Attributes \n        `max_size_window_features`, `window_features_names`, \n        `window_features_class_names` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        window_features : object, list, default `None`\n            Instance or list of instances used to create window features. Window features\n            are created from the original time series and are included as predictors.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        if window_features is None and self.lags is None:\n            raise ValueError(\n                \"At least one of the arguments `lags` or `window_features` \"\n                \"must be different from None. This is required to create the \"\n                \"predictors used in training the forecaster.\"\n            )\n        \n        self.window_features, self.window_features_names, self.max_size_window_features = (\n            initialize_window_features(window_features)\n        )\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [\n                type(wf).__name__ for wf in self.window_features\n            ] \n        self.window_size = max(\n            [ws for ws in [self.max_lag, self.max_size_window_features] \n             if ws is not None]\n        )\n        if self.differentiation is not None:\n            self.window_size += self.differentiation\n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_out_sample_residuals(\n        self,\n        y_true: Union[pd.Series, np.ndarray],\n        y_pred: Union[pd.Series, np.ndarray],\n        append: bool = False,\n        random_state: int = 123\n    ) -> None:\n        \"\"\"\n        Set new values to the attribute `out_sample_residuals_`. Out of sample\n        residuals are meant to be calculated using observations that did not\n        participate in the training process. `y_true` and `y_pred` are expected\n        to be in the original scale of the time series. Residuals are calculated\n        as `y_true` - `y_pred`, after applying the necessary transformations and\n        differentiations if the forecaster includes them (`self.transformer_y`\n        and `self.differentiation`). Two internal attributes are updated:\n\n        + `out_sample_residuals_`: residuals stored in a numpy ndarray.\n        + `out_sample_residuals_by_bin_`: residuals are binned according to the\n        predicted value they are associated with and stored in a dictionary, where\n        the keys are the  intervals of the predicted values and the values are\n        the residuals associated with that range. If a bin binning is empty, it\n        is filled with a random sample of residuals from other bins. This is done\n        to ensure that all bins have at least one residual and can be used in the\n        prediction process.\n\n        A total of 10_000 residuals are stored in the attribute `out_sample_residuals_`.\n        If the number of residuals is greater than 10_000, a random sample of\n        10_000 residuals is stored. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n        \n        Parameters\n        ----------\n        y_true : pandas Series, numpy ndarray, default `None`\n            True values of the time series from which the residuals have been\n            calculated.\n        y_pred : pandas Series, numpy ndarray, default `None`\n            Predicted values of the time series.\n        append : bool, default `False`\n            If `True`, new residuals are added to the once already stored in the\n            forecaster. If after appending the new residuals, the limit of\n            `10_000 // self.binner.n_bins_` values per bin is reached, a random\n            sample of residuals is stored.\n        random_state : int, default `123`\n            Sets a seed to the random sampling for reproducible output.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n\n        if not self.is_fitted:\n            raise NotFittedError(\n                \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n                \"arguments before using `set_out_sample_residuals()`.\"\n            )\n\n        if not isinstance(y_true, (np.ndarray, pd.Series)):\n            raise TypeError(\n                f\"`y_true` argument must be `numpy ndarray` or `pandas Series`. \"\n                f\"Got {type(y_true)}.\"\n            )\n        \n        if not isinstance(y_pred, (np.ndarray, pd.Series)):\n            raise TypeError(\n                f\"`y_pred` argument must be `numpy ndarray` or `pandas Series`. \"\n                f\"Got {type(y_pred)}.\"\n            )\n        \n        if len(y_true) != len(y_pred):\n            raise ValueError(\n                f\"`y_true` and `y_pred` must have the same length. \"\n                f\"Got {len(y_true)} and {len(y_pred)}.\"\n            )\n        \n        if isinstance(y_true, pd.Series) and isinstance(y_pred, pd.Series):\n            if not y_true.index.equals(y_pred.index):\n                raise ValueError(\n                    \"`y_true` and `y_pred` must have the same index.\"\n                )\n\n        if not isinstance(y_pred, np.ndarray):\n            y_pred = y_pred.to_numpy()\n\n        if not isinstance(y_true, np.ndarray):\n            y_true = y_true.to_numpy()\n\n        if self.transformer_y:\n            y_true = transform_numpy(\n                         array             = y_true,\n                         transformer       = self.transformer_y,\n                         fit               = False,\n                         inverse_transform = False\n                     )\n            y_pred = transform_numpy(\n                         array             = y_pred,\n                         transformer       = self.transformer_y,\n                         fit               = False,\n                         inverse_transform = False\n                     )\n        \n        if self.differentiation is not None:\n            differentiator = copy(self.differentiator)\n            differentiator.set_params(window_size=None)\n            y_true = differentiator.fit_transform(y_true)[self.differentiation:]\n            y_pred = differentiator.fit_transform(y_pred)[self.differentiation:]\n        \n        residuals = y_true - y_pred\n        data = pd.DataFrame({'prediction': y_pred, 'residuals': residuals})\n        data['bin'] = self.binner.transform(y_pred).astype(int)\n        residuals_by_bin = data.groupby('bin')['residuals'].apply(np.array).to_dict()\n\n        if append and self.out_sample_residuals_by_bin_ is not None:\n            for k, v in residuals_by_bin.items():\n                if k in self.out_sample_residuals_by_bin_:\n                    self.out_sample_residuals_by_bin_[k] = np.concatenate((\n                        self.out_sample_residuals_by_bin_[k], v)\n                    )\n                else:\n                    self.out_sample_residuals_by_bin_[k] = v\n        else:\n            self.out_sample_residuals_by_bin_ = residuals_by_bin\n\n        max_samples = 10_000 // self.binner.n_bins_\n        rng = np.random.default_rng(seed=random_state)\n        for k, v in self.out_sample_residuals_by_bin_.items():\n            if len(v) > max_samples:\n                sample = rng.choice(a=v, size=max_samples, replace=False)\n                self.out_sample_residuals_by_bin_[k] = sample\n\n        for k in self.in_sample_residuals_by_bin_.keys():\n            if k not in self.out_sample_residuals_by_bin_:\n                self.out_sample_residuals_by_bin_[k] = np.array([])\n\n        empty_bins = [\n            k for k, v in self.out_sample_residuals_by_bin_.items() \n            if len(v) == 0\n        ]\n        if empty_bins:\n            warnings.warn(\n                f\"The following bins have no out of sample residuals: {empty_bins}. \"\n                f\"No predicted values fall in the interval \"\n                f\"{[self.binner_intervals_[bin] for bin in empty_bins]}. \"\n                f\"Empty bins will be filled with a random sample of residuals.\"\n            )\n            for k in empty_bins:\n                self.out_sample_residuals_by_bin_[k] = rng.choice(\n                    a       = residuals,\n                    size    = max_samples,\n                    replace = True\n                )\n\n        self.out_sample_residuals_ = np.concatenate(list(\n                                         self.out_sample_residuals_by_bin_.values()\n                                     ))\n\n    def get_feature_importances(\n        self,\n        sort_importance: bool = True\n    ) -> pd.DataFrame:\n        \"\"\"\n        Return feature importances of the regressor stored in the forecaster.\n        Only valid when regressor stores internally the feature importances in the\n        attribute `feature_importances_` or `coef_`. Otherwise, returns `None`.\n\n        Parameters\n        ----------\n        sort_importance: bool, default `True`\n            If `True`, sorts the feature importances in descending order.\n\n        Returns\n        -------\n        feature_importances : pandas DataFrame\n            Feature importances associated with each predictor.\n\n        \"\"\"\n\n        if not self.is_fitted:\n            raise NotFittedError(\n                \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n                \"arguments before using `get_feature_importances()`.\"\n            )\n\n        if isinstance(self.regressor, Pipeline):\n            estimator = self.regressor[-1]\n        else:\n            estimator = self.regressor\n\n        if hasattr(estimator, 'feature_importances_'):\n            feature_importances = estimator.feature_importances_\n        elif hasattr(estimator, 'coef_'):\n            feature_importances = estimator.coef_\n        else:\n            warnings.warn(\n                f\"Impossible to access feature importances for regressor of type \"\n                f\"{type(estimator)}. This method is only valid when the \"\n                f\"regressor stores internally the feature importances in the \"\n                f\"attribute `feature_importances_` or `coef_`.\"\n            )\n            feature_importances = None\n\n        if feature_importances is not None:\n            feature_importances = pd.DataFrame({\n                                      'feature': self.X_train_features_names_out_,\n                                      'importance': feature_importances\n                                  })\n            if sort_importance:\n                feature_importances = feature_importances.sort_values(\n                                          by='importance', ascending=False\n                                      )\n\n        return feature_importances\n",
    "skforecast/preprocessing/preprocessing.py": "################################################################################\n#                           skforecast.preprocessing                           #\n#                                                                              #\n# This work by skforecast team is licensed under the BSD 3-Clause License.     #\n################################################################################\n# coding=utf-8\n\nfrom typing import Any, Union, Optional\nfrom typing_extensions import Self\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom sklearn.base import BaseEstimator\nfrom sklearn.base import TransformerMixin\nfrom sklearn.exceptions import NotFittedError\nfrom ..exceptions import MissingValuesWarning\nfrom numba import njit\n\n\ndef _check_X_numpy_ndarray_1d(ensure_1d=True):\n    \"\"\"\n    This decorator checks if the argument X is a numpy ndarray with 1 dimension.\n\n    Parameters\n    ----------\n    ensure_1d : bool, default=True\n        Whether to ensure if X is a 1D numpy array.\n    \n    Returns\n    -------\n    decorator : Callable\n        A decorator function.\n\n    \"\"\"\n\n    def decorator(func):\n        def wrapper(self, *args, **kwargs):\n\n            if args:\n                X = args[0] \n            elif 'X' in kwargs:\n                X = kwargs['X']\n            else:\n                raise ValueError(\"Methods must be called with 'X' as argument.\")\n\n            if not isinstance(X, np.ndarray):\n                raise TypeError(f\"'X' must be a numpy ndarray. Found {type(X)}.\")\n            if ensure_1d and not X.ndim == 1:\n                raise ValueError(f\"'X' must be a 1D array. Found {X.ndim} dimensions.\")\n            \n            result = func(self, *args, **kwargs)\n            \n            return result\n        \n        return wrapper\n    \n    return decorator\n\n\nclass TimeSeriesDifferentiator(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transforms a time series into a differentiated time series of a specified order\n    and provides functionality to revert the differentiation. \n    \n    When using a `direct` module Forecaster, the model in step 1 must be \n    used if you want to reverse the differentiation of the training time \n    series with the `inverse_transform_training` method.\n\n    Parameters\n    ----------\n    order : int\n        The order of differentiation to be applied.\n    window_size : int, default None\n        The window size used by the forecaster. This is required to revert the \n        differentiation for the target variable `y` or its predicted values.\n\n    Attributes\n    ----------\n    order : int\n        The order of differentiation.\n    initial_values : list\n        List with the first value of the time series before each differentiation.\n        If `order = 2`, first value correspond with the first value of the original\n        time series and the second value correspond with the first value of the\n        differentiated time series of order 1. These values are necessary to \n        revert the differentiation and reconstruct the original time series.\n    pre_train_values : list\n        List with the first training value of the time series before each differentiation.\n        For `order = 1`, the value correspond with the last value of the window used to\n        create the predictors. For order > 1, the value correspond with the first\n        value of the differentiated time series prior to the next differentiation.\n        These values are necessary to revert the differentiation and reconstruct the\n        training time series.\n    last_values : list\n        List with the last value of the time series before each differentiation, \n        used to revert differentiation on subsequent data windows. If `order = 2`, \n        first value correspond with the last value of the original time series \n        and the second value correspond with the last value of the differentiated \n        time series of order 1. This is essential for correctly transforming a \n        time series that follows immediately after the series used to fit the \n        transformer.\n\n    \"\"\"\n\n    def __init__(\n        self, \n        order: int = 1,\n        window_size: int = None\n    ) -> None:\n\n        if not isinstance(order, (int, np.integer)):\n            raise TypeError(\n                f\"Parameter `order` must be an integer greater than 0. Found {type(order)}.\"\n            )\n        if order < 1:\n            raise ValueError(\n                f\"Parameter `order` must be an integer greater than 0. Found {order}.\"\n            )\n\n        if window_size is not None:\n            if not isinstance(window_size, (int, np.integer)):\n                raise TypeError(\n                    f\"Parameter `window_size` must be an integer greater than 0. \"\n                    f\"Found {type(window_size)}.\"\n                )\n            if window_size < 1:\n                raise ValueError(\n                    f\"Parameter `window_size` must be an integer greater than 0. \"\n                    f\"Found {window_size}.\"\n                )\n\n        self.order = order\n        self.window_size = window_size\n        self.initial_values = []\n        self.pre_train_values = []\n        self.last_values = []\n\n    def __repr__(\n        self\n    ) -> str:\n        \"\"\"\n        Information displayed when printed.\n        \"\"\"\n            \n        return (\n            f\"TimeSeriesDifferentiator(order={self.order}, window_size={self.window_size})\"\n        )\n\n    @_check_X_numpy_ndarray_1d()\n    def fit(\n        self, \n        X: np.ndarray, \n        y: Any = None\n    ) -> Self:\n        \"\"\"\n        Fits the transformer. Stores the values needed to revert the \n        differentiation of different window of the time series, original \n        time series, training time series, and a time series that follows\n        immediately after the series used to fit the transformer.\n\n        Parameters\n        ----------\n        X : numpy ndarray\n            Time series to be differentiated.\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self : TimeSeriesDifferentiator\n\n        \"\"\"\n\n        self.initial_values = []\n        self.pre_train_values = []\n        self.last_values = []\n\n        for i in range(self.order):\n            if i == 0:\n                self.initial_values.append(X[0])\n                if self.window_size is not None:\n                    self.pre_train_values.append(X[self.window_size - self.order])\n                self.last_values.append(X[-1])\n                X_diff = np.diff(X, n=1)\n            else:\n                self.initial_values.append(X_diff[0])\n                if self.window_size is not None:\n                    self.pre_train_values.append(X_diff[self.window_size - self.order])\n                self.last_values.append(X_diff[-1])\n                X_diff = np.diff(X_diff, n=1)\n\n        return self\n\n    @_check_X_numpy_ndarray_1d()\n    def transform(\n        self, \n        X: np.ndarray, \n        y: Any = None\n    ) -> np.ndarray:\n        \"\"\"\n        Transforms a time series into a differentiated time series of order n.\n\n        Parameters\n        ----------\n        X : numpy ndarray\n            Time series to be differentiated.\n        y : Ignored\n            Not used, present here for API consistency by convention.\n        \n        Returns\n        -------\n        X_diff : numpy ndarray\n            Differentiated time series. The length of the array is the same as\n            the original time series but the first n `order` values are nan.\n\n        \"\"\"\n\n        X_diff = np.diff(X, n=self.order)\n        X_diff = np.append((np.full(shape=self.order, fill_value=np.nan)), X_diff)\n\n        return X_diff\n\n    @_check_X_numpy_ndarray_1d()\n    def inverse_transform(\n        self, \n        X: np.ndarray, \n        y: Any = None\n    ) -> np.ndarray:\n        \"\"\"\n        Reverts the differentiation. To do so, the input array is assumed to be\n        the same time series used to fit the transformer but differentiated.\n\n        Parameters\n        ----------\n        X : numpy ndarray\n            Differentiated time series.\n        y : Ignored\n            Not used, present here for API consistency by convention.\n        \n        Returns\n        -------\n        X_diff : numpy ndarray\n            Reverted differentiated time series.\n        \n        \"\"\"\n\n        # Remove initial nan values if present\n        X = X[np.argmax(~np.isnan(X)):]\n        for i in range(self.order):\n            if i == 0:\n                X_undiff = np.insert(X, 0, self.initial_values[-1])\n                X_undiff = np.cumsum(X_undiff, dtype=float)\n            else:\n                X_undiff = np.insert(X_undiff, 0, self.initial_values[-(i + 1)])\n                X_undiff = np.cumsum(X_undiff, dtype=float)\n\n        return X_undiff\n\n    @_check_X_numpy_ndarray_1d()\n    def inverse_transform_training(\n        self, \n        X: np.ndarray, \n        y: Any = None\n    ) -> np.ndarray:\n        \"\"\"\n        Reverts the differentiation. To do so, the input array is assumed to be\n        the differentiated training time series generated with the original \n        time series used to fit the transformer.\n\n        When using a `direct` module Forecaster, the model in step 1 must be \n        used if you want to reverse the differentiation of the training time \n        series with the `inverse_transform_training` method.\n\n        Parameters\n        ----------\n        X : numpy ndarray\n            Differentiated time series.\n        y : Ignored\n            Not used, present here for API consistency by convention.\n        \n        Returns\n        -------\n        X_diff : numpy ndarray\n            Reverted differentiated time series.\n        \n        \"\"\"\n\n        if not self.pre_train_values:\n            raise ValueError(\n                \"The `window_size` parameter must be set before fitting the \"\n                \"transformer to revert the differentiation of the training \"\n                \"time series.\"\n            )\n\n        # Remove initial nan values if present\n        X = X[np.argmax(~np.isnan(X)):]\n        for i in range(self.order):\n            if i == 0:\n                X_undiff = np.insert(X, 0, self.pre_train_values[-1])\n                X_undiff = np.cumsum(X_undiff, dtype=float)\n            else:\n                X_undiff = np.insert(X_undiff, 0, self.pre_train_values[-(i + 1)])\n                X_undiff = np.cumsum(X_undiff, dtype=float)\n\n        # Remove initial values as they are not part of the training time series\n        X_undiff = X_undiff[self.order:]\n\n        return X_undiff\n\n    @_check_X_numpy_ndarray_1d(ensure_1d=False)\n    def inverse_transform_next_window(\n        self,\n        X: np.ndarray,\n        y: Any = None\n    ) -> np.ndarray:\n        \"\"\"\n        Reverts the differentiation. The input array `X` is assumed to be a \n        differentiated time series of order n that starts right after the\n        the time series used to fit the transformer.\n\n        Parameters\n        ----------\n        X : numpy ndarray\n            Differentiated time series. It is assumed o start right after\n            the time series used to fit the transformer.\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        X_undiff : numpy ndarray\n            Reverted differentiated time series.\n        \n        \"\"\"\n        \n        array_ndim = X.ndim\n        if array_ndim == 1:\n            X = X[:, np.newaxis]\n\n        # Remove initial rows with nan values if present\n        X = X[~np.isnan(X).any(axis=1)]\n\n        for i in range(self.order):\n            if i == 0:\n                X_undiff = np.cumsum(X, axis=0, dtype=float) + self.last_values[-1]\n            else:\n                X_undiff = np.cumsum(X_undiff, axis=0, dtype=float) + self.last_values[-(i + 1)]\n\n        if array_ndim == 1:\n            X_undiff = X_undiff.ravel()\n\n        return X_undiff\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of the TimeSeriesDifferentiator.\n        \n        Parameters\n        ----------\n        params : dict\n            A dictionary of the parameters to set.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        for param, value in params.items():\n            setattr(self, param, value)\n\n\ndef series_long_to_dict(\n    data: pd.DataFrame,\n    series_id: str,\n    index: str,\n    values: str,\n    freq: str,\n    suppress_warnings: bool = False\n) -> dict:\n    \"\"\"\n    Convert long format series to dictionary of pandas Series with frequency.\n    Input data must be a pandas DataFrame with columns for the series identifier,\n    time index, and values. The function will group the data by the series\n    identifier and convert the time index to a datetime index with the given\n    frequency.\n\n    Parameters\n    ----------\n    data: pandas DataFrame\n        Long format series.\n    series_id: str\n        Column name with the series identifier.\n    index: str\n        Column name with the time index.\n    values: str\n        Column name with the values.\n    freq: str\n        Frequency of the series.\n    suppress_warnings: bool, default `False`\n        If True, suppress warnings when a series is incomplete after setting the\n        frequency.\n\n    Returns\n    -------\n    series_dict: dict\n        Dictionary with the series.\n\n    \"\"\"\n\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"`data` must be a pandas DataFrame.\")\n\n    for col in [series_id, index, values]:\n        if col not in data.columns:\n            raise ValueError(f\"Column '{col}' not found in `data`.\")\n        \n    original_sizes = data.groupby(series_id).size()\n    series_dict = {}\n    for k, v in data.groupby(series_id):\n        series_dict[k] = v.set_index(index)[values].asfreq(freq).rename(k)\n        series_dict[k].index.name = None\n        if not suppress_warnings and len(series_dict[k]) != original_sizes[k]:\n            warnings.warn(\n                f\"Series '{k}' is incomplete. NaNs have been introduced after \"\n                f\"setting the frequency.\",\n                MissingValuesWarning\n            )\n\n    return series_dict\n\n\ndef exog_long_to_dict(\n    data: pd.DataFrame,\n    series_id: str,\n    index: str,\n    freq: str,\n    dropna: bool = False,\n    suppress_warnings: bool = False\n) -> dict:\n    \"\"\"\n    Convert long format exogenous variables to dictionary. Input data must be a\n    pandas DataFrame with columns for the series identifier, time index, and\n    exogenous variables. The function will group the data by the series identifier\n    and convert the time index to a datetime index with the given frequency.\n\n    Parameters\n    ----------\n    data: pandas DataFrame\n        Long format exogenous variables.\n    series_id: str\n        Column name with the series identifier.\n    index: str\n        Column name with the time index.\n    freq: str\n        Frequency of the series.\n    dropna: bool, default False\n        If True, drop columns with all values as NaN. This is useful when\n        there are series without some exogenous variables.\n    suppress_warnings: bool, default False\n        If True, suppress warnings when exog is incomplete after setting the\n        frequency.\n        \n    Returns\n    -------\n    exog_dict: dict\n        Dictionary with the exogenous variables.\n\n    \"\"\"\n\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"`data` must be a pandas DataFrame.\")\n\n    for col in [series_id, index]:\n        if col not in data.columns:\n            raise ValueError(f\"Column '{col}' not found in `data`.\")\n\n    original_sizes = data.groupby(series_id).size()\n    exog_dict = dict(tuple(data.groupby(series_id)))\n    exog_dict = {\n        k: v.set_index(index).asfreq(freq).drop(columns=series_id)\n        for k, v in exog_dict.items()\n    }\n\n    for k in exog_dict.keys():\n        exog_dict[k].index.name = None\n\n    if dropna:\n        exog_dict = {k: v.dropna(how=\"all\", axis=1) for k, v in exog_dict.items()}\n    else: \n        if not suppress_warnings:\n            for k, v in exog_dict.items():\n                if len(v) != original_sizes[k]:\n                    warnings.warn(\n                        f\"Exogenous variables for series '{k}' are incomplete. \"\n                        f\"NaNs have been introduced after setting the frequency.\",\n                        MissingValuesWarning\n                    )\n\n    return exog_dict\n\n\ndef create_datetime_features(\n    X: Union[pd.Series, pd.DataFrame],\n    features: Optional[list] = None,\n    encoding: str = \"cyclical\",\n    max_values: Optional[dict] = None,\n) -> pd.DataFrame:\n    \"\"\"\n    Extract datetime features from the DateTime index of a pandas DataFrame or Series.\n\n    Parameters\n    ----------\n    X : pandas Series, pandas DataFrame\n        Input DataFrame or Series with a datetime index.\n    features : list, default `None`\n        List of calendar features (strings) to extract from the index. When `None`,\n        the following features are extracted: 'year', 'month', 'week', 'day_of_week',\n        'day_of_month', 'day_of_year', 'weekend', 'hour', 'minute', 'second'.\n    encoding : str, default `'cyclical'`\n        Encoding method for the extracted features. Options are None, 'cyclical' or\n        'onehot'.\n    max_values : dict, default `None`\n        Dictionary of maximum values for the cyclical encoding of calendar features.\n        When `None`, the following values are used: {'month': 12, 'week': 52, \n        'day_of_week': 7, 'day_of_month': 31, 'day_of_year': 365, 'hour': 24, \n        'minute': 60, 'second': 60}.\n\n    Returns\n    -------\n    X_new : pandas DataFrame\n        DataFrame with the extracted (and optionally encoded) datetime features.\n    \n    \"\"\"\n\n    if not isinstance(X, (pd.DataFrame, pd.Series)):\n        raise TypeError(\"Input `X` must be a pandas Series or DataFrame\")\n    if not isinstance(X.index, pd.DatetimeIndex):\n        raise TypeError(\"Input `X` must have a pandas DatetimeIndex\")\n    if encoding not in [\"cyclical\", \"onehot\", None]:\n        raise ValueError(\"Encoding must be one of 'cyclical', 'onehot' or None\")\n\n    default_features = [\n        \"year\",\n        \"month\",\n        \"week\",\n        \"day_of_week\",\n        \"day_of_month\",\n        \"day_of_year\",\n        \"weekend\",\n        \"hour\",\n        \"minute\",\n        \"second\",\n    ]\n    features = features or default_features\n\n    default_max_values = {\n        \"month\": 12,\n        \"week\": 52,\n        \"day_of_week\": 7,\n        \"day_of_month\": 31,\n        \"day_of_year\": 365,\n        \"hour\": 24,\n        \"minute\": 60,\n        \"second\": 60,\n    }\n    max_values = max_values or default_max_values\n\n    X_new = pd.DataFrame(index=X.index)\n\n    datetime_attrs = {\n        \"year\": \"year\",\n        \"month\": \"month\",\n        \"week\": lambda idx: idx.isocalendar().week,\n        \"day_of_week\": \"dayofweek\",\n        \"day_of_year\": \"dayofyear\",\n        \"day_of_month\": \"day\",\n        \"weekend\": lambda idx: (idx.weekday >= 5).astype(int),\n        \"hour\": \"hour\",\n        \"minute\": \"minute\",\n        \"second\": \"second\",\n    }\n\n    not_supported_features = set(features) - set(datetime_attrs.keys())\n    if not_supported_features:\n        raise ValueError(\n            f\"Features {not_supported_features} are not supported. \"\n            f\"Supported features are {list(datetime_attrs.keys())}.\"\n        )\n\n    for feature in features:\n        attr = datetime_attrs[feature]\n        X_new[feature] = (\n            attr(X.index) if callable(attr) else getattr(X.index, attr).astype(int)\n        )\n\n    if encoding == \"cyclical\":\n        cols_to_drop = []\n        for feature, max_val in max_values.items():\n            if feature in X_new.columns:\n                X_new[f\"{feature}_sin\"] = np.sin(2 * np.pi * X_new[feature] / max_val)\n                X_new[f\"{feature}_cos\"] = np.cos(2 * np.pi * X_new[feature] / max_val)\n                cols_to_drop.append(feature)\n        X_new = X_new.drop(columns=cols_to_drop)\n    elif encoding == \"onehot\":\n        X_new = pd.get_dummies(\n            X_new, columns=features, drop_first=False, sparse=False, dtype=int\n        )\n\n    return X_new\n\n\nclass DateTimeFeatureTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    A transformer for extracting datetime features from the DateTime index of a\n    pandas DataFrame or Series. It can also apply encoding to the extracted features.\n\n    Parameters\n    ----------\n    features : list, default `None`\n        List of calendar features (strings) to extract from the index. When `None`,\n        the following features are extracted: 'year', 'month', 'week', 'day_of_week',\n        'day_of_month', 'day_of_year', 'weekend', 'hour', 'minute', 'second'.\n    encoding : str, default `'cyclical'`\n        Encoding method for the extracted features. Options are None, 'cyclical' or\n        'onehot'.\n    max_values : dict, default `None`\n        Dictionary of maximum values for the cyclical encoding of calendar features.\n        When `None`, the following values are used: {'month': 12, 'week': 52, \n        'day_of_week': 7, 'day_of_month': 31, 'day_of_year': 365, 'hour': 24, \n        'minute': 60, 'second': 60}.\n    \n    Attributes\n    ----------\n    features : list\n        List of calendar features to extract from the index.\n    encoding : str\n        Encoding method for the extracted features.\n    max_values : dict\n        Dictionary of maximum values for the cyclical encoding of calendar features.\n    \n    \"\"\"\n\n    def __init__(\n        self,\n        features: Optional[list] = None,\n        encoding: str = \"cyclical\",\n        max_values: Optional[dict] = None\n    ) -> None:\n\n        if encoding not in [\"cyclical\", \"onehot\", None]:\n            raise ValueError(\"Encoding must be one of 'cyclical', 'onehot' or None\")\n\n        self.features = (\n            features\n            if features is not None\n            else [\n                \"year\",\n                \"month\",\n                \"week\",\n                \"day_of_week\",\n                \"day_of_month\",\n                \"day_of_year\",\n                \"weekend\",\n                \"hour\",\n                \"minute\",\n                \"second\",\n            ]\n        )\n        self.encoding = encoding\n        self.max_values = (\n            max_values\n            if max_values is not None\n            else {\n                \"month\": 12,\n                \"week\": 52,\n                \"day_of_week\": 7,\n                \"day_of_month\": 31,\n                \"day_of_year\": 365,\n                \"hour\": 24,\n                \"minute\": 60,\n                \"second\": 60,\n            }\n        )\n\n    def fit(self, X, y=None):\n        \"\"\"\n        A no-op method to satisfy the scikit-learn API.\n        \"\"\"\n        return self\n\n    def transform(\n        self,\n        X: Union[pd.Series, pd.DataFrame]\n    ) -> pd.DataFrame:\n        \"\"\"\n        Create datetime features from the DateTime index of a pandas DataFrame or Series.\n\n        Parameters\n        ----------\n        X : pandas Series, pandas DataFrame\n            Input DataFrame or Series with a datetime index.\n        \n        Returns\n        -------\n        X_new : pandas DataFrame\n            DataFrame with the extracted (and optionally encoded) datetime features.\n\n        \"\"\"\n\n        X_new = create_datetime_features(\n                    X          = X,\n                    encoding   = self.encoding,\n                    features   = self.features,\n                    max_values = self.max_values,\n                )\n\n        return X_new\n\n\n@njit\ndef _np_mean_jit(x):  # pragma: no cover\n    \"\"\"\n    NumPy mean function implemented with Numba JIT.\n    \"\"\"\n    return np.mean(x)\n\n\n@njit\ndef _np_std_jit(x, ddof=1):  # pragma: no cover\n    \"\"\"\n    Standard deviation function implemented with Numba JIT.\n    If the array has only one element, the function returns 0.\n    \"\"\"\n    if len(x) == 1:\n        return 0.\n    \n    a_a, b_b = 0, 0\n    for i in x:\n        a_a = a_a + i\n        b_b = b_b + i * i\n    var = b_b / (len(x)) - ((a_a / (len(x))) ** 2)\n    var = var * (len(x) / (len(x) - ddof))\n    std = np.sqrt(var)\n\n    return std\n\n\n@njit\ndef _np_min_jit(x):  # pragma: no cover\n    \"\"\"\n    NumPy min function implemented with Numba JIT.\n    \"\"\"\n    return np.min(x)\n\n\n@njit\ndef _np_max_jit(x):  # pragma: no cover\n    \"\"\"\n    NumPy max function implemented with Numba JIT.\n    \"\"\"\n    return np.max(x)\n\n\n@njit\ndef _np_sum_jit(x):  # pragma: no cover\n    \"\"\"\n    NumPy sum function implemented with Numba JIT.\n    \"\"\"\n    return np.sum(x)\n\n\n@njit\ndef _np_median_jit(x):  # pragma: no cover\n    \"\"\"\n    NumPy median function implemented with Numba JIT.\n    \"\"\"\n    return np.median(x)\n\n\n@njit\ndef _np_min_max_ratio_jit(x):  # pragma: no cover\n    \"\"\"\n    NumPy min-max ratio function implemented with Numba JIT.\n    \"\"\"\n    return np.min(x) / np.max(x)\n\n\n@njit\ndef _np_cv_jit(x):  # pragma: no cover\n    \"\"\"\n    Coefficient of variation function implemented with Numba JIT.\n    If the array has only one element, the function returns 0.\n    \"\"\"\n    if len(x) == 1:\n        return 0.\n    \n    a_a, b_b = 0, 0\n    for i in x:\n        a_a = a_a + i\n        b_b = b_b + i * i\n    var = b_b / (len(x)) - ((a_a / (len(x))) ** 2)\n    var = var * (len(x) / (len(x) - 1))\n    std = np.sqrt(var)\n\n    return std / np.mean(x)\n\n\nclass RollingFeatures():\n    \"\"\"\n    This class computes rolling features. To avoid data leakage, the last point \n    in the window is excluded from calculations, ('closed': 'left' and \n    'center': False).\n\n    Parameters\n    ----------\n    stats : str, list\n        Statistics to compute over the rolling window. Can be a `string` or a `list`,\n        and can have repeats. Available statistics are: 'mean', 'std', 'min', 'max',\n        'sum', 'median', 'ratio_min_max', 'coef_variation'.\n    window_sizes : int, list\n        Size of the rolling window for each statistic. If an `int`, all stats share \n        the same window size. If a `list`, it should have the same length as stats.\n    min_periods : int, list, default `None`\n        Minimum number of observations in window required to have a value. \n        Same as the `min_periods` argument of pandas rolling. If `None`, \n        defaults to `window_sizes`.\n    features_names : list, default `None`\n        Names of the output features. If `None`, default names will be used in the \n        format 'roll_stat_window_size', for example 'roll_mean_7'.\n    fillna : str, float, default `None`\n        Fill missing values in `transform_batch` method. Available \n        methods are: 'mean', 'median', 'ffill', 'bfill', or a float value.\n    \n    Attributes\n    ----------\n    stats : list\n        Statistics to compute over the rolling window.\n    n_stats : int\n        Number of statistics to compute.\n    window_sizes : list\n        Size of the rolling window for each statistic.\n    max_window_size : int\n        Maximum window size.\n    min_periods : list\n        Minimum number of observations in window required to have a value.\n    features_names : list\n        Names of the output features.\n    fillna : str, float\n        Method to fill missing values in `transform_batch` method.\n    unique_rolling_windows : dict\n        Dictionary containing unique rolling window parameters and the corresponding\n        statistics.\n        \n    \"\"\"\n\n    def __init__(\n        self, \n        stats: Union[str, list],\n        window_sizes: Union[int, list],\n        min_periods: Optional[Union[int, list]] = None,\n        features_names: Optional[list] = None, \n        fillna: Optional[Union[str, float]] = None\n    ) -> None:\n        \n        self._validate_params(\n            stats,\n            window_sizes,\n            min_periods,\n            features_names,\n            fillna\n        )\n\n        if isinstance(stats, str):\n            stats = [stats]\n        self.stats = stats\n        self.n_stats = len(stats)\n\n        if isinstance(window_sizes, int):\n            window_sizes = [window_sizes] * self.n_stats\n        self.window_sizes = window_sizes\n        self.max_window_size = max(window_sizes)\n        \n        if min_periods is None:\n            min_periods = self.window_sizes\n        elif isinstance(min_periods, int):\n            min_periods = [min_periods] * self.n_stats\n        self.min_periods = min_periods\n\n        if features_names is None:\n            features_names = [\n                f\"roll_{stat}_{window_size}\" \n                for stat, window_size in zip(self.stats, self.window_sizes)\n            ]\n        self.features_names = features_names\n        \n        self.fillna = fillna\n\n        window_params_list = []\n        for i in range(len(self.stats)):\n            window_params = (self.window_sizes[i], self.min_periods[i])\n            window_params_list.append(window_params)\n\n        # Find unique window parameter combinations\n        unique_rolling_windows = {}\n        for i, params in enumerate(window_params_list):\n            key = f\"{params[0]}_{params[1]}\"\n            if key not in unique_rolling_windows:\n                unique_rolling_windows[key] = {\n                    'params': {\n                        'window': params[0], \n                        'min_periods': params[1], \n                        'center': False,\n                        'closed': 'left'\n                    },\n                    'stats_idx': [], \n                    'stats_names': [], \n                    'rolling_obj': None\n                }\n            unique_rolling_windows[key]['stats_idx'].append(i)\n            unique_rolling_windows[key]['stats_names'].append(self.features_names[i])\n\n        self.unique_rolling_windows = unique_rolling_windows\n\n    def __repr__(\n        self\n    ) -> str:\n        \"\"\"\n        Information displayed when printed.\n        \"\"\"\n            \n        return (\n            f\"RollingFeatures(\\n\"\n            f\"    stats           = {self.stats},\\n\"\n            f\"    window_sizes    = {self.window_sizes},\\n\"\n            f\"    Max window size = {self.max_window_size},\\n\"\n            f\"    min_periods     = {self.min_periods},\\n\"\n            f\"    features_names  = {self.features_names},\\n\"\n            f\"    fillna          = {self.fillna}\\n\"\n            f\")\"\n        )\n\n    def _validate_params(\n        self, \n        stats, \n        window_sizes, \n        min_periods: Optional[Union[int, list]] = None,\n        features_names: Optional[Union[str, list]] = None, \n        fillna: Optional[Union[str, float]] = None\n    ) -> None:\n        \"\"\"\n        Validate the parameters of the RollingFeatures class.\n\n        Parameters\n        ----------\n        stats : str, list\n            Statistics to compute over the rolling window. Can be a `string` or a `list`,\n            and can have repeats. Available statistics are: 'mean', 'std', 'min', 'max',\n            'sum', 'median', 'ratio_min_max', 'coef_variation'.\n        window_sizes : int, list\n            Size of the rolling window for each statistic. If an `int`, all stats share \n            the same window size. If a `list`, it should have the same length as stats.\n        min_periods : int, list, default `None`\n            Minimum number of observations in window required to have a value. \n            Same as the `min_periods` argument of pandas rolling. If `None`, \n            defaults to `window_sizes`.\n        features_names : list, default `None`\n            Names of the output features. If `None`, default names will be used in the \n            format 'roll_stat_window_size', for example 'roll_mean_7'.\n        fillna : str, float, default `None`\n            Fill missing values in `transform_batch` method. Available \n            methods are: 'mean', 'median', 'ffill', 'bfill', or a float value.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n\n        # stats\n        if not isinstance(stats, (str, list)):\n            raise TypeError(\n                f\"`stats` must be a string or a list of strings. Got {type(stats)}.\"\n            )        \n        \n        if isinstance(stats, str):\n            stats = [stats]\n        allowed_stats = ['mean', 'std', 'min', 'max', 'sum', 'median', \n                         'ratio_min_max', 'coef_variation']\n        for stat in set(stats):\n            if stat not in allowed_stats:\n                raise ValueError(\n                    f\"Statistic '{stat}' is not allowed. Allowed stats are: {allowed_stats}.\"\n                )\n        \n        n_stats = len(stats)\n        \n        # window_sizes\n        if not isinstance(window_sizes, (int, list)):\n            raise TypeError(\n                f\"`window_sizes` must be an int or a list of ints. Got {type(window_sizes)}.\"\n            )\n        \n        if isinstance(window_sizes, list):\n            n_window_sizes = len(window_sizes)\n            if n_window_sizes != n_stats:\n                raise ValueError(\n                    f\"Length of `window_sizes` list ({n_window_sizes}) \"\n                    f\"must match length of `stats` list ({n_stats}).\"\n                )\n            \n        # Check duplicates (stats, window_sizes)\n        if isinstance(window_sizes, int):\n            window_sizes = [window_sizes] * n_stats\n        if len(set(zip(stats, window_sizes))) != n_stats:\n            raise ValueError(\n                f\"Duplicate (stat, window_size) pairs are not allowed.\\n\"\n                f\"    `stats`       : {stats}\\n\"\n                f\"    `window_sizes : {window_sizes}\"\n            )\n        \n        # min_periods\n        if not isinstance(min_periods, (int, list, type(None))):\n            raise TypeError(\n                f\"`min_periods` must be an int, list of ints, or None. Got {type(min_periods)}.\"\n            )\n        \n        if min_periods is not None:\n            if isinstance(min_periods, int):\n                min_periods = [min_periods] * n_stats\n            elif isinstance(min_periods, list):\n                n_min_periods = len(min_periods)\n                if n_min_periods != n_stats:\n                    raise ValueError(\n                        f\"Length of `min_periods` list ({n_min_periods}) \"\n                        f\"must match length of `stats` list ({n_stats}).\"\n                    )\n            \n            for i, min_period in enumerate(min_periods):\n                if min_period > window_sizes[i]:\n                    raise ValueError(\n                        \"Each `min_period` must be less than or equal to its \"\n                        \"corresponding `window_size`.\"\n                    )\n        \n        # features_names\n        if not isinstance(features_names, (list, type(None))):\n            raise TypeError(\n                f\"`features_names` must be a list of strings or None. Got {type(features_names)}.\"\n            )\n        \n        if isinstance(features_names, list):\n            n_features_names = len(features_names)\n            if n_features_names != n_stats:\n                raise ValueError(\n                    f\"Length of `features_names` list ({n_features_names}) \"\n                    f\"must match length of `stats` list ({n_stats}).\"\n                )\n        \n        # fillna\n        if fillna is not None:\n            if not isinstance(fillna, (int, float, str)):\n                raise TypeError(\n                    f\"`fillna` must be a float, string, or None. Got {type(fillna)}.\"\n                )\n            \n            if isinstance(fillna, str):\n                allowed_fill_strategy = ['mean', 'median', 'ffill', 'bfill']\n                if fillna not in allowed_fill_strategy:\n                    raise ValueError(\n                        f\"'{fillna}' is not allowed. Allowed `fillna` \"\n                        f\"values are: {allowed_fill_strategy} or a float value.\"\n                    )\n\n    def _apply_stat_pandas(\n        self, \n        rolling_obj: pd.core.window.rolling.Rolling, \n        stat: str\n    ) -> pd.Series:\n        \"\"\"\n        Apply the specified statistic to a pandas rolling object.\n\n        Parameters\n        ----------\n        rolling_obj : pandas Rolling\n            Rolling object to apply the statistic.\n        stat : str\n            Statistic to compute.\n        \n        Returns\n        -------\n        stat_series : pandas Series\n            Series with the computed statistic.\n        \n        \"\"\"\n\n        if stat == 'mean':\n            return rolling_obj.mean()\n        elif stat == 'std':\n            return rolling_obj.std()\n        elif stat == 'min':\n            return rolling_obj.min()\n        elif stat == 'max':\n            return rolling_obj.max()\n        elif stat == 'sum':\n            return rolling_obj.sum()\n        elif stat == 'median':\n            return rolling_obj.median()\n        elif stat == 'ratio_min_max':\n            return rolling_obj.min() / rolling_obj.max()\n        elif stat == 'coef_variation':\n            return rolling_obj.std() / rolling_obj.mean()\n        else:\n            raise ValueError(f\"Statistic '{stat}' is not implemented.\")\n\n    def transform_batch(\n        self, \n        X: pd.Series\n    ) -> pd.DataFrame:\n        \"\"\"\n        Transform an entire pandas Series using rolling windows and compute the \n        specified statistics.\n\n        Parameters\n        ----------\n        X : pandas Series\n            The input data series to transform.\n\n        Returns\n        -------\n        rolling_features : pandas DataFrame\n            A DataFrame containing the rolling features.\n        \n        \"\"\"\n\n        for k in self.unique_rolling_windows.keys():\n            rolling_obj = X.rolling(**self.unique_rolling_windows[k]['params'])\n            self.unique_rolling_windows[k]['rolling_obj'] = rolling_obj\n        \n        rolling_features = []\n        for i, stat in enumerate(self.stats):\n            window_size = self.window_sizes[i]\n            min_periods = self.min_periods[i]\n\n            key = f\"{window_size}_{min_periods}\"\n            rolling_obj = self.unique_rolling_windows[key]['rolling_obj']\n\n            stat_series = self._apply_stat_pandas(rolling_obj=rolling_obj, stat=stat)            \n            rolling_features.append(stat_series)\n\n        rolling_features = pd.concat(rolling_features, axis=1)\n        rolling_features.columns = self.features_names\n        rolling_features = rolling_features.iloc[self.max_window_size:]\n\n        if self.fillna is not None:\n            if self.fillna == 'mean':\n                rolling_features = rolling_features.fillna(rolling_features.mean())\n            elif self.fillna == 'median':\n                rolling_features = rolling_features.fillna(rolling_features.median())\n            elif self.fillna == 'ffill':\n                rolling_features = rolling_features.ffill()\n            elif self.fillna == 'bfill':\n                rolling_features = rolling_features.bfill()\n            else:\n                rolling_features = rolling_features.fillna(self.fillna)\n        \n        return rolling_features\n\n    def _apply_stat_numpy_jit(\n        self, \n        X_window: np.ndarray, \n        stat: str\n    ) -> float:\n        \"\"\"\n        Apply the specified statistic to a numpy array using Numba JIT.\n\n        Parameters\n        ----------\n        X_window : numpy array\n            Array with the rolling window.\n        stat : str\n            Statistic to compute.\n\n        Returns\n        -------\n        stat_value : float\n            Value of the computed statistic.\n        \n        \"\"\"\n        \n        if stat == 'mean':\n            return _np_mean_jit(X_window)\n        elif stat == 'std':\n            return _np_std_jit(X_window)\n        elif stat == 'min':\n            return _np_min_jit(X_window)\n        elif stat == 'max':\n            return _np_max_jit(X_window)\n        elif stat == 'sum':\n            return _np_sum_jit(X_window)\n        elif stat == 'median':\n            return _np_median_jit(X_window)\n        elif stat == 'ratio_min_max':\n            return _np_min_max_ratio_jit(X_window)\n        elif stat == 'coef_variation':\n            return _np_cv_jit(X_window)\n        else:\n            raise ValueError(f\"Statistic '{stat}' is not implemented.\")\n\n    def transform(\n        self, \n        X: np.ndarray\n    ) -> np.ndarray:\n        \"\"\"\n        Transform a numpy array using rolling windows and compute the \n        specified statistics. The returned array will have the shape \n        (X.shape[1] if exists, n_stats). For example, if X is a flat\n        array, the output will have shape (n_stats,). If X is a 2D array,\n        the output will have shape (X.shape[1], n_stats).\n\n        Parameters\n        ----------\n        X : numpy ndarray\n            The input data array to transform.\n\n        Returns\n        -------\n        rolling_features : numpy ndarray\n            An array containing the computed statistics.\n        \n        \"\"\"\n\n        array_ndim = X.ndim\n        if array_ndim == 1:\n            X = X[:, np.newaxis]\n            \n        rolling_features = np.full(\n            shape=(X.shape[1], self.n_stats), fill_value=np.nan, dtype=float\n        )\n\n        for i in range(X.shape[1]):\n            for j, stat in enumerate(self.stats):\n                X_window = X[-self.window_sizes[j]:, i]\n                X_window = X_window[~np.isnan(X_window)]\n                if len(X_window) > 0: \n                    rolling_features[i, j] = self._apply_stat_numpy_jit(X_window, stat)\n                else:\n                    rolling_features[i, j] = np.nan\n\n        if array_ndim == 1:\n            rolling_features = rolling_features.ravel()\n        \n        return rolling_features\n    \n\nclass QuantileBinner:\n    \"\"\"\n    QuantileBinner class to bin data into quantile-based bins using `numpy.percentile`.\n    This class is similar to `KBinsDiscretizer` but faster for binning data into\n    quantile-based bins. Bin  intervals are defined following the convention:\n    bins[i-1] <= x < bins[i]. See more information in `numpy.percentile` and\n    `numpy.digitize`.\n    \n    Parameters\n    ----------\n    n_bins : int\n        The number of quantile-based bins to create.\n    method : str, default='linear'\n        The method used to compute the quantiles. This parameter is passed to \n        `numpy.percentile`. Default is 'linear'. Valid values are \"inverse_cdf\",\n        \"averaged_inverse_cdf\", \"closest_observation\", \"interpolated_inverse_cdf\",\n        \"hazen\", \"weibull\", \"linear\", \"median_unbiased\", \"normal_unbiased\".\n    subsample : int, default=200000\n        The number of samples to use for computing quantiles. If the dataset \n        has more samples than `subsample`, a random subset will be used.\n    random_state : int, default=789654\n        The random seed to use for generating a random subset of the data.\n    dtype : data type, default=numpy.float64\n        The data type to use for the bin indices. Default is `numpy.float64`.\n    \n    Attributes\n    ----------\n    n_bins : int\n        The number of quantile-based bins to create.\n    method : str, default='linear'\n        The method used to compute the quantiles. This parameter is passed to \n        `numpy.percentile`. Default is 'linear'. Valid values are 'linear',\n        'lower', 'higher', 'midpoint', 'nearest'.\n    subsample : int, default=200000\n        The number of samples to use for computing quantiles. If the dataset \n        has more samples than `subsample`, a random subset will be used.\n    random_state : int, default=789654\n        The random seed to use for generating a random subset of the data.\n    dtype : data type, default=numpy.float64\n        The data type to use for the bin indices. Default is `numpy.float64`.\n    n_bins_ : int\n        The number of bins learned during fitting.\n    bin_edges_ : numpy ndarray\n        The edges of the bins learned during fitting.\n    \n    \"\"\"\n\n    def __init__(\n        self,\n        n_bins: int,\n        method: Optional[str] = \"linear\",\n        subsample: int = 200000,\n        dtype: Optional[type] = np.float64,\n        random_state: Optional[int] = 789654\n    ):\n        \n        self._validate_params(\n            n_bins,\n            method,\n            subsample,\n            dtype,\n            random_state\n        )\n\n        self.n_bins       = n_bins\n        self.method       = method\n        self.subsample    = subsample\n        self.random_state = random_state\n        self.dtype        = dtype\n        self.n_bins_      = None\n        self.bin_edges_   = None\n        self.intervals_   = None\n\n    def _validate_params(\n        self,\n        n_bins: int,\n        method: str,\n        subsample: int,\n        dtype: type,\n        random_state: int\n    ):\n        \"\"\"\n        Validate the parameters passed to the class initializer.\n        \"\"\"\n    \n        if not isinstance(n_bins, int) or n_bins < 2:\n            raise ValueError(\n                f\"`n_bins` must be an int greater than 1. Got {n_bins}.\"\n            )\n\n        valid_methods = [\n            \"inverse_cdf\",\n            \"averaged_inverse_cdf\",\n            \"closest_observation\",\n            \"interpolated_inverse_cdf\",\n            \"hazen\",\n            \"weibull\",\n            \"linear\",\n            \"median_unbiased\",\n            \"normal_unbiased\",\n        ]\n        if method not in valid_methods:\n            raise ValueError(\n                f\"`method` must be one of {valid_methods}. Got {method}.\"\n            )\n        if not isinstance(subsample, int) or subsample < 1:\n            raise ValueError(\n                f\"`subsample` must be an integer greater than or equal to 1. \"\n                f\"Got {subsample}.\"\n            )\n        if not isinstance(random_state, int) or random_state < 0:\n            raise ValueError(\n                f\"`random_state` must be an integer greater than or equal to 0. \"\n                f\"Got {random_state}.\"\n            )\n        if not isinstance(dtype, type):\n            raise ValueError(\n                f\"`dtype` must be a valid numpy dtype. Got {dtype}.\"\n            )\n\n    def fit(self, X: np.ndarray):\n        \"\"\"\n        Learn the bin edges based on quantiles from the training data.\n        \n        Parameters\n        ----------\n        X : numpy ndarray\n            The training data used to compute the quantiles.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        if X.size == 0:\n            raise ValueError(\"Input data `X` cannot be empty.\")\n        if len(X) > self.subsample:\n            rng = np.random.default_rng(self.random_state)\n            X = X[rng.integers(0, len(X), self.subsample)]\n\n        self.bin_edges_ = np.percentile(\n            a      = X,\n            q      = np.linspace(0, 100, self.n_bins + 1),\n            method = self.method\n        )\n\n        self.n_bins_ = len(self.bin_edges_) - 1\n        self.intervals_ = {\n            float(i): (float(self.bin_edges_[i]), float(self.bin_edges_[i + 1]))\n            for i in range(self.n_bins_)\n        }\n\n    def transform(self, X: np.ndarray):\n        \"\"\"\n        Assign new data to the learned bins.\n        \n        Parameters\n        ----------\n        X : numpy ndarray\n            The data to assign to the bins.\n        \n        Returns\n        -------\n        bin_indices : numpy ndarray \n            The indices of the bins each value belongs to.\n            Values less than the smallest bin edge are assigned to the first bin,\n            and values greater than the largest bin edge are assigned to the last bin.\n       \n        \"\"\"\n\n        if self.bin_edges_ is None:\n            raise NotFittedError(\n                \"The model has not been fitted yet. Call 'fit' with training data first.\"\n            )\n\n        bin_indices = np.digitize(X, bins=self.bin_edges_, right=False)\n        bin_indices = np.clip(bin_indices, 1, self.n_bins_).astype(self.dtype) - 1\n\n        return bin_indices\n\n    def fit_transform(self, X):\n        \"\"\"\n        Fit the model to the data and return the bin indices for the same data.\n        \n        Parameters\n        ----------\n        X : numpy.ndarray\n            The data to fit and transform.\n        \n        Returns\n        -------\n        bin_indices : numpy.ndarray\n            The indices of the bins each value belongs to.\n            Values less than the smallest bin edge are assigned to the first bin,\n            and values greater than the largest bin edge are assigned to the last bin.\n        \n        \"\"\"\n\n        self.fit(X)\n\n        return self.transform(X)\n\n    def get_params(self):\n        \"\"\"\n        Get the parameters of the quantile binner.\n        \n        Parameters\n        ----------\n        self\n        \n        Returns\n        -------\n        params : dict\n            A dictionary of the parameters of the quantile binner.\n        \n        \"\"\"\n\n        return {\n            \"n_bins\": self.n_bins,\n            \"method\": self.method,\n            \"subsample\": self.subsample,\n            \"dtype\": self.dtype,\n            \"random_state\": self.random_state,\n        }\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of the QuantileBinner.\n        \n        Parameters\n        ----------\n        params : dict\n            A dictionary of the parameters to set.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        for param, value in params.items():\n            setattr(self, param, value)\n"
  },
  "GT_src_dict": {
    "skforecast/utils/utils.py": {
      "initialize_lags": {
        "code": "def initialize_lags(forecaster_name: str, lags: Any) -> Union[Optional[np.ndarray], Optional[list], Optional[int]]:\n    \"\"\"Check and initialize lags for the forecaster by validating the input and generating corresponding numpy ndarray for the lags, their names, and the maximum lag value.\n\nParameters\n----------\nforecaster_name : str\n    Name of the forecaster for which lags are being initialized.\nlags : Any\n    The lags used as predictors, which can be an integer, list, tuple, range, or numpy array.\n\nReturns\n-------\nlags : numpy.ndarray, None\n    An array of validated lags to be used as predictors.\nlags_names : list, None\n    A list of names corresponding to the lags.\nmax_lag : int, None\n    The maximum value of the lags.\n\nRaises\n------\nValueError\n    If the lags parameter does not meet the required conditions, such as being less than 1 or not being a valid 1-dimensional structure.\nTypeError\n    If the lags parameter is of an unsupported type.\n\nNotes\n-----\nThe function ensures that the lags are integers greater than or equal to 1 and returns their names in the form 'lag_i' where i is each lag value. It is designed to be compatible with different forecaster types, checking against specified conditions particular to 'ForecasterDirectMultiVariate'. It is utilized in the context of the skforecast library to facilitate time series forecasting and ensure proper handling of past data for model training.\"\"\"\n    '\\n    Check lags argument input and generate the corresponding numpy ndarray.\\n\\n    Parameters\\n    ----------\\n    forecaster_name : str\\n        Forecaster name.\\n    lags : Any\\n        Lags used as predictors.\\n\\n    Returns\\n    -------\\n    lags : numpy ndarray, None\\n        Lags used as predictors.\\n    lags_names : list, None\\n        Names of the lags used as predictors.\\n    max_lag : int, None\\n        Maximum value of the lags.\\n    \\n    '\n    lags_names = None\n    max_lag = None\n    if lags is not None:\n        if isinstance(lags, int):\n            if lags < 1:\n                raise ValueError('Minimum value of lags allowed is 1.')\n            lags = np.arange(1, lags + 1)\n        if isinstance(lags, (list, tuple, range)):\n            lags = np.array(lags)\n        if isinstance(lags, np.ndarray):\n            if lags.size == 0:\n                return (None, None, None)\n            if lags.ndim != 1:\n                raise ValueError('`lags` must be a 1-dimensional array.')\n            if not np.issubdtype(lags.dtype, np.integer):\n                raise TypeError('All values in `lags` must be integers.')\n            if np.any(lags < 1):\n                raise ValueError('Minimum value of lags allowed is 1.')\n        elif forecaster_name != 'ForecasterDirectMultiVariate':\n            raise TypeError(f'`lags` argument must be an int, 1d numpy ndarray, range, tuple or list. Got {type(lags)}.')\n        else:\n            raise TypeError(f'`lags` argument must be a dict, int, 1d numpy ndarray, range, tuple or list. Got {type(lags)}.')\n        lags_names = [f'lag_{i}' for i in lags]\n        max_lag = max(lags)\n    return (lags, lags_names, max_lag)",
        "docstring": "Check and initialize lags for the forecaster by validating the input and generating corresponding numpy ndarray for the lags, their names, and the maximum lag value.\n\nParameters\n----------\nforecaster_name : str\n    Name of the forecaster for which lags are being initialized.\nlags : Any\n    The lags used as predictors, which can be an integer, list, tuple, range, or numpy array.\n\nReturns\n-------\nlags : numpy.ndarray, None\n    An array of validated lags to be used as predictors.\nlags_names : list, None\n    A list of names corresponding to the lags.\nmax_lag : int, None\n    The maximum value of the lags.\n\nRaises\n------\nValueError\n    If the lags parameter does not meet the required conditions, such as being less than 1 or not being a valid 1-dimensional structure.\nTypeError\n    If the lags parameter is of an unsupported type.\n\nNotes\n-----\nThe function ensures that the lags are integers greater than or equal to 1 and returns their names in the form 'lag_i' where i is each lag value. It is designed to be compatible with different forecaster types, checking against specified conditions particular to 'ForecasterDirectMultiVariate'. It is utilized in the context of the skforecast library to facilitate time series forecasting and ensure proper handling of past data for model training.",
        "signature": "def initialize_lags(forecaster_name: str, lags: Any) -> Union[Optional[np.ndarray], Optional[list], Optional[int]]:",
        "type": "Function",
        "class_signature": null
      },
      "initialize_window_features": {
        "code": "def initialize_window_features(window_features: Any) -> Union[Optional[list], Optional[list], Optional[int]]:\n    \"\"\"Check the input for `window_features`, which are classes used to create window features for time series forecasting. It validates the presence of required attributes and methods, extracts features' names, and determines the maximum window size across all provided classes.\n\nParameters\n----------\nwindow_features : Any\n    Classes used to create window features. This can be a single class or a list of classes.\n\nReturns\n-------\nwindow_features : list, None\n    List of classes used to create window features.\nwindow_features_names : list, None\n    List of feature names extracted from the window features classes.\nmax_size_window_features : int, None\n    Maximum value of the `window_sizes` attribute among all classes.\n\nRaises \n------\nValueError\n    If any class in `window_features` does not fulfill the required attributes or methods.\nTypeError\n    If attributes have invalid types, particularly `window_sizes` or `features_names`.\n\nThis function relies on the constants `needed_atts` and `needed_methods` to specify the required attributes and methods that each window feature class must have, enhancing consistency and reducing errors during feature extraction.\"\"\"\n    '\\n    Check window_features argument input and generate the corresponding list.\\n\\n    Parameters\\n    ----------\\n    window_features : Any\\n        Classes used to create window features.\\n\\n    Returns\\n    -------\\n    window_features : list, None\\n        List of classes used to create window features.\\n    window_features_names : list, None\\n        List with all the features names of the window features.\\n    max_size_window_features : int, None\\n        Maximum value of the `window_sizes` attribute of all classes.\\n    \\n    '\n    needed_atts = ['window_sizes', 'features_names']\n    needed_methods = ['transform_batch', 'transform']\n    max_window_sizes = None\n    window_features_names = None\n    max_size_window_features = None\n    if window_features is not None:\n        if isinstance(window_features, list) and len(window_features) < 1:\n            raise ValueError('Argument `window_features` must contain at least one element.')\n        if not isinstance(window_features, list):\n            window_features = [window_features]\n        link_to_docs = '\\nVisit the documentation for more information about how to create custom window features:\\nhttps://skforecast.org/latest/user_guides/window-features-and-custom-features.html#create-your-custom-window-features'\n        max_window_sizes = []\n        window_features_names = []\n        for wf in window_features:\n            wf_name = type(wf).__name__\n            atts_methods = set([a for a in dir(wf)])\n            if not set(needed_atts).issubset(atts_methods):\n                raise ValueError(f'{wf_name} must have the attributes: {needed_atts}.' + link_to_docs)\n            if not set(needed_methods).issubset(atts_methods):\n                raise ValueError(f'{wf_name} must have the methods: {needed_methods}.' + link_to_docs)\n            window_sizes = wf.window_sizes\n            if not isinstance(window_sizes, (int, list)):\n                raise TypeError(f'Attribute `window_sizes` of {wf_name} must be an int or a list of ints. Got {type(window_sizes)}.' + link_to_docs)\n            if isinstance(window_sizes, int):\n                if window_sizes < 1:\n                    raise ValueError(f'If argument `window_sizes` is an integer, it must be equal to or greater than 1. Got {window_sizes} from {wf_name}.' + link_to_docs)\n                max_window_sizes.append(window_sizes)\n            else:\n                if not all((isinstance(ws, int) for ws in window_sizes)) or not all((ws >= 1 for ws in window_sizes)):\n                    raise ValueError(f'If argument `window_sizes` is a list, all elements must be integers equal to or greater than 1. Got {window_sizes} from {wf_name}.' + link_to_docs)\n                max_window_sizes.append(max(window_sizes))\n            features_names = wf.features_names\n            if not isinstance(features_names, (str, list)):\n                raise TypeError(f'Attribute `features_names` of {wf_name} must be a str or a list of strings. Got {type(features_names)}.' + link_to_docs)\n            if isinstance(features_names, str):\n                window_features_names.append(features_names)\n            else:\n                if not all((isinstance(fn, str) for fn in features_names)):\n                    raise TypeError(f'If argument `features_names` is a list, all elements must be strings. Got {features_names} from {wf_name}.' + link_to_docs)\n                window_features_names.extend(features_names)\n        max_size_window_features = max(max_window_sizes)\n        if len(set(window_features_names)) != len(window_features_names):\n            raise ValueError(f'All window features names must be unique. Got {window_features_names}.')\n    return (window_features, window_features_names, max_size_window_features)",
        "docstring": "Check the input for `window_features`, which are classes used to create window features for time series forecasting. It validates the presence of required attributes and methods, extracts features' names, and determines the maximum window size across all provided classes.\n\nParameters\n----------\nwindow_features : Any\n    Classes used to create window features. This can be a single class or a list of classes.\n\nReturns\n-------\nwindow_features : list, None\n    List of classes used to create window features.\nwindow_features_names : list, None\n    List of feature names extracted from the window features classes.\nmax_size_window_features : int, None\n    Maximum value of the `window_sizes` attribute among all classes.\n\nRaises \n------\nValueError\n    If any class in `window_features` does not fulfill the required attributes or methods.\nTypeError\n    If attributes have invalid types, particularly `window_sizes` or `features_names`.\n\nThis function relies on the constants `needed_atts` and `needed_methods` to specify the required attributes and methods that each window feature class must have, enhancing consistency and reducing errors during feature extraction.",
        "signature": "def initialize_window_features(window_features: Any) -> Union[Optional[list], Optional[list], Optional[int]]:",
        "type": "Function",
        "class_signature": null
      },
      "initialize_weights": {
        "code": "def initialize_weights(forecaster_name: str, regressor: object, weight_func: Union[Callable, dict], series_weights: dict) -> Tuple[Union[Callable, dict], Union[str, dict], dict]:\n    \"\"\"Check and initialize the weights for forecasters within the skforecast library. This function validates the `weight_func` and `series_weights` parameters, creating a source code representation of any custom weight functions provided. If the specified regressor does not accept `sample_weight`, appropriate warnings are issued, and the corresponding arguments are set to None.\n\nParameters\n----------\nforecaster_name : str\n    The name of the forecaster, which influences the validation of the `weight_func`.\nregressor : object\n    A scikit-learn compatible regressor or pipeline used in the forecaster.\nweight_func : Callable, dict\n    A custom weighting function or a dictionary of functions used for sample weighting.\nseries_weights : dict\n    A dictionary specifying weights for each time series.\n\nReturns\n-------\nweight_func : Callable, dict\n    The validated and possibly modified `weight_func`.\nsource_code_weight_func : str, dict\n    The source code of the custom weight function(s), if applicable.\nseries_weights : dict\n    The validated `series_weights`.\n\nNotes\n-----\nThis function raises TypeErrors if the types of `weight_func` or `series_weights` are incorrect. It also issues warnings when arguments are ignored due to incompatibility with the specified regressor. The `IgnoredArgumentWarning` is used for warning messages related to ignored function parameters. The function relies on the `inspect` module to access and reproduce the source code of weight functions, emphasizing its need for callable objects.\"\"\"\n    '\\n    Check weights arguments, `weight_func` and `series_weights` for the different \\n    forecasters. Create `source_code_weight_func`, source code of the custom \\n    function(s) used to create weights.\\n    \\n    Parameters\\n    ----------\\n    forecaster_name : str\\n        Forecaster name.\\n    regressor : regressor or pipeline compatible with the scikit-learn API\\n        Regressor of the forecaster.\\n    weight_func : Callable, dict\\n        Argument `weight_func` of the forecaster.\\n    series_weights : dict\\n        Argument `series_weights` of the forecaster.\\n\\n    Returns\\n    -------\\n    weight_func : Callable, dict\\n        Argument `weight_func` of the forecaster.\\n    source_code_weight_func : str, dict\\n        Argument `source_code_weight_func` of the forecaster.\\n    series_weights : dict\\n        Argument `series_weights` of the forecaster.\\n    \\n    '\n    source_code_weight_func = None\n    if weight_func is not None:\n        if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n            if not isinstance(weight_func, (Callable, dict)):\n                raise TypeError(f'Argument `weight_func` must be a Callable or a dict of Callables. Got {type(weight_func)}.')\n        elif not isinstance(weight_func, Callable):\n            raise TypeError(f'Argument `weight_func` must be a Callable. Got {type(weight_func)}.')\n        if isinstance(weight_func, dict):\n            source_code_weight_func = {}\n            for key in weight_func:\n                source_code_weight_func[key] = inspect.getsource(weight_func[key])\n        else:\n            source_code_weight_func = inspect.getsource(weight_func)\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(f'Argument `weight_func` is ignored since regressor {regressor} does not accept `sample_weight` in its `fit` method.', IgnoredArgumentWarning)\n            weight_func = None\n            source_code_weight_func = None\n    if series_weights is not None:\n        if not isinstance(series_weights, dict):\n            raise TypeError(f'Argument `series_weights` must be a dict of floats or ints.Got {type(series_weights)}.')\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(f'Argument `series_weights` is ignored since regressor {regressor} does not accept `sample_weight` in its `fit` method.', IgnoredArgumentWarning)\n            series_weights = None\n    return (weight_func, source_code_weight_func, series_weights)",
        "docstring": "Check and initialize the weights for forecasters within the skforecast library. This function validates the `weight_func` and `series_weights` parameters, creating a source code representation of any custom weight functions provided. If the specified regressor does not accept `sample_weight`, appropriate warnings are issued, and the corresponding arguments are set to None.\n\nParameters\n----------\nforecaster_name : str\n    The name of the forecaster, which influences the validation of the `weight_func`.\nregressor : object\n    A scikit-learn compatible regressor or pipeline used in the forecaster.\nweight_func : Callable, dict\n    A custom weighting function or a dictionary of functions used for sample weighting.\nseries_weights : dict\n    A dictionary specifying weights for each time series.\n\nReturns\n-------\nweight_func : Callable, dict\n    The validated and possibly modified `weight_func`.\nsource_code_weight_func : str, dict\n    The source code of the custom weight function(s), if applicable.\nseries_weights : dict\n    The validated `series_weights`.\n\nNotes\n-----\nThis function raises TypeErrors if the types of `weight_func` or `series_weights` are incorrect. It also issues warnings when arguments are ignored due to incompatibility with the specified regressor. The `IgnoredArgumentWarning` is used for warning messages related to ignored function parameters. The function relies on the `inspect` module to access and reproduce the source code of weight functions, emphasizing its need for callable objects.",
        "signature": "def initialize_weights(forecaster_name: str, regressor: object, weight_func: Union[Callable, dict], series_weights: dict) -> Tuple[Union[Callable, dict], Union[str, dict], dict]:",
        "type": "Function",
        "class_signature": null
      },
      "check_select_fit_kwargs": {
        "code": "def check_select_fit_kwargs(regressor: object, fit_kwargs: Optional[dict]=None) -> dict:\n    \"\"\"Check and filter keyword arguments to be used in the `fit` method of a regressor.\n\nThis function accepts a regressor object and a dictionary of keyword arguments (`fit_kwargs`). It validates that `fit_kwargs` is a dictionary, removes any keys that are not recognized by the regressor's `fit` method, and issues warnings for ignored keys. Specifically, if `sample_weight` is included in `fit_kwargs`, a warning is raised, and it is removed from the dictionary.\n\nParameters\n----------\nregressor : object\n    An instance of a regressor that adheres to the scikit-learn API, specifically requiring a `fit` method.\nfit_kwargs : dict, default `None`\n    A dictionary containing arguments intended for the `fit` method of the regressor.\n\nReturns\n-------\nfit_kwargs : dict\n    A filtered dictionary of arguments to be used in the `fit` method, containing only valid keys recognized by the regressor.\n\nNotes\n-----\nThe function leverages `inspect.signature()` from the `inspect` module to retrieve the parameter names of the `fit` method. It also utilizes `IgnoredArgumentWarning` from `..exceptions` to signal any ignored arguments to the user.\"\"\"\n    \"\\n    Check if `fit_kwargs` is a dict and select only the keys that are used by\\n    the `fit` method of the regressor.\\n\\n    Parameters\\n    ----------\\n    regressor : object\\n        Regressor object.\\n    fit_kwargs : dict, default `None`\\n        Dictionary with the arguments to pass to the `fit' method of the forecaster.\\n\\n    Returns\\n    -------\\n    fit_kwargs : dict\\n        Dictionary with the arguments to be passed to the `fit` method of the \\n        regressor after removing the unused keys.\\n    \\n    \"\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    else:\n        if not isinstance(fit_kwargs, dict):\n            raise TypeError(f'Argument `fit_kwargs` must be a dict. Got {type(fit_kwargs)}.')\n        non_used_keys = [k for k in fit_kwargs.keys() if k not in inspect.signature(regressor.fit).parameters]\n        if non_used_keys:\n            warnings.warn(f\"Argument/s {non_used_keys} ignored since they are not used by the regressor's `fit` method.\", IgnoredArgumentWarning)\n        if 'sample_weight' in fit_kwargs.keys():\n            warnings.warn('The `sample_weight` argument is ignored. Use `weight_func` to pass a function that defines the individual weights for each sample based on its index.', IgnoredArgumentWarning)\n            del fit_kwargs['sample_weight']\n        fit_kwargs = {k: v for k, v in fit_kwargs.items() if k in inspect.signature(regressor.fit).parameters}\n    return fit_kwargs",
        "docstring": "Check and filter keyword arguments to be used in the `fit` method of a regressor.\n\nThis function accepts a regressor object and a dictionary of keyword arguments (`fit_kwargs`). It validates that `fit_kwargs` is a dictionary, removes any keys that are not recognized by the regressor's `fit` method, and issues warnings for ignored keys. Specifically, if `sample_weight` is included in `fit_kwargs`, a warning is raised, and it is removed from the dictionary.\n\nParameters\n----------\nregressor : object\n    An instance of a regressor that adheres to the scikit-learn API, specifically requiring a `fit` method.\nfit_kwargs : dict, default `None`\n    A dictionary containing arguments intended for the `fit` method of the regressor.\n\nReturns\n-------\nfit_kwargs : dict\n    A filtered dictionary of arguments to be used in the `fit` method, containing only valid keys recognized by the regressor.\n\nNotes\n-----\nThe function leverages `inspect.signature()` from the `inspect` module to retrieve the parameter names of the `fit` method. It also utilizes `IgnoredArgumentWarning` from `..exceptions` to signal any ignored arguments to the user.",
        "signature": "def check_select_fit_kwargs(regressor: object, fit_kwargs: Optional[dict]=None) -> dict:",
        "type": "Function",
        "class_signature": null
      },
      "preprocess_y": {
        "code": "def preprocess_y(y: Union[pd.Series, pd.DataFrame], return_values: bool=True) -> Tuple[Union[None, np.ndarray], pd.Index]:\n    \"\"\"Preprocess the input time series `y` by separating its values and index, ensuring that the index adheres to specific rules related to its type and frequency. This function modifies the index based on the following criteria:\n\n- If the index is a `DatetimeIndex` with a defined frequency, it remains unchanged.\n- If the index is a `RangeIndex`, it also remains unchanged.\n- If the index is a `DatetimeIndex` without a frequency, it is replaced with a `RangeIndex`.\n- If the index is neither, it is replaced with a `RangeIndex`.\n\nParameters\n----------\ny : pandas Series, pandas DataFrame\n    The input time series that needs to be processed.\nreturn_values : bool, default `True`\n    Determines whether to return the values of `y` as a numpy ndarray. If set to `False`, only the index is returned.\n\nReturns\n-------\ny_values : None, numpy ndarray\n    A numpy array containing the values of `y` if `return_values` is `True`. Otherwise, returns `None`.\ny_index : pandas Index\n    The modified index of `y`, adhering to the stated rules about index types and frequencies.\n\nNotes\n-----\nThis function serves as a utility within the broader context of time series forecasting, ensuring consistent data formats before model fitting or prediction. It is crucial for functions that later use the output, enforcing expected formats for time series data, which are vital in methods like initialization and input checks within forecasters defined in the skforecast library.\"\"\"\n    '\\n    Return values and index of series separately. Index is overwritten \\n    according to the next rules:\\n    \\n    - If index is of type `DatetimeIndex` and has frequency, nothing is \\n    changed.\\n    - If index is of type `RangeIndex`, nothing is changed.\\n    - If index is of type `DatetimeIndex` but has no frequency, a \\n    `RangeIndex` is created.\\n    - If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\\n    \\n    Parameters\\n    ----------\\n    y : pandas Series, pandas DataFrame\\n        Time series.\\n    return_values : bool, default `True`\\n        If `True` return the values of `y` as numpy ndarray. This option is \\n        intended to avoid copying data when it is not necessary.\\n\\n    Returns\\n    -------\\n    y_values : None, numpy ndarray\\n        Numpy array with values of `y`.\\n    y_index : pandas Index\\n        Index of `y` modified according to the rules.\\n    \\n    '\n    if isinstance(y.index, pd.DatetimeIndex) and y.index.freq is not None:\n        y_index = y.index\n    elif isinstance(y.index, pd.RangeIndex):\n        y_index = y.index\n    elif isinstance(y.index, pd.DatetimeIndex) and y.index.freq is None:\n        warnings.warn('Series has DatetimeIndex index but no frequency. Index is overwritten with a RangeIndex of step 1.')\n        y_index = pd.RangeIndex(start=0, stop=len(y), step=1)\n    else:\n        warnings.warn('Series has no DatetimeIndex nor RangeIndex index. Index is overwritten with a RangeIndex.')\n        y_index = pd.RangeIndex(start=0, stop=len(y), step=1)\n    y_values = y.to_numpy(copy=True).ravel() if return_values else None\n    return (y_values, y_index)",
        "docstring": "Preprocess the input time series `y` by separating its values and index, ensuring that the index adheres to specific rules related to its type and frequency. This function modifies the index based on the following criteria:\n\n- If the index is a `DatetimeIndex` with a defined frequency, it remains unchanged.\n- If the index is a `RangeIndex`, it also remains unchanged.\n- If the index is a `DatetimeIndex` without a frequency, it is replaced with a `RangeIndex`.\n- If the index is neither, it is replaced with a `RangeIndex`.\n\nParameters\n----------\ny : pandas Series, pandas DataFrame\n    The input time series that needs to be processed.\nreturn_values : bool, default `True`\n    Determines whether to return the values of `y` as a numpy ndarray. If set to `False`, only the index is returned.\n\nReturns\n-------\ny_values : None, numpy ndarray\n    A numpy array containing the values of `y` if `return_values` is `True`. Otherwise, returns `None`.\ny_index : pandas Index\n    The modified index of `y`, adhering to the stated rules about index types and frequencies.\n\nNotes\n-----\nThis function serves as a utility within the broader context of time series forecasting, ensuring consistent data formats before model fitting or prediction. It is crucial for functions that later use the output, enforcing expected formats for time series data, which are vital in methods like initialization and input checks within forecasters defined in the skforecast library.",
        "signature": "def preprocess_y(y: Union[pd.Series, pd.DataFrame], return_values: bool=True) -> Tuple[Union[None, np.ndarray], pd.Index]:",
        "type": "Function",
        "class_signature": null
      }
    },
    "skforecast/recursive/_forecaster_recursive.py": {
      "ForecasterRecursive.__init__": {
        "code": "    def __init__(self, regressor: object, lags: Optional[Union[int, list, np.ndarray, range]]=None, window_features: Optional[Union[object, list]]=None, transformer_y: Optional[object]=None, transformer_exog: Optional[object]=None, weight_func: Optional[Callable]=None, differentiation: Optional[int]=None, fit_kwargs: Optional[dict]=None, binner_kwargs: Optional[dict]=None, forecaster_id: Optional[Union[str, int]]=None) -> None:\n        \"\"\"Initializes a ForecasterRecursive instance, which enables the use of any scikit-learn compatible regressor for recursive autoregressive forecasting. The constructor sets up key parameters such as lags, window features, transformers for target and exogenous variables, differentiation order, and additional arguments for fitting the regressor. It also checks that at least one predictor source (lags or window features) is provided.\n\n    Parameters\n    ----------\n    regressor : object\n        An instance of a regressor or pipeline compatible with the scikit-learn API for training and predictions.\n    lags : Optional[Union[int, list, np.ndarray, range]], default=None\n        Specifies the lags used as predictors. It can be an integer defining a range, a list of specific lags, or None if no lags are to be included.\n    window_features : Optional[Union[object, list]], default=None\n        Instance or list of instances for creating additional predictors from the time series.\n    transformer_y : Optional[object], default=None\n        Preprocessor compatible with scikit-learn's API for the target variable.\n    transformer_exog : Optional[object], default=None\n        Preprocessor for the exogenous variables.\n    weight_func : Optional[Callable], default=None\n        Custom function to assign weights to samples.\n    differentiation : Optional[int], default=None\n        Specifies the order of differencing to apply to the time series; if None, no differencing occurs.\n    fit_kwargs : Optional[dict], default=None\n        Additional fitting parameters for the regressor's fit method.\n    binner_kwargs : Optional[dict], default=None\n        Configuration parameters for binning residuals based on predicted values.\n    forecaster_id : Optional[Union[str, int]], default=None\n        Identifier for the forecaster instance.\n\n    Attributes\n    ----------\n    self.lags, self.window_features, self.differentiation, self.binner : sets up varying configurations based on the provided parameters, ensuring that the forecaster is capable of handling the input data and specified transformations effectively.\n    \n    Raises\n    ------\n    ValueError\n        If both `lags` and `window_features` are None, or if `differentiation` is not a valid integer.\"\"\"\n        self.regressor = copy(regressor)\n        self.transformer_y = transformer_y\n        self.transformer_exog = transformer_exog\n        self.weight_func = weight_func\n        self.source_code_weight_func = None\n        self.differentiation = differentiation\n        self.differentiator = None\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.exog_in_ = False\n        self.exog_names_in_ = None\n        self.exog_type_in_ = None\n        self.exog_dtypes_in_ = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_ = None\n        self.X_train_features_names_out_ = None\n        self.in_sample_residuals_ = None\n        self.out_sample_residuals_ = None\n        self.in_sample_residuals_by_bin_ = None\n        self.out_sample_residuals_by_bin_ = None\n        self.creation_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.is_fitted = False\n        self.fit_date = None\n        self.skforecast_version = skforecast.__version__\n        self.python_version = sys.version.split(' ')[0]\n        self.forecaster_id = forecaster_id\n        self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\n        self.window_features, self.window_features_names, self.max_size_window_features = initialize_window_features(window_features)\n        if self.window_features is None and self.lags is None:\n            raise ValueError('At least one of the arguments `lags` or `window_features` must be different from None. This is required to create the predictors used in training the forecaster.')\n        self.window_size = max([ws for ws in [self.max_lag, self.max_size_window_features] if ws is not None])\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [type(wf).__name__ for wf in self.window_features]\n        self.binner_kwargs = binner_kwargs\n        if binner_kwargs is None:\n            self.binner_kwargs = {'n_bins': 10, 'method': 'linear', 'subsample': 200000, 'random_state': 789654, 'dtype': np.float64}\n        self.binner = QuantileBinner(**self.binner_kwargs)\n        self.binner_intervals_ = None\n        if self.differentiation is not None:\n            if not isinstance(differentiation, int) or differentiation < 1:\n                raise ValueError(f'Argument `differentiation` must be an integer equal to or greater than 1. Got {differentiation}.')\n            self.window_size += self.differentiation\n            self.differentiator = TimeSeriesDifferentiator(order=self.differentiation, window_size=self.window_size)\n        self.weight_func, self.source_code_weight_func, _ = initialize_weights(forecaster_name=type(self).__name__, regressor=regressor, weight_func=weight_func, series_weights=None)\n        self.fit_kwargs = check_select_fit_kwargs(regressor=regressor, fit_kwargs=fit_kwargs)",
        "docstring": "Initializes a ForecasterRecursive instance, which enables the use of any scikit-learn compatible regressor for recursive autoregressive forecasting. The constructor sets up key parameters such as lags, window features, transformers for target and exogenous variables, differentiation order, and additional arguments for fitting the regressor. It also checks that at least one predictor source (lags or window features) is provided.\n\nParameters\n----------\nregressor : object\n    An instance of a regressor or pipeline compatible with the scikit-learn API for training and predictions.\nlags : Optional[Union[int, list, np.ndarray, range]], default=None\n    Specifies the lags used as predictors. It can be an integer defining a range, a list of specific lags, or None if no lags are to be included.\nwindow_features : Optional[Union[object, list]], default=None\n    Instance or list of instances for creating additional predictors from the time series.\ntransformer_y : Optional[object], default=None\n    Preprocessor compatible with scikit-learn's API for the target variable.\ntransformer_exog : Optional[object], default=None\n    Preprocessor for the exogenous variables.\nweight_func : Optional[Callable], default=None\n    Custom function to assign weights to samples.\ndifferentiation : Optional[int], default=None\n    Specifies the order of differencing to apply to the time series; if None, no differencing occurs.\nfit_kwargs : Optional[dict], default=None\n    Additional fitting parameters for the regressor's fit method.\nbinner_kwargs : Optional[dict], default=None\n    Configuration parameters for binning residuals based on predicted values.\nforecaster_id : Optional[Union[str, int]], default=None\n    Identifier for the forecaster instance.\n\nAttributes\n----------\nself.lags, self.window_features, self.differentiation, self.binner : sets up varying configurations based on the provided parameters, ensuring that the forecaster is capable of handling the input data and specified transformations effectively.\n\nRaises\n------\nValueError\n    If both `lags` and `window_features` are None, or if `differentiation` is not a valid integer.",
        "signature": "def __init__(self, regressor: object, lags: Optional[Union[int, list, np.ndarray, range]]=None, window_features: Optional[Union[object, list]]=None, transformer_y: Optional[object]=None, transformer_exog: Optional[object]=None, weight_func: Optional[Callable]=None, differentiation: Optional[int]=None, fit_kwargs: Optional[dict]=None, binner_kwargs: Optional[dict]=None, forecaster_id: Optional[Union[str, int]]=None) -> None:",
        "type": "Method",
        "class_signature": "class ForecasterRecursive(ForecasterBase):"
      },
      "ForecasterRecursive._create_train_X_y": {
        "code": "    def _create_train_X_y(self, y: pd.Series, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, pd.Series, list, list, list, list, dict]:\n        \"\"\"Create training matrices from a univariate time series and optional\n        exogenous variables for the ForecasterRecursive model. This method generates\n        the predictor matrix (`X_train`) and the target variable array (`y_train`)\n        which will be used for training the underlying regression model. It also\n        handles data transformation, window features, and lag generation based on\n        forecaster parameters.\n\n        Parameters\n        ----------\n        y : pandas.Series\n            The training time series data with aligned index.\n        exog : Optional[Union[pandas.Series, pandas.DataFrame]], default `None`\n            Optional exogenous variables included as predictors. They must have\n            the same number of observations as `y`.\n\n        Returns\n        -------\n        Tuple[pandas.DataFrame, pandas.Series, list, list, list, list, dict]\n            A tuple containing the following elements:\n                - X_train : pandas.DataFrame\n                    The training values (predictors) created from lags and window features.\n                - y_train : pandas.Series\n                    The values of the time series corresponding to each row of `X_train`.\n                - exog_names_in_ : list\n                    The names of exogenous variables used during training.\n                - X_train_window_features_names_out_ : list\n                    The names of window features included in `X_train`.\n                - X_train_exog_names_out_ : list\n                    The names of exogenous variables included in `X_train`.\n                - X_train_features_names_out_ : list\n                    The names of the columns in the training matrix created internally.\n                - exog_dtypes_in_ : dict\n                    Data types of each exogenous variable used in training, if any.\n\n        Raises\n        ------\n        ValueError\n            If the length of `y` is not greater than the maximum window size \n            required for the forecaster.\n        TypeError\n            If the exogenous data format is incorrect or does not match the expected input structure.\n\n        Dependencies\n        -------------\n        - validate_y() and preprocess_y() are utility functions to validate \n          the time series and preprocess it.\n        - input_to_frame() transforms the input data format into a pandas DataFrame.\n        - transform_dataframe() applies transformations to `y` and `exog` based on any provided transformers.\n        - _create_lags() generates the predictor variables by creating lagged versions of the input series.\n        - _create_window_features() generates additional features based on window transformations if any are supplied.\"\"\"\n        '\\n        Create training matrices from univariate time series and exogenous\\n        variables.\\n        \\n        Parameters\\n        ----------\\n        y : pandas Series\\n            Training time series.\\n        exog : pandas Series, pandas DataFrame, default `None`\\n            Exogenous variable/s included as predictor/s. Must have the same\\n            number of observations as `y` and their indexes must be aligned.\\n\\n        Returns\\n        -------\\n        X_train : pandas DataFrame\\n            Training values (predictors).\\n        y_train : pandas Series\\n            Values of the time series related to each row of `X_train`.\\n        exog_names_in_ : list\\n            Names of the exogenous variables used during training.\\n        X_train_window_features_names_out_ : list\\n            Names of the window features included in the matrix `X_train` created\\n            internally for training.\\n        X_train_exog_names_out_ : list\\n            Names of the exogenous variables included in the matrix `X_train` created\\n            internally for training. It can be different from `exog_names_in_` if\\n            some exogenous variables are transformed during the training process.\\n        X_train_features_names_out_ : list\\n            Names of the columns of the matrix created internally for training.\\n        exog_dtypes_in_ : dict\\n            Type of each exogenous variable/s used in training. If `transformer_exog` \\n            is used, the dtypes are calculated before the transformation.\\n        \\n        '\n        check_y(y=y)\n        y = input_to_frame(data=y, input_name='y')\n        if len(y) <= self.window_size:\n            raise ValueError(f'Length of `y` must be greater than the maximum window size needed by the forecaster.\\n    Length `y`: {len(y)}.\\n    Max window size: {self.window_size}.\\n    Lags window size: {self.max_lag}.\\n    Window features window size: {self.max_size_window_features}.')\n        fit_transformer = False if self.is_fitted else True\n        y = transform_dataframe(df=y, transformer=self.transformer_y, fit=fit_transformer, inverse_transform=False)\n        y_values, y_index = preprocess_y(y=y)\n        train_index = y_index[self.window_size:]\n        if self.differentiation is not None:\n            if not self.is_fitted:\n                y_values = self.differentiator.fit_transform(y_values)\n            else:\n                differentiator = copy(self.differentiator)\n                y_values = differentiator.fit_transform(y_values)\n        exog_names_in_ = None\n        exog_dtypes_in_ = None\n        categorical_features = False\n        if exog is not None:\n            check_exog(exog=exog, allow_nan=True)\n            exog = input_to_frame(data=exog, input_name='exog')\n            len_y = len(y_values)\n            len_train_index = len(train_index)\n            len_exog = len(exog)\n            if not len_exog == len_y and (not len_exog == len_train_index):\n                raise ValueError(f'Length of `exog` must be equal to the length of `y` (if index is fully aligned) or length of `y` - `window_size` (if `exog` starts after the first `window_size` values).\\n    `exog`              : ({exog.index[0]} -- {exog.index[-1]})  (n={len_exog})\\n    `y`                 : ({y.index[0]} -- {y.index[-1]})  (n={len_y})\\n    `y` - `window_size` : ({train_index[0]} -- {train_index[-1]})  (n={len_train_index})')\n            exog_names_in_ = exog.columns.to_list()\n            exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n            exog = transform_dataframe(df=exog, transformer=self.transformer_exog, fit=fit_transformer, inverse_transform=False)\n            check_exog_dtypes(exog, call_check_exog=True)\n            categorical_features = exog.select_dtypes(include=np.number).shape[1] != exog.shape[1]\n            _, exog_index = preprocess_exog(exog=exog, return_values=False)\n            if len_exog == len_y:\n                if not (exog_index == y_index).all():\n                    raise ValueError('When `exog` has the same length as `y`, the index of `exog` must be aligned with the index of `y` to ensure the correct alignment of values.')\n                exog = exog.iloc[self.window_size:,]\n            elif not (exog_index == train_index).all():\n                raise ValueError(\"When `exog` doesn't contain the first `window_size` observations, the index of `exog` must be aligned with the index of `y` minus the first `window_size` observations to ensure the correct alignment of values.\")\n        X_train = []\n        X_train_features_names_out_ = []\n        X_as_pandas = True if categorical_features else False\n        X_train_lags, y_train = self._create_lags(y=y_values, X_as_pandas=X_as_pandas, train_index=train_index)\n        if X_train_lags is not None:\n            X_train.append(X_train_lags)\n            X_train_features_names_out_.extend(self.lags_names)\n        X_train_window_features_names_out_ = None\n        if self.window_features is not None:\n            n_diff = 0 if self.differentiation is None else self.differentiation\n            y_window_features = pd.Series(y_values[n_diff:], index=y_index[n_diff:])\n            X_train_window_features, X_train_window_features_names_out_ = self._create_window_features(y=y_window_features, X_as_pandas=X_as_pandas, train_index=train_index)\n            X_train.extend(X_train_window_features)\n            X_train_features_names_out_.extend(X_train_window_features_names_out_)\n        X_train_exog_names_out_ = None\n        if exog is not None:\n            X_train_exog_names_out_ = exog.columns.to_list()\n            if not X_as_pandas:\n                exog = exog.to_numpy()\n            X_train_features_names_out_.extend(X_train_exog_names_out_)\n            X_train.append(exog)\n        if len(X_train) == 1:\n            X_train = X_train[0]\n        elif X_as_pandas:\n            X_train = pd.concat(X_train, axis=1)\n        else:\n            X_train = np.concatenate(X_train, axis=1)\n        if X_as_pandas:\n            X_train.index = train_index\n        else:\n            X_train = pd.DataFrame(data=X_train, index=train_index, columns=X_train_features_names_out_)\n        y_train = pd.Series(data=y_train, index=train_index, name='y')\n        return (X_train, y_train, exog_names_in_, X_train_window_features_names_out_, X_train_exog_names_out_, X_train_features_names_out_, exog_dtypes_in_)",
        "docstring": "Create training matrices from a univariate time series and optional\nexogenous variables for the ForecasterRecursive model. This method generates\nthe predictor matrix (`X_train`) and the target variable array (`y_train`)\nwhich will be used for training the underlying regression model. It also\nhandles data transformation, window features, and lag generation based on\nforecaster parameters.\n\nParameters\n----------\ny : pandas.Series\n    The training time series data with aligned index.\nexog : Optional[Union[pandas.Series, pandas.DataFrame]], default `None`\n    Optional exogenous variables included as predictors. They must have\n    the same number of observations as `y`.\n\nReturns\n-------\nTuple[pandas.DataFrame, pandas.Series, list, list, list, list, dict]\n    A tuple containing the following elements:\n        - X_train : pandas.DataFrame\n            The training values (predictors) created from lags and window features.\n        - y_train : pandas.Series\n            The values of the time series corresponding to each row of `X_train`.\n        - exog_names_in_ : list\n            The names of exogenous variables used during training.\n        - X_train_window_features_names_out_ : list\n            The names of window features included in `X_train`.\n        - X_train_exog_names_out_ : list\n            The names of exogenous variables included in `X_train`.\n        - X_train_features_names_out_ : list\n            The names of the columns in the training matrix created internally.\n        - exog_dtypes_in_ : dict\n            Data types of each exogenous variable used in training, if any.\n\nRaises\n------\nValueError\n    If the length of `y` is not greater than the maximum window size \n    required for the forecaster.\nTypeError\n    If the exogenous data format is incorrect or does not match the expected input structure.\n\nDependencies\n-------------\n- validate_y() and preprocess_y() are utility functions to validate \n  the time series and preprocess it.\n- input_to_frame() transforms the input data format into a pandas DataFrame.\n- transform_dataframe() applies transformations to `y` and `exog` based on any provided transformers.\n- _create_lags() generates the predictor variables by creating lagged versions of the input series.\n- _create_window_features() generates additional features based on window transformations if any are supplied.",
        "signature": "def _create_train_X_y(self, y: pd.Series, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, pd.Series, list, list, list, list, dict]:",
        "type": "Method",
        "class_signature": "class ForecasterRecursive(ForecasterBase):"
      },
      "ForecasterRecursive.create_sample_weights": {
        "code": "    def create_sample_weights(self, X_train: pd.DataFrame) -> np.ndarray:\n        \"\"\"Create observation weights for each instance in the training dataset based on the provided `weight_func`.\n\nParameters\n----------\nX_train : pd.DataFrame\n    Dataframe containing the training features created from the `create_train_X_y` method.\n\nReturns\n-------\nsample_weight : numpy ndarray\n    Array of weights corresponding to each observation in `X_train` to be used in the `fit` method of the regressor.\n\nRaises\n------\nValueError\n    If any of the generated weights are NaN, negative, or if the sum of weights is zero.\n\nDependencies\n------------\nThis method relies on the `weight_func` attribute of the ForecasterRecursive instance, which is set during initialization. The `weight_func` should be a callable that takes an index and returns an array-like of weights. It influences how individual observations are weighted during model fitting, thereby affecting the learning process of the underlying regressor.\"\"\"\n        \"\\n        Crate weights for each observation according to the forecaster's attribute\\n        `weight_func`.\\n\\n        Parameters\\n        ----------\\n        X_train : pandas DataFrame\\n            Dataframe created with the `create_train_X_y` method, first return.\\n\\n        Returns\\n        -------\\n        sample_weight : numpy ndarray\\n            Weights to use in `fit` method.\\n\\n        \"\n        sample_weight = None\n        if self.weight_func is not None:\n            sample_weight = self.weight_func(X_train.index)\n        if sample_weight is not None:\n            if np.isnan(sample_weight).any():\n                raise ValueError('The resulting `sample_weight` cannot have NaN values.')\n            if np.any(sample_weight < 0):\n                raise ValueError('The resulting `sample_weight` cannot have negative values.')\n            if np.sum(sample_weight) == 0:\n                raise ValueError('The resulting `sample_weight` cannot be normalized because the sum of the weights is zero.')\n        return sample_weight",
        "docstring": "Create observation weights for each instance in the training dataset based on the provided `weight_func`.\n\nParameters\n----------\nX_train : pd.DataFrame\n    Dataframe containing the training features created from the `create_train_X_y` method.\n\nReturns\n-------\nsample_weight : numpy ndarray\n    Array of weights corresponding to each observation in `X_train` to be used in the `fit` method of the regressor.\n\nRaises\n------\nValueError\n    If any of the generated weights are NaN, negative, or if the sum of weights is zero.\n\nDependencies\n------------\nThis method relies on the `weight_func` attribute of the ForecasterRecursive instance, which is set during initialization. The `weight_func` should be a callable that takes an index and returns an array-like of weights. It influences how individual observations are weighted during model fitting, thereby affecting the learning process of the underlying regressor.",
        "signature": "def create_sample_weights(self, X_train: pd.DataFrame) -> np.ndarray:",
        "type": "Method",
        "class_signature": "class ForecasterRecursive(ForecasterBase):"
      },
      "ForecasterRecursive.fit": {
        "code": "    def fit(self, y: pd.Series, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, store_last_window: bool=True, store_in_sample_residuals: bool=True, random_state: int=123) -> None:\n        \"\"\"Fit the ForecasterRecursive model to the training time series and optional exogenous variables. This method processes the input data, trains the regressor specified during initialization, and stores important attributes related to the trained model. It allows for the use of sample weights and manages the storage of residuals and the last observed window for future predictions.\n\nParameters\n----------\ny : pandas Series\n    The training time series values used to fit the forecaster.\n\nexog : pandas Series or pandas DataFrame, optional, default `None`\n    Exogenous variables included as additional predictors. Must align with the index of `y` in terms of length and ordering.\n\nstore_last_window : bool, optional, default `True`\n    If `True`, stores the last observed data window in attribute `last_window_` for making future predictions.\n\nstore_in_sample_residuals : bool, optional, default `True`\n    If `True`, calculates and stores in-sample residuals as `in_sample_residuals_`.\n\nrandom_state : int, optional, default `123`\n    Seed for random number generation to ensure reproducibility, particularly regarding residual sampling.\n\nReturns\n-------\nNone\n\nNotes\n-----\nThe method initializes several attributes such as `last_window_`, `exog_in_`, and `training_range_`, and handles transformations of `y` and `exog` using the appropriate transformers, stored in `transformer_y` and `transformer_exog`. The method will raise errors if the input data is not of the expected alignments or types. It also prepares the sample weights if defined, and invokes the regression model's `fit` method to train the forecaster.\"\"\"\n        '\\n        Training Forecaster.\\n\\n        Additional arguments to be passed to the `fit` method of the regressor \\n        can be added with the `fit_kwargs` argument when initializing the forecaster.\\n        \\n        Parameters\\n        ----------\\n        y : pandas Series\\n            Training time series.\\n        exog : pandas Series, pandas DataFrame, default `None`\\n            Exogenous variable/s included as predictor/s. Must have the same\\n            number of observations as `y` and their indexes must be aligned so\\n            that y[i] is regressed on exog[i].\\n        store_last_window : bool, default `True`\\n            Whether or not to store the last window (`last_window_`) of training data.\\n        store_in_sample_residuals : bool, default `True`\\n            If `True`, in-sample residuals will be stored in the forecaster object\\n            after fitting (`in_sample_residuals_` attribute).\\n        random_state : int, default `123`\\n            Set a seed for the random generator so that the stored sample \\n            residuals are always deterministic.\\n\\n        Returns\\n        -------\\n        None\\n        \\n        '\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.exog_in_ = False\n        self.exog_names_in_ = None\n        self.exog_type_in_ = None\n        self.exog_dtypes_in_ = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_ = None\n        self.X_train_features_names_out_ = None\n        self.in_sample_residuals_ = None\n        self.is_fitted = False\n        self.fit_date = None\n        X_train, y_train, exog_names_in_, X_train_window_features_names_out_, X_train_exog_names_out_, X_train_features_names_out_, exog_dtypes_in_ = self._create_train_X_y(y=y, exog=exog)\n        sample_weight = self.create_sample_weights(X_train=X_train)\n        if sample_weight is not None:\n            self.regressor.fit(X=X_train, y=y_train, sample_weight=sample_weight, **self.fit_kwargs)\n        else:\n            self.regressor.fit(X=X_train, y=y_train, **self.fit_kwargs)\n        self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n        self.X_train_features_names_out_ = X_train_features_names_out_\n        self.is_fitted = True\n        self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.training_range_ = preprocess_y(y=y, return_values=False)[1][[0, -1]]\n        self.index_type_ = type(X_train.index)\n        if isinstance(X_train.index, pd.DatetimeIndex):\n            self.index_freq_ = X_train.index.freqstr\n        else:\n            self.index_freq_ = X_train.index.step\n        if exog is not None:\n            self.exog_in_ = True\n            self.exog_type_in_ = type(exog)\n            self.exog_names_in_ = exog_names_in_\n            self.exog_dtypes_in_ = exog_dtypes_in_\n            self.X_train_exog_names_out_ = X_train_exog_names_out_\n        if store_in_sample_residuals:\n            self._binning_in_sample_residuals(y_true=y_train.to_numpy(), y_pred=self.regressor.predict(X_train).ravel(), random_state=random_state)\n        if store_last_window:\n            self.last_window_ = y.iloc[-self.window_size:].copy().to_frame(name=y.name if y.name is not None else 'y')",
        "docstring": "Fit the ForecasterRecursive model to the training time series and optional exogenous variables. This method processes the input data, trains the regressor specified during initialization, and stores important attributes related to the trained model. It allows for the use of sample weights and manages the storage of residuals and the last observed window for future predictions.\n\nParameters\n----------\ny : pandas Series\n    The training time series values used to fit the forecaster.\n\nexog : pandas Series or pandas DataFrame, optional, default `None`\n    Exogenous variables included as additional predictors. Must align with the index of `y` in terms of length and ordering.\n\nstore_last_window : bool, optional, default `True`\n    If `True`, stores the last observed data window in attribute `last_window_` for making future predictions.\n\nstore_in_sample_residuals : bool, optional, default `True`\n    If `True`, calculates and stores in-sample residuals as `in_sample_residuals_`.\n\nrandom_state : int, optional, default `123`\n    Seed for random number generation to ensure reproducibility, particularly regarding residual sampling.\n\nReturns\n-------\nNone\n\nNotes\n-----\nThe method initializes several attributes such as `last_window_`, `exog_in_`, and `training_range_`, and handles transformations of `y` and `exog` using the appropriate transformers, stored in `transformer_y` and `transformer_exog`. The method will raise errors if the input data is not of the expected alignments or types. It also prepares the sample weights if defined, and invokes the regression model's `fit` method to train the forecaster.",
        "signature": "def fit(self, y: pd.Series, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, store_last_window: bool=True, store_in_sample_residuals: bool=True, random_state: int=123) -> None:",
        "type": "Method",
        "class_signature": "class ForecasterRecursive(ForecasterBase):"
      },
      "ForecasterRecursive._binning_in_sample_residuals": {
        "code": "    def _binning_in_sample_residuals(self, y_true: np.ndarray, y_pred: np.ndarray, random_state: int=123) -> None:\n        \"\"\"Bins residuals based on the predicted values generated by the forecaster. \n        The method stores residuals associated with each bin, which is derived from \n        the predicted values. Residuals are calculated as the difference between \n        the true values (`y_true`) and the predicted values (`y_pred`). \n        The `QuantileBinner` is used to create bins for the residuals, ensuring \n        that the residuals can be sampled effectively for bootstrapping purposes \n        during prediction.\n\n        Parameters\n        ----------\n        y_true : numpy ndarray\n            The true values of the time series used to compute the residuals. \n        y_pred : numpy ndarray\n            The predicted values of the time series used to compute the residuals. \n        random_state : int, default `123`\n            Controls the random number generator for reproducibility of the \n            sampled residuals.\n\n        Returns\n        -------\n        None\n        \n        Side Effects\n        -------------\n        Residuals are stored in the attributes `in_sample_residuals_` and \n        `in_sample_residuals_by_bin_`. The attribute `binner_intervals_` \n        is updated to store the interval boundaries used during binning. \n        The maximum number of residuals kept per bin is limited to `10_000 // \n        self.binner.n_bins_`, and the overall count of stored residuals is \n        capped at `10,000`.\n\n        Dependencies\n        -------------\n        This method relies on the `QuantileBinner` class from the library \n        `skforecast.preprocessing`, specifically its ability to define bins \n        based on quantiles of the predicted values.\"\"\"\n        '\\n        Binning residuals according to the predicted value each residual is\\n        associated with. First a skforecast.preprocessing.QuantileBinner object\\n        is fitted to the predicted values. Then, residuals are binned according\\n        to the predicted value each residual is associated with. Residuals are\\n        stored in the forecaster object as `in_sample_residuals_` and\\n        `in_sample_residuals_by_bin_`.\\n        If `transformer_y` is not `None`, `y_true` and `y_pred` are transformed\\n        before calculating residuals. If `differentiation` is not `None`, `y_true`\\n        and `y_pred` are differentiated before calculating residuals. If both,\\n        `transformer_y` and `differentiation` are not `None`, transformation is\\n        done before differentiation. The number of residuals stored per bin is\\n        limited to  `10_000 // self.binner.n_bins_`. The total number of residuals\\n        stored is `10_000`.\\n        **New in version 0.14.0**\\n\\n        Parameters\\n        ----------\\n        y_true : numpy ndarray\\n            True values of the time series.\\n        y_pred : numpy ndarray\\n            Predicted values of the time series.\\n        random_state : int, default `123`\\n            Set a seed for the random generator so that the stored sample \\n            residuals are always deterministic.\\n\\n        Returns\\n        -------\\n        None\\n        \\n        '\n        data = pd.DataFrame({'prediction': y_pred, 'residuals': y_true - y_pred})\n        data['bin'] = self.binner.fit_transform(y_pred).astype(int)\n        self.in_sample_residuals_by_bin_ = data.groupby('bin')['residuals'].apply(np.array).to_dict()\n        rng = np.random.default_rng(seed=random_state)\n        max_sample = 10000 // self.binner.n_bins_\n        for k, v in self.in_sample_residuals_by_bin_.items():\n            if len(v) > max_sample:\n                sample = v[rng.integers(low=0, high=len(v), size=max_sample)]\n                self.in_sample_residuals_by_bin_[k] = sample\n        self.in_sample_residuals_ = np.concatenate(list(self.in_sample_residuals_by_bin_.values()))\n        self.binner_intervals_ = self.binner.intervals_",
        "docstring": "Bins residuals based on the predicted values generated by the forecaster. \nThe method stores residuals associated with each bin, which is derived from \nthe predicted values. Residuals are calculated as the difference between \nthe true values (`y_true`) and the predicted values (`y_pred`). \nThe `QuantileBinner` is used to create bins for the residuals, ensuring \nthat the residuals can be sampled effectively for bootstrapping purposes \nduring prediction.\n\nParameters\n----------\ny_true : numpy ndarray\n    The true values of the time series used to compute the residuals. \ny_pred : numpy ndarray\n    The predicted values of the time series used to compute the residuals. \nrandom_state : int, default `123`\n    Controls the random number generator for reproducibility of the \n    sampled residuals.\n\nReturns\n-------\nNone\n\nSide Effects\n-------------\nResiduals are stored in the attributes `in_sample_residuals_` and \n`in_sample_residuals_by_bin_`. The attribute `binner_intervals_` \nis updated to store the interval boundaries used during binning. \nThe maximum number of residuals kept per bin is limited to `10_000 // \nself.binner.n_bins_`, and the overall count of stored residuals is \ncapped at `10,000`.\n\nDependencies\n-------------\nThis method relies on the `QuantileBinner` class from the library \n`skforecast.preprocessing`, specifically its ability to define bins \nbased on quantiles of the predicted values.",
        "signature": "def _binning_in_sample_residuals(self, y_true: np.ndarray, y_pred: np.ndarray, random_state: int=123) -> None:",
        "type": "Method",
        "class_signature": "class ForecasterRecursive(ForecasterBase):"
      }
    },
    "skforecast/preprocessing/preprocessing.py": {
      "QuantileBinner.__init__": {
        "code": "    def __init__(self, n_bins: int, method: Optional[str]='linear', subsample: int=200000, dtype: Optional[type]=np.float64, random_state: Optional[int]=789654):\n        \"\"\"Initialize the QuantileBinner class for binning data into quantile-based bins.\n\nParameters\n----------\nn_bins : int\n    The number of quantile-based bins to create. Must be greater than 1.\nmethod : Optional[str], default='linear'\n    The method used to compute the quantiles, as passed to `numpy.percentile`. \n    Valid options include 'linear', 'median_unbiased', 'normal_unbiased', and others.\nsubsample : int, default=200000\n    The number of samples to use for computing quantiles. If the dataset has \n    more samples than `subsample`, a random subset will be used.\ndtype : Optional[type], default=numpy.float64\n    The data type for the bin indices.\nrandom_state : Optional[int], default=789654\n    The random seed for generating a random subset of the data.\n\nAttributes\n----------\nn_bins : int\n    The number of quantile-based bins defined by the user.\nmethod : str\n    The method selected for quantile computation.\nsubsample : int\n    The maximum number of samples to consider for fitting.\nrandom_state : int\n    The random seed used for sampling.\ndtype : type\n    The numpy data type for the bin indices.\nn_bins_ : int or None\n    The number of bins learned during fitting.\nbin_edges_ : numpy.ndarray or None\n    The edges of the bins determined from training data.\nintervals_ : dict or None\n    The intervals defining the bin edges.\n\nThis constructor calls `_validate_params` to ensure that input parameters are valid \nbefore initializing the class attributes. It interacts with the overall functionality \nof the QuantileBinner class, which allows users to fit data into quantile-based bins \nand transform new data into bin indices based on those fitted quantiles.\"\"\"\n        self._validate_params(n_bins, method, subsample, dtype, random_state)\n        self.n_bins = n_bins\n        self.method = method\n        self.subsample = subsample\n        self.random_state = random_state\n        self.dtype = dtype\n        self.n_bins_ = None\n        self.bin_edges_ = None\n        self.intervals_ = None",
        "docstring": "Initialize the QuantileBinner class for binning data into quantile-based bins.\n\nParameters\n----------\nn_bins : int\n    The number of quantile-based bins to create. Must be greater than 1.\nmethod : Optional[str], default='linear'\n    The method used to compute the quantiles, as passed to `numpy.percentile`. \n    Valid options include 'linear', 'median_unbiased', 'normal_unbiased', and others.\nsubsample : int, default=200000\n    The number of samples to use for computing quantiles. If the dataset has \n    more samples than `subsample`, a random subset will be used.\ndtype : Optional[type], default=numpy.float64\n    The data type for the bin indices.\nrandom_state : Optional[int], default=789654\n    The random seed for generating a random subset of the data.\n\nAttributes\n----------\nn_bins : int\n    The number of quantile-based bins defined by the user.\nmethod : str\n    The method selected for quantile computation.\nsubsample : int\n    The maximum number of samples to consider for fitting.\nrandom_state : int\n    The random seed used for sampling.\ndtype : type\n    The numpy data type for the bin indices.\nn_bins_ : int or None\n    The number of bins learned during fitting.\nbin_edges_ : numpy.ndarray or None\n    The edges of the bins determined from training data.\nintervals_ : dict or None\n    The intervals defining the bin edges.\n\nThis constructor calls `_validate_params` to ensure that input parameters are valid \nbefore initializing the class attributes. It interacts with the overall functionality \nof the QuantileBinner class, which allows users to fit data into quantile-based bins \nand transform new data into bin indices based on those fitted quantiles.",
        "signature": "def __init__(self, n_bins: int, method: Optional[str]='linear', subsample: int=200000, dtype: Optional[type]=np.float64, random_state: Optional[int]=789654):",
        "type": "Method",
        "class_signature": "class QuantileBinner:"
      },
      "QuantileBinner.fit_transform": {
        "code": "    def fit_transform(self, X):\n        \"\"\"Fit the model to the provided data and return the bin indices corresponding \nto the input values based on learned quantile-based bins.\n\nParameters\n----------\nX : numpy.ndarray\n    The data to fit the model and transform into bin indices.\n\nReturns\n-------\nbin_indices : numpy.ndarray\n    An array of indices representing the bins each value from `X` belongs to. \n    Values less than the smallest bin edge are assigned to the first bin \n    (index 0), and values greater than the largest bin edge are assigned to \n    the last bin (index n_bins - 1).\n\nThis method interacts with the `fit` method, which computes the bin edges \nbased on quantiles, and the `transform` method, which assigns bin indices \nto the new data. The bin edges are stored in the `bin_edges_` attribute \nand are essential for mapping input values to their respective bins during \nthe transformation process.\"\"\"\n        '\\n        Fit the model to the data and return the bin indices for the same data.\\n        \\n        Parameters\\n        ----------\\n        X : numpy.ndarray\\n            The data to fit and transform.\\n        \\n        Returns\\n        -------\\n        bin_indices : numpy.ndarray\\n            The indices of the bins each value belongs to.\\n            Values less than the smallest bin edge are assigned to the first bin,\\n            and values greater than the largest bin edge are assigned to the last bin.\\n        \\n        '\n        self.fit(X)\n        return self.transform(X)",
        "docstring": "Fit the model to the provided data and return the bin indices corresponding \nto the input values based on learned quantile-based bins.\n\nParameters\n----------\nX : numpy.ndarray\n    The data to fit the model and transform into bin indices.\n\nReturns\n-------\nbin_indices : numpy.ndarray\n    An array of indices representing the bins each value from `X` belongs to. \n    Values less than the smallest bin edge are assigned to the first bin \n    (index 0), and values greater than the largest bin edge are assigned to \n    the last bin (index n_bins - 1).\n\nThis method interacts with the `fit` method, which computes the bin edges \nbased on quantiles, and the `transform` method, which assigns bin indices \nto the new data. The bin edges are stored in the `bin_edges_` attribute \nand are essential for mapping input values to their respective bins during \nthe transformation process.",
        "signature": "def fit_transform(self, X):",
        "type": "Method",
        "class_signature": "class QuantileBinner:"
      }
    }
  },
  "dependency_dict": {
    "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:__init__": {},
    "skforecast/utils/utils.py:initialize_lags": {},
    "skforecast/utils/utils.py:initialize_window_features": {},
    "skforecast/preprocessing/preprocessing.py:QuantileBinner:__init__": {
      "skforecast/preprocessing/preprocessing.py": {
        "QuantileBinner._validate_params": {
          "code": "    def _validate_params(self, n_bins: int, method: str, subsample: int, dtype: type, random_state: int):\n        \"\"\"\n        Validate the parameters passed to the class initializer.\n        \"\"\"\n        if not isinstance(n_bins, int) or n_bins < 2:\n            raise ValueError(f'`n_bins` must be an int greater than 1. Got {n_bins}.')\n        valid_methods = ['inverse_cdf', 'averaged_inverse_cdf', 'closest_observation', 'interpolated_inverse_cdf', 'hazen', 'weibull', 'linear', 'median_unbiased', 'normal_unbiased']\n        if method not in valid_methods:\n            raise ValueError(f'`method` must be one of {valid_methods}. Got {method}.')\n        if not isinstance(subsample, int) or subsample < 1:\n            raise ValueError(f'`subsample` must be an integer greater than or equal to 1. Got {subsample}.')\n        if not isinstance(random_state, int) or random_state < 0:\n            raise ValueError(f'`random_state` must be an integer greater than or equal to 0. Got {random_state}.')\n        if not isinstance(dtype, type):\n            raise ValueError(f'`dtype` must be a valid numpy dtype. Got {dtype}.')",
          "docstring": "Validate the parameters passed to the class initializer.",
          "signature": "def _validate_params(self, n_bins: int, method: str, subsample: int, dtype: type, random_state: int):",
          "type": "Method",
          "class_signature": "class QuantileBinner:"
        }
      }
    },
    "skforecast/utils/utils.py:initialize_weights": {},
    "skforecast/utils/utils.py:check_select_fit_kwargs": {},
    "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:_binning_in_sample_residuals": {},
    "skforecast/preprocessing/preprocessing.py:QuantileBinner:fit_transform": {
      "skforecast/preprocessing/preprocessing.py": {
        "QuantileBinner.fit": {
          "code": "    def fit(self, X: np.ndarray):\n        \"\"\"\n        Learn the bin edges based on quantiles from the training data.\n        \n        Parameters\n        ----------\n        X : numpy ndarray\n            The training data used to compute the quantiles.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        if X.size == 0:\n            raise ValueError('Input data `X` cannot be empty.')\n        if len(X) > self.subsample:\n            rng = np.random.default_rng(self.random_state)\n            X = X[rng.integers(0, len(X), self.subsample)]\n        self.bin_edges_ = np.percentile(a=X, q=np.linspace(0, 100, self.n_bins + 1), method=self.method)\n        self.n_bins_ = len(self.bin_edges_) - 1\n        self.intervals_ = {float(i): (float(self.bin_edges_[i]), float(self.bin_edges_[i + 1])) for i in range(self.n_bins_)}",
          "docstring": "Learn the bin edges based on quantiles from the training data.\n\nParameters\n----------\nX : numpy ndarray\n    The training data used to compute the quantiles.\n\nReturns\n-------\nNone",
          "signature": "def fit(self, X: np.ndarray):",
          "type": "Method",
          "class_signature": "class QuantileBinner:"
        },
        "QuantileBinner.transform": {
          "code": "    def transform(self, X: np.ndarray):\n        \"\"\"\n        Assign new data to the learned bins.\n        \n        Parameters\n        ----------\n        X : numpy ndarray\n            The data to assign to the bins.\n        \n        Returns\n        -------\n        bin_indices : numpy ndarray \n            The indices of the bins each value belongs to.\n            Values less than the smallest bin edge are assigned to the first bin,\n            and values greater than the largest bin edge are assigned to the last bin.\n       \n        \"\"\"\n        if self.bin_edges_ is None:\n            raise NotFittedError(\"The model has not been fitted yet. Call 'fit' with training data first.\")\n        bin_indices = np.digitize(X, bins=self.bin_edges_, right=False)\n        bin_indices = np.clip(bin_indices, 1, self.n_bins_).astype(self.dtype) - 1\n        return bin_indices",
          "docstring": "Assign new data to the learned bins.\n\nParameters\n----------\nX : numpy ndarray\n    The data to assign to the bins.\n\nReturns\n-------\nbin_indices : numpy ndarray \n    The indices of the bins each value belongs to.\n    Values less than the smallest bin edge are assigned to the first bin,\n    and values greater than the largest bin edge are assigned to the last bin.",
          "signature": "def transform(self, X: np.ndarray):",
          "type": "Method",
          "class_signature": "class QuantileBinner:"
        }
      }
    },
    "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:fit": {},
    "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:_create_train_X_y": {
      "skforecast/recursive/_forecaster_recursive.py": {
        "ForecasterRecursive._create_lags": {
          "code": "    def _create_lags(self, y: np.ndarray, X_as_pandas: bool=False, train_index: Optional[pd.Index]=None) -> Tuple[Optional[Union[np.ndarray, pd.DataFrame]], np.ndarray]:\n        \"\"\"\n        Create the lagged values and their target variable from a time series.\n        \n        Note that the returned matrix `X_data` contains the lag 1 in the first \n        column, the lag 2 in the in the second column and so on.\n        \n        Parameters\n        ----------\n        y : numpy ndarray\n            Training time series values.\n        X_as_pandas : bool, default `False`\n            If `True`, the returned matrix `X_data` is a pandas DataFrame.\n        train_index : pandas Index, default `None`\n            Index of the training data. It is used to create the pandas DataFrame\n            `X_data` when `X_as_pandas` is `True`.\n\n        Returns\n        -------\n        X_data : numpy ndarray, pandas DataFrame, None\n            Lagged values (predictors).\n        y_data : numpy ndarray\n            Values of the time series related to each row of `X_data`.\n        \n        \"\"\"\n        X_data = None\n        if self.lags is not None:\n            n_rows = len(y) - self.window_size\n            X_data = np.full(shape=(n_rows, len(self.lags)), fill_value=np.nan, order='F', dtype=float)\n            for i, lag in enumerate(self.lags):\n                X_data[:, i] = y[self.window_size - lag:-lag]\n            if X_as_pandas:\n                X_data = pd.DataFrame(data=X_data, columns=self.lags_names, index=train_index)\n        y_data = y[self.window_size:]\n        return (X_data, y_data)",
          "docstring": "Create the lagged values and their target variable from a time series.\n\nNote that the returned matrix `X_data` contains the lag 1 in the first \ncolumn, the lag 2 in the in the second column and so on.\n\nParameters\n----------\ny : numpy ndarray\n    Training time series values.\nX_as_pandas : bool, default `False`\n    If `True`, the returned matrix `X_data` is a pandas DataFrame.\ntrain_index : pandas Index, default `None`\n    Index of the training data. It is used to create the pandas DataFrame\n    `X_data` when `X_as_pandas` is `True`.\n\nReturns\n-------\nX_data : numpy ndarray, pandas DataFrame, None\n    Lagged values (predictors).\ny_data : numpy ndarray\n    Values of the time series related to each row of `X_data`.",
          "signature": "def _create_lags(self, y: np.ndarray, X_as_pandas: bool=False, train_index: Optional[pd.Index]=None) -> Tuple[Optional[Union[np.ndarray, pd.DataFrame]], np.ndarray]:",
          "type": "Method",
          "class_signature": "class ForecasterRecursive(ForecasterBase):"
        }
      },
      "skforecast/utils/utils.py": {
        "check_y": {
          "code": "def check_y(y: Any, series_id: str='`y`') -> None:\n    \"\"\"\n    Raise Exception if `y` is not pandas Series or if it has missing values.\n    \n    Parameters\n    ----------\n    y : Any\n        Time series values.\n    series_id : str, default '`y`'\n        Identifier of the series used in the warning message.\n    \n    Returns\n    -------\n    None\n    \n    \"\"\"\n    if not isinstance(y, pd.Series):\n        raise TypeError(f'{series_id} must be a pandas Series.')\n    if y.isnull().any():\n        raise ValueError(f'{series_id} has missing values.')\n    return",
          "docstring": "Raise Exception if `y` is not pandas Series or if it has missing values.\n\nParameters\n----------\ny : Any\n    Time series values.\nseries_id : str, default '`y`'\n    Identifier of the series used in the warning message.\n\nReturns\n-------\nNone",
          "signature": "def check_y(y: Any, series_id: str='`y`') -> None:",
          "type": "Function",
          "class_signature": null
        },
        "input_to_frame": {
          "code": "def input_to_frame(data: Union[pd.Series, pd.DataFrame], input_name: str) -> pd.DataFrame:\n    \"\"\"\n    Convert data to a pandas DataFrame. If data is a pandas Series, it is \n    converted to a DataFrame with a single column. If data is a DataFrame, \n    it is returned as is.\n\n    Parameters\n    ----------\n    data : pandas Series, pandas DataFrame\n        Input data.\n    input_name : str\n        Name of the input data. Accepted values are 'y', 'last_window' and 'exog'.\n\n    Returns\n    -------\n    data : pandas DataFrame\n        Input data as a DataFrame.\n\n    \"\"\"\n    output_col_name = {'y': 'y', 'last_window': 'y', 'exog': 'exog'}\n    if isinstance(data, pd.Series):\n        data = data.to_frame(name=data.name if data.name is not None else output_col_name[input_name])\n    return data",
          "docstring": "Convert data to a pandas DataFrame. If data is a pandas Series, it is \nconverted to a DataFrame with a single column. If data is a DataFrame, \nit is returned as is.\n\nParameters\n----------\ndata : pandas Series, pandas DataFrame\n    Input data.\ninput_name : str\n    Name of the input data. Accepted values are 'y', 'last_window' and 'exog'.\n\nReturns\n-------\ndata : pandas DataFrame\n    Input data as a DataFrame.",
          "signature": "def input_to_frame(data: Union[pd.Series, pd.DataFrame], input_name: str) -> pd.DataFrame:",
          "type": "Function",
          "class_signature": null
        },
        "transform_dataframe": {
          "code": "def transform_dataframe(df: pd.DataFrame, transformer, fit: bool=False, inverse_transform: bool=False) -> pd.DataFrame:\n    \"\"\"\n    Transform raw values of pandas DataFrame with a scikit-learn alike \n    transformer, preprocessor or ColumnTransformer. The transformer used must \n    have the following methods: fit, transform, fit_transform and \n    inverse_transform. ColumnTransformers are not allowed since they do not \n    have inverse_transform method.\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        DataFrame to be transformed.\n    transformer : scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n        Scikit-learn alike transformer (preprocessor) with methods: fit, transform,\n        fit_transform and inverse_transform.\n    fit : bool, default `False`\n        Train the transformer before applying it.\n    inverse_transform : bool, default `False`\n        Transform back the data to the original representation. This is not available\n        when using transformers of class scikit-learn ColumnTransformers.\n\n    Returns\n    -------\n    df_transformed : pandas DataFrame\n        Transformed DataFrame.\n\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(f'`df` argument must be a pandas DataFrame. Got {type(df)}')\n    if transformer is None:\n        return df\n    if inverse_transform and isinstance(transformer, ColumnTransformer):\n        raise ValueError('`inverse_transform` is not available when using ColumnTransformers.')\n    if not inverse_transform:\n        if fit:\n            values_transformed = transformer.fit_transform(df)\n        else:\n            values_transformed = transformer.transform(df)\n    else:\n        values_transformed = transformer.inverse_transform(df)\n    if hasattr(values_transformed, 'toarray'):\n        values_transformed = values_transformed.toarray()\n    if hasattr(transformer, 'get_feature_names_out'):\n        feature_names_out = transformer.get_feature_names_out()\n    elif hasattr(transformer, 'categories_'):\n        feature_names_out = transformer.categories_\n    else:\n        feature_names_out = df.columns\n    df_transformed = pd.DataFrame(data=values_transformed, index=df.index, columns=feature_names_out)\n    return df_transformed",
          "docstring": "Transform raw values of pandas DataFrame with a scikit-learn alike \ntransformer, preprocessor or ColumnTransformer. The transformer used must \nhave the following methods: fit, transform, fit_transform and \ninverse_transform. ColumnTransformers are not allowed since they do not \nhave inverse_transform method.\n\nParameters\n----------\ndf : pandas DataFrame\n    DataFrame to be transformed.\ntransformer : scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n    Scikit-learn alike transformer (preprocessor) with methods: fit, transform,\n    fit_transform and inverse_transform.\nfit : bool, default `False`\n    Train the transformer before applying it.\ninverse_transform : bool, default `False`\n    Transform back the data to the original representation. This is not available\n    when using transformers of class scikit-learn ColumnTransformers.\n\nReturns\n-------\ndf_transformed : pandas DataFrame\n    Transformed DataFrame.",
          "signature": "def transform_dataframe(df: pd.DataFrame, transformer, fit: bool=False, inverse_transform: bool=False) -> pd.DataFrame:",
          "type": "Function",
          "class_signature": null
        }
      }
    },
    "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:create_sample_weights": {},
    "skforecast/utils/utils.py:preprocess_y": {}
  },
  "PRD": "# PROJECT NAME: skforecast-test_binning_in_sample_residuals\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 skforecast/\n    \u251c\u2500\u2500 preprocessing/\n    \u2502   \u2514\u2500\u2500 preprocessing.py\n    \u2502       \u251c\u2500\u2500 QuantileBinner.__init__\n    \u2502       \u2514\u2500\u2500 QuantileBinner.fit_transform\n    \u251c\u2500\u2500 recursive/\n    \u2502   \u2514\u2500\u2500 _forecaster_recursive.py\n    \u2502       \u251c\u2500\u2500 ForecasterRecursive.__init__\n    \u2502       \u251c\u2500\u2500 ForecasterRecursive._binning_in_sample_residuals\n    \u2502       \u251c\u2500\u2500 ForecasterRecursive._create_train_X_y\n    \u2502       \u251c\u2500\u2500 ForecasterRecursive.create_sample_weights\n    \u2502       \u2514\u2500\u2500 ForecasterRecursive.fit\n    \u2514\u2500\u2500 utils/\n        \u2514\u2500\u2500 utils.py\n            \u251c\u2500\u2500 check_select_fit_kwargs\n            \u251c\u2500\u2500 initialize_lags\n            \u251c\u2500\u2500 initialize_weights\n            \u251c\u2500\u2500 initialize_window_features\n            \u2514\u2500\u2500 preprocess_y\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThe module is responsible for managing and analyzing in-sample residuals generated by the `ForecasterRecursive` model to support time series forecasting tasks. It provides functionality to compute, bin, and store residuals, enabling efficient exploration of error distributions and patterns across different forecasted ranges. By dynamically segmenting residuals into user-defined bins with adjustable constraints, the module enhances the interpretability of forecasting errors and ensures memory efficiency by limiting the number of stored residuals to a predefined maximum. This facilitates robust error analysis and model refinement, solving the challenge of managing large residual datasets in iterative forecasting applications.\n\n## FILE 1: skforecast/utils/utils.py\n\n- FUNCTION NAME: initialize_lags\n  - SIGNATURE: def initialize_lags(forecaster_name: str, lags: Any) -> Union[Optional[np.ndarray], Optional[list], Optional[int]]:\n  - DOCSTRING: \n```python\n\"\"\"\nCheck and initialize lags for the forecaster by validating the input and generating corresponding numpy ndarray for the lags, their names, and the maximum lag value.\n\nParameters\n----------\nforecaster_name : str\n    Name of the forecaster for which lags are being initialized.\nlags : Any\n    The lags used as predictors, which can be an integer, list, tuple, range, or numpy array.\n\nReturns\n-------\nlags : numpy.ndarray, None\n    An array of validated lags to be used as predictors.\nlags_names : list, None\n    A list of names corresponding to the lags.\nmax_lag : int, None\n    The maximum value of the lags.\n\nRaises\n------\nValueError\n    If the lags parameter does not meet the required conditions, such as being less than 1 or not being a valid 1-dimensional structure.\nTypeError\n    If the lags parameter is of an unsupported type.\n\nNotes\n-----\nThe function ensures that the lags are integers greater than or equal to 1 and returns their names in the form 'lag_i' where i is each lag value. It is designed to be compatible with different forecaster types, checking against specified conditions particular to 'ForecasterDirectMultiVariate'. It is utilized in the context of the skforecast library to facilitate time series forecasting and ensure proper handling of past data for model training.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:__init__\n\n- FUNCTION NAME: initialize_weights\n  - SIGNATURE: def initialize_weights(forecaster_name: str, regressor: object, weight_func: Union[Callable, dict], series_weights: dict) -> Tuple[Union[Callable, dict], Union[str, dict], dict]:\n  - DOCSTRING: \n```python\n\"\"\"\nCheck and initialize the weights for forecasters within the skforecast library. This function validates the `weight_func` and `series_weights` parameters, creating a source code representation of any custom weight functions provided. If the specified regressor does not accept `sample_weight`, appropriate warnings are issued, and the corresponding arguments are set to None.\n\nParameters\n----------\nforecaster_name : str\n    The name of the forecaster, which influences the validation of the `weight_func`.\nregressor : object\n    A scikit-learn compatible regressor or pipeline used in the forecaster.\nweight_func : Callable, dict\n    A custom weighting function or a dictionary of functions used for sample weighting.\nseries_weights : dict\n    A dictionary specifying weights for each time series.\n\nReturns\n-------\nweight_func : Callable, dict\n    The validated and possibly modified `weight_func`.\nsource_code_weight_func : str, dict\n    The source code of the custom weight function(s), if applicable.\nseries_weights : dict\n    The validated `series_weights`.\n\nNotes\n-----\nThis function raises TypeErrors if the types of `weight_func` or `series_weights` are incorrect. It also issues warnings when arguments are ignored due to incompatibility with the specified regressor. The `IgnoredArgumentWarning` is used for warning messages related to ignored function parameters. The function relies on the `inspect` module to access and reproduce the source code of weight functions, emphasizing its need for callable objects.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:__init__\n\n- FUNCTION NAME: initialize_window_features\n  - SIGNATURE: def initialize_window_features(window_features: Any) -> Union[Optional[list], Optional[list], Optional[int]]:\n  - DOCSTRING: \n```python\n\"\"\"\nCheck the input for `window_features`, which are classes used to create window features for time series forecasting. It validates the presence of required attributes and methods, extracts features' names, and determines the maximum window size across all provided classes.\n\nParameters\n----------\nwindow_features : Any\n    Classes used to create window features. This can be a single class or a list of classes.\n\nReturns\n-------\nwindow_features : list, None\n    List of classes used to create window features.\nwindow_features_names : list, None\n    List of feature names extracted from the window features classes.\nmax_size_window_features : int, None\n    Maximum value of the `window_sizes` attribute among all classes.\n\nRaises \n------\nValueError\n    If any class in `window_features` does not fulfill the required attributes or methods.\nTypeError\n    If attributes have invalid types, particularly `window_sizes` or `features_names`.\n\nThis function relies on the constants `needed_atts` and `needed_methods` to specify the required attributes and methods that each window feature class must have, enhancing consistency and reducing errors during feature extraction.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:__init__\n\n- FUNCTION NAME: preprocess_y\n  - SIGNATURE: def preprocess_y(y: Union[pd.Series, pd.DataFrame], return_values: bool=True) -> Tuple[Union[None, np.ndarray], pd.Index]:\n  - DOCSTRING: \n```python\n\"\"\"\nPreprocess the input time series `y` by separating its values and index, ensuring that the index adheres to specific rules related to its type and frequency. This function modifies the index based on the following criteria:\n\n- If the index is a `DatetimeIndex` with a defined frequency, it remains unchanged.\n- If the index is a `RangeIndex`, it also remains unchanged.\n- If the index is a `DatetimeIndex` without a frequency, it is replaced with a `RangeIndex`.\n- If the index is neither, it is replaced with a `RangeIndex`.\n\nParameters\n----------\ny : pandas Series, pandas DataFrame\n    The input time series that needs to be processed.\nreturn_values : bool, default `True`\n    Determines whether to return the values of `y` as a numpy ndarray. If set to `False`, only the index is returned.\n\nReturns\n-------\ny_values : None, numpy ndarray\n    A numpy array containing the values of `y` if `return_values` is `True`. Otherwise, returns `None`.\ny_index : pandas Index\n    The modified index of `y`, adhering to the stated rules about index types and frequencies.\n\nNotes\n-----\nThis function serves as a utility within the broader context of time series forecasting, ensuring consistent data formats before model fitting or prediction. It is crucial for functions that later use the output, enforcing expected formats for time series data, which are vital in methods like initialization and input checks within forecasters defined in the skforecast library.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:fit\n\n- FUNCTION NAME: check_select_fit_kwargs\n  - SIGNATURE: def check_select_fit_kwargs(regressor: object, fit_kwargs: Optional[dict]=None) -> dict:\n  - DOCSTRING: \n```python\n\"\"\"\nCheck and filter keyword arguments to be used in the `fit` method of a regressor.\n\nThis function accepts a regressor object and a dictionary of keyword arguments (`fit_kwargs`). It validates that `fit_kwargs` is a dictionary, removes any keys that are not recognized by the regressor's `fit` method, and issues warnings for ignored keys. Specifically, if `sample_weight` is included in `fit_kwargs`, a warning is raised, and it is removed from the dictionary.\n\nParameters\n----------\nregressor : object\n    An instance of a regressor that adheres to the scikit-learn API, specifically requiring a `fit` method.\nfit_kwargs : dict, default `None`\n    A dictionary containing arguments intended for the `fit` method of the regressor.\n\nReturns\n-------\nfit_kwargs : dict\n    A filtered dictionary of arguments to be used in the `fit` method, containing only valid keys recognized by the regressor.\n\nNotes\n-----\nThe function leverages `inspect.signature()` from the `inspect` module to retrieve the parameter names of the `fit` method. It also utilizes `IgnoredArgumentWarning` from `..exceptions` to signal any ignored arguments to the user.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:__init__\n\n## FILE 2: skforecast/recursive/_forecaster_recursive.py\n\n- CLASS METHOD: ForecasterRecursive._binning_in_sample_residuals\n  - CLASS SIGNATURE: class ForecasterRecursive(ForecasterBase):\n  - SIGNATURE: def _binning_in_sample_residuals(self, y_true: np.ndarray, y_pred: np.ndarray, random_state: int=123) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nBins residuals based on the predicted values generated by the forecaster. \nThe method stores residuals associated with each bin, which is derived from \nthe predicted values. Residuals are calculated as the difference between \nthe true values (`y_true`) and the predicted values (`y_pred`). \nThe `QuantileBinner` is used to create bins for the residuals, ensuring \nthat the residuals can be sampled effectively for bootstrapping purposes \nduring prediction.\n\nParameters\n----------\ny_true : numpy ndarray\n    The true values of the time series used to compute the residuals. \ny_pred : numpy ndarray\n    The predicted values of the time series used to compute the residuals. \nrandom_state : int, default `123`\n    Controls the random number generator for reproducibility of the \n    sampled residuals.\n\nReturns\n-------\nNone\n\nSide Effects\n-------------\nResiduals are stored in the attributes `in_sample_residuals_` and \n`in_sample_residuals_by_bin_`. The attribute `binner_intervals_` \nis updated to store the interval boundaries used during binning. \nThe maximum number of residuals kept per bin is limited to `10_000 // \nself.binner.n_bins_`, and the overall count of stored residuals is \ncapped at `10,000`.\n\nDependencies\n-------------\nThis method relies on the `QuantileBinner` class from the library \n`skforecast.preprocessing`, specifically its ability to define bins \nbased on quantiles of the predicted values.\n\"\"\"\n```\n\n- CLASS METHOD: ForecasterRecursive.__init__\n  - CLASS SIGNATURE: class ForecasterRecursive(ForecasterBase):\n  - SIGNATURE: def __init__(self, regressor: object, lags: Optional[Union[int, list, np.ndarray, range]]=None, window_features: Optional[Union[object, list]]=None, transformer_y: Optional[object]=None, transformer_exog: Optional[object]=None, weight_func: Optional[Callable]=None, differentiation: Optional[int]=None, fit_kwargs: Optional[dict]=None, binner_kwargs: Optional[dict]=None, forecaster_id: Optional[Union[str, int]]=None) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nInitializes a ForecasterRecursive instance, which enables the use of any scikit-learn compatible regressor for recursive autoregressive forecasting. The constructor sets up key parameters such as lags, window features, transformers for target and exogenous variables, differentiation order, and additional arguments for fitting the regressor. It also checks that at least one predictor source (lags or window features) is provided.\n\nParameters\n----------\nregressor : object\n    An instance of a regressor or pipeline compatible with the scikit-learn API for training and predictions.\nlags : Optional[Union[int, list, np.ndarray, range]], default=None\n    Specifies the lags used as predictors. It can be an integer defining a range, a list of specific lags, or None if no lags are to be included.\nwindow_features : Optional[Union[object, list]], default=None\n    Instance or list of instances for creating additional predictors from the time series.\ntransformer_y : Optional[object], default=None\n    Preprocessor compatible with scikit-learn's API for the target variable.\ntransformer_exog : Optional[object], default=None\n    Preprocessor for the exogenous variables.\nweight_func : Optional[Callable], default=None\n    Custom function to assign weights to samples.\ndifferentiation : Optional[int], default=None\n    Specifies the order of differencing to apply to the time series; if None, no differencing occurs.\nfit_kwargs : Optional[dict], default=None\n    Additional fitting parameters for the regressor's fit method.\nbinner_kwargs : Optional[dict], default=None\n    Configuration parameters for binning residuals based on predicted values.\nforecaster_id : Optional[Union[str, int]], default=None\n    Identifier for the forecaster instance.\n\nAttributes\n----------\nself.lags, self.window_features, self.differentiation, self.binner : sets up varying configurations based on the provided parameters, ensuring that the forecaster is capable of handling the input data and specified transformations effectively.\n\nRaises\n------\nValueError\n    If both `lags` and `window_features` are None, or if `differentiation` is not a valid integer.\n\"\"\"\n```\n\n- CLASS METHOD: ForecasterRecursive._create_train_X_y\n  - CLASS SIGNATURE: class ForecasterRecursive(ForecasterBase):\n  - SIGNATURE: def _create_train_X_y(self, y: pd.Series, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, pd.Series, list, list, list, list, dict]:\n  - DOCSTRING: \n```python\n\"\"\"\nCreate training matrices from a univariate time series and optional\nexogenous variables for the ForecasterRecursive model. This method generates\nthe predictor matrix (`X_train`) and the target variable array (`y_train`)\nwhich will be used for training the underlying regression model. It also\nhandles data transformation, window features, and lag generation based on\nforecaster parameters.\n\nParameters\n----------\ny : pandas.Series\n    The training time series data with aligned index.\nexog : Optional[Union[pandas.Series, pandas.DataFrame]], default `None`\n    Optional exogenous variables included as predictors. They must have\n    the same number of observations as `y`.\n\nReturns\n-------\nTuple[pandas.DataFrame, pandas.Series, list, list, list, list, dict]\n    A tuple containing the following elements:\n        - X_train : pandas.DataFrame\n            The training values (predictors) created from lags and window features.\n        - y_train : pandas.Series\n            The values of the time series corresponding to each row of `X_train`.\n        - exog_names_in_ : list\n            The names of exogenous variables used during training.\n        - X_train_window_features_names_out_ : list\n            The names of window features included in `X_train`.\n        - X_train_exog_names_out_ : list\n            The names of exogenous variables included in `X_train`.\n        - X_train_features_names_out_ : list\n            The names of the columns in the training matrix created internally.\n        - exog_dtypes_in_ : dict\n            Data types of each exogenous variable used in training, if any.\n\nRaises\n------\nValueError\n    If the length of `y` is not greater than the maximum window size \n    required for the forecaster.\nTypeError\n    If the exogenous data format is incorrect or does not match the expected input structure.\n\nDependencies\n-------------\n- validate_y() and preprocess_y() are utility functions to validate \n  the time series and preprocess it.\n- input_to_frame() transforms the input data format into a pandas DataFrame.\n- transform_dataframe() applies transformations to `y` and `exog` based on any provided transformers.\n- _create_lags() generates the predictor variables by creating lagged versions of the input series.\n- _create_window_features() generates additional features based on window transformations if any are supplied.\n\"\"\"\n```\n\n- CLASS METHOD: ForecasterRecursive.create_sample_weights\n  - CLASS SIGNATURE: class ForecasterRecursive(ForecasterBase):\n  - SIGNATURE: def create_sample_weights(self, X_train: pd.DataFrame) -> np.ndarray:\n  - DOCSTRING: \n```python\n\"\"\"\nCreate observation weights for each instance in the training dataset based on the provided `weight_func`.\n\nParameters\n----------\nX_train : pd.DataFrame\n    Dataframe containing the training features created from the `create_train_X_y` method.\n\nReturns\n-------\nsample_weight : numpy ndarray\n    Array of weights corresponding to each observation in `X_train` to be used in the `fit` method of the regressor.\n\nRaises\n------\nValueError\n    If any of the generated weights are NaN, negative, or if the sum of weights is zero.\n\nDependencies\n------------\nThis method relies on the `weight_func` attribute of the ForecasterRecursive instance, which is set during initialization. The `weight_func` should be a callable that takes an index and returns an array-like of weights. It influences how individual observations are weighted during model fitting, thereby affecting the learning process of the underlying regressor.\n\"\"\"\n```\n\n- CLASS METHOD: ForecasterRecursive.fit\n  - CLASS SIGNATURE: class ForecasterRecursive(ForecasterBase):\n  - SIGNATURE: def fit(self, y: pd.Series, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, store_last_window: bool=True, store_in_sample_residuals: bool=True, random_state: int=123) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nFit the ForecasterRecursive model to the training time series and optional exogenous variables. This method processes the input data, trains the regressor specified during initialization, and stores important attributes related to the trained model. It allows for the use of sample weights and manages the storage of residuals and the last observed window for future predictions.\n\nParameters\n----------\ny : pandas Series\n    The training time series values used to fit the forecaster.\n\nexog : pandas Series or pandas DataFrame, optional, default `None`\n    Exogenous variables included as additional predictors. Must align with the index of `y` in terms of length and ordering.\n\nstore_last_window : bool, optional, default `True`\n    If `True`, stores the last observed data window in attribute `last_window_` for making future predictions.\n\nstore_in_sample_residuals : bool, optional, default `True`\n    If `True`, calculates and stores in-sample residuals as `in_sample_residuals_`.\n\nrandom_state : int, optional, default `123`\n    Seed for random number generation to ensure reproducibility, particularly regarding residual sampling.\n\nReturns\n-------\nNone\n\nNotes\n-----\nThe method initializes several attributes such as `last_window_`, `exog_in_`, and `training_range_`, and handles transformations of `y` and `exog` using the appropriate transformers, stored in `transformer_y` and `transformer_exog`. The method will raise errors if the input data is not of the expected alignments or types. It also prepares the sample weights if defined, and invokes the regression model's `fit` method to train the forecaster.\n\"\"\"\n```\n\n## FILE 3: skforecast/preprocessing/preprocessing.py\n\n- CLASS METHOD: QuantileBinner.__init__\n  - CLASS SIGNATURE: class QuantileBinner:\n  - SIGNATURE: def __init__(self, n_bins: int, method: Optional[str]='linear', subsample: int=200000, dtype: Optional[type]=np.float64, random_state: Optional[int]=789654):\n  - DOCSTRING: \n```python\n\"\"\"\nInitialize the QuantileBinner class for binning data into quantile-based bins.\n\nParameters\n----------\nn_bins : int\n    The number of quantile-based bins to create. Must be greater than 1.\nmethod : Optional[str], default='linear'\n    The method used to compute the quantiles, as passed to `numpy.percentile`. \n    Valid options include 'linear', 'median_unbiased', 'normal_unbiased', and others.\nsubsample : int, default=200000\n    The number of samples to use for computing quantiles. If the dataset has \n    more samples than `subsample`, a random subset will be used.\ndtype : Optional[type], default=numpy.float64\n    The data type for the bin indices.\nrandom_state : Optional[int], default=789654\n    The random seed for generating a random subset of the data.\n\nAttributes\n----------\nn_bins : int\n    The number of quantile-based bins defined by the user.\nmethod : str\n    The method selected for quantile computation.\nsubsample : int\n    The maximum number of samples to consider for fitting.\nrandom_state : int\n    The random seed used for sampling.\ndtype : type\n    The numpy data type for the bin indices.\nn_bins_ : int or None\n    The number of bins learned during fitting.\nbin_edges_ : numpy.ndarray or None\n    The edges of the bins determined from training data.\nintervals_ : dict or None\n    The intervals defining the bin edges.\n\nThis constructor calls `_validate_params` to ensure that input parameters are valid \nbefore initializing the class attributes. It interacts with the overall functionality \nof the QuantileBinner class, which allows users to fit data into quantile-based bins \nand transform new data into bin indices based on those fitted quantiles.\n\"\"\"\n```\n\n- CLASS METHOD: QuantileBinner.fit_transform\n  - CLASS SIGNATURE: class QuantileBinner:\n  - SIGNATURE: def fit_transform(self, X):\n  - DOCSTRING: \n```python\n\"\"\"\nFit the model to the provided data and return the bin indices corresponding \nto the input values based on learned quantile-based bins.\n\nParameters\n----------\nX : numpy.ndarray\n    The data to fit the model and transform into bin indices.\n\nReturns\n-------\nbin_indices : numpy.ndarray\n    An array of indices representing the bins each value from `X` belongs to. \n    Values less than the smallest bin edge are assigned to the first bin \n    (index 0), and values greater than the largest bin edge are assigned to \n    the last bin (index n_bins - 1).\n\nThis method interacts with the `fit` method, which computes the bin edges \nbased on quantiles, and the `transform` method, which assigns bin indices \nto the new data. The bin edges are stored in the `bin_edges_` attribute \nand are essential for mapping input values to their respective bins during \nthe transformation process.\n\"\"\"\n```\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "skforecast/utils/utils.py": "import importlib\nimport inspect\nimport warnings\nfrom copy import deepcopy\nfrom typing import Any, Callable, Optional, Tuple, Union\nfrom pathlib import Path\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport sklearn.linear_model\nfrom sklearn.base import clone\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.exceptions import NotFittedError\nimport skforecast\nfrom ..exceptions import warn_skforecast_categories\nfrom ..exceptions import MissingValuesWarning, MissingExogWarning, DataTypeWarning, UnknownLevelWarning, IgnoredArgumentWarning, SaveLoadSkforecastWarning, SkforecastVersionWarning\noptional_dependencies = {'sarimax': ['statsmodels>=0.12, <0.15'], 'deeplearning': ['matplotlib>=3.3, <3.10', 'keras>=2.6, <4.0'], 'plotting': ['matplotlib>=3.3, <3.10', 'seaborn>=0.11, <0.14', 'statsmodels>=0.12, <0.15']}\n\ndef initialize_transformer_series(forecaster_name: str, series_names_in_: list, encoding: Optional[str]=None, transformer_series: Optional[Union[object, dict]]=None) -> dict:\n    \"\"\"\n    Initialize `transformer_series_` attribute for the Forecasters Multiseries.\n\n    - If `transformer_series` is `None`, no transformation is applied.\n    - If `transformer_series` is a scikit-learn transformer (object), the same \n    transformer is applied to all series (`series_names_in_`).\n    - If `transformer_series` is a `dict`, a different transformer can be\n    applied to each series. The keys of the dictionary must be the same as the\n    names of the series in `series_names_in_`.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    series_names_in_ : list\n        Names of the series (levels) used during training.\n    encoding : str, default `None`\n        Encoding used to identify the different series (`ForecasterRecursiveMultiSeries`).\n    transformer_series : object, dict, default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and \n        inverse_transform. \n\n    Returns\n    -------\n    transformer_series_ : dict\n        Dictionary with the transformer for each series. It is created cloning the \n        objects in `transformer_series` and is used internally to avoid overwriting.\n    \n    \"\"\"\n    multiseries_forecasters = ['ForecasterRecursiveMultiSeries']\n    if forecaster_name in multiseries_forecasters:\n        if encoding is None:\n            series_names_in_ = ['_unknown_level']\n        else:\n            series_names_in_ = series_names_in_ + ['_unknown_level']\n    if transformer_series is None:\n        transformer_series_ = {serie: None for serie in series_names_in_}\n    elif not isinstance(transformer_series, dict):\n        transformer_series_ = {serie: clone(transformer_series) for serie in series_names_in_}\n    else:\n        transformer_series_ = {serie: None for serie in series_names_in_}\n        transformer_series_.update(((k, v) for k, v in deepcopy(transformer_series).items() if k in transformer_series_))\n        series_not_in_transformer_series = set(series_names_in_) - set(transformer_series.keys()) - {'_unknown_level'}\n        if series_not_in_transformer_series:\n            warnings.warn(f'{series_not_in_transformer_series} not present in `transformer_series`. No transformation is applied to these series.', IgnoredArgumentWarning)\n    return transformer_series_\n\ndef check_y(y: Any, series_id: str='`y`') -> None:\n    \"\"\"\n    Raise Exception if `y` is not pandas Series or if it has missing values.\n    \n    Parameters\n    ----------\n    y : Any\n        Time series values.\n    series_id : str, default '`y`'\n        Identifier of the series used in the warning message.\n    \n    Returns\n    -------\n    None\n    \n    \"\"\"\n    if not isinstance(y, pd.Series):\n        raise TypeError(f'{series_id} must be a pandas Series.')\n    if y.isnull().any():\n        raise ValueError(f'{series_id} has missing values.')\n    return\n\ndef check_exog(exog: Union[pd.Series, pd.DataFrame], allow_nan: bool=True, series_id: str='`exog`') -> None:\n    \"\"\"\n    Raise Exception if `exog` is not pandas Series or pandas DataFrame.\n    If `allow_nan = True`, issue a warning if `exog` contains NaN values.\n    \n    Parameters\n    ----------\n    exog : pandas DataFrame, pandas Series\n        Exogenous variable/s included as predictor/s.\n    allow_nan : bool, default `True`\n        If True, allows the presence of NaN values in `exog`. If False (default),\n        issue a warning if `exog` contains NaN values.\n    series_id : str, default '`exog`'\n        Identifier of the series for which the exogenous variable/s are used\n        in the warning message.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(f'{series_id} must be a pandas Series or DataFrame. Got {type(exog)}.')\n    if isinstance(exog, pd.Series) and exog.name is None:\n        raise ValueError(f'When {series_id} is a pandas Series, it must have a name.')\n    if not allow_nan:\n        if exog.isnull().any().any():\n            warnings.warn(f'{series_id} has missing values. Most machine learning models do not allow missing values. Fitting the forecaster may fail.', MissingValuesWarning)\n    return\n\ndef get_exog_dtypes(exog: Union[pd.DataFrame, pd.Series]) -> dict:\n    \"\"\"\n    Store dtypes of `exog`.\n\n    Parameters\n    ----------\n    exog : pandas DataFrame, pandas Series\n        Exogenous variable/s included as predictor/s.\n\n    Returns\n    -------\n    exog_dtypes : dict\n        Dictionary with the dtypes in `exog`.\n    \n    \"\"\"\n    if isinstance(exog, pd.Series):\n        exog_dtypes = {exog.name: exog.dtypes}\n    else:\n        exog_dtypes = exog.dtypes.to_dict()\n    return exog_dtypes\n\ndef check_exog_dtypes(exog: Union[pd.DataFrame, pd.Series], call_check_exog: bool=True, series_id: str='`exog`') -> None:\n    \"\"\"\n    Raise Exception if `exog` has categorical columns with non integer values.\n    This is needed when using machine learning regressors that allow categorical\n    features.\n    Issue a Warning if `exog` has columns that are not `init`, `float`, or `category`.\n    \n    Parameters\n    ----------\n    exog : pandas DataFrame, pandas Series\n        Exogenous variable/s included as predictor/s.\n    call_check_exog : bool, default `True`\n        If `True`, call `check_exog` function.\n    series_id : str, default '`exog`'\n        Identifier of the series for which the exogenous variable/s are used\n        in the warning message.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    if call_check_exog:\n        check_exog(exog=exog, allow_nan=False, series_id=series_id)\n    if isinstance(exog, pd.DataFrame):\n        if not exog.select_dtypes(exclude=[np.number, 'category']).columns.empty:\n            warnings.warn(f'{series_id} may contain only `int`, `float` or `category` dtypes. Most machine learning models do not allow other types of values. Fitting the forecaster may fail.', DataTypeWarning)\n        for col in exog.select_dtypes(include='category'):\n            if exog[col].cat.categories.dtype not in [int, np.int32, np.int64]:\n                raise TypeError('Categorical dtypes in exog must contain only integer values. See skforecast docs for more info about how to include categorical features https://skforecast.org/latest/user_guides/categorical-features.html')\n    else:\n        if exog.dtype.name not in ['int', 'int8', 'int16', 'int32', 'int64', 'float', 'float16', 'float32', 'float64', 'uint8', 'uint16', 'uint32', 'uint64', 'category']:\n            warnings.warn(f'{series_id} may contain only `int`, `float` or `category` dtypes. Most machine learning models do not allow other types of values. Fitting the forecaster may fail.', DataTypeWarning)\n        if exog.dtype.name == 'category' and exog.cat.categories.dtype not in [int, np.int32, np.int64]:\n            raise TypeError('Categorical dtypes in exog must contain only integer values. See skforecast docs for more info about how to include categorical features https://skforecast.org/latest/user_guides/categorical-features.html')\n    return\n\ndef check_interval(interval: list=None, quantiles: float=None, alpha: float=None) -> None:\n    \"\"\"\n    Check provided confidence interval sequence is valid.\n\n    Parameters\n    ----------\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. For example, \n        interval of 95% should be as `interval = [2.5, 97.5]`.\n    quantiles : list, default `None`\n        Sequence of quantiles to compute, which must be between 0 and 1 \n        inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n        `quantiles = [0.05, 0.5, 0.95]`.\n    alpha : float, default `None`\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %.\n\n    Returns\n    -------\n    None\n    \n    \"\"\"\n    if interval is not None:\n        if not isinstance(interval, list):\n            raise TypeError('`interval` must be a `list`. For example, interval of 95% should be as `interval = [2.5, 97.5]`.')\n        if len(interval) != 2:\n            raise ValueError('`interval` must contain exactly 2 values, respectively the lower and upper interval bounds. For example, interval of 95% should be as `interval = [2.5, 97.5]`.')\n        if interval[0] < 0.0 or interval[0] >= 100.0:\n            raise ValueError(f'Lower interval bound ({interval[0]}) must be >= 0 and < 100.')\n        if interval[1] <= 0.0 or interval[1] > 100.0:\n            raise ValueError(f'Upper interval bound ({interval[1]}) must be > 0 and <= 100.')\n        if interval[0] >= interval[1]:\n            raise ValueError(f'Lower interval bound ({interval[0]}) must be less than the upper interval bound ({interval[1]}).')\n    if quantiles is not None:\n        if not isinstance(quantiles, list):\n            raise TypeError('`quantiles` must be a `list`. For example, quantiles 0.05, 0.5, and 0.95 should be as `quantiles = [0.05, 0.5, 0.95]`.')\n        for q in quantiles:\n            if q < 0.0 or q > 1.0:\n                raise ValueError('All elements in `quantiles` must be >= 0 and <= 1.')\n    if alpha is not None:\n        if not isinstance(alpha, float):\n            raise TypeError('`alpha` must be a `float`. For example, interval of 95% should be as `alpha = 0.05`.')\n        if alpha <= 0.0 or alpha >= 1:\n            raise ValueError(f'`alpha` must have a value between 0 and 1. Got {alpha}.')\n    return\n\ndef check_predict_input(forecaster_name: str, steps: Union[int, list], is_fitted: bool, exog_in_: bool, index_type_: type, index_freq_: str, window_size: int, last_window: Union[pd.Series, pd.DataFrame, None], last_window_exog: Optional[Union[pd.Series, pd.DataFrame]]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, exog_type_in_: Optional[type]=None, exog_names_in_: Optional[list]=None, interval: Optional[list]=None, alpha: Optional[float]=None, max_steps: Optional[int]=None, levels: Optional[Union[str, list]]=None, levels_forecaster: Optional[Union[str, list]]=None, series_names_in_: Optional[list]=None, encoding: Optional[str]=None) -> None:\n    \"\"\"\n    Check all inputs of predict method. This is a helper function to validate\n    that inputs used in predict method match attributes of a forecaster already\n    trained.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    steps : int, list\n        Number of future steps predicted.\n    is_fitted: bool\n        Tag to identify if the regressor has been fitted (trained).\n    exog_in_ : bool\n        If the forecaster has been trained using exogenous variable/s.\n    index_type_ : type\n        Type of index of the input used in training.\n    index_freq_ : str\n        Frequency of Index of the input used in training.\n    window_size: int\n        Size of the window needed to create the predictors. It is equal to \n        `max_lag`.\n    last_window : pandas Series, pandas DataFrame, None\n        Values of the series used to create the predictors (lags) need in the \n        first iteration of prediction (t + 1).\n    last_window_exog : pandas Series, pandas DataFrame, default `None`\n        Values of the exogenous variables aligned with `last_window` in \n        ForecasterSarimax predictions.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s.\n    exog_type_in_ : type, default `None`\n        Type of exogenous variable/s used in training.\n    exog_names_in_ : list, default `None`\n        Names of the exogenous variables used during training.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive. For example, \n        interval of 95% should be as `interval = [2.5, 97.5]`.\n    alpha : float, default `None`\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %.\n    max_steps: int, default `None`\n        Maximum number of steps allowed (`ForecasterDirect` and \n        `ForecasterDirectMultiVariate`).\n    levels : str, list, default `None`\n        Time series to be predicted (`ForecasterRecursiveMultiSeries`\n        and `ForecasterRnn).\n    levels_forecaster : str, list, default `None`\n        Time series used as output data of a multiseries problem in a RNN problem\n        (`ForecasterRnn`).\n    series_names_in_ : list, default `None`\n        Names of the columns used during fit (`ForecasterRecursiveMultiSeries`, \n        `ForecasterDirectMultiVariate` and `ForecasterRnn`).\n    encoding : str, default `None`\n        Encoding used to identify the different series (`ForecasterRecursiveMultiSeries`).\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    if not is_fitted:\n        raise NotFittedError('This Forecaster instance is not fitted yet. Call `fit` with appropriate arguments before using predict.')\n    if isinstance(steps, (int, np.integer)) and steps < 1:\n        raise ValueError(f'`steps` must be an integer greater than or equal to 1. Got {steps}.')\n    if isinstance(steps, list) and min(steps) < 1:\n        raise ValueError(f'The minimum value of `steps` must be equal to or greater than 1. Got {min(steps)}.')\n    if max_steps is not None:\n        if max(steps) > max_steps:\n            raise ValueError(f'The maximum value of `steps` must be less than or equal to the value of steps defined when initializing the forecaster. Got {max(steps)}, but the maximum is {max_steps}.')\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n    if forecaster_name in ['ForecasterRecursiveMultiSeries', 'ForecasterRnn']:\n        if not isinstance(levels, (type(None), str, list)):\n            raise TypeError('`levels` must be a `list` of column names, a `str` of a column name or `None`.')\n        levels_to_check = levels_forecaster if forecaster_name == 'ForecasterRnn' else series_names_in_\n        unknown_levels = set(levels) - set(levels_to_check)\n        if forecaster_name == 'ForecasterRnn':\n            if len(unknown_levels) != 0:\n                raise ValueError(f'`levels` names must be included in the series used during fit ({levels_to_check}). Got {levels}.')\n        elif len(unknown_levels) != 0 and last_window is not None and (encoding is not None):\n            if encoding == 'onehot':\n                warnings.warn(f'`levels` {unknown_levels} were not included in training. The resulting one-hot encoded columns for this feature will be all zeros.', UnknownLevelWarning)\n            else:\n                warnings.warn(f'`levels` {unknown_levels} were not included in training. Unknown levels are encoded as NaN, which may cause the prediction to fail if the regressor does not accept NaN values.', UnknownLevelWarning)\n    if exog is None and exog_in_:\n        raise ValueError('Forecaster trained with exogenous variable/s. Same variable/s must be provided when predicting.')\n    if exog is not None and (not exog_in_):\n        raise ValueError('Forecaster trained without exogenous variable/s. `exog` must be `None` when predicting.')\n    if isinstance(last_window, type(None)) and forecaster_name not in ['ForecasterRecursiveMultiSeries', 'ForecasterRnn']:\n        raise ValueError(\"`last_window` was not stored during training. If you don't want to retrain the Forecaster, provide `last_window` as argument.\")\n    if forecaster_name in ['ForecasterRecursiveMultiSeries', 'ForecasterDirectMultiVariate', 'ForecasterRnn']:\n        if not isinstance(last_window, pd.DataFrame):\n            raise TypeError(f'`last_window` must be a pandas DataFrame. Got {type(last_window)}.')\n        last_window_cols = last_window.columns.to_list()\n        if forecaster_name in ['ForecasterRecursiveMultiSeries', 'ForecasterRnn'] and len(set(levels) - set(last_window_cols)) != 0:\n            raise ValueError(f'`last_window` must contain a column(s) named as the level(s) to be predicted.\\n    `levels` : {levels}\\n    `last_window` columns : {last_window_cols}')\n        if forecaster_name == 'ForecasterDirectMultiVariate':\n            if len(set(series_names_in_) - set(last_window_cols)) > 0:\n                raise ValueError(f'`last_window` columns must be the same as the `series` column names used to create the X_train matrix.\\n    `last_window` columns    : {last_window_cols}\\n    `series` columns X train : {series_names_in_}')\n    elif not isinstance(last_window, (pd.Series, pd.DataFrame)):\n        raise TypeError(f'`last_window` must be a pandas Series or DataFrame. Got {type(last_window)}.')\n    if len(last_window) < window_size:\n        raise ValueError(f'`last_window` must have as many values as needed to generate the predictors. For this forecaster it is {window_size}.')\n    if last_window.isnull().any().all():\n        warnings.warn('`last_window` has missing values. Most of machine learning models do not allow missing values. Prediction method may fail.', MissingValuesWarning)\n    _, last_window_index = preprocess_last_window(last_window=last_window.iloc[:0], return_values=False)\n    if not isinstance(last_window_index, index_type_):\n        raise TypeError(f'Expected index of type {index_type_} for `last_window`. Got {type(last_window_index)}.')\n    if isinstance(last_window_index, pd.DatetimeIndex):\n        if not last_window_index.freqstr == index_freq_:\n            raise TypeError(f'Expected frequency of type {index_freq_} for `last_window`. Got {last_window_index.freqstr}.')\n    if exog is not None:\n        if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n            if not isinstance(exog, (pd.Series, pd.DataFrame, dict)):\n                raise TypeError(f'`exog` must be a pandas Series, DataFrame or dict. Got {type(exog)}.')\n            if exog_type_in_ == dict and (not isinstance(exog, dict)):\n                raise TypeError(f'Expected type for `exog`: {exog_type_in_}. Got {type(exog)}.')\n        elif not isinstance(exog, (pd.Series, pd.DataFrame)):\n            raise TypeError(f'`exog` must be a pandas Series or DataFrame. Got {type(exog)}.')\n        if isinstance(exog, dict):\n            no_exog_levels = set(levels) - set(exog.keys())\n            if no_exog_levels:\n                warnings.warn(f'`exog` does not contain keys for levels {no_exog_levels}. Missing levels are filled with NaN. Most of machine learning models do not allow missing values. Prediction method may fail.', MissingExogWarning)\n            exogs_to_check = [(f\"`exog` for series '{k}'\", v) for k, v in exog.items() if v is not None and k in levels]\n        else:\n            exogs_to_check = [('`exog`', exog)]\n        for exog_name, exog_to_check in exogs_to_check:\n            if not isinstance(exog_to_check, (pd.Series, pd.DataFrame)):\n                raise TypeError(f'{exog_name} must be a pandas Series or DataFrame. Got {type(exog_to_check)}')\n            if exog_to_check.isnull().any().any():\n                warnings.warn(f'{exog_name} has missing values. Most of machine learning models do not allow missing values. Prediction method may fail.', MissingValuesWarning)\n            last_step = max(steps) if isinstance(steps, list) else steps\n            if len(exog_to_check) < last_step:\n                if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n                    warnings.warn(f\"{exog_name} doesn't have as many values as steps predicted, {last_step}. Missing values are filled with NaN. Most of machine learning models do not allow missing values. Prediction method may fail.\", MissingValuesWarning)\n                else:\n                    raise ValueError(f'{exog_name} must have at least as many values as steps predicted, {last_step}.')\n            if isinstance(exog_to_check, pd.DataFrame):\n                col_missing = set(exog_names_in_).difference(set(exog_to_check.columns))\n                if col_missing:\n                    if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n                        warnings.warn(f'{col_missing} not present in {exog_name}. All values will be NaN.', MissingExogWarning)\n                    else:\n                        raise ValueError(f'Missing columns in {exog_name}. Expected {exog_names_in_}. Got {exog_to_check.columns.to_list()}.')\n            else:\n                if exog_to_check.name is None:\n                    raise ValueError(f'When {exog_name} is a pandas Series, it must have a name. Got None.')\n                if exog_to_check.name not in exog_names_in_:\n                    if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n                        warnings.warn(f\"'{exog_to_check.name}' was not observed during training. {exog_name} is ignored. Exogenous variables must be one of: {exog_names_in_}.\", IgnoredArgumentWarning)\n                    else:\n                        raise ValueError(f\"'{exog_to_check.name}' was not observed during training. Exogenous variables must be: {exog_names_in_}.\")\n            _, exog_index = preprocess_exog(exog=exog_to_check.iloc[:0,], return_values=False)\n            if not isinstance(exog_index, index_type_):\n                raise TypeError(f'Expected index of type {index_type_} for {exog_name}. Got {type(exog_index)}.')\n            if forecaster_name not in ['ForecasterRecursiveMultiSeries']:\n                if isinstance(exog_index, pd.DatetimeIndex):\n                    if not exog_index.freqstr == index_freq_:\n                        raise TypeError(f'Expected frequency of type {index_freq_} for {exog_name}. Got {exog_index.freqstr}.')\n            expected_index = expand_index(last_window.index, 1)[0]\n            if expected_index != exog_to_check.index[0]:\n                if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n                    warnings.warn(f'To make predictions {exog_name} must start one step ahead of `last_window`. Missing values are filled with NaN.\\n    `last_window` ends at : {last_window.index[-1]}.\\n    {exog_name} starts at : {exog_to_check.index[0]}.\\n     Expected index       : {expected_index}.', MissingValuesWarning)\n                else:\n                    raise ValueError(f'To make predictions {exog_name} must start one step ahead of `last_window`.\\n    `last_window` ends at : {last_window.index[-1]}.\\n    {exog_name} starts at : {exog_to_check.index[0]}.\\n     Expected index : {expected_index}.')\n    if forecaster_name == 'ForecasterSarimax':\n        if last_window_exog is not None:\n            if not exog_in_:\n                raise ValueError('Forecaster trained without exogenous variable/s. `last_window_exog` must be `None` when predicting.')\n            if not isinstance(last_window_exog, (pd.Series, pd.DataFrame)):\n                raise TypeError(f'`last_window_exog` must be a pandas Series or a pandas DataFrame. Got {type(last_window_exog)}.')\n            if len(last_window_exog) < window_size:\n                raise ValueError(f'`last_window_exog` must have as many values as needed to generate the predictors. For this forecaster it is {window_size}.')\n            if last_window_exog.isnull().any().all():\n                warnings.warn('`last_window_exog` has missing values. Most of machine learning models do not allow missing values. Prediction method may fail.', MissingValuesWarning)\n            _, last_window_exog_index = preprocess_last_window(last_window=last_window_exog.iloc[:0], return_values=False)\n            if not isinstance(last_window_exog_index, index_type_):\n                raise TypeError(f'Expected index of type {index_type_} for `last_window_exog`. Got {type(last_window_exog_index)}.')\n            if isinstance(last_window_exog_index, pd.DatetimeIndex):\n                if not last_window_exog_index.freqstr == index_freq_:\n                    raise TypeError(f'Expected frequency of type {index_freq_} for `last_window_exog`. Got {last_window_exog_index.freqstr}.')\n            if isinstance(last_window_exog, pd.DataFrame):\n                col_missing = set(exog_names_in_).difference(set(last_window_exog.columns))\n                if col_missing:\n                    raise ValueError(f'Missing columns in `last_window_exog`. Expected {exog_names_in_}. Got {last_window_exog.columns.to_list()}.')\n            else:\n                if last_window_exog.name is None:\n                    raise ValueError('When `last_window_exog` is a pandas Series, it must have a name. Got None.')\n                if last_window_exog.name not in exog_names_in_:\n                    raise ValueError(f\"'{last_window_exog.name}' was not observed during training. Exogenous variables must be: {exog_names_in_}.\")\n    return\n\ndef preprocess_last_window(last_window: Union[pd.Series, pd.DataFrame], return_values: bool=True) -> Tuple[np.ndarray, pd.Index]:\n    \"\"\"\n    Return values and index of series separately. Index is overwritten \n    according to the next rules:\n    \n    - If index is of type `DatetimeIndex` and has frequency, nothing is \n    changed.\n    - If index is of type `RangeIndex`, nothing is changed.\n    - If index is of type `DatetimeIndex` but has no frequency, a \n    `RangeIndex` is created.\n    - If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\n    \n    Parameters\n    ----------\n    last_window : pandas Series, pandas DataFrame\n        Time series values.\n    return_values : bool, default `True`\n        If `True` return the values of `last_window` as numpy ndarray. This option \n        is intended to avoid copying data when it is not necessary.\n\n    Returns\n    -------\n    last_window_values : numpy ndarray\n        Numpy array with values of `last_window`.\n    last_window_index : pandas Index\n        Index of `last_window` modified according to the rules.\n    \n    \"\"\"\n    if isinstance(last_window.index, pd.DatetimeIndex) and last_window.index.freq is not None:\n        last_window_index = last_window.index\n    elif isinstance(last_window.index, pd.RangeIndex):\n        last_window_index = last_window.index\n    elif isinstance(last_window.index, pd.DatetimeIndex) and last_window.index.freq is None:\n        warnings.warn('`last_window` has DatetimeIndex index but no frequency. Index is overwritten with a RangeIndex of step 1.')\n        last_window_index = pd.RangeIndex(start=0, stop=len(last_window), step=1)\n    else:\n        warnings.warn('`last_window` has no DatetimeIndex nor RangeIndex index. Index is overwritten with a RangeIndex.')\n        last_window_index = pd.RangeIndex(start=0, stop=len(last_window), step=1)\n    last_window_values = last_window.to_numpy(copy=True).ravel() if return_values else None\n    return (last_window_values, last_window_index)\n\ndef preprocess_exog(exog: Union[pd.Series, pd.DataFrame], return_values: bool=True) -> Tuple[Union[None, np.ndarray], pd.Index]:\n    \"\"\"\n    Return values and index of series or data frame separately. Index is\n    overwritten  according to the next rules:\n    \n    - If index is of type `DatetimeIndex` and has frequency, nothing is \n    changed.\n    - If index is of type `RangeIndex`, nothing is changed.\n    - If index is of type `DatetimeIndex` but has no frequency, a \n    `RangeIndex` is created.\n    - If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\n\n    Parameters\n    ----------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables.\n    return_values : bool, default `True`\n        If `True` return the values of `exog` as numpy ndarray. This option is \n        intended to avoid copying data when it is not necessary.\n\n    Returns\n    -------\n    exog_values : None, numpy ndarray\n        Numpy array with values of `exog`.\n    exog_index : pandas Index\n        Index of `exog` modified according to the rules.\n    \n    \"\"\"\n    if isinstance(exog.index, pd.DatetimeIndex) and exog.index.freq is not None:\n        exog_index = exog.index\n    elif isinstance(exog.index, pd.RangeIndex):\n        exog_index = exog.index\n    elif isinstance(exog.index, pd.DatetimeIndex) and exog.index.freq is None:\n        warnings.warn('`exog` has DatetimeIndex index but no frequency. Index is overwritten with a RangeIndex of step 1.')\n        exog_index = pd.RangeIndex(start=0, stop=len(exog), step=1)\n    else:\n        warnings.warn('`exog` has no DatetimeIndex nor RangeIndex index. Index is overwritten with a RangeIndex.')\n        exog_index = pd.RangeIndex(start=0, stop=len(exog), step=1)\n    exog_values = exog.to_numpy(copy=True) if return_values else None\n    return (exog_values, exog_index)\n\ndef input_to_frame(data: Union[pd.Series, pd.DataFrame], input_name: str) -> pd.DataFrame:\n    \"\"\"\n    Convert data to a pandas DataFrame. If data is a pandas Series, it is \n    converted to a DataFrame with a single column. If data is a DataFrame, \n    it is returned as is.\n\n    Parameters\n    ----------\n    data : pandas Series, pandas DataFrame\n        Input data.\n    input_name : str\n        Name of the input data. Accepted values are 'y', 'last_window' and 'exog'.\n\n    Returns\n    -------\n    data : pandas DataFrame\n        Input data as a DataFrame.\n\n    \"\"\"\n    output_col_name = {'y': 'y', 'last_window': 'y', 'exog': 'exog'}\n    if isinstance(data, pd.Series):\n        data = data.to_frame(name=data.name if data.name is not None else output_col_name[input_name])\n    return data\n\ndef cast_exog_dtypes(exog: Union[pd.Series, pd.DataFrame], exog_dtypes: dict) -> Union[pd.Series, pd.DataFrame]:\n    \"\"\"\n    Cast `exog` to a specified types. This is done because, for a forecaster to \n    accept a categorical exog, it must contain only integer values. Due to the \n    internal modifications of numpy, the values may be casted to `float`, so \n    they have to be re-converted to `int`.\n\n    - If `exog` is a pandas Series, `exog_dtypes` must be a dict with a \n    single value.\n    - If `exog_dtypes` is `category` but the current type of `exog` is `float`, \n    then the type is cast to `int` and then to `category`. \n\n    Parameters\n    ----------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables.\n    exog_dtypes: dict\n        Dictionary with name and type of the series or data frame columns.\n\n    Returns\n    -------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables casted to the indicated dtypes.\n\n    \"\"\"\n    exog_dtypes = {k: v for k, v in exog_dtypes.items() if k in exog.columns}\n    if isinstance(exog, pd.Series) and exog.dtypes != list(exog_dtypes.values())[0]:\n        exog = exog.astype(list(exog_dtypes.values())[0])\n    elif isinstance(exog, pd.DataFrame):\n        for col, initial_dtype in exog_dtypes.items():\n            if exog[col].dtypes != initial_dtype:\n                if initial_dtype == 'category' and exog[col].dtypes == float:\n                    exog[col] = exog[col].astype(int).astype('category')\n                else:\n                    exog[col] = exog[col].astype(initial_dtype)\n    return exog\n\ndef exog_to_direct(exog: Union[pd.Series, pd.DataFrame], steps: int) -> Union[pd.DataFrame, list]:\n    \"\"\"\n    Transforms `exog` to a pandas DataFrame with the shape needed for Direct\n    forecasting.\n    \n    Parameters\n    ----------\n    exog : pandas Series, pandas DataFrame\n        Exogenous variables.\n    steps : int\n        Number of steps that will be predicted using exog.\n\n    Returns\n    -------\n    exog_direct : pandas DataFrame\n        Exogenous variables transformed.\n    exog_direct_names : list\n        Names of the columns of the exogenous variables transformed. Only \n        created if `exog` is a pandas Series or DataFrame.\n    \n    \"\"\"\n    if not isinstance(exog, (pd.Series, pd.DataFrame)):\n        raise TypeError(f'`exog` must be a pandas Series or DataFrame. Got {type(exog)}.')\n    if isinstance(exog, pd.Series):\n        exog = exog.to_frame()\n    n_rows = len(exog)\n    exog_idx = exog.index\n    exog_cols = exog.columns\n    exog_direct = []\n    for i in range(steps):\n        exog_step = exog.iloc[i:n_rows - (steps - 1 - i),]\n        exog_step.index = pd.RangeIndex(len(exog_step))\n        exog_step.columns = [f'{col}_step_{i + 1}' for col in exog_cols]\n        exog_direct.append(exog_step)\n    if len(exog_direct) > 1:\n        exog_direct = pd.concat(exog_direct, axis=1, copy=False)\n    else:\n        exog_direct = exog_direct[0]\n    exog_direct_names = exog_direct.columns.to_list()\n    exog_direct.index = exog_idx[-len(exog_direct):]\n    return (exog_direct, exog_direct_names)\n\ndef exog_to_direct_numpy(exog: Union[np.ndarray, pd.Series, pd.DataFrame], steps: int) -> Tuple[np.ndarray, Optional[list]]:\n    \"\"\"\n    Transforms `exog` to numpy ndarray with the shape needed for Direct\n    forecasting.\n    \n    Parameters\n    ----------\n    exog : numpy ndarray, pandas Series, pandas DataFrame\n        Exogenous variables, shape(samples,). If exog is a pandas format, the \n        direct exog names are created.\n    steps : int\n        Number of steps that will be predicted using exog.\n\n    Returns\n    -------\n    exog_direct : numpy ndarray\n        Exogenous variables transformed.\n    exog_direct_names : list, None\n        Names of the columns of the exogenous variables transformed. Only \n        created if `exog` is a pandas Series or DataFrame.\n\n    \"\"\"\n    if isinstance(exog, (pd.Series, pd.DataFrame)):\n        exog_cols = exog.columns if isinstance(exog, pd.DataFrame) else [exog.name]\n        exog_direct_names = [f'{col}_step_{i + 1}' for i in range(steps) for col in exog_cols]\n        exog = exog.to_numpy()\n    else:\n        exog_direct_names = None\n        if not isinstance(exog, np.ndarray):\n            raise TypeError(f'`exog` must be a numpy ndarray, pandas Series or DataFrame. Got {type(exog)}.')\n    if exog.ndim == 1:\n        exog = np.expand_dims(exog, axis=1)\n    n_rows = len(exog)\n    exog_direct = []\n    for i in range(steps):\n        exog_step = exog[i:n_rows - (steps - 1 - i)]\n        exog_direct.append(exog_step)\n    if len(exog_direct) > 1:\n        exog_direct = np.concatenate(exog_direct, axis=1)\n    else:\n        exog_direct = exog_direct[0]\n    return (exog_direct, exog_direct_names)\n\ndef date_to_index_position(index: pd.Index, date_input: Union[int, str, pd.Timestamp], date_literal: str='steps', kwargs_pd_to_datetime: dict={}) -> int:\n    \"\"\"\n    Transform a datetime string or pandas Timestamp to an integer. The integer\n    represents the position of the datetime in the index.\n    \n    Parameters\n    ----------\n    index : pandas Index\n        Original datetime index (must be a pandas DatetimeIndex if `date_input` \n        is not an int).\n    date_input : int, str, pandas Timestamp\n        Datetime to transform to integer.\n        \n        + If int, returns the same integer.\n        + If str or pandas Timestamp, it is converted and expanded into the index.\n    date_literal : str, default 'steps'\n        Variable name used in error messages.\n    kwargs_pd_to_datetime : dict, default {}\n        Additional keyword arguments to pass to `pd.to_datetime()`.\n    \n    Returns\n    -------\n    date_position : int\n        Integer representing the position of the datetime in the index.\n    \n    \"\"\"\n    if isinstance(date_input, (str, pd.Timestamp)):\n        if not isinstance(index, pd.DatetimeIndex):\n            raise TypeError(f'Index must be a pandas DatetimeIndex when `{date_literal}` is not an integer. Check input series or last window.')\n        target_date = pd.to_datetime(date_input, **kwargs_pd_to_datetime)\n        last_date = pd.to_datetime(index[-1])\n        if target_date <= last_date:\n            raise ValueError('The provided date must be later than the last date in the index.')\n        steps_diff = pd.date_range(start=last_date, end=target_date, freq=index.freq)\n        date_position = len(steps_diff) - 1\n    elif isinstance(date_input, (int, np.integer)):\n        date_position = date_input\n    else:\n        raise TypeError(f'`{date_literal}` must be an integer, string, or pandas Timestamp.')\n    return date_position\n\ndef expand_index(index: Union[pd.Index, None], steps: int) -> pd.Index:\n    \"\"\"\n    Create a new index of length `steps` starting at the end of the index.\n    \n    Parameters\n    ----------\n    index : pandas Index, None\n        Original index.\n    steps : int\n        Number of steps to expand.\n\n    Returns\n    -------\n    new_index : pandas Index\n        New index.\n\n    \"\"\"\n    if not isinstance(steps, (int, np.integer)):\n        raise TypeError(f'`steps` must be an integer. Got {type(steps)}.')\n    if isinstance(index, pd.Index):\n        if isinstance(index, pd.DatetimeIndex):\n            new_index = pd.date_range(start=index[-1] + index.freq, periods=steps, freq=index.freq)\n        elif isinstance(index, pd.RangeIndex):\n            new_index = pd.RangeIndex(start=index[-1] + 1, stop=index[-1] + 1 + steps)\n        else:\n            raise TypeError('Argument `index` must be a pandas DatetimeIndex or RangeIndex.')\n    else:\n        new_index = pd.RangeIndex(start=0, stop=steps)\n    return new_index\n\ndef transform_numpy(array: np.ndarray, transformer, fit: bool=False, inverse_transform: bool=False) -> np.ndarray:\n    \"\"\"\n    Transform raw values of a numpy ndarray with a scikit-learn alike \n    transformer, preprocessor or ColumnTransformer. The transformer used must \n    have the following methods: fit, transform, fit_transform and \n    inverse_transform. ColumnTransformers are not allowed since they do not \n    have inverse_transform method.\n\n    Parameters\n    ----------\n    array : numpy ndarray\n        Array to be transformed.\n    transformer : scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n        Scikit-learn alike transformer (preprocessor) with methods: fit, transform,\n        fit_transform and inverse_transform.\n    fit : bool, default `False`\n        Train the transformer before applying it.\n    inverse_transform : bool, default `False`\n        Transform back the data to the original representation. This is not available\n        when using transformers of class scikit-learn ColumnTransformers.\n\n    Returns\n    -------\n    array_transformed : numpy ndarray\n        Transformed array.\n\n    \"\"\"\n    if not isinstance(array, np.ndarray):\n        raise TypeError(f'`array` argument must be a numpy ndarray. Got {type(array)}')\n    if transformer is None:\n        return array\n    array_ndim = array.ndim\n    if array_ndim == 1:\n        array = array.reshape(-1, 1)\n    if inverse_transform and isinstance(transformer, ColumnTransformer):\n        raise ValueError('`inverse_transform` is not available when using ColumnTransformers.')\n    if not inverse_transform:\n        if fit:\n            array_transformed = transformer.fit_transform(array)\n        else:\n            with warnings.catch_warnings():\n                warnings.filterwarnings('ignore', message='X does not have valid feature names', category=UserWarning)\n                array_transformed = transformer.transform(array)\n    else:\n        array_transformed = transformer.inverse_transform(array)\n    if hasattr(array_transformed, 'toarray'):\n        array_transformed = array_transformed.toarray()\n    if array_ndim == 1:\n        array_transformed = array_transformed.ravel()\n    return array_transformed\n\ndef transform_series(series: pd.Series, transformer, fit: bool=False, inverse_transform: bool=False) -> Union[pd.Series, pd.DataFrame]:\n    \"\"\"\n    Transform raw values of pandas Series with a scikit-learn alike \n    transformer, preprocessor or ColumnTransformer. The transformer used must \n    have the following methods: fit, transform, fit_transform and \n    inverse_transform. ColumnTransformers are not allowed since they do not \n    have inverse_transform method.\n\n    Parameters\n    ----------\n    series : pandas Series\n        Series to be transformed.\n    transformer : scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n        Scikit-learn alike transformer (preprocessor) with methods: fit, transform,\n        fit_transform and inverse_transform.\n    fit : bool, default `False`\n        Train the transformer before applying it.\n    inverse_transform : bool, default `False`\n        Transform back the data to the original representation. This is not available\n        when using transformers of class scikit-learn ColumnTransformers.\n\n    Returns\n    -------\n    series_transformed : pandas Series, pandas DataFrame\n        Transformed Series. Depending on the transformer used, the output may \n        be a Series or a DataFrame.\n\n    \"\"\"\n    if not isinstance(series, pd.Series):\n        raise TypeError(f'`series` argument must be a pandas Series. Got {type(series)}.')\n    if transformer is None:\n        return series\n    if series.name is None:\n        series.name = 'no_name'\n    data = series.to_frame()\n    if fit and hasattr(transformer, 'fit'):\n        transformer.fit(data)\n    if hasattr(transformer, 'feature_names_in_') and transformer.feature_names_in_[0] != data.columns[0]:\n        transformer = deepcopy(transformer)\n        transformer.feature_names_in_ = np.array([data.columns[0]], dtype=object)\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore', category=UserWarning)\n        if inverse_transform:\n            values_transformed = transformer.inverse_transform(data)\n        else:\n            values_transformed = transformer.transform(data)\n    if hasattr(values_transformed, 'toarray'):\n        values_transformed = values_transformed.toarray()\n    if isinstance(values_transformed, np.ndarray) and values_transformed.shape[1] == 1:\n        series_transformed = pd.Series(data=values_transformed.ravel(), index=data.index, name=data.columns[0])\n    elif isinstance(values_transformed, pd.DataFrame) and values_transformed.shape[1] == 1:\n        series_transformed = values_transformed.squeeze()\n    else:\n        series_transformed = pd.DataFrame(data=values_transformed, index=data.index, columns=transformer.get_feature_names_out())\n    return series_transformed\n\ndef transform_dataframe(df: pd.DataFrame, transformer, fit: bool=False, inverse_transform: bool=False) -> pd.DataFrame:\n    \"\"\"\n    Transform raw values of pandas DataFrame with a scikit-learn alike \n    transformer, preprocessor or ColumnTransformer. The transformer used must \n    have the following methods: fit, transform, fit_transform and \n    inverse_transform. ColumnTransformers are not allowed since they do not \n    have inverse_transform method.\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        DataFrame to be transformed.\n    transformer : scikit-learn alike transformer, preprocessor, or ColumnTransformer.\n        Scikit-learn alike transformer (preprocessor) with methods: fit, transform,\n        fit_transform and inverse_transform.\n    fit : bool, default `False`\n        Train the transformer before applying it.\n    inverse_transform : bool, default `False`\n        Transform back the data to the original representation. This is not available\n        when using transformers of class scikit-learn ColumnTransformers.\n\n    Returns\n    -------\n    df_transformed : pandas DataFrame\n        Transformed DataFrame.\n\n    \"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise TypeError(f'`df` argument must be a pandas DataFrame. Got {type(df)}')\n    if transformer is None:\n        return df\n    if inverse_transform and isinstance(transformer, ColumnTransformer):\n        raise ValueError('`inverse_transform` is not available when using ColumnTransformers.')\n    if not inverse_transform:\n        if fit:\n            values_transformed = transformer.fit_transform(df)\n        else:\n            values_transformed = transformer.transform(df)\n    else:\n        values_transformed = transformer.inverse_transform(df)\n    if hasattr(values_transformed, 'toarray'):\n        values_transformed = values_transformed.toarray()\n    if hasattr(transformer, 'get_feature_names_out'):\n        feature_names_out = transformer.get_feature_names_out()\n    elif hasattr(transformer, 'categories_'):\n        feature_names_out = transformer.categories_\n    else:\n        feature_names_out = df.columns\n    df_transformed = pd.DataFrame(data=values_transformed, index=df.index, columns=feature_names_out)\n    return df_transformed\n\ndef save_forecaster(forecaster: object, file_name: str, save_custom_functions: bool=True, verbose: bool=True) -> None:\n    \"\"\"\n    Save forecaster model using joblib. If custom functions are used to create\n    weights, they are saved as .py files.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster created with skforecast library.\n    file_name : str\n        File name given to the object. The save extension will be .joblib.\n    save_custom_functions : bool, default True\n        If True, save custom functions used in the forecaster (weight_func) as \n        .py files. Custom functions need to be available in the environment \n        where the forecaster is going to be loaded.\n    verbose : bool, default True\n        Print summary about the forecaster saved.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    file_name = Path(file_name).with_suffix('.joblib')\n    joblib.dump(forecaster, filename=file_name)\n    if save_custom_functions:\n        if hasattr(forecaster, 'weight_func') and forecaster.weight_func is not None:\n            if isinstance(forecaster.weight_func, dict):\n                for fun in set(forecaster.weight_func.values()):\n                    file_name = fun.__name__ + '.py'\n                    with open(file_name, 'w') as file:\n                        file.write(inspect.getsource(fun))\n            else:\n                file_name = forecaster.weight_func.__name__ + '.py'\n                with open(file_name, 'w') as file:\n                    file.write(inspect.getsource(forecaster.weight_func))\n    elif hasattr(forecaster, 'weight_func') and forecaster.weight_func is not None:\n        warnings.warn('Custom function(s) used to create weights are not saved. To save them, set `save_custom_functions` to `True`.', SaveLoadSkforecastWarning)\n    if hasattr(forecaster, 'window_features') and forecaster.window_features is not None:\n        skforecast_classes = {'RollingFeatures'}\n        custom_classes = set(forecaster.window_features_class_names) - skforecast_classes\n        if custom_classes:\n            warnings.warn('The Forecaster includes custom user-defined classes in the `window_features` argument. These classes are not saved automatically when saving the Forecaster. Please ensure you save these classes manually and import them before loading the Forecaster.\\n    Custom classes: ' + ', '.join(custom_classes) + '\\nVisit the documentation for more information: https://skforecast.org/latest/user_guides/save-load-forecaster.html#saving-and-loading-a-forecaster-model-with-custom-features', SaveLoadSkforecastWarning)\n    if verbose:\n        forecaster.summary()\n\ndef load_forecaster(file_name: str, verbose: bool=True) -> object:\n    \"\"\"\n    Load forecaster model using joblib. If the forecaster was saved with \n    custom user-defined classes as as window features or custom\n    functions to create weights, these objects must be available\n    in the environment where the forecaster is going to be loaded.\n\n    Parameters\n    ----------\n    file_name: str\n        Object file name.\n    verbose: bool, default `True`\n        Print summary about the forecaster loaded.\n\n    Returns\n    -------\n    forecaster: Forecaster\n        Forecaster created with skforecast library.\n    \n    \"\"\"\n    forecaster = joblib.load(filename=Path(file_name))\n    skforecast_v = skforecast.__version__\n    forecaster_v = forecaster.skforecast_version\n    if forecaster_v != skforecast_v:\n        warnings.warn(f'The skforecast version installed in the environment differs from the version used to create the forecaster.\\n    Installed Version  : {skforecast_v}\\n    Forecaster Version : {forecaster_v}\\nThis may create incompatibilities when using the library.', SkforecastVersionWarning)\n    if verbose:\n        forecaster.summary()\n    return forecaster\n\ndef _find_optional_dependency(package_name: str, optional_dependencies: dict=optional_dependencies) -> Tuple[str, str]:\n    \"\"\"\n    Find if a package is an optional dependency. If True, find the version and \n    the extension it belongs to.\n\n    Parameters\n    ----------\n    package_name : str\n        Name of the package to check.\n    optional_dependencies : dict, default `optional_dependencies`\n        Skforecast optional dependencies.\n\n    Returns\n    -------\n    extra: str\n        Name of the extra extension where the optional dependency is needed.\n    package_version: srt\n        Name and versions of the dependency.\n\n    \"\"\"\n    for extra, packages in optional_dependencies.items():\n        package_version = [package for package in packages if package_name in package]\n        if package_version:\n            return (extra, package_version[0])\n\ndef check_optional_dependency(package_name: str) -> None:\n    \"\"\"\n    Check if an optional dependency is installed, if not raise an ImportError  \n    with installation instructions.\n\n    Parameters\n    ----------\n    package_name : str\n        Name of the package to check.\n\n    Returns\n    -------\n    None\n    \n    \"\"\"\n    if importlib.util.find_spec(package_name) is None:\n        try:\n            extra, package_version = _find_optional_dependency(package_name=package_name)\n            msg = f'''\\n'{package_name}' is an optional dependency not included in the default skforecast installation. Please run: `pip install \"{package_version}\"` to install it.\\n\\nAlternately, you can install it by running `pip install skforecast[{extra}]`'''\n        except:\n            msg = f\"\\n'{package_name}' is needed but not installed. Please install it.\"\n        raise ImportError(msg)\n\ndef multivariate_time_series_corr(time_series: pd.Series, other: pd.DataFrame, lags: Union[int, list, np.array], method: str='pearson') -> pd.DataFrame:\n    \"\"\"\n    Compute correlation between a time_series and the lagged values of other \n    time series. \n\n    Parameters\n    ----------\n    time_series : pandas Series\n        Target time series.\n    other : pandas DataFrame\n        Time series whose lagged values are correlated to `time_series`.\n    lags : int, list, numpy ndarray\n        Lags to be included in the correlation analysis.\n    method : str, default 'pearson'\n        - 'pearson': standard correlation coefficient.\n        - 'kendall': Kendall Tau correlation coefficient.\n        - 'spearman': Spearman rank correlation.\n\n    Returns\n    -------\n    corr : pandas DataFrame\n        Correlation values.\n\n    \"\"\"\n    if not len(time_series) == len(other):\n        raise ValueError('`time_series` and `other` must have the same length.')\n    if not (time_series.index == other.index).all():\n        raise ValueError('`time_series` and `other` must have the same index.')\n    if isinstance(lags, int):\n        lags = range(lags)\n    corr = {}\n    for col in other.columns:\n        lag_values = {}\n        for lag in lags:\n            lag_values[lag] = other[col].shift(lag)\n        lag_values = pd.DataFrame(lag_values)\n        lag_values.insert(0, None, time_series)\n        corr[col] = lag_values.corr(method=method).iloc[1:, 0]\n    corr = pd.DataFrame(corr)\n    corr.index = corr.index.astype('int64')\n    corr.index.name = 'lag'\n    return corr\n\ndef select_n_jobs_fit_forecaster(forecaster_name: str, regressor: object) -> int:\n    \"\"\"\n    Select the optimal number of jobs to use in the fitting process. This\n    selection is based on heuristics and is not guaranteed to be optimal. \n    \n    The number of jobs is chosen as follows:\n    \n    - If forecaster_name is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and regressor_name is a linear regressor then `n_jobs = 1`, \n    otherwise `n_jobs = cpu_count() - 1`.\n    - If regressor is a `LGBMRegressor(n_jobs=1)`, then `n_jobs = cpu_count() - 1`.\n    - If regressor is a `LGBMRegressor` with internal n_jobs != 1, then `n_jobs = 1`.\n    This is because `lightgbm` is highly optimized for gradient boosting and\n    parallelizes operations at a very fine-grained level, making additional\n    parallelization unnecessary and potentially harmful due to resource contention.\n    \n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n\n    Returns\n    -------\n    n_jobs : int\n        The number of jobs to run in parallel.\n    \n    \"\"\"\n    if isinstance(regressor, Pipeline):\n        regressor = regressor[-1]\n        regressor_name = type(regressor).__name__\n    else:\n        regressor_name = type(regressor).__name__\n    linear_regressors = [regressor_name for regressor_name in dir(sklearn.linear_model) if not regressor_name.startswith('_')]\n    if forecaster_name in ['ForecasterDirect', 'ForecasterDirectMultiVariate']:\n        if regressor_name in linear_regressors:\n            n_jobs = 1\n        elif regressor_name == 'LGBMRegressor':\n            n_jobs = joblib.cpu_count() - 1 if regressor.n_jobs == 1 else 1\n        else:\n            n_jobs = joblib.cpu_count() - 1\n    else:\n        n_jobs = 1\n    return n_jobs\n\ndef check_preprocess_series(series: Union[pd.DataFrame, dict]) -> Tuple[dict, pd.Index]:\n    \"\"\"\n    Check and preprocess `series` argument in `ForecasterRecursiveMultiSeries` class.\n\n    - If `series` is a pandas DataFrame, it is converted to a dict of pandas \n    Series and index is overwritten according to the rules of preprocess_y.\n    - If `series` is a dict, all values are converted to pandas Series. Checks\n    if all index are pandas DatetimeIndex and, at least, one Series has a non-null\n    frequency. No multiple frequency is allowed.\n\n    Parameters\n    ----------\n    series : pandas DataFrame, dict\n        Training time series.\n\n    Returns\n    -------\n    series_dict : dict\n        Dictionary with the series used during training.\n    series_indexes : dict\n        Dictionary with the index of each series.\n    \n    \"\"\"\n    if isinstance(series, pd.DataFrame):\n        _, series_index = preprocess_y(y=series, return_values=False)\n        series = series.copy()\n        series.index = series_index\n        series_dict = series.to_dict('series')\n    elif isinstance(series, dict):\n        not_valid_series = [k for k, v in series.items() if not isinstance(v, (pd.Series, pd.DataFrame))]\n        if not_valid_series:\n            raise TypeError(f'If `series` is a dictionary, all series must be a named pandas Series or a pandas DataFrame with a single column. Review series: {not_valid_series}')\n        series_dict = {k: v.copy() for k, v in series.items()}\n        for k, v in series_dict.items():\n            if isinstance(v, pd.DataFrame):\n                if v.shape[1] != 1:\n                    raise ValueError(f\"If `series` is a dictionary, all series must be a named pandas Series or a pandas DataFrame with a single column. Review series: '{k}'\")\n                series_dict[k] = v.iloc[:, 0]\n            series_dict[k].name = k\n        not_valid_index = [k for k, v in series_dict.items() if not isinstance(v.index, pd.DatetimeIndex)]\n        if not_valid_index:\n            raise TypeError(f'If `series` is a dictionary, all series must have a Pandas DatetimeIndex as index with the same frequency. Review series: {not_valid_index}')\n        indexes_freq = [f'{v.index.freq}' for v in series_dict.values()]\n        indexes_freq = sorted(set(indexes_freq))\n        if not len(indexes_freq) == 1:\n            raise ValueError(f'If `series` is a dictionary, all series must have a Pandas DatetimeIndex as index with the same frequency. Found frequencies: {indexes_freq}')\n    else:\n        raise TypeError(f'`series` must be a pandas DataFrame or a dict of DataFrames or Series. Got {type(series)}.')\n    for k, v in series_dict.items():\n        if np.isnan(v).all():\n            raise ValueError(f\"All values of series '{k}' are NaN.\")\n    series_indexes = {k: v.index for k, v in series_dict.items()}\n    return (series_dict, series_indexes)\n\ndef check_preprocess_exog_multiseries(input_series_is_dict: bool, series_indexes: dict, series_names_in_: list, exog: Union[pd.Series, pd.DataFrame, dict], exog_dict: dict) -> Tuple[dict, list]:\n    \"\"\"\n    Check and preprocess `exog` argument in `ForecasterRecursiveMultiSeries` class.\n\n    - If input series is a pandas DataFrame (input_series_is_dict = False),  \n    checks that input exog (pandas Series, DataFrame or dict) has the same index \n    (type, length and frequency). Index is overwritten according to the rules \n    of preprocess_exog. Create a dict of exog with the same keys as series.\n    - If input series is a dict (input_series_is_dict = True), then input \n    exog must be a dict. Check exog has a pandas DatetimeIndex and convert all\n    values to pandas DataFrames.\n\n    Parameters\n    ----------\n    input_series_is_dict : bool\n        Indicates if input series argument is a dict.\n    series_indexes : dict\n        Dictionary with the index of each series.\n    series_names_in_ : list\n        Names of the series (levels) used during training.\n    exog : pandas Series, pandas DataFrame, dict\n        Exogenous variable/s used during training.\n    exog_dict : dict\n        Dictionary with the exogenous variable/s used during training.\n\n    Returns\n    -------\n    exog_dict : dict\n        Dictionary with the exogenous variable/s used during training.\n    exog_names_in_ : list\n        Names of the exogenous variables used during training.\n    \n    \"\"\"\n    if not isinstance(exog, (pd.Series, pd.DataFrame, dict)):\n        raise TypeError(f'`exog` must be a pandas Series, DataFrame, dictionary of pandas Series/DataFrames or None. Got {type(exog)}.')\n    if not input_series_is_dict:\n        series_index = series_indexes[series_names_in_[0]]\n    if isinstance(exog, (pd.Series, pd.DataFrame)):\n        if input_series_is_dict:\n            raise TypeError(f'`exog` must be a dict of DataFrames or Series if `series` is a dict. Got {type(exog)}.')\n        _, exog_index = preprocess_exog(exog=exog, return_values=False)\n        exog = exog.copy().to_frame() if isinstance(exog, pd.Series) else exog.copy()\n        exog.index = exog_index\n        if len(exog) != len(series_index):\n            raise ValueError(f'`exog` must have same number of samples as `series`. length `exog`: ({len(exog)}), length `series`: ({len(series_index)})')\n        if not (exog_index == series_index).all():\n            raise ValueError('Different index for `series` and `exog`. They must be equal to ensure the correct alignment of values.')\n        exog_dict = {serie: exog for serie in series_names_in_}\n    else:\n        not_valid_exog = [k for k, v in exog.items() if not isinstance(v, (pd.Series, pd.DataFrame, type(None)))]\n        if not_valid_exog:\n            raise TypeError(f'If `exog` is a dictionary, all exog must be a named pandas Series, a pandas DataFrame or None. Review exog: {not_valid_exog}')\n        exog_dict.update(((k, v.copy()) for k, v in exog.items() if k in exog_dict and v is not None))\n        series_not_in_exog = set(series_names_in_) - set(exog.keys())\n        if series_not_in_exog:\n            warnings.warn(f'{series_not_in_exog} not present in `exog`. All values of the exogenous variables for these series will be NaN.', MissingExogWarning)\n        for k, v in exog_dict.items():\n            if v is not None:\n                check_exog(exog=v, allow_nan=True)\n                if isinstance(v, pd.Series):\n                    v = v.to_frame()\n                exog_dict[k] = v\n        if not input_series_is_dict:\n            for k, v in exog_dict.items():\n                if v is not None:\n                    if len(v) != len(series_index):\n                        raise ValueError(f\"`exog` for series '{k}' must have same number of samples as `series`. length `exog`: ({len(v)}), length `series`: ({len(series_index)})\")\n                    _, v_index = preprocess_exog(exog=v, return_values=False)\n                    exog_dict[k].index = v_index\n                    if not (exog_dict[k].index == series_index).all():\n                        raise ValueError(f\"Different index for series '{k}' and its exog. When `series` is a pandas DataFrame, they must be equal to ensure the correct alignment of values.\")\n        else:\n            not_valid_index = [k for k, v in exog_dict.items() if v is not None and (not isinstance(v.index, pd.DatetimeIndex))]\n            if not_valid_index:\n                raise TypeError(f'All exog must have a Pandas DatetimeIndex as index with the same frequency. Check exog for series: {not_valid_index}')\n        exog_dtypes_buffer = [df.dtypes for df in exog_dict.values() if df is not None]\n        exog_dtypes_buffer = pd.concat(exog_dtypes_buffer, axis=1)\n        exog_dtypes_nunique = exog_dtypes_buffer.nunique(axis=1).eq(1)\n        if not exog_dtypes_nunique.all():\n            non_unique_dtyeps_exogs = exog_dtypes_nunique[exog_dtypes_nunique != 1].index.to_list()\n            raise TypeError(f'Exog/s: {non_unique_dtyeps_exogs} have different dtypes in different series.')\n    exog_names_in_ = list(set((column for df in exog_dict.values() if df is not None for column in df.columns.to_list())))\n    if len(set(exog_names_in_) - set(series_names_in_)) != len(exog_names_in_):\n        raise ValueError(f'`exog` cannot contain a column named the same as one of the series.\\n    `series` columns : {series_names_in_}.\\n    `exog`   columns : {exog_names_in_}.')\n    return (exog_dict, exog_names_in_)\n\ndef align_series_and_exog_multiseries(series_dict: dict, input_series_is_dict: bool, exog_dict: dict=None) -> Tuple[Union[pd.Series, pd.DataFrame], Union[pd.Series, pd.DataFrame]]:\n    \"\"\"\n    Align series and exog according to their index. If needed, reindexing is\n    applied. Heading and trailing NaNs are removed from all series in \n    `series_dict`.\n\n    - If input series is a pandas DataFrame (input_series_is_dict = False),  \n    input exog (pandas Series, DataFrame or dict) must have the same index \n    (type, length and frequency). Reindexing is not applied.\n    - If input series is a dict (input_series_is_dict = True), then input \n    exog must be a dict. Both must have a pandas DatetimeIndex, but can have \n    different lengths. Reindexing is applied.\n\n    Parameters\n    ----------\n    series_dict : dict\n        Dictionary with the series used during training.\n    input_series_is_dict : bool\n        Indicates if input series argument is a dict.\n    exog_dict : dict, default `None`\n        Dictionary with the exogenous variable/s used during training.\n\n    Returns\n    -------\n    series_dict : dict\n        Dictionary with the series used during training.\n    exog_dict : dict\n        Dictionary with the exogenous variable/s used during training.\n    \n    \"\"\"\n    for k in series_dict.keys():\n        first_valid_index = series_dict[k].first_valid_index()\n        last_valid_index = series_dict[k].last_valid_index()\n        series_dict[k] = series_dict[k].loc[first_valid_index:last_valid_index]\n        if exog_dict[k] is not None:\n            if input_series_is_dict:\n                index_intersection = series_dict[k].index.intersection(exog_dict[k].index)\n                if len(index_intersection) == 0:\n                    warnings.warn(f\"Series '{k}' and its `exog` do not have the same index. All exog values will be NaN for the period of the series.\", MissingValuesWarning)\n                elif len(index_intersection) != len(series_dict[k]):\n                    warnings.warn(f\"Series '{k}' and its `exog` do not have the same length. Exog values will be NaN for the not matched period of the series.\", MissingValuesWarning)\n                exog_dict[k] = exog_dict[k].loc[index_intersection]\n                if len(index_intersection) != len(series_dict[k]):\n                    exog_dict[k] = exog_dict[k].reindex(series_dict[k].index, fill_value=np.nan)\n            else:\n                exog_dict[k] = exog_dict[k].loc[first_valid_index:last_valid_index]\n    return (series_dict, exog_dict)\n\ndef prepare_levels_multiseries(X_train_series_names_in_: list, levels: Optional[Union[str, list]]=None) -> Tuple[list, bool]:\n    \"\"\"\n    Prepare list of levels to be predicted in multiseries Forecasters.\n\n    Parameters\n    ----------\n    X_train_series_names_in_ : list\n        Names of the series (levels) included in the matrix `X_train`.\n    levels : str, list, default `None`\n        Names of the series (levels) to be predicted.\n\n    Returns\n    -------\n    levels : list\n        Names of the series (levels) to be predicted.\n\n    \"\"\"\n    input_levels_is_list = False\n    if levels is None:\n        levels = X_train_series_names_in_\n    elif isinstance(levels, str):\n        levels = [levels]\n    else:\n        input_levels_is_list = True\n    return (levels, input_levels_is_list)\n\ndef preprocess_levels_self_last_window_multiseries(levels: list, input_levels_is_list: bool, last_window_: dict) -> Tuple[list, pd.DataFrame]:\n    \"\"\"\n    Preprocess `levels` and `last_window` (when using self.last_window_) arguments \n    in multiseries Forecasters when predicting. Only levels whose last window \n    ends at the same datetime index will be predicted together.\n\n    Parameters\n    ----------\n    levels : list\n        Names of the series (levels) to be predicted.\n    input_levels_is_list : bool\n        Indicates if input levels argument is a list.\n    last_window_ : dict\n        Dictionary with the last window of each series (self.last_window_).\n\n    Returns\n    -------\n    levels : list\n        Names of the series (levels) to be predicted.\n    last_window : pandas DataFrame\n        Series values used to create the predictors (lags) needed in the \n        first iteration of the prediction (t + 1).\n\n    \"\"\"\n    available_last_windows = set() if last_window_ is None else set(last_window_.keys())\n    not_available_last_window = set(levels) - available_last_windows\n    if not_available_last_window:\n        levels = [level for level in levels if level not in not_available_last_window]\n        if not levels:\n            raise ValueError(f'No series to predict. None of the series {not_available_last_window} are present in `last_window_` attribute. Provide `last_window` as argument in predict method.')\n        else:\n            warnings.warn(f\"Levels {not_available_last_window} are excluded from prediction since they were not stored in `last_window_` attribute during training. If you don't want to retrain the Forecaster, provide `last_window` as argument.\", IgnoredArgumentWarning)\n    last_index_levels = [v.index[-1] for k, v in last_window_.items() if k in levels]\n    if len(set(last_index_levels)) > 1:\n        max_index_levels = max(last_index_levels)\n        selected_levels = [k for k, v in last_window_.items() if k in levels and v.index[-1] == max_index_levels]\n        series_excluded_from_last_window = set(levels) - set(selected_levels)\n        levels = selected_levels\n        if input_levels_is_list and series_excluded_from_last_window:\n            warnings.warn(f\"Only series whose last window ends at the same index can be predicted together. Series that do not reach the maximum index, '{max_index_levels}', are excluded from prediction: {series_excluded_from_last_window}.\", IgnoredArgumentWarning)\n    last_window = pd.DataFrame({k: v for k, v in last_window_.items() if k in levels})\n    return (levels, last_window)\n\ndef prepare_residuals_multiseries(levels: list, use_in_sample_residuals: bool, encoding: Optional[str]=None, in_sample_residuals_: Optional[dict]=None, out_sample_residuals_: Optional[dict]=None) -> Tuple[list, bool]:\n    \"\"\"\n    Prepare residuals for bootstrapping prediction in multiseries Forecasters.\n\n    Parameters\n    ----------\n    levels : list\n        Names of the series (levels) to be predicted.\n    use_in_sample_residuals : bool\n        Indicates if `forecaster.in_sample_residuals_` are used.\n    encoding : str, default `None`\n        Encoding used to identify the different series (`ForecasterRecursiveMultiSeries`).\n    in_sample_residuals_ : dict, default `None`\n        Residuals of the model when predicting training data. Only stored up to\n        1000 values in the form `{level: residuals}`. If `transformer_series` \n        is not `None`, residuals are stored in the transformed scale.\n    out_sample_residuals_ : dict, default `None`\n        Residuals of the model when predicting non-training data. Only stored\n        up to 1000 values in the form `{level: residuals}`. If `transformer_series` \n        is not `None`, residuals are assumed to be in the transformed scale. Use \n        `set_out_sample_residuals()` method to set values.\n\n    Returns\n    -------\n    levels : list\n        Names of the series (levels) to be predicted.\n    residuals : dict\n        Residuals of the model for each level to use in bootstrapping prediction.\n\n    \"\"\"\n    if use_in_sample_residuals:\n        unknown_levels = set(levels) - set(in_sample_residuals_.keys())\n        if unknown_levels and encoding is not None:\n            warnings.warn(f'`levels` {unknown_levels} are not present in `forecaster.in_sample_residuals_`, most likely because they were not present in the training data. A random sample of the residuals from other levels will be used. This can lead to inaccurate intervals for the unknown levels.', UnknownLevelWarning)\n        residuals = in_sample_residuals_.copy()\n    elif out_sample_residuals_ is None:\n        raise ValueError('`forecaster.out_sample_residuals_` is `None`. Use `use_in_sample_residuals=True` or the `set_out_sample_residuals()` method before predicting.')\n    else:\n        unknown_levels = set(levels) - set(out_sample_residuals_.keys())\n        if unknown_levels and encoding is not None:\n            warnings.warn(f'`levels` {unknown_levels} are not present in `forecaster.out_sample_residuals_`. A random sample of the residuals from other levels will be used. This can lead to inaccurate intervals for the unknown levels. Otherwise, Use the `set_out_sample_residuals()` method before predicting to set the residuals for these levels.', UnknownLevelWarning)\n        residuals = out_sample_residuals_.copy()\n    check_residuals = 'forecaster.in_sample_residuals_' if use_in_sample_residuals else 'forecaster.out_sample_residuals_'\n    for level in levels:\n        if level in unknown_levels:\n            residuals[level] = residuals['_unknown_level']\n        if residuals[level] is None or len(residuals[level]) == 0:\n            raise ValueError(f\"Not available residuals for level '{level}'. Check `{check_residuals}`.\")\n        elif any((element is None for element in residuals[level])) or np.any(np.isnan(residuals[level])):\n            raise ValueError(f\"forecaster residuals for level '{level}' contains `None` or `NaNs` values. Check `{check_residuals}`.\")\n    return residuals\n\ndef prepare_steps_direct(max_step: int, steps: Optional[Union[int, list]]=None) -> list:\n    \"\"\"\n    Prepare list of steps to be predicted in Direct Forecasters.\n\n    Parameters\n    ----------\n    max_step : int\n        Maximum number of future steps the forecaster will predict \n        when using method `predict()`.\n    steps : int, list, None, default `None`\n        Predict n steps. The value of `steps` must be less than or equal to the \n        value of steps defined when initializing the forecaster. Starts at 1.\n    \n        - If `int`: Only steps within the range of 1 to int are predicted.\n        - If `list`: List of ints. Only the steps contained in the list \n        are predicted.\n        - If `None`: As many steps are predicted as were defined at \n        initialization.\n\n    Returns\n    -------\n    steps : list\n        Steps to be predicted.\n\n    \"\"\"\n    if isinstance(steps, int):\n        steps = list(np.arange(steps) + 1)\n    elif steps is None:\n        steps = list(np.arange(max_step) + 1)\n    elif isinstance(steps, list):\n        steps = list(np.array(steps))\n    for step in steps:\n        if not isinstance(step, (int, np.int64, np.int32)):\n            raise TypeError(f'`steps` argument must be an int, a list of ints or `None`. Got {type(steps)}.')\n    steps = [int(step) for step in steps if step is not None]\n    return steps\n\ndef set_skforecast_warnings(suppress_warnings: bool, action: str='default') -> None:\n    \"\"\"\n    Set skforecast warnings action.\n\n    Parameters\n    ----------\n    suppress_warnings : bool\n        If `True`, skforecast warnings will be suppressed. If `False`, skforecast\n        warnings will be shown as default. See \n        skforecast.exceptions.warn_skforecast_categories for more information.\n    action : str, default `'default'`\n        Action to be taken when a warning is raised. See the warnings module\n        for more information.\n\n    Returns\n    -------\n    None\n    \n    \"\"\"\n    if suppress_warnings:\n        for category in warn_skforecast_categories:\n            warnings.filterwarnings(action, category=category)",
    "skforecast/recursive/_forecaster_recursive.py": "from typing import Union, Tuple, Optional, Callable\nimport warnings\nimport sys\nimport numpy as np\nimport pandas as pd\nimport inspect\nfrom copy import copy\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import clone\nimport skforecast\nfrom ..base import ForecasterBase\nfrom ..exceptions import DataTransformationWarning\nfrom ..utils import initialize_lags, initialize_window_features, initialize_weights, check_select_fit_kwargs, check_y, check_exog, get_exog_dtypes, check_exog_dtypes, check_predict_input, check_interval, preprocess_y, preprocess_last_window, preprocess_exog, input_to_frame, date_to_index_position, expand_index, transform_numpy, transform_dataframe\nfrom ..preprocessing import TimeSeriesDifferentiator\nfrom ..preprocessing import QuantileBinner\n\nclass ForecasterRecursive(ForecasterBase):\n    \"\"\"\n    This class turns any regressor compatible with the scikit-learn API into a\n    recursive autoregressive (multi-step) forecaster.\n    \n    Parameters\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n    lags : int, list, numpy ndarray, range, default `None`\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n    \n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n        - `None`: no lags are included as predictors. \n    window_features : object, list, default `None`\n        Instance or list of instances used to create window features. Window features\n        are created from the original time series and are included as predictors.\n    transformer_y : object transformer (preprocessor), default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and inverse_transform.\n        ColumnTransformers are not allowed since they do not have inverse_transform method.\n        The transformation is applied to `y` before training the forecaster. \n    transformer_exog : object transformer (preprocessor), default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable, default `None`\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\n        method. The resulting `sample_weight` cannot have negative values.\n    differentiation : int, default `None`\n        Order of differencing applied to the time series before training the forecaster.\n        If `None`, no differencing is applied. The order of differentiation is the number\n        of times the differencing operation is applied to a time series. Differencing\n        involves computing the differences between consecutive data points in the series.\n        Differentiation is reversed in the output of `predict()` and `predict_interval()`.\n        **WARNING: This argument is newly introduced and requires special attention. It\n        is still experimental and may undergo changes.**\n    fit_kwargs : dict, default `None`\n        Additional arguments to be passed to the `fit` method of the regressor.\n    binner_kwargs : dict, default `None`\n        Additional arguments to pass to the `QuantileBinner` used to discretize \n        the residuals into k bins according to the predicted values associated \n        with each residual. Available arguments are: `n_bins`, `method`, `subsample`,\n        `random_state` and `dtype`. Argument `method` is passed internally to the\n        fucntion `numpy.percentile`.\n        **New in version 0.14.0**\n    forecaster_id : str, int, default `None`\n        Name used as an identifier of the forecaster.\n    \n    Attributes\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n    lags : numpy ndarray\n        Lags used as predictors.\n    lags_names : list\n        Names of the lags used as predictors.\n    max_lag : int\n        Maximum lag included in `lags`.\n    window_features : list\n        Class or list of classes used to create window features.\n    window_features_names : list\n        Names of the window features to be included in the `X_train` matrix.\n    window_features_class_names : list\n        Names of the classes used to create the window features.\n    max_size_window_features : int\n        Maximum window size required by the window features.\n    window_size : int\n        The window size needed to create the predictors. It is calculated as the \n        maximum value between `max_lag` and `max_size_window_features`. If \n        differentiation is used, `window_size` is increased by n units equal to \n        the order of differentiation so that predictors can be generated correctly.\n    transformer_y : object transformer (preprocessor)\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and inverse_transform.\n        ColumnTransformers are not allowed since they do not have inverse_transform method.\n        The transformation is applied to `y` before training the forecaster.\n    transformer_exog : object transformer (preprocessor)\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\n        method. The resulting `sample_weight` cannot have negative values.\n    differentiation : int\n        Order of differencing applied to the time series before training the forecaster.\n        If `None`, no differencing is applied. The order of differentiation is the number\n        of times the differencing operation is applied to a time series. Differencing\n        involves computing the differences between consecutive data points in the series.\n        Differentiation is reversed in the output of `predict()` and `predict_interval()`.\n        **WARNING: This argument is newly introduced and requires special attention. It\n        is still experimental and may undergo changes.**\n        **New in version 0.10.0**\n    binner : sklearn.preprocessing.KBinsDiscretizer\n        `KBinsDiscretizer` used to discretize residuals into k bins according \n        to the predicted values associated with each residual.\n        **New in version 0.12.0**\n    binner_intervals_ : dict\n        Intervals used to discretize residuals into k bins according to the predicted\n        values associated with each residual.\n        **New in version 0.12.0**\n    binner_kwargs : dict\n        Additional arguments to pass to the `QuantileBinner` used to discretize \n        the residuals into k bins according to the predicted values associated \n        with each residual. Available arguments are: `n_bins`, `method`, `subsample`,\n        `random_state` and `dtype`. Argument `method` is passed internally to the\n        fucntion `numpy.percentile`.\n        **New in version 0.14.0**\n    source_code_weight_func : str\n        Source code of the custom function used to create weights.\n    differentiation : int\n        Order of differencing applied to the time series before training the \n        forecaster.\n    differentiator : TimeSeriesDifferentiator\n        Skforecast object used to differentiate the time series.\n    last_window_ : pandas DataFrame\n        This window represents the most recent data observed by the predictor\n        during its training phase. It contains the values needed to predict the\n        next step immediately after the training data. These values are stored\n        in the original scale of the time series before undergoing any transformations\n        or differentiation. When `differentiation` parameter is specified, the\n        dimensions of the `last_window_` are expanded as many values as the order\n        of differentiation. For example, if `lags` = 7 and `differentiation` = 1,\n        `last_window_` will have 8 values.\n    index_type_ : type\n        Type of index of the input used in training.\n    index_freq_ : str\n        Frequency of Index of the input used in training.\n    training_range_ : pandas Index\n        First and last values of index of the data used during training.\n    exog_in_ : bool\n        If the forecaster has been trained using exogenous variable/s.\n    exog_names_in_ : list\n        Names of the exogenous variables used during training.\n    exog_type_in_ : type\n        Type of exogenous data (pandas Series or DataFrame) used in training.\n    exog_dtypes_in_ : dict\n        Type of each exogenous variable/s used in training. If `transformer_exog` \n        is used, the dtypes are calculated before the transformation.\n    X_train_window_features_names_out_ : list\n        Names of the window features included in the matrix `X_train` created\n        internally for training.\n    X_train_exog_names_out_ : list\n        Names of the exogenous variables included in the matrix `X_train` created\n        internally for training. It can be different from `exog_names_in_` if\n        some exogenous variables are transformed during the training process.\n    X_train_features_names_out_ : list\n        Names of columns of the matrix created internally for training.\n    fit_kwargs : dict\n        Additional arguments to be passed to the `fit` method of the regressor.\n    in_sample_residuals_ : numpy ndarray\n        Residuals of the model when predicting training data. Only stored up to\n        10_000 values. If `transformer_y` is not `None`, residuals are stored in\n        the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation.\n    in_sample_residuals_by_bin_ : dict\n        In sample residuals binned according to the predicted value each residual\n        is associated with. If `transformer_y` is not `None`, residuals are stored\n        in the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n        **New in version 0.14.0**\n    out_sample_residuals_ : numpy ndarray\n        Residuals of the model when predicting non training data. Only stored up to\n        10_000 values. If `transformer_y` is not `None`, residuals are stored in\n        the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation.\n    out_sample_residuals_by_bin_ : dict\n        Out of sample residuals binned according to the predicted value each residual\n        is associated with. If `transformer_y` is not `None`, residuals are stored\n        in the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n        **New in version 0.12.0**\n    creation_date : str\n        Date of creation.\n    is_fitted : bool\n        Tag to identify if the regressor has been fitted (trained).\n    fit_date : str\n        Date of last fit.\n    skforecast_version : str\n        Version of skforecast library used to create the forecaster.\n    python_version : str\n        Version of python used to create the forecaster.\n    forecaster_id : str, int\n        Name used as an identifier of the forecaster.\n    \n    \"\"\"\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Information displayed when a ForecasterRecursive object is printed.\n        \"\"\"\n        params, _, _, exog_names_in_, _ = self._preprocess_repr(regressor=self.regressor, exog_names_in_=self.exog_names_in_)\n        params = self._format_text_repr(params)\n        exog_names_in_ = self._format_text_repr(exog_names_in_)\n        info = f'{'=' * len(type(self).__name__)} \\n{type(self).__name__} \\n{'=' * len(type(self).__name__)} \\nRegressor: {type(self.regressor).__name__} \\nLags: {self.lags} \\nWindow features: {self.window_features_names} \\nWindow size: {self.window_size} \\nExogenous included: {self.exog_in_} \\nExogenous names: {exog_names_in_} \\nTransformer for y: {self.transformer_y} \\nTransformer for exog: {self.transformer_exog} \\nWeight function included: {(True if self.weight_func is not None else False)} \\nDifferentiation order: {self.differentiation} \\nTraining range: {(self.training_range_.to_list() if self.is_fitted else None)} \\nTraining index type: {(str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None)} \\nTraining index frequency: {(self.index_freq_ if self.is_fitted else None)} \\nRegressor parameters: {params} \\nfit_kwargs: {self.fit_kwargs} \\nCreation date: {self.creation_date} \\nLast fit date: {self.fit_date} \\nSkforecast version: {self.skforecast_version} \\nPython version: {self.python_version} \\nForecaster id: {self.forecaster_id} \\n'\n        return info\n\n    def _repr_html_(self):\n        \"\"\"\n        HTML representation of the object.\n        The \"General Information\" section is expanded by default.\n        \"\"\"\n        params, _, _, exog_names_in_, _ = self._preprocess_repr(regressor=self.regressor, exog_names_in_=self.exog_names_in_)\n        style, unique_id = self._get_style_repr_html(self.is_fitted)\n        content = f'\\n        <div class=\"container-{unique_id}\">\\n            <h2>{type(self).__name__}</h2>\\n            <details open>\\n                <summary>General Information</summary>\\n                <ul>\\n                    <li><strong>Regressor:</strong> {type(self.regressor).__name__}</li>\\n                    <li><strong>Lags:</strong> {self.lags}</li>\\n                    <li><strong>Window features:</strong> {self.window_features_names}</li>\\n                    <li><strong>Window size:</strong> {self.window_size}</li>\\n                    <li><strong>Exogenous included:</strong> {self.exog_in_}</li>\\n                    <li><strong>Weight function included:</strong> {self.weight_func is not None}</li>\\n                    <li><strong>Differentiation order:</strong> {self.differentiation}</li>\\n                    <li><strong>Creation date:</strong> {self.creation_date}</li>\\n                    <li><strong>Last fit date:</strong> {self.fit_date}</li>\\n                    <li><strong>Skforecast version:</strong> {self.skforecast_version}</li>\\n                    <li><strong>Python version:</strong> {self.python_version}</li>\\n                    <li><strong>Forecaster id:</strong> {self.forecaster_id}</li>\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Exogenous Variables</summary>\\n                <ul>\\n                    {exog_names_in_}\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Data Transformations</summary>\\n                <ul>\\n                    <li><strong>Transformer for y:</strong> {self.transformer_y}</li>\\n                    <li><strong>Transformer for exog:</strong> {self.transformer_exog}</li>\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Training Information</summary>\\n                <ul>\\n                    <li><strong>Training range:</strong> {(self.training_range_.to_list() if self.is_fitted else 'Not fitted')}</li>\\n                    <li><strong>Training index type:</strong> {(str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted')}</li>\\n                    <li><strong>Training index frequency:</strong> {(self.index_freq_ if self.is_fitted else 'Not fitted')}</li>\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Regressor Parameters</summary>\\n                <ul>\\n                    {params}\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Fit Kwargs</summary>\\n                <ul>\\n                    {self.fit_kwargs}\\n                </ul>\\n            </details>\\n            <p>\\n                <a href=\"https://skforecast.org/{skforecast.__version__}/api/forecasterrecursive.html\">&#128712 <strong>API Reference</strong></a>\\n                &nbsp;&nbsp;\\n                <a href=\"https://skforecast.org/{skforecast.__version__}/user_guides/autoregresive-forecaster.html\">&#128462 <strong>User Guide</strong></a>\\n            </p>\\n        </div>\\n        '\n        return style + content\n\n    def _create_lags(self, y: np.ndarray, X_as_pandas: bool=False, train_index: Optional[pd.Index]=None) -> Tuple[Optional[Union[np.ndarray, pd.DataFrame]], np.ndarray]:\n        \"\"\"\n        Create the lagged values and their target variable from a time series.\n        \n        Note that the returned matrix `X_data` contains the lag 1 in the first \n        column, the lag 2 in the in the second column and so on.\n        \n        Parameters\n        ----------\n        y : numpy ndarray\n            Training time series values.\n        X_as_pandas : bool, default `False`\n            If `True`, the returned matrix `X_data` is a pandas DataFrame.\n        train_index : pandas Index, default `None`\n            Index of the training data. It is used to create the pandas DataFrame\n            `X_data` when `X_as_pandas` is `True`.\n\n        Returns\n        -------\n        X_data : numpy ndarray, pandas DataFrame, None\n            Lagged values (predictors).\n        y_data : numpy ndarray\n            Values of the time series related to each row of `X_data`.\n        \n        \"\"\"\n        X_data = None\n        if self.lags is not None:\n            n_rows = len(y) - self.window_size\n            X_data = np.full(shape=(n_rows, len(self.lags)), fill_value=np.nan, order='F', dtype=float)\n            for i, lag in enumerate(self.lags):\n                X_data[:, i] = y[self.window_size - lag:-lag]\n            if X_as_pandas:\n                X_data = pd.DataFrame(data=X_data, columns=self.lags_names, index=train_index)\n        y_data = y[self.window_size:]\n        return (X_data, y_data)\n\n    def _create_window_features(self, y: pd.Series, train_index: pd.Index, X_as_pandas: bool=False) -> Tuple[list, list]:\n        \"\"\"\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        train_index : pandas Index\n            Index of the training data. It is used to create the pandas DataFrame\n            `X_train_window_features` when `X_as_pandas` is `True`.\n        X_as_pandas : bool, default `False`\n            If `True`, the returned matrix `X_train_window_features` is a \n            pandas DataFrame.\n\n        Returns\n        -------\n        X_train_window_features : list\n            List of numpy ndarrays or pandas DataFrames with the window features.\n        X_train_window_features_names_out_ : list\n            Names of the window features.\n        \n        \"\"\"\n        len_train_index = len(train_index)\n        X_train_window_features = []\n        X_train_window_features_names_out_ = []\n        for wf in self.window_features:\n            X_train_wf = wf.transform_batch(y)\n            if not isinstance(X_train_wf, pd.DataFrame):\n                raise TypeError(f'The method `transform_batch` of {type(wf).__name__} must return a pandas DataFrame.')\n            X_train_wf = X_train_wf.iloc[-len_train_index:]\n            if not len(X_train_wf) == len_train_index:\n                raise ValueError(f'The method `transform_batch` of {type(wf).__name__} must return a DataFrame with the same number of rows as the input time series - `window_size`: {len_train_index}.')\n            if not (X_train_wf.index == train_index).all():\n                raise ValueError(f'The method `transform_batch` of {type(wf).__name__} must return a DataFrame with the same index as the input time series - `window_size`.')\n            X_train_window_features_names_out_.extend(X_train_wf.columns)\n            if not X_as_pandas:\n                X_train_wf = X_train_wf.to_numpy()\n            X_train_window_features.append(X_train_wf)\n        return (X_train_window_features, X_train_window_features_names_out_)\n\n    def create_train_X_y(self, y: pd.Series, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, pd.Series]:\n        \"\"\"\n        Create training matrices from univariate time series and exogenous\n        variables.\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors).\n        y_train : pandas Series\n            Values of the time series related to each row of `X_data`.\n        \n        \"\"\"\n        output = self._create_train_X_y(y=y, exog=exog)\n        X_train = output[0]\n        y_train = output[1]\n        return (X_train, y_train)\n\n    def _train_test_split_one_step_ahead(self, y: pd.Series, initial_train_size: int, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n        \"\"\"\n        Create matrices needed to train and test the forecaster for one-step-ahead\n        predictions.\n\n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        initial_train_size : int\n            Initial size of the training set. It is the number of observations used\n            to train the forecaster before making the first prediction.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n        \n        Returns\n        -------\n        X_train : pandas DataFrame\n            Predictor values used to train the model.\n        y_train : pandas Series\n            Target values related to each row of `X_train`.\n        X_test : pandas DataFrame\n            Predictor values used to test the model.\n        y_test : pandas Series\n            Target values related to each row of `X_test`.\n        \n        \"\"\"\n        is_fitted = self.is_fitted\n        self.is_fitted = False\n        X_train, y_train, *_ = self._create_train_X_y(y=y.iloc[:initial_train_size], exog=exog.iloc[:initial_train_size] if exog is not None else None)\n        test_init = initial_train_size - self.window_size\n        self.is_fitted = True\n        X_test, y_test, *_ = self._create_train_X_y(y=y.iloc[test_init:], exog=exog.iloc[test_init:] if exog is not None else None)\n        self.is_fitted = is_fitted\n        return (X_train, y_train, X_test, y_test)\n\n    def _create_predict_inputs(self, steps: Union[int, str, pd.Timestamp], last_window: Optional[Union[pd.Series, pd.DataFrame]]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, predict_boot: bool=False, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False, check_inputs: bool=True) -> Tuple[np.ndarray, Optional[np.ndarray], pd.Index, int]:\n        \"\"\"\n        Create the inputs needed for the first iteration of the prediction \n        process. As this is a recursive process, the last window is updated at \n        each iteration of the prediction process.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        predict_boot : bool, default `False`\n            If `True`, residuals are returned to generate bootstrapping predictions.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n\n        Returns\n        -------\n        last_window_values : numpy ndarray\n            Series values used to create the predictors needed in the first \n            iteration of the prediction (t + 1).\n        exog_values : numpy ndarray, None\n            Exogenous variable/s included as predictor/s.\n        prediction_index : pandas Index\n            Index of the predictions.\n        steps: int\n            Number of future steps predicted.\n        \n        \"\"\"\n        if last_window is None:\n            last_window = self.last_window_\n        if self.is_fitted:\n            steps = date_to_index_position(index=last_window.index, date_input=steps, date_literal='steps')\n        if check_inputs:\n            check_predict_input(forecaster_name=type(self).__name__, steps=steps, is_fitted=self.is_fitted, exog_in_=self.exog_in_, index_type_=self.index_type_, index_freq_=self.index_freq_, window_size=self.window_size, last_window=last_window, exog=exog, exog_type_in_=self.exog_type_in_, exog_names_in_=self.exog_names_in_, interval=None)\n            if predict_boot and (not use_in_sample_residuals):\n                if not use_binned_residuals and self.out_sample_residuals_ is None:\n                    raise ValueError('`forecaster.out_sample_residuals_` is `None`. Use `use_in_sample_residuals=True` or the `set_out_sample_residuals()` method before predicting.')\n                if use_binned_residuals and self.out_sample_residuals_by_bin_ is None:\n                    raise ValueError('`forecaster.out_sample_residuals_by_bin_` is `None`. Use `use_in_sample_residuals=True` or the `set_out_sample_residuals()` method before predicting.')\n        last_window = last_window.iloc[-self.window_size:].copy()\n        last_window_values, last_window_index = preprocess_last_window(last_window=last_window)\n        last_window_values = transform_numpy(array=last_window_values, transformer=self.transformer_y, fit=False, inverse_transform=False)\n        if self.differentiation is not None:\n            last_window_values = self.differentiator.fit_transform(last_window_values)\n        if exog is not None:\n            exog = input_to_frame(data=exog, input_name='exog')\n            exog = exog.loc[:, self.exog_names_in_]\n            exog = transform_dataframe(df=exog, transformer=self.transformer_exog, fit=False, inverse_transform=False)\n            check_exog_dtypes(exog=exog)\n            exog_values = exog.to_numpy()[:steps]\n        else:\n            exog_values = None\n        prediction_index = expand_index(index=last_window_index, steps=steps)\n        return (last_window_values, exog_values, prediction_index, steps)\n\n    def _recursive_predict(self, steps: int, last_window_values: np.ndarray, exog_values: Optional[np.ndarray]=None, residuals: Optional[Union[np.ndarray, dict]]=None, use_binned_residuals: bool=False) -> np.ndarray:\n        \"\"\"\n        Predict n steps ahead. It is an iterative process in which, each prediction,\n        is used as a predictor for the next step.\n        \n        Parameters\n        ----------\n        steps : int\n            Number of future steps predicted.\n        last_window_values : numpy ndarray\n            Series values used to create the predictors needed in the first \n            iteration of the prediction (t + 1).\n        exog_values : numpy ndarray, default `None`\n            Exogenous variable/s included as predictor/s.\n        residuals : numpy ndarray, dict, default `None`\n            Residuals used to generate bootstrapping predictions.\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : numpy ndarray\n            Predicted values.\n        \n        \"\"\"\n        n_lags = len(self.lags) if self.lags is not None else 0\n        n_window_features = len(self.X_train_window_features_names_out_) if self.window_features is not None else 0\n        n_exog = exog_values.shape[1] if exog_values is not None else 0\n        X = np.full(shape=n_lags + n_window_features + n_exog, fill_value=np.nan, dtype=float)\n        predictions = np.full(shape=steps, fill_value=np.nan, dtype=float)\n        last_window = np.concatenate((last_window_values, predictions))\n        for i in range(steps):\n            if self.lags is not None:\n                X[:n_lags] = last_window[-self.lags - (steps - i)]\n            if self.window_features is not None:\n                X[n_lags:n_lags + n_window_features] = np.concatenate([wf.transform(last_window[i:-(steps - i)]) for wf in self.window_features])\n            if exog_values is not None:\n                X[n_lags + n_window_features:] = exog_values[i]\n            pred = self.regressor.predict(X.reshape(1, -1)).ravel()\n            if residuals is not None:\n                if use_binned_residuals:\n                    predicted_bin = self.binner.transform(pred).item()\n                    step_residual = residuals[predicted_bin][i]\n                else:\n                    step_residual = residuals[i]\n                pred += step_residual\n            predictions[i] = pred[0]\n            last_window[-(steps - i)] = pred[0]\n        return predictions\n\n    def create_predict_X(self, steps: int, last_window: Optional[Union[pd.Series, pd.DataFrame]]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> pd.DataFrame:\n        \"\"\"\n        Create the predictors needed to predict `steps` ahead. As it is a recursive\n        process, the predictors are created at each iteration of the prediction \n        process.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n\n        Returns\n        -------\n        X_predict : pandas DataFrame\n            Pandas DataFrame with the predictors for each step. The index \n            is the same as the prediction index.\n        \n        \"\"\"\n        last_window_values, exog_values, prediction_index, steps = self._create_predict_inputs(steps=steps, last_window=last_window, exog=exog)\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', message='X does not have valid feature names', category=UserWarning)\n            predictions = self._recursive_predict(steps=steps, last_window_values=last_window_values, exog_values=exog_values)\n        X_predict = []\n        full_predictors = np.concatenate((last_window_values, predictions))\n        if self.lags is not None:\n            idx = np.arange(-steps, 0)[:, None] - self.lags\n            X_lags = full_predictors[idx + len(full_predictors)]\n            X_predict.append(X_lags)\n        if self.window_features is not None:\n            X_window_features = np.full(shape=(steps, len(self.X_train_window_features_names_out_)), fill_value=np.nan, order='C', dtype=float)\n            for i in range(steps):\n                X_window_features[i, :] = np.concatenate([wf.transform(full_predictors[i:-(steps - i)]) for wf in self.window_features])\n            X_predict.append(X_window_features)\n        if exog is not None:\n            X_predict.append(exog_values)\n        X_predict = pd.DataFrame(data=np.concatenate(X_predict, axis=1), columns=self.X_train_features_names_out_, index=prediction_index)\n        if self.transformer_y is not None or self.differentiation is not None:\n            warnings.warn('The output matrix is in the transformed scale due to the inclusion of transformations or differentiation in the Forecaster. As a result, any predictions generated using this matrix will also be in the transformed scale. Please refer to the documentation for more details: https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html', DataTransformationWarning)\n        return X_predict\n\n    def predict(self, steps: Union[int, str, pd.Timestamp], last_window: Optional[Union[pd.Series, pd.DataFrame]]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, check_inputs: bool=True) -> pd.Series:\n        \"\"\"\n        Predict n steps ahead. It is an recursive process in which, each prediction,\n        is used as a predictor for the next step.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n\n        Returns\n        -------\n        predictions : pandas Series\n            Predicted values.\n        \n        \"\"\"\n        last_window_values, exog_values, prediction_index, steps = self._create_predict_inputs(steps=steps, last_window=last_window, exog=exog, check_inputs=check_inputs)\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', message='X does not have valid feature names', category=UserWarning)\n            predictions = self._recursive_predict(steps=steps, last_window_values=last_window_values, exog_values=exog_values)\n        if self.differentiation is not None:\n            predictions = self.differentiator.inverse_transform_next_window(predictions)\n        predictions = transform_numpy(array=predictions, transformer=self.transformer_y, fit=False, inverse_transform=True)\n        predictions = pd.Series(data=predictions, index=prediction_index, name='pred')\n        return predictions\n\n    def predict_bootstrapping(self, steps: Union[int, str, pd.Timestamp], last_window: Optional[pd.Series]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False) -> pd.DataFrame:\n        \"\"\"\n        Generate multiple forecasting predictions using a bootstrapping process. \n        By sampling from a collection of past observed errors (the residuals),\n        each iteration of bootstrapping generates a different set of predictions. \n        See the Notes section for more information. \n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        boot_predictions : pandas DataFrame\n            Predictions generated by bootstrapping.\n            Shape: (steps, n_boot)\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n        Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n        \"\"\"\n        last_window_values, exog_values, prediction_index, steps = self._create_predict_inputs(steps=steps, last_window=last_window, exog=exog, predict_boot=True, use_in_sample_residuals=use_in_sample_residuals, use_binned_residuals=use_binned_residuals)\n        if use_in_sample_residuals:\n            residuals = self.in_sample_residuals_\n            residuals_by_bin = self.in_sample_residuals_by_bin_\n        else:\n            residuals = self.out_sample_residuals_\n            residuals_by_bin = self.out_sample_residuals_by_bin_\n        rng = np.random.default_rng(seed=random_state)\n        if use_binned_residuals:\n            sampled_residuals = {k: v[rng.integers(low=0, high=len(v), size=(steps, n_boot))] for k, v in residuals_by_bin.items()}\n        else:\n            sampled_residuals = residuals[rng.integers(low=0, high=len(residuals), size=(steps, n_boot))]\n        boot_columns = []\n        boot_predictions = np.full(shape=(steps, n_boot), fill_value=np.nan, order='F', dtype=float)\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', message='X does not have valid feature names', category=UserWarning)\n            for i in range(n_boot):\n                if use_binned_residuals:\n                    boot_sampled_residuals = {k: v[:, i] for k, v in sampled_residuals.items()}\n                else:\n                    boot_sampled_residuals = sampled_residuals[:, i]\n                boot_columns.append(f'pred_boot_{i}')\n                boot_predictions[:, i] = self._recursive_predict(steps=steps, last_window_values=last_window_values, exog_values=exog_values, residuals=boot_sampled_residuals, use_binned_residuals=use_binned_residuals)\n        if self.differentiation is not None:\n            boot_predictions = self.differentiator.inverse_transform_next_window(boot_predictions)\n        if self.transformer_y:\n            boot_predictions = np.apply_along_axis(func1d=transform_numpy, axis=0, arr=boot_predictions, transformer=self.transformer_y, fit=False, inverse_transform=True)\n        boot_predictions = pd.DataFrame(data=boot_predictions, index=prediction_index, columns=boot_columns)\n        return boot_predictions\n\n    def predict_interval(self, steps: Union[int, str, pd.Timestamp], last_window: Optional[pd.Series]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, interval: list=[5, 95], n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False) -> pd.DataFrame:\n        \"\"\"\n        Iterative process in which each prediction is used as a predictor\n        for the next step, and bootstrapping is used to estimate prediction\n        intervals. Both predictions and intervals are returned.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        interval : list, default `[5, 95]`\n            Confidence of the prediction interval estimated. Sequence of \n            percentiles to compute, which must be between 0 and 100 inclusive. \n            For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Values predicted by the forecaster and their estimated interval.\n\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n        check_interval(interval=interval)\n        boot_predictions = self.predict_bootstrapping(steps=steps, last_window=last_window, exog=exog, n_boot=n_boot, random_state=random_state, use_in_sample_residuals=use_in_sample_residuals, use_binned_residuals=use_binned_residuals)\n        predictions = self.predict(steps=steps, last_window=last_window, exog=exog, check_inputs=False)\n        interval = np.array(interval) / 100\n        predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n        predictions_interval.columns = ['lower_bound', 'upper_bound']\n        predictions = pd.concat((predictions, predictions_interval), axis=1)\n        return predictions\n\n    def predict_quantiles(self, steps: Union[int, str, pd.Timestamp], last_window: Optional[pd.Series]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, quantiles: list=[0.05, 0.5, 0.95], n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False) -> pd.DataFrame:\n        \"\"\"\n        Calculate the specified quantiles for each step. After generating \n        multiple forecasting predictions through a bootstrapping process, each \n        quantile is calculated for each step.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        quantiles : list, default `[0.05, 0.5, 0.95]`\n            Sequence of quantiles to compute, which must be between 0 and 1 \n            inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n            `quantiles = [0.05, 0.5, 0.95]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate quantiles.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot quantiles are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create prediction quantiles. If `False`, out of\n            sample residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Quantiles predicted by the forecaster.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n        check_interval(quantiles=quantiles)\n        boot_predictions = self.predict_bootstrapping(steps=steps, last_window=last_window, exog=exog, n_boot=n_boot, random_state=random_state, use_in_sample_residuals=use_in_sample_residuals, use_binned_residuals=use_binned_residuals)\n        predictions = boot_predictions.quantile(q=quantiles, axis=1).transpose()\n        predictions.columns = [f'q_{q}' for q in quantiles]\n        return predictions\n\n    def predict_dist(self, steps: Union[int, str, pd.Timestamp], distribution: object, last_window: Optional[pd.Series]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False) -> pd.DataFrame:\n        \"\"\"\n        Fit a given probability distribution for each step. After generating \n        multiple forecasting predictions through a bootstrapping process, each \n        step is fitted to the given distribution.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        distribution : Object\n            A distribution object from scipy.stats.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).  \n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Distribution parameters estimated for each step.\n\n        \"\"\"\n        boot_samples = self.predict_bootstrapping(steps=steps, last_window=last_window, exog=exog, n_boot=n_boot, random_state=random_state, use_in_sample_residuals=use_in_sample_residuals, use_binned_residuals=use_binned_residuals)\n        param_names = [p for p in inspect.signature(distribution._pdf).parameters if not p == 'x'] + ['loc', 'scale']\n        param_values = np.apply_along_axis(lambda x: distribution.fit(x), axis=1, arr=boot_samples)\n        predictions = pd.DataFrame(data=param_values, columns=param_names, index=boot_samples.index)\n        return predictions\n\n    def set_params(self, params: dict) -> None:\n        \"\"\"\n        Set new values to the parameters of the scikit learn model stored in the\n        forecaster.\n        \n        Parameters\n        ----------\n        params : dict\n            Parameters values.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        self.regressor = clone(self.regressor)\n        self.regressor.set_params(**params)\n\n    def set_fit_kwargs(self, fit_kwargs: dict) -> None:\n        \"\"\"\n        Set new values for the additional keyword arguments passed to the `fit` \n        method of the regressor.\n        \n        Parameters\n        ----------\n        fit_kwargs : dict\n            Dict of the form {\"argument\": new_value}.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n\n    def set_lags(self, lags: Optional[Union[int, list, np.ndarray, range]]=None) -> None:\n        \"\"\"\n        Set new value to the attribute `lags`. Attributes `lags_names`, \n        `max_lag` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        lags : int, list, numpy ndarray, range, default `None`\n            Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. \n        \n            - `int`: include lags from 1 to `lags` (included).\n            - `list`, `1d numpy ndarray` or `range`: include only lags present in \n            `lags`, all elements must be int.\n            - `None`: no lags are included as predictors. \n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        if self.window_features is None and lags is None:\n            raise ValueError('At least one of the arguments `lags` or `window_features` must be different from None. This is required to create the predictors used in training the forecaster.')\n        self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\n        self.window_size = max([ws for ws in [self.max_lag, self.max_size_window_features] if ws is not None])\n        if self.differentiation is not None:\n            self.window_size += self.differentiation\n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_window_features(self, window_features: Optional[Union[object, list]]=None) -> None:\n        \"\"\"\n        Set new value to the attribute `window_features`. Attributes \n        `max_size_window_features`, `window_features_names`, \n        `window_features_class_names` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        window_features : object, list, default `None`\n            Instance or list of instances used to create window features. Window features\n            are created from the original time series and are included as predictors.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        if window_features is None and self.lags is None:\n            raise ValueError('At least one of the arguments `lags` or `window_features` must be different from None. This is required to create the predictors used in training the forecaster.')\n        self.window_features, self.window_features_names, self.max_size_window_features = initialize_window_features(window_features)\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [type(wf).__name__ for wf in self.window_features]\n        self.window_size = max([ws for ws in [self.max_lag, self.max_size_window_features] if ws is not None])\n        if self.differentiation is not None:\n            self.window_size += self.differentiation\n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_out_sample_residuals(self, y_true: Union[pd.Series, np.ndarray], y_pred: Union[pd.Series, np.ndarray], append: bool=False, random_state: int=123) -> None:\n        \"\"\"\n        Set new values to the attribute `out_sample_residuals_`. Out of sample\n        residuals are meant to be calculated using observations that did not\n        participate in the training process. `y_true` and `y_pred` are expected\n        to be in the original scale of the time series. Residuals are calculated\n        as `y_true` - `y_pred`, after applying the necessary transformations and\n        differentiations if the forecaster includes them (`self.transformer_y`\n        and `self.differentiation`). Two internal attributes are updated:\n\n        + `out_sample_residuals_`: residuals stored in a numpy ndarray.\n        + `out_sample_residuals_by_bin_`: residuals are binned according to the\n        predicted value they are associated with and stored in a dictionary, where\n        the keys are the  intervals of the predicted values and the values are\n        the residuals associated with that range. If a bin binning is empty, it\n        is filled with a random sample of residuals from other bins. This is done\n        to ensure that all bins have at least one residual and can be used in the\n        prediction process.\n\n        A total of 10_000 residuals are stored in the attribute `out_sample_residuals_`.\n        If the number of residuals is greater than 10_000, a random sample of\n        10_000 residuals is stored. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n        \n        Parameters\n        ----------\n        y_true : pandas Series, numpy ndarray, default `None`\n            True values of the time series from which the residuals have been\n            calculated.\n        y_pred : pandas Series, numpy ndarray, default `None`\n            Predicted values of the time series.\n        append : bool, default `False`\n            If `True`, new residuals are added to the once already stored in the\n            forecaster. If after appending the new residuals, the limit of\n            `10_000 // self.binner.n_bins_` values per bin is reached, a random\n            sample of residuals is stored.\n        random_state : int, default `123`\n            Sets a seed to the random sampling for reproducible output.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n        if not self.is_fitted:\n            raise NotFittedError('This forecaster is not fitted yet. Call `fit` with appropriate arguments before using `set_out_sample_residuals()`.')\n        if not isinstance(y_true, (np.ndarray, pd.Series)):\n            raise TypeError(f'`y_true` argument must be `numpy ndarray` or `pandas Series`. Got {type(y_true)}.')\n        if not isinstance(y_pred, (np.ndarray, pd.Series)):\n            raise TypeError(f'`y_pred` argument must be `numpy ndarray` or `pandas Series`. Got {type(y_pred)}.')\n        if len(y_true) != len(y_pred):\n            raise ValueError(f'`y_true` and `y_pred` must have the same length. Got {len(y_true)} and {len(y_pred)}.')\n        if isinstance(y_true, pd.Series) and isinstance(y_pred, pd.Series):\n            if not y_true.index.equals(y_pred.index):\n                raise ValueError('`y_true` and `y_pred` must have the same index.')\n        if not isinstance(y_pred, np.ndarray):\n            y_pred = y_pred.to_numpy()\n        if not isinstance(y_true, np.ndarray):\n            y_true = y_true.to_numpy()\n        if self.transformer_y:\n            y_true = transform_numpy(array=y_true, transformer=self.transformer_y, fit=False, inverse_transform=False)\n            y_pred = transform_numpy(array=y_pred, transformer=self.transformer_y, fit=False, inverse_transform=False)\n        if self.differentiation is not None:\n            differentiator = copy(self.differentiator)\n            differentiator.set_params(window_size=None)\n            y_true = differentiator.fit_transform(y_true)[self.differentiation:]\n            y_pred = differentiator.fit_transform(y_pred)[self.differentiation:]\n        residuals = y_true - y_pred\n        data = pd.DataFrame({'prediction': y_pred, 'residuals': residuals})\n        data['bin'] = self.binner.transform(y_pred).astype(int)\n        residuals_by_bin = data.groupby('bin')['residuals'].apply(np.array).to_dict()\n        if append and self.out_sample_residuals_by_bin_ is not None:\n            for k, v in residuals_by_bin.items():\n                if k in self.out_sample_residuals_by_bin_:\n                    self.out_sample_residuals_by_bin_[k] = np.concatenate((self.out_sample_residuals_by_bin_[k], v))\n                else:\n                    self.out_sample_residuals_by_bin_[k] = v\n        else:\n            self.out_sample_residuals_by_bin_ = residuals_by_bin\n        max_samples = 10000 // self.binner.n_bins_\n        rng = np.random.default_rng(seed=random_state)\n        for k, v in self.out_sample_residuals_by_bin_.items():\n            if len(v) > max_samples:\n                sample = rng.choice(a=v, size=max_samples, replace=False)\n                self.out_sample_residuals_by_bin_[k] = sample\n        for k in self.in_sample_residuals_by_bin_.keys():\n            if k not in self.out_sample_residuals_by_bin_:\n                self.out_sample_residuals_by_bin_[k] = np.array([])\n        empty_bins = [k for k, v in self.out_sample_residuals_by_bin_.items() if len(v) == 0]\n        if empty_bins:\n            warnings.warn(f'The following bins have no out of sample residuals: {empty_bins}. No predicted values fall in the interval {[self.binner_intervals_[bin] for bin in empty_bins]}. Empty bins will be filled with a random sample of residuals.')\n            for k in empty_bins:\n                self.out_sample_residuals_by_bin_[k] = rng.choice(a=residuals, size=max_samples, replace=True)\n        self.out_sample_residuals_ = np.concatenate(list(self.out_sample_residuals_by_bin_.values()))\n\n    def get_feature_importances(self, sort_importance: bool=True) -> pd.DataFrame:\n        \"\"\"\n        Return feature importances of the regressor stored in the forecaster.\n        Only valid when regressor stores internally the feature importances in the\n        attribute `feature_importances_` or `coef_`. Otherwise, returns `None`.\n\n        Parameters\n        ----------\n        sort_importance: bool, default `True`\n            If `True`, sorts the feature importances in descending order.\n\n        Returns\n        -------\n        feature_importances : pandas DataFrame\n            Feature importances associated with each predictor.\n\n        \"\"\"\n        if not self.is_fitted:\n            raise NotFittedError('This forecaster is not fitted yet. Call `fit` with appropriate arguments before using `get_feature_importances()`.')\n        if isinstance(self.regressor, Pipeline):\n            estimator = self.regressor[-1]\n        else:\n            estimator = self.regressor\n        if hasattr(estimator, 'feature_importances_'):\n            feature_importances = estimator.feature_importances_\n        elif hasattr(estimator, 'coef_'):\n            feature_importances = estimator.coef_\n        else:\n            warnings.warn(f'Impossible to access feature importances for regressor of type {type(estimator)}. This method is only valid when the regressor stores internally the feature importances in the attribute `feature_importances_` or `coef_`.')\n            feature_importances = None\n        if feature_importances is not None:\n            feature_importances = pd.DataFrame({'feature': self.X_train_features_names_out_, 'importance': feature_importances})\n            if sort_importance:\n                feature_importances = feature_importances.sort_values(by='importance', ascending=False)\n        return feature_importances",
    "skforecast/preprocessing/preprocessing.py": "from typing import Any, Union, Optional\nfrom typing_extensions import Self\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom sklearn.base import BaseEstimator\nfrom sklearn.base import TransformerMixin\nfrom sklearn.exceptions import NotFittedError\nfrom ..exceptions import MissingValuesWarning\nfrom numba import njit\n\ndef _check_X_numpy_ndarray_1d(ensure_1d=True):\n    \"\"\"\n    This decorator checks if the argument X is a numpy ndarray with 1 dimension.\n\n    Parameters\n    ----------\n    ensure_1d : bool, default=True\n        Whether to ensure if X is a 1D numpy array.\n    \n    Returns\n    -------\n    decorator : Callable\n        A decorator function.\n\n    \"\"\"\n\n    def decorator(func):\n\n        def wrapper(self, *args, **kwargs):\n            if args:\n                X = args[0]\n            elif 'X' in kwargs:\n                X = kwargs['X']\n            else:\n                raise ValueError(\"Methods must be called with 'X' as argument.\")\n            if not isinstance(X, np.ndarray):\n                raise TypeError(f\"'X' must be a numpy ndarray. Found {type(X)}.\")\n            if ensure_1d and (not X.ndim == 1):\n                raise ValueError(f\"'X' must be a 1D array. Found {X.ndim} dimensions.\")\n            result = func(self, *args, **kwargs)\n            return result\n        return wrapper\n    return decorator\n\nclass TimeSeriesDifferentiator(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transforms a time series into a differentiated time series of a specified order\n    and provides functionality to revert the differentiation. \n    \n    When using a `direct` module Forecaster, the model in step 1 must be \n    used if you want to reverse the differentiation of the training time \n    series with the `inverse_transform_training` method.\n\n    Parameters\n    ----------\n    order : int\n        The order of differentiation to be applied.\n    window_size : int, default None\n        The window size used by the forecaster. This is required to revert the \n        differentiation for the target variable `y` or its predicted values.\n\n    Attributes\n    ----------\n    order : int\n        The order of differentiation.\n    initial_values : list\n        List with the first value of the time series before each differentiation.\n        If `order = 2`, first value correspond with the first value of the original\n        time series and the second value correspond with the first value of the\n        differentiated time series of order 1. These values are necessary to \n        revert the differentiation and reconstruct the original time series.\n    pre_train_values : list\n        List with the first training value of the time series before each differentiation.\n        For `order = 1`, the value correspond with the last value of the window used to\n        create the predictors. For order > 1, the value correspond with the first\n        value of the differentiated time series prior to the next differentiation.\n        These values are necessary to revert the differentiation and reconstruct the\n        training time series.\n    last_values : list\n        List with the last value of the time series before each differentiation, \n        used to revert differentiation on subsequent data windows. If `order = 2`, \n        first value correspond with the last value of the original time series \n        and the second value correspond with the last value of the differentiated \n        time series of order 1. This is essential for correctly transforming a \n        time series that follows immediately after the series used to fit the \n        transformer.\n\n    \"\"\"\n\n    def __init__(self, order: int=1, window_size: int=None) -> None:\n        if not isinstance(order, (int, np.integer)):\n            raise TypeError(f'Parameter `order` must be an integer greater than 0. Found {type(order)}.')\n        if order < 1:\n            raise ValueError(f'Parameter `order` must be an integer greater than 0. Found {order}.')\n        if window_size is not None:\n            if not isinstance(window_size, (int, np.integer)):\n                raise TypeError(f'Parameter `window_size` must be an integer greater than 0. Found {type(window_size)}.')\n            if window_size < 1:\n                raise ValueError(f'Parameter `window_size` must be an integer greater than 0. Found {window_size}.')\n        self.order = order\n        self.window_size = window_size\n        self.initial_values = []\n        self.pre_train_values = []\n        self.last_values = []\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Information displayed when printed.\n        \"\"\"\n        return f'TimeSeriesDifferentiator(order={self.order}, window_size={self.window_size})'\n\n    @_check_X_numpy_ndarray_1d()\n    def fit(self, X: np.ndarray, y: Any=None) -> Self:\n        \"\"\"\n        Fits the transformer. Stores the values needed to revert the \n        differentiation of different window of the time series, original \n        time series, training time series, and a time series that follows\n        immediately after the series used to fit the transformer.\n\n        Parameters\n        ----------\n        X : numpy ndarray\n            Time series to be differentiated.\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self : TimeSeriesDifferentiator\n\n        \"\"\"\n        self.initial_values = []\n        self.pre_train_values = []\n        self.last_values = []\n        for i in range(self.order):\n            if i == 0:\n                self.initial_values.append(X[0])\n                if self.window_size is not None:\n                    self.pre_train_values.append(X[self.window_size - self.order])\n                self.last_values.append(X[-1])\n                X_diff = np.diff(X, n=1)\n            else:\n                self.initial_values.append(X_diff[0])\n                if self.window_size is not None:\n                    self.pre_train_values.append(X_diff[self.window_size - self.order])\n                self.last_values.append(X_diff[-1])\n                X_diff = np.diff(X_diff, n=1)\n        return self\n\n    @_check_X_numpy_ndarray_1d()\n    def transform(self, X: np.ndarray, y: Any=None) -> np.ndarray:\n        \"\"\"\n        Transforms a time series into a differentiated time series of order n.\n\n        Parameters\n        ----------\n        X : numpy ndarray\n            Time series to be differentiated.\n        y : Ignored\n            Not used, present here for API consistency by convention.\n        \n        Returns\n        -------\n        X_diff : numpy ndarray\n            Differentiated time series. The length of the array is the same as\n            the original time series but the first n `order` values are nan.\n\n        \"\"\"\n        X_diff = np.diff(X, n=self.order)\n        X_diff = np.append(np.full(shape=self.order, fill_value=np.nan), X_diff)\n        return X_diff\n\n    @_check_X_numpy_ndarray_1d()\n    def inverse_transform(self, X: np.ndarray, y: Any=None) -> np.ndarray:\n        \"\"\"\n        Reverts the differentiation. To do so, the input array is assumed to be\n        the same time series used to fit the transformer but differentiated.\n\n        Parameters\n        ----------\n        X : numpy ndarray\n            Differentiated time series.\n        y : Ignored\n            Not used, present here for API consistency by convention.\n        \n        Returns\n        -------\n        X_diff : numpy ndarray\n            Reverted differentiated time series.\n        \n        \"\"\"\n        X = X[np.argmax(~np.isnan(X)):]\n        for i in range(self.order):\n            if i == 0:\n                X_undiff = np.insert(X, 0, self.initial_values[-1])\n                X_undiff = np.cumsum(X_undiff, dtype=float)\n            else:\n                X_undiff = np.insert(X_undiff, 0, self.initial_values[-(i + 1)])\n                X_undiff = np.cumsum(X_undiff, dtype=float)\n        return X_undiff\n\n    @_check_X_numpy_ndarray_1d()\n    def inverse_transform_training(self, X: np.ndarray, y: Any=None) -> np.ndarray:\n        \"\"\"\n        Reverts the differentiation. To do so, the input array is assumed to be\n        the differentiated training time series generated with the original \n        time series used to fit the transformer.\n\n        When using a `direct` module Forecaster, the model in step 1 must be \n        used if you want to reverse the differentiation of the training time \n        series with the `inverse_transform_training` method.\n\n        Parameters\n        ----------\n        X : numpy ndarray\n            Differentiated time series.\n        y : Ignored\n            Not used, present here for API consistency by convention.\n        \n        Returns\n        -------\n        X_diff : numpy ndarray\n            Reverted differentiated time series.\n        \n        \"\"\"\n        if not self.pre_train_values:\n            raise ValueError('The `window_size` parameter must be set before fitting the transformer to revert the differentiation of the training time series.')\n        X = X[np.argmax(~np.isnan(X)):]\n        for i in range(self.order):\n            if i == 0:\n                X_undiff = np.insert(X, 0, self.pre_train_values[-1])\n                X_undiff = np.cumsum(X_undiff, dtype=float)\n            else:\n                X_undiff = np.insert(X_undiff, 0, self.pre_train_values[-(i + 1)])\n                X_undiff = np.cumsum(X_undiff, dtype=float)\n        X_undiff = X_undiff[self.order:]\n        return X_undiff\n\n    @_check_X_numpy_ndarray_1d(ensure_1d=False)\n    def inverse_transform_next_window(self, X: np.ndarray, y: Any=None) -> np.ndarray:\n        \"\"\"\n        Reverts the differentiation. The input array `X` is assumed to be a \n        differentiated time series of order n that starts right after the\n        the time series used to fit the transformer.\n\n        Parameters\n        ----------\n        X : numpy ndarray\n            Differentiated time series. It is assumed o start right after\n            the time series used to fit the transformer.\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        X_undiff : numpy ndarray\n            Reverted differentiated time series.\n        \n        \"\"\"\n        array_ndim = X.ndim\n        if array_ndim == 1:\n            X = X[:, np.newaxis]\n        X = X[~np.isnan(X).any(axis=1)]\n        for i in range(self.order):\n            if i == 0:\n                X_undiff = np.cumsum(X, axis=0, dtype=float) + self.last_values[-1]\n            else:\n                X_undiff = np.cumsum(X_undiff, axis=0, dtype=float) + self.last_values[-(i + 1)]\n        if array_ndim == 1:\n            X_undiff = X_undiff.ravel()\n        return X_undiff\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of the TimeSeriesDifferentiator.\n        \n        Parameters\n        ----------\n        params : dict\n            A dictionary of the parameters to set.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        for param, value in params.items():\n            setattr(self, param, value)\n\ndef series_long_to_dict(data: pd.DataFrame, series_id: str, index: str, values: str, freq: str, suppress_warnings: bool=False) -> dict:\n    \"\"\"\n    Convert long format series to dictionary of pandas Series with frequency.\n    Input data must be a pandas DataFrame with columns for the series identifier,\n    time index, and values. The function will group the data by the series\n    identifier and convert the time index to a datetime index with the given\n    frequency.\n\n    Parameters\n    ----------\n    data: pandas DataFrame\n        Long format series.\n    series_id: str\n        Column name with the series identifier.\n    index: str\n        Column name with the time index.\n    values: str\n        Column name with the values.\n    freq: str\n        Frequency of the series.\n    suppress_warnings: bool, default `False`\n        If True, suppress warnings when a series is incomplete after setting the\n        frequency.\n\n    Returns\n    -------\n    series_dict: dict\n        Dictionary with the series.\n\n    \"\"\"\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError('`data` must be a pandas DataFrame.')\n    for col in [series_id, index, values]:\n        if col not in data.columns:\n            raise ValueError(f\"Column '{col}' not found in `data`.\")\n    original_sizes = data.groupby(series_id).size()\n    series_dict = {}\n    for k, v in data.groupby(series_id):\n        series_dict[k] = v.set_index(index)[values].asfreq(freq).rename(k)\n        series_dict[k].index.name = None\n        if not suppress_warnings and len(series_dict[k]) != original_sizes[k]:\n            warnings.warn(f\"Series '{k}' is incomplete. NaNs have been introduced after setting the frequency.\", MissingValuesWarning)\n    return series_dict\n\ndef exog_long_to_dict(data: pd.DataFrame, series_id: str, index: str, freq: str, dropna: bool=False, suppress_warnings: bool=False) -> dict:\n    \"\"\"\n    Convert long format exogenous variables to dictionary. Input data must be a\n    pandas DataFrame with columns for the series identifier, time index, and\n    exogenous variables. The function will group the data by the series identifier\n    and convert the time index to a datetime index with the given frequency.\n\n    Parameters\n    ----------\n    data: pandas DataFrame\n        Long format exogenous variables.\n    series_id: str\n        Column name with the series identifier.\n    index: str\n        Column name with the time index.\n    freq: str\n        Frequency of the series.\n    dropna: bool, default False\n        If True, drop columns with all values as NaN. This is useful when\n        there are series without some exogenous variables.\n    suppress_warnings: bool, default False\n        If True, suppress warnings when exog is incomplete after setting the\n        frequency.\n        \n    Returns\n    -------\n    exog_dict: dict\n        Dictionary with the exogenous variables.\n\n    \"\"\"\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError('`data` must be a pandas DataFrame.')\n    for col in [series_id, index]:\n        if col not in data.columns:\n            raise ValueError(f\"Column '{col}' not found in `data`.\")\n    original_sizes = data.groupby(series_id).size()\n    exog_dict = dict(tuple(data.groupby(series_id)))\n    exog_dict = {k: v.set_index(index).asfreq(freq).drop(columns=series_id) for k, v in exog_dict.items()}\n    for k in exog_dict.keys():\n        exog_dict[k].index.name = None\n    if dropna:\n        exog_dict = {k: v.dropna(how='all', axis=1) for k, v in exog_dict.items()}\n    elif not suppress_warnings:\n        for k, v in exog_dict.items():\n            if len(v) != original_sizes[k]:\n                warnings.warn(f\"Exogenous variables for series '{k}' are incomplete. NaNs have been introduced after setting the frequency.\", MissingValuesWarning)\n    return exog_dict\n\ndef create_datetime_features(X: Union[pd.Series, pd.DataFrame], features: Optional[list]=None, encoding: str='cyclical', max_values: Optional[dict]=None) -> pd.DataFrame:\n    \"\"\"\n    Extract datetime features from the DateTime index of a pandas DataFrame or Series.\n\n    Parameters\n    ----------\n    X : pandas Series, pandas DataFrame\n        Input DataFrame or Series with a datetime index.\n    features : list, default `None`\n        List of calendar features (strings) to extract from the index. When `None`,\n        the following features are extracted: 'year', 'month', 'week', 'day_of_week',\n        'day_of_month', 'day_of_year', 'weekend', 'hour', 'minute', 'second'.\n    encoding : str, default `'cyclical'`\n        Encoding method for the extracted features. Options are None, 'cyclical' or\n        'onehot'.\n    max_values : dict, default `None`\n        Dictionary of maximum values for the cyclical encoding of calendar features.\n        When `None`, the following values are used: {'month': 12, 'week': 52, \n        'day_of_week': 7, 'day_of_month': 31, 'day_of_year': 365, 'hour': 24, \n        'minute': 60, 'second': 60}.\n\n    Returns\n    -------\n    X_new : pandas DataFrame\n        DataFrame with the extracted (and optionally encoded) datetime features.\n    \n    \"\"\"\n    if not isinstance(X, (pd.DataFrame, pd.Series)):\n        raise TypeError('Input `X` must be a pandas Series or DataFrame')\n    if not isinstance(X.index, pd.DatetimeIndex):\n        raise TypeError('Input `X` must have a pandas DatetimeIndex')\n    if encoding not in ['cyclical', 'onehot', None]:\n        raise ValueError(\"Encoding must be one of 'cyclical', 'onehot' or None\")\n    default_features = ['year', 'month', 'week', 'day_of_week', 'day_of_month', 'day_of_year', 'weekend', 'hour', 'minute', 'second']\n    features = features or default_features\n    default_max_values = {'month': 12, 'week': 52, 'day_of_week': 7, 'day_of_month': 31, 'day_of_year': 365, 'hour': 24, 'minute': 60, 'second': 60}\n    max_values = max_values or default_max_values\n    X_new = pd.DataFrame(index=X.index)\n    datetime_attrs = {'year': 'year', 'month': 'month', 'week': lambda idx: idx.isocalendar().week, 'day_of_week': 'dayofweek', 'day_of_year': 'dayofyear', 'day_of_month': 'day', 'weekend': lambda idx: (idx.weekday >= 5).astype(int), 'hour': 'hour', 'minute': 'minute', 'second': 'second'}\n    not_supported_features = set(features) - set(datetime_attrs.keys())\n    if not_supported_features:\n        raise ValueError(f'Features {not_supported_features} are not supported. Supported features are {list(datetime_attrs.keys())}.')\n    for feature in features:\n        attr = datetime_attrs[feature]\n        X_new[feature] = attr(X.index) if callable(attr) else getattr(X.index, attr).astype(int)\n    if encoding == 'cyclical':\n        cols_to_drop = []\n        for feature, max_val in max_values.items():\n            if feature in X_new.columns:\n                X_new[f'{feature}_sin'] = np.sin(2 * np.pi * X_new[feature] / max_val)\n                X_new[f'{feature}_cos'] = np.cos(2 * np.pi * X_new[feature] / max_val)\n                cols_to_drop.append(feature)\n        X_new = X_new.drop(columns=cols_to_drop)\n    elif encoding == 'onehot':\n        X_new = pd.get_dummies(X_new, columns=features, drop_first=False, sparse=False, dtype=int)\n    return X_new\n\nclass DateTimeFeatureTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    A transformer for extracting datetime features from the DateTime index of a\n    pandas DataFrame or Series. It can also apply encoding to the extracted features.\n\n    Parameters\n    ----------\n    features : list, default `None`\n        List of calendar features (strings) to extract from the index. When `None`,\n        the following features are extracted: 'year', 'month', 'week', 'day_of_week',\n        'day_of_month', 'day_of_year', 'weekend', 'hour', 'minute', 'second'.\n    encoding : str, default `'cyclical'`\n        Encoding method for the extracted features. Options are None, 'cyclical' or\n        'onehot'.\n    max_values : dict, default `None`\n        Dictionary of maximum values for the cyclical encoding of calendar features.\n        When `None`, the following values are used: {'month': 12, 'week': 52, \n        'day_of_week': 7, 'day_of_month': 31, 'day_of_year': 365, 'hour': 24, \n        'minute': 60, 'second': 60}.\n    \n    Attributes\n    ----------\n    features : list\n        List of calendar features to extract from the index.\n    encoding : str\n        Encoding method for the extracted features.\n    max_values : dict\n        Dictionary of maximum values for the cyclical encoding of calendar features.\n    \n    \"\"\"\n\n    def __init__(self, features: Optional[list]=None, encoding: str='cyclical', max_values: Optional[dict]=None) -> None:\n        if encoding not in ['cyclical', 'onehot', None]:\n            raise ValueError(\"Encoding must be one of 'cyclical', 'onehot' or None\")\n        self.features = features if features is not None else ['year', 'month', 'week', 'day_of_week', 'day_of_month', 'day_of_year', 'weekend', 'hour', 'minute', 'second']\n        self.encoding = encoding\n        self.max_values = max_values if max_values is not None else {'month': 12, 'week': 52, 'day_of_week': 7, 'day_of_month': 31, 'day_of_year': 365, 'hour': 24, 'minute': 60, 'second': 60}\n\n    def fit(self, X, y=None):\n        \"\"\"\n        A no-op method to satisfy the scikit-learn API.\n        \"\"\"\n        return self\n\n    def transform(self, X: Union[pd.Series, pd.DataFrame]) -> pd.DataFrame:\n        \"\"\"\n        Create datetime features from the DateTime index of a pandas DataFrame or Series.\n\n        Parameters\n        ----------\n        X : pandas Series, pandas DataFrame\n            Input DataFrame or Series with a datetime index.\n        \n        Returns\n        -------\n        X_new : pandas DataFrame\n            DataFrame with the extracted (and optionally encoded) datetime features.\n\n        \"\"\"\n        X_new = create_datetime_features(X=X, encoding=self.encoding, features=self.features, max_values=self.max_values)\n        return X_new\n\n@njit\ndef _np_mean_jit(x):\n    \"\"\"\n    NumPy mean function implemented with Numba JIT.\n    \"\"\"\n    return np.mean(x)\n\n@njit\ndef _np_std_jit(x, ddof=1):\n    \"\"\"\n    Standard deviation function implemented with Numba JIT.\n    If the array has only one element, the function returns 0.\n    \"\"\"\n    if len(x) == 1:\n        return 0.0\n    a_a, b_b = (0, 0)\n    for i in x:\n        a_a = a_a + i\n        b_b = b_b + i * i\n    var = b_b / len(x) - (a_a / len(x)) ** 2\n    var = var * (len(x) / (len(x) - ddof))\n    std = np.sqrt(var)\n    return std\n\n@njit\ndef _np_min_jit(x):\n    \"\"\"\n    NumPy min function implemented with Numba JIT.\n    \"\"\"\n    return np.min(x)\n\n@njit\ndef _np_max_jit(x):\n    \"\"\"\n    NumPy max function implemented with Numba JIT.\n    \"\"\"\n    return np.max(x)\n\n@njit\ndef _np_sum_jit(x):\n    \"\"\"\n    NumPy sum function implemented with Numba JIT.\n    \"\"\"\n    return np.sum(x)\n\n@njit\ndef _np_median_jit(x):\n    \"\"\"\n    NumPy median function implemented with Numba JIT.\n    \"\"\"\n    return np.median(x)\n\n@njit\ndef _np_min_max_ratio_jit(x):\n    \"\"\"\n    NumPy min-max ratio function implemented with Numba JIT.\n    \"\"\"\n    return np.min(x) / np.max(x)\n\n@njit\ndef _np_cv_jit(x):\n    \"\"\"\n    Coefficient of variation function implemented with Numba JIT.\n    If the array has only one element, the function returns 0.\n    \"\"\"\n    if len(x) == 1:\n        return 0.0\n    a_a, b_b = (0, 0)\n    for i in x:\n        a_a = a_a + i\n        b_b = b_b + i * i\n    var = b_b / len(x) - (a_a / len(x)) ** 2\n    var = var * (len(x) / (len(x) - 1))\n    std = np.sqrt(var)\n    return std / np.mean(x)\n\nclass RollingFeatures:\n    \"\"\"\n    This class computes rolling features. To avoid data leakage, the last point \n    in the window is excluded from calculations, ('closed': 'left' and \n    'center': False).\n\n    Parameters\n    ----------\n    stats : str, list\n        Statistics to compute over the rolling window. Can be a `string` or a `list`,\n        and can have repeats. Available statistics are: 'mean', 'std', 'min', 'max',\n        'sum', 'median', 'ratio_min_max', 'coef_variation'.\n    window_sizes : int, list\n        Size of the rolling window for each statistic. If an `int`, all stats share \n        the same window size. If a `list`, it should have the same length as stats.\n    min_periods : int, list, default `None`\n        Minimum number of observations in window required to have a value. \n        Same as the `min_periods` argument of pandas rolling. If `None`, \n        defaults to `window_sizes`.\n    features_names : list, default `None`\n        Names of the output features. If `None`, default names will be used in the \n        format 'roll_stat_window_size', for example 'roll_mean_7'.\n    fillna : str, float, default `None`\n        Fill missing values in `transform_batch` method. Available \n        methods are: 'mean', 'median', 'ffill', 'bfill', or a float value.\n    \n    Attributes\n    ----------\n    stats : list\n        Statistics to compute over the rolling window.\n    n_stats : int\n        Number of statistics to compute.\n    window_sizes : list\n        Size of the rolling window for each statistic.\n    max_window_size : int\n        Maximum window size.\n    min_periods : list\n        Minimum number of observations in window required to have a value.\n    features_names : list\n        Names of the output features.\n    fillna : str, float\n        Method to fill missing values in `transform_batch` method.\n    unique_rolling_windows : dict\n        Dictionary containing unique rolling window parameters and the corresponding\n        statistics.\n        \n    \"\"\"\n\n    def __init__(self, stats: Union[str, list], window_sizes: Union[int, list], min_periods: Optional[Union[int, list]]=None, features_names: Optional[list]=None, fillna: Optional[Union[str, float]]=None) -> None:\n        self._validate_params(stats, window_sizes, min_periods, features_names, fillna)\n        if isinstance(stats, str):\n            stats = [stats]\n        self.stats = stats\n        self.n_stats = len(stats)\n        if isinstance(window_sizes, int):\n            window_sizes = [window_sizes] * self.n_stats\n        self.window_sizes = window_sizes\n        self.max_window_size = max(window_sizes)\n        if min_periods is None:\n            min_periods = self.window_sizes\n        elif isinstance(min_periods, int):\n            min_periods = [min_periods] * self.n_stats\n        self.min_periods = min_periods\n        if features_names is None:\n            features_names = [f'roll_{stat}_{window_size}' for stat, window_size in zip(self.stats, self.window_sizes)]\n        self.features_names = features_names\n        self.fillna = fillna\n        window_params_list = []\n        for i in range(len(self.stats)):\n            window_params = (self.window_sizes[i], self.min_periods[i])\n            window_params_list.append(window_params)\n        unique_rolling_windows = {}\n        for i, params in enumerate(window_params_list):\n            key = f'{params[0]}_{params[1]}'\n            if key not in unique_rolling_windows:\n                unique_rolling_windows[key] = {'params': {'window': params[0], 'min_periods': params[1], 'center': False, 'closed': 'left'}, 'stats_idx': [], 'stats_names': [], 'rolling_obj': None}\n            unique_rolling_windows[key]['stats_idx'].append(i)\n            unique_rolling_windows[key]['stats_names'].append(self.features_names[i])\n        self.unique_rolling_windows = unique_rolling_windows\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Information displayed when printed.\n        \"\"\"\n        return f'RollingFeatures(\\n    stats           = {self.stats},\\n    window_sizes    = {self.window_sizes},\\n    Max window size = {self.max_window_size},\\n    min_periods     = {self.min_periods},\\n    features_names  = {self.features_names},\\n    fillna          = {self.fillna}\\n)'\n\n    def _validate_params(self, stats, window_sizes, min_periods: Optional[Union[int, list]]=None, features_names: Optional[Union[str, list]]=None, fillna: Optional[Union[str, float]]=None) -> None:\n        \"\"\"\n        Validate the parameters of the RollingFeatures class.\n\n        Parameters\n        ----------\n        stats : str, list\n            Statistics to compute over the rolling window. Can be a `string` or a `list`,\n            and can have repeats. Available statistics are: 'mean', 'std', 'min', 'max',\n            'sum', 'median', 'ratio_min_max', 'coef_variation'.\n        window_sizes : int, list\n            Size of the rolling window for each statistic. If an `int`, all stats share \n            the same window size. If a `list`, it should have the same length as stats.\n        min_periods : int, list, default `None`\n            Minimum number of observations in window required to have a value. \n            Same as the `min_periods` argument of pandas rolling. If `None`, \n            defaults to `window_sizes`.\n        features_names : list, default `None`\n            Names of the output features. If `None`, default names will be used in the \n            format 'roll_stat_window_size', for example 'roll_mean_7'.\n        fillna : str, float, default `None`\n            Fill missing values in `transform_batch` method. Available \n            methods are: 'mean', 'median', 'ffill', 'bfill', or a float value.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n        if not isinstance(stats, (str, list)):\n            raise TypeError(f'`stats` must be a string or a list of strings. Got {type(stats)}.')\n        if isinstance(stats, str):\n            stats = [stats]\n        allowed_stats = ['mean', 'std', 'min', 'max', 'sum', 'median', 'ratio_min_max', 'coef_variation']\n        for stat in set(stats):\n            if stat not in allowed_stats:\n                raise ValueError(f\"Statistic '{stat}' is not allowed. Allowed stats are: {allowed_stats}.\")\n        n_stats = len(stats)\n        if not isinstance(window_sizes, (int, list)):\n            raise TypeError(f'`window_sizes` must be an int or a list of ints. Got {type(window_sizes)}.')\n        if isinstance(window_sizes, list):\n            n_window_sizes = len(window_sizes)\n            if n_window_sizes != n_stats:\n                raise ValueError(f'Length of `window_sizes` list ({n_window_sizes}) must match length of `stats` list ({n_stats}).')\n        if isinstance(window_sizes, int):\n            window_sizes = [window_sizes] * n_stats\n        if len(set(zip(stats, window_sizes))) != n_stats:\n            raise ValueError(f'Duplicate (stat, window_size) pairs are not allowed.\\n    `stats`       : {stats}\\n    `window_sizes : {window_sizes}')\n        if not isinstance(min_periods, (int, list, type(None))):\n            raise TypeError(f'`min_periods` must be an int, list of ints, or None. Got {type(min_periods)}.')\n        if min_periods is not None:\n            if isinstance(min_periods, int):\n                min_periods = [min_periods] * n_stats\n            elif isinstance(min_periods, list):\n                n_min_periods = len(min_periods)\n                if n_min_periods != n_stats:\n                    raise ValueError(f'Length of `min_periods` list ({n_min_periods}) must match length of `stats` list ({n_stats}).')\n            for i, min_period in enumerate(min_periods):\n                if min_period > window_sizes[i]:\n                    raise ValueError('Each `min_period` must be less than or equal to its corresponding `window_size`.')\n        if not isinstance(features_names, (list, type(None))):\n            raise TypeError(f'`features_names` must be a list of strings or None. Got {type(features_names)}.')\n        if isinstance(features_names, list):\n            n_features_names = len(features_names)\n            if n_features_names != n_stats:\n                raise ValueError(f'Length of `features_names` list ({n_features_names}) must match length of `stats` list ({n_stats}).')\n        if fillna is not None:\n            if not isinstance(fillna, (int, float, str)):\n                raise TypeError(f'`fillna` must be a float, string, or None. Got {type(fillna)}.')\n            if isinstance(fillna, str):\n                allowed_fill_strategy = ['mean', 'median', 'ffill', 'bfill']\n                if fillna not in allowed_fill_strategy:\n                    raise ValueError(f\"'{fillna}' is not allowed. Allowed `fillna` values are: {allowed_fill_strategy} or a float value.\")\n\n    def _apply_stat_pandas(self, rolling_obj: pd.core.window.rolling.Rolling, stat: str) -> pd.Series:\n        \"\"\"\n        Apply the specified statistic to a pandas rolling object.\n\n        Parameters\n        ----------\n        rolling_obj : pandas Rolling\n            Rolling object to apply the statistic.\n        stat : str\n            Statistic to compute.\n        \n        Returns\n        -------\n        stat_series : pandas Series\n            Series with the computed statistic.\n        \n        \"\"\"\n        if stat == 'mean':\n            return rolling_obj.mean()\n        elif stat == 'std':\n            return rolling_obj.std()\n        elif stat == 'min':\n            return rolling_obj.min()\n        elif stat == 'max':\n            return rolling_obj.max()\n        elif stat == 'sum':\n            return rolling_obj.sum()\n        elif stat == 'median':\n            return rolling_obj.median()\n        elif stat == 'ratio_min_max':\n            return rolling_obj.min() / rolling_obj.max()\n        elif stat == 'coef_variation':\n            return rolling_obj.std() / rolling_obj.mean()\n        else:\n            raise ValueError(f\"Statistic '{stat}' is not implemented.\")\n\n    def transform_batch(self, X: pd.Series) -> pd.DataFrame:\n        \"\"\"\n        Transform an entire pandas Series using rolling windows and compute the \n        specified statistics.\n\n        Parameters\n        ----------\n        X : pandas Series\n            The input data series to transform.\n\n        Returns\n        -------\n        rolling_features : pandas DataFrame\n            A DataFrame containing the rolling features.\n        \n        \"\"\"\n        for k in self.unique_rolling_windows.keys():\n            rolling_obj = X.rolling(**self.unique_rolling_windows[k]['params'])\n            self.unique_rolling_windows[k]['rolling_obj'] = rolling_obj\n        rolling_features = []\n        for i, stat in enumerate(self.stats):\n            window_size = self.window_sizes[i]\n            min_periods = self.min_periods[i]\n            key = f'{window_size}_{min_periods}'\n            rolling_obj = self.unique_rolling_windows[key]['rolling_obj']\n            stat_series = self._apply_stat_pandas(rolling_obj=rolling_obj, stat=stat)\n            rolling_features.append(stat_series)\n        rolling_features = pd.concat(rolling_features, axis=1)\n        rolling_features.columns = self.features_names\n        rolling_features = rolling_features.iloc[self.max_window_size:]\n        if self.fillna is not None:\n            if self.fillna == 'mean':\n                rolling_features = rolling_features.fillna(rolling_features.mean())\n            elif self.fillna == 'median':\n                rolling_features = rolling_features.fillna(rolling_features.median())\n            elif self.fillna == 'ffill':\n                rolling_features = rolling_features.ffill()\n            elif self.fillna == 'bfill':\n                rolling_features = rolling_features.bfill()\n            else:\n                rolling_features = rolling_features.fillna(self.fillna)\n        return rolling_features\n\n    def _apply_stat_numpy_jit(self, X_window: np.ndarray, stat: str) -> float:\n        \"\"\"\n        Apply the specified statistic to a numpy array using Numba JIT.\n\n        Parameters\n        ----------\n        X_window : numpy array\n            Array with the rolling window.\n        stat : str\n            Statistic to compute.\n\n        Returns\n        -------\n        stat_value : float\n            Value of the computed statistic.\n        \n        \"\"\"\n        if stat == 'mean':\n            return _np_mean_jit(X_window)\n        elif stat == 'std':\n            return _np_std_jit(X_window)\n        elif stat == 'min':\n            return _np_min_jit(X_window)\n        elif stat == 'max':\n            return _np_max_jit(X_window)\n        elif stat == 'sum':\n            return _np_sum_jit(X_window)\n        elif stat == 'median':\n            return _np_median_jit(X_window)\n        elif stat == 'ratio_min_max':\n            return _np_min_max_ratio_jit(X_window)\n        elif stat == 'coef_variation':\n            return _np_cv_jit(X_window)\n        else:\n            raise ValueError(f\"Statistic '{stat}' is not implemented.\")\n\n    def transform(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Transform a numpy array using rolling windows and compute the \n        specified statistics. The returned array will have the shape \n        (X.shape[1] if exists, n_stats). For example, if X is a flat\n        array, the output will have shape (n_stats,). If X is a 2D array,\n        the output will have shape (X.shape[1], n_stats).\n\n        Parameters\n        ----------\n        X : numpy ndarray\n            The input data array to transform.\n\n        Returns\n        -------\n        rolling_features : numpy ndarray\n            An array containing the computed statistics.\n        \n        \"\"\"\n        array_ndim = X.ndim\n        if array_ndim == 1:\n            X = X[:, np.newaxis]\n        rolling_features = np.full(shape=(X.shape[1], self.n_stats), fill_value=np.nan, dtype=float)\n        for i in range(X.shape[1]):\n            for j, stat in enumerate(self.stats):\n                X_window = X[-self.window_sizes[j]:, i]\n                X_window = X_window[~np.isnan(X_window)]\n                if len(X_window) > 0:\n                    rolling_features[i, j] = self._apply_stat_numpy_jit(X_window, stat)\n                else:\n                    rolling_features[i, j] = np.nan\n        if array_ndim == 1:\n            rolling_features = rolling_features.ravel()\n        return rolling_features\n\nclass QuantileBinner:\n    \"\"\"\n    QuantileBinner class to bin data into quantile-based bins using `numpy.percentile`.\n    This class is similar to `KBinsDiscretizer` but faster for binning data into\n    quantile-based bins. Bin  intervals are defined following the convention:\n    bins[i-1] <= x < bins[i]. See more information in `numpy.percentile` and\n    `numpy.digitize`.\n    \n    Parameters\n    ----------\n    n_bins : int\n        The number of quantile-based bins to create.\n    method : str, default='linear'\n        The method used to compute the quantiles. This parameter is passed to \n        `numpy.percentile`. Default is 'linear'. Valid values are \"inverse_cdf\",\n        \"averaged_inverse_cdf\", \"closest_observation\", \"interpolated_inverse_cdf\",\n        \"hazen\", \"weibull\", \"linear\", \"median_unbiased\", \"normal_unbiased\".\n    subsample : int, default=200000\n        The number of samples to use for computing quantiles. If the dataset \n        has more samples than `subsample`, a random subset will be used.\n    random_state : int, default=789654\n        The random seed to use for generating a random subset of the data.\n    dtype : data type, default=numpy.float64\n        The data type to use for the bin indices. Default is `numpy.float64`.\n    \n    Attributes\n    ----------\n    n_bins : int\n        The number of quantile-based bins to create.\n    method : str, default='linear'\n        The method used to compute the quantiles. This parameter is passed to \n        `numpy.percentile`. Default is 'linear'. Valid values are 'linear',\n        'lower', 'higher', 'midpoint', 'nearest'.\n    subsample : int, default=200000\n        The number of samples to use for computing quantiles. If the dataset \n        has more samples than `subsample`, a random subset will be used.\n    random_state : int, default=789654\n        The random seed to use for generating a random subset of the data.\n    dtype : data type, default=numpy.float64\n        The data type to use for the bin indices. Default is `numpy.float64`.\n    n_bins_ : int\n        The number of bins learned during fitting.\n    bin_edges_ : numpy ndarray\n        The edges of the bins learned during fitting.\n    \n    \"\"\"\n\n    def _validate_params(self, n_bins: int, method: str, subsample: int, dtype: type, random_state: int):\n        \"\"\"\n        Validate the parameters passed to the class initializer.\n        \"\"\"\n        if not isinstance(n_bins, int) or n_bins < 2:\n            raise ValueError(f'`n_bins` must be an int greater than 1. Got {n_bins}.')\n        valid_methods = ['inverse_cdf', 'averaged_inverse_cdf', 'closest_observation', 'interpolated_inverse_cdf', 'hazen', 'weibull', 'linear', 'median_unbiased', 'normal_unbiased']\n        if method not in valid_methods:\n            raise ValueError(f'`method` must be one of {valid_methods}. Got {method}.')\n        if not isinstance(subsample, int) or subsample < 1:\n            raise ValueError(f'`subsample` must be an integer greater than or equal to 1. Got {subsample}.')\n        if not isinstance(random_state, int) or random_state < 0:\n            raise ValueError(f'`random_state` must be an integer greater than or equal to 0. Got {random_state}.')\n        if not isinstance(dtype, type):\n            raise ValueError(f'`dtype` must be a valid numpy dtype. Got {dtype}.')\n\n    def fit(self, X: np.ndarray):\n        \"\"\"\n        Learn the bin edges based on quantiles from the training data.\n        \n        Parameters\n        ----------\n        X : numpy ndarray\n            The training data used to compute the quantiles.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        if X.size == 0:\n            raise ValueError('Input data `X` cannot be empty.')\n        if len(X) > self.subsample:\n            rng = np.random.default_rng(self.random_state)\n            X = X[rng.integers(0, len(X), self.subsample)]\n        self.bin_edges_ = np.percentile(a=X, q=np.linspace(0, 100, self.n_bins + 1), method=self.method)\n        self.n_bins_ = len(self.bin_edges_) - 1\n        self.intervals_ = {float(i): (float(self.bin_edges_[i]), float(self.bin_edges_[i + 1])) for i in range(self.n_bins_)}\n\n    def transform(self, X: np.ndarray):\n        \"\"\"\n        Assign new data to the learned bins.\n        \n        Parameters\n        ----------\n        X : numpy ndarray\n            The data to assign to the bins.\n        \n        Returns\n        -------\n        bin_indices : numpy ndarray \n            The indices of the bins each value belongs to.\n            Values less than the smallest bin edge are assigned to the first bin,\n            and values greater than the largest bin edge are assigned to the last bin.\n       \n        \"\"\"\n        if self.bin_edges_ is None:\n            raise NotFittedError(\"The model has not been fitted yet. Call 'fit' with training data first.\")\n        bin_indices = np.digitize(X, bins=self.bin_edges_, right=False)\n        bin_indices = np.clip(bin_indices, 1, self.n_bins_).astype(self.dtype) - 1\n        return bin_indices\n\n    def get_params(self):\n        \"\"\"\n        Get the parameters of the quantile binner.\n        \n        Parameters\n        ----------\n        self\n        \n        Returns\n        -------\n        params : dict\n            A dictionary of the parameters of the quantile binner.\n        \n        \"\"\"\n        return {'n_bins': self.n_bins, 'method': self.method, 'subsample': self.subsample, 'dtype': self.dtype, 'random_state': self.random_state}\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of the QuantileBinner.\n        \n        Parameters\n        ----------\n        params : dict\n            A dictionary of the parameters to set.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        for param, value in params.items():\n            setattr(self, param, value)"
  },
  "call_tree": {
    "skforecast/recursive/tests/tests_forecaster_recursive/test_binning_in_sample_residuals.py:test_binning_in_sample_residuals_output": {
      "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:__init__": {
        "skforecast/utils/utils.py:initialize_lags": {},
        "skforecast/utils/utils.py:initialize_window_features": {},
        "skforecast/preprocessing/preprocessing.py:QuantileBinner:__init__": {
          "skforecast/preprocessing/preprocessing.py:QuantileBinner:_validate_params": {}
        },
        "skforecast/utils/utils.py:initialize_weights": {},
        "skforecast/utils/utils.py:check_select_fit_kwargs": {}
      },
      "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:_binning_in_sample_residuals": {
        "skforecast/preprocessing/preprocessing.py:QuantileBinner:fit_transform": {
          "skforecast/preprocessing/preprocessing.py:QuantileBinner:fit": {},
          "skforecast/preprocessing/preprocessing.py:QuantileBinner:transform": {}
        }
      }
    },
    "skforecast/recursive/tests/tests_forecaster_recursive/test_binning_in_sample_residuals.py:test_binning_in_sample_residuals_stores_maximum_10000_residuals": {
      "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:__init__": {
        "skforecast/utils/utils.py:initialize_lags": {},
        "skforecast/utils/utils.py:initialize_window_features": {},
        "skforecast/preprocessing/preprocessing.py:QuantileBinner:__init__": {
          "skforecast/preprocessing/preprocessing.py:QuantileBinner:_validate_params": {}
        },
        "skforecast/utils/utils.py:initialize_weights": {},
        "skforecast/utils/utils.py:check_select_fit_kwargs": {}
      },
      "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:fit": {
        "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:_create_train_X_y": {
          "skforecast/utils/utils.py:check_y": {},
          "skforecast/utils/utils.py:input_to_frame": {},
          "skforecast/utils/utils.py:transform_dataframe": {},
          "skforecast/utils/utils.py:preprocess_y": {},
          "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:_create_lags": {}
        },
        "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:create_sample_weights": {},
        "skforecast/utils/utils.py:preprocess_y": {},
        "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:_binning_in_sample_residuals": {
          "skforecast/preprocessing/preprocessing.py:QuantileBinner:fit_transform": {
            "skforecast/preprocessing/preprocessing.py:QuantileBinner:fit": {},
            "skforecast/preprocessing/preprocessing.py:QuantileBinner:transform": {}
          }
        }
      }
    }
  }
}