{
  "dir_path": "/app/skforecast",
  "package_name": "skforecast",
  "sample_name": "skforecast-test_calculate_metrics_one_step_ahead",
  "src_dir": "skforecast/",
  "test_dir": "tests/",
  "test_file": "skforecast/model_selection/tests/tests_utils/test_calculate_metrics_one_step_ahead.py",
  "test_code": "# Unit test _calculate_metrics_one_step_ahead\n# ==============================================================================\nimport numpy as np\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom skforecast.metrics import mean_absolute_scaled_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.direct import ForecasterDirect\nfrom skforecast.model_selection._utils import _calculate_metrics_one_step_ahead\nfrom skforecast.metrics import add_y_train_argument\n\n# Fixtures\nfrom ..fixtures_model_selection import y\nfrom ..fixtures_model_selection import exog\n\n\ndef test_calculate_metrics_one_step_ahead_when_ForecasterRecursive():\n    \"\"\"\n    Testing _calculate_metrics_one_step_ahead when forecaster is of type ForecasterRecursive.\n    \"\"\"\n\n    forecaster = ForecasterRecursive(\n        regressor=LinearRegression(),\n        lags=5,\n        transformer_y=StandardScaler(),\n        transformer_exog=StandardScaler(),\n        differentiation=1,\n    )\n    metrics = [\n        mean_absolute_error,\n        mean_absolute_percentage_error,\n        mean_absolute_scaled_error,\n    ]\n    metrics = [add_y_train_argument(metric) for metric in metrics]\n    X_train, y_train, X_test, y_test = forecaster._train_test_split_one_step_ahead(\n        y=y, exog=exog, initial_train_size=10\n    )\n    results = _calculate_metrics_one_step_ahead(\n        forecaster=forecaster,\n        y=y,\n        metrics=metrics,\n        X_train=X_train,\n        y_train=y_train,\n        X_test=X_test,\n        y_test=y_test,\n    )\n    results = np.array([float(result) for result in results])\n\n    expected = np.array([0.5516310508466604, 1.2750659053445799, 2.811352223272513])\n\n    np.testing.assert_array_almost_equal(results, expected)\n\n\ndef test_calculate_metrics_one_step_ahead_when_ForecasterDirect():\n    \"\"\"\n    Testing _calculate_metrics_one_step_ahead when forecaster is of type ForecasterDirect.\n    \"\"\"\n\n    forecaster = ForecasterDirect(\n        regressor=LinearRegression(),\n        lags=5,\n        steps=3,\n        transformer_y=StandardScaler(),\n        transformer_exog=StandardScaler(),\n    )\n    metrics = [\n        mean_absolute_error,\n        mean_absolute_percentage_error,\n        mean_absolute_scaled_error,\n    ]\n    metrics = [add_y_train_argument(metric) for metric in metrics]\n    X_train, y_train, X_test, y_test = forecaster._train_test_split_one_step_ahead(\n        y=y, exog=exog, initial_train_size=10\n    )\n    results = _calculate_metrics_one_step_ahead(\n        forecaster=forecaster,\n        y=y,\n        metrics=metrics,\n        X_train=X_train,\n        y_train=y_train,\n        X_test=X_test,\n        y_test=y_test,\n    )\n    results = np.array([float(result) for result in results])\n    expected = np.array([0.3277718194807295, 1.3574261666383498, 0.767982227299475])\n\n    np.testing.assert_array_almost_equal(results, expected)\n",
  "GT_file_code": {
    "skforecast/model_selection/_utils.py": "################################################################################\n#                     skforecast.model_selection._utils                        #\n#                                                                              #\n# This work by skforecast team is licensed under the BSD 3-Clause License.     #\n################################################################################\n# coding=utf-8\n\nfrom typing import Union, Tuple, Optional, Callable, Generator\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom joblib import cpu_count\nfrom tqdm.auto import tqdm\nfrom sklearn.pipeline import Pipeline\nimport sklearn.linear_model\nfrom sklearn.exceptions import NotFittedError\n\nfrom ..exceptions import IgnoredArgumentWarning\nfrom ..metrics import add_y_train_argument, _get_metric\nfrom ..utils import check_interval\n\n\ndef initialize_lags_grid(\n    forecaster: object, \n    lags_grid: Optional[Union[list, dict]] = None\n) -> Tuple[dict, str]:\n    \"\"\"\n    Initialize lags grid and lags label for model selection. \n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model. ForecasterRecursive, ForecasterDirect, \n        ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate.\n    lags_grid : list, dict, default `None`\n        Lists of lags to try, containing int, lists, numpy ndarray, or range \n        objects. If `dict`, the keys are used as labels in the `results` \n        DataFrame, and the values are used as the lists of lags to try.\n\n    Returns\n    -------\n    lags_grid : dict\n        Dictionary with lags configuration for each iteration.\n    lags_label : str\n        Label for lags representation in the results object.\n\n    \"\"\"\n\n    if not isinstance(lags_grid, (list, dict, type(None))):\n        raise TypeError(\n            (f\"`lags_grid` argument must be a list, dict or None. \"\n             f\"Got {type(lags_grid)}.\")\n        )\n\n    lags_label = 'values'\n    if isinstance(lags_grid, list):\n        lags_grid = {f'{lags}': lags for lags in lags_grid}\n    elif lags_grid is None:\n        lags = [int(lag) for lag in forecaster.lags]  # Required since numpy 2.0\n        lags_grid = {f'{lags}': lags}\n    else:\n        lags_label = 'keys'\n\n    return lags_grid, lags_label\n\n\ndef check_backtesting_input(\n    forecaster: object,\n    cv: object,\n    metric: Union[str, Callable, list],\n    add_aggregated_metric: bool = True,\n    y: Optional[pd.Series] = None,\n    series: Optional[Union[pd.DataFrame, dict]] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    interval: Optional[list] = None,\n    alpha: Optional[float] = None,\n    n_boot: int = 250,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = False,\n    n_jobs: Union[int, str] = 'auto',\n    show_progress: bool = True,\n    suppress_warnings: bool = False,\n    suppress_warnings_fit: bool = False\n) -> None:\n    \"\"\"\n    This is a helper function to check most inputs of backtesting functions in \n    modules `model_selection`.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model.\n    cv : TimeSeriesFold\n        TimeSeriesFold object with the information needed to split the data into folds.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n    add_aggregated_metric : bool, default `True`\n        If `True`, the aggregated metrics (average, weighted average and pooling)\n        over all levels are also returned (only multiseries).\n    y : pandas Series, default `None`\n        Training time series for uni-series forecasters.\n    series : pandas DataFrame, dict, default `None`\n        Training time series for multi-series forecasters.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive.\n    alpha : float, default `None`\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %. \n    n_boot : int, default `250`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of prediction \n        error to create prediction intervals.  If `False`, out_sample_residuals \n        are used if they are already stored inside the forecaster.\n    use_binned_residuals : bool, default `False`\n        If `True`, residuals used in each bootstrapping iteration are selected\n        conditioning on the predicted values. If `False`, residuals are selected\n        randomly without conditioning on the predicted values.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the fuction\n        skforecast.utils.select_n_jobs_fit_forecaster.\n        **New in version 0.9.0**\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n    suppress_warnings: bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the backtesting \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    suppress_warnings_fit : bool, default `False`\n        If `True`, warnings generated during fitting will be ignored. Only \n        `ForecasterSarimax`.\n\n    Returns\n    -------\n    None\n    \n    \"\"\"\n\n    forecaster_name = type(forecaster).__name__\n    cv_name = type(cv).__name__\n\n    if cv_name != \"TimeSeriesFold\":\n        raise TypeError(f\"`cv` must be a TimeSeriesFold object. Got {cv_name}.\")\n\n    steps = cv.steps\n    initial_train_size = cv.initial_train_size\n    gap = cv.gap\n    allow_incomplete_fold = cv.allow_incomplete_fold\n    refit = cv.refit\n\n    forecasters_uni = [\n        \"ForecasterRecursive\",\n        \"ForecasterDirect\",\n        \"ForecasterSarimax\",\n        \"ForecasterEquivalentDate\",\n    ]\n    forecasters_multi = [\n        \"ForecasterDirectMultiVariate\",\n        \"ForecasterRnn\",\n    ]\n    forecasters_multi_dict = [\n        \"ForecasterRecursiveMultiSeries\"\n    ]\n\n    if forecaster_name in forecasters_uni:\n        if not isinstance(y, pd.Series):\n            raise TypeError(\"`y` must be a pandas Series.\")\n        data_name = 'y'\n        data_length = len(y)\n\n    elif forecaster_name in forecasters_multi:\n        if not isinstance(series, pd.DataFrame):\n            raise TypeError(\"`series` must be a pandas DataFrame.\")\n        data_name = 'series'\n        data_length = len(series)\n    \n    elif forecaster_name in forecasters_multi_dict:\n        if not isinstance(series, (pd.DataFrame, dict)):\n            raise TypeError(\n                f\"`series` must be a pandas DataFrame or a dict of DataFrames or Series. \"\n                f\"Got {type(series)}.\"\n            )\n        \n        data_name = 'series'\n        if isinstance(series, dict):\n            not_valid_series = [\n                k \n                for k, v in series.items()\n                if not isinstance(v, (pd.Series, pd.DataFrame))\n            ]\n            if not_valid_series:\n                raise TypeError(\n                    f\"If `series` is a dictionary, all series must be a named \"\n                    f\"pandas Series or a pandas DataFrame with a single column. \"\n                    f\"Review series: {not_valid_series}\"\n                )\n            not_valid_index = [\n                k \n                for k, v in series.items()\n                if not isinstance(v.index, pd.DatetimeIndex)\n            ]\n            if not_valid_index:\n                raise ValueError(\n                    f\"If `series` is a dictionary, all series must have a Pandas \"\n                    f\"DatetimeIndex as index with the same frequency. \"\n                    f\"Review series: {not_valid_index}\"\n                )\n\n            indexes_freq = [f'{v.index.freq}' for v in series.values()]\n            indexes_freq = sorted(set(indexes_freq))\n            if not len(indexes_freq) == 1:\n                raise ValueError(\n                    f\"If `series` is a dictionary, all series must have a Pandas \"\n                    f\"DatetimeIndex as index with the same frequency. \"\n                    f\"Found frequencies: {indexes_freq}\"\n                )\n            data_length = max([len(series[serie]) for serie in series])\n        else:\n            data_length = len(series)\n\n    if exog is not None:\n        if forecaster_name in forecasters_multi_dict:\n            if not isinstance(exog, (pd.Series, pd.DataFrame, dict)):\n                raise TypeError(\n                    f\"`exog` must be a pandas Series, DataFrame, dictionary of pandas \"\n                    f\"Series/DataFrames or None. Got {type(exog)}.\"\n                )\n            if isinstance(exog, dict):\n                not_valid_exog = [\n                    k \n                    for k, v in exog.items()\n                    if not isinstance(v, (pd.Series, pd.DataFrame, type(None)))\n                ]\n                if not_valid_exog:\n                    raise TypeError(\n                        f\"If `exog` is a dictionary, All exog must be a named pandas \"\n                        f\"Series, a pandas DataFrame or None. Review exog: {not_valid_exog}\"\n                    )\n        else:\n            if not isinstance(exog, (pd.Series, pd.DataFrame)):\n                raise TypeError(\n                    f\"`exog` must be a pandas Series, DataFrame or None. Got {type(exog)}.\"\n                )\n\n    if hasattr(forecaster, 'differentiation'):\n        if forecaster.differentiation != cv.differentiation:\n            raise ValueError(\n                f\"The differentiation included in the forecaster \"\n                f\"({forecaster.differentiation}) differs from the differentiation \"\n                f\"included in the cv ({cv.differentiation}). Set the same value \"\n                f\"for both using the `differentiation` argument.\"\n            )\n\n    if not isinstance(metric, (str, Callable, list)):\n        raise TypeError(\n            f\"`metric` must be a string, a callable function, or a list containing \"\n            f\"multiple strings and/or callables. Got {type(metric)}.\"\n        )\n\n    if forecaster_name == \"ForecasterEquivalentDate\" and isinstance(\n        forecaster.offset, pd.tseries.offsets.DateOffset\n    ):\n        if initial_train_size is None:\n            raise ValueError(\n                f\"`initial_train_size` must be an integer greater than \"\n                f\"the `window_size` of the forecaster ({forecaster.window_size}) \"\n                f\"and smaller than the length of `{data_name}` ({data_length}).\"\n            )\n    elif initial_train_size is not None:\n        if initial_train_size < forecaster.window_size or initial_train_size >= data_length:\n            raise ValueError(\n                f\"If used, `initial_train_size` must be an integer greater than \"\n                f\"the `window_size` of the forecaster ({forecaster.window_size}) \"\n                f\"and smaller than the length of `{data_name}` ({data_length}).\"\n            )\n        if initial_train_size + gap >= data_length:\n            raise ValueError(\n                f\"The combination of initial_train_size {initial_train_size} and \"\n                f\"gap {gap} cannot be greater than the length of `{data_name}` \"\n                f\"({data_length}).\"\n            )\n    else:\n        if forecaster_name in ['ForecasterSarimax', 'ForecasterEquivalentDate']:\n            raise ValueError(\n                f\"`initial_train_size` must be an integer smaller than the \"\n                f\"length of `{data_name}` ({data_length}).\"\n            )\n        else:\n            if not forecaster.is_fitted:\n                raise NotFittedError(\n                    \"`forecaster` must be already trained if no `initial_train_size` \"\n                    \"is provided.\"\n                )\n            if refit:\n                raise ValueError(\n                    \"`refit` is only allowed when `initial_train_size` is not `None`.\"\n                )\n\n    if forecaster_name == 'ForecasterSarimax' and cv.skip_folds is not None:\n        raise ValueError(\n            \"`skip_folds` is not allowed for ForecasterSarimax. Set it to `None`.\"\n        )\n\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError(\"`add_aggregated_metric` must be a boolean: `True`, `False`.\")\n    if not isinstance(n_boot, (int, np.integer)) or n_boot < 0:\n        raise TypeError(f\"`n_boot` must be an integer greater than 0. Got {n_boot}.\")\n    if not isinstance(random_state, (int, np.integer)) or random_state < 0:\n        raise TypeError(f\"`random_state` must be an integer greater than 0. Got {random_state}.\")\n    if not isinstance(use_in_sample_residuals, bool):\n        raise TypeError(\"`use_in_sample_residuals` must be a boolean: `True`, `False`.\")\n    if not isinstance(use_binned_residuals, bool):\n        raise TypeError(\"`use_binned_residuals` must be a boolean: `True`, `False`.\")\n    if not isinstance(n_jobs, int) and n_jobs != 'auto':\n        raise TypeError(f\"`n_jobs` must be an integer or `'auto'`. Got {n_jobs}.\")\n    if not isinstance(show_progress, bool):\n        raise TypeError(\"`show_progress` must be a boolean: `True`, `False`.\")\n    if not isinstance(suppress_warnings, bool):\n        raise TypeError(\"`suppress_warnings` must be a boolean: `True`, `False`.\")\n    if not isinstance(suppress_warnings_fit, bool):\n        raise TypeError(\"`suppress_warnings_fit` must be a boolean: `True`, `False`.\")\n\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n\n    if not allow_incomplete_fold and data_length - (initial_train_size + gap) < steps:\n        raise ValueError(\n            f\"There is not enough data to evaluate {steps} steps in a single \"\n            f\"fold. Set `allow_incomplete_fold` to `True` to allow incomplete folds.\\n\"\n            f\"    Data available for test : {data_length - (initial_train_size + gap)}\\n\"\n            f\"    Steps                   : {steps}\"\n        )\n\n\ndef select_n_jobs_backtesting(\n    forecaster: object,\n    refit: Union[bool, int]\n) -> int:\n    \"\"\"\n    Select the optimal number of jobs to use in the backtesting process. This\n    selection is based on heuristics and is not guaranteed to be optimal.\n\n    The number of jobs is chosen as follows:\n\n    - If `refit` is an integer, then `n_jobs = 1`. This is because parallelization doesn't \n    work with intermittent refit.\n    - If forecaster is 'ForecasterRecursive' and regressor is a linear regressor, \n    then `n_jobs = 1`.\n    - If forecaster is 'ForecasterRecursive' and regressor is not a linear \n    regressor then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and `refit = True`, then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and `refit = False`, then `n_jobs = 1`.\n    - If forecaster is 'ForecasterRecursiveMultiSeries', then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterSarimax' or 'ForecasterEquivalentDate', \n    then `n_jobs = 1`.\n    - If regressor is a `LGBMRegressor(n_jobs=1)`, then `n_jobs = cpu_count() - 1`.\n    - If regressor is a `LGBMRegressor` with internal n_jobs != 1, then `n_jobs = 1`.\n    This is because `lightgbm` is highly optimized for gradient boosting and\n    parallelizes operations at a very fine-grained level, making additional\n    parallelization unnecessary and potentially harmful due to resource contention.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model.\n    refit : bool, int\n        If the forecaster is refitted during the backtesting process.\n\n    Returns\n    -------\n    n_jobs : int\n        The number of jobs to run in parallel.\n    \n    \"\"\"\n\n    forecaster_name = type(forecaster).__name__\n\n    if isinstance(forecaster.regressor, Pipeline):\n        regressor = forecaster.regressor[-1]\n        regressor_name = type(regressor).__name__\n    else:\n        regressor = forecaster.regressor\n        regressor_name = type(regressor).__name__\n\n    linear_regressors = [\n        regressor_name\n        for regressor_name in dir(sklearn.linear_model)\n        if not regressor_name.startswith('_')\n    ]\n\n    refit = False if refit == 0 else refit\n    if not isinstance(refit, bool) and refit != 1:\n        n_jobs = 1\n    else:\n        if forecaster_name in ['ForecasterRecursive']:\n            if regressor_name in linear_regressors:\n                n_jobs = 1\n            elif regressor_name == 'LGBMRegressor':\n                n_jobs = cpu_count() - 1 if regressor.n_jobs == 1 else 1\n            else:\n                n_jobs = cpu_count() - 1\n        elif forecaster_name in ['ForecasterDirect', 'ForecasterDirectMultiVariate']:\n            # Parallelization is applied during the fitting process.\n            n_jobs = 1\n        elif forecaster_name in ['ForecasterRecursiveMultiSeries']:\n            if regressor_name == 'LGBMRegressor':\n                n_jobs = cpu_count() - 1 if regressor.n_jobs == 1 else 1\n            else:\n                n_jobs = cpu_count() - 1\n        elif forecaster_name in ['ForecasterSarimax', 'ForecasterEquivalentDate']:\n            n_jobs = 1\n        else:\n            n_jobs = 1\n\n    return n_jobs\n\n\ndef _calculate_metrics_one_step_ahead(\n    forecaster: object,\n    y: pd.Series,\n    metrics: list,\n    X_train: pd.DataFrame,\n    y_train: Union[pd.Series, dict],\n    X_test: pd.DataFrame,\n    y_test: Union[pd.Series, dict]\n) -> list:\n    \"\"\"\n    Calculate metrics when predictions are one-step-ahead. When forecaster is\n    of type ForecasterDirect only the regressor for step 1 is used.\n\n    Parameters\n    ----------\n    forecaster : object\n        Forecaster model.\n    y : pandas Series\n        Time series data used to train and test the model.\n    metrics : list\n        List of metrics.\n    X_train : pandas DataFrame\n        Predictor values used to train the model.\n    y_train : pandas Series\n        Target values related to each row of `X_train`.\n    X_test : pandas DataFrame\n        Predictor values used to test the model.\n    y_test : pandas Series\n        Target values related to each row of `X_test`.\n\n    Returns\n    -------\n    metric_values : list\n        List with metric values.\n    \n    \"\"\"\n\n    if type(forecaster).__name__ == 'ForecasterDirect':\n\n        step = 1  # Only the model for step 1 is optimized.\n        X_train, y_train = forecaster.filter_train_X_y_for_step(\n                               step    = step,\n                               X_train = X_train,\n                               y_train = y_train\n                           )\n        X_test, y_test = forecaster.filter_train_X_y_for_step(\n                             step    = step,  \n                             X_train = X_test,\n                             y_train = y_test\n                         )\n        forecaster.regressors_[step].fit(X_train, y_train)\n        y_pred = forecaster.regressors_[step].predict(X_test)\n\n    else:\n        forecaster.regressor.fit(X_train, y_train)\n        y_pred = forecaster.regressor.predict(X_test)\n\n    y_true = y_test.to_numpy()\n    y_pred = y_pred.ravel()\n    y_train = y_train.to_numpy()\n\n    if forecaster.differentiation is not None:\n        y_true = forecaster.differentiator.inverse_transform_next_window(y_true)\n        y_pred = forecaster.differentiator.inverse_transform_next_window(y_pred)\n        y_train = forecaster.differentiator.inverse_transform_training(y_train)\n\n    if forecaster.transformer_y is not None:\n        y_true = forecaster.transformer_y.inverse_transform(y_true.reshape(-1, 1))\n        y_pred = forecaster.transformer_y.inverse_transform(y_pred.reshape(-1, 1))\n        y_train = forecaster.transformer_y.inverse_transform(y_train.reshape(-1, 1))\n\n    metric_values = []\n    for m in metrics:\n        metric_values.append(\n            m(y_true=y_true.ravel(), y_pred=y_pred.ravel(), y_train=y_train.ravel())\n        )\n\n    return metric_values\n\n\ndef _initialize_levels_model_selection_multiseries(\n    forecaster: object, \n    series: Union[pd.DataFrame, dict],\n    levels: Optional[Union[str, list]] = None\n) -> list:\n    \"\"\"\n    Initialize levels for model_selection multi-series functions.\n\n    Parameters\n    ----------\n    forecaster : ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate, ForecasterRnn\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Training time series.\n    levels : str, list, default `None`\n        level (`str`) or levels (`list`) at which the forecaster is optimized. \n        If `None`, all levels are taken into account. The resulting metric will be\n        the average of the optimization of all levels.\n\n    Returns\n    -------\n    levels : list\n        List of levels to be used in model_selection multi-series functions.\n    \n    \"\"\"\n\n    multi_series_forecasters_with_levels = [\n        'ForecasterRecursiveMultiSeries', \n        'ForecasterRnn'\n    ]\n\n    if type(forecaster).__name__ in multi_series_forecasters_with_levels  \\\n        and not isinstance(levels, (str, list, type(None))):\n        raise TypeError(\n            (f\"`levels` must be a `list` of column names, a `str` of a column \"\n             f\"name or `None` when using a forecaster of type \"\n             f\"{multi_series_forecasters_with_levels}. If the forecaster is of \"\n             f\"type `ForecasterDirectMultiVariate`, this argument is ignored.\")\n        )\n\n    if type(forecaster).__name__ == 'ForecasterDirectMultiVariate':\n        if levels and levels != forecaster.level and levels != [forecaster.level]:\n            warnings.warn(\n                (f\"`levels` argument have no use when the forecaster is of type \"\n                 f\"`ForecasterDirectMultiVariate`. The level of this forecaster \"\n                 f\"is '{forecaster.level}', to predict another level, change \"\n                 f\"the `level` argument when initializing the forecaster. \\n\"),\n                 IgnoredArgumentWarning\n            )\n        levels = [forecaster.level]\n    else:\n        if levels is None:\n            # Forecaster could be untrained, so self.series_col_names cannot be used.\n            if isinstance(series, pd.DataFrame):\n                levels = list(series.columns)\n            else:\n                levels = list(series.keys())\n        elif isinstance(levels, str):\n            levels = [levels]\n\n    return levels\n\n\ndef _extract_data_folds_multiseries(\n    series: Union[pd.Series, pd.DataFrame, dict],\n    folds: list,\n    span_index: Union[pd.DatetimeIndex, pd.RangeIndex],\n    window_size: int,\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    dropna_last_window: bool = False,\n    externally_fitted: bool = False\n) -> Generator[\n        Tuple[\n            Union[pd.Series, pd.DataFrame, dict],\n            pd.DataFrame,\n            list,\n            Optional[Union[pd.Series, pd.DataFrame, dict]],\n            Optional[Union[pd.Series, pd.DataFrame, dict]],\n            list\n        ],\n        None,\n        None\n    ]:\n    \"\"\"\n    Select the data from series and exog that corresponds to each fold created using the\n    skforecast.model_selection._create_backtesting_folds function.\n\n    Parameters\n    ----------\n    series : pandas Series, pandas DataFrame, dict\n        Time series.\n    folds : list\n        Folds created using the skforecast.model_selection._create_backtesting_folds\n        function.\n    span_index : pandas DatetimeIndex, pandas RangeIndex\n        Full index from the minimum to the maximum index among all series.\n    window_size : int\n        Size of the window needed to create the predictors.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    dropna_last_window : bool, default `False`\n        If `True`, drop the columns of the last window that have NaN values.\n    externally_fitted : bool, default `False`\n        Flag indicating whether the forecaster is already trained. Only used when \n        `initial_train_size` is None and `refit` is False.\n\n    Yield\n    -----\n    series_train : pandas Series, pandas DataFrame, dict\n        Time series corresponding to the training set of the fold.\n    series_last_window: pandas DataFrame\n        Time series corresponding to the last window of the fold.\n    levels_last_window: list\n        Levels of the time series present in the last window of the fold.\n    exog_train: pandas Series, pandas DataFrame, dict, None\n        Exogenous variable corresponding to the training set of the fold.\n    exog_test: pandas Series, pandas DataFrame, dict, None\n        Exogenous variable corresponding to the test set of the fold.\n    fold: list\n        Fold created using the skforecast.model_selection._create_backtesting_folds\n\n    \"\"\"\n\n    for fold in folds:\n        train_iloc_start       = fold[0][0]\n        train_iloc_end         = fold[0][1]\n        last_window_iloc_start = fold[1][0]\n        last_window_iloc_end   = fold[1][1]\n        test_iloc_start        = fold[2][0]\n        test_iloc_end          = fold[2][1]\n\n        if isinstance(series, dict) or isinstance(exog, dict):\n            # Substract 1 to the iloc indexes to get the loc indexes\n            train_loc_start       = span_index[train_iloc_start]\n            train_loc_end         = span_index[train_iloc_end - 1]\n            last_window_loc_start = span_index[last_window_iloc_start]\n            last_window_loc_end   = span_index[last_window_iloc_end - 1]\n            test_loc_start        = span_index[test_iloc_start]\n            test_loc_end          = span_index[test_iloc_end - 1]\n\n        if isinstance(series, pd.DataFrame):\n            series_train = series.iloc[train_iloc_start:train_iloc_end, ]\n\n            series_to_drop = []\n            for col in series_train.columns:\n                if series_train[col].isna().all():\n                    series_to_drop.append(col)\n                else:\n                    first_valid_index = series_train[col].first_valid_index()\n                    last_valid_index = series_train[col].last_valid_index()\n                    if (\n                        len(series_train[col].loc[first_valid_index:last_valid_index])\n                        < window_size\n                    ):\n                        series_to_drop.append(col)\n\n            series_last_window = series.iloc[\n                last_window_iloc_start:last_window_iloc_end,\n            ]\n            \n            series_train = series_train.drop(columns=series_to_drop)\n            if not externally_fitted:\n                series_last_window = series_last_window.drop(columns=series_to_drop)\n        else:\n            series_train = {}\n            for k in series.keys():\n                v = series[k].loc[train_loc_start:train_loc_end]\n                if not v.isna().all():\n                    first_valid_index = v.first_valid_index()\n                    last_valid_index  = v.last_valid_index()\n                    if first_valid_index is not None and last_valid_index is not None:\n                        v = v.loc[first_valid_index : last_valid_index]\n                        if len(v) >= window_size:\n                            series_train[k] = v\n\n            series_last_window = {}\n            for k, v in series.items():\n                v = series[k].loc[last_window_loc_start:last_window_loc_end]\n                if ((externally_fitted or k in series_train) and len(v) >= window_size):\n                    series_last_window[k] = v\n\n            series_last_window = pd.DataFrame(series_last_window)\n\n        if dropna_last_window:\n            series_last_window = series_last_window.dropna(axis=1, how=\"any\")\n            # TODO: add the option to drop the series without minimum non NaN values.\n            # Similar to how pandas does in the rolling window function.\n        \n        levels_last_window = list(series_last_window.columns)\n\n        if exog is not None:\n            if isinstance(exog, (pd.Series, pd.DataFrame)):\n                exog_train = exog.iloc[train_iloc_start:train_iloc_end, ]\n                exog_test = exog.iloc[test_iloc_start:test_iloc_end, ]\n            else:\n                exog_train = {\n                    k: v.loc[train_loc_start:train_loc_end] \n                    for k, v in exog.items()\n                }\n                exog_train = {k: v for k, v in exog_train.items() if len(v) > 0}\n\n                exog_test = {\n                    k: v.loc[test_loc_start:test_loc_end]\n                    for k, v in exog.items()\n                    if externally_fitted or k in exog_train\n                }\n\n                exog_test = {k: v for k, v in exog_test.items() if len(v) > 0}\n        else:\n            exog_train = None\n            exog_test = None\n\n        yield series_train, series_last_window, levels_last_window, exog_train, exog_test, fold\n\n\ndef _calculate_metrics_backtesting_multiseries(\n    series: Union[pd.DataFrame, dict],\n    predictions: pd.DataFrame,\n    folds: Union[list, tqdm],\n    span_index: Union[pd.DatetimeIndex, pd.RangeIndex],\n    window_size: int,\n    metrics: list,\n    levels: list,\n    add_aggregated_metric: bool = True\n) -> pd.DataFrame:\n    \"\"\"   \n    Calculate metrics for each level and also for all levels aggregated using\n    average, weighted average or pooling.\n\n    - 'average': the average (arithmetic mean) of all levels.\n    - 'weighted_average': the average of the metrics weighted by the number of\n    predicted values of each level.\n    - 'pooling': the values of all levels are pooled and then the metric is\n    calculated.\n\n    Parameters\n    ----------\n    series : pandas DataFrame, dict\n        Series data used for backtesting.\n    predictions : pandas DataFrame\n        Predictions generated during the backtesting process.\n    folds : list, tqdm\n        Folds created during the backtesting process.\n    span_index : pandas DatetimeIndex, pandas RangeIndex\n        Full index from the minimum to the maximum index among all series.\n    window_size : int\n        Size of the window used by the forecaster to create the predictors.\n        This is used remove the first `window_size` (differentiation included) \n        values from y_train since they are not part of the training matrix.\n    metrics : list\n        List of metrics to calculate.\n    levels : list\n        Levels to calculate the metrics.\n    add_aggregated_metric : bool, default `True`\n        If `True`, and multiple series (`levels`) are predicted, the aggregated\n        metrics (average, weighted average and pooled) are also returned.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s).\n    \n    \"\"\"\n\n    if not isinstance(series, (pd.DataFrame, dict)):\n        raise TypeError(\n            (\"`series` must be a pandas DataFrame or a dictionary of pandas \"\n             \"DataFrames.\")\n        )\n    if not isinstance(predictions, pd.DataFrame):\n        raise TypeError(\"`predictions` must be a pandas DataFrame.\")\n    if not isinstance(folds, (list, tqdm)):\n        raise TypeError(\"`folds` must be a list or a tqdm object.\")\n    if not isinstance(span_index, (pd.DatetimeIndex, pd.RangeIndex)):\n        raise TypeError(\"`span_index` must be a pandas DatetimeIndex or pandas RangeIndex.\")\n    if not isinstance(window_size, (int, np.integer)):\n        raise TypeError(\"`window_size` must be an integer.\")\n    if not isinstance(metrics, list):\n        raise TypeError(\"`metrics` must be a list.\")\n    if not isinstance(levels, list):\n        raise TypeError(\"`levels` must be a list.\")\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError(\"`add_aggregated_metric` must be a boolean.\")\n    \n    metric_names = [(m if isinstance(m, str) else m.__name__) for m in metrics]\n\n    y_true_pred_levels = []\n    y_train_levels = []\n    for level in levels:\n        y_true_pred_level = None\n        y_train = None\n        if level in predictions.columns:\n            # TODO: avoid merges inside the loop, instead merge outside and then filter\n            y_true_pred_level = pd.merge(\n                series[level],\n                predictions[level],\n                left_index  = True,\n                right_index = True,\n                how         = \"inner\",\n            ).dropna(axis=0, how=\"any\")\n            y_true_pred_level.columns = ['y_true', 'y_pred']\n\n            train_indexes = []\n            for i, fold in enumerate(folds):\n                fit_fold = fold[-1]\n                if i == 0 or fit_fold:\n                    train_iloc_start = fold[0][0]\n                    train_iloc_end = fold[0][1]\n                    train_indexes.append(np.arange(train_iloc_start, train_iloc_end))\n            train_indexes = np.unique(np.concatenate(train_indexes))\n            train_indexes = span_index[train_indexes]\n            y_train = series[level].loc[series[level].index.intersection(train_indexes)]\n\n        y_true_pred_levels.append(y_true_pred_level)\n        y_train_levels.append(y_train)\n            \n    metrics_levels = []\n    for i, level in enumerate(levels):\n        if y_true_pred_levels[i] is not None and not y_true_pred_levels[i].empty:\n            metrics_level = [\n                m(\n                    y_true = y_true_pred_levels[i].iloc[:, 0],\n                    y_pred = y_true_pred_levels[i].iloc[:, 1],\n                    y_train = y_train_levels[i].iloc[window_size:]  # Exclude observations used to create predictors\n                )\n                for m in metrics\n            ]\n            metrics_levels.append(metrics_level)\n        else:\n            metrics_levels.append([None for _ in metrics])\n\n    metrics_levels = pd.DataFrame(\n                         data    = metrics_levels,\n                         columns = [m if isinstance(m, str) else m.__name__\n                                    for m in metrics]\n                     )\n    metrics_levels.insert(0, 'levels', levels)\n\n    if len(levels) < 2:\n        add_aggregated_metric = False\n    \n    if add_aggregated_metric:\n\n        # aggragation: average\n        average = metrics_levels.drop(columns='levels').mean(skipna=True)\n        average = average.to_frame().transpose()\n        average['levels'] = 'average'\n\n        # aggregation: weighted_average\n        weighted_averages = {}\n        n_predictions_levels = (\n            predictions\n            .notna()\n            .sum()\n            .to_frame(name='n_predictions')\n            .reset_index(names='levels')\n        )\n        metrics_levels_no_missing = (\n            metrics_levels.merge(n_predictions_levels, on='levels', how='inner')\n        )\n        for col in metric_names:\n            weighted_averages[col] = np.average(\n                metrics_levels_no_missing[col],\n                weights=metrics_levels_no_missing['n_predictions']\n            )\n        weighted_average = pd.DataFrame(weighted_averages, index=[0])\n        weighted_average['levels'] = 'weighted_average'\n\n        # aggregation: pooling\n        y_true_pred_levels, y_train_levels = zip(\n            *[\n                (a, b.iloc[window_size:])  # Exclude observations used to create predictors\n                for a, b in zip(y_true_pred_levels, y_train_levels)\n                if a is not None\n            ]\n        )\n        y_train_levels = list(y_train_levels)\n        y_true_pred_levels = pd.concat(y_true_pred_levels)\n        y_train_levels_concat = pd.concat(y_train_levels)\n\n        pooled = []\n        for m, m_name in zip(metrics, metric_names):\n            if m_name in ['mean_absolute_scaled_error', 'root_mean_squared_scaled_error']:\n                pooled.append(\n                    m(\n                        y_true = y_true_pred_levels.loc[:, 'y_true'],\n                        y_pred = y_true_pred_levels.loc[:, 'y_pred'],\n                        y_train = y_train_levels\n                    )\n                )\n            else:\n                pooled.append(\n                    m(\n                        y_true = y_true_pred_levels.loc[:, 'y_true'],\n                        y_pred = y_true_pred_levels.loc[:, 'y_pred'],\n                        y_train = y_train_levels_concat\n                    )\n                )\n        pooled = pd.DataFrame([pooled], columns=metric_names)\n        pooled['levels'] = 'pooling'\n\n        metrics_levels = pd.concat(\n            [metrics_levels, average, weighted_average, pooled],\n            axis=0,\n            ignore_index=True\n        )\n\n    return metrics_levels\n\n\ndef _predict_and_calculate_metrics_one_step_ahead_multiseries(\n    forecaster: object,\n    series: Union[pd.DataFrame, dict],\n    X_train: pd.DataFrame,\n    y_train: Union[pd.Series, dict],\n    X_test: pd.DataFrame,\n    y_test: Union[pd.Series, dict],\n    X_train_encoding: pd.Series,\n    X_test_encoding: pd.Series,\n    levels: list,\n    metrics: list,\n    add_aggregated_metric: bool = True\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"   \n    One-step-ahead predictions and metrics for each level and also for all levels\n    aggregated using average, weighted average or pooling.\n    Input matrices (X_train, y_train, X_train_encoding, X_test, y_test, X_test_encoding)\n    should have been generated using the forecaster._train_test_split_one_step_ahead().\n\n    - 'average': the average (arithmetic mean) of all levels.\n    - 'weighted_average': the average of the metrics weighted by the number of\n    predicted values of each level.\n    - 'pooling': the values of all levels are pooled and then the metric is\n    calculated.\n\n    Parameters\n    ----------\n    forecaster : object\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Series data used to train and test the forecaster.\n    X_train : pandas DataFrame\n        Training matrix.\n    y_train : pandas Series, dict\n        Target values of the training set.\n    X_test : pandas DataFrame\n        Test matrix.\n    y_test : pandas Series, dict\n        Target values of the test set.\n    X_train_encoding : pandas Series\n        Series identifiers for each row of `X_train`.\n    X_test_encoding : pandas Series\n        Series identifiers for each row of `X_test`.\n    levels : list\n        Levels to calculate the metrics.\n    metrics : list\n        List of metrics to calculate.\n    add_aggregated_metric : bool, default `True`\n        If `True`, and multiple series (`levels`) are predicted, the aggregated\n        metrics (average, weighted average and pooled) are also returned.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s).\n    predictions : pandas DataFrame\n        Value of predictions for each level.\n    \n    \"\"\"\n\n    if not isinstance(series, (pd.DataFrame, dict)):\n        raise TypeError(\n            \"`series` must be a pandas DataFrame or a dictionary of pandas \"\n            \"DataFrames.\"\n        )\n    if not isinstance(X_train, pd.DataFrame):\n        raise TypeError(f\"`X_train` must be a pandas DataFrame. Got: {type(X_train)}\")\n    if not isinstance(y_train, (pd.Series, dict)):\n        raise TypeError(\n            f\"`y_train` must be a pandas Series or a dictionary of pandas Series. \"\n            f\"Got: {type(y_train)}\"\n        )        \n    if not isinstance(X_test, pd.DataFrame):\n        raise TypeError(f\"`X_test` must be a pandas DataFrame. Got: {type(X_test)}\")\n    if not isinstance(y_test, (pd.Series, dict)):\n        raise TypeError(\n            f\"`y_test` must be a pandas Series or a dictionary of pandas Series. \"\n            f\"Got: {type(y_test)}\"\n        )\n    if not isinstance(X_train_encoding, pd.Series):\n        raise TypeError(\n            f\"`X_train_encoding` must be a pandas Series. Got: {type(X_train_encoding)}\"\n        )\n    if not isinstance(X_test_encoding, pd.Series):\n        raise TypeError(\n            f\"`X_test_encoding` must be a pandas Series. Got: {type(X_test_encoding)}\"\n        )\n    if not isinstance(levels, list):\n        raise TypeError(f\"`levels` must be a list. Got: {type(levels)}\")\n    if not isinstance(metrics, list):\n        raise TypeError(f\"`metrics` must be a list. Got: {type(metrics)}\")\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError(\n            f\"`add_aggregated_metric` must be a boolean. Got: {type(add_aggregated_metric)}\"\n        )\n    \n    metrics = [\n        _get_metric(metric=m)\n        if isinstance(m, str)\n        else add_y_train_argument(m) \n        for m in metrics\n    ]\n    metric_names = [(m if isinstance(m, str) else m.__name__) for m in metrics]\n\n    if isinstance(series[levels[0]].index, pd.DatetimeIndex):\n        freq = series[levels[0]].index.freq\n    else:\n        freq = series[levels[0]].index.step\n\n    if type(forecaster).__name__ == 'ForecasterDirectMultiVariate':\n        step = 1\n        X_train, y_train = forecaster.filter_train_X_y_for_step(\n                               step    = step,\n                               X_train = X_train,\n                               y_train = y_train\n                           )\n        X_test, y_test = forecaster.filter_train_X_y_for_step(\n                             step    = step,  \n                             X_train = X_test,\n                             y_train = y_test\n                         )                 \n        forecaster.regressors_[step].fit(X_train, y_train)\n        pred = forecaster.regressors_[step].predict(X_test)\n    else:\n        forecaster.regressor.fit(X_train, y_train)\n        pred = forecaster.regressor.predict(X_test)\n\n    predictions_per_level = pd.DataFrame(\n        {\n            'y_true': y_test,\n            'y_pred': pred,\n            '_level_skforecast': X_test_encoding,\n        },\n        index=y_test.index,\n    ).groupby('_level_skforecast')\n    predictions_per_level = {key: group for key, group in predictions_per_level}\n\n    y_train_per_level = pd.DataFrame(\n        {\"y_train\": y_train, \"_level_skforecast\": X_train_encoding},\n        index=y_train.index,\n    ).groupby(\"_level_skforecast\")\n    # Interleaved Nan values were excluded fom y_train. They are reestored\n    y_train_per_level = {key: group.asfreq(freq) for key, group in y_train_per_level}\n\n    if forecaster.differentiation is not None:\n        for level in predictions_per_level:\n            predictions_per_level[level][\"y_true\"] = (\n                forecaster.differentiator_[level].inverse_transform_next_window(\n                    predictions_per_level[level][\"y_true\"].to_numpy()\n                )\n            )\n            predictions_per_level[level][\"y_pred\"] = (\n                forecaster.differentiator_[level].inverse_transform_next_window(\n                    predictions_per_level[level][\"y_pred\"].to_numpy()\n                )   \n            )\n            y_train_per_level[level][\"y_train\"] = (\n                forecaster.differentiator_[level].inverse_transform_training(\n                    y_train_per_level[level][\"y_train\"].to_numpy()\n                )\n            )\n\n    if forecaster.transformer_series is not None:\n        for level in predictions_per_level:\n            transformer = forecaster.transformer_series_[level]\n            predictions_per_level[level][\"y_true\"] = transformer.inverse_transform(\n                predictions_per_level[level][[\"y_true\"]]\n            )\n            predictions_per_level[level][\"y_pred\"] = transformer.inverse_transform(\n                predictions_per_level[level][[\"y_pred\"]]\n            )\n            y_train_per_level[level][\"y_train\"] = transformer.inverse_transform(\n                y_train_per_level[level][[\"y_train\"]]\n            )\n    \n    metrics_levels = []\n    for level in levels:\n        if level in predictions_per_level:\n            metrics_level = [\n                m(\n                    y_true  = predictions_per_level[level].loc[:, 'y_true'],\n                    y_pred  = predictions_per_level[level].loc[:, 'y_pred'],\n                    y_train = y_train_per_level[level].loc[:, 'y_train']\n                )\n                for m in metrics\n            ]\n            metrics_levels.append(metrics_level)\n        else:\n            metrics_levels.append([None for _ in metrics])\n\n    metrics_levels = pd.DataFrame(\n                         data    = metrics_levels,\n                         columns = [m if isinstance(m, str) else m.__name__\n                                    for m in metrics]\n                     )\n    metrics_levels.insert(0, 'levels', levels)\n\n    if len(levels) < 2:\n        add_aggregated_metric = False\n\n    if add_aggregated_metric:\n\n        # aggragation: average\n        average = metrics_levels.drop(columns='levels').mean(skipna=True)\n        average = average.to_frame().transpose()\n        average['levels'] = 'average'\n\n        # aggregation: weighted_average\n        weighted_averages = {}\n        n_predictions_levels = {\n            k: v['y_pred'].notna().sum()\n            for k, v in predictions_per_level.items()\n        }\n        n_predictions_levels = pd.DataFrame(\n            n_predictions_levels.items(),\n            columns=['levels', 'n_predictions']\n        )\n        metrics_levels_no_missing = (\n            metrics_levels.merge(n_predictions_levels, on='levels', how='inner')\n        )\n        for col in metric_names:\n            weighted_averages[col] = np.average(\n                metrics_levels_no_missing[col],\n                weights=metrics_levels_no_missing['n_predictions']\n            )\n        weighted_average = pd.DataFrame(weighted_averages, index=[0])\n        weighted_average['levels'] = 'weighted_average'\n\n        # aggregation: pooling\n        list_y_train_by_level = [\n            v['y_train'].to_numpy()\n            for k, v in y_train_per_level.items()\n            if k in predictions_per_level\n        ]\n        predictions_pooled = pd.concat(predictions_per_level.values())\n        y_train_pooled = pd.concat(\n            [v for k, v in y_train_per_level.items() if k in predictions_per_level]\n        )\n        pooled = []\n        for m, m_name in zip(metrics, metric_names):\n            if m_name in ['mean_absolute_scaled_error', 'root_mean_squared_scaled_error']:\n                pooled.append(\n                    m(\n                        y_true  = predictions_pooled['y_true'],\n                        y_pred  = predictions_pooled['y_pred'],\n                        y_train = list_y_train_by_level\n                    )\n                )\n            else:\n                pooled.append(\n                    m(\n                        y_true  = predictions_pooled['y_true'],\n                        y_pred  = predictions_pooled['y_pred'],\n                        y_train = y_train_pooled['y_train']\n                    )\n                )\n        pooled = pd.DataFrame([pooled], columns=metric_names)\n        pooled['levels'] = 'pooling'\n\n        metrics_levels = pd.concat(\n            [metrics_levels, average, weighted_average, pooled],\n            axis=0,\n            ignore_index=True\n        )\n\n    predictions = (\n        pd.concat(predictions_per_level.values())\n        .loc[:, [\"y_pred\", \"_level_skforecast\"]]\n        .pivot(columns=\"_level_skforecast\", values=\"y_pred\")\n        .rename_axis(columns=None, index=None)\n    )\n    predictions = predictions.asfreq(X_test.index.freq)\n\n    return metrics_levels, predictions\n",
    "skforecast/recursive/_forecaster_recursive.py": "################################################################################\n#                           ForecasterRecursive                                #\n#                                                                              #\n# This work by skforecast team is licensed under the BSD 3-Clause License.     #\n################################################################################\n# coding=utf-8\n\nfrom typing import Union, Tuple, Optional, Callable\nimport warnings\nimport sys\nimport numpy as np\nimport pandas as pd\nimport inspect\nfrom copy import copy\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import clone\n\nimport skforecast\nfrom ..base import ForecasterBase\nfrom ..exceptions import DataTransformationWarning\nfrom ..utils import (\n    initialize_lags,\n    initialize_window_features,\n    initialize_weights,\n    check_select_fit_kwargs,\n    check_y,\n    check_exog,\n    get_exog_dtypes,\n    check_exog_dtypes,\n    check_predict_input,\n    check_interval,\n    preprocess_y,\n    preprocess_last_window,\n    preprocess_exog,\n    input_to_frame,\n    date_to_index_position,\n    expand_index,\n    transform_numpy,\n    transform_dataframe,\n)\nfrom ..preprocessing import TimeSeriesDifferentiator\nfrom ..preprocessing import QuantileBinner\n\n\nclass ForecasterRecursive(ForecasterBase):\n    \"\"\"\n    This class turns any regressor compatible with the scikit-learn API into a\n    recursive autoregressive (multi-step) forecaster.\n    \n    Parameters\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n    lags : int, list, numpy ndarray, range, default `None`\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n    \n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n        - `None`: no lags are included as predictors. \n    window_features : object, list, default `None`\n        Instance or list of instances used to create window features. Window features\n        are created from the original time series and are included as predictors.\n    transformer_y : object transformer (preprocessor), default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and inverse_transform.\n        ColumnTransformers are not allowed since they do not have inverse_transform method.\n        The transformation is applied to `y` before training the forecaster. \n    transformer_exog : object transformer (preprocessor), default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable, default `None`\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\n        method. The resulting `sample_weight` cannot have negative values.\n    differentiation : int, default `None`\n        Order of differencing applied to the time series before training the forecaster.\n        If `None`, no differencing is applied. The order of differentiation is the number\n        of times the differencing operation is applied to a time series. Differencing\n        involves computing the differences between consecutive data points in the series.\n        Differentiation is reversed in the output of `predict()` and `predict_interval()`.\n        **WARNING: This argument is newly introduced and requires special attention. It\n        is still experimental and may undergo changes.**\n    fit_kwargs : dict, default `None`\n        Additional arguments to be passed to the `fit` method of the regressor.\n    binner_kwargs : dict, default `None`\n        Additional arguments to pass to the `QuantileBinner` used to discretize \n        the residuals into k bins according to the predicted values associated \n        with each residual. Available arguments are: `n_bins`, `method`, `subsample`,\n        `random_state` and `dtype`. Argument `method` is passed internally to the\n        fucntion `numpy.percentile`.\n        **New in version 0.14.0**\n    forecaster_id : str, int, default `None`\n        Name used as an identifier of the forecaster.\n    \n    Attributes\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n    lags : numpy ndarray\n        Lags used as predictors.\n    lags_names : list\n        Names of the lags used as predictors.\n    max_lag : int\n        Maximum lag included in `lags`.\n    window_features : list\n        Class or list of classes used to create window features.\n    window_features_names : list\n        Names of the window features to be included in the `X_train` matrix.\n    window_features_class_names : list\n        Names of the classes used to create the window features.\n    max_size_window_features : int\n        Maximum window size required by the window features.\n    window_size : int\n        The window size needed to create the predictors. It is calculated as the \n        maximum value between `max_lag` and `max_size_window_features`. If \n        differentiation is used, `window_size` is increased by n units equal to \n        the order of differentiation so that predictors can be generated correctly.\n    transformer_y : object transformer (preprocessor)\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and inverse_transform.\n        ColumnTransformers are not allowed since they do not have inverse_transform method.\n        The transformation is applied to `y` before training the forecaster.\n    transformer_exog : object transformer (preprocessor)\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\n        method. The resulting `sample_weight` cannot have negative values.\n    differentiation : int\n        Order of differencing applied to the time series before training the forecaster.\n        If `None`, no differencing is applied. The order of differentiation is the number\n        of times the differencing operation is applied to a time series. Differencing\n        involves computing the differences between consecutive data points in the series.\n        Differentiation is reversed in the output of `predict()` and `predict_interval()`.\n        **WARNING: This argument is newly introduced and requires special attention. It\n        is still experimental and may undergo changes.**\n        **New in version 0.10.0**\n    binner : sklearn.preprocessing.KBinsDiscretizer\n        `KBinsDiscretizer` used to discretize residuals into k bins according \n        to the predicted values associated with each residual.\n        **New in version 0.12.0**\n    binner_intervals_ : dict\n        Intervals used to discretize residuals into k bins according to the predicted\n        values associated with each residual.\n        **New in version 0.12.0**\n    binner_kwargs : dict\n        Additional arguments to pass to the `QuantileBinner` used to discretize \n        the residuals into k bins according to the predicted values associated \n        with each residual. Available arguments are: `n_bins`, `method`, `subsample`,\n        `random_state` and `dtype`. Argument `method` is passed internally to the\n        fucntion `numpy.percentile`.\n        **New in version 0.14.0**\n    source_code_weight_func : str\n        Source code of the custom function used to create weights.\n    differentiation : int\n        Order of differencing applied to the time series before training the \n        forecaster.\n    differentiator : TimeSeriesDifferentiator\n        Skforecast object used to differentiate the time series.\n    last_window_ : pandas DataFrame\n        This window represents the most recent data observed by the predictor\n        during its training phase. It contains the values needed to predict the\n        next step immediately after the training data. These values are stored\n        in the original scale of the time series before undergoing any transformations\n        or differentiation. When `differentiation` parameter is specified, the\n        dimensions of the `last_window_` are expanded as many values as the order\n        of differentiation. For example, if `lags` = 7 and `differentiation` = 1,\n        `last_window_` will have 8 values.\n    index_type_ : type\n        Type of index of the input used in training.\n    index_freq_ : str\n        Frequency of Index of the input used in training.\n    training_range_ : pandas Index\n        First and last values of index of the data used during training.\n    exog_in_ : bool\n        If the forecaster has been trained using exogenous variable/s.\n    exog_names_in_ : list\n        Names of the exogenous variables used during training.\n    exog_type_in_ : type\n        Type of exogenous data (pandas Series or DataFrame) used in training.\n    exog_dtypes_in_ : dict\n        Type of each exogenous variable/s used in training. If `transformer_exog` \n        is used, the dtypes are calculated before the transformation.\n    X_train_window_features_names_out_ : list\n        Names of the window features included in the matrix `X_train` created\n        internally for training.\n    X_train_exog_names_out_ : list\n        Names of the exogenous variables included in the matrix `X_train` created\n        internally for training. It can be different from `exog_names_in_` if\n        some exogenous variables are transformed during the training process.\n    X_train_features_names_out_ : list\n        Names of columns of the matrix created internally for training.\n    fit_kwargs : dict\n        Additional arguments to be passed to the `fit` method of the regressor.\n    in_sample_residuals_ : numpy ndarray\n        Residuals of the model when predicting training data. Only stored up to\n        10_000 values. If `transformer_y` is not `None`, residuals are stored in\n        the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation.\n    in_sample_residuals_by_bin_ : dict\n        In sample residuals binned according to the predicted value each residual\n        is associated with. If `transformer_y` is not `None`, residuals are stored\n        in the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n        **New in version 0.14.0**\n    out_sample_residuals_ : numpy ndarray\n        Residuals of the model when predicting non training data. Only stored up to\n        10_000 values. If `transformer_y` is not `None`, residuals are stored in\n        the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation.\n    out_sample_residuals_by_bin_ : dict\n        Out of sample residuals binned according to the predicted value each residual\n        is associated with. If `transformer_y` is not `None`, residuals are stored\n        in the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n        **New in version 0.12.0**\n    creation_date : str\n        Date of creation.\n    is_fitted : bool\n        Tag to identify if the regressor has been fitted (trained).\n    fit_date : str\n        Date of last fit.\n    skforecast_version : str\n        Version of skforecast library used to create the forecaster.\n    python_version : str\n        Version of python used to create the forecaster.\n    forecaster_id : str, int\n        Name used as an identifier of the forecaster.\n    \n    \"\"\"\n\n    def __init__(\n        self,\n        regressor: object,\n        lags: Optional[Union[int, list, np.ndarray, range]] = None,\n        window_features: Optional[Union[object, list]] = None,\n        transformer_y: Optional[object] = None,\n        transformer_exog: Optional[object] = None,\n        weight_func: Optional[Callable] = None,\n        differentiation: Optional[int] = None,\n        fit_kwargs: Optional[dict] = None,\n        binner_kwargs: Optional[dict] = None,\n        forecaster_id: Optional[Union[str, int]] = None\n    ) -> None:\n        \n        self.regressor                          = copy(regressor)\n        self.transformer_y                      = transformer_y\n        self.transformer_exog                   = transformer_exog\n        self.weight_func                        = weight_func\n        self.source_code_weight_func            = None\n        self.differentiation                    = differentiation\n        self.differentiator                     = None\n        self.last_window_                       = None\n        self.index_type_                        = None\n        self.index_freq_                        = None\n        self.training_range_                    = None\n        self.exog_in_                           = False\n        self.exog_names_in_                     = None\n        self.exog_type_in_                      = None\n        self.exog_dtypes_in_                    = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_            = None\n        self.X_train_features_names_out_        = None\n        self.in_sample_residuals_               = None\n        self.out_sample_residuals_              = None\n        self.in_sample_residuals_by_bin_        = None\n        self.out_sample_residuals_by_bin_       = None\n        self.creation_date                      = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.is_fitted                          = False\n        self.fit_date                           = None\n        self.skforecast_version                 = skforecast.__version__\n        self.python_version                     = sys.version.split(\" \")[0]\n        self.forecaster_id                      = forecaster_id\n\n        self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\n        self.window_features, self.window_features_names, self.max_size_window_features = (\n            initialize_window_features(window_features)\n        )\n        if self.window_features is None and self.lags is None:\n            raise ValueError(\n                \"At least one of the arguments `lags` or `window_features` \"\n                \"must be different from None. This is required to create the \"\n                \"predictors used in training the forecaster.\"\n            )\n        \n        self.window_size = max(\n            [ws for ws in [self.max_lag, self.max_size_window_features] \n             if ws is not None]\n        )\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [\n                type(wf).__name__ for wf in self.window_features\n            ] \n\n        self.binner_kwargs = binner_kwargs\n        if binner_kwargs is None:\n            self.binner_kwargs = {\n                'n_bins': 10, 'method': 'linear', 'subsample': 200000,\n                'random_state': 789654, 'dtype': np.float64\n            }\n        self.binner = QuantileBinner(**self.binner_kwargs)\n        self.binner_intervals_ = None\n\n        if self.differentiation is not None:\n            if not isinstance(differentiation, int) or differentiation < 1:\n                raise ValueError(\n                    f\"Argument `differentiation` must be an integer equal to or \"\n                    f\"greater than 1. Got {differentiation}.\"\n                )\n            self.window_size += self.differentiation\n            self.differentiator = TimeSeriesDifferentiator(\n                order=self.differentiation, window_size=self.window_size\n            )\n\n        self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n            forecaster_name = type(self).__name__, \n            regressor       = regressor, \n            weight_func     = weight_func, \n            series_weights  = None\n        )\n\n        self.fit_kwargs = check_select_fit_kwargs(\n                              regressor  = regressor,\n                              fit_kwargs = fit_kwargs\n                          )\n\n    def __repr__(\n        self\n    ) -> str:\n        \"\"\"\n        Information displayed when a ForecasterRecursive object is printed.\n        \"\"\"\n\n        (\n            params,\n            _,\n            _,\n            exog_names_in_,\n            _,\n        ) = self._preprocess_repr(\n                regressor      = self.regressor,\n                exog_names_in_ = self.exog_names_in_\n            )\n        \n        params = self._format_text_repr(params)\n        exog_names_in_ = self._format_text_repr(exog_names_in_)\n\n        info = (\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"{type(self).__name__} \\n\"\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"Regressor: {type(self.regressor).__name__} \\n\"\n            f\"Lags: {self.lags} \\n\"\n            f\"Window features: {self.window_features_names} \\n\"\n            f\"Window size: {self.window_size} \\n\"\n            f\"Exogenous included: {self.exog_in_} \\n\"\n            f\"Exogenous names: {exog_names_in_} \\n\"\n            f\"Transformer for y: {self.transformer_y} \\n\"\n            f\"Transformer for exog: {self.transformer_exog} \\n\"\n            f\"Weight function included: {True if self.weight_func is not None else False} \\n\"\n            f\"Differentiation order: {self.differentiation} \\n\"\n            f\"Training range: {self.training_range_.to_list() if self.is_fitted else None} \\n\"\n            f\"Training index type: {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None} \\n\"\n            f\"Training index frequency: {self.index_freq_ if self.is_fitted else None} \\n\"\n            f\"Regressor parameters: {params} \\n\"\n            f\"fit_kwargs: {self.fit_kwargs} \\n\"\n            f\"Creation date: {self.creation_date} \\n\"\n            f\"Last fit date: {self.fit_date} \\n\"\n            f\"Skforecast version: {self.skforecast_version} \\n\"\n            f\"Python version: {self.python_version} \\n\"\n            f\"Forecaster id: {self.forecaster_id} \\n\"\n        )\n\n        return info\n\n\n    def _repr_html_(self):\n        \"\"\"\n        HTML representation of the object.\n        The \"General Information\" section is expanded by default.\n        \"\"\"\n\n        (\n            params,\n            _,\n            _,\n            exog_names_in_,\n            _,\n        ) = self._preprocess_repr(\n                regressor      = self.regressor,\n                exog_names_in_ = self.exog_names_in_\n            )\n\n        style, unique_id = self._get_style_repr_html(self.is_fitted)\n        \n        content = f\"\"\"\n        <div class=\"container-{unique_id}\">\n            <h2>{type(self).__name__}</h2>\n            <details open>\n                <summary>General Information</summary>\n                <ul>\n                    <li><strong>Regressor:</strong> {type(self.regressor).__name__}</li>\n                    <li><strong>Lags:</strong> {self.lags}</li>\n                    <li><strong>Window features:</strong> {self.window_features_names}</li>\n                    <li><strong>Window size:</strong> {self.window_size}</li>\n                    <li><strong>Exogenous included:</strong> {self.exog_in_}</li>\n                    <li><strong>Weight function included:</strong> {self.weight_func is not None}</li>\n                    <li><strong>Differentiation order:</strong> {self.differentiation}</li>\n                    <li><strong>Creation date:</strong> {self.creation_date}</li>\n                    <li><strong>Last fit date:</strong> {self.fit_date}</li>\n                    <li><strong>Skforecast version:</strong> {self.skforecast_version}</li>\n                    <li><strong>Python version:</strong> {self.python_version}</li>\n                    <li><strong>Forecaster id:</strong> {self.forecaster_id}</li>\n                </ul>\n            </details>\n            <details>\n                <summary>Exogenous Variables</summary>\n                <ul>\n                    {exog_names_in_}\n                </ul>\n            </details>\n            <details>\n                <summary>Data Transformations</summary>\n                <ul>\n                    <li><strong>Transformer for y:</strong> {self.transformer_y}</li>\n                    <li><strong>Transformer for exog:</strong> {self.transformer_exog}</li>\n                </ul>\n            </details>\n            <details>\n                <summary>Training Information</summary>\n                <ul>\n                    <li><strong>Training range:</strong> {self.training_range_.to_list() if self.is_fitted else 'Not fitted'}</li>\n                    <li><strong>Training index type:</strong> {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted'}</li>\n                    <li><strong>Training index frequency:</strong> {self.index_freq_ if self.is_fitted else 'Not fitted'}</li>\n                </ul>\n            </details>\n            <details>\n                <summary>Regressor Parameters</summary>\n                <ul>\n                    {params}\n                </ul>\n            </details>\n            <details>\n                <summary>Fit Kwargs</summary>\n                <ul>\n                    {self.fit_kwargs}\n                </ul>\n            </details>\n            <p>\n                <a href=\"https://skforecast.org/{skforecast.__version__}/api/forecasterrecursive.html\">&#128712 <strong>API Reference</strong></a>\n                &nbsp;&nbsp;\n                <a href=\"https://skforecast.org/{skforecast.__version__}/user_guides/autoregresive-forecaster.html\">&#128462 <strong>User Guide</strong></a>\n            </p>\n        </div>\n        \"\"\"\n\n        # Return the combined style and content\n        return style + content\n\n\n    def _create_lags(\n        self,\n        y: np.ndarray,\n        X_as_pandas: bool = False,\n        train_index: Optional[pd.Index] = None\n    ) -> Tuple[Optional[Union[np.ndarray, pd.DataFrame]], np.ndarray]:\n        \"\"\"\n        Create the lagged values and their target variable from a time series.\n        \n        Note that the returned matrix `X_data` contains the lag 1 in the first \n        column, the lag 2 in the in the second column and so on.\n        \n        Parameters\n        ----------\n        y : numpy ndarray\n            Training time series values.\n        X_as_pandas : bool, default `False`\n            If `True`, the returned matrix `X_data` is a pandas DataFrame.\n        train_index : pandas Index, default `None`\n            Index of the training data. It is used to create the pandas DataFrame\n            `X_data` when `X_as_pandas` is `True`.\n\n        Returns\n        -------\n        X_data : numpy ndarray, pandas DataFrame, None\n            Lagged values (predictors).\n        y_data : numpy ndarray\n            Values of the time series related to each row of `X_data`.\n        \n        \"\"\"\n\n        X_data = None\n        if self.lags is not None:\n            n_rows = len(y) - self.window_size\n            X_data = np.full(\n                shape=(n_rows, len(self.lags)), fill_value=np.nan, order='F', dtype=float\n            )\n            for i, lag in enumerate(self.lags):\n                X_data[:, i] = y[self.window_size - lag: -lag]\n\n            if X_as_pandas:\n                X_data = pd.DataFrame(\n                             data    = X_data,\n                             columns = self.lags_names,\n                             index   = train_index\n                         )\n\n        y_data = y[self.window_size:]\n\n        return X_data, y_data\n\n\n    def _create_window_features(\n        self, \n        y: pd.Series,\n        train_index: pd.Index,\n        X_as_pandas: bool = False,\n    ) -> Tuple[list, list]:\n        \"\"\"\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        train_index : pandas Index\n            Index of the training data. It is used to create the pandas DataFrame\n            `X_train_window_features` when `X_as_pandas` is `True`.\n        X_as_pandas : bool, default `False`\n            If `True`, the returned matrix `X_train_window_features` is a \n            pandas DataFrame.\n\n        Returns\n        -------\n        X_train_window_features : list\n            List of numpy ndarrays or pandas DataFrames with the window features.\n        X_train_window_features_names_out_ : list\n            Names of the window features.\n        \n        \"\"\"\n\n        len_train_index = len(train_index)\n        X_train_window_features = []\n        X_train_window_features_names_out_ = []\n        for wf in self.window_features:\n            X_train_wf = wf.transform_batch(y)\n            if not isinstance(X_train_wf, pd.DataFrame):\n                raise TypeError(\n                    (f\"The method `transform_batch` of {type(wf).__name__} \"\n                     f\"must return a pandas DataFrame.\")\n                )\n            X_train_wf = X_train_wf.iloc[-len_train_index:]\n            if not len(X_train_wf) == len_train_index:\n                raise ValueError(\n                    (f\"The method `transform_batch` of {type(wf).__name__} \"\n                     f\"must return a DataFrame with the same number of rows as \"\n                     f\"the input time series - `window_size`: {len_train_index}.\")\n                )\n            if not (X_train_wf.index == train_index).all():\n                raise ValueError(\n                    (f\"The method `transform_batch` of {type(wf).__name__} \"\n                     f\"must return a DataFrame with the same index as \"\n                     f\"the input time series - `window_size`.\")\n                )\n            \n            X_train_window_features_names_out_.extend(X_train_wf.columns)\n            if not X_as_pandas:\n                X_train_wf = X_train_wf.to_numpy()     \n            X_train_window_features.append(X_train_wf)\n\n        return X_train_window_features, X_train_window_features_names_out_\n\n\n    def _create_train_X_y(\n        self,\n        y: pd.Series,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n    ) -> Tuple[pd.DataFrame, pd.Series, list, list, list, list, dict]:\n        \"\"\"\n        Create training matrices from univariate time series and exogenous\n        variables.\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors).\n        y_train : pandas Series\n            Values of the time series related to each row of `X_train`.\n        exog_names_in_ : list\n            Names of the exogenous variables used during training.\n        X_train_window_features_names_out_ : list\n            Names of the window features included in the matrix `X_train` created\n            internally for training.\n        X_train_exog_names_out_ : list\n            Names of the exogenous variables included in the matrix `X_train` created\n            internally for training. It can be different from `exog_names_in_` if\n            some exogenous variables are transformed during the training process.\n        X_train_features_names_out_ : list\n            Names of the columns of the matrix created internally for training.\n        exog_dtypes_in_ : dict\n            Type of each exogenous variable/s used in training. If `transformer_exog` \n            is used, the dtypes are calculated before the transformation.\n        \n        \"\"\"\n\n        check_y(y=y)\n        y = input_to_frame(data=y, input_name='y')\n\n        if len(y) <= self.window_size:\n            raise ValueError(\n                (f\"Length of `y` must be greater than the maximum window size \"\n                 f\"needed by the forecaster.\\n\"\n                 f\"    Length `y`: {len(y)}.\\n\"\n                 f\"    Max window size: {self.window_size}.\\n\"\n                 f\"    Lags window size: {self.max_lag}.\\n\"\n                 f\"    Window features window size: {self.max_size_window_features}.\")\n            )\n\n        fit_transformer = False if self.is_fitted else True\n        y = transform_dataframe(\n                df                = y, \n                transformer       = self.transformer_y,\n                fit               = fit_transformer,\n                inverse_transform = False,\n            )\n        y_values, y_index = preprocess_y(y=y)\n        train_index = y_index[self.window_size:]\n\n        if self.differentiation is not None:\n            if not self.is_fitted:\n                y_values = self.differentiator.fit_transform(y_values)\n            else:\n                differentiator = copy(self.differentiator)\n                y_values = differentiator.fit_transform(y_values)\n\n        exog_names_in_ = None\n        exog_dtypes_in_ = None\n        categorical_features = False\n        if exog is not None:\n            check_exog(exog=exog, allow_nan=True)\n            exog = input_to_frame(data=exog, input_name='exog')\n\n            len_y = len(y_values)\n            len_train_index = len(train_index)\n            len_exog = len(exog)\n            if not len_exog == len_y and not len_exog == len_train_index:\n                raise ValueError(\n                    f\"Length of `exog` must be equal to the length of `y` (if index is \"\n                    f\"fully aligned) or length of `y` - `window_size` (if `exog` \"\n                    f\"starts after the first `window_size` values).\\n\"\n                    f\"    `exog`              : ({exog.index[0]} -- {exog.index[-1]})  (n={len_exog})\\n\"\n                    f\"    `y`                 : ({y.index[0]} -- {y.index[-1]})  (n={len_y})\\n\"\n                    f\"    `y` - `window_size` : ({train_index[0]} -- {train_index[-1]})  (n={len_train_index})\"\n                )\n\n            exog_names_in_ = exog.columns.to_list()\n            exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = fit_transformer,\n                       inverse_transform = False\n                   )\n\n            check_exog_dtypes(exog, call_check_exog=True)\n            categorical_features = (\n                exog.select_dtypes(include=np.number).shape[1] != exog.shape[1]\n            )\n\n            _, exog_index = preprocess_exog(exog=exog, return_values=False)\n            if len_exog == len_y:\n                if not (exog_index == y_index).all():\n                    raise ValueError(\n                        \"When `exog` has the same length as `y`, the index of \"\n                        \"`exog` must be aligned with the index of `y` \"\n                        \"to ensure the correct alignment of values.\"\n                    )\n                # The first `self.window_size` positions have to be removed from \n                # exog since they are not in X_train.\n                exog = exog.iloc[self.window_size:, ]\n            else:\n                if not (exog_index == train_index).all():\n                    raise ValueError(\n                        \"When `exog` doesn't contain the first `window_size` observations, \"\n                        \"the index of `exog` must be aligned with the index of `y` minus \"\n                        \"the first `window_size` observations to ensure the correct \"\n                        \"alignment of values.\"\n                    )\n            \n        X_train = []\n        X_train_features_names_out_ = []\n        X_as_pandas = True if categorical_features else False\n\n        X_train_lags, y_train = self._create_lags(\n            y=y_values, X_as_pandas=X_as_pandas, train_index=train_index\n        )\n        if X_train_lags is not None:\n            X_train.append(X_train_lags)\n            X_train_features_names_out_.extend(self.lags_names)\n        \n        X_train_window_features_names_out_ = None\n        if self.window_features is not None:\n            n_diff = 0 if self.differentiation is None else self.differentiation\n            y_window_features = pd.Series(y_values[n_diff:], index=y_index[n_diff:])\n            X_train_window_features, X_train_window_features_names_out_ = (\n                self._create_window_features(\n                    y=y_window_features, X_as_pandas=X_as_pandas, train_index=train_index\n                )\n            )\n            X_train.extend(X_train_window_features)\n            X_train_features_names_out_.extend(X_train_window_features_names_out_)\n\n        X_train_exog_names_out_ = None\n        if exog is not None:\n            X_train_exog_names_out_ = exog.columns.to_list()  \n            if not X_as_pandas:\n                exog = exog.to_numpy()     \n            X_train_features_names_out_.extend(X_train_exog_names_out_)\n            X_train.append(exog)\n        \n        if len(X_train) == 1:\n            X_train = X_train[0]\n        else:\n            if X_as_pandas:\n                X_train = pd.concat(X_train, axis=1)\n            else:\n                X_train = np.concatenate(X_train, axis=1)\n                \n        if X_as_pandas:\n            X_train.index = train_index\n        else:\n            X_train = pd.DataFrame(\n                          data    = X_train,\n                          index   = train_index,\n                          columns = X_train_features_names_out_\n                      )\n        \n        y_train = pd.Series(\n                      data  = y_train,\n                      index = train_index,\n                      name  = 'y'\n                  )\n\n        return (\n            X_train,\n            y_train,\n            exog_names_in_,\n            X_train_window_features_names_out_,\n            X_train_exog_names_out_,\n            X_train_features_names_out_,\n            exog_dtypes_in_\n        )\n\n\n    def create_train_X_y(\n        self,\n        y: pd.Series,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n    ) -> Tuple[pd.DataFrame, pd.Series]:\n        \"\"\"\n        Create training matrices from univariate time series and exogenous\n        variables.\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors).\n        y_train : pandas Series\n            Values of the time series related to each row of `X_data`.\n        \n        \"\"\"\n\n        output = self._create_train_X_y(y=y, exog=exog)\n\n        X_train = output[0]\n        y_train = output[1]\n\n        return X_train, y_train\n\n\n    def _train_test_split_one_step_ahead(\n        self,\n        y: pd.Series,\n        initial_train_size: int,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n    ) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n        \"\"\"\n        Create matrices needed to train and test the forecaster for one-step-ahead\n        predictions.\n\n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        initial_train_size : int\n            Initial size of the training set. It is the number of observations used\n            to train the forecaster before making the first prediction.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n        \n        Returns\n        -------\n        X_train : pandas DataFrame\n            Predictor values used to train the model.\n        y_train : pandas Series\n            Target values related to each row of `X_train`.\n        X_test : pandas DataFrame\n            Predictor values used to test the model.\n        y_test : pandas Series\n            Target values related to each row of `X_test`.\n        \n        \"\"\"\n\n        is_fitted = self.is_fitted\n        self.is_fitted = False\n        X_train, y_train, *_ = self._create_train_X_y(\n            y    = y.iloc[: initial_train_size],\n            exog = exog.iloc[: initial_train_size] if exog is not None else None\n        )\n\n        test_init = initial_train_size - self.window_size\n        self.is_fitted = True\n        X_test, y_test, *_ = self._create_train_X_y(\n            y    = y.iloc[test_init:],\n            exog = exog.iloc[test_init:] if exog is not None else None\n        )\n\n        self.is_fitted = is_fitted\n\n        return X_train, y_train, X_test, y_test\n\n\n    def create_sample_weights(\n        self,\n        X_train: pd.DataFrame,\n    ) -> np.ndarray:\n        \"\"\"\n        Crate weights for each observation according to the forecaster's attribute\n        `weight_func`.\n\n        Parameters\n        ----------\n        X_train : pandas DataFrame\n            Dataframe created with the `create_train_X_y` method, first return.\n\n        Returns\n        -------\n        sample_weight : numpy ndarray\n            Weights to use in `fit` method.\n\n        \"\"\"\n\n        sample_weight = None\n\n        if self.weight_func is not None:\n            sample_weight = self.weight_func(X_train.index)\n\n        if sample_weight is not None:\n            if np.isnan(sample_weight).any():\n                raise ValueError(\n                    \"The resulting `sample_weight` cannot have NaN values.\"\n                )\n            if np.any(sample_weight < 0):\n                raise ValueError(\n                    \"The resulting `sample_weight` cannot have negative values.\"\n                )\n            if np.sum(sample_weight) == 0:\n                raise ValueError(\n                    (\"The resulting `sample_weight` cannot be normalized because \"\n                     \"the sum of the weights is zero.\")\n                )\n\n        return sample_weight\n\n\n    def fit(\n        self,\n        y: pd.Series,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        store_last_window: bool = True,\n        store_in_sample_residuals: bool = True,\n        random_state: int = 123\n    ) -> None:\n        \"\"\"\n        Training Forecaster.\n\n        Additional arguments to be passed to the `fit` method of the regressor \n        can be added with the `fit_kwargs` argument when initializing the forecaster.\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned so\n            that y[i] is regressed on exog[i].\n        store_last_window : bool, default `True`\n            Whether or not to store the last window (`last_window_`) of training data.\n        store_in_sample_residuals : bool, default `True`\n            If `True`, in-sample residuals will be stored in the forecaster object\n            after fitting (`in_sample_residuals_` attribute).\n        random_state : int, default `123`\n            Set a seed for the random generator so that the stored sample \n            residuals are always deterministic.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        # Reset values in case the forecaster has already been fitted.\n        self.last_window_                       = None\n        self.index_type_                        = None\n        self.index_freq_                        = None\n        self.training_range_                    = None\n        self.exog_in_                           = False\n        self.exog_names_in_                     = None\n        self.exog_type_in_                      = None\n        self.exog_dtypes_in_                    = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_            = None\n        self.X_train_features_names_out_        = None\n        self.in_sample_residuals_               = None\n        self.is_fitted                          = False\n        self.fit_date                           = None\n\n        (\n            X_train,\n            y_train,\n            exog_names_in_,\n            X_train_window_features_names_out_,\n            X_train_exog_names_out_,\n            X_train_features_names_out_,\n            exog_dtypes_in_\n        ) = self._create_train_X_y(y=y, exog=exog)\n        sample_weight = self.create_sample_weights(X_train=X_train)\n\n        if sample_weight is not None:\n            self.regressor.fit(\n                X             = X_train,\n                y             = y_train,\n                sample_weight = sample_weight,\n                **self.fit_kwargs\n            )\n        else:\n            self.regressor.fit(X=X_train, y=y_train, **self.fit_kwargs)\n\n        self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n        self.X_train_features_names_out_ = X_train_features_names_out_\n\n        self.is_fitted = True\n        self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.training_range_ = preprocess_y(y=y, return_values=False)[1][[0, -1]]\n        self.index_type_ = type(X_train.index)\n        if isinstance(X_train.index, pd.DatetimeIndex):\n            self.index_freq_ = X_train.index.freqstr\n        else: \n            self.index_freq_ = X_train.index.step\n\n        if exog is not None:\n            self.exog_in_ = True\n            self.exog_type_in_ = type(exog)\n            self.exog_names_in_ = exog_names_in_\n            self.exog_dtypes_in_ = exog_dtypes_in_\n            self.X_train_exog_names_out_ = X_train_exog_names_out_\n\n        # This is done to save time during fit in functions such as backtesting()\n        if store_in_sample_residuals:\n            self._binning_in_sample_residuals(\n                y_true       = y_train.to_numpy(),\n                y_pred       = self.regressor.predict(X_train).ravel(),\n                random_state = random_state\n            )\n\n        # The last time window of training data is stored so that lags needed as\n        # predictors in the first iteration of `predict()` can be calculated. It\n        # also includes the values need to calculate the diferenctiation.\n        if store_last_window:\n            self.last_window_ = (\n                y.iloc[-self.window_size:]\n                .copy()\n                .to_frame(name=y.name if y.name is not None else 'y')\n            )\n\n\n    def _binning_in_sample_residuals(\n        self,\n        y_true: np.ndarray,\n        y_pred: np.ndarray,\n        random_state: int = 123\n    ) -> None:\n        \"\"\"\n        Binning residuals according to the predicted value each residual is\n        associated with. First a skforecast.preprocessing.QuantileBinner object\n        is fitted to the predicted values. Then, residuals are binned according\n        to the predicted value each residual is associated with. Residuals are\n        stored in the forecaster object as `in_sample_residuals_` and\n        `in_sample_residuals_by_bin_`.\n        If `transformer_y` is not `None`, `y_true` and `y_pred` are transformed\n        before calculating residuals. If `differentiation` is not `None`, `y_true`\n        and `y_pred` are differentiated before calculating residuals. If both,\n        `transformer_y` and `differentiation` are not `None`, transformation is\n        done before differentiation. The number of residuals stored per bin is\n        limited to  `10_000 // self.binner.n_bins_`. The total number of residuals\n        stored is `10_000`.\n        **New in version 0.14.0**\n\n        Parameters\n        ----------\n        y_true : numpy ndarray\n            True values of the time series.\n        y_pred : numpy ndarray\n            Predicted values of the time series.\n        random_state : int, default `123`\n            Set a seed for the random generator so that the stored sample \n            residuals are always deterministic.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        data = pd.DataFrame({'prediction': y_pred, 'residuals': (y_true - y_pred)})\n        data['bin'] = self.binner.fit_transform(y_pred).astype(int)\n        self.in_sample_residuals_by_bin_ = (\n            data.groupby('bin')['residuals'].apply(np.array).to_dict()\n        )\n\n        rng = np.random.default_rng(seed=random_state)\n        max_sample = 10_000 // self.binner.n_bins_\n        for k, v in self.in_sample_residuals_by_bin_.items():\n            \n            if len(v) > max_sample:\n                sample = v[rng.integers(low=0, high=len(v), size=max_sample)]\n                self.in_sample_residuals_by_bin_[k] = sample\n\n        self.in_sample_residuals_ = np.concatenate(list(\n            self.in_sample_residuals_by_bin_.values()\n        ))\n\n        self.binner_intervals_ = self.binner.intervals_\n\n\n    def _create_predict_inputs(\n        self,\n        steps: Union[int, str, pd.Timestamp], \n        last_window: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        predict_boot: bool = False,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = False,\n        check_inputs: bool = True,\n    ) -> Tuple[np.ndarray, Optional[np.ndarray], pd.Index, int]:\n        \"\"\"\n        Create the inputs needed for the first iteration of the prediction \n        process. As this is a recursive process, the last window is updated at \n        each iteration of the prediction process.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        predict_boot : bool, default `False`\n            If `True`, residuals are returned to generate bootstrapping predictions.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n\n        Returns\n        -------\n        last_window_values : numpy ndarray\n            Series values used to create the predictors needed in the first \n            iteration of the prediction (t + 1).\n        exog_values : numpy ndarray, None\n            Exogenous variable/s included as predictor/s.\n        prediction_index : pandas Index\n            Index of the predictions.\n        steps: int\n            Number of future steps predicted.\n        \n        \"\"\"\n\n        if last_window is None:\n            last_window = self.last_window_\n\n        if self.is_fitted:\n            steps = date_to_index_position(\n                        index        = last_window.index,\n                        date_input   = steps,\n                        date_literal = 'steps'\n                    )\n\n        if check_inputs:\n            check_predict_input(\n                forecaster_name  = type(self).__name__,\n                steps            = steps,\n                is_fitted        = self.is_fitted,\n                exog_in_         = self.exog_in_,\n                index_type_      = self.index_type_,\n                index_freq_      = self.index_freq_,\n                window_size      = self.window_size,\n                last_window      = last_window,\n                exog             = exog,\n                exog_type_in_    = self.exog_type_in_,\n                exog_names_in_   = self.exog_names_in_,\n                interval         = None\n            )\n        \n            if predict_boot and not use_in_sample_residuals:\n                if not use_binned_residuals and self.out_sample_residuals_ is None:\n                    raise ValueError(\n                        \"`forecaster.out_sample_residuals_` is `None`. Use \"\n                        \"`use_in_sample_residuals=True` or the \"\n                        \"`set_out_sample_residuals()` method before predicting.\"\n                    )\n                if use_binned_residuals and self.out_sample_residuals_by_bin_ is None:\n                    raise ValueError(\n                        \"`forecaster.out_sample_residuals_by_bin_` is `None`. Use \"\n                        \"`use_in_sample_residuals=True` or the \"\n                        \"`set_out_sample_residuals()` method before predicting.\"\n                    )\n\n        last_window = last_window.iloc[-self.window_size:].copy()\n        last_window_values, last_window_index = preprocess_last_window(\n                                                    last_window = last_window\n                                                )\n\n        last_window_values = transform_numpy(\n                                 array             = last_window_values,\n                                 transformer       = self.transformer_y,\n                                 fit               = False,\n                                 inverse_transform = False\n                             )\n        if self.differentiation is not None:\n            last_window_values = self.differentiator.fit_transform(last_window_values)\n\n        if exog is not None:\n            exog = input_to_frame(data=exog, input_name='exog')\n            exog = exog.loc[:, self.exog_names_in_]\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n            check_exog_dtypes(exog=exog)\n            exog_values = exog.to_numpy()[:steps]\n        else:\n            exog_values = None\n\n        prediction_index = expand_index(\n                               index = last_window_index,\n                               steps = steps,\n                           )\n\n        return last_window_values, exog_values, prediction_index, steps\n\n\n    def _recursive_predict(\n        self,\n        steps: int,\n        last_window_values: np.ndarray,\n        exog_values: Optional[np.ndarray] = None,\n        residuals: Optional[Union[np.ndarray, dict]] = None,\n        use_binned_residuals: bool = False,\n    ) -> np.ndarray:\n        \"\"\"\n        Predict n steps ahead. It is an iterative process in which, each prediction,\n        is used as a predictor for the next step.\n        \n        Parameters\n        ----------\n        steps : int\n            Number of future steps predicted.\n        last_window_values : numpy ndarray\n            Series values used to create the predictors needed in the first \n            iteration of the prediction (t + 1).\n        exog_values : numpy ndarray, default `None`\n            Exogenous variable/s included as predictor/s.\n        residuals : numpy ndarray, dict, default `None`\n            Residuals used to generate bootstrapping predictions.\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : numpy ndarray\n            Predicted values.\n        \n        \"\"\"\n\n        n_lags = len(self.lags) if self.lags is not None else 0\n        n_window_features = (\n            len(self.X_train_window_features_names_out_)\n            if self.window_features is not None\n            else 0\n        )\n        n_exog = exog_values.shape[1] if exog_values is not None else 0\n\n        X = np.full(\n            shape=(n_lags + n_window_features + n_exog), fill_value=np.nan, dtype=float\n        )\n        predictions = np.full(shape=steps, fill_value=np.nan, dtype=float)\n        last_window = np.concatenate((last_window_values, predictions))\n\n        for i in range(steps):\n\n            if self.lags is not None:\n                X[:n_lags] = last_window[-self.lags - (steps - i)]\n            if self.window_features is not None:\n                X[n_lags : n_lags + n_window_features] = np.concatenate(\n                    [\n                        wf.transform(last_window[i : -(steps - i)])\n                        for wf in self.window_features\n                    ]\n                )\n            if exog_values is not None:\n                X[n_lags + n_window_features:] = exog_values[i]\n        \n            pred = self.regressor.predict(X.reshape(1, -1)).ravel()\n            \n            if residuals is not None:\n                if use_binned_residuals:\n                    predicted_bin = (\n                        self.binner.transform(pred).item()\n                    )\n                    step_residual = residuals[predicted_bin][i]\n                else:\n                    step_residual = residuals[i]\n                \n                pred += step_residual\n            \n            predictions[i] = pred[0]\n\n            # Update `last_window` values. The first position is discarded and \n            # the new prediction is added at the end.\n            last_window[-(steps - i)] = pred[0]\n\n        return predictions\n\n\n    def create_predict_X(\n        self,\n        steps: int,\n        last_window: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        Create the predictors needed to predict `steps` ahead. As it is a recursive\n        process, the predictors are created at each iteration of the prediction \n        process.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n\n        Returns\n        -------\n        X_predict : pandas DataFrame\n            Pandas DataFrame with the predictors for each step. The index \n            is the same as the prediction index.\n        \n        \"\"\"\n\n        last_window_values, exog_values, prediction_index, steps = (\n            self._create_predict_inputs(steps=steps, last_window=last_window, exog=exog)\n        )\n        \n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\", \n                message=\"X does not have valid feature names\", \n                category=UserWarning\n            )\n            predictions = self._recursive_predict(\n                              steps              = steps,\n                              last_window_values = last_window_values,\n                              exog_values        = exog_values\n                          )\n\n        X_predict = []\n        full_predictors = np.concatenate((last_window_values, predictions))\n\n        if self.lags is not None:\n            idx = np.arange(-steps, 0)[:, None] - self.lags\n            X_lags = full_predictors[idx + len(full_predictors)]\n            X_predict.append(X_lags)\n\n        if self.window_features is not None:\n            X_window_features = np.full(\n                shape      = (steps, len(self.X_train_window_features_names_out_)), \n                fill_value = np.nan, \n                order      = 'C',\n                dtype      = float\n            )\n            for i in range(steps):\n                X_window_features[i, :] = np.concatenate(\n                    [wf.transform(full_predictors[i:-(steps - i)]) \n                     for wf in self.window_features]\n                )\n            X_predict.append(X_window_features)\n\n        if exog is not None:\n            X_predict.append(exog_values)\n\n        X_predict = pd.DataFrame(\n                        data    = np.concatenate(X_predict, axis=1),\n                        columns = self.X_train_features_names_out_,\n                        index   = prediction_index\n                    )\n        \n        if self.transformer_y is not None or self.differentiation is not None:\n            warnings.warn(\n                \"The output matrix is in the transformed scale due to the \"\n                \"inclusion of transformations or differentiation in the Forecaster. \"\n                \"As a result, any predictions generated using this matrix will also \"\n                \"be in the transformed scale. Please refer to the documentation \"\n                \"for more details: \"\n                \"https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html\",\n                DataTransformationWarning\n            )\n\n        return X_predict\n\n\n    def predict(\n        self,\n        steps: Union[int, str, pd.Timestamp],\n        last_window: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        check_inputs: bool = True\n    ) -> pd.Series:\n        \"\"\"\n        Predict n steps ahead. It is an recursive process in which, each prediction,\n        is used as a predictor for the next step.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n\n        Returns\n        -------\n        predictions : pandas Series\n            Predicted values.\n        \n        \"\"\"\n\n        last_window_values, exog_values, prediction_index, steps = (\n            self._create_predict_inputs(\n                steps=steps,\n                last_window=last_window,\n                exog=exog,\n                check_inputs=check_inputs,\n            )\n        )\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\", \n                message=\"X does not have valid feature names\", \n                category=UserWarning\n            )\n            predictions = self._recursive_predict(\n                              steps              = steps,\n                              last_window_values = last_window_values,\n                              exog_values        = exog_values\n                          )\n\n        if self.differentiation is not None:\n            predictions = self.differentiator.inverse_transform_next_window(predictions)\n\n        predictions = transform_numpy(\n                          array             = predictions,\n                          transformer       = self.transformer_y,\n                          fit               = False,\n                          inverse_transform = True\n                      )\n\n        predictions = pd.Series(\n                          data  = predictions,\n                          index = prediction_index,\n                          name  = 'pred'\n                      )\n\n        return predictions\n\n\n    def predict_bootstrapping(\n        self,\n        steps: Union[int, str, pd.Timestamp],\n        last_window: Optional[pd.Series] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = False\n    ) -> pd.DataFrame:\n        \"\"\"\n        Generate multiple forecasting predictions using a bootstrapping process. \n        By sampling from a collection of past observed errors (the residuals),\n        each iteration of bootstrapping generates a different set of predictions. \n        See the Notes section for more information. \n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        boot_predictions : pandas DataFrame\n            Predictions generated by bootstrapping.\n            Shape: (steps, n_boot)\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n        Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n        \"\"\"\n\n        (\n            last_window_values,\n            exog_values,\n            prediction_index,\n            steps\n        ) = self._create_predict_inputs(\n            steps                   = steps, \n            last_window             = last_window, \n            exog                    = exog,\n            predict_boot            = True, \n            use_in_sample_residuals = use_in_sample_residuals,\n            use_binned_residuals    = use_binned_residuals\n        )\n\n        if use_in_sample_residuals:\n            residuals = self.in_sample_residuals_\n            residuals_by_bin = self.in_sample_residuals_by_bin_\n        else:\n            residuals = self.out_sample_residuals_\n            residuals_by_bin = self.out_sample_residuals_by_bin_\n\n        rng = np.random.default_rng(seed=random_state)\n        if use_binned_residuals:\n            sampled_residuals = {\n                k: v[rng.integers(low=0, high=len(v), size=(steps, n_boot))]\n                for k, v in residuals_by_bin.items()\n            }\n        else:\n            sampled_residuals = residuals[\n                rng.integers(low=0, high=len(residuals), size=(steps, n_boot))\n            ]\n        \n        boot_columns = []\n        boot_predictions = np.full(\n                               shape      = (steps, n_boot),\n                               fill_value = np.nan,\n                               order      = 'F',\n                               dtype      = float\n                           )\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\", \n                message=\"X does not have valid feature names\", \n                category=UserWarning\n            )\n            for i in range(n_boot):\n\n                if use_binned_residuals:\n                    boot_sampled_residuals = {\n                        k: v[:, i]\n                        for k, v in sampled_residuals.items()\n                    }\n                else:\n                    boot_sampled_residuals = sampled_residuals[:, i]\n\n                boot_columns.append(f\"pred_boot_{i}\")\n                boot_predictions[:, i] = self._recursive_predict(\n                    steps                = steps,\n                    last_window_values   = last_window_values,\n                    exog_values          = exog_values,\n                    residuals            = boot_sampled_residuals,\n                    use_binned_residuals = use_binned_residuals,\n                )\n\n        if self.differentiation is not None:\n            boot_predictions = (\n                self.differentiator.inverse_transform_next_window(boot_predictions)\n            )\n        \n        if self.transformer_y:\n            boot_predictions = np.apply_along_axis(\n                                   func1d            = transform_numpy,\n                                   axis              = 0,\n                                   arr               = boot_predictions,\n                                   transformer       = self.transformer_y,\n                                   fit               = False,\n                                   inverse_transform = True\n                               )\n\n        boot_predictions = pd.DataFrame(\n                               data    = boot_predictions,\n                               index   = prediction_index,\n                               columns = boot_columns\n                           )\n\n        return boot_predictions\n\n\n    def predict_interval(\n        self,\n        steps: Union[int, str, pd.Timestamp],\n        last_window: Optional[pd.Series] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        interval: list = [5, 95],\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = False\n    ) -> pd.DataFrame:\n        \"\"\"\n        Iterative process in which each prediction is used as a predictor\n        for the next step, and bootstrapping is used to estimate prediction\n        intervals. Both predictions and intervals are returned.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        interval : list, default `[5, 95]`\n            Confidence of the prediction interval estimated. Sequence of \n            percentiles to compute, which must be between 0 and 100 inclusive. \n            For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Values predicted by the forecaster and their estimated interval.\n\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n\n        check_interval(interval=interval)\n\n        boot_predictions = self.predict_bootstrapping(\n                               steps                   = steps,\n                               last_window             = last_window,\n                               exog                    = exog,\n                               n_boot                  = n_boot,\n                               random_state            = random_state,\n                               use_in_sample_residuals = use_in_sample_residuals,\n                               use_binned_residuals    = use_binned_residuals\n                           )\n\n        predictions = self.predict(\n                          steps        = steps,\n                          last_window  = last_window,\n                          exog         = exog,\n                          check_inputs = False\n                      )\n\n        interval = np.array(interval) / 100\n        predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n        predictions_interval.columns = ['lower_bound', 'upper_bound']\n        predictions = pd.concat((predictions, predictions_interval), axis=1)\n\n        return predictions\n\n\n    def predict_quantiles(\n        self,\n        steps: Union[int, str, pd.Timestamp],\n        last_window: Optional[pd.Series] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        quantiles: list = [0.05, 0.5, 0.95],\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = False\n    ) -> pd.DataFrame:\n        \"\"\"\n        Calculate the specified quantiles for each step. After generating \n        multiple forecasting predictions through a bootstrapping process, each \n        quantile is calculated for each step.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        quantiles : list, default `[0.05, 0.5, 0.95]`\n            Sequence of quantiles to compute, which must be between 0 and 1 \n            inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n            `quantiles = [0.05, 0.5, 0.95]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate quantiles.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot quantiles are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create prediction quantiles. If `False`, out of\n            sample residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Quantiles predicted by the forecaster.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n\n        check_interval(quantiles=quantiles)\n\n        boot_predictions = self.predict_bootstrapping(\n                               steps                   = steps,\n                               last_window             = last_window,\n                               exog                    = exog,\n                               n_boot                  = n_boot,\n                               random_state            = random_state,\n                               use_in_sample_residuals = use_in_sample_residuals,\n                               use_binned_residuals    = use_binned_residuals\n                           )\n\n        predictions = boot_predictions.quantile(q=quantiles, axis=1).transpose()\n        predictions.columns = [f'q_{q}' for q in quantiles]\n\n        return predictions\n\n\n    def predict_dist(\n        self,\n        steps: Union[int, str, pd.Timestamp],\n        distribution: object,\n        last_window: Optional[pd.Series] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = False\n    ) -> pd.DataFrame:\n        \"\"\"\n        Fit a given probability distribution for each step. After generating \n        multiple forecasting predictions through a bootstrapping process, each \n        step is fitted to the given distribution.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        distribution : Object\n            A distribution object from scipy.stats.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).  \n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Distribution parameters estimated for each step.\n\n        \"\"\"\n\n        boot_samples = self.predict_bootstrapping(\n                           steps                   = steps,\n                           last_window             = last_window,\n                           exog                    = exog,\n                           n_boot                  = n_boot,\n                           random_state            = random_state,\n                           use_in_sample_residuals = use_in_sample_residuals,\n                           use_binned_residuals    = use_binned_residuals\n                       )       \n\n        param_names = [p for p in inspect.signature(distribution._pdf).parameters\n                       if not p == 'x'] + [\"loc\", \"scale\"]\n        param_values = np.apply_along_axis(\n                           lambda x: distribution.fit(x),\n                           axis = 1,\n                           arr  = boot_samples\n                       )\n        predictions = pd.DataFrame(\n                          data    = param_values,\n                          columns = param_names,\n                          index   = boot_samples.index\n                      )\n\n        return predictions\n\n    def set_params(\n        self, \n        params: dict\n    ) -> None:\n        \"\"\"\n        Set new values to the parameters of the scikit learn model stored in the\n        forecaster.\n        \n        Parameters\n        ----------\n        params : dict\n            Parameters values.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        self.regressor = clone(self.regressor)\n        self.regressor.set_params(**params)\n\n    def set_fit_kwargs(\n        self, \n        fit_kwargs: dict\n    ) -> None:\n        \"\"\"\n        Set new values for the additional keyword arguments passed to the `fit` \n        method of the regressor.\n        \n        Parameters\n        ----------\n        fit_kwargs : dict\n            Dict of the form {\"argument\": new_value}.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n\n    def set_lags(\n        self, \n        lags: Optional[Union[int, list, np.ndarray, range]] = None\n    ) -> None:\n        \"\"\"\n        Set new value to the attribute `lags`. Attributes `lags_names`, \n        `max_lag` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        lags : int, list, numpy ndarray, range, default `None`\n            Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. \n        \n            - `int`: include lags from 1 to `lags` (included).\n            - `list`, `1d numpy ndarray` or `range`: include only lags present in \n            `lags`, all elements must be int.\n            - `None`: no lags are included as predictors. \n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        if self.window_features is None and lags is None:\n            raise ValueError(\n                \"At least one of the arguments `lags` or `window_features` \"\n                \"must be different from None. This is required to create the \"\n                \"predictors used in training the forecaster.\"\n            )\n        \n        self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\n        self.window_size = max(\n            [ws for ws in [self.max_lag, self.max_size_window_features] \n             if ws is not None]\n        )\n        if self.differentiation is not None:\n            self.window_size += self.differentiation\n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_window_features(\n        self, \n        window_features: Optional[Union[object, list]] = None\n    ) -> None:\n        \"\"\"\n        Set new value to the attribute `window_features`. Attributes \n        `max_size_window_features`, `window_features_names`, \n        `window_features_class_names` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        window_features : object, list, default `None`\n            Instance or list of instances used to create window features. Window features\n            are created from the original time series and are included as predictors.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        if window_features is None and self.lags is None:\n            raise ValueError(\n                \"At least one of the arguments `lags` or `window_features` \"\n                \"must be different from None. This is required to create the \"\n                \"predictors used in training the forecaster.\"\n            )\n        \n        self.window_features, self.window_features_names, self.max_size_window_features = (\n            initialize_window_features(window_features)\n        )\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [\n                type(wf).__name__ for wf in self.window_features\n            ] \n        self.window_size = max(\n            [ws for ws in [self.max_lag, self.max_size_window_features] \n             if ws is not None]\n        )\n        if self.differentiation is not None:\n            self.window_size += self.differentiation\n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_out_sample_residuals(\n        self,\n        y_true: Union[pd.Series, np.ndarray],\n        y_pred: Union[pd.Series, np.ndarray],\n        append: bool = False,\n        random_state: int = 123\n    ) -> None:\n        \"\"\"\n        Set new values to the attribute `out_sample_residuals_`. Out of sample\n        residuals are meant to be calculated using observations that did not\n        participate in the training process. `y_true` and `y_pred` are expected\n        to be in the original scale of the time series. Residuals are calculated\n        as `y_true` - `y_pred`, after applying the necessary transformations and\n        differentiations if the forecaster includes them (`self.transformer_y`\n        and `self.differentiation`). Two internal attributes are updated:\n\n        + `out_sample_residuals_`: residuals stored in a numpy ndarray.\n        + `out_sample_residuals_by_bin_`: residuals are binned according to the\n        predicted value they are associated with and stored in a dictionary, where\n        the keys are the  intervals of the predicted values and the values are\n        the residuals associated with that range. If a bin binning is empty, it\n        is filled with a random sample of residuals from other bins. This is done\n        to ensure that all bins have at least one residual and can be used in the\n        prediction process.\n\n        A total of 10_000 residuals are stored in the attribute `out_sample_residuals_`.\n        If the number of residuals is greater than 10_000, a random sample of\n        10_000 residuals is stored. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n        \n        Parameters\n        ----------\n        y_true : pandas Series, numpy ndarray, default `None`\n            True values of the time series from which the residuals have been\n            calculated.\n        y_pred : pandas Series, numpy ndarray, default `None`\n            Predicted values of the time series.\n        append : bool, default `False`\n            If `True`, new residuals are added to the once already stored in the\n            forecaster. If after appending the new residuals, the limit of\n            `10_000 // self.binner.n_bins_` values per bin is reached, a random\n            sample of residuals is stored.\n        random_state : int, default `123`\n            Sets a seed to the random sampling for reproducible output.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n\n        if not self.is_fitted:\n            raise NotFittedError(\n                \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n                \"arguments before using `set_out_sample_residuals()`.\"\n            )\n\n        if not isinstance(y_true, (np.ndarray, pd.Series)):\n            raise TypeError(\n                f\"`y_true` argument must be `numpy ndarray` or `pandas Series`. \"\n                f\"Got {type(y_true)}.\"\n            )\n        \n        if not isinstance(y_pred, (np.ndarray, pd.Series)):\n            raise TypeError(\n                f\"`y_pred` argument must be `numpy ndarray` or `pandas Series`. \"\n                f\"Got {type(y_pred)}.\"\n            )\n        \n        if len(y_true) != len(y_pred):\n            raise ValueError(\n                f\"`y_true` and `y_pred` must have the same length. \"\n                f\"Got {len(y_true)} and {len(y_pred)}.\"\n            )\n        \n        if isinstance(y_true, pd.Series) and isinstance(y_pred, pd.Series):\n            if not y_true.index.equals(y_pred.index):\n                raise ValueError(\n                    \"`y_true` and `y_pred` must have the same index.\"\n                )\n\n        if not isinstance(y_pred, np.ndarray):\n            y_pred = y_pred.to_numpy()\n\n        if not isinstance(y_true, np.ndarray):\n            y_true = y_true.to_numpy()\n\n        if self.transformer_y:\n            y_true = transform_numpy(\n                         array             = y_true,\n                         transformer       = self.transformer_y,\n                         fit               = False,\n                         inverse_transform = False\n                     )\n            y_pred = transform_numpy(\n                         array             = y_pred,\n                         transformer       = self.transformer_y,\n                         fit               = False,\n                         inverse_transform = False\n                     )\n        \n        if self.differentiation is not None:\n            differentiator = copy(self.differentiator)\n            differentiator.set_params(window_size=None)\n            y_true = differentiator.fit_transform(y_true)[self.differentiation:]\n            y_pred = differentiator.fit_transform(y_pred)[self.differentiation:]\n        \n        residuals = y_true - y_pred\n        data = pd.DataFrame({'prediction': y_pred, 'residuals': residuals})\n        data['bin'] = self.binner.transform(y_pred).astype(int)\n        residuals_by_bin = data.groupby('bin')['residuals'].apply(np.array).to_dict()\n\n        if append and self.out_sample_residuals_by_bin_ is not None:\n            for k, v in residuals_by_bin.items():\n                if k in self.out_sample_residuals_by_bin_:\n                    self.out_sample_residuals_by_bin_[k] = np.concatenate((\n                        self.out_sample_residuals_by_bin_[k], v)\n                    )\n                else:\n                    self.out_sample_residuals_by_bin_[k] = v\n        else:\n            self.out_sample_residuals_by_bin_ = residuals_by_bin\n\n        max_samples = 10_000 // self.binner.n_bins_\n        rng = np.random.default_rng(seed=random_state)\n        for k, v in self.out_sample_residuals_by_bin_.items():\n            if len(v) > max_samples:\n                sample = rng.choice(a=v, size=max_samples, replace=False)\n                self.out_sample_residuals_by_bin_[k] = sample\n\n        for k in self.in_sample_residuals_by_bin_.keys():\n            if k not in self.out_sample_residuals_by_bin_:\n                self.out_sample_residuals_by_bin_[k] = np.array([])\n\n        empty_bins = [\n            k for k, v in self.out_sample_residuals_by_bin_.items() \n            if len(v) == 0\n        ]\n        if empty_bins:\n            warnings.warn(\n                f\"The following bins have no out of sample residuals: {empty_bins}. \"\n                f\"No predicted values fall in the interval \"\n                f\"{[self.binner_intervals_[bin] for bin in empty_bins]}. \"\n                f\"Empty bins will be filled with a random sample of residuals.\"\n            )\n            for k in empty_bins:\n                self.out_sample_residuals_by_bin_[k] = rng.choice(\n                    a       = residuals,\n                    size    = max_samples,\n                    replace = True\n                )\n\n        self.out_sample_residuals_ = np.concatenate(list(\n                                         self.out_sample_residuals_by_bin_.values()\n                                     ))\n\n    def get_feature_importances(\n        self,\n        sort_importance: bool = True\n    ) -> pd.DataFrame:\n        \"\"\"\n        Return feature importances of the regressor stored in the forecaster.\n        Only valid when regressor stores internally the feature importances in the\n        attribute `feature_importances_` or `coef_`. Otherwise, returns `None`.\n\n        Parameters\n        ----------\n        sort_importance: bool, default `True`\n            If `True`, sorts the feature importances in descending order.\n\n        Returns\n        -------\n        feature_importances : pandas DataFrame\n            Feature importances associated with each predictor.\n\n        \"\"\"\n\n        if not self.is_fitted:\n            raise NotFittedError(\n                \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n                \"arguments before using `get_feature_importances()`.\"\n            )\n\n        if isinstance(self.regressor, Pipeline):\n            estimator = self.regressor[-1]\n        else:\n            estimator = self.regressor\n\n        if hasattr(estimator, 'feature_importances_'):\n            feature_importances = estimator.feature_importances_\n        elif hasattr(estimator, 'coef_'):\n            feature_importances = estimator.coef_\n        else:\n            warnings.warn(\n                f\"Impossible to access feature importances for regressor of type \"\n                f\"{type(estimator)}. This method is only valid when the \"\n                f\"regressor stores internally the feature importances in the \"\n                f\"attribute `feature_importances_` or `coef_`.\"\n            )\n            feature_importances = None\n\n        if feature_importances is not None:\n            feature_importances = pd.DataFrame({\n                                      'feature': self.X_train_features_names_out_,\n                                      'importance': feature_importances\n                                  })\n            if sort_importance:\n                feature_importances = feature_importances.sort_values(\n                                          by='importance', ascending=False\n                                      )\n\n        return feature_importances\n",
    "skforecast/metrics/metrics.py": "################################################################################\n#                                metrics                                       #\n#                                                                              #\n# This work by skforecast team is licensed under the BSD 3-Clause License.     #\n################################################################################\n# coding=utf-8\n\nfrom typing import Union, Callable\nimport numpy as np\nimport pandas as pd\nimport inspect\nfrom functools import wraps\nfrom sklearn.metrics import (\n    mean_squared_error,\n    mean_absolute_error,\n    mean_absolute_percentage_error,\n    mean_squared_log_error,\n    median_absolute_error,\n)\n\n\ndef _get_metric(metric: str) -> Callable:\n    \"\"\"\n    Get the corresponding scikit-learn function to calculate the metric.\n\n    Parameters\n    ----------\n    metric : str\n        Metric used to quantify the goodness of fit of the model.\n\n    Returns\n    -------\n    metric : Callable\n        scikit-learn function to calculate the desired metric.\n\n    \"\"\"\n\n    allowed_metrics = [\n        \"mean_squared_error\",\n        \"mean_absolute_error\",\n        \"mean_absolute_percentage_error\",\n        \"mean_squared_log_error\",\n        \"mean_absolute_scaled_error\",\n        \"root_mean_squared_scaled_error\",\n        \"median_absolute_error\",\n    ]\n\n    if metric not in allowed_metrics:\n        raise ValueError((f\"Allowed metrics are: {allowed_metrics}. Got {metric}.\"))\n\n    metrics = {\n        \"mean_squared_error\": mean_squared_error,\n        \"mean_absolute_error\": mean_absolute_error,\n        \"mean_absolute_percentage_error\": mean_absolute_percentage_error,\n        \"mean_squared_log_error\": mean_squared_log_error,\n        \"mean_absolute_scaled_error\": mean_absolute_scaled_error,\n        \"root_mean_squared_scaled_error\": root_mean_squared_scaled_error,\n        \"median_absolute_error\": median_absolute_error,\n    }\n\n    metric = add_y_train_argument(metrics[metric])\n\n    return metric\n\n\ndef add_y_train_argument(func: Callable) -> Callable:\n    \"\"\"\n    Add `y_train` argument to a function if it is not already present.\n\n    Parameters\n    ----------\n    func : callable\n        Function to which the argument is added.\n\n    Returns\n    -------\n    wrapper : callable\n        Function with `y_train` argument added.\n    \n    \"\"\"\n\n    sig = inspect.signature(func)\n    \n    if \"y_train\" in sig.parameters:\n        return func\n\n    new_params = list(sig.parameters.values()) + [\n        inspect.Parameter(\"y_train\", inspect.Parameter.KEYWORD_ONLY, default=None)\n    ]\n    new_sig = sig.replace(parameters=new_params)\n\n    @wraps(func)\n    def wrapper(*args, y_train=None, **kwargs):\n        return func(*args, **kwargs)\n    \n    wrapper.__signature__ = new_sig\n    \n    return wrapper\n\n\ndef mean_absolute_scaled_error(\n    y_true: Union[pd.Series, np.ndarray],\n    y_pred: Union[pd.Series, np.ndarray],\n    y_train: Union[list, pd.Series, np.ndarray],\n) -> float:\n    \"\"\"\n    Mean Absolute Scaled Error (MASE)\n\n    MASE is a scale-independent error metric that measures the accuracy of\n    a forecast. It is the mean absolute error of the forecast divided by the\n    mean absolute error of a naive forecast in the training set. The naive\n    forecast is the one obtained by shifting the time series by one period.\n    If y_train is a list of numpy arrays or pandas Series, it is considered\n    that each element is the true value of the target variable in the training\n    set for each time series. In this case, the naive forecast is calculated\n    for each time series separately.\n\n    Parameters\n    ----------\n    y_true : pandas Series, numpy ndarray\n        True values of the target variable.\n    y_pred : pandas Series, numpy ndarray\n        Predicted values of the target variable.\n    y_train : list, pandas Series, numpy ndarray\n        True values of the target variable in the training set. If `list`, it\n        is consider that each element is the true value of the target variable\n        in the training set for each time series.\n\n    Returns\n    -------\n    mase : float\n        MASE value.\n    \n    \"\"\"\n\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_true` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_pred` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_train, (list, pd.Series, np.ndarray)):\n        raise TypeError(\"`y_train` must be a list, pandas Series or numpy ndarray.\")\n    if isinstance(y_train, list):\n        for x in y_train:\n            if not isinstance(x, (pd.Series, np.ndarray)):\n                raise TypeError(\n                    (\"When `y_train` is a list, each element must be a pandas Series \"\n                     \"or numpy ndarray.\")\n                )\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"`y_true` and `y_pred` must have the same length.\")\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError(\"`y_true` and `y_pred` must have at least one element.\")\n\n    if isinstance(y_train, list):\n        naive_forecast = np.concatenate([np.diff(x) for x in y_train])\n    else:\n        naive_forecast = np.diff(y_train)\n\n    mase = np.mean(np.abs(y_true - y_pred)) / np.nanmean(np.abs(naive_forecast))\n\n    return mase\n\n\ndef root_mean_squared_scaled_error(\n    y_true: Union[pd.Series, np.ndarray],\n    y_pred: Union[pd.Series, np.ndarray],\n    y_train: Union[list, pd.Series, np.ndarray],\n) -> float:\n    \"\"\"\n    Root Mean Squared Scaled Error (RMSSE)\n\n    RMSSE is a scale-independent error metric that measures the accuracy of\n    a forecast. It is the root mean squared error of the forecast divided by\n    the root mean squared error of a naive forecast in the training set. The\n    naive forecast is the one obtained by shifting the time series by one period.\n    If y_train is a list of numpy arrays or pandas Series, it is considered\n    that each element is the true value of the target variable in the training\n    set for each time series. In this case, the naive forecast is calculated\n    for each time series separately.\n\n    Parameters\n    ----------\n    y_true : pandas Series, numpy ndarray\n        True values of the target variable.\n    y_pred : pandas Series, numpy ndarray\n        Predicted values of the target variable.\n    y_train : list, pandas Series, numpy ndarray\n        True values of the target variable in the training set. If list, it\n        is consider that each element is the true value of the target variable\n        in the training set for each time series.\n\n    Returns\n    -------\n    rmsse : float\n        RMSSE value.\n    \n    \"\"\"\n\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_true` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_pred` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_train, (list, pd.Series, np.ndarray)):\n        raise TypeError(\"`y_train` must be a list, pandas Series or numpy ndarray.\")\n    if isinstance(y_train, list):\n        for x in y_train:\n            if not isinstance(x, (pd.Series, np.ndarray)):\n                raise TypeError(\n                    (\"When `y_train` is a list, each element must be a pandas Series \"\n                     \"or numpy ndarray.\")\n                )\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"`y_true` and `y_pred` must have the same length.\")\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError(\"`y_true` and `y_pred` must have at least one element.\")\n\n    if isinstance(y_train, list):\n        naive_forecast = np.concatenate([np.diff(x) for x in y_train])\n    else:\n        naive_forecast = np.diff(y_train)\n    \n    rmsse = np.sqrt(np.mean((y_true - y_pred) ** 2)) / np.sqrt(np.nanmean(naive_forecast ** 2))\n    \n    return rmsse\n",
    "skforecast/direct/_forecaster_direct.py": "################################################################################\n#                               ForecasterDirect                               #\n#                                                                              #\n# This work by skforecast team is licensed under the BSD 3-Clause License.     #\n################################################################################\n# coding=utf-8\n\nfrom typing import Union, Tuple, Optional, Callable, Any\nimport warnings\nimport sys\nimport numpy as np\nimport pandas as pd\nimport inspect\nfrom copy import copy\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import clone\nfrom joblib import Parallel, delayed, cpu_count\n\nimport skforecast\nfrom ..base import ForecasterBase\nfrom ..exceptions import DataTransformationWarning\nfrom ..utils import (\n    initialize_lags,\n    initialize_window_features,\n    initialize_weights,\n    check_select_fit_kwargs,\n    check_y,\n    check_exog,\n    get_exog_dtypes,\n    check_exog_dtypes,\n    prepare_steps_direct,\n    check_predict_input,\n    check_interval,\n    preprocess_y,\n    preprocess_last_window,\n    preprocess_exog,\n    input_to_frame,\n    exog_to_direct,\n    exog_to_direct_numpy,\n    expand_index,\n    transform_numpy,\n    transform_dataframe,\n    select_n_jobs_fit_forecaster\n)\nfrom ..preprocessing import TimeSeriesDifferentiator\n\n\nclass ForecasterDirect(ForecasterBase):\n    \"\"\"\n    This class turns any regressor compatible with the scikit-learn API into a\n    autoregressive direct multi-step forecaster. A separate model is created for\n    each forecast time step. See documentation for more details.\n    \n    Parameters\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n    steps : int\n        Maximum number of future steps the forecaster will predict when using\n        method `predict()`. Since a different model is created for each step,\n        this value should be defined before training.\n    lags : int, list, numpy ndarray, range, default `None`\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n    \n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n        - `None`: no lags are included as predictors. \n    window_features : object, list, default `None`\n        Instance or list of instances used to create window features. Window features\n        are created from the original time series and are included as predictors.\n    transformer_y : object transformer (preprocessor), default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and inverse_transform.\n        ColumnTransformers are not allowed since they do not have inverse_transform method.\n        The transformation is applied to `y` before training the forecaster. \n    transformer_exog : object transformer (preprocessor), default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable, default `None`\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\n        method. The resulting `sample_weight` cannot have negative values.\n    differentiation : int, default `None`\n        Order of differencing applied to the time series before training the forecaster.\n        If `None`, no differencing is applied. The order of differentiation is the number\n        of times the differencing operation is applied to a time series. Differencing\n        involves computing the differences between consecutive data points in the series.\n        Differentiation is reversed in the output of `predict()` and `predict_interval()`.\n        **WARNING: This argument is newly introduced and requires special attention. It\n        is still experimental and may undergo changes.**\n    fit_kwargs : dict, default `None`\n        Additional arguments to be passed to the `fit` method of the regressor.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_fit_forecaster.\n    forecaster_id : str, int, default `None`\n        Name used as an identifier of the forecaster.\n    \n    Attributes\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n        An instance of this regressor is trained for each step. All of them \n        are stored in `self.regressors_`.\n    regressors_ : dict\n        Dictionary with regressors trained for each step. They are initialized \n        as a copy of `regressor`.\n    steps : int\n        Number of future steps the forecaster will predict when using method\n        `predict()`. Since a different model is created for each step, this value\n        should be defined before training.\n    lags : numpy ndarray\n        Lags used as predictors.\n    lags_names : list\n        Names of the lags used as predictors.\n    max_lag : int\n        Maximum lag included in `lags`.\n    window_features : list\n        Class or list of classes used to create window features.\n    window_features_names : list\n        Names of the window features to be included in the `X_train` matrix.\n    window_features_class_names : list\n        Names of the classes used to create the window features.\n    max_size_window_features : int\n        Maximum window size required by the window features.\n    window_size : int\n        The window size needed to create the predictors. It is calculated as the \n        maximum value between `max_lag` and `max_size_window_features`. If \n        differentiation is used, `window_size` is increased by n units equal to \n        the order of differentiation so that predictors can be generated correctly.\n    transformer_y : object transformer (preprocessor)\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and inverse_transform.\n        ColumnTransformers are not allowed since they do not have inverse_transform method.\n        The transformation is applied to `y` before training the forecaster.\n    transformer_exog : object transformer (preprocessor)\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\n        method. The resulting `sample_weight` cannot have negative values.\n    source_code_weight_func : str\n        Source code of the custom function used to create weights.\n    differentiation : int\n        Order of differencing applied to the time series before training the \n        forecaster.\n    differentiator : TimeSeriesDifferentiator\n        Skforecast object used to differentiate the time series.\n    last_window_ : pandas DataFrame\n        This window represents the most recent data observed by the predictor\n        during its training phase. It contains the values needed to predict the\n        next step immediately after the training data. These values are stored\n        in the original scale of the time series before undergoing any transformations\n        or differentiation. When `differentiation` parameter is specified, the\n        dimensions of the `last_window_` are expanded as many values as the order\n        of differentiation. For example, if `lags` = 7 and `differentiation` = 1,\n        `last_window_` will have 8 values.\n    index_type_ : type\n        Type of index of the input used in training.\n    index_freq_ : str\n        Frequency of Index of the input used in training.\n    training_range_ : pandas Index\n        First and last values of index of the data used during training.\n    exog_in_ : bool\n        If the forecaster has been trained using exogenous variable/s.\n    exog_names_in_ : list\n        Names of the exogenous variables used during training.\n    exog_type_in_ : type\n        Type of exogenous data (pandas Series or DataFrame) used in training.\n    exog_dtypes_in_ : dict\n        Type of each exogenous variable/s used in training. If `transformer_exog` \n        is used, the dtypes are calculated after the transformation.\n    X_train_window_features_names_out_ : list\n        Names of the window features included in the matrix `X_train` created\n        internally for training.\n    X_train_exog_names_out_ : list\n        Names of the exogenous variables included in the matrix `X_train` created\n        internally for training. It can be different from `exog_names_in_` if\n        some exogenous variables are transformed during the training process.\n    X_train_direct_exog_names_out_ : list\n        Same as `X_train_exog_names_out_` but using the direct format. The same \n        exogenous variable is repeated for each step.\n    X_train_features_names_out_ : list\n        Names of columns of the matrix created internally for training.\n    fit_kwargs : dict\n        Additional arguments to be passed to the `fit` method of the regressor.\n    in_sample_residuals_ : dict\n        Residuals of the models when predicting training data. Only stored up to\n        1000 values per model in the form `{step: residuals}`. If `transformer_y` \n        is not `None`, residuals are stored in the transformed scale.\n    out_sample_residuals_ : dict\n        Residuals of the models when predicting non training data. Only stored\n        up to 1000 values per model in the form `{step: residuals}`. If `transformer_y` \n        is not `None`, residuals are assumed to be in the transformed scale. Use \n        `set_out_sample_residuals()` method to set values.\n    creation_date : str\n        Date of creation.\n    is_fitted : bool\n        Tag to identify if the regressor has been fitted (trained).\n    fit_date : str\n        Date of last fit.\n    skforecast_version : str\n        Version of skforecast library used to create the forecaster.\n    python_version : str\n        Version of python used to create the forecaster.\n    n_jobs : int, 'auto'\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_fit_forecaster.\n    forecaster_id : str, int\n        Name used as an identifier of the forecaster.\n    binner : Ignored\n        Not used, present here for API consistency by convention.\n    binner_intervals_ : Ignored\n        Not used, present here for API consistency by convention.\n    binner_kwargs : Ignored\n        Not used, present here for API consistency by convention.\n    in_sample_residuals_by_bin_ : Ignored\n        Not used, present here for API consistency by convention.\n    out_sample_residuals_by_bin_ : Ignored\n        Not used, present here for API consistency by convention.\n\n    Notes\n    -----\n    A separate model is created for each forecasting time step. It is important to\n    note that all models share the same parameter and hyperparameter configuration.\n     \n    \"\"\"\n\n    def __init__(\n        self,\n        regressor: object,\n        steps: int,\n        lags: Optional[Union[int, list, np.ndarray, range]] = None,\n        window_features: Optional[Union[object, list]] = None,\n        transformer_y: Optional[object] = None,\n        transformer_exog: Optional[object] = None,\n        weight_func: Optional[Callable] = None,\n        differentiation: Optional[int] = None,\n        fit_kwargs: Optional[dict] = None,\n        n_jobs: Union[int, str] = 'auto',\n        forecaster_id: Optional[Union[str, int]] = None\n    ) -> None:\n        \n        self.regressor                          = copy(regressor)\n        self.steps                              = steps\n        self.transformer_y                      = transformer_y\n        self.transformer_exog                   = transformer_exog\n        self.weight_func                        = weight_func\n        self.source_code_weight_func            = None\n        self.differentiation                    = differentiation\n        self.differentiator                     = None\n        self.last_window_                       = None\n        self.index_type_                        = None\n        self.index_freq_                        = None\n        self.training_range_                    = None\n        self.exog_in_                           = False\n        self.exog_names_in_                     = None\n        self.exog_type_in_                      = None\n        self.exog_dtypes_in_                    = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_            = None\n        self.X_train_direct_exog_names_out_     = None\n        self.X_train_features_names_out_        = None\n        self.creation_date                      = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.is_fitted                          = False\n        self.fit_date                           = None\n        self.skforecast_version                 = skforecast.__version__\n        self.python_version                     = sys.version.split(\" \")[0]\n        self.forecaster_id                      = forecaster_id\n        self.binner                             = None\n        self.binner_intervals_                  = None\n        self.binner_kwargs                      = None\n        self.in_sample_residuals_by_bin_        = None\n        self.out_sample_residuals_by_bin_       = None\n\n        if not isinstance(steps, int):\n            raise TypeError(\n                f\"`steps` argument must be an int greater than or equal to 1. \"\n                f\"Got {type(steps)}.\"\n            )\n\n        if steps < 1:\n            raise ValueError(\n                f\"`steps` argument must be greater than or equal to 1. Got {steps}.\"\n            )\n\n        self.regressors_ = {step: clone(self.regressor) for step in range(1, steps + 1)}\n        self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\n        self.window_features, self.window_features_names, self.max_size_window_features = (\n            initialize_window_features(window_features)\n        )\n        if self.window_features is None and self.lags is None:\n            raise ValueError(\n                \"At least one of the arguments `lags` or `window_features` \"\n                \"must be different from None. This is required to create the \"\n                \"predictors used in training the forecaster.\"\n            )\n        \n        self.window_size = max(\n            [ws for ws in [self.max_lag, self.max_size_window_features] \n             if ws is not None]\n        )\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [\n                type(wf).__name__ for wf in self.window_features\n            ]\n\n        if self.differentiation is not None:\n            if not isinstance(differentiation, int) or differentiation < 1:\n                raise ValueError(\n                    f\"Argument `differentiation` must be an integer equal to or \"\n                    f\"greater than 1. Got {differentiation}.\"\n                )\n            self.window_size += self.differentiation\n            self.differentiator = TimeSeriesDifferentiator(\n                order=self.differentiation, window_size=self.window_size\n            )\n\n        self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n            forecaster_name = type(self).__name__, \n            regressor       = regressor, \n            weight_func     = weight_func, \n            series_weights  = None\n        )\n\n        self.fit_kwargs = check_select_fit_kwargs(\n                              regressor  = regressor,\n                              fit_kwargs = fit_kwargs\n                          )\n\n        self.in_sample_residuals_ = {step: None for step in range(1, steps + 1)}\n        self.out_sample_residuals_ = None\n\n        if n_jobs == 'auto':\n            self.n_jobs = select_n_jobs_fit_forecaster(\n                              forecaster_name = type(self).__name__,\n                              regressor       = self.regressor\n                          )\n        else:\n            if not isinstance(n_jobs, int):\n                raise TypeError(\n                    f\"`n_jobs` must be an integer or `'auto'`. Got {type(n_jobs)}.\"\n                )\n            self.n_jobs = n_jobs if n_jobs > 0 else cpu_count()\n\n    def __repr__(\n        self\n    ) -> str:\n        \"\"\"\n        Information displayed when a ForecasterDirect object is printed.\n        \"\"\"\n\n        (\n            params,\n            _,\n            _,\n            exog_names_in_,\n            _,\n        ) = self._preprocess_repr(\n                regressor      = self.regressor,\n                exog_names_in_ = self.exog_names_in_\n            )\n        \n        params = self._format_text_repr(params)\n        exog_names_in_ = self._format_text_repr(exog_names_in_)\n\n        info = (\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"{type(self).__name__} \\n\"\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"Regressor: {type(self.regressor).__name__} \\n\"\n            f\"Lags: {self.lags} \\n\"\n            f\"Window features: {self.window_features_names} \\n\"\n            f\"Window size: {self.window_size} \\n\"\n            f\"Maximum steps to predict: {self.steps} \\n\"\n            f\"Exogenous included: {self.exog_in_} \\n\"\n            f\"Exogenous names: {exog_names_in_} \\n\"\n            f\"Transformer for y: {self.transformer_y} \\n\"\n            f\"Transformer for exog: {self.transformer_exog} \\n\"\n            f\"Weight function included: {True if self.weight_func is not None else False} \\n\"\n            f\"Differentiation order: {self.differentiation} \\n\"\n            f\"Training range: {self.training_range_.to_list() if self.is_fitted else None} \\n\"\n            f\"Training index type: {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None} \\n\"\n            f\"Training index frequency: {self.index_freq_ if self.is_fitted else None} \\n\"\n            f\"Regressor parameters: {params} \\n\"\n            f\"fit_kwargs: {self.fit_kwargs} \\n\"\n            f\"Creation date: {self.creation_date} \\n\"\n            f\"Last fit date: {self.fit_date} \\n\"\n            f\"Skforecast version: {self.skforecast_version} \\n\"\n            f\"Python version: {self.python_version} \\n\"\n            f\"Forecaster id: {self.forecaster_id} \\n\"\n        )\n\n        return info\n\n    def _repr_html_(self):\n        \"\"\"\n        HTML representation of the object.\n        The \"General Information\" section is expanded by default.\n        \"\"\"\n\n        (\n            params,\n            _,\n            _,\n            exog_names_in_,\n            _,\n        ) = self._preprocess_repr(\n                regressor      = self.regressor,\n                exog_names_in_ = self.exog_names_in_\n            )\n\n        style, unique_id = self._get_style_repr_html(self.is_fitted)\n        \n        content = f\"\"\"\n        <div class=\"container-{unique_id}\">\n            <h2>{type(self).__name__}</h2>\n            <details open>\n                <summary>General Information</summary>\n                <ul>\n                    <li><strong>Regressor:</strong> {type(self.regressor).__name__}</li>\n                    <li><strong>Lags:</strong> {self.lags}</li>\n                    <li><strong>Window features:</strong> {self.window_features_names}</li>\n                    <li><strong>Window size:</strong> {self.window_size}</li>\n                    <li><strong>Maximum steps to predict:</strong> {self.steps}</li>\n                    <li><strong>Exogenous included:</strong> {self.exog_in_}</li>\n                    <li><strong>Weight function included:</strong> {self.weight_func is not None}</li>\n                    <li><strong>Differentiation order:</strong> {self.differentiation}</li>\n                    <li><strong>Creation date:</strong> {self.creation_date}</li>\n                    <li><strong>Last fit date:</strong> {self.fit_date}</li>\n                    <li><strong>Skforecast version:</strong> {self.skforecast_version}</li>\n                    <li><strong>Python version:</strong> {self.python_version}</li>\n                    <li><strong>Forecaster id:</strong> {self.forecaster_id}</li>\n                </ul>\n            </details>\n            <details>\n                <summary>Exogenous Variables</summary>\n                <ul>\n                    {exog_names_in_}\n                </ul>\n            </details>\n            <details>\n                <summary>Data Transformations</summary>\n                <ul>\n                    <li><strong>Transformer for y:</strong> {self.transformer_y}</li>\n                    <li><strong>Transformer for exog:</strong> {self.transformer_exog}</li>\n                </ul>\n            </details>\n            <details>\n                <summary>Training Information</summary>\n                <ul>\n                    <li><strong>Training range:</strong> {self.training_range_.to_list() if self.is_fitted else 'Not fitted'}</li>\n                    <li><strong>Training index type:</strong> {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted'}</li>\n                    <li><strong>Training index frequency:</strong> {self.index_freq_ if self.is_fitted else 'Not fitted'}</li>\n                </ul>\n            </details>\n            <details>\n                <summary>Regressor Parameters</summary>\n                <ul>\n                    {params}\n                </ul>\n            </details>\n            <details>\n                <summary>Fit Kwargs</summary>\n                <ul>\n                    {self.fit_kwargs}\n                </ul>\n            </details>\n            <p>\n                <a href=\"https://skforecast.org/{skforecast.__version__}/api/forecasterdirect.html\">&#128712 <strong>API Reference</strong></a>\n                &nbsp;&nbsp;\n                <a href=\"https://skforecast.org/{skforecast.__version__}/user_guides/direct-multi-step-forecasting.html\">&#128462 <strong>User Guide</strong></a>\n            </p>\n        </div>\n        \"\"\"\n\n        # Return the combined style and content\n        return style + content\n\n    def _create_lags(\n        self, \n        y: np.ndarray,\n        X_as_pandas: bool = False,\n        train_index: Optional[pd.Index] = None\n    ) -> Tuple[Optional[Union[np.ndarray, pd.DataFrame]], np.ndarray]:\n        \"\"\"\n        Create the lagged values and their target variable from a time series.\n        \n        Note that the returned matrix `X_data` contains the lag 1 in the first \n        column, the lag 2 in the in the second column and so on.\n        \n        Parameters\n        ----------\n        y : numpy ndarray\n            Training time series values.\n        X_as_pandas : bool, default `False`\n            If `True`, the returned matrix `X_data` is a pandas DataFrame.\n        train_index : pandas Index, default `None`\n            Index of the training data. It is used to create the pandas DataFrame\n            `X_data` when `X_as_pandas` is `True`.\n\n        Returns\n        -------\n        X_data : numpy ndarray, pandas DataFrame, None\n            Lagged values (predictors).\n        y_data : numpy ndarray\n            Values of the time series related to each row of `X_data`.\n        \n        \"\"\"\n        \n        n_rows = len(y) - self.window_size - (self.steps - 1)\n        \n        X_data = None\n        if self.lags is not None:\n            X_data = np.full(\n                shape=(n_rows, len(self.lags)), fill_value=np.nan, order='F', dtype=float\n            )\n            for i, lag in enumerate(self.lags):\n                X_data[:, i] = y[self.window_size - lag : -(lag + self.steps - 1)] \n\n            if X_as_pandas:\n                X_data = pd.DataFrame(\n                             data    = X_data,\n                             columns = self.lags_names,\n                             index   = train_index\n                         )\n\n        y_data = np.full(\n            shape=(n_rows, self.steps), fill_value=np.nan, order='F', dtype=float\n        )\n        for step in range(self.steps):\n            y_data[:, step] = y[self.window_size + step : self.window_size + step + n_rows]\n        \n        return X_data, y_data\n\n    def _create_window_features(\n        self, \n        y: pd.Series,\n        train_index: pd.Index,\n        X_as_pandas: bool = False,\n    ) -> Tuple[list, list]:\n        \"\"\"\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        train_index : pandas Index\n            Index of the training data. It is used to create the pandas DataFrame\n            `X_train_window_features` when `X_as_pandas` is `True`.\n        X_as_pandas : bool, default `False`\n            If `True`, the returned matrix `X_train_window_features` is a \n            pandas DataFrame.\n\n        Returns\n        -------\n        X_train_window_features : list\n            List of numpy ndarrays or pandas DataFrames with the window features.\n        X_train_window_features_names_out_ : list\n            Names of the window features.\n        \n        \"\"\"\n\n        len_train_index = len(train_index)\n        X_train_window_features = []\n        X_train_window_features_names_out_ = []\n        for wf in self.window_features:\n            X_train_wf = wf.transform_batch(y)\n            if not isinstance(X_train_wf, pd.DataFrame):\n                raise TypeError(\n                    (f\"The method `transform_batch` of {type(wf).__name__} \"\n                     f\"must return a pandas DataFrame.\")\n                )\n            X_train_wf = X_train_wf.iloc[-len_train_index:]\n            if not len(X_train_wf) == len_train_index:\n                raise ValueError(\n                    (f\"The method `transform_batch` of {type(wf).__name__} \"\n                     f\"must return a DataFrame with the same number of rows as \"\n                     f\"the input time series - (`window_size` + (`steps` - 1)): {len_train_index}.\")\n                )\n            X_train_wf.index = train_index\n            \n            X_train_window_features_names_out_.extend(X_train_wf.columns)\n            if not X_as_pandas:\n                X_train_wf = X_train_wf.to_numpy()     \n            X_train_window_features.append(X_train_wf)\n\n        return X_train_window_features, X_train_window_features_names_out_\n\n    def _create_train_X_y(\n        self,\n        y: pd.Series,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n    ) -> Tuple[pd.DataFrame, dict, list, list, list, dict]:\n        \"\"\"\n        Create training matrices from univariate time series and exogenous\n        variables. The resulting matrices contain the target variable and \n        predictors needed to train all the regressors (one per step).\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors) for each step. Note that the index \n            corresponds to that of the last step. It is updated for the corresponding \n            step in the `filter_train_X_y_for_step` method.\n        y_train : dict\n            Values of the time series related to each row of `X_train` for each \n            step in the form {step: y_step_[i]}.\n        exog_names_in_ : list\n            Names of the exogenous variables used during training.\n        X_train_exog_names_out_ : list\n            Names of the exogenous variables included in the matrix `X_train` created\n            internally for training. It can be different from `exog_names_in_` if\n            some exogenous variables are transformed during the training process.\n        X_train_features_names_out_ : list\n            Names of the columns of the matrix created internally for training.\n        exog_dtypes_in_ : dict\n            Type of each exogenous variable/s used in training. If `transformer_exog` \n            is used, the dtypes are calculated before the transformation.\n        \n        \"\"\"\n\n        check_y(y=y)\n        y = input_to_frame(data=y, input_name='y')\n\n        if len(y) < self.window_size + self.steps:\n            raise ValueError(\n                f\"Minimum length of `y` for training this forecaster is \"\n                f\"{self.window_size + self.steps}. Reduce the number of \"\n                f\"predicted steps, {self.steps}, or the maximum \"\n                f\"window_size, {self.window_size}, if no more data is available.\\n\"\n                f\"    Length `y`: {len(y)}.\\n\"\n                f\"    Max step : {self.steps}.\\n\"\n                f\"    Max window size: {self.window_size}.\\n\"\n                f\"    Lags window size: {self.max_lag}.\\n\"\n                f\"    Window features window size: {self.max_size_window_features}.\"\n            )\n\n        fit_transformer = False if self.is_fitted else True\n        y = transform_dataframe(\n                df                = y, \n                transformer       = self.transformer_y,\n                fit               = fit_transformer,\n                inverse_transform = False,\n            )\n        y_values, y_index = preprocess_y(y=y)\n\n        if self.differentiation is not None:\n            if not self.is_fitted:\n                y_values = self.differentiator.fit_transform(y_values)\n            else:\n                differentiator = copy(self.differentiator)\n                y_values = differentiator.fit_transform(y_values)\n\n        exog_names_in_ = None\n        exog_dtypes_in_ = None\n        categorical_features = False\n        if exog is not None:\n            check_exog(exog=exog, allow_nan=True)\n            exog = input_to_frame(data=exog, input_name='exog')\n\n            y_index_no_ws = y_index[self.window_size:]\n            len_y = len(y_values)\n            len_y_no_ws = len_y - self.window_size\n            len_exog = len(exog)\n            if not len_exog == len_y and not len_exog == len_y_no_ws:\n                raise ValueError(\n                    f\"Length of `exog` must be equal to the length of `y` (if index is \"\n                    f\"fully aligned) or length of `y` - `window_size` (if `exog` \"\n                    f\"starts after the first `window_size` values).\\n\"\n                    f\"    `exog`              : ({exog.index[0]} -- {exog.index[-1]})  (n={len_exog})\\n\"\n                    f\"    `y`                 : ({y.index[0]} -- {y.index[-1]})  (n={len_y})\\n\"\n                    f\"    `y` - `window_size` : ({y_index_no_ws[0]} -- {y_index_no_ws[-1]})  (n={len_y_no_ws})\"\n                )\n            \n            # NOTE: Need here for filter_train_X_y_for_step to work without fitting\n            self.exog_in_ = True\n            exog_names_in_ = exog.columns.to_list()\n            exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = fit_transformer,\n                       inverse_transform = False\n                   )\n\n            check_exog_dtypes(exog, call_check_exog=True)\n            categorical_features = (\n                exog.select_dtypes(include=np.number).shape[1] != exog.shape[1]\n            )\n\n            _, exog_index = preprocess_exog(exog=exog, return_values=False)\n            if len_exog == len_y:\n                if not (exog_index == y_index).all():\n                    raise ValueError(\n                        \"When `exog` has the same length as `y`, the index of \"\n                        \"`exog` must be aligned with the index of `y` \"\n                        \"to ensure the correct alignment of values.\"\n                    )\n                # The first `self.window_size` positions have to be removed from \n                # exog since they are not in X_train.\n                exog = exog.iloc[self.window_size:, ]\n            else:\n                if not (exog_index == y_index_no_ws).all():\n                    raise ValueError(\n                        \"When `exog` doesn't contain the first `window_size` observations, \"\n                        \"the index of `exog` must be aligned with the index of `y` minus \"\n                        \"the first `window_size` observations to ensure the correct \"\n                        \"alignment of values.\"\n                    )\n        \n        X_train = []\n        X_train_features_names_out_ = []\n        train_index = y_index[self.window_size + (self.steps - 1):]\n        len_train_index = len(train_index)\n        X_as_pandas = True if categorical_features else False\n\n        X_train_lags, y_train = self._create_lags(\n            y=y_values, X_as_pandas=X_as_pandas, train_index=train_index\n        )\n        if X_train_lags is not None:\n            X_train.append(X_train_lags)\n            X_train_features_names_out_.extend(self.lags_names)\n        \n        X_train_window_features_names_out_ = None\n        if self.window_features is not None:\n            n_diff = 0 if self.differentiation is None else self.differentiation\n            end_wf = None if self.steps == 1 else -(self.steps - 1)\n            y_window_features = pd.Series(\n                y_values[n_diff:end_wf], index=y_index[n_diff:end_wf]\n            )\n            X_train_window_features, X_train_window_features_names_out_ = (\n                self._create_window_features(\n                    y           = y_window_features, \n                    X_as_pandas = X_as_pandas, \n                    train_index = train_index\n                )\n            )\n            X_train.extend(X_train_window_features)\n            X_train_features_names_out_.extend(X_train_window_features_names_out_)\n\n        # NOTE: Need here for filter_train_X_y_for_step to work without fitting\n        self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n\n        X_train_exog_names_out_ = None\n        if exog is not None:\n            X_train_exog_names_out_ = exog.columns.to_list()\n            if X_as_pandas:\n                exog_direct, X_train_direct_exog_names_out_ = exog_to_direct(\n                    exog=exog, steps=self.steps\n                )\n                exog_direct.index = train_index\n            else:\n                exog_direct, X_train_direct_exog_names_out_ = exog_to_direct_numpy(\n                    exog=exog, steps=self.steps\n                )\n\n            # NOTE: Need here for filter_train_X_y_for_step to work without fitting\n            self.X_train_direct_exog_names_out_ = X_train_direct_exog_names_out_\n\n            X_train_features_names_out_.extend(self.X_train_direct_exog_names_out_)\n            X_train.append(exog_direct)\n        \n        if len(X_train) == 1:\n            X_train = X_train[0]\n        else:\n            if X_as_pandas:\n                X_train = pd.concat(X_train, axis=1)\n            else:\n                X_train = np.concatenate(X_train, axis=1)\n                \n        if X_as_pandas:\n            X_train.index = train_index\n        else:\n            X_train = pd.DataFrame(\n                          data    = X_train,\n                          index   = train_index,\n                          columns = X_train_features_names_out_\n                      )\n\n        y_train = {\n            step: pd.Series(\n                      data  = y_train[:, step - 1], \n                      index = y_index[self.window_size + step - 1:][:len_train_index],\n                      name  = f\"y_step_{step}\"\n                  )\n            for step in range(1, self.steps + 1)\n        }\n        \n        return (\n            X_train,\n            y_train,\n            exog_names_in_,\n            X_train_exog_names_out_,\n            X_train_features_names_out_,\n            exog_dtypes_in_\n        )\n\n    def create_train_X_y(\n        self,\n        y: pd.Series,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n    ) -> Tuple[pd.DataFrame, dict]:\n        \"\"\"\n        Create training matrices from univariate time series and exogenous\n        variables. The resulting matrices contain the target variable and \n        predictors needed to train all the regressors (one per step).\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors) for each step. Note that the index \n            corresponds to that of the last step. It is updated for the corresponding \n            step in the `filter_train_X_y_for_step` method.\n        y_train : dict\n            Values of the time series related to each row of `X_train` for each \n            step in the form {step: y_step_[i]}.\n        \n        \"\"\"\n\n        output = self._create_train_X_y(y=y, exog=exog)\n\n        X_train = output[0]\n        y_train = output[1]\n\n        return X_train, y_train\n\n    def filter_train_X_y_for_step(\n        self,\n        step: int,\n        X_train: pd.DataFrame,\n        y_train: dict,\n        remove_suffix: bool = False\n    ) -> Tuple[pd.DataFrame, pd.Series]:\n        \"\"\"\n        Select the columns needed to train a forecaster for a specific step.  \n        The input matrices should be created using `create_train_X_y` method. \n        This method updates the index of `X_train` to the corresponding one \n        according to `y_train`. If `remove_suffix=True` the suffix \"_step_i\" \n        will be removed from the column names. \n\n        Parameters\n        ----------\n        step : int\n            Step for which columns must be selected selected. Starts at 1.\n        X_train : pandas DataFrame\n            Dataframe created with the `create_train_X_y` method, first return.\n        y_train : dict\n            Dict created with the `create_train_X_y` method, second return.\n        remove_suffix : bool, default `False`\n            If True, suffix \"_step_i\" is removed from the column names.\n\n        Returns\n        -------\n        X_train_step : pandas DataFrame\n            Training values (predictors) for the selected step.\n        y_train_step : pandas Series\n            Values of the time series related to each row of `X_train`.\n\n        \"\"\"\n\n        if (step < 1) or (step > self.steps):\n            raise ValueError(\n                (f\"Invalid value `step`. For this forecaster, minimum value is 1 \"\n                 f\"and the maximum step is {self.steps}.\")\n            )\n\n        y_train_step = y_train[step]\n\n        # Matrix X_train starts at index 0.\n        if not self.exog_in_:\n            X_train_step = X_train\n        else:\n            n_lags = len(self.lags) if self.lags is not None else 0\n            n_window_features = (\n                len(self.X_train_window_features_names_out_) if self.window_features is not None else 0\n            )\n            idx_columns_autoreg = np.arange(n_lags + n_window_features)\n            n_exog = len(self.X_train_direct_exog_names_out_) / self.steps\n            idx_columns_exog = (\n                np.arange((step - 1) * n_exog, (step) * n_exog) + idx_columns_autoreg[-1] + 1\n            )\n            idx_columns = np.concatenate((idx_columns_autoreg, idx_columns_exog))\n            X_train_step = X_train.iloc[:, idx_columns]\n\n        X_train_step.index = y_train_step.index\n\n        if remove_suffix:\n            X_train_step.columns = [\n                col_name.replace(f\"_step_{step}\", \"\")\n                for col_name in X_train_step.columns\n            ]\n            y_train_step.name = y_train_step.name.replace(f\"_step_{step}\", \"\")\n\n        return X_train_step, y_train_step\n\n    def _train_test_split_one_step_ahead(\n        self,\n        y: pd.Series,\n        initial_train_size: int,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n    ) -> Tuple[pd.DataFrame, dict, pd.DataFrame, dict]:\n        \"\"\"\n        Create matrices needed to train and test the forecaster for one-step-ahead\n        predictions.\n\n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        initial_train_size : int\n            Initial size of the training set. It is the number of observations used\n            to train the forecaster before making the first prediction.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n        \n        Returns\n        -------\n        X_train : pandas DataFrame\n            Predictor values used to train the model.\n        y_train : dict\n            Values of the time series related to each row of `X_train` for each \n            step in the form {step: y_step_[i]}.\n        X_test : pandas DataFrame\n            Predictor values used to test the model.\n        y_test : dict\n            Values of the time series related to each row of `X_test` for each \n            step in the form {step: y_step_[i]}.\n        \n        \"\"\"\n\n        is_fitted = self.is_fitted\n        self.is_fitted = False\n        X_train, y_train, *_ = self._create_train_X_y(\n            y    = y.iloc[: initial_train_size],\n            exog = exog.iloc[: initial_train_size] if exog is not None else None\n        )\n\n        test_init = initial_train_size - self.window_size\n        self.is_fitted = True\n        X_test, y_test, *_ = self._create_train_X_y(\n            y    = y.iloc[test_init:],\n            exog = exog.iloc[test_init:] if exog is not None else None\n        )\n\n        self.is_fitted = is_fitted\n\n        return X_train, y_train, X_test, y_test\n\n    def create_sample_weights(\n        self,\n        X_train: pd.DataFrame,\n    ) -> np.ndarray:\n        \"\"\"\n        Crate weights for each observation according to the forecaster's attribute\n        `weight_func`.\n\n        Parameters\n        ----------\n        X_train : pandas DataFrame\n            Dataframe created with `create_train_X_y` and `filter_train_X_y_for_step`\n            methods, first return.\n\n        Returns\n        -------\n        sample_weight : numpy ndarray\n            Weights to use in `fit` method.\n\n        \"\"\"\n\n        sample_weight = None\n\n        if self.weight_func is not None:\n            sample_weight = self.weight_func(X_train.index)\n\n        if sample_weight is not None:\n            if np.isnan(sample_weight).any():\n                raise ValueError(\n                    \"The resulting `sample_weight` cannot have NaN values.\"\n                )\n            if np.any(sample_weight < 0):\n                raise ValueError(\n                    \"The resulting `sample_weight` cannot have negative values.\"\n                )\n            if np.sum(sample_weight) == 0:\n                raise ValueError(\n                    (\"The resulting `sample_weight` cannot be normalized because \"\n                     \"the sum of the weights is zero.\")\n                )\n\n        return sample_weight\n\n    def fit(\n        self,\n        y: pd.Series,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        store_last_window: bool = True,\n        store_in_sample_residuals: bool = True\n    ) -> None:\n        \"\"\"\n        Training Forecaster.\n\n        Additional arguments to be passed to the `fit` method of the regressor \n        can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned so\n            that y[i] is regressed on exog[i].\n        store_last_window : bool, default `True`\n            Whether or not to store the last window (`last_window_`) of training data.\n        store_in_sample_residuals : bool, default `True`\n            If `True`, in-sample residuals will be stored in the forecaster object\n            after fitting (`in_sample_residuals_` attribute).\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        # Reset values in case the forecaster has already been fitted.\n        self.last_window_                       = None\n        self.index_type_                        = None\n        self.index_freq_                        = None\n        self.training_range_                    = None\n        self.exog_in_                           = False\n        self.exog_names_in_                     = None\n        self.exog_type_in_                      = None\n        self.exog_dtypes_in_                    = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_            = None\n        self.X_train_direct_exog_names_out_     = None\n        self.X_train_features_names_out_        = None\n        self.in_sample_residuals_               = {step: None for step in range(1, self.steps + 1)}\n        self.is_fitted                          = False\n        self.fit_date                           = None\n\n        (\n            X_train,\n            y_train,\n            exog_names_in_,\n            X_train_exog_names_out_,\n            X_train_features_names_out_,\n            exog_dtypes_in_\n        ) = self._create_train_X_y(y=y, exog=exog)\n\n        def fit_forecaster(regressor, X_train, y_train, step, store_in_sample_residuals):\n            \"\"\"\n            Auxiliary function to fit each of the forecaster's regressors in parallel.\n\n            Parameters\n            ----------\n            regressor : object\n                Regressor to be fitted.\n            X_train : pandas DataFrame\n                Dataframe created with the `create_train_X_y` method, first return.\n            y_train : dict\n                Dict created with the `create_train_X_y` method, second return.\n            step : int\n                Step of the forecaster to be fitted.\n            store_in_sample_residuals : bool\n                If `True`, in-sample residuals will be stored in the forecaster object\n                after fitting (`in_sample_residuals_` attribute).\n            \n            Returns\n            -------\n            Tuple with the step, fitted regressor and in-sample residuals.\n\n            \"\"\"\n\n            X_train_step, y_train_step = self.filter_train_X_y_for_step(\n                                             step          = step,\n                                             X_train       = X_train,\n                                             y_train       = y_train,\n                                             remove_suffix = True\n                                         )\n            sample_weight = self.create_sample_weights(X_train=X_train_step)\n            if sample_weight is not None:\n                regressor.fit(\n                    X             = X_train_step,\n                    y             = y_train_step,\n                    sample_weight = sample_weight,\n                    **self.fit_kwargs\n                )\n            else:\n                regressor.fit(\n                    X = X_train_step,\n                    y = y_train_step,\n                    **self.fit_kwargs\n                )\n\n            # This is done to save time during fit in functions such as backtesting()\n            if store_in_sample_residuals:\n                residuals = (\n                    (y_train_step - regressor.predict(X_train_step))\n                ).to_numpy()\n\n                if len(residuals) > 1000:\n                    rng = np.random.default_rng(seed=123)\n                    residuals = rng.choice(\n                                    a       = residuals, \n                                    size    = 1000, \n                                    replace = False\n                                )\n            else:\n                residuals = None\n\n            return step, regressor, residuals\n\n        results_fit = (\n            Parallel(n_jobs=self.n_jobs)\n            (delayed(fit_forecaster)\n            (\n                regressor                 = copy(self.regressor),\n                X_train                   = X_train,\n                y_train                   = y_train,\n                step                      = step,\n                store_in_sample_residuals = store_in_sample_residuals\n            )\n            for step in range(1, self.steps + 1))\n        )\n\n        self.regressors_ = {step: regressor for step, regressor, _ in results_fit}\n\n        if store_in_sample_residuals:\n            self.in_sample_residuals_ = {\n                step: residuals \n                for step, _, residuals in results_fit\n            }\n        \n        self.X_train_features_names_out_ = X_train_features_names_out_\n\n        self.is_fitted = True\n        self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.training_range_ = preprocess_y(y=y, return_values=False)[1][[0, -1]]\n        self.index_type_ = type(X_train.index)\n        if isinstance(X_train.index, pd.DatetimeIndex):\n            self.index_freq_ = X_train.index.freqstr\n        else: \n            self.index_freq_ = X_train.index.step\n\n        if exog is not None:\n            self.exog_in_ = True\n            self.exog_type_in_ = type(exog)\n            self.exog_names_in_ = exog_names_in_\n            self.exog_dtypes_in_ = exog_dtypes_in_\n            self.X_train_exog_names_out_ = X_train_exog_names_out_\n\n        if store_last_window:\n            self.last_window_ = (\n                y.iloc[-self.window_size:]\n                .copy()\n                .to_frame(name=y.name if y.name is not None else 'y')\n            )\n\n    def _create_predict_inputs(\n        self,\n        steps: Optional[Union[int, list]] = None,\n        last_window: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        check_inputs: bool = True\n    ) -> Tuple[list, list, list, pd.Index]:\n        \"\"\"\n        Create the inputs needed for the prediction process.\n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n\n        Returns\n        -------\n        Xs : list\n            List of numpy arrays with the predictors for each step.\n        Xs_col_names : list\n            Names of the columns of the matrix created internally for prediction.\n        steps : list\n            Steps to predict.\n        prediction_index : pandas Index\n            Index of the predictions.\n        \n        \"\"\"\n        \n        steps = prepare_steps_direct(\n                    max_step = self.steps,\n                    steps    = steps\n                )\n\n        if last_window is None:\n            last_window = self.last_window_\n\n        if check_inputs:\n            check_predict_input(\n                forecaster_name = type(self).__name__,\n                steps           = steps,\n                is_fitted       = self.is_fitted,\n                exog_in_        = self.exog_in_,\n                index_type_     = self.index_type_,\n                index_freq_     = self.index_freq_,\n                window_size     = self.window_size,\n                last_window     = last_window,\n                exog            = exog,\n                exog_type_in_   = self.exog_type_in_,\n                exog_names_in_  = self.exog_names_in_,\n                interval        = None,\n                max_steps       = self.steps\n            )\n\n        last_window = last_window.iloc[-self.window_size:].copy()\n        last_window_values, last_window_index = preprocess_last_window(\n                                                    last_window = last_window\n                                                )\n\n        last_window_values = transform_numpy(\n                                 array             = last_window_values,\n                                 transformer       = self.transformer_y,\n                                 fit               = False,\n                                 inverse_transform = False\n                             )\n        if self.differentiation is not None:\n            last_window_values = self.differentiator.fit_transform(last_window_values)\n\n        X_autoreg = []\n        Xs_col_names = []\n        if self.lags is not None:\n            X_lags = last_window_values[-self.lags]\n            X_autoreg.append(X_lags)\n            Xs_col_names.extend(self.lags_names)\n\n        if self.window_features is not None:\n            n_diff = 0 if self.differentiation is None else self.differentiation\n            X_window_features = np.concatenate(\n                [\n                    wf.transform(last_window_values[n_diff:])\n                    for wf in self.window_features\n                ]\n            )\n            X_autoreg.append(X_window_features)\n            Xs_col_names.extend(self.X_train_window_features_names_out_)\n\n        X_autoreg = np.concatenate(X_autoreg).reshape(1, -1)\n        if exog is not None:\n            exog = input_to_frame(data=exog, input_name='exog')\n            exog = exog.loc[:, self.exog_names_in_]\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n            check_exog_dtypes(exog=exog)\n            exog_values, _ = exog_to_direct_numpy(\n                                 exog  = exog.to_numpy()[:max(steps)],\n                                 steps = max(steps)\n                             )\n            exog_values = exog_values[0]\n            \n            n_exog = exog.shape[1]\n            Xs = [\n                np.concatenate(\n                    [\n                        X_autoreg,\n                        exog_values[(step - 1) * n_exog : step * n_exog].reshape(1, -1),\n                    ],\n                    axis=1\n                )\n                for step in steps\n            ]\n            # HACK: This is not the best way to do it. Can have any problem\n            # if the exog_columns are not in the same order as the\n            # self.window_features_names.\n            Xs_col_names = Xs_col_names + exog.columns.to_list()\n        else:\n            Xs = [X_autoreg] * len(steps)\n\n        prediction_index = expand_index(\n                               index = last_window_index,\n                               steps = max(steps)\n                           )[np.array(steps) - 1]\n        if isinstance(last_window_index, pd.DatetimeIndex) and np.array_equal(\n            steps, np.arange(min(steps), max(steps) + 1)\n        ):\n            prediction_index.freq = last_window_index.freq\n        \n        # HACK: Why no use self.X_train_features_names_out_ as Xs_col_names?\n        return Xs, Xs_col_names, steps, prediction_index\n\n    def create_predict_X(\n        self,\n        steps: Optional[Union[int, list]] = None,\n        last_window: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        Create the predictors needed to predict `steps` ahead.\n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n\n        Returns\n        -------\n        X_predict : pandas DataFrame\n            Pandas DataFrame with the predictors for each step. The index \n            is the same as the prediction index.\n        \n        \"\"\"\n\n        Xs, Xs_col_names, steps, prediction_index = self._create_predict_inputs(\n            steps=steps, last_window=last_window, exog=exog\n        )\n\n        X_predict = pd.DataFrame(\n                        data    = np.concatenate(Xs, axis=0), \n                        columns = Xs_col_names, \n                        index   = prediction_index\n                    )\n        \n        if self.transformer_y is not None or self.differentiation is not None:\n            warnings.warn(\n                \"The output matrix is in the transformed scale due to the \"\n                \"inclusion of transformations or differentiation in the Forecaster. \"\n                \"As a result, any predictions generated using this matrix will also \"\n                \"be in the transformed scale. Please refer to the documentation \"\n                \"for more details: \"\n                \"https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html\",\n                DataTransformationWarning\n            )\n\n        return X_predict\n\n    def predict(\n        self,\n        steps: Optional[Union[int, list]] = None,\n        last_window: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        check_inputs: bool = True\n    ) -> pd.Series:\n        \"\"\"\n        Predict n steps ahead.\n\n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n\n        Returns\n        -------\n        predictions : pandas Series\n            Predicted values.\n\n        \"\"\"\n\n        Xs, _, steps, prediction_index = self._create_predict_inputs(\n            steps=steps, last_window=last_window, exog=exog, check_inputs=check_inputs\n        )\n\n        regressors = [self.regressors_[step] for step in steps]\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\", \n                message=\"X does not have valid feature names\", \n                category=UserWarning\n            )\n            predictions = np.array([\n                regressor.predict(X).ravel()[0] \n                for regressor, X in zip(regressors, Xs)\n            ])\n\n        if self.differentiation is not None:\n            predictions = self.differentiator.inverse_transform_next_window(predictions)\n\n        predictions = transform_numpy(\n                          array             = predictions,\n                          transformer       = self.transformer_y,\n                          fit               = False,\n                          inverse_transform = True\n                      )\n\n        predictions = pd.Series(\n                          data  = predictions,\n                          index = prediction_index,\n                          name  = 'pred'\n                      )\n\n        return predictions\n\n    def predict_bootstrapping(\n        self,\n        steps: Optional[Union[int, list]] = None,\n        last_window: Optional[pd.Series] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: Any = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        Generate multiple forecasting predictions using a bootstrapping process. \n        By sampling from a collection of past observed errors (the residuals),\n        each iteration of bootstrapping generates a different set of predictions. \n        See the Notes section for more information. \n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        boot_predictions : pandas DataFrame\n            Predictions generated by bootstrapping.\n            Shape: (steps, n_boot)\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n        Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n        \"\"\"\n\n        if self.is_fitted:\n            \n            steps = prepare_steps_direct(\n                        steps    = steps,\n                        max_step = self.steps\n                    )\n            \n            if use_in_sample_residuals:\n                if not set(steps).issubset(set(self.in_sample_residuals_.keys())):\n                    raise ValueError(\n                        f\"Not `forecaster.in_sample_residuals_` for steps: \"\n                        f\"{set(steps) - set(self.in_sample_residuals_.keys())}.\"\n                    )\n                residuals = self.in_sample_residuals_\n            else:\n                if self.out_sample_residuals_ is None:\n                    raise ValueError(\n                        \"`forecaster.out_sample_residuals_` is `None`. Use \"\n                        \"`use_in_sample_residuals=True` or the \"\n                        \"`set_out_sample_residuals()` method before predicting.\"\n                    )\n                else:\n                    if not set(steps).issubset(set(self.out_sample_residuals_.keys())):\n                        raise ValueError(\n                            f\"No `forecaster.out_sample_residuals_` for steps: \"\n                            f\"{set(steps) - set(self.out_sample_residuals_.keys())}. \"\n                            f\"Use method `set_out_sample_residuals()`.\"\n                        )\n                residuals = self.out_sample_residuals_\n            \n            check_residuals = (\n                \"forecaster.in_sample_residuals_\" if use_in_sample_residuals\n                else \"forecaster.out_sample_residuals_\"\n            )\n            for step in steps:\n                if residuals[step] is None:\n                    raise ValueError(\n                        f\"forecaster residuals for step {step} are `None`. \"\n                        f\"Check {check_residuals}.\"\n                    )\n                elif any(x is None or np.isnan(x) for x in residuals[step]):\n                    raise ValueError(\n                        f\"forecaster residuals for step {step} contains `None` values. \"\n                        f\"Check {check_residuals}.\"\n                    )\n\n        Xs, _, steps, prediction_index = self._create_predict_inputs(\n            steps=steps, last_window=last_window, exog=exog\n        )\n\n        # NOTE: Predictions must be transformed and differenced before adding residuals\n        regressors = [self.regressors_[step] for step in steps]\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\", \n                message=\"X does not have valid feature names\", \n                category=UserWarning\n            )\n            predictions = np.array([\n                regressor.predict(X).ravel()[0] \n                for regressor, X in zip(regressors, Xs)\n            ])\n\n        boot_predictions = np.tile(predictions, (n_boot, 1)).T\n        boot_columns = [f\"pred_boot_{i}\" for i in range(n_boot)]\n\n        rng = np.random.default_rng(seed=random_state)\n        for i, step in enumerate(steps):\n            sampled_residuals = residuals[step][\n                rng.integers(low=0, high=len(residuals[step]), size=n_boot)\n            ]\n            boot_predictions[i, :] = boot_predictions[i, :] + sampled_residuals\n\n        if self.differentiation is not None:\n            boot_predictions = (\n                self.differentiator.inverse_transform_next_window(boot_predictions)\n            )\n\n        if self.transformer_y:\n            boot_predictions = np.apply_along_axis(\n                                   func1d            = transform_numpy,\n                                   axis              = 0,\n                                   arr               = boot_predictions,\n                                   transformer       = self.transformer_y,\n                                   fit               = False,\n                                   inverse_transform = True\n                               )\n    \n        boot_predictions = pd.DataFrame(\n                               data    = boot_predictions,\n                               index   = prediction_index,\n                               columns = boot_columns\n                           )\n        \n        return boot_predictions\n\n    def predict_interval(\n        self,\n        steps: Optional[Union[int, list]] = None,\n        last_window: Optional[pd.Series] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        interval: list = [5, 95],\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: Any = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        Bootstrapping based predicted intervals.\n        Both predictions and intervals are returned.\n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        interval : list, default `[5, 95]`\n            Confidence of the prediction interval estimated. Sequence of \n            percentiles to compute, which must be between 0 and 100 inclusive. \n            For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Values predicted by the forecaster and their estimated interval.\n\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n\n        check_interval(interval=interval)\n\n        boot_predictions = self.predict_bootstrapping(\n                               steps                   = steps,\n                               last_window             = last_window,\n                               exog                    = exog,\n                               n_boot                  = n_boot,\n                               random_state            = random_state,\n                               use_in_sample_residuals = use_in_sample_residuals\n                           )\n\n        predictions = self.predict(\n                          steps        = steps,\n                          last_window  = last_window,\n                          exog         = exog,\n                          check_inputs = False\n                      )\n\n        interval = np.array(interval) / 100\n        predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n        predictions_interval.columns = ['lower_bound', 'upper_bound']\n        predictions = pd.concat((predictions, predictions_interval), axis=1)\n\n        return predictions\n\n    def predict_quantiles(\n        self,\n        steps: Optional[Union[int, list]] = None,\n        last_window: Optional[pd.Series] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        quantiles: list = [0.05, 0.5, 0.95],\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: Any = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        Bootstrapping based predicted quantiles.\n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        quantiles : list, default `[0.05, 0.5, 0.95]`\n            Sequence of quantiles to compute, which must be between 0 and 1 \n            inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n            `quantiles = [0.05, 0.5, 0.95]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate quantiles.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot quantiles are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create prediction quantiles. If `False`, out of\n            sample residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Quantiles predicted by the forecaster.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n        \n        check_interval(quantiles=quantiles)\n\n        boot_predictions = self.predict_bootstrapping(\n                               steps                   = steps,\n                               last_window             = last_window,\n                               exog                    = exog,\n                               n_boot                  = n_boot,\n                               random_state            = random_state,\n                               use_in_sample_residuals = use_in_sample_residuals\n                           )\n\n        predictions = boot_predictions.quantile(q=quantiles, axis=1).transpose()\n        predictions.columns = [f'q_{q}' for q in quantiles]\n\n        return predictions\n    \n    def predict_dist(\n        self,\n        distribution: object,\n        steps: Optional[Union[int, list]] = None,\n        last_window: Optional[pd.Series] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: Any = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        Fit a given probability distribution for each step. After generating \n        multiple forecasting predictions through a bootstrapping process, each \n        step is fitted to the given distribution.\n        \n        Parameters\n        ----------\n        distribution : Object\n            A distribution object from scipy.stats.\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Distribution parameters estimated for each step.\n\n        \"\"\"\n        \n        boot_samples = self.predict_bootstrapping(\n                           steps                   = steps,\n                           last_window             = last_window,\n                           exog                    = exog,\n                           n_boot                  = n_boot,\n                           random_state            = random_state,\n                           use_in_sample_residuals = use_in_sample_residuals\n                       )       \n\n        param_names = [p for p in inspect.signature(distribution._pdf).parameters\n                       if not p == 'x'] + [\"loc\", \"scale\"]\n        param_values = np.apply_along_axis(\n                           lambda x: distribution.fit(x),\n                           axis = 1,\n                           arr  = boot_samples\n                       )\n        predictions = pd.DataFrame(\n                          data    = param_values,\n                          columns = param_names,\n                          index   = boot_samples.index\n                      )\n\n        return predictions\n\n    def set_params(\n        self, \n        params: dict\n    ) -> None:\n        \"\"\"\n        Set new values to the parameters of the scikit learn model stored in the\n        forecaster. It is important to note that all models share the same \n        configuration of parameters and hyperparameters.\n        \n        Parameters\n        ----------\n        params : dict\n            Parameters values.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        self.regressor = clone(self.regressor)\n        self.regressor.set_params(**params)\n        self.regressors_ = {\n            step: clone(self.regressor)\n            for step in range(1, self.steps + 1)\n        }\n\n    def set_fit_kwargs(\n        self, \n        fit_kwargs: dict\n    ) -> None:\n        \"\"\"\n        Set new values for the additional keyword arguments passed to the `fit` \n        method of the regressor.\n        \n        Parameters\n        ----------\n        fit_kwargs : dict\n            Dict of the form {\"argument\": new_value}.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n\n    def set_lags(\n        self, \n        lags: Optional[Union[int, list, np.ndarray, range]] = None\n    ) -> None:\n        \"\"\"\n        Set new value to the attribute `lags`. Attributes `lags_names`, \n        `max_lag` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        lags : int, list, numpy ndarray, range, default `None`\n            Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. \n        \n            - `int`: include lags from 1 to `lags` (included).\n            - `list`, `1d numpy ndarray` or `range`: include only lags present in \n            `lags`, all elements must be int.\n            - `None`: no lags are included as predictors. \n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        if self.window_features is None and lags is None:\n            raise ValueError(\n                (\"At least one of the arguments `lags` or `window_features` \"\n                 \"must be different from None. This is required to create the \"\n                 \"predictors used in training the forecaster.\")\n            )\n        \n        self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\n        self.window_size = max(\n            [ws for ws in [self.max_lag, self.max_size_window_features] \n             if ws is not None]\n        )\n        if self.differentiation is not None:\n            self.window_size += self.differentiation\n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_window_features(\n        self, \n        window_features: Optional[Union[object, list]] = None\n    ) -> None:\n        \"\"\"\n        Set new value to the attribute `window_features`. Attributes \n        `max_size_window_features`, `window_features_names`, \n        `window_features_class_names` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        window_features : object, list, default `None`\n            Instance or list of instances used to create window features. Window features\n            are created from the original time series and are included as predictors.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        if window_features is None and self.lags is None:\n            raise ValueError(\n                (\"At least one of the arguments `lags` or `window_features` \"\n                 \"must be different from None. This is required to create the \"\n                 \"predictors used in training the forecaster.\")\n            )\n        \n        self.window_features, self.window_features_names, self.max_size_window_features = (\n            initialize_window_features(window_features)\n        )\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [\n                type(wf).__name__ for wf in self.window_features\n            ] \n        self.window_size = max(\n            [ws for ws in [self.max_lag, self.max_size_window_features] \n             if ws is not None]\n        )\n        if self.differentiation is not None:\n            self.window_size += self.differentiation   \n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_out_sample_residuals(\n        self,\n        y_true: dict,\n        y_pred: dict,\n        append: bool = False,\n        random_state: int = 36987\n    ) -> None:\n        \"\"\"\n        Set new values to the attribute `out_sample_residuals_`. Out of sample\n        residuals are meant to be calculated using observations that did not\n        participate in the training process. `y_true` and `y_pred` are expected\n        to be in the original scale of the time series. Residuals are calculated\n        as `y_true` - `y_pred`, after applying the necessary transformations and\n        differentiations if the forecaster includes them (`self.transformer_y`\n        and `self.differentiation`).\n\n        A total of 10_000 residuals are stored in the attribute `out_sample_residuals_`.\n        If the number of residuals is greater than 10_000, a random sample of\n        10_000 residuals is stored.\n        \n        Parameters\n        ----------\n        y_true : dict\n            Dictionary of numpy ndarrays or pandas Series with the true values of\n            the time series for each model in the form {step: y_true}.\n        y_pred : dict\n            Dictionary of numpy ndarrays or pandas Series with the predicted values\n            of the time series for each model in the form {step: y_pred}.\n        append : bool, default `False`\n            If `True`, new residuals are added to the once already stored in the\n            attribute `out_sample_residuals_`. If after appending the new residuals,\n            the limit of 10_000 samples is exceeded, a random sample of 10_000 is\n            kept.\n        random_state : int, default `36987`\n            Sets a seed to the random sampling for reproducible output.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n\n        if not self.is_fitted:\n            raise NotFittedError(\n                \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n                \"arguments before using `set_out_sample_residuals()`.\"\n            )\n\n        if not isinstance(y_true, dict):\n            raise TypeError(\n                f\"`y_true` must be a dictionary of numpy ndarrays or pandas Series. \"\n                f\"Got {type(y_true)}.\"\n            )\n\n        if not isinstance(y_pred, dict):\n            raise TypeError(\n                f\"`y_pred` must be a dictionary of numpy ndarrays or pandas Series. \"\n                f\"Got {type(y_pred)}.\"\n            )\n        \n        if not set(y_true.keys()) == set(y_pred.keys()):\n            raise ValueError(\n                f\"`y_true` and `y_pred` must have the same keys. \"\n                f\"Got {set(y_true.keys())} and {set(y_pred.keys())}.\"\n            )\n        \n        for k in y_true.keys():\n            if not isinstance(y_true[k], (np.ndarray, pd.Series)):\n                raise TypeError(\n                    f\"Values of `y_true` must be numpy ndarrays or pandas Series. \"\n                    f\"Got {type(y_true[k])} for step {k}.\"\n                )\n            if not isinstance(y_pred[k], (np.ndarray, pd.Series)):\n                raise TypeError(\n                    f\"Values of `y_pred` must be numpy ndarrays or pandas Series. \"\n                    f\"Got {type(y_pred[k])} for step {k}.\"\n                )\n            if len(y_true[k]) != len(y_pred[k]):\n                raise ValueError(\n                    f\"`y_true` and `y_pred` must have the same length. \"\n                    f\"Got {len(y_true[k])} and {len(y_pred[k])} for step {k}.\"\n                )\n            if isinstance(y_true[k], pd.Series) and isinstance(y_pred[k], pd.Series):\n                if not y_true[k].index.equals(y_pred[k].index):\n                    raise ValueError(\n                        f\"When containing pandas Series, elements in `y_true` and \"\n                        f\"`y_pred` must have the same index. Error in step {k}.\"\n                    )\n        \n        if self.out_sample_residuals_ is None:\n            self.out_sample_residuals_ = {\n                step: None for step in range(1, self.steps + 1)\n            }\n        \n        steps_to_update = set(range(1, self.steps + 1)).intersection(set(y_pred.keys()))\n        if not steps_to_update:\n            raise ValueError(\n                \"Provided keys in `y_pred` and `y_true` do not match any step. \"\n                \"Residuals cannot be updated.\"\n            )\n\n        residuals = {}\n        rng = np.random.default_rng(seed=random_state)\n        y_true = y_true.copy()\n        y_pred = y_pred.copy()\n        if self.differentiation is not None:\n            differentiator = copy(self.differentiator)\n            differentiator.set_params(window_size=None)\n        \n        for k in steps_to_update:\n            if isinstance(y_true[k], pd.Series):\n                y_true[k] = y_true[k].to_numpy()\n            if isinstance(y_pred[k], pd.Series):\n                y_pred[k] = y_pred[k].to_numpy()\n            if self.transformer_y:\n                y_true[k] = transform_numpy(\n                                array             = y_true[k],\n                                transformer       = self.transformer_y,\n                                fit               = False,\n                                inverse_transform = False\n                            )\n                y_pred[k] = transform_numpy(\n                                array             = y_pred[k],\n                                transformer       = self.transformer_y,\n                                fit               = False,\n                                inverse_transform = False\n                            )\n            if self.differentiation is not None:\n                y_true[k] = differentiator.fit_transform(y_true[k])[self.differentiation:]\n                y_pred[k] = differentiator.fit_transform(y_pred[k])[self.differentiation:]\n\n            residuals[k] = y_true[k] - y_pred[k]\n\n        for key, value in residuals.items():\n            if append and self.out_sample_residuals_[key] is not None:\n                value = np.concatenate((\n                            self.out_sample_residuals_[key],\n                            value\n                        ))\n            if len(value) > 10000:\n                value = rng.choice(value, size=10000, replace=False)\n            self.out_sample_residuals_[key] = value\n\n    def get_feature_importances(\n        self, \n        step: int,\n        sort_importance: bool = True\n    ) -> pd.DataFrame:\n        \"\"\"\n        Return feature importance of the model stored in the forecaster for a\n        specific step. Since a separate model is created for each forecast time\n        step, it is necessary to select the model from which retrieve information.\n        Only valid when regressor stores internally the feature importances in\n        the attribute `feature_importances_` or `coef_`. Otherwise, it returns  \n        `None`.\n\n        Parameters\n        ----------\n        step : int\n            Model from which retrieve information (a separate model is created \n            for each forecast time step). First step is 1.\n        sort_importance: bool, default `True`\n            If `True`, sorts the feature importances in descending order.\n\n        Returns\n        -------\n        feature_importances : pandas DataFrame\n            Feature importances associated with each predictor.\n        \n        \"\"\"\n\n        if not isinstance(step, int):\n            raise TypeError(f\"`step` must be an integer. Got {type(step)}.\")\n        \n        if not self.is_fitted:\n            raise NotFittedError(\n                \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n                \"arguments before using `get_feature_importances()`.\"\n            )\n\n        if (step < 1) or (step > self.steps):\n            raise ValueError(\n                f\"The step must have a value from 1 to the maximum number of steps \"\n                f\"({self.steps}). Got {step}.\"\n            )\n\n        if isinstance(self.regressor, Pipeline):\n            estimator = self.regressors_[step][-1]\n        else:\n            estimator = self.regressors_[step]\n\n        n_lags = len(self.lags) if self.lags is not None else 0\n        n_window_features = (\n            len(self.window_features_names) if self.window_features is not None else 0\n        )\n        idx_columns_autoreg = np.arange(n_lags + n_window_features)\n        if not self.exog_in_:\n            idx_columns = idx_columns_autoreg\n        else:\n            n_exog = len(self.X_train_direct_exog_names_out_) / self.steps\n            idx_columns_exog = (\n                np.arange((step - 1) * n_exog, (step) * n_exog) + idx_columns_autoreg[-1] + 1\n            )\n            idx_columns = np.concatenate((idx_columns_autoreg, idx_columns_exog))\n        \n        idx_columns = [int(x) for x in idx_columns]  # Required since numpy 2.0\n        feature_names = [self.X_train_features_names_out_[i].replace(f\"_step_{step}\", \"\") \n                         for i in idx_columns]\n\n        if hasattr(estimator, 'feature_importances_'):\n            feature_importances = estimator.feature_importances_\n        elif hasattr(estimator, 'coef_'):\n            feature_importances = estimator.coef_\n        else:\n            warnings.warn(\n                (f\"Impossible to access feature importances for regressor of type \"\n                 f\"{type(estimator)}. This method is only valid when the \"\n                 f\"regressor stores internally the feature importances in the \"\n                 f\"attribute `feature_importances_` or `coef_`.\")\n            )\n            feature_importances = None\n\n        if feature_importances is not None:\n            feature_importances = pd.DataFrame({\n                                      'feature': feature_names,\n                                      'importance': feature_importances\n                                  })\n            if sort_importance:\n                feature_importances = feature_importances.sort_values(\n                                          by='importance', ascending=False\n                                      )\n\n        return feature_importances\n"
  },
  "GT_src_dict": {
    "skforecast/model_selection/_utils.py": {
      "_calculate_metrics_one_step_ahead": {
        "code": "def _calculate_metrics_one_step_ahead(forecaster: object, y: pd.Series, metrics: list, X_train: pd.DataFrame, y_train: Union[pd.Series, dict], X_test: pd.DataFrame, y_test: Union[pd.Series, dict]) -> list:\n    \"\"\"Calculate metrics for one-step-ahead predictions of a forecaster model. This function evaluates the model's predictive performance using specified metrics over the given training and testing datasets.\n\nParameters\n----------\nforecaster : object\n    The forecaster model being evaluated, which can be of various types using methods for prediction.\ny : pandas Series\n    The complete time series data used for training and testing.\nmetrics : list\n    A list of metric functions to evaluate the predictions against the actual values.\nX_train : pandas DataFrame\n    The predictor variables used for training the model.\ny_train : Union[pd.Series, dict]\n    The target values corresponding to `X_train` for training the model.\nX_test : pandas DataFrame\n    The predictor variables used for testing the model.\ny_test : Union[pd.Series, dict]\n    The true values corresponding to `X_test`, used for evaluating the model's performance.\n\nReturns\n-------\nmetric_values : list\n    A list containing the computed metric values for the predictions.\n\nNotes\n-----\nThe function determines if the forecaster is of type 'ForecasterDirect', in which case it only optimizes the model for step 1 of the predictions. The predictions may be inversely transformed if the forecaster uses differentiation or a transformation method. The `y_test`, `y_pred`, and `y_train` values are flattened to ensure compatibility with the metric functions before evaluation.\"\"\"\n    '\\n    Calculate metrics when predictions are one-step-ahead. When forecaster is\\n    of type ForecasterDirect only the regressor for step 1 is used.\\n\\n    Parameters\\n    ----------\\n    forecaster : object\\n        Forecaster model.\\n    y : pandas Series\\n        Time series data used to train and test the model.\\n    metrics : list\\n        List of metrics.\\n    X_train : pandas DataFrame\\n        Predictor values used to train the model.\\n    y_train : pandas Series\\n        Target values related to each row of `X_train`.\\n    X_test : pandas DataFrame\\n        Predictor values used to test the model.\\n    y_test : pandas Series\\n        Target values related to each row of `X_test`.\\n\\n    Returns\\n    -------\\n    metric_values : list\\n        List with metric values.\\n    \\n    '\n    if type(forecaster).__name__ == 'ForecasterDirect':\n        step = 1\n        X_train, y_train = forecaster.filter_train_X_y_for_step(step=step, X_train=X_train, y_train=y_train)\n        X_test, y_test = forecaster.filter_train_X_y_for_step(step=step, X_train=X_test, y_train=y_test)\n        forecaster.regressors_[step].fit(X_train, y_train)\n        y_pred = forecaster.regressors_[step].predict(X_test)\n    else:\n        forecaster.regressor.fit(X_train, y_train)\n        y_pred = forecaster.regressor.predict(X_test)\n    y_true = y_test.to_numpy()\n    y_pred = y_pred.ravel()\n    y_train = y_train.to_numpy()\n    if forecaster.differentiation is not None:\n        y_true = forecaster.differentiator.inverse_transform_next_window(y_true)\n        y_pred = forecaster.differentiator.inverse_transform_next_window(y_pred)\n        y_train = forecaster.differentiator.inverse_transform_training(y_train)\n    if forecaster.transformer_y is not None:\n        y_true = forecaster.transformer_y.inverse_transform(y_true.reshape(-1, 1))\n        y_pred = forecaster.transformer_y.inverse_transform(y_pred.reshape(-1, 1))\n        y_train = forecaster.transformer_y.inverse_transform(y_train.reshape(-1, 1))\n    metric_values = []\n    for m in metrics:\n        metric_values.append(m(y_true=y_true.ravel(), y_pred=y_pred.ravel(), y_train=y_train.ravel()))\n    return metric_values",
        "docstring": "Calculate metrics for one-step-ahead predictions of a forecaster model. This function evaluates the model's predictive performance using specified metrics over the given training and testing datasets.\n\nParameters\n----------\nforecaster : object\n    The forecaster model being evaluated, which can be of various types using methods for prediction.\ny : pandas Series\n    The complete time series data used for training and testing.\nmetrics : list\n    A list of metric functions to evaluate the predictions against the actual values.\nX_train : pandas DataFrame\n    The predictor variables used for training the model.\ny_train : Union[pd.Series, dict]\n    The target values corresponding to `X_train` for training the model.\nX_test : pandas DataFrame\n    The predictor variables used for testing the model.\ny_test : Union[pd.Series, dict]\n    The true values corresponding to `X_test`, used for evaluating the model's performance.\n\nReturns\n-------\nmetric_values : list\n    A list containing the computed metric values for the predictions.\n\nNotes\n-----\nThe function determines if the forecaster is of type 'ForecasterDirect', in which case it only optimizes the model for step 1 of the predictions. The predictions may be inversely transformed if the forecaster uses differentiation or a transformation method. The `y_test`, `y_pred`, and `y_train` values are flattened to ensure compatibility with the metric functions before evaluation.",
        "signature": "def _calculate_metrics_one_step_ahead(forecaster: object, y: pd.Series, metrics: list, X_train: pd.DataFrame, y_train: Union[pd.Series, dict], X_test: pd.DataFrame, y_test: Union[pd.Series, dict]) -> list:",
        "type": "Function",
        "class_signature": null
      }
    },
    "skforecast/recursive/_forecaster_recursive.py": {
      "ForecasterRecursive.__init__": {
        "code": "    def __init__(self, regressor: object, lags: Optional[Union[int, list, np.ndarray, range]]=None, window_features: Optional[Union[object, list]]=None, transformer_y: Optional[object]=None, transformer_exog: Optional[object]=None, weight_func: Optional[Callable]=None, differentiation: Optional[int]=None, fit_kwargs: Optional[dict]=None, binner_kwargs: Optional[dict]=None, forecaster_id: Optional[Union[str, int]]=None) -> None:\n        \"\"\"Initializes the ForecasterRecursive class, which transforms a scikit-learn compatible regressor into a recursive autoregressive forecaster for time series data.\n\n    Parameters\n    ----------\n    regressor : object\n        A scikit-learn compatible regressor or pipeline that will be trained to predict future values based on past observations and external features.\n    lags : Optional[Union[int, list, np.ndarray, range]], default=None\n        Specifies the lagged observations to use as predictors. Can be an integer, list, numpy array, or range.\n    window_features : Optional[Union[object, list]], default=None\n        Instances or lists of instances that create additional features from the time series.\n    transformer_y : Optional[object], default=None\n        A transformer for the target variable `y`, compatible with scikit-learn's API, used for preprocessing the target before fitting.\n    transformer_exog : Optional[object], default=None\n        A transformer for exogenous variables, compatible with scikit-learn's API, used for preprocessing external features before fitting.\n    weight_func : Optional[Callable], default=None\n        A function defining sample weights based on the index for the regression process, if applicable.\n    differentiation : Optional[int], default=None\n        Specifies the order of differencing to apply to the time series prior to fitting the model.\n    fit_kwargs : Optional[dict], default=None\n        Additional keyword arguments for fitting the regressor.\n    binner_kwargs : Optional[dict], default=None\n        Parameters for the QuantileBinner used for binning residuals based on predictions.\n    forecaster_id : Optional[Union[str, int]], default=None\n        An identifier for the forecaster instance.\n\n    Attributes\n    ----------\n    regressor : object\n        A copy of the provided regressor, ready for fitting.\n    lags : numpy ndarray\n        The lags selected for the training process.\n    window_size : int\n        The size of the window for generating predictors, determined by the maximum lag and size of window features.\n    differentiation : int\n        Stores the order of differentiation for the time series, modified based on input.\n    binner : QuantileBinner\n        An instance of the QuantileBinner, initialized with the binner_kwargs for discretizing residuals.\n\n    Raises\n    ------\n    ValueError\n        If neither `lags` nor `window_features` are provided, at least one of them must be specified.\n    TypeError\n        If incompatible types are supplied to parameters expecting specific types, such as `y` or `exog`.\"\"\"\n        self.regressor = copy(regressor)\n        self.transformer_y = transformer_y\n        self.transformer_exog = transformer_exog\n        self.weight_func = weight_func\n        self.source_code_weight_func = None\n        self.differentiation = differentiation\n        self.differentiator = None\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.exog_in_ = False\n        self.exog_names_in_ = None\n        self.exog_type_in_ = None\n        self.exog_dtypes_in_ = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_ = None\n        self.X_train_features_names_out_ = None\n        self.in_sample_residuals_ = None\n        self.out_sample_residuals_ = None\n        self.in_sample_residuals_by_bin_ = None\n        self.out_sample_residuals_by_bin_ = None\n        self.creation_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.is_fitted = False\n        self.fit_date = None\n        self.skforecast_version = skforecast.__version__\n        self.python_version = sys.version.split(' ')[0]\n        self.forecaster_id = forecaster_id\n        self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\n        self.window_features, self.window_features_names, self.max_size_window_features = initialize_window_features(window_features)\n        if self.window_features is None and self.lags is None:\n            raise ValueError('At least one of the arguments `lags` or `window_features` must be different from None. This is required to create the predictors used in training the forecaster.')\n        self.window_size = max([ws for ws in [self.max_lag, self.max_size_window_features] if ws is not None])\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [type(wf).__name__ for wf in self.window_features]\n        self.binner_kwargs = binner_kwargs\n        if binner_kwargs is None:\n            self.binner_kwargs = {'n_bins': 10, 'method': 'linear', 'subsample': 200000, 'random_state': 789654, 'dtype': np.float64}\n        self.binner = QuantileBinner(**self.binner_kwargs)\n        self.binner_intervals_ = None\n        if self.differentiation is not None:\n            if not isinstance(differentiation, int) or differentiation < 1:\n                raise ValueError(f'Argument `differentiation` must be an integer equal to or greater than 1. Got {differentiation}.')\n            self.window_size += self.differentiation\n            self.differentiator = TimeSeriesDifferentiator(order=self.differentiation, window_size=self.window_size)\n        self.weight_func, self.source_code_weight_func, _ = initialize_weights(forecaster_name=type(self).__name__, regressor=regressor, weight_func=weight_func, series_weights=None)\n        self.fit_kwargs = check_select_fit_kwargs(regressor=regressor, fit_kwargs=fit_kwargs)",
        "docstring": "Initializes the ForecasterRecursive class, which transforms a scikit-learn compatible regressor into a recursive autoregressive forecaster for time series data.\n\nParameters\n----------\nregressor : object\n    A scikit-learn compatible regressor or pipeline that will be trained to predict future values based on past observations and external features.\nlags : Optional[Union[int, list, np.ndarray, range]], default=None\n    Specifies the lagged observations to use as predictors. Can be an integer, list, numpy array, or range.\nwindow_features : Optional[Union[object, list]], default=None\n    Instances or lists of instances that create additional features from the time series.\ntransformer_y : Optional[object], default=None\n    A transformer for the target variable `y`, compatible with scikit-learn's API, used for preprocessing the target before fitting.\ntransformer_exog : Optional[object], default=None\n    A transformer for exogenous variables, compatible with scikit-learn's API, used for preprocessing external features before fitting.\nweight_func : Optional[Callable], default=None\n    A function defining sample weights based on the index for the regression process, if applicable.\ndifferentiation : Optional[int], default=None\n    Specifies the order of differencing to apply to the time series prior to fitting the model.\nfit_kwargs : Optional[dict], default=None\n    Additional keyword arguments for fitting the regressor.\nbinner_kwargs : Optional[dict], default=None\n    Parameters for the QuantileBinner used for binning residuals based on predictions.\nforecaster_id : Optional[Union[str, int]], default=None\n    An identifier for the forecaster instance.\n\nAttributes\n----------\nregressor : object\n    A copy of the provided regressor, ready for fitting.\nlags : numpy ndarray\n    The lags selected for the training process.\nwindow_size : int\n    The size of the window for generating predictors, determined by the maximum lag and size of window features.\ndifferentiation : int\n    Stores the order of differentiation for the time series, modified based on input.\nbinner : QuantileBinner\n    An instance of the QuantileBinner, initialized with the binner_kwargs for discretizing residuals.\n\nRaises\n------\nValueError\n    If neither `lags` nor `window_features` are provided, at least one of them must be specified.\nTypeError\n    If incompatible types are supplied to parameters expecting specific types, such as `y` or `exog`.",
        "signature": "def __init__(self, regressor: object, lags: Optional[Union[int, list, np.ndarray, range]]=None, window_features: Optional[Union[object, list]]=None, transformer_y: Optional[object]=None, transformer_exog: Optional[object]=None, weight_func: Optional[Callable]=None, differentiation: Optional[int]=None, fit_kwargs: Optional[dict]=None, binner_kwargs: Optional[dict]=None, forecaster_id: Optional[Union[str, int]]=None) -> None:",
        "type": "Method",
        "class_signature": "class ForecasterRecursive(ForecasterBase):"
      },
      "ForecasterRecursive._train_test_split_one_step_ahead": {
        "code": "    def _train_test_split_one_step_ahead(self, y: pd.Series, initial_train_size: int, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n        \"\"\"Create training and testing matrices for one-step-ahead predictions using the specified initial training size.\n\nParameters\n----------\ny : pandas Series\n    The time series data to be used for training and testing.\ninitial_train_size : int\n    The number of observations to be used for the initial training set.\nexog : pandas Series, pandas DataFrame, default `None`\n    Exogenous variables to be used as predictors. They must have the same number of observations as `y`, and their indices should be aligned with `y`.\n\nReturns\n-------\nX_train : pandas DataFrame\n    Matrix of predictor values used for training the model.\ny_train : pandas Series\n    Target values associated with each row of `X_train`.\nX_test : pandas DataFrame\n    Matrix of predictor values used for testing the model.\ny_test : pandas Series\n    Target values associated with each row of `X_test`.\n\nNotes\n-----\nThe method updates the internal state variable `is_fitted` to manage the fitting status of the forecaster temporarily during the split and restores it afterward. The function interacts with the `_create_train_X_y()` method to generate the training and testing datasets. It assumes that the predictions should be made after a specified initial training size minus the window size necessary for the forecaster.\"\"\"\n        '\\n        Create matrices needed to train and test the forecaster for one-step-ahead\\n        predictions.\\n\\n        Parameters\\n        ----------\\n        y : pandas Series\\n            Training time series.\\n        initial_train_size : int\\n            Initial size of the training set. It is the number of observations used\\n            to train the forecaster before making the first prediction.\\n        exog : pandas Series, pandas DataFrame, default `None`\\n            Exogenous variable/s included as predictor/s. Must have the same\\n            number of observations as `y` and their indexes must be aligned.\\n        \\n        Returns\\n        -------\\n        X_train : pandas DataFrame\\n            Predictor values used to train the model.\\n        y_train : pandas Series\\n            Target values related to each row of `X_train`.\\n        X_test : pandas DataFrame\\n            Predictor values used to test the model.\\n        y_test : pandas Series\\n            Target values related to each row of `X_test`.\\n        \\n        '\n        is_fitted = self.is_fitted\n        self.is_fitted = False\n        X_train, y_train, *_ = self._create_train_X_y(y=y.iloc[:initial_train_size], exog=exog.iloc[:initial_train_size] if exog is not None else None)\n        test_init = initial_train_size - self.window_size\n        self.is_fitted = True\n        X_test, y_test, *_ = self._create_train_X_y(y=y.iloc[test_init:], exog=exog.iloc[test_init:] if exog is not None else None)\n        self.is_fitted = is_fitted\n        return (X_train, y_train, X_test, y_test)",
        "docstring": "Create training and testing matrices for one-step-ahead predictions using the specified initial training size.\n\nParameters\n----------\ny : pandas Series\n    The time series data to be used for training and testing.\ninitial_train_size : int\n    The number of observations to be used for the initial training set.\nexog : pandas Series, pandas DataFrame, default `None`\n    Exogenous variables to be used as predictors. They must have the same number of observations as `y`, and their indices should be aligned with `y`.\n\nReturns\n-------\nX_train : pandas DataFrame\n    Matrix of predictor values used for training the model.\ny_train : pandas Series\n    Target values associated with each row of `X_train`.\nX_test : pandas DataFrame\n    Matrix of predictor values used for testing the model.\ny_test : pandas Series\n    Target values associated with each row of `X_test`.\n\nNotes\n-----\nThe method updates the internal state variable `is_fitted` to manage the fitting status of the forecaster temporarily during the split and restores it afterward. The function interacts with the `_create_train_X_y()` method to generate the training and testing datasets. It assumes that the predictions should be made after a specified initial training size minus the window size necessary for the forecaster.",
        "signature": "def _train_test_split_one_step_ahead(self, y: pd.Series, initial_train_size: int, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:",
        "type": "Method",
        "class_signature": "class ForecasterRecursive(ForecasterBase):"
      }
    },
    "skforecast/metrics/metrics.py": {
      "add_y_train_argument": {
        "code": "def add_y_train_argument(func: Callable) -> Callable:\n    \"\"\"Add a `y_train` argument to the given function if it is not already present in its signature.\n\nParameters\n----------\nfunc : Callable\n    The original function to which the `y_train` argument is added.\n\nReturns\n-------\nwrapper : Callable\n    A new function (wrapper) that includes the `y_train` argument, allowing it to be specified as a keyword argument.\n\nThis function leverages the `inspect` module to check the parameters of the original function and modifies its signature accordingly. It is intended to enhance compatibility with functions that require both target and training set data in applications such as model evaluation metrics.\"\"\"\n    '\\n    Add `y_train` argument to a function if it is not already present.\\n\\n    Parameters\\n    ----------\\n    func : callable\\n        Function to which the argument is added.\\n\\n    Returns\\n    -------\\n    wrapper : callable\\n        Function with `y_train` argument added.\\n    \\n    '\n    sig = inspect.signature(func)\n    if 'y_train' in sig.parameters:\n        return func\n    new_params = list(sig.parameters.values()) + [inspect.Parameter('y_train', inspect.Parameter.KEYWORD_ONLY, default=None)]\n    new_sig = sig.replace(parameters=new_params)\n\n    @wraps(func)\n    def wrapper(*args, y_train=None, **kwargs):\n        return func(*args, **kwargs)\n    wrapper.__signature__ = new_sig\n    return wrapper",
        "docstring": "Add a `y_train` argument to the given function if it is not already present in its signature.\n\nParameters\n----------\nfunc : Callable\n    The original function to which the `y_train` argument is added.\n\nReturns\n-------\nwrapper : Callable\n    A new function (wrapper) that includes the `y_train` argument, allowing it to be specified as a keyword argument.\n\nThis function leverages the `inspect` module to check the parameters of the original function and modifies its signature accordingly. It is intended to enhance compatibility with functions that require both target and training set data in applications such as model evaluation metrics.",
        "signature": "def add_y_train_argument(func: Callable) -> Callable:",
        "type": "Function",
        "class_signature": null
      }
    },
    "skforecast/direct/_forecaster_direct.py": {
      "ForecasterDirect.__init__": {
        "code": "    def __init__(self, regressor: object, steps: int, lags: Optional[Union[int, list, np.ndarray, range]]=None, window_features: Optional[Union[object, list]]=None, transformer_y: Optional[object]=None, transformer_exog: Optional[object]=None, weight_func: Optional[Callable]=None, differentiation: Optional[int]=None, fit_kwargs: Optional[dict]=None, n_jobs: Union[int, str]='auto', forecaster_id: Optional[Union[str, int]]=None) -> None:\n        \"\"\"Initializes a ForecasterDirect object, which turns any scikit-learn compatible regressor into an autoregressive direct multi-step forecaster.\n\nParameters\n----------\nregressor : object\n    A regressor or pipeline compatible with the scikit-learn API.\nsteps : int\n    The maximum number of future steps the forecaster will predict using the `predict()` method.\nlags : Optional[Union[int, list, np.ndarray, range]], default `None`\n    Lags used as predictors, specifying how many previous time steps are included as features.\nwindow_features : Optional[Union[object, list]], default `None`\n    Instance or list of instances for creating window features from the original time series.\ntransformer_y : Optional[object], default `None`\n    A transformer compatible with scikit-learn's preprocessing API for preprocessing the target `y`.\ntransformer_exog : Optional[object], default `None`\n    A transformer compatible with scikit-learn's preprocessing API for preprocessing exogenous variables.\nweight_func : Optional[Callable], default `None`\n    A function defining sample weights based on the index.\ndifferentiation : Optional[int], default `None`\n    Order of differencing applied to the time series before training.\nfit_kwargs : Optional[dict], default `None`\n    Additional arguments to pass to the `fit` method of the regressor.\nn_jobs : Union[int, str], default `'auto'`\n    Number of jobs to run in parallel; 'auto' uses all available cores.\nforecaster_id : Optional[Union[str, int]], default `None`\n    Identifier for the forecaster instance.\n\nReturns\n-------\nNone\n\nAttributes\n----------\nself.regressors_ : dict\n    Dictionary of cloned regressors for each forecasting step.\nself.window_size : int\n    The size of the window needed to create predictors, derived from lags and window features.\nself.last_window_ : Optional[pd.DataFrame]\n    Stores the last observed values needed for immediate predictions after training.\n\nRaises\n------\nTypeError\n    If `steps` is not an integer or if `n_jobs` is not an integer or 'auto'.\nValueError\n    If `steps` is less than 1, or if both `lags` and `window_features` are `None`.\"\"\"\n        self.regressor = copy(regressor)\n        self.steps = steps\n        self.transformer_y = transformer_y\n        self.transformer_exog = transformer_exog\n        self.weight_func = weight_func\n        self.source_code_weight_func = None\n        self.differentiation = differentiation\n        self.differentiator = None\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.exog_in_ = False\n        self.exog_names_in_ = None\n        self.exog_type_in_ = None\n        self.exog_dtypes_in_ = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_ = None\n        self.X_train_direct_exog_names_out_ = None\n        self.X_train_features_names_out_ = None\n        self.creation_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.is_fitted = False\n        self.fit_date = None\n        self.skforecast_version = skforecast.__version__\n        self.python_version = sys.version.split(' ')[0]\n        self.forecaster_id = forecaster_id\n        self.binner = None\n        self.binner_intervals_ = None\n        self.binner_kwargs = None\n        self.in_sample_residuals_by_bin_ = None\n        self.out_sample_residuals_by_bin_ = None\n        if not isinstance(steps, int):\n            raise TypeError(f'`steps` argument must be an int greater than or equal to 1. Got {type(steps)}.')\n        if steps < 1:\n            raise ValueError(f'`steps` argument must be greater than or equal to 1. Got {steps}.')\n        self.regressors_ = {step: clone(self.regressor) for step in range(1, steps + 1)}\n        self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\n        self.window_features, self.window_features_names, self.max_size_window_features = initialize_window_features(window_features)\n        if self.window_features is None and self.lags is None:\n            raise ValueError('At least one of the arguments `lags` or `window_features` must be different from None. This is required to create the predictors used in training the forecaster.')\n        self.window_size = max([ws for ws in [self.max_lag, self.max_size_window_features] if ws is not None])\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [type(wf).__name__ for wf in self.window_features]\n        if self.differentiation is not None:\n            if not isinstance(differentiation, int) or differentiation < 1:\n                raise ValueError(f'Argument `differentiation` must be an integer equal to or greater than 1. Got {differentiation}.')\n            self.window_size += self.differentiation\n            self.differentiator = TimeSeriesDifferentiator(order=self.differentiation, window_size=self.window_size)\n        self.weight_func, self.source_code_weight_func, _ = initialize_weights(forecaster_name=type(self).__name__, regressor=regressor, weight_func=weight_func, series_weights=None)\n        self.fit_kwargs = check_select_fit_kwargs(regressor=regressor, fit_kwargs=fit_kwargs)\n        self.in_sample_residuals_ = {step: None for step in range(1, steps + 1)}\n        self.out_sample_residuals_ = None\n        if n_jobs == 'auto':\n            self.n_jobs = select_n_jobs_fit_forecaster(forecaster_name=type(self).__name__, regressor=self.regressor)\n        else:\n            if not isinstance(n_jobs, int):\n                raise TypeError(f\"`n_jobs` must be an integer or `'auto'`. Got {type(n_jobs)}.\")\n            self.n_jobs = n_jobs if n_jobs > 0 else cpu_count()",
        "docstring": "Initializes a ForecasterDirect object, which turns any scikit-learn compatible regressor into an autoregressive direct multi-step forecaster.\n\nParameters\n----------\nregressor : object\n    A regressor or pipeline compatible with the scikit-learn API.\nsteps : int\n    The maximum number of future steps the forecaster will predict using the `predict()` method.\nlags : Optional[Union[int, list, np.ndarray, range]], default `None`\n    Lags used as predictors, specifying how many previous time steps are included as features.\nwindow_features : Optional[Union[object, list]], default `None`\n    Instance or list of instances for creating window features from the original time series.\ntransformer_y : Optional[object], default `None`\n    A transformer compatible with scikit-learn's preprocessing API for preprocessing the target `y`.\ntransformer_exog : Optional[object], default `None`\n    A transformer compatible with scikit-learn's preprocessing API for preprocessing exogenous variables.\nweight_func : Optional[Callable], default `None`\n    A function defining sample weights based on the index.\ndifferentiation : Optional[int], default `None`\n    Order of differencing applied to the time series before training.\nfit_kwargs : Optional[dict], default `None`\n    Additional arguments to pass to the `fit` method of the regressor.\nn_jobs : Union[int, str], default `'auto'`\n    Number of jobs to run in parallel; 'auto' uses all available cores.\nforecaster_id : Optional[Union[str, int]], default `None`\n    Identifier for the forecaster instance.\n\nReturns\n-------\nNone\n\nAttributes\n----------\nself.regressors_ : dict\n    Dictionary of cloned regressors for each forecasting step.\nself.window_size : int\n    The size of the window needed to create predictors, derived from lags and window features.\nself.last_window_ : Optional[pd.DataFrame]\n    Stores the last observed values needed for immediate predictions after training.\n\nRaises\n------\nTypeError\n    If `steps` is not an integer or if `n_jobs` is not an integer or 'auto'.\nValueError\n    If `steps` is less than 1, or if both `lags` and `window_features` are `None`.",
        "signature": "def __init__(self, regressor: object, steps: int, lags: Optional[Union[int, list, np.ndarray, range]]=None, window_features: Optional[Union[object, list]]=None, transformer_y: Optional[object]=None, transformer_exog: Optional[object]=None, weight_func: Optional[Callable]=None, differentiation: Optional[int]=None, fit_kwargs: Optional[dict]=None, n_jobs: Union[int, str]='auto', forecaster_id: Optional[Union[str, int]]=None) -> None:",
        "type": "Method",
        "class_signature": "class ForecasterDirect(ForecasterBase):"
      },
      "ForecasterDirect._train_test_split_one_step_ahead": {
        "code": "    def _train_test_split_one_step_ahead(self, y: pd.Series, initial_train_size: int, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, dict, pd.DataFrame, dict]:\n        \"\"\"Create matrices needed to train and test the forecaster for one-step-ahead predictions.\n\nParameters\n----------\ny : pandas Series\n    The training time series.\ninitial_train_size : int\n    The initial size of the training set, representing the number of observations \n    used to train the forecaster before making the first prediction.\nexog : pandas Series, pandas DataFrame, optional, default `None`\n    Exogenous variables included as predictors, which must have the same number \n    of observations as `y` and aligned indexes.\n\nReturns\n-------\nX_train : pandas DataFrame\n    Predictor values used to train the model.\ny_train : dict\n    Values of the time series related to each row of `X_train` for each step \n    in the form {step: y_step_[i]}.\nX_test : pandas DataFrame\n    Predictor values used to test the model.\ny_test : dict\n    Values of the time series related to each row of `X_test` for each step \n    in the form {step: y_step_[i]}.\n\nNotes\n-----\nThe method temporarily sets `self.is_fitted` to `False` to allow for training \ndata preparation without needing a fully fitted forecaster state. The `test_init` \nvariable is computed as the initial training size minus the `window_size` to \nproperly segment the testing data after the training. This method relies on \nother class methods such as `_create_train_X_y` to generate training matrices.\"\"\"\n        '\\n        Create matrices needed to train and test the forecaster for one-step-ahead\\n        predictions.\\n\\n        Parameters\\n        ----------\\n        y : pandas Series\\n            Training time series.\\n        initial_train_size : int\\n            Initial size of the training set. It is the number of observations used\\n            to train the forecaster before making the first prediction.\\n        exog : pandas Series, pandas DataFrame, default `None`\\n            Exogenous variable/s included as predictor/s. Must have the same\\n            number of observations as `y` and their indexes must be aligned.\\n        \\n        Returns\\n        -------\\n        X_train : pandas DataFrame\\n            Predictor values used to train the model.\\n        y_train : dict\\n            Values of the time series related to each row of `X_train` for each \\n            step in the form {step: y_step_[i]}.\\n        X_test : pandas DataFrame\\n            Predictor values used to test the model.\\n        y_test : dict\\n            Values of the time series related to each row of `X_test` for each \\n            step in the form {step: y_step_[i]}.\\n        \\n        '\n        is_fitted = self.is_fitted\n        self.is_fitted = False\n        X_train, y_train, *_ = self._create_train_X_y(y=y.iloc[:initial_train_size], exog=exog.iloc[:initial_train_size] if exog is not None else None)\n        test_init = initial_train_size - self.window_size\n        self.is_fitted = True\n        X_test, y_test, *_ = self._create_train_X_y(y=y.iloc[test_init:], exog=exog.iloc[test_init:] if exog is not None else None)\n        self.is_fitted = is_fitted\n        return (X_train, y_train, X_test, y_test)",
        "docstring": "Create matrices needed to train and test the forecaster for one-step-ahead predictions.\n\nParameters\n----------\ny : pandas Series\n    The training time series.\ninitial_train_size : int\n    The initial size of the training set, representing the number of observations \n    used to train the forecaster before making the first prediction.\nexog : pandas Series, pandas DataFrame, optional, default `None`\n    Exogenous variables included as predictors, which must have the same number \n    of observations as `y` and aligned indexes.\n\nReturns\n-------\nX_train : pandas DataFrame\n    Predictor values used to train the model.\ny_train : dict\n    Values of the time series related to each row of `X_train` for each step \n    in the form {step: y_step_[i]}.\nX_test : pandas DataFrame\n    Predictor values used to test the model.\ny_test : dict\n    Values of the time series related to each row of `X_test` for each step \n    in the form {step: y_step_[i]}.\n\nNotes\n-----\nThe method temporarily sets `self.is_fitted` to `False` to allow for training \ndata preparation without needing a fully fitted forecaster state. The `test_init` \nvariable is computed as the initial training size minus the `window_size` to \nproperly segment the testing data after the training. This method relies on \nother class methods such as `_create_train_X_y` to generate training matrices.",
        "signature": "def _train_test_split_one_step_ahead(self, y: pd.Series, initial_train_size: int, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, dict, pd.DataFrame, dict]:",
        "type": "Method",
        "class_signature": "class ForecasterDirect(ForecasterBase):"
      }
    }
  },
  "dependency_dict": {
    "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:__init__": {
      "skforecast/utils/utils.py": {
        "initialize_lags": {
          "code": "def initialize_lags(\n    forecaster_name: str,\n    lags: Any\n) -> Union[Optional[np.ndarray], Optional[list], Optional[int]]:\n    \"\"\"\n    Check lags argument input and generate the corresponding numpy ndarray.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    lags : Any\n        Lags used as predictors.\n\n    Returns\n    -------\n    lags : numpy ndarray, None\n        Lags used as predictors.\n    lags_names : list, None\n        Names of the lags used as predictors.\n    max_lag : int, None\n        Maximum value of the lags.\n    \n    \"\"\"\n\n    lags_names = None\n    max_lag = None\n    if lags is not None:\n        if isinstance(lags, int):\n            if lags < 1:\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n            lags = np.arange(1, lags + 1)\n\n        if isinstance(lags, (list, tuple, range)):\n            lags = np.array(lags)\n        \n        if isinstance(lags, np.ndarray):\n            if lags.size == 0:\n                return None, None, None\n            if lags.ndim != 1:\n                raise ValueError(\"`lags` must be a 1-dimensional array.\")\n            if not np.issubdtype(lags.dtype, np.integer):\n                raise TypeError(\"All values in `lags` must be integers.\")\n            if np.any(lags < 1):\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n        else:\n            if forecaster_name != 'ForecasterDirectMultiVariate':\n                raise TypeError(\n                    (f\"`lags` argument must be an int, 1d numpy ndarray, range, \"\n                     f\"tuple or list. Got {type(lags)}.\")\n                )\n            else:\n                raise TypeError(\n                    (f\"`lags` argument must be a dict, int, 1d numpy ndarray, range, \"\n                     f\"tuple or list. Got {type(lags)}.\")\n                )\n        \n        lags_names = [f'lag_{i}' for i in lags]\n        max_lag = max(lags)\n\n    return lags, lags_names, max_lag",
          "docstring": "Check lags argument input and generate the corresponding numpy ndarray.\n\nParameters\n----------\nforecaster_name : str\n    Forecaster name.\nlags : Any\n    Lags used as predictors.\n\nReturns\n-------\nlags : numpy ndarray, None\n    Lags used as predictors.\nlags_names : list, None\n    Names of the lags used as predictors.\nmax_lag : int, None\n    Maximum value of the lags.",
          "signature": "def initialize_lags(forecaster_name: str, lags: Any) -> Union[Optional[np.ndarray], Optional[list], Optional[int]]:",
          "type": "Function",
          "class_signature": null
        },
        "initialize_window_features": {
          "code": "def initialize_window_features(\n    window_features: Any\n) -> Union[Optional[list], Optional[list], Optional[int]]:\n    \"\"\"\n    Check window_features argument input and generate the corresponding list.\n\n    Parameters\n    ----------\n    window_features : Any\n        Classes used to create window features.\n\n    Returns\n    -------\n    window_features : list, None\n        List of classes used to create window features.\n    window_features_names : list, None\n        List with all the features names of the window features.\n    max_size_window_features : int, None\n        Maximum value of the `window_sizes` attribute of all classes.\n    \n    \"\"\"\n\n    needed_atts = ['window_sizes', 'features_names']\n    needed_methods = ['transform_batch', 'transform']\n\n    max_window_sizes = None\n    window_features_names = None\n    max_size_window_features = None\n    if window_features is not None:\n        if isinstance(window_features, list) and len(window_features) < 1:\n            raise ValueError(\n                \"Argument `window_features` must contain at least one element.\"\n            )\n        if not isinstance(window_features, list):\n            window_features = [window_features]\n\n        link_to_docs = (\n            \"\\nVisit the documentation for more information about how to create \"\n            \"custom window features:\\n\"\n            \"https://skforecast.org/latest/user_guides/window-features-and-custom-features.html#create-your-custom-window-features\"\n        )\n        \n        max_window_sizes = []\n        window_features_names = []\n        for wf in window_features:\n            wf_name = type(wf).__name__\n            atts_methods = set([a for a in dir(wf)])\n            if not set(needed_atts).issubset(atts_methods):\n                raise ValueError(\n                    f\"{wf_name} must have the attributes: {needed_atts}.\" + link_to_docs\n                )\n            if not set(needed_methods).issubset(atts_methods):\n                raise ValueError(\n                    f\"{wf_name} must have the methods: {needed_methods}.\" + link_to_docs\n                )\n            \n            window_sizes = wf.window_sizes\n            if not isinstance(window_sizes, (int, list)):\n                raise TypeError(\n                    f\"Attribute `window_sizes` of {wf_name} must be an int or a list \"\n                    f\"of ints. Got {type(window_sizes)}.\" + link_to_docs\n                )\n            \n            if isinstance(window_sizes, int):\n                if window_sizes < 1:\n                    raise ValueError(\n                        f\"If argument `window_sizes` is an integer, it must be equal to or \"\n                        f\"greater than 1. Got {window_sizes} from {wf_name}.\" + link_to_docs\n                    )\n                max_window_sizes.append(window_sizes)\n            else:\n                if not all(isinstance(ws, int) for ws in window_sizes) or not all(\n                    ws >= 1 for ws in window_sizes\n                ):                    \n                    raise ValueError(\n                        f\"If argument `window_sizes` is a list, all elements must be integers \"\n                        f\"equal to or greater than 1. Got {window_sizes} from {wf_name}.\" + link_to_docs\n                    )\n                max_window_sizes.append(max(window_sizes))\n\n            features_names = wf.features_names\n            if not isinstance(features_names, (str, list)):\n                raise TypeError(\n                    f\"Attribute `features_names` of {wf_name} must be a str or \"\n                    f\"a list of strings. Got {type(features_names)}.\" + link_to_docs\n                )\n            if isinstance(features_names, str):\n                window_features_names.append(features_names)\n            else:\n                if not all(isinstance(fn, str) for fn in features_names):\n                    raise TypeError(\n                        f\"If argument `features_names` is a list, all elements \"\n                        f\"must be strings. Got {features_names} from {wf_name}.\" + link_to_docs\n                    )\n                window_features_names.extend(features_names)\n\n        max_size_window_features = max(max_window_sizes)\n        if len(set(window_features_names)) != len(window_features_names):\n            raise ValueError(\n                f\"All window features names must be unique. Got {window_features_names}.\"\n            )\n\n    return window_features, window_features_names, max_size_window_features",
          "docstring": "Check window_features argument input and generate the corresponding list.\n\nParameters\n----------\nwindow_features : Any\n    Classes used to create window features.\n\nReturns\n-------\nwindow_features : list, None\n    List of classes used to create window features.\nwindow_features_names : list, None\n    List with all the features names of the window features.\nmax_size_window_features : int, None\n    Maximum value of the `window_sizes` attribute of all classes.",
          "signature": "def initialize_window_features(window_features: Any) -> Union[Optional[list], Optional[list], Optional[int]]:",
          "type": "Function",
          "class_signature": null
        },
        "initialize_weights": {
          "code": "def initialize_weights(\n    forecaster_name: str,\n    regressor: object,\n    weight_func: Union[Callable, dict],\n    series_weights: dict\n) -> Tuple[Union[Callable, dict], Union[str, dict], dict]:\n    \"\"\"\n    Check weights arguments, `weight_func` and `series_weights` for the different \n    forecasters. Create `source_code_weight_func`, source code of the custom \n    function(s) used to create weights.\n    \n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        Regressor of the forecaster.\n    weight_func : Callable, dict\n        Argument `weight_func` of the forecaster.\n    series_weights : dict\n        Argument `series_weights` of the forecaster.\n\n    Returns\n    -------\n    weight_func : Callable, dict\n        Argument `weight_func` of the forecaster.\n    source_code_weight_func : str, dict\n        Argument `source_code_weight_func` of the forecaster.\n    series_weights : dict\n        Argument `series_weights` of the forecaster.\n    \n    \"\"\"\n\n    source_code_weight_func = None\n\n    if weight_func is not None:\n\n        if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n            if not isinstance(weight_func, (Callable, dict)):\n                raise TypeError(\n                    (f\"Argument `weight_func` must be a Callable or a dict of \"\n                     f\"Callables. Got {type(weight_func)}.\")\n                )\n        elif not isinstance(weight_func, Callable):\n            raise TypeError(\n                f\"Argument `weight_func` must be a Callable. Got {type(weight_func)}.\"\n            )\n        \n        if isinstance(weight_func, dict):\n            source_code_weight_func = {}\n            for key in weight_func:\n                source_code_weight_func[key] = inspect.getsource(weight_func[key])\n        else:\n            source_code_weight_func = inspect.getsource(weight_func)\n\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(\n                (f\"Argument `weight_func` is ignored since regressor {regressor} \"\n                 f\"does not accept `sample_weight` in its `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n            weight_func = None\n            source_code_weight_func = None\n\n    if series_weights is not None:\n        if not isinstance(series_weights, dict):\n            raise TypeError(\n                (f\"Argument `series_weights` must be a dict of floats or ints.\"\n                 f\"Got {type(series_weights)}.\")\n            )\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(\n                (f\"Argument `series_weights` is ignored since regressor {regressor} \"\n                 f\"does not accept `sample_weight` in its `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n            series_weights = None\n\n    return weight_func, source_code_weight_func, series_weights",
          "docstring": "Check weights arguments, `weight_func` and `series_weights` for the different \nforecasters. Create `source_code_weight_func`, source code of the custom \nfunction(s) used to create weights.\n\nParameters\n----------\nforecaster_name : str\n    Forecaster name.\nregressor : regressor or pipeline compatible with the scikit-learn API\n    Regressor of the forecaster.\nweight_func : Callable, dict\n    Argument `weight_func` of the forecaster.\nseries_weights : dict\n    Argument `series_weights` of the forecaster.\n\nReturns\n-------\nweight_func : Callable, dict\n    Argument `weight_func` of the forecaster.\nsource_code_weight_func : str, dict\n    Argument `source_code_weight_func` of the forecaster.\nseries_weights : dict\n    Argument `series_weights` of the forecaster.",
          "signature": "def initialize_weights(forecaster_name: str, regressor: object, weight_func: Union[Callable, dict], series_weights: dict) -> Tuple[Union[Callable, dict], Union[str, dict], dict]:",
          "type": "Function",
          "class_signature": null
        },
        "check_select_fit_kwargs": {
          "code": "def check_select_fit_kwargs(\n    regressor: object,\n    fit_kwargs: Optional[dict] = None\n) -> dict:\n    \"\"\"\n    Check if `fit_kwargs` is a dict and select only the keys that are used by\n    the `fit` method of the regressor.\n\n    Parameters\n    ----------\n    regressor : object\n        Regressor object.\n    fit_kwargs : dict, default `None`\n        Dictionary with the arguments to pass to the `fit' method of the forecaster.\n\n    Returns\n    -------\n    fit_kwargs : dict\n        Dictionary with the arguments to be passed to the `fit` method of the \n        regressor after removing the unused keys.\n    \n    \"\"\"\n\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    else:\n        if not isinstance(fit_kwargs, dict):\n            raise TypeError(\n                f\"Argument `fit_kwargs` must be a dict. Got {type(fit_kwargs)}.\"\n            )\n\n        # Non used keys\n        non_used_keys = [k for k in fit_kwargs.keys()\n                         if k not in inspect.signature(regressor.fit).parameters]\n        if non_used_keys:\n            warnings.warn(\n                (f\"Argument/s {non_used_keys} ignored since they are not used by the \"\n                 f\"regressor's `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n\n        if 'sample_weight' in fit_kwargs.keys():\n            warnings.warn(\n                (\"The `sample_weight` argument is ignored. Use `weight_func` to pass \"\n                 \"a function that defines the individual weights for each sample \"\n                 \"based on its index.\"),\n                 IgnoredArgumentWarning\n            )\n            del fit_kwargs['sample_weight']\n\n        # Select only the keyword arguments allowed by the regressor's `fit` method.\n        fit_kwargs = {k: v for k, v in fit_kwargs.items()\n                      if k in inspect.signature(regressor.fit).parameters}\n\n    return fit_kwargs",
          "docstring": "Check if `fit_kwargs` is a dict and select only the keys that are used by\nthe `fit` method of the regressor.\n\nParameters\n----------\nregressor : object\n    Regressor object.\nfit_kwargs : dict, default `None`\n    Dictionary with the arguments to pass to the `fit' method of the forecaster.\n\nReturns\n-------\nfit_kwargs : dict\n    Dictionary with the arguments to be passed to the `fit` method of the \n    regressor after removing the unused keys.",
          "signature": "def check_select_fit_kwargs(regressor: object, fit_kwargs: Optional[dict]=None) -> dict:",
          "type": "Function",
          "class_signature": null
        }
      },
      "skforecast/preprocessing/preprocessing.py": {
        "TimeSeriesDifferentiator.__init__": {
          "code": "    def __init__(\n        self, \n        order: int = 1,\n        window_size: int = None\n    ) -> None:\n\n        if not isinstance(order, (int, np.integer)):\n            raise TypeError(\n                f\"Parameter `order` must be an integer greater than 0. Found {type(order)}.\"\n            )\n        if order < 1:\n            raise ValueError(\n                f\"Parameter `order` must be an integer greater than 0. Found {order}.\"\n            )\n\n        if window_size is not None:\n            if not isinstance(window_size, (int, np.integer)):\n                raise TypeError(\n                    f\"Parameter `window_size` must be an integer greater than 0. \"\n                    f\"Found {type(window_size)}.\"\n                )\n            if window_size < 1:\n                raise ValueError(\n                    f\"Parameter `window_size` must be an integer greater than 0. \"\n                    f\"Found {window_size}.\"\n                )\n\n        self.order = order\n        self.window_size = window_size\n        self.initial_values = []\n        self.pre_train_values = []\n        self.last_values = []",
          "docstring": "",
          "signature": "def __init__(self, order: int=1, window_size: int=None) -> None:",
          "type": "Method",
          "class_signature": "class TimeSeriesDifferentiator(BaseEstimator, TransformerMixin):"
        },
        "QuantileBinner.__init__": {
          "code": "    def __init__(\n        self,\n        n_bins: int,\n        method: Optional[str] = \"linear\",\n        subsample: int = 200000,\n        dtype: Optional[type] = np.float64,\n        random_state: Optional[int] = 789654\n    ):\n        \n        self._validate_params(\n            n_bins,\n            method,\n            subsample,\n            dtype,\n            random_state\n        )\n\n        self.n_bins       = n_bins\n        self.method       = method\n        self.subsample    = subsample\n        self.random_state = random_state\n        self.dtype        = dtype\n        self.n_bins_      = None\n        self.bin_edges_   = None\n        self.intervals_   = None",
          "docstring": "",
          "signature": "def __init__(self, n_bins: int, method: Optional[str]='linear', subsample: int=200000, dtype: Optional[type]=np.float64, random_state: Optional[int]=789654):",
          "type": "Method",
          "class_signature": "class QuantileBinner:"
        }
      }
    },
    "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:_train_test_split_one_step_ahead": {
      "skforecast/recursive/_forecaster_recursive.py": {
        "ForecasterRecursive._create_train_X_y": {
          "code": "    def _create_train_X_y(self, y: pd.Series, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, pd.Series, list, list, list, list, dict]:\n        \"\"\"\n        Create training matrices from univariate time series and exogenous\n        variables.\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors).\n        y_train : pandas Series\n            Values of the time series related to each row of `X_train`.\n        exog_names_in_ : list\n            Names of the exogenous variables used during training.\n        X_train_window_features_names_out_ : list\n            Names of the window features included in the matrix `X_train` created\n            internally for training.\n        X_train_exog_names_out_ : list\n            Names of the exogenous variables included in the matrix `X_train` created\n            internally for training. It can be different from `exog_names_in_` if\n            some exogenous variables are transformed during the training process.\n        X_train_features_names_out_ : list\n            Names of the columns of the matrix created internally for training.\n        exog_dtypes_in_ : dict\n            Type of each exogenous variable/s used in training. If `transformer_exog` \n            is used, the dtypes are calculated before the transformation.\n        \n        \"\"\"\n        check_y(y=y)\n        y = input_to_frame(data=y, input_name='y')\n        if len(y) <= self.window_size:\n            raise ValueError(f'Length of `y` must be greater than the maximum window size needed by the forecaster.\\n    Length `y`: {len(y)}.\\n    Max window size: {self.window_size}.\\n    Lags window size: {self.max_lag}.\\n    Window features window size: {self.max_size_window_features}.')\n        fit_transformer = False if self.is_fitted else True\n        y = transform_dataframe(df=y, transformer=self.transformer_y, fit=fit_transformer, inverse_transform=False)\n        y_values, y_index = preprocess_y(y=y)\n        train_index = y_index[self.window_size:]\n        if self.differentiation is not None:\n            if not self.is_fitted:\n                y_values = self.differentiator.fit_transform(y_values)\n            else:\n                differentiator = copy(self.differentiator)\n                y_values = differentiator.fit_transform(y_values)\n        exog_names_in_ = None\n        exog_dtypes_in_ = None\n        categorical_features = False\n        if exog is not None:\n            check_exog(exog=exog, allow_nan=True)\n            exog = input_to_frame(data=exog, input_name='exog')\n            len_y = len(y_values)\n            len_train_index = len(train_index)\n            len_exog = len(exog)\n            if not len_exog == len_y and (not len_exog == len_train_index):\n                raise ValueError(f'Length of `exog` must be equal to the length of `y` (if index is fully aligned) or length of `y` - `window_size` (if `exog` starts after the first `window_size` values).\\n    `exog`              : ({exog.index[0]} -- {exog.index[-1]})  (n={len_exog})\\n    `y`                 : ({y.index[0]} -- {y.index[-1]})  (n={len_y})\\n    `y` - `window_size` : ({train_index[0]} -- {train_index[-1]})  (n={len_train_index})')\n            exog_names_in_ = exog.columns.to_list()\n            exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n            exog = transform_dataframe(df=exog, transformer=self.transformer_exog, fit=fit_transformer, inverse_transform=False)\n            check_exog_dtypes(exog, call_check_exog=True)\n            categorical_features = exog.select_dtypes(include=np.number).shape[1] != exog.shape[1]\n            _, exog_index = preprocess_exog(exog=exog, return_values=False)\n            if len_exog == len_y:\n                if not (exog_index == y_index).all():\n                    raise ValueError('When `exog` has the same length as `y`, the index of `exog` must be aligned with the index of `y` to ensure the correct alignment of values.')\n                exog = exog.iloc[self.window_size:,]\n            elif not (exog_index == train_index).all():\n                raise ValueError(\"When `exog` doesn't contain the first `window_size` observations, the index of `exog` must be aligned with the index of `y` minus the first `window_size` observations to ensure the correct alignment of values.\")\n        X_train = []\n        X_train_features_names_out_ = []\n        X_as_pandas = True if categorical_features else False\n        X_train_lags, y_train = self._create_lags(y=y_values, X_as_pandas=X_as_pandas, train_index=train_index)\n        if X_train_lags is not None:\n            X_train.append(X_train_lags)\n            X_train_features_names_out_.extend(self.lags_names)\n        X_train_window_features_names_out_ = None\n        if self.window_features is not None:\n            n_diff = 0 if self.differentiation is None else self.differentiation\n            y_window_features = pd.Series(y_values[n_diff:], index=y_index[n_diff:])\n            X_train_window_features, X_train_window_features_names_out_ = self._create_window_features(y=y_window_features, X_as_pandas=X_as_pandas, train_index=train_index)\n            X_train.extend(X_train_window_features)\n            X_train_features_names_out_.extend(X_train_window_features_names_out_)\n        X_train_exog_names_out_ = None\n        if exog is not None:\n            X_train_exog_names_out_ = exog.columns.to_list()\n            if not X_as_pandas:\n                exog = exog.to_numpy()\n            X_train_features_names_out_.extend(X_train_exog_names_out_)\n            X_train.append(exog)\n        if len(X_train) == 1:\n            X_train = X_train[0]\n        elif X_as_pandas:\n            X_train = pd.concat(X_train, axis=1)\n        else:\n            X_train = np.concatenate(X_train, axis=1)\n        if X_as_pandas:\n            X_train.index = train_index\n        else:\n            X_train = pd.DataFrame(data=X_train, index=train_index, columns=X_train_features_names_out_)\n        y_train = pd.Series(data=y_train, index=train_index, name='y')\n        return (X_train, y_train, exog_names_in_, X_train_window_features_names_out_, X_train_exog_names_out_, X_train_features_names_out_, exog_dtypes_in_)",
          "docstring": "Create training matrices from univariate time series and exogenous\nvariables.\n\nParameters\n----------\ny : pandas Series\n    Training time series.\nexog : pandas Series, pandas DataFrame, default `None`\n    Exogenous variable/s included as predictor/s. Must have the same\n    number of observations as `y` and their indexes must be aligned.\n\nReturns\n-------\nX_train : pandas DataFrame\n    Training values (predictors).\ny_train : pandas Series\n    Values of the time series related to each row of `X_train`.\nexog_names_in_ : list\n    Names of the exogenous variables used during training.\nX_train_window_features_names_out_ : list\n    Names of the window features included in the matrix `X_train` created\n    internally for training.\nX_train_exog_names_out_ : list\n    Names of the exogenous variables included in the matrix `X_train` created\n    internally for training. It can be different from `exog_names_in_` if\n    some exogenous variables are transformed during the training process.\nX_train_features_names_out_ : list\n    Names of the columns of the matrix created internally for training.\nexog_dtypes_in_ : dict\n    Type of each exogenous variable/s used in training. If `transformer_exog` \n    is used, the dtypes are calculated before the transformation.",
          "signature": "def _create_train_X_y(self, y: pd.Series, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, pd.Series, list, list, list, list, dict]:",
          "type": "Method",
          "class_signature": "class ForecasterRecursive(ForecasterBase):"
        }
      }
    },
    "skforecast/model_selection/_utils.py:_calculate_metrics_one_step_ahead": {
      "skforecast/direct/_forecaster_direct.py": {
        "ForecasterDirect.filter_train_X_y_for_step": {
          "code": "    def filter_train_X_y_for_step(self, step: int, X_train: pd.DataFrame, y_train: dict, remove_suffix: bool=False) -> Tuple[pd.DataFrame, pd.Series]:\n        \"\"\"\n        Select the columns needed to train a forecaster for a specific step.  \n        The input matrices should be created using `create_train_X_y` method. \n        This method updates the index of `X_train` to the corresponding one \n        according to `y_train`. If `remove_suffix=True` the suffix \"_step_i\" \n        will be removed from the column names. \n\n        Parameters\n        ----------\n        step : int\n            Step for which columns must be selected selected. Starts at 1.\n        X_train : pandas DataFrame\n            Dataframe created with the `create_train_X_y` method, first return.\n        y_train : dict\n            Dict created with the `create_train_X_y` method, second return.\n        remove_suffix : bool, default `False`\n            If True, suffix \"_step_i\" is removed from the column names.\n\n        Returns\n        -------\n        X_train_step : pandas DataFrame\n            Training values (predictors) for the selected step.\n        y_train_step : pandas Series\n            Values of the time series related to each row of `X_train`.\n\n        \"\"\"\n        if step < 1 or step > self.steps:\n            raise ValueError(f'Invalid value `step`. For this forecaster, minimum value is 1 and the maximum step is {self.steps}.')\n        y_train_step = y_train[step]\n        if not self.exog_in_:\n            X_train_step = X_train\n        else:\n            n_lags = len(self.lags) if self.lags is not None else 0\n            n_window_features = len(self.X_train_window_features_names_out_) if self.window_features is not None else 0\n            idx_columns_autoreg = np.arange(n_lags + n_window_features)\n            n_exog = len(self.X_train_direct_exog_names_out_) / self.steps\n            idx_columns_exog = np.arange((step - 1) * n_exog, step * n_exog) + idx_columns_autoreg[-1] + 1\n            idx_columns = np.concatenate((idx_columns_autoreg, idx_columns_exog))\n            X_train_step = X_train.iloc[:, idx_columns]\n        X_train_step.index = y_train_step.index\n        if remove_suffix:\n            X_train_step.columns = [col_name.replace(f'_step_{step}', '') for col_name in X_train_step.columns]\n            y_train_step.name = y_train_step.name.replace(f'_step_{step}', '')\n        return (X_train_step, y_train_step)",
          "docstring": "Select the columns needed to train a forecaster for a specific step.  \nThe input matrices should be created using `create_train_X_y` method. \nThis method updates the index of `X_train` to the corresponding one \naccording to `y_train`. If `remove_suffix=True` the suffix \"_step_i\" \nwill be removed from the column names. \n\nParameters\n----------\nstep : int\n    Step for which columns must be selected selected. Starts at 1.\nX_train : pandas DataFrame\n    Dataframe created with the `create_train_X_y` method, first return.\ny_train : dict\n    Dict created with the `create_train_X_y` method, second return.\nremove_suffix : bool, default `False`\n    If True, suffix \"_step_i\" is removed from the column names.\n\nReturns\n-------\nX_train_step : pandas DataFrame\n    Training values (predictors) for the selected step.\ny_train_step : pandas Series\n    Values of the time series related to each row of `X_train`.",
          "signature": "def filter_train_X_y_for_step(self, step: int, X_train: pd.DataFrame, y_train: dict, remove_suffix: bool=False) -> Tuple[pd.DataFrame, pd.Series]:",
          "type": "Method",
          "class_signature": "class ForecasterDirect(ForecasterBase):"
        }
      },
      "skforecast/metrics/metrics.py": {
        "mean_absolute_scaled_error": {
          "code": "def mean_absolute_scaled_error(y_true: Union[pd.Series, np.ndarray], y_pred: Union[pd.Series, np.ndarray], y_train: Union[list, pd.Series, np.ndarray]) -> float:\n    \"\"\"\n    Mean Absolute Scaled Error (MASE)\n\n    MASE is a scale-independent error metric that measures the accuracy of\n    a forecast. It is the mean absolute error of the forecast divided by the\n    mean absolute error of a naive forecast in the training set. The naive\n    forecast is the one obtained by shifting the time series by one period.\n    If y_train is a list of numpy arrays or pandas Series, it is considered\n    that each element is the true value of the target variable in the training\n    set for each time series. In this case, the naive forecast is calculated\n    for each time series separately.\n\n    Parameters\n    ----------\n    y_true : pandas Series, numpy ndarray\n        True values of the target variable.\n    y_pred : pandas Series, numpy ndarray\n        Predicted values of the target variable.\n    y_train : list, pandas Series, numpy ndarray\n        True values of the target variable in the training set. If `list`, it\n        is consider that each element is the true value of the target variable\n        in the training set for each time series.\n\n    Returns\n    -------\n    mase : float\n        MASE value.\n    \n    \"\"\"\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError('`y_true` must be a pandas Series or numpy ndarray.')\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError('`y_pred` must be a pandas Series or numpy ndarray.')\n    if not isinstance(y_train, (list, pd.Series, np.ndarray)):\n        raise TypeError('`y_train` must be a list, pandas Series or numpy ndarray.')\n    if isinstance(y_train, list):\n        for x in y_train:\n            if not isinstance(x, (pd.Series, np.ndarray)):\n                raise TypeError('When `y_train` is a list, each element must be a pandas Series or numpy ndarray.')\n    if len(y_true) != len(y_pred):\n        raise ValueError('`y_true` and `y_pred` must have the same length.')\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError('`y_true` and `y_pred` must have at least one element.')\n    if isinstance(y_train, list):\n        naive_forecast = np.concatenate([np.diff(x) for x in y_train])\n    else:\n        naive_forecast = np.diff(y_train)\n    mase = np.mean(np.abs(y_true - y_pred)) / np.nanmean(np.abs(naive_forecast))\n    return mase",
          "docstring": "Mean Absolute Scaled Error (MASE)\n\nMASE is a scale-independent error metric that measures the accuracy of\na forecast. It is the mean absolute error of the forecast divided by the\nmean absolute error of a naive forecast in the training set. The naive\nforecast is the one obtained by shifting the time series by one period.\nIf y_train is a list of numpy arrays or pandas Series, it is considered\nthat each element is the true value of the target variable in the training\nset for each time series. In this case, the naive forecast is calculated\nfor each time series separately.\n\nParameters\n----------\ny_true : pandas Series, numpy ndarray\n    True values of the target variable.\ny_pred : pandas Series, numpy ndarray\n    Predicted values of the target variable.\ny_train : list, pandas Series, numpy ndarray\n    True values of the target variable in the training set. If `list`, it\n    is consider that each element is the true value of the target variable\n    in the training set for each time series.\n\nReturns\n-------\nmase : float\n    MASE value.",
          "signature": "def mean_absolute_scaled_error(y_true: Union[pd.Series, np.ndarray], y_pred: Union[pd.Series, np.ndarray], y_train: Union[list, pd.Series, np.ndarray]) -> float:",
          "type": "Function",
          "class_signature": null
        }
      },
      "skforecast/preprocessing/preprocessing.py": {
        "wrapper": {
          "code": "        def wrapper(self, *args, **kwargs):\n\n            if args:\n                X = args[0] \n            elif 'X' in kwargs:\n                X = kwargs['X']\n            else:\n                raise ValueError(\"Methods must be called with 'X' as argument.\")\n\n            if not isinstance(X, np.ndarray):\n                raise TypeError(f\"'X' must be a numpy ndarray. Found {type(X)}.\")\n            if ensure_1d and not X.ndim == 1:\n                raise ValueError(f\"'X' must be a 1D array. Found {X.ndim} dimensions.\")\n            \n            result = func(self, *args, **kwargs)\n            \n            return result",
          "docstring": "",
          "signature": "def wrapper(self, *args, **kwargs):",
          "type": "Function",
          "class_signature": null
        }
      }
    },
    "skforecast/direct/_forecaster_direct.py:ForecasterDirect:__init__": {
      "skforecast/utils/utils.py": {
        "initialize_lags": {
          "code": "def initialize_lags(\n    forecaster_name: str,\n    lags: Any\n) -> Union[Optional[np.ndarray], Optional[list], Optional[int]]:\n    \"\"\"\n    Check lags argument input and generate the corresponding numpy ndarray.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    lags : Any\n        Lags used as predictors.\n\n    Returns\n    -------\n    lags : numpy ndarray, None\n        Lags used as predictors.\n    lags_names : list, None\n        Names of the lags used as predictors.\n    max_lag : int, None\n        Maximum value of the lags.\n    \n    \"\"\"\n\n    lags_names = None\n    max_lag = None\n    if lags is not None:\n        if isinstance(lags, int):\n            if lags < 1:\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n            lags = np.arange(1, lags + 1)\n\n        if isinstance(lags, (list, tuple, range)):\n            lags = np.array(lags)\n        \n        if isinstance(lags, np.ndarray):\n            if lags.size == 0:\n                return None, None, None\n            if lags.ndim != 1:\n                raise ValueError(\"`lags` must be a 1-dimensional array.\")\n            if not np.issubdtype(lags.dtype, np.integer):\n                raise TypeError(\"All values in `lags` must be integers.\")\n            if np.any(lags < 1):\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n        else:\n            if forecaster_name != 'ForecasterDirectMultiVariate':\n                raise TypeError(\n                    (f\"`lags` argument must be an int, 1d numpy ndarray, range, \"\n                     f\"tuple or list. Got {type(lags)}.\")\n                )\n            else:\n                raise TypeError(\n                    (f\"`lags` argument must be a dict, int, 1d numpy ndarray, range, \"\n                     f\"tuple or list. Got {type(lags)}.\")\n                )\n        \n        lags_names = [f'lag_{i}' for i in lags]\n        max_lag = max(lags)\n\n    return lags, lags_names, max_lag",
          "docstring": "Check lags argument input and generate the corresponding numpy ndarray.\n\nParameters\n----------\nforecaster_name : str\n    Forecaster name.\nlags : Any\n    Lags used as predictors.\n\nReturns\n-------\nlags : numpy ndarray, None\n    Lags used as predictors.\nlags_names : list, None\n    Names of the lags used as predictors.\nmax_lag : int, None\n    Maximum value of the lags.",
          "signature": "def initialize_lags(forecaster_name: str, lags: Any) -> Union[Optional[np.ndarray], Optional[list], Optional[int]]:",
          "type": "Function",
          "class_signature": null
        },
        "initialize_window_features": {
          "code": "def initialize_window_features(\n    window_features: Any\n) -> Union[Optional[list], Optional[list], Optional[int]]:\n    \"\"\"\n    Check window_features argument input and generate the corresponding list.\n\n    Parameters\n    ----------\n    window_features : Any\n        Classes used to create window features.\n\n    Returns\n    -------\n    window_features : list, None\n        List of classes used to create window features.\n    window_features_names : list, None\n        List with all the features names of the window features.\n    max_size_window_features : int, None\n        Maximum value of the `window_sizes` attribute of all classes.\n    \n    \"\"\"\n\n    needed_atts = ['window_sizes', 'features_names']\n    needed_methods = ['transform_batch', 'transform']\n\n    max_window_sizes = None\n    window_features_names = None\n    max_size_window_features = None\n    if window_features is not None:\n        if isinstance(window_features, list) and len(window_features) < 1:\n            raise ValueError(\n                \"Argument `window_features` must contain at least one element.\"\n            )\n        if not isinstance(window_features, list):\n            window_features = [window_features]\n\n        link_to_docs = (\n            \"\\nVisit the documentation for more information about how to create \"\n            \"custom window features:\\n\"\n            \"https://skforecast.org/latest/user_guides/window-features-and-custom-features.html#create-your-custom-window-features\"\n        )\n        \n        max_window_sizes = []\n        window_features_names = []\n        for wf in window_features:\n            wf_name = type(wf).__name__\n            atts_methods = set([a for a in dir(wf)])\n            if not set(needed_atts).issubset(atts_methods):\n                raise ValueError(\n                    f\"{wf_name} must have the attributes: {needed_atts}.\" + link_to_docs\n                )\n            if not set(needed_methods).issubset(atts_methods):\n                raise ValueError(\n                    f\"{wf_name} must have the methods: {needed_methods}.\" + link_to_docs\n                )\n            \n            window_sizes = wf.window_sizes\n            if not isinstance(window_sizes, (int, list)):\n                raise TypeError(\n                    f\"Attribute `window_sizes` of {wf_name} must be an int or a list \"\n                    f\"of ints. Got {type(window_sizes)}.\" + link_to_docs\n                )\n            \n            if isinstance(window_sizes, int):\n                if window_sizes < 1:\n                    raise ValueError(\n                        f\"If argument `window_sizes` is an integer, it must be equal to or \"\n                        f\"greater than 1. Got {window_sizes} from {wf_name}.\" + link_to_docs\n                    )\n                max_window_sizes.append(window_sizes)\n            else:\n                if not all(isinstance(ws, int) for ws in window_sizes) or not all(\n                    ws >= 1 for ws in window_sizes\n                ):                    \n                    raise ValueError(\n                        f\"If argument `window_sizes` is a list, all elements must be integers \"\n                        f\"equal to or greater than 1. Got {window_sizes} from {wf_name}.\" + link_to_docs\n                    )\n                max_window_sizes.append(max(window_sizes))\n\n            features_names = wf.features_names\n            if not isinstance(features_names, (str, list)):\n                raise TypeError(\n                    f\"Attribute `features_names` of {wf_name} must be a str or \"\n                    f\"a list of strings. Got {type(features_names)}.\" + link_to_docs\n                )\n            if isinstance(features_names, str):\n                window_features_names.append(features_names)\n            else:\n                if not all(isinstance(fn, str) for fn in features_names):\n                    raise TypeError(\n                        f\"If argument `features_names` is a list, all elements \"\n                        f\"must be strings. Got {features_names} from {wf_name}.\" + link_to_docs\n                    )\n                window_features_names.extend(features_names)\n\n        max_size_window_features = max(max_window_sizes)\n        if len(set(window_features_names)) != len(window_features_names):\n            raise ValueError(\n                f\"All window features names must be unique. Got {window_features_names}.\"\n            )\n\n    return window_features, window_features_names, max_size_window_features",
          "docstring": "Check window_features argument input and generate the corresponding list.\n\nParameters\n----------\nwindow_features : Any\n    Classes used to create window features.\n\nReturns\n-------\nwindow_features : list, None\n    List of classes used to create window features.\nwindow_features_names : list, None\n    List with all the features names of the window features.\nmax_size_window_features : int, None\n    Maximum value of the `window_sizes` attribute of all classes.",
          "signature": "def initialize_window_features(window_features: Any) -> Union[Optional[list], Optional[list], Optional[int]]:",
          "type": "Function",
          "class_signature": null
        },
        "initialize_weights": {
          "code": "def initialize_weights(\n    forecaster_name: str,\n    regressor: object,\n    weight_func: Union[Callable, dict],\n    series_weights: dict\n) -> Tuple[Union[Callable, dict], Union[str, dict], dict]:\n    \"\"\"\n    Check weights arguments, `weight_func` and `series_weights` for the different \n    forecasters. Create `source_code_weight_func`, source code of the custom \n    function(s) used to create weights.\n    \n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        Regressor of the forecaster.\n    weight_func : Callable, dict\n        Argument `weight_func` of the forecaster.\n    series_weights : dict\n        Argument `series_weights` of the forecaster.\n\n    Returns\n    -------\n    weight_func : Callable, dict\n        Argument `weight_func` of the forecaster.\n    source_code_weight_func : str, dict\n        Argument `source_code_weight_func` of the forecaster.\n    series_weights : dict\n        Argument `series_weights` of the forecaster.\n    \n    \"\"\"\n\n    source_code_weight_func = None\n\n    if weight_func is not None:\n\n        if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n            if not isinstance(weight_func, (Callable, dict)):\n                raise TypeError(\n                    (f\"Argument `weight_func` must be a Callable or a dict of \"\n                     f\"Callables. Got {type(weight_func)}.\")\n                )\n        elif not isinstance(weight_func, Callable):\n            raise TypeError(\n                f\"Argument `weight_func` must be a Callable. Got {type(weight_func)}.\"\n            )\n        \n        if isinstance(weight_func, dict):\n            source_code_weight_func = {}\n            for key in weight_func:\n                source_code_weight_func[key] = inspect.getsource(weight_func[key])\n        else:\n            source_code_weight_func = inspect.getsource(weight_func)\n\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(\n                (f\"Argument `weight_func` is ignored since regressor {regressor} \"\n                 f\"does not accept `sample_weight` in its `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n            weight_func = None\n            source_code_weight_func = None\n\n    if series_weights is not None:\n        if not isinstance(series_weights, dict):\n            raise TypeError(\n                (f\"Argument `series_weights` must be a dict of floats or ints.\"\n                 f\"Got {type(series_weights)}.\")\n            )\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(\n                (f\"Argument `series_weights` is ignored since regressor {regressor} \"\n                 f\"does not accept `sample_weight` in its `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n            series_weights = None\n\n    return weight_func, source_code_weight_func, series_weights",
          "docstring": "Check weights arguments, `weight_func` and `series_weights` for the different \nforecasters. Create `source_code_weight_func`, source code of the custom \nfunction(s) used to create weights.\n\nParameters\n----------\nforecaster_name : str\n    Forecaster name.\nregressor : regressor or pipeline compatible with the scikit-learn API\n    Regressor of the forecaster.\nweight_func : Callable, dict\n    Argument `weight_func` of the forecaster.\nseries_weights : dict\n    Argument `series_weights` of the forecaster.\n\nReturns\n-------\nweight_func : Callable, dict\n    Argument `weight_func` of the forecaster.\nsource_code_weight_func : str, dict\n    Argument `source_code_weight_func` of the forecaster.\nseries_weights : dict\n    Argument `series_weights` of the forecaster.",
          "signature": "def initialize_weights(forecaster_name: str, regressor: object, weight_func: Union[Callable, dict], series_weights: dict) -> Tuple[Union[Callable, dict], Union[str, dict], dict]:",
          "type": "Function",
          "class_signature": null
        },
        "check_select_fit_kwargs": {
          "code": "def check_select_fit_kwargs(\n    regressor: object,\n    fit_kwargs: Optional[dict] = None\n) -> dict:\n    \"\"\"\n    Check if `fit_kwargs` is a dict and select only the keys that are used by\n    the `fit` method of the regressor.\n\n    Parameters\n    ----------\n    regressor : object\n        Regressor object.\n    fit_kwargs : dict, default `None`\n        Dictionary with the arguments to pass to the `fit' method of the forecaster.\n\n    Returns\n    -------\n    fit_kwargs : dict\n        Dictionary with the arguments to be passed to the `fit` method of the \n        regressor after removing the unused keys.\n    \n    \"\"\"\n\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    else:\n        if not isinstance(fit_kwargs, dict):\n            raise TypeError(\n                f\"Argument `fit_kwargs` must be a dict. Got {type(fit_kwargs)}.\"\n            )\n\n        # Non used keys\n        non_used_keys = [k for k in fit_kwargs.keys()\n                         if k not in inspect.signature(regressor.fit).parameters]\n        if non_used_keys:\n            warnings.warn(\n                (f\"Argument/s {non_used_keys} ignored since they are not used by the \"\n                 f\"regressor's `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n\n        if 'sample_weight' in fit_kwargs.keys():\n            warnings.warn(\n                (\"The `sample_weight` argument is ignored. Use `weight_func` to pass \"\n                 \"a function that defines the individual weights for each sample \"\n                 \"based on its index.\"),\n                 IgnoredArgumentWarning\n            )\n            del fit_kwargs['sample_weight']\n\n        # Select only the keyword arguments allowed by the regressor's `fit` method.\n        fit_kwargs = {k: v for k, v in fit_kwargs.items()\n                      if k in inspect.signature(regressor.fit).parameters}\n\n    return fit_kwargs",
          "docstring": "Check if `fit_kwargs` is a dict and select only the keys that are used by\nthe `fit` method of the regressor.\n\nParameters\n----------\nregressor : object\n    Regressor object.\nfit_kwargs : dict, default `None`\n    Dictionary with the arguments to pass to the `fit' method of the forecaster.\n\nReturns\n-------\nfit_kwargs : dict\n    Dictionary with the arguments to be passed to the `fit` method of the \n    regressor after removing the unused keys.",
          "signature": "def check_select_fit_kwargs(regressor: object, fit_kwargs: Optional[dict]=None) -> dict:",
          "type": "Function",
          "class_signature": null
        },
        "select_n_jobs_fit_forecaster": {
          "code": "def select_n_jobs_fit_forecaster(\n    forecaster_name: str,\n    regressor: object,\n) -> int:\n    \"\"\"\n    Select the optimal number of jobs to use in the fitting process. This\n    selection is based on heuristics and is not guaranteed to be optimal. \n    \n    The number of jobs is chosen as follows:\n    \n    - If forecaster_name is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and regressor_name is a linear regressor then `n_jobs = 1`, \n    otherwise `n_jobs = cpu_count() - 1`.\n    - If regressor is a `LGBMRegressor(n_jobs=1)`, then `n_jobs = cpu_count() - 1`.\n    - If regressor is a `LGBMRegressor` with internal n_jobs != 1, then `n_jobs = 1`.\n    This is because `lightgbm` is highly optimized for gradient boosting and\n    parallelizes operations at a very fine-grained level, making additional\n    parallelization unnecessary and potentially harmful due to resource contention.\n    \n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n\n    Returns\n    -------\n    n_jobs : int\n        The number of jobs to run in parallel.\n    \n    \"\"\"\n\n    if isinstance(regressor, Pipeline):\n        regressor = regressor[-1]\n        regressor_name = type(regressor).__name__\n    else:\n        regressor_name = type(regressor).__name__\n\n    linear_regressors = [\n        regressor_name\n        for regressor_name in dir(sklearn.linear_model)\n        if not regressor_name.startswith('_')\n    ]\n\n    if forecaster_name in ['ForecasterDirect', \n                           'ForecasterDirectMultiVariate']:\n        if regressor_name in linear_regressors:\n            n_jobs = 1\n        elif regressor_name == 'LGBMRegressor':\n            n_jobs = joblib.cpu_count() - 1 if regressor.n_jobs == 1 else 1\n        else:\n            n_jobs = joblib.cpu_count() - 1\n    else:\n        n_jobs = 1\n\n    return n_jobs",
          "docstring": "Select the optimal number of jobs to use in the fitting process. This\nselection is based on heuristics and is not guaranteed to be optimal. \n\nThe number of jobs is chosen as follows:\n\n- If forecaster_name is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\nand regressor_name is a linear regressor then `n_jobs = 1`, \notherwise `n_jobs = cpu_count() - 1`.\n- If regressor is a `LGBMRegressor(n_jobs=1)`, then `n_jobs = cpu_count() - 1`.\n- If regressor is a `LGBMRegressor` with internal n_jobs != 1, then `n_jobs = 1`.\nThis is because `lightgbm` is highly optimized for gradient boosting and\nparallelizes operations at a very fine-grained level, making additional\nparallelization unnecessary and potentially harmful due to resource contention.\n\nParameters\n----------\nforecaster_name : str\n    Forecaster name.\nregressor : regressor or pipeline compatible with the scikit-learn API\n    An instance of a regressor or pipeline compatible with the scikit-learn API.\n\nReturns\n-------\nn_jobs : int\n    The number of jobs to run in parallel.",
          "signature": "def select_n_jobs_fit_forecaster(forecaster_name: str, regressor: object) -> int:",
          "type": "Function",
          "class_signature": null
        }
      }
    },
    "skforecast/direct/_forecaster_direct.py:ForecasterDirect:_train_test_split_one_step_ahead": {
      "skforecast/direct/_forecaster_direct.py": {
        "ForecasterDirect._create_train_X_y": {
          "code": "    def _create_train_X_y(self, y: pd.Series, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, dict, list, list, list, dict]:\n        \"\"\"\n        Create training matrices from univariate time series and exogenous\n        variables. The resulting matrices contain the target variable and \n        predictors needed to train all the regressors (one per step).\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors) for each step. Note that the index \n            corresponds to that of the last step. It is updated for the corresponding \n            step in the `filter_train_X_y_for_step` method.\n        y_train : dict\n            Values of the time series related to each row of `X_train` for each \n            step in the form {step: y_step_[i]}.\n        exog_names_in_ : list\n            Names of the exogenous variables used during training.\n        X_train_exog_names_out_ : list\n            Names of the exogenous variables included in the matrix `X_train` created\n            internally for training. It can be different from `exog_names_in_` if\n            some exogenous variables are transformed during the training process.\n        X_train_features_names_out_ : list\n            Names of the columns of the matrix created internally for training.\n        exog_dtypes_in_ : dict\n            Type of each exogenous variable/s used in training. If `transformer_exog` \n            is used, the dtypes are calculated before the transformation.\n        \n        \"\"\"\n        check_y(y=y)\n        y = input_to_frame(data=y, input_name='y')\n        if len(y) < self.window_size + self.steps:\n            raise ValueError(f'Minimum length of `y` for training this forecaster is {self.window_size + self.steps}. Reduce the number of predicted steps, {self.steps}, or the maximum window_size, {self.window_size}, if no more data is available.\\n    Length `y`: {len(y)}.\\n    Max step : {self.steps}.\\n    Max window size: {self.window_size}.\\n    Lags window size: {self.max_lag}.\\n    Window features window size: {self.max_size_window_features}.')\n        fit_transformer = False if self.is_fitted else True\n        y = transform_dataframe(df=y, transformer=self.transformer_y, fit=fit_transformer, inverse_transform=False)\n        y_values, y_index = preprocess_y(y=y)\n        if self.differentiation is not None:\n            if not self.is_fitted:\n                y_values = self.differentiator.fit_transform(y_values)\n            else:\n                differentiator = copy(self.differentiator)\n                y_values = differentiator.fit_transform(y_values)\n        exog_names_in_ = None\n        exog_dtypes_in_ = None\n        categorical_features = False\n        if exog is not None:\n            check_exog(exog=exog, allow_nan=True)\n            exog = input_to_frame(data=exog, input_name='exog')\n            y_index_no_ws = y_index[self.window_size:]\n            len_y = len(y_values)\n            len_y_no_ws = len_y - self.window_size\n            len_exog = len(exog)\n            if not len_exog == len_y and (not len_exog == len_y_no_ws):\n                raise ValueError(f'Length of `exog` must be equal to the length of `y` (if index is fully aligned) or length of `y` - `window_size` (if `exog` starts after the first `window_size` values).\\n    `exog`              : ({exog.index[0]} -- {exog.index[-1]})  (n={len_exog})\\n    `y`                 : ({y.index[0]} -- {y.index[-1]})  (n={len_y})\\n    `y` - `window_size` : ({y_index_no_ws[0]} -- {y_index_no_ws[-1]})  (n={len_y_no_ws})')\n            self.exog_in_ = True\n            exog_names_in_ = exog.columns.to_list()\n            exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n            exog = transform_dataframe(df=exog, transformer=self.transformer_exog, fit=fit_transformer, inverse_transform=False)\n            check_exog_dtypes(exog, call_check_exog=True)\n            categorical_features = exog.select_dtypes(include=np.number).shape[1] != exog.shape[1]\n            _, exog_index = preprocess_exog(exog=exog, return_values=False)\n            if len_exog == len_y:\n                if not (exog_index == y_index).all():\n                    raise ValueError('When `exog` has the same length as `y`, the index of `exog` must be aligned with the index of `y` to ensure the correct alignment of values.')\n                exog = exog.iloc[self.window_size:,]\n            elif not (exog_index == y_index_no_ws).all():\n                raise ValueError(\"When `exog` doesn't contain the first `window_size` observations, the index of `exog` must be aligned with the index of `y` minus the first `window_size` observations to ensure the correct alignment of values.\")\n        X_train = []\n        X_train_features_names_out_ = []\n        train_index = y_index[self.window_size + (self.steps - 1):]\n        len_train_index = len(train_index)\n        X_as_pandas = True if categorical_features else False\n        X_train_lags, y_train = self._create_lags(y=y_values, X_as_pandas=X_as_pandas, train_index=train_index)\n        if X_train_lags is not None:\n            X_train.append(X_train_lags)\n            X_train_features_names_out_.extend(self.lags_names)\n        X_train_window_features_names_out_ = None\n        if self.window_features is not None:\n            n_diff = 0 if self.differentiation is None else self.differentiation\n            end_wf = None if self.steps == 1 else -(self.steps - 1)\n            y_window_features = pd.Series(y_values[n_diff:end_wf], index=y_index[n_diff:end_wf])\n            X_train_window_features, X_train_window_features_names_out_ = self._create_window_features(y=y_window_features, X_as_pandas=X_as_pandas, train_index=train_index)\n            X_train.extend(X_train_window_features)\n            X_train_features_names_out_.extend(X_train_window_features_names_out_)\n        self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n        X_train_exog_names_out_ = None\n        if exog is not None:\n            X_train_exog_names_out_ = exog.columns.to_list()\n            if X_as_pandas:\n                exog_direct, X_train_direct_exog_names_out_ = exog_to_direct(exog=exog, steps=self.steps)\n                exog_direct.index = train_index\n            else:\n                exog_direct, X_train_direct_exog_names_out_ = exog_to_direct_numpy(exog=exog, steps=self.steps)\n            self.X_train_direct_exog_names_out_ = X_train_direct_exog_names_out_\n            X_train_features_names_out_.extend(self.X_train_direct_exog_names_out_)\n            X_train.append(exog_direct)\n        if len(X_train) == 1:\n            X_train = X_train[0]\n        elif X_as_pandas:\n            X_train = pd.concat(X_train, axis=1)\n        else:\n            X_train = np.concatenate(X_train, axis=1)\n        if X_as_pandas:\n            X_train.index = train_index\n        else:\n            X_train = pd.DataFrame(data=X_train, index=train_index, columns=X_train_features_names_out_)\n        y_train = {step: pd.Series(data=y_train[:, step - 1], index=y_index[self.window_size + step - 1:][:len_train_index], name=f'y_step_{step}') for step in range(1, self.steps + 1)}\n        return (X_train, y_train, exog_names_in_, X_train_exog_names_out_, X_train_features_names_out_, exog_dtypes_in_)",
          "docstring": "Create training matrices from univariate time series and exogenous\nvariables. The resulting matrices contain the target variable and \npredictors needed to train all the regressors (one per step).\n\nParameters\n----------\ny : pandas Series\n    Training time series.\nexog : pandas Series, pandas DataFrame, default `None`\n    Exogenous variable/s included as predictor/s. Must have the same\n    number of observations as `y` and their indexes must be aligned.\n\nReturns\n-------\nX_train : pandas DataFrame\n    Training values (predictors) for each step. Note that the index \n    corresponds to that of the last step. It is updated for the corresponding \n    step in the `filter_train_X_y_for_step` method.\ny_train : dict\n    Values of the time series related to each row of `X_train` for each \n    step in the form {step: y_step_[i]}.\nexog_names_in_ : list\n    Names of the exogenous variables used during training.\nX_train_exog_names_out_ : list\n    Names of the exogenous variables included in the matrix `X_train` created\n    internally for training. It can be different from `exog_names_in_` if\n    some exogenous variables are transformed during the training process.\nX_train_features_names_out_ : list\n    Names of the columns of the matrix created internally for training.\nexog_dtypes_in_ : dict\n    Type of each exogenous variable/s used in training. If `transformer_exog` \n    is used, the dtypes are calculated before the transformation.",
          "signature": "def _create_train_X_y(self, y: pd.Series, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, dict, list, list, list, dict]:",
          "type": "Method",
          "class_signature": "class ForecasterDirect(ForecasterBase):"
        }
      }
    }
  },
  "PRD": "# PROJECT NAME: skforecast-test_calculate_metrics_one_step_ahead\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 skforecast/\n    \u251c\u2500\u2500 direct/\n    \u2502   \u2514\u2500\u2500 _forecaster_direct.py\n    \u2502       \u251c\u2500\u2500 ForecasterDirect.__init__\n    \u2502       \u2514\u2500\u2500 ForecasterDirect._train_test_split_one_step_ahead\n    \u251c\u2500\u2500 metrics/\n    \u2502   \u2514\u2500\u2500 metrics.py\n    \u2502       \u2514\u2500\u2500 add_y_train_argument\n    \u251c\u2500\u2500 model_selection/\n    \u2502   \u2514\u2500\u2500 _utils.py\n    \u2502       \u2514\u2500\u2500 _calculate_metrics_one_step_ahead\n    \u2514\u2500\u2500 recursive/\n        \u2514\u2500\u2500 _forecaster_recursive.py\n            \u251c\u2500\u2500 ForecasterRecursive.__init__\n            \u2514\u2500\u2500 ForecasterRecursive._train_test_split_one_step_ahead\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThis module is designed to evaluate the performance of forecasting models by calculating relevant metrics for one-step-ahead predictions. It supports compatibility with different types of forecasters, such as ForecasterRecursive and ForecasterDirect, enabling seamless metric computation for single-step forecasting workflows. The module provides functionality to compute error metrics like mean absolute error, mean absolute percentage error, and mean absolute scaled error, offering a standardized way to quantify forecast accuracy. By streamlining the evaluation of forecasting models, it helps developers validate and compare model performance, simplifying the model selection and optimization process in forecasting applications.\n\n## FILE 1: skforecast/model_selection/_utils.py\n\n- FUNCTION NAME: _calculate_metrics_one_step_ahead\n  - SIGNATURE: def _calculate_metrics_one_step_ahead(forecaster: object, y: pd.Series, metrics: list, X_train: pd.DataFrame, y_train: Union[pd.Series, dict], X_test: pd.DataFrame, y_test: Union[pd.Series, dict]) -> list:\n  - DOCSTRING: \n```python\n\"\"\"\nCalculate metrics for one-step-ahead predictions of a forecaster model. This function evaluates the model's predictive performance using specified metrics over the given training and testing datasets.\n\nParameters\n----------\nforecaster : object\n    The forecaster model being evaluated, which can be of various types using methods for prediction.\ny : pandas Series\n    The complete time series data used for training and testing.\nmetrics : list\n    A list of metric functions to evaluate the predictions against the actual values.\nX_train : pandas DataFrame\n    The predictor variables used for training the model.\ny_train : Union[pd.Series, dict]\n    The target values corresponding to `X_train` for training the model.\nX_test : pandas DataFrame\n    The predictor variables used for testing the model.\ny_test : Union[pd.Series, dict]\n    The true values corresponding to `X_test`, used for evaluating the model's performance.\n\nReturns\n-------\nmetric_values : list\n    A list containing the computed metric values for the predictions.\n\nNotes\n-----\nThe function determines if the forecaster is of type 'ForecasterDirect', in which case it only optimizes the model for step 1 of the predictions. The predictions may be inversely transformed if the forecaster uses differentiation or a transformation method. The `y_test`, `y_pred`, and `y_train` values are flattened to ensure compatibility with the metric functions before evaluation.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - skforecast/direct/_forecaster_direct.py:ForecasterDirect:filter_train_X_y_for_step\n    - skforecast/metrics/metrics.py:wrapper\n    - skforecast/preprocessing/preprocessing.py:wrapper\n    - skforecast/metrics/metrics.py:mean_absolute_scaled_error\n\n## FILE 2: skforecast/recursive/_forecaster_recursive.py\n\n- CLASS METHOD: ForecasterRecursive.__init__\n  - CLASS SIGNATURE: class ForecasterRecursive(ForecasterBase):\n  - SIGNATURE: def __init__(self, regressor: object, lags: Optional[Union[int, list, np.ndarray, range]]=None, window_features: Optional[Union[object, list]]=None, transformer_y: Optional[object]=None, transformer_exog: Optional[object]=None, weight_func: Optional[Callable]=None, differentiation: Optional[int]=None, fit_kwargs: Optional[dict]=None, binner_kwargs: Optional[dict]=None, forecaster_id: Optional[Union[str, int]]=None) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nInitializes the ForecasterRecursive class, which transforms a scikit-learn compatible regressor into a recursive autoregressive forecaster for time series data.\n\nParameters\n----------\nregressor : object\n    A scikit-learn compatible regressor or pipeline that will be trained to predict future values based on past observations and external features.\nlags : Optional[Union[int, list, np.ndarray, range]], default=None\n    Specifies the lagged observations to use as predictors. Can be an integer, list, numpy array, or range.\nwindow_features : Optional[Union[object, list]], default=None\n    Instances or lists of instances that create additional features from the time series.\ntransformer_y : Optional[object], default=None\n    A transformer for the target variable `y`, compatible with scikit-learn's API, used for preprocessing the target before fitting.\ntransformer_exog : Optional[object], default=None\n    A transformer for exogenous variables, compatible with scikit-learn's API, used for preprocessing external features before fitting.\nweight_func : Optional[Callable], default=None\n    A function defining sample weights based on the index for the regression process, if applicable.\ndifferentiation : Optional[int], default=None\n    Specifies the order of differencing to apply to the time series prior to fitting the model.\nfit_kwargs : Optional[dict], default=None\n    Additional keyword arguments for fitting the regressor.\nbinner_kwargs : Optional[dict], default=None\n    Parameters for the QuantileBinner used for binning residuals based on predictions.\nforecaster_id : Optional[Union[str, int]], default=None\n    An identifier for the forecaster instance.\n\nAttributes\n----------\nregressor : object\n    A copy of the provided regressor, ready for fitting.\nlags : numpy ndarray\n    The lags selected for the training process.\nwindow_size : int\n    The size of the window for generating predictors, determined by the maximum lag and size of window features.\ndifferentiation : int\n    Stores the order of differentiation for the time series, modified based on input.\nbinner : QuantileBinner\n    An instance of the QuantileBinner, initialized with the binner_kwargs for discretizing residuals.\n\nRaises\n------\nValueError\n    If neither `lags` nor `window_features` are provided, at least one of them must be specified.\nTypeError\n    If incompatible types are supplied to parameters expecting specific types, such as `y` or `exog`.\n\"\"\"\n```\n\n- CLASS METHOD: ForecasterRecursive._train_test_split_one_step_ahead\n  - CLASS SIGNATURE: class ForecasterRecursive(ForecasterBase):\n  - SIGNATURE: def _train_test_split_one_step_ahead(self, y: pd.Series, initial_train_size: int, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n  - DOCSTRING: \n```python\n\"\"\"\nCreate training and testing matrices for one-step-ahead predictions using the specified initial training size.\n\nParameters\n----------\ny : pandas Series\n    The time series data to be used for training and testing.\ninitial_train_size : int\n    The number of observations to be used for the initial training set.\nexog : pandas Series, pandas DataFrame, default `None`\n    Exogenous variables to be used as predictors. They must have the same number of observations as `y`, and their indices should be aligned with `y`.\n\nReturns\n-------\nX_train : pandas DataFrame\n    Matrix of predictor values used for training the model.\ny_train : pandas Series\n    Target values associated with each row of `X_train`.\nX_test : pandas DataFrame\n    Matrix of predictor values used for testing the model.\ny_test : pandas Series\n    Target values associated with each row of `X_test`.\n\nNotes\n-----\nThe method updates the internal state variable `is_fitted` to manage the fitting status of the forecaster temporarily during the split and restores it afterward. The function interacts with the `_create_train_X_y()` method to generate the training and testing datasets. It assumes that the predictions should be made after a specified initial training size minus the window size necessary for the forecaster.\n\"\"\"\n```\n\n## FILE 3: skforecast/metrics/metrics.py\n\n- FUNCTION NAME: add_y_train_argument\n  - SIGNATURE: def add_y_train_argument(func: Callable) -> Callable:\n  - DOCSTRING: \n```python\n\"\"\"\nAdd a `y_train` argument to the given function if it is not already present in its signature.\n\nParameters\n----------\nfunc : Callable\n    The original function to which the `y_train` argument is added.\n\nReturns\n-------\nwrapper : Callable\n    A new function (wrapper) that includes the `y_train` argument, allowing it to be specified as a keyword argument.\n\nThis function leverages the `inspect` module to check the parameters of the original function and modifies its signature accordingly. It is intended to enhance compatibility with functions that require both target and training set data in applications such as model evaluation metrics.\n\"\"\"\n```\n\n## FILE 4: skforecast/direct/_forecaster_direct.py\n\n- CLASS METHOD: ForecasterDirect._train_test_split_one_step_ahead\n  - CLASS SIGNATURE: class ForecasterDirect(ForecasterBase):\n  - SIGNATURE: def _train_test_split_one_step_ahead(self, y: pd.Series, initial_train_size: int, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, dict, pd.DataFrame, dict]:\n  - DOCSTRING: \n```python\n\"\"\"\nCreate matrices needed to train and test the forecaster for one-step-ahead predictions.\n\nParameters\n----------\ny : pandas Series\n    The training time series.\ninitial_train_size : int\n    The initial size of the training set, representing the number of observations \n    used to train the forecaster before making the first prediction.\nexog : pandas Series, pandas DataFrame, optional, default `None`\n    Exogenous variables included as predictors, which must have the same number \n    of observations as `y` and aligned indexes.\n\nReturns\n-------\nX_train : pandas DataFrame\n    Predictor values used to train the model.\ny_train : dict\n    Values of the time series related to each row of `X_train` for each step \n    in the form {step: y_step_[i]}.\nX_test : pandas DataFrame\n    Predictor values used to test the model.\ny_test : dict\n    Values of the time series related to each row of `X_test` for each step \n    in the form {step: y_step_[i]}.\n\nNotes\n-----\nThe method temporarily sets `self.is_fitted` to `False` to allow for training \ndata preparation without needing a fully fitted forecaster state. The `test_init` \nvariable is computed as the initial training size minus the `window_size` to \nproperly segment the testing data after the training. This method relies on \nother class methods such as `_create_train_X_y` to generate training matrices.\n\"\"\"\n```\n\n- CLASS METHOD: ForecasterDirect.__init__\n  - CLASS SIGNATURE: class ForecasterDirect(ForecasterBase):\n  - SIGNATURE: def __init__(self, regressor: object, steps: int, lags: Optional[Union[int, list, np.ndarray, range]]=None, window_features: Optional[Union[object, list]]=None, transformer_y: Optional[object]=None, transformer_exog: Optional[object]=None, weight_func: Optional[Callable]=None, differentiation: Optional[int]=None, fit_kwargs: Optional[dict]=None, n_jobs: Union[int, str]='auto', forecaster_id: Optional[Union[str, int]]=None) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nInitializes a ForecasterDirect object, which turns any scikit-learn compatible regressor into an autoregressive direct multi-step forecaster.\n\nParameters\n----------\nregressor : object\n    A regressor or pipeline compatible with the scikit-learn API.\nsteps : int\n    The maximum number of future steps the forecaster will predict using the `predict()` method.\nlags : Optional[Union[int, list, np.ndarray, range]], default `None`\n    Lags used as predictors, specifying how many previous time steps are included as features.\nwindow_features : Optional[Union[object, list]], default `None`\n    Instance or list of instances for creating window features from the original time series.\ntransformer_y : Optional[object], default `None`\n    A transformer compatible with scikit-learn's preprocessing API for preprocessing the target `y`.\ntransformer_exog : Optional[object], default `None`\n    A transformer compatible with scikit-learn's preprocessing API for preprocessing exogenous variables.\nweight_func : Optional[Callable], default `None`\n    A function defining sample weights based on the index.\ndifferentiation : Optional[int], default `None`\n    Order of differencing applied to the time series before training.\nfit_kwargs : Optional[dict], default `None`\n    Additional arguments to pass to the `fit` method of the regressor.\nn_jobs : Union[int, str], default `'auto'`\n    Number of jobs to run in parallel; 'auto' uses all available cores.\nforecaster_id : Optional[Union[str, int]], default `None`\n    Identifier for the forecaster instance.\n\nReturns\n-------\nNone\n\nAttributes\n----------\nself.regressors_ : dict\n    Dictionary of cloned regressors for each forecasting step.\nself.window_size : int\n    The size of the window needed to create predictors, derived from lags and window features.\nself.last_window_ : Optional[pd.DataFrame]\n    Stores the last observed values needed for immediate predictions after training.\n\nRaises\n------\nTypeError\n    If `steps` is not an integer or if `n_jobs` is not an integer or 'auto'.\nValueError\n    If `steps` is less than 1, or if both `lags` and `window_features` are `None`.\n\"\"\"\n```\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "skforecast/model_selection/_utils.py": "from typing import Union, Tuple, Optional, Callable, Generator\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom joblib import cpu_count\nfrom tqdm.auto import tqdm\nfrom sklearn.pipeline import Pipeline\nimport sklearn.linear_model\nfrom sklearn.exceptions import NotFittedError\nfrom ..exceptions import IgnoredArgumentWarning\nfrom ..metrics import add_y_train_argument, _get_metric\nfrom ..utils import check_interval\n\ndef initialize_lags_grid(forecaster: object, lags_grid: Optional[Union[list, dict]]=None) -> Tuple[dict, str]:\n    \"\"\"\n    Initialize lags grid and lags label for model selection. \n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model. ForecasterRecursive, ForecasterDirect, \n        ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate.\n    lags_grid : list, dict, default `None`\n        Lists of lags to try, containing int, lists, numpy ndarray, or range \n        objects. If `dict`, the keys are used as labels in the `results` \n        DataFrame, and the values are used as the lists of lags to try.\n\n    Returns\n    -------\n    lags_grid : dict\n        Dictionary with lags configuration for each iteration.\n    lags_label : str\n        Label for lags representation in the results object.\n\n    \"\"\"\n    if not isinstance(lags_grid, (list, dict, type(None))):\n        raise TypeError(f'`lags_grid` argument must be a list, dict or None. Got {type(lags_grid)}.')\n    lags_label = 'values'\n    if isinstance(lags_grid, list):\n        lags_grid = {f'{lags}': lags for lags in lags_grid}\n    elif lags_grid is None:\n        lags = [int(lag) for lag in forecaster.lags]\n        lags_grid = {f'{lags}': lags}\n    else:\n        lags_label = 'keys'\n    return (lags_grid, lags_label)\n\ndef check_backtesting_input(forecaster: object, cv: object, metric: Union[str, Callable, list], add_aggregated_metric: bool=True, y: Optional[pd.Series]=None, series: Optional[Union[pd.DataFrame, dict]]=None, exog: Optional[Union[pd.Series, pd.DataFrame, dict]]=None, interval: Optional[list]=None, alpha: Optional[float]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False, n_jobs: Union[int, str]='auto', show_progress: bool=True, suppress_warnings: bool=False, suppress_warnings_fit: bool=False) -> None:\n    \"\"\"\n    This is a helper function to check most inputs of backtesting functions in \n    modules `model_selection`.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model.\n    cv : TimeSeriesFold\n        TimeSeriesFold object with the information needed to split the data into folds.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n    add_aggregated_metric : bool, default `True`\n        If `True`, the aggregated metrics (average, weighted average and pooling)\n        over all levels are also returned (only multiseries).\n    y : pandas Series, default `None`\n        Training time series for uni-series forecasters.\n    series : pandas DataFrame, dict, default `None`\n        Training time series for multi-series forecasters.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive.\n    alpha : float, default `None`\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %. \n    n_boot : int, default `250`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of prediction \n        error to create prediction intervals.  If `False`, out_sample_residuals \n        are used if they are already stored inside the forecaster.\n    use_binned_residuals : bool, default `False`\n        If `True`, residuals used in each bootstrapping iteration are selected\n        conditioning on the predicted values. If `False`, residuals are selected\n        randomly without conditioning on the predicted values.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the fuction\n        skforecast.utils.select_n_jobs_fit_forecaster.\n        **New in version 0.9.0**\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n    suppress_warnings: bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the backtesting \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    suppress_warnings_fit : bool, default `False`\n        If `True`, warnings generated during fitting will be ignored. Only \n        `ForecasterSarimax`.\n\n    Returns\n    -------\n    None\n    \n    \"\"\"\n    forecaster_name = type(forecaster).__name__\n    cv_name = type(cv).__name__\n    if cv_name != 'TimeSeriesFold':\n        raise TypeError(f'`cv` must be a TimeSeriesFold object. Got {cv_name}.')\n    steps = cv.steps\n    initial_train_size = cv.initial_train_size\n    gap = cv.gap\n    allow_incomplete_fold = cv.allow_incomplete_fold\n    refit = cv.refit\n    forecasters_uni = ['ForecasterRecursive', 'ForecasterDirect', 'ForecasterSarimax', 'ForecasterEquivalentDate']\n    forecasters_multi = ['ForecasterDirectMultiVariate', 'ForecasterRnn']\n    forecasters_multi_dict = ['ForecasterRecursiveMultiSeries']\n    if forecaster_name in forecasters_uni:\n        if not isinstance(y, pd.Series):\n            raise TypeError('`y` must be a pandas Series.')\n        data_name = 'y'\n        data_length = len(y)\n    elif forecaster_name in forecasters_multi:\n        if not isinstance(series, pd.DataFrame):\n            raise TypeError('`series` must be a pandas DataFrame.')\n        data_name = 'series'\n        data_length = len(series)\n    elif forecaster_name in forecasters_multi_dict:\n        if not isinstance(series, (pd.DataFrame, dict)):\n            raise TypeError(f'`series` must be a pandas DataFrame or a dict of DataFrames or Series. Got {type(series)}.')\n        data_name = 'series'\n        if isinstance(series, dict):\n            not_valid_series = [k for k, v in series.items() if not isinstance(v, (pd.Series, pd.DataFrame))]\n            if not_valid_series:\n                raise TypeError(f'If `series` is a dictionary, all series must be a named pandas Series or a pandas DataFrame with a single column. Review series: {not_valid_series}')\n            not_valid_index = [k for k, v in series.items() if not isinstance(v.index, pd.DatetimeIndex)]\n            if not_valid_index:\n                raise ValueError(f'If `series` is a dictionary, all series must have a Pandas DatetimeIndex as index with the same frequency. Review series: {not_valid_index}')\n            indexes_freq = [f'{v.index.freq}' for v in series.values()]\n            indexes_freq = sorted(set(indexes_freq))\n            if not len(indexes_freq) == 1:\n                raise ValueError(f'If `series` is a dictionary, all series must have a Pandas DatetimeIndex as index with the same frequency. Found frequencies: {indexes_freq}')\n            data_length = max([len(series[serie]) for serie in series])\n        else:\n            data_length = len(series)\n    if exog is not None:\n        if forecaster_name in forecasters_multi_dict:\n            if not isinstance(exog, (pd.Series, pd.DataFrame, dict)):\n                raise TypeError(f'`exog` must be a pandas Series, DataFrame, dictionary of pandas Series/DataFrames or None. Got {type(exog)}.')\n            if isinstance(exog, dict):\n                not_valid_exog = [k for k, v in exog.items() if not isinstance(v, (pd.Series, pd.DataFrame, type(None)))]\n                if not_valid_exog:\n                    raise TypeError(f'If `exog` is a dictionary, All exog must be a named pandas Series, a pandas DataFrame or None. Review exog: {not_valid_exog}')\n        elif not isinstance(exog, (pd.Series, pd.DataFrame)):\n            raise TypeError(f'`exog` must be a pandas Series, DataFrame or None. Got {type(exog)}.')\n    if hasattr(forecaster, 'differentiation'):\n        if forecaster.differentiation != cv.differentiation:\n            raise ValueError(f'The differentiation included in the forecaster ({forecaster.differentiation}) differs from the differentiation included in the cv ({cv.differentiation}). Set the same value for both using the `differentiation` argument.')\n    if not isinstance(metric, (str, Callable, list)):\n        raise TypeError(f'`metric` must be a string, a callable function, or a list containing multiple strings and/or callables. Got {type(metric)}.')\n    if forecaster_name == 'ForecasterEquivalentDate' and isinstance(forecaster.offset, pd.tseries.offsets.DateOffset):\n        if initial_train_size is None:\n            raise ValueError(f'`initial_train_size` must be an integer greater than the `window_size` of the forecaster ({forecaster.window_size}) and smaller than the length of `{data_name}` ({data_length}).')\n    elif initial_train_size is not None:\n        if initial_train_size < forecaster.window_size or initial_train_size >= data_length:\n            raise ValueError(f'If used, `initial_train_size` must be an integer greater than the `window_size` of the forecaster ({forecaster.window_size}) and smaller than the length of `{data_name}` ({data_length}).')\n        if initial_train_size + gap >= data_length:\n            raise ValueError(f'The combination of initial_train_size {initial_train_size} and gap {gap} cannot be greater than the length of `{data_name}` ({data_length}).')\n    elif forecaster_name in ['ForecasterSarimax', 'ForecasterEquivalentDate']:\n        raise ValueError(f'`initial_train_size` must be an integer smaller than the length of `{data_name}` ({data_length}).')\n    else:\n        if not forecaster.is_fitted:\n            raise NotFittedError('`forecaster` must be already trained if no `initial_train_size` is provided.')\n        if refit:\n            raise ValueError('`refit` is only allowed when `initial_train_size` is not `None`.')\n    if forecaster_name == 'ForecasterSarimax' and cv.skip_folds is not None:\n        raise ValueError('`skip_folds` is not allowed for ForecasterSarimax. Set it to `None`.')\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError('`add_aggregated_metric` must be a boolean: `True`, `False`.')\n    if not isinstance(n_boot, (int, np.integer)) or n_boot < 0:\n        raise TypeError(f'`n_boot` must be an integer greater than 0. Got {n_boot}.')\n    if not isinstance(random_state, (int, np.integer)) or random_state < 0:\n        raise TypeError(f'`random_state` must be an integer greater than 0. Got {random_state}.')\n    if not isinstance(use_in_sample_residuals, bool):\n        raise TypeError('`use_in_sample_residuals` must be a boolean: `True`, `False`.')\n    if not isinstance(use_binned_residuals, bool):\n        raise TypeError('`use_binned_residuals` must be a boolean: `True`, `False`.')\n    if not isinstance(n_jobs, int) and n_jobs != 'auto':\n        raise TypeError(f\"`n_jobs` must be an integer or `'auto'`. Got {n_jobs}.\")\n    if not isinstance(show_progress, bool):\n        raise TypeError('`show_progress` must be a boolean: `True`, `False`.')\n    if not isinstance(suppress_warnings, bool):\n        raise TypeError('`suppress_warnings` must be a boolean: `True`, `False`.')\n    if not isinstance(suppress_warnings_fit, bool):\n        raise TypeError('`suppress_warnings_fit` must be a boolean: `True`, `False`.')\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n    if not allow_incomplete_fold and data_length - (initial_train_size + gap) < steps:\n        raise ValueError(f'There is not enough data to evaluate {steps} steps in a single fold. Set `allow_incomplete_fold` to `True` to allow incomplete folds.\\n    Data available for test : {data_length - (initial_train_size + gap)}\\n    Steps                   : {steps}')\n\ndef select_n_jobs_backtesting(forecaster: object, refit: Union[bool, int]) -> int:\n    \"\"\"\n    Select the optimal number of jobs to use in the backtesting process. This\n    selection is based on heuristics and is not guaranteed to be optimal.\n\n    The number of jobs is chosen as follows:\n\n    - If `refit` is an integer, then `n_jobs = 1`. This is because parallelization doesn't \n    work with intermittent refit.\n    - If forecaster is 'ForecasterRecursive' and regressor is a linear regressor, \n    then `n_jobs = 1`.\n    - If forecaster is 'ForecasterRecursive' and regressor is not a linear \n    regressor then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and `refit = True`, then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and `refit = False`, then `n_jobs = 1`.\n    - If forecaster is 'ForecasterRecursiveMultiSeries', then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterSarimax' or 'ForecasterEquivalentDate', \n    then `n_jobs = 1`.\n    - If regressor is a `LGBMRegressor(n_jobs=1)`, then `n_jobs = cpu_count() - 1`.\n    - If regressor is a `LGBMRegressor` with internal n_jobs != 1, then `n_jobs = 1`.\n    This is because `lightgbm` is highly optimized for gradient boosting and\n    parallelizes operations at a very fine-grained level, making additional\n    parallelization unnecessary and potentially harmful due to resource contention.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model.\n    refit : bool, int\n        If the forecaster is refitted during the backtesting process.\n\n    Returns\n    -------\n    n_jobs : int\n        The number of jobs to run in parallel.\n    \n    \"\"\"\n    forecaster_name = type(forecaster).__name__\n    if isinstance(forecaster.regressor, Pipeline):\n        regressor = forecaster.regressor[-1]\n        regressor_name = type(regressor).__name__\n    else:\n        regressor = forecaster.regressor\n        regressor_name = type(regressor).__name__\n    linear_regressors = [regressor_name for regressor_name in dir(sklearn.linear_model) if not regressor_name.startswith('_')]\n    refit = False if refit == 0 else refit\n    if not isinstance(refit, bool) and refit != 1:\n        n_jobs = 1\n    elif forecaster_name in ['ForecasterRecursive']:\n        if regressor_name in linear_regressors:\n            n_jobs = 1\n        elif regressor_name == 'LGBMRegressor':\n            n_jobs = cpu_count() - 1 if regressor.n_jobs == 1 else 1\n        else:\n            n_jobs = cpu_count() - 1\n    elif forecaster_name in ['ForecasterDirect', 'ForecasterDirectMultiVariate']:\n        n_jobs = 1\n    elif forecaster_name in ['ForecasterRecursiveMultiSeries']:\n        if regressor_name == 'LGBMRegressor':\n            n_jobs = cpu_count() - 1 if regressor.n_jobs == 1 else 1\n        else:\n            n_jobs = cpu_count() - 1\n    elif forecaster_name in ['ForecasterSarimax', 'ForecasterEquivalentDate']:\n        n_jobs = 1\n    else:\n        n_jobs = 1\n    return n_jobs\n\ndef _initialize_levels_model_selection_multiseries(forecaster: object, series: Union[pd.DataFrame, dict], levels: Optional[Union[str, list]]=None) -> list:\n    \"\"\"\n    Initialize levels for model_selection multi-series functions.\n\n    Parameters\n    ----------\n    forecaster : ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate, ForecasterRnn\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Training time series.\n    levels : str, list, default `None`\n        level (`str`) or levels (`list`) at which the forecaster is optimized. \n        If `None`, all levels are taken into account. The resulting metric will be\n        the average of the optimization of all levels.\n\n    Returns\n    -------\n    levels : list\n        List of levels to be used in model_selection multi-series functions.\n    \n    \"\"\"\n    multi_series_forecasters_with_levels = ['ForecasterRecursiveMultiSeries', 'ForecasterRnn']\n    if type(forecaster).__name__ in multi_series_forecasters_with_levels and (not isinstance(levels, (str, list, type(None)))):\n        raise TypeError(f'`levels` must be a `list` of column names, a `str` of a column name or `None` when using a forecaster of type {multi_series_forecasters_with_levels}. If the forecaster is of type `ForecasterDirectMultiVariate`, this argument is ignored.')\n    if type(forecaster).__name__ == 'ForecasterDirectMultiVariate':\n        if levels and levels != forecaster.level and (levels != [forecaster.level]):\n            warnings.warn(f\"`levels` argument have no use when the forecaster is of type `ForecasterDirectMultiVariate`. The level of this forecaster is '{forecaster.level}', to predict another level, change the `level` argument when initializing the forecaster. \\n\", IgnoredArgumentWarning)\n        levels = [forecaster.level]\n    elif levels is None:\n        if isinstance(series, pd.DataFrame):\n            levels = list(series.columns)\n        else:\n            levels = list(series.keys())\n    elif isinstance(levels, str):\n        levels = [levels]\n    return levels\n\ndef _extract_data_folds_multiseries(series: Union[pd.Series, pd.DataFrame, dict], folds: list, span_index: Union[pd.DatetimeIndex, pd.RangeIndex], window_size: int, exog: Optional[Union[pd.Series, pd.DataFrame, dict]]=None, dropna_last_window: bool=False, externally_fitted: bool=False) -> Generator[Tuple[Union[pd.Series, pd.DataFrame, dict], pd.DataFrame, list, Optional[Union[pd.Series, pd.DataFrame, dict]], Optional[Union[pd.Series, pd.DataFrame, dict]], list], None, None]:\n    \"\"\"\n    Select the data from series and exog that corresponds to each fold created using the\n    skforecast.model_selection._create_backtesting_folds function.\n\n    Parameters\n    ----------\n    series : pandas Series, pandas DataFrame, dict\n        Time series.\n    folds : list\n        Folds created using the skforecast.model_selection._create_backtesting_folds\n        function.\n    span_index : pandas DatetimeIndex, pandas RangeIndex\n        Full index from the minimum to the maximum index among all series.\n    window_size : int\n        Size of the window needed to create the predictors.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    dropna_last_window : bool, default `False`\n        If `True`, drop the columns of the last window that have NaN values.\n    externally_fitted : bool, default `False`\n        Flag indicating whether the forecaster is already trained. Only used when \n        `initial_train_size` is None and `refit` is False.\n\n    Yield\n    -----\n    series_train : pandas Series, pandas DataFrame, dict\n        Time series corresponding to the training set of the fold.\n    series_last_window: pandas DataFrame\n        Time series corresponding to the last window of the fold.\n    levels_last_window: list\n        Levels of the time series present in the last window of the fold.\n    exog_train: pandas Series, pandas DataFrame, dict, None\n        Exogenous variable corresponding to the training set of the fold.\n    exog_test: pandas Series, pandas DataFrame, dict, None\n        Exogenous variable corresponding to the test set of the fold.\n    fold: list\n        Fold created using the skforecast.model_selection._create_backtesting_folds\n\n    \"\"\"\n    for fold in folds:\n        train_iloc_start = fold[0][0]\n        train_iloc_end = fold[0][1]\n        last_window_iloc_start = fold[1][0]\n        last_window_iloc_end = fold[1][1]\n        test_iloc_start = fold[2][0]\n        test_iloc_end = fold[2][1]\n        if isinstance(series, dict) or isinstance(exog, dict):\n            train_loc_start = span_index[train_iloc_start]\n            train_loc_end = span_index[train_iloc_end - 1]\n            last_window_loc_start = span_index[last_window_iloc_start]\n            last_window_loc_end = span_index[last_window_iloc_end - 1]\n            test_loc_start = span_index[test_iloc_start]\n            test_loc_end = span_index[test_iloc_end - 1]\n        if isinstance(series, pd.DataFrame):\n            series_train = series.iloc[train_iloc_start:train_iloc_end,]\n            series_to_drop = []\n            for col in series_train.columns:\n                if series_train[col].isna().all():\n                    series_to_drop.append(col)\n                else:\n                    first_valid_index = series_train[col].first_valid_index()\n                    last_valid_index = series_train[col].last_valid_index()\n                    if len(series_train[col].loc[first_valid_index:last_valid_index]) < window_size:\n                        series_to_drop.append(col)\n            series_last_window = series.iloc[last_window_iloc_start:last_window_iloc_end,]\n            series_train = series_train.drop(columns=series_to_drop)\n            if not externally_fitted:\n                series_last_window = series_last_window.drop(columns=series_to_drop)\n        else:\n            series_train = {}\n            for k in series.keys():\n                v = series[k].loc[train_loc_start:train_loc_end]\n                if not v.isna().all():\n                    first_valid_index = v.first_valid_index()\n                    last_valid_index = v.last_valid_index()\n                    if first_valid_index is not None and last_valid_index is not None:\n                        v = v.loc[first_valid_index:last_valid_index]\n                        if len(v) >= window_size:\n                            series_train[k] = v\n            series_last_window = {}\n            for k, v in series.items():\n                v = series[k].loc[last_window_loc_start:last_window_loc_end]\n                if (externally_fitted or k in series_train) and len(v) >= window_size:\n                    series_last_window[k] = v\n            series_last_window = pd.DataFrame(series_last_window)\n        if dropna_last_window:\n            series_last_window = series_last_window.dropna(axis=1, how='any')\n        levels_last_window = list(series_last_window.columns)\n        if exog is not None:\n            if isinstance(exog, (pd.Series, pd.DataFrame)):\n                exog_train = exog.iloc[train_iloc_start:train_iloc_end,]\n                exog_test = exog.iloc[test_iloc_start:test_iloc_end,]\n            else:\n                exog_train = {k: v.loc[train_loc_start:train_loc_end] for k, v in exog.items()}\n                exog_train = {k: v for k, v in exog_train.items() if len(v) > 0}\n                exog_test = {k: v.loc[test_loc_start:test_loc_end] for k, v in exog.items() if externally_fitted or k in exog_train}\n                exog_test = {k: v for k, v in exog_test.items() if len(v) > 0}\n        else:\n            exog_train = None\n            exog_test = None\n        yield (series_train, series_last_window, levels_last_window, exog_train, exog_test, fold)\n\ndef _calculate_metrics_backtesting_multiseries(series: Union[pd.DataFrame, dict], predictions: pd.DataFrame, folds: Union[list, tqdm], span_index: Union[pd.DatetimeIndex, pd.RangeIndex], window_size: int, metrics: list, levels: list, add_aggregated_metric: bool=True) -> pd.DataFrame:\n    \"\"\"   \n    Calculate metrics for each level and also for all levels aggregated using\n    average, weighted average or pooling.\n\n    - 'average': the average (arithmetic mean) of all levels.\n    - 'weighted_average': the average of the metrics weighted by the number of\n    predicted values of each level.\n    - 'pooling': the values of all levels are pooled and then the metric is\n    calculated.\n\n    Parameters\n    ----------\n    series : pandas DataFrame, dict\n        Series data used for backtesting.\n    predictions : pandas DataFrame\n        Predictions generated during the backtesting process.\n    folds : list, tqdm\n        Folds created during the backtesting process.\n    span_index : pandas DatetimeIndex, pandas RangeIndex\n        Full index from the minimum to the maximum index among all series.\n    window_size : int\n        Size of the window used by the forecaster to create the predictors.\n        This is used remove the first `window_size` (differentiation included) \n        values from y_train since they are not part of the training matrix.\n    metrics : list\n        List of metrics to calculate.\n    levels : list\n        Levels to calculate the metrics.\n    add_aggregated_metric : bool, default `True`\n        If `True`, and multiple series (`levels`) are predicted, the aggregated\n        metrics (average, weighted average and pooled) are also returned.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s).\n    \n    \"\"\"\n    if not isinstance(series, (pd.DataFrame, dict)):\n        raise TypeError('`series` must be a pandas DataFrame or a dictionary of pandas DataFrames.')\n    if not isinstance(predictions, pd.DataFrame):\n        raise TypeError('`predictions` must be a pandas DataFrame.')\n    if not isinstance(folds, (list, tqdm)):\n        raise TypeError('`folds` must be a list or a tqdm object.')\n    if not isinstance(span_index, (pd.DatetimeIndex, pd.RangeIndex)):\n        raise TypeError('`span_index` must be a pandas DatetimeIndex or pandas RangeIndex.')\n    if not isinstance(window_size, (int, np.integer)):\n        raise TypeError('`window_size` must be an integer.')\n    if not isinstance(metrics, list):\n        raise TypeError('`metrics` must be a list.')\n    if not isinstance(levels, list):\n        raise TypeError('`levels` must be a list.')\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError('`add_aggregated_metric` must be a boolean.')\n    metric_names = [m if isinstance(m, str) else m.__name__ for m in metrics]\n    y_true_pred_levels = []\n    y_train_levels = []\n    for level in levels:\n        y_true_pred_level = None\n        y_train = None\n        if level in predictions.columns:\n            y_true_pred_level = pd.merge(series[level], predictions[level], left_index=True, right_index=True, how='inner').dropna(axis=0, how='any')\n            y_true_pred_level.columns = ['y_true', 'y_pred']\n            train_indexes = []\n            for i, fold in enumerate(folds):\n                fit_fold = fold[-1]\n                if i == 0 or fit_fold:\n                    train_iloc_start = fold[0][0]\n                    train_iloc_end = fold[0][1]\n                    train_indexes.append(np.arange(train_iloc_start, train_iloc_end))\n            train_indexes = np.unique(np.concatenate(train_indexes))\n            train_indexes = span_index[train_indexes]\n            y_train = series[level].loc[series[level].index.intersection(train_indexes)]\n        y_true_pred_levels.append(y_true_pred_level)\n        y_train_levels.append(y_train)\n    metrics_levels = []\n    for i, level in enumerate(levels):\n        if y_true_pred_levels[i] is not None and (not y_true_pred_levels[i].empty):\n            metrics_level = [m(y_true=y_true_pred_levels[i].iloc[:, 0], y_pred=y_true_pred_levels[i].iloc[:, 1], y_train=y_train_levels[i].iloc[window_size:]) for m in metrics]\n            metrics_levels.append(metrics_level)\n        else:\n            metrics_levels.append([None for _ in metrics])\n    metrics_levels = pd.DataFrame(data=metrics_levels, columns=[m if isinstance(m, str) else m.__name__ for m in metrics])\n    metrics_levels.insert(0, 'levels', levels)\n    if len(levels) < 2:\n        add_aggregated_metric = False\n    if add_aggregated_metric:\n        average = metrics_levels.drop(columns='levels').mean(skipna=True)\n        average = average.to_frame().transpose()\n        average['levels'] = 'average'\n        weighted_averages = {}\n        n_predictions_levels = predictions.notna().sum().to_frame(name='n_predictions').reset_index(names='levels')\n        metrics_levels_no_missing = metrics_levels.merge(n_predictions_levels, on='levels', how='inner')\n        for col in metric_names:\n            weighted_averages[col] = np.average(metrics_levels_no_missing[col], weights=metrics_levels_no_missing['n_predictions'])\n        weighted_average = pd.DataFrame(weighted_averages, index=[0])\n        weighted_average['levels'] = 'weighted_average'\n        y_true_pred_levels, y_train_levels = zip(*[(a, b.iloc[window_size:]) for a, b in zip(y_true_pred_levels, y_train_levels) if a is not None])\n        y_train_levels = list(y_train_levels)\n        y_true_pred_levels = pd.concat(y_true_pred_levels)\n        y_train_levels_concat = pd.concat(y_train_levels)\n        pooled = []\n        for m, m_name in zip(metrics, metric_names):\n            if m_name in ['mean_absolute_scaled_error', 'root_mean_squared_scaled_error']:\n                pooled.append(m(y_true=y_true_pred_levels.loc[:, 'y_true'], y_pred=y_true_pred_levels.loc[:, 'y_pred'], y_train=y_train_levels))\n            else:\n                pooled.append(m(y_true=y_true_pred_levels.loc[:, 'y_true'], y_pred=y_true_pred_levels.loc[:, 'y_pred'], y_train=y_train_levels_concat))\n        pooled = pd.DataFrame([pooled], columns=metric_names)\n        pooled['levels'] = 'pooling'\n        metrics_levels = pd.concat([metrics_levels, average, weighted_average, pooled], axis=0, ignore_index=True)\n    return metrics_levels\n\ndef _predict_and_calculate_metrics_one_step_ahead_multiseries(forecaster: object, series: Union[pd.DataFrame, dict], X_train: pd.DataFrame, y_train: Union[pd.Series, dict], X_test: pd.DataFrame, y_test: Union[pd.Series, dict], X_train_encoding: pd.Series, X_test_encoding: pd.Series, levels: list, metrics: list, add_aggregated_metric: bool=True) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"   \n    One-step-ahead predictions and metrics for each level and also for all levels\n    aggregated using average, weighted average or pooling.\n    Input matrices (X_train, y_train, X_train_encoding, X_test, y_test, X_test_encoding)\n    should have been generated using the forecaster._train_test_split_one_step_ahead().\n\n    - 'average': the average (arithmetic mean) of all levels.\n    - 'weighted_average': the average of the metrics weighted by the number of\n    predicted values of each level.\n    - 'pooling': the values of all levels are pooled and then the metric is\n    calculated.\n\n    Parameters\n    ----------\n    forecaster : object\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Series data used to train and test the forecaster.\n    X_train : pandas DataFrame\n        Training matrix.\n    y_train : pandas Series, dict\n        Target values of the training set.\n    X_test : pandas DataFrame\n        Test matrix.\n    y_test : pandas Series, dict\n        Target values of the test set.\n    X_train_encoding : pandas Series\n        Series identifiers for each row of `X_train`.\n    X_test_encoding : pandas Series\n        Series identifiers for each row of `X_test`.\n    levels : list\n        Levels to calculate the metrics.\n    metrics : list\n        List of metrics to calculate.\n    add_aggregated_metric : bool, default `True`\n        If `True`, and multiple series (`levels`) are predicted, the aggregated\n        metrics (average, weighted average and pooled) are also returned.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s).\n    predictions : pandas DataFrame\n        Value of predictions for each level.\n    \n    \"\"\"\n    if not isinstance(series, (pd.DataFrame, dict)):\n        raise TypeError('`series` must be a pandas DataFrame or a dictionary of pandas DataFrames.')\n    if not isinstance(X_train, pd.DataFrame):\n        raise TypeError(f'`X_train` must be a pandas DataFrame. Got: {type(X_train)}')\n    if not isinstance(y_train, (pd.Series, dict)):\n        raise TypeError(f'`y_train` must be a pandas Series or a dictionary of pandas Series. Got: {type(y_train)}')\n    if not isinstance(X_test, pd.DataFrame):\n        raise TypeError(f'`X_test` must be a pandas DataFrame. Got: {type(X_test)}')\n    if not isinstance(y_test, (pd.Series, dict)):\n        raise TypeError(f'`y_test` must be a pandas Series or a dictionary of pandas Series. Got: {type(y_test)}')\n    if not isinstance(X_train_encoding, pd.Series):\n        raise TypeError(f'`X_train_encoding` must be a pandas Series. Got: {type(X_train_encoding)}')\n    if not isinstance(X_test_encoding, pd.Series):\n        raise TypeError(f'`X_test_encoding` must be a pandas Series. Got: {type(X_test_encoding)}')\n    if not isinstance(levels, list):\n        raise TypeError(f'`levels` must be a list. Got: {type(levels)}')\n    if not isinstance(metrics, list):\n        raise TypeError(f'`metrics` must be a list. Got: {type(metrics)}')\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError(f'`add_aggregated_metric` must be a boolean. Got: {type(add_aggregated_metric)}')\n    metrics = [_get_metric(metric=m) if isinstance(m, str) else add_y_train_argument(m) for m in metrics]\n    metric_names = [m if isinstance(m, str) else m.__name__ for m in metrics]\n    if isinstance(series[levels[0]].index, pd.DatetimeIndex):\n        freq = series[levels[0]].index.freq\n    else:\n        freq = series[levels[0]].index.step\n    if type(forecaster).__name__ == 'ForecasterDirectMultiVariate':\n        step = 1\n        X_train, y_train = forecaster.filter_train_X_y_for_step(step=step, X_train=X_train, y_train=y_train)\n        X_test, y_test = forecaster.filter_train_X_y_for_step(step=step, X_train=X_test, y_train=y_test)\n        forecaster.regressors_[step].fit(X_train, y_train)\n        pred = forecaster.regressors_[step].predict(X_test)\n    else:\n        forecaster.regressor.fit(X_train, y_train)\n        pred = forecaster.regressor.predict(X_test)\n    predictions_per_level = pd.DataFrame({'y_true': y_test, 'y_pred': pred, '_level_skforecast': X_test_encoding}, index=y_test.index).groupby('_level_skforecast')\n    predictions_per_level = {key: group for key, group in predictions_per_level}\n    y_train_per_level = pd.DataFrame({'y_train': y_train, '_level_skforecast': X_train_encoding}, index=y_train.index).groupby('_level_skforecast')\n    y_train_per_level = {key: group.asfreq(freq) for key, group in y_train_per_level}\n    if forecaster.differentiation is not None:\n        for level in predictions_per_level:\n            predictions_per_level[level]['y_true'] = forecaster.differentiator_[level].inverse_transform_next_window(predictions_per_level[level]['y_true'].to_numpy())\n            predictions_per_level[level]['y_pred'] = forecaster.differentiator_[level].inverse_transform_next_window(predictions_per_level[level]['y_pred'].to_numpy())\n            y_train_per_level[level]['y_train'] = forecaster.differentiator_[level].inverse_transform_training(y_train_per_level[level]['y_train'].to_numpy())\n    if forecaster.transformer_series is not None:\n        for level in predictions_per_level:\n            transformer = forecaster.transformer_series_[level]\n            predictions_per_level[level]['y_true'] = transformer.inverse_transform(predictions_per_level[level][['y_true']])\n            predictions_per_level[level]['y_pred'] = transformer.inverse_transform(predictions_per_level[level][['y_pred']])\n            y_train_per_level[level]['y_train'] = transformer.inverse_transform(y_train_per_level[level][['y_train']])\n    metrics_levels = []\n    for level in levels:\n        if level in predictions_per_level:\n            metrics_level = [m(y_true=predictions_per_level[level].loc[:, 'y_true'], y_pred=predictions_per_level[level].loc[:, 'y_pred'], y_train=y_train_per_level[level].loc[:, 'y_train']) for m in metrics]\n            metrics_levels.append(metrics_level)\n        else:\n            metrics_levels.append([None for _ in metrics])\n    metrics_levels = pd.DataFrame(data=metrics_levels, columns=[m if isinstance(m, str) else m.__name__ for m in metrics])\n    metrics_levels.insert(0, 'levels', levels)\n    if len(levels) < 2:\n        add_aggregated_metric = False\n    if add_aggregated_metric:\n        average = metrics_levels.drop(columns='levels').mean(skipna=True)\n        average = average.to_frame().transpose()\n        average['levels'] = 'average'\n        weighted_averages = {}\n        n_predictions_levels = {k: v['y_pred'].notna().sum() for k, v in predictions_per_level.items()}\n        n_predictions_levels = pd.DataFrame(n_predictions_levels.items(), columns=['levels', 'n_predictions'])\n        metrics_levels_no_missing = metrics_levels.merge(n_predictions_levels, on='levels', how='inner')\n        for col in metric_names:\n            weighted_averages[col] = np.average(metrics_levels_no_missing[col], weights=metrics_levels_no_missing['n_predictions'])\n        weighted_average = pd.DataFrame(weighted_averages, index=[0])\n        weighted_average['levels'] = 'weighted_average'\n        list_y_train_by_level = [v['y_train'].to_numpy() for k, v in y_train_per_level.items() if k in predictions_per_level]\n        predictions_pooled = pd.concat(predictions_per_level.values())\n        y_train_pooled = pd.concat([v for k, v in y_train_per_level.items() if k in predictions_per_level])\n        pooled = []\n        for m, m_name in zip(metrics, metric_names):\n            if m_name in ['mean_absolute_scaled_error', 'root_mean_squared_scaled_error']:\n                pooled.append(m(y_true=predictions_pooled['y_true'], y_pred=predictions_pooled['y_pred'], y_train=list_y_train_by_level))\n            else:\n                pooled.append(m(y_true=predictions_pooled['y_true'], y_pred=predictions_pooled['y_pred'], y_train=y_train_pooled['y_train']))\n        pooled = pd.DataFrame([pooled], columns=metric_names)\n        pooled['levels'] = 'pooling'\n        metrics_levels = pd.concat([metrics_levels, average, weighted_average, pooled], axis=0, ignore_index=True)\n    predictions = pd.concat(predictions_per_level.values()).loc[:, ['y_pred', '_level_skforecast']].pivot(columns='_level_skforecast', values='y_pred').rename_axis(columns=None, index=None)\n    predictions = predictions.asfreq(X_test.index.freq)\n    return (metrics_levels, predictions)",
    "skforecast/recursive/_forecaster_recursive.py": "from typing import Union, Tuple, Optional, Callable\nimport warnings\nimport sys\nimport numpy as np\nimport pandas as pd\nimport inspect\nfrom copy import copy\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import clone\nimport skforecast\nfrom ..base import ForecasterBase\nfrom ..exceptions import DataTransformationWarning\nfrom ..utils import initialize_lags, initialize_window_features, initialize_weights, check_select_fit_kwargs, check_y, check_exog, get_exog_dtypes, check_exog_dtypes, check_predict_input, check_interval, preprocess_y, preprocess_last_window, preprocess_exog, input_to_frame, date_to_index_position, expand_index, transform_numpy, transform_dataframe\nfrom ..preprocessing import TimeSeriesDifferentiator\nfrom ..preprocessing import QuantileBinner\n\nclass ForecasterRecursive(ForecasterBase):\n    \"\"\"\n    This class turns any regressor compatible with the scikit-learn API into a\n    recursive autoregressive (multi-step) forecaster.\n    \n    Parameters\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n    lags : int, list, numpy ndarray, range, default `None`\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n    \n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n        - `None`: no lags are included as predictors. \n    window_features : object, list, default `None`\n        Instance or list of instances used to create window features. Window features\n        are created from the original time series and are included as predictors.\n    transformer_y : object transformer (preprocessor), default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and inverse_transform.\n        ColumnTransformers are not allowed since they do not have inverse_transform method.\n        The transformation is applied to `y` before training the forecaster. \n    transformer_exog : object transformer (preprocessor), default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable, default `None`\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\n        method. The resulting `sample_weight` cannot have negative values.\n    differentiation : int, default `None`\n        Order of differencing applied to the time series before training the forecaster.\n        If `None`, no differencing is applied. The order of differentiation is the number\n        of times the differencing operation is applied to a time series. Differencing\n        involves computing the differences between consecutive data points in the series.\n        Differentiation is reversed in the output of `predict()` and `predict_interval()`.\n        **WARNING: This argument is newly introduced and requires special attention. It\n        is still experimental and may undergo changes.**\n    fit_kwargs : dict, default `None`\n        Additional arguments to be passed to the `fit` method of the regressor.\n    binner_kwargs : dict, default `None`\n        Additional arguments to pass to the `QuantileBinner` used to discretize \n        the residuals into k bins according to the predicted values associated \n        with each residual. Available arguments are: `n_bins`, `method`, `subsample`,\n        `random_state` and `dtype`. Argument `method` is passed internally to the\n        fucntion `numpy.percentile`.\n        **New in version 0.14.0**\n    forecaster_id : str, int, default `None`\n        Name used as an identifier of the forecaster.\n    \n    Attributes\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n    lags : numpy ndarray\n        Lags used as predictors.\n    lags_names : list\n        Names of the lags used as predictors.\n    max_lag : int\n        Maximum lag included in `lags`.\n    window_features : list\n        Class or list of classes used to create window features.\n    window_features_names : list\n        Names of the window features to be included in the `X_train` matrix.\n    window_features_class_names : list\n        Names of the classes used to create the window features.\n    max_size_window_features : int\n        Maximum window size required by the window features.\n    window_size : int\n        The window size needed to create the predictors. It is calculated as the \n        maximum value between `max_lag` and `max_size_window_features`. If \n        differentiation is used, `window_size` is increased by n units equal to \n        the order of differentiation so that predictors can be generated correctly.\n    transformer_y : object transformer (preprocessor)\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and inverse_transform.\n        ColumnTransformers are not allowed since they do not have inverse_transform method.\n        The transformation is applied to `y` before training the forecaster.\n    transformer_exog : object transformer (preprocessor)\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\n        method. The resulting `sample_weight` cannot have negative values.\n    differentiation : int\n        Order of differencing applied to the time series before training the forecaster.\n        If `None`, no differencing is applied. The order of differentiation is the number\n        of times the differencing operation is applied to a time series. Differencing\n        involves computing the differences between consecutive data points in the series.\n        Differentiation is reversed in the output of `predict()` and `predict_interval()`.\n        **WARNING: This argument is newly introduced and requires special attention. It\n        is still experimental and may undergo changes.**\n        **New in version 0.10.0**\n    binner : sklearn.preprocessing.KBinsDiscretizer\n        `KBinsDiscretizer` used to discretize residuals into k bins according \n        to the predicted values associated with each residual.\n        **New in version 0.12.0**\n    binner_intervals_ : dict\n        Intervals used to discretize residuals into k bins according to the predicted\n        values associated with each residual.\n        **New in version 0.12.0**\n    binner_kwargs : dict\n        Additional arguments to pass to the `QuantileBinner` used to discretize \n        the residuals into k bins according to the predicted values associated \n        with each residual. Available arguments are: `n_bins`, `method`, `subsample`,\n        `random_state` and `dtype`. Argument `method` is passed internally to the\n        fucntion `numpy.percentile`.\n        **New in version 0.14.0**\n    source_code_weight_func : str\n        Source code of the custom function used to create weights.\n    differentiation : int\n        Order of differencing applied to the time series before training the \n        forecaster.\n    differentiator : TimeSeriesDifferentiator\n        Skforecast object used to differentiate the time series.\n    last_window_ : pandas DataFrame\n        This window represents the most recent data observed by the predictor\n        during its training phase. It contains the values needed to predict the\n        next step immediately after the training data. These values are stored\n        in the original scale of the time series before undergoing any transformations\n        or differentiation. When `differentiation` parameter is specified, the\n        dimensions of the `last_window_` are expanded as many values as the order\n        of differentiation. For example, if `lags` = 7 and `differentiation` = 1,\n        `last_window_` will have 8 values.\n    index_type_ : type\n        Type of index of the input used in training.\n    index_freq_ : str\n        Frequency of Index of the input used in training.\n    training_range_ : pandas Index\n        First and last values of index of the data used during training.\n    exog_in_ : bool\n        If the forecaster has been trained using exogenous variable/s.\n    exog_names_in_ : list\n        Names of the exogenous variables used during training.\n    exog_type_in_ : type\n        Type of exogenous data (pandas Series or DataFrame) used in training.\n    exog_dtypes_in_ : dict\n        Type of each exogenous variable/s used in training. If `transformer_exog` \n        is used, the dtypes are calculated before the transformation.\n    X_train_window_features_names_out_ : list\n        Names of the window features included in the matrix `X_train` created\n        internally for training.\n    X_train_exog_names_out_ : list\n        Names of the exogenous variables included in the matrix `X_train` created\n        internally for training. It can be different from `exog_names_in_` if\n        some exogenous variables are transformed during the training process.\n    X_train_features_names_out_ : list\n        Names of columns of the matrix created internally for training.\n    fit_kwargs : dict\n        Additional arguments to be passed to the `fit` method of the regressor.\n    in_sample_residuals_ : numpy ndarray\n        Residuals of the model when predicting training data. Only stored up to\n        10_000 values. If `transformer_y` is not `None`, residuals are stored in\n        the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation.\n    in_sample_residuals_by_bin_ : dict\n        In sample residuals binned according to the predicted value each residual\n        is associated with. If `transformer_y` is not `None`, residuals are stored\n        in the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n        **New in version 0.14.0**\n    out_sample_residuals_ : numpy ndarray\n        Residuals of the model when predicting non training data. Only stored up to\n        10_000 values. If `transformer_y` is not `None`, residuals are stored in\n        the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation.\n    out_sample_residuals_by_bin_ : dict\n        Out of sample residuals binned according to the predicted value each residual\n        is associated with. If `transformer_y` is not `None`, residuals are stored\n        in the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n        **New in version 0.12.0**\n    creation_date : str\n        Date of creation.\n    is_fitted : bool\n        Tag to identify if the regressor has been fitted (trained).\n    fit_date : str\n        Date of last fit.\n    skforecast_version : str\n        Version of skforecast library used to create the forecaster.\n    python_version : str\n        Version of python used to create the forecaster.\n    forecaster_id : str, int\n        Name used as an identifier of the forecaster.\n    \n    \"\"\"\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Information displayed when a ForecasterRecursive object is printed.\n        \"\"\"\n        params, _, _, exog_names_in_, _ = self._preprocess_repr(regressor=self.regressor, exog_names_in_=self.exog_names_in_)\n        params = self._format_text_repr(params)\n        exog_names_in_ = self._format_text_repr(exog_names_in_)\n        info = f'{'=' * len(type(self).__name__)} \\n{type(self).__name__} \\n{'=' * len(type(self).__name__)} \\nRegressor: {type(self.regressor).__name__} \\nLags: {self.lags} \\nWindow features: {self.window_features_names} \\nWindow size: {self.window_size} \\nExogenous included: {self.exog_in_} \\nExogenous names: {exog_names_in_} \\nTransformer for y: {self.transformer_y} \\nTransformer for exog: {self.transformer_exog} \\nWeight function included: {(True if self.weight_func is not None else False)} \\nDifferentiation order: {self.differentiation} \\nTraining range: {(self.training_range_.to_list() if self.is_fitted else None)} \\nTraining index type: {(str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None)} \\nTraining index frequency: {(self.index_freq_ if self.is_fitted else None)} \\nRegressor parameters: {params} \\nfit_kwargs: {self.fit_kwargs} \\nCreation date: {self.creation_date} \\nLast fit date: {self.fit_date} \\nSkforecast version: {self.skforecast_version} \\nPython version: {self.python_version} \\nForecaster id: {self.forecaster_id} \\n'\n        return info\n\n    def _repr_html_(self):\n        \"\"\"\n        HTML representation of the object.\n        The \"General Information\" section is expanded by default.\n        \"\"\"\n        params, _, _, exog_names_in_, _ = self._preprocess_repr(regressor=self.regressor, exog_names_in_=self.exog_names_in_)\n        style, unique_id = self._get_style_repr_html(self.is_fitted)\n        content = f'\\n        <div class=\"container-{unique_id}\">\\n            <h2>{type(self).__name__}</h2>\\n            <details open>\\n                <summary>General Information</summary>\\n                <ul>\\n                    <li><strong>Regressor:</strong> {type(self.regressor).__name__}</li>\\n                    <li><strong>Lags:</strong> {self.lags}</li>\\n                    <li><strong>Window features:</strong> {self.window_features_names}</li>\\n                    <li><strong>Window size:</strong> {self.window_size}</li>\\n                    <li><strong>Exogenous included:</strong> {self.exog_in_}</li>\\n                    <li><strong>Weight function included:</strong> {self.weight_func is not None}</li>\\n                    <li><strong>Differentiation order:</strong> {self.differentiation}</li>\\n                    <li><strong>Creation date:</strong> {self.creation_date}</li>\\n                    <li><strong>Last fit date:</strong> {self.fit_date}</li>\\n                    <li><strong>Skforecast version:</strong> {self.skforecast_version}</li>\\n                    <li><strong>Python version:</strong> {self.python_version}</li>\\n                    <li><strong>Forecaster id:</strong> {self.forecaster_id}</li>\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Exogenous Variables</summary>\\n                <ul>\\n                    {exog_names_in_}\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Data Transformations</summary>\\n                <ul>\\n                    <li><strong>Transformer for y:</strong> {self.transformer_y}</li>\\n                    <li><strong>Transformer for exog:</strong> {self.transformer_exog}</li>\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Training Information</summary>\\n                <ul>\\n                    <li><strong>Training range:</strong> {(self.training_range_.to_list() if self.is_fitted else 'Not fitted')}</li>\\n                    <li><strong>Training index type:</strong> {(str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted')}</li>\\n                    <li><strong>Training index frequency:</strong> {(self.index_freq_ if self.is_fitted else 'Not fitted')}</li>\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Regressor Parameters</summary>\\n                <ul>\\n                    {params}\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Fit Kwargs</summary>\\n                <ul>\\n                    {self.fit_kwargs}\\n                </ul>\\n            </details>\\n            <p>\\n                <a href=\"https://skforecast.org/{skforecast.__version__}/api/forecasterrecursive.html\">&#128712 <strong>API Reference</strong></a>\\n                &nbsp;&nbsp;\\n                <a href=\"https://skforecast.org/{skforecast.__version__}/user_guides/autoregresive-forecaster.html\">&#128462 <strong>User Guide</strong></a>\\n            </p>\\n        </div>\\n        '\n        return style + content\n\n    def _create_lags(self, y: np.ndarray, X_as_pandas: bool=False, train_index: Optional[pd.Index]=None) -> Tuple[Optional[Union[np.ndarray, pd.DataFrame]], np.ndarray]:\n        \"\"\"\n        Create the lagged values and their target variable from a time series.\n        \n        Note that the returned matrix `X_data` contains the lag 1 in the first \n        column, the lag 2 in the in the second column and so on.\n        \n        Parameters\n        ----------\n        y : numpy ndarray\n            Training time series values.\n        X_as_pandas : bool, default `False`\n            If `True`, the returned matrix `X_data` is a pandas DataFrame.\n        train_index : pandas Index, default `None`\n            Index of the training data. It is used to create the pandas DataFrame\n            `X_data` when `X_as_pandas` is `True`.\n\n        Returns\n        -------\n        X_data : numpy ndarray, pandas DataFrame, None\n            Lagged values (predictors).\n        y_data : numpy ndarray\n            Values of the time series related to each row of `X_data`.\n        \n        \"\"\"\n        X_data = None\n        if self.lags is not None:\n            n_rows = len(y) - self.window_size\n            X_data = np.full(shape=(n_rows, len(self.lags)), fill_value=np.nan, order='F', dtype=float)\n            for i, lag in enumerate(self.lags):\n                X_data[:, i] = y[self.window_size - lag:-lag]\n            if X_as_pandas:\n                X_data = pd.DataFrame(data=X_data, columns=self.lags_names, index=train_index)\n        y_data = y[self.window_size:]\n        return (X_data, y_data)\n\n    def _create_window_features(self, y: pd.Series, train_index: pd.Index, X_as_pandas: bool=False) -> Tuple[list, list]:\n        \"\"\"\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        train_index : pandas Index\n            Index of the training data. It is used to create the pandas DataFrame\n            `X_train_window_features` when `X_as_pandas` is `True`.\n        X_as_pandas : bool, default `False`\n            If `True`, the returned matrix `X_train_window_features` is a \n            pandas DataFrame.\n\n        Returns\n        -------\n        X_train_window_features : list\n            List of numpy ndarrays or pandas DataFrames with the window features.\n        X_train_window_features_names_out_ : list\n            Names of the window features.\n        \n        \"\"\"\n        len_train_index = len(train_index)\n        X_train_window_features = []\n        X_train_window_features_names_out_ = []\n        for wf in self.window_features:\n            X_train_wf = wf.transform_batch(y)\n            if not isinstance(X_train_wf, pd.DataFrame):\n                raise TypeError(f'The method `transform_batch` of {type(wf).__name__} must return a pandas DataFrame.')\n            X_train_wf = X_train_wf.iloc[-len_train_index:]\n            if not len(X_train_wf) == len_train_index:\n                raise ValueError(f'The method `transform_batch` of {type(wf).__name__} must return a DataFrame with the same number of rows as the input time series - `window_size`: {len_train_index}.')\n            if not (X_train_wf.index == train_index).all():\n                raise ValueError(f'The method `transform_batch` of {type(wf).__name__} must return a DataFrame with the same index as the input time series - `window_size`.')\n            X_train_window_features_names_out_.extend(X_train_wf.columns)\n            if not X_as_pandas:\n                X_train_wf = X_train_wf.to_numpy()\n            X_train_window_features.append(X_train_wf)\n        return (X_train_window_features, X_train_window_features_names_out_)\n\n    def _create_train_X_y(self, y: pd.Series, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, pd.Series, list, list, list, list, dict]:\n        \"\"\"\n        Create training matrices from univariate time series and exogenous\n        variables.\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors).\n        y_train : pandas Series\n            Values of the time series related to each row of `X_train`.\n        exog_names_in_ : list\n            Names of the exogenous variables used during training.\n        X_train_window_features_names_out_ : list\n            Names of the window features included in the matrix `X_train` created\n            internally for training.\n        X_train_exog_names_out_ : list\n            Names of the exogenous variables included in the matrix `X_train` created\n            internally for training. It can be different from `exog_names_in_` if\n            some exogenous variables are transformed during the training process.\n        X_train_features_names_out_ : list\n            Names of the columns of the matrix created internally for training.\n        exog_dtypes_in_ : dict\n            Type of each exogenous variable/s used in training. If `transformer_exog` \n            is used, the dtypes are calculated before the transformation.\n        \n        \"\"\"\n        check_y(y=y)\n        y = input_to_frame(data=y, input_name='y')\n        if len(y) <= self.window_size:\n            raise ValueError(f'Length of `y` must be greater than the maximum window size needed by the forecaster.\\n    Length `y`: {len(y)}.\\n    Max window size: {self.window_size}.\\n    Lags window size: {self.max_lag}.\\n    Window features window size: {self.max_size_window_features}.')\n        fit_transformer = False if self.is_fitted else True\n        y = transform_dataframe(df=y, transformer=self.transformer_y, fit=fit_transformer, inverse_transform=False)\n        y_values, y_index = preprocess_y(y=y)\n        train_index = y_index[self.window_size:]\n        if self.differentiation is not None:\n            if not self.is_fitted:\n                y_values = self.differentiator.fit_transform(y_values)\n            else:\n                differentiator = copy(self.differentiator)\n                y_values = differentiator.fit_transform(y_values)\n        exog_names_in_ = None\n        exog_dtypes_in_ = None\n        categorical_features = False\n        if exog is not None:\n            check_exog(exog=exog, allow_nan=True)\n            exog = input_to_frame(data=exog, input_name='exog')\n            len_y = len(y_values)\n            len_train_index = len(train_index)\n            len_exog = len(exog)\n            if not len_exog == len_y and (not len_exog == len_train_index):\n                raise ValueError(f'Length of `exog` must be equal to the length of `y` (if index is fully aligned) or length of `y` - `window_size` (if `exog` starts after the first `window_size` values).\\n    `exog`              : ({exog.index[0]} -- {exog.index[-1]})  (n={len_exog})\\n    `y`                 : ({y.index[0]} -- {y.index[-1]})  (n={len_y})\\n    `y` - `window_size` : ({train_index[0]} -- {train_index[-1]})  (n={len_train_index})')\n            exog_names_in_ = exog.columns.to_list()\n            exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n            exog = transform_dataframe(df=exog, transformer=self.transformer_exog, fit=fit_transformer, inverse_transform=False)\n            check_exog_dtypes(exog, call_check_exog=True)\n            categorical_features = exog.select_dtypes(include=np.number).shape[1] != exog.shape[1]\n            _, exog_index = preprocess_exog(exog=exog, return_values=False)\n            if len_exog == len_y:\n                if not (exog_index == y_index).all():\n                    raise ValueError('When `exog` has the same length as `y`, the index of `exog` must be aligned with the index of `y` to ensure the correct alignment of values.')\n                exog = exog.iloc[self.window_size:,]\n            elif not (exog_index == train_index).all():\n                raise ValueError(\"When `exog` doesn't contain the first `window_size` observations, the index of `exog` must be aligned with the index of `y` minus the first `window_size` observations to ensure the correct alignment of values.\")\n        X_train = []\n        X_train_features_names_out_ = []\n        X_as_pandas = True if categorical_features else False\n        X_train_lags, y_train = self._create_lags(y=y_values, X_as_pandas=X_as_pandas, train_index=train_index)\n        if X_train_lags is not None:\n            X_train.append(X_train_lags)\n            X_train_features_names_out_.extend(self.lags_names)\n        X_train_window_features_names_out_ = None\n        if self.window_features is not None:\n            n_diff = 0 if self.differentiation is None else self.differentiation\n            y_window_features = pd.Series(y_values[n_diff:], index=y_index[n_diff:])\n            X_train_window_features, X_train_window_features_names_out_ = self._create_window_features(y=y_window_features, X_as_pandas=X_as_pandas, train_index=train_index)\n            X_train.extend(X_train_window_features)\n            X_train_features_names_out_.extend(X_train_window_features_names_out_)\n        X_train_exog_names_out_ = None\n        if exog is not None:\n            X_train_exog_names_out_ = exog.columns.to_list()\n            if not X_as_pandas:\n                exog = exog.to_numpy()\n            X_train_features_names_out_.extend(X_train_exog_names_out_)\n            X_train.append(exog)\n        if len(X_train) == 1:\n            X_train = X_train[0]\n        elif X_as_pandas:\n            X_train = pd.concat(X_train, axis=1)\n        else:\n            X_train = np.concatenate(X_train, axis=1)\n        if X_as_pandas:\n            X_train.index = train_index\n        else:\n            X_train = pd.DataFrame(data=X_train, index=train_index, columns=X_train_features_names_out_)\n        y_train = pd.Series(data=y_train, index=train_index, name='y')\n        return (X_train, y_train, exog_names_in_, X_train_window_features_names_out_, X_train_exog_names_out_, X_train_features_names_out_, exog_dtypes_in_)\n\n    def create_train_X_y(self, y: pd.Series, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, pd.Series]:\n        \"\"\"\n        Create training matrices from univariate time series and exogenous\n        variables.\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors).\n        y_train : pandas Series\n            Values of the time series related to each row of `X_data`.\n        \n        \"\"\"\n        output = self._create_train_X_y(y=y, exog=exog)\n        X_train = output[0]\n        y_train = output[1]\n        return (X_train, y_train)\n\n    def create_sample_weights(self, X_train: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Crate weights for each observation according to the forecaster's attribute\n        `weight_func`.\n\n        Parameters\n        ----------\n        X_train : pandas DataFrame\n            Dataframe created with the `create_train_X_y` method, first return.\n\n        Returns\n        -------\n        sample_weight : numpy ndarray\n            Weights to use in `fit` method.\n\n        \"\"\"\n        sample_weight = None\n        if self.weight_func is not None:\n            sample_weight = self.weight_func(X_train.index)\n        if sample_weight is not None:\n            if np.isnan(sample_weight).any():\n                raise ValueError('The resulting `sample_weight` cannot have NaN values.')\n            if np.any(sample_weight < 0):\n                raise ValueError('The resulting `sample_weight` cannot have negative values.')\n            if np.sum(sample_weight) == 0:\n                raise ValueError('The resulting `sample_weight` cannot be normalized because the sum of the weights is zero.')\n        return sample_weight\n\n    def fit(self, y: pd.Series, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, store_last_window: bool=True, store_in_sample_residuals: bool=True, random_state: int=123) -> None:\n        \"\"\"\n        Training Forecaster.\n\n        Additional arguments to be passed to the `fit` method of the regressor \n        can be added with the `fit_kwargs` argument when initializing the forecaster.\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned so\n            that y[i] is regressed on exog[i].\n        store_last_window : bool, default `True`\n            Whether or not to store the last window (`last_window_`) of training data.\n        store_in_sample_residuals : bool, default `True`\n            If `True`, in-sample residuals will be stored in the forecaster object\n            after fitting (`in_sample_residuals_` attribute).\n        random_state : int, default `123`\n            Set a seed for the random generator so that the stored sample \n            residuals are always deterministic.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.exog_in_ = False\n        self.exog_names_in_ = None\n        self.exog_type_in_ = None\n        self.exog_dtypes_in_ = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_ = None\n        self.X_train_features_names_out_ = None\n        self.in_sample_residuals_ = None\n        self.is_fitted = False\n        self.fit_date = None\n        X_train, y_train, exog_names_in_, X_train_window_features_names_out_, X_train_exog_names_out_, X_train_features_names_out_, exog_dtypes_in_ = self._create_train_X_y(y=y, exog=exog)\n        sample_weight = self.create_sample_weights(X_train=X_train)\n        if sample_weight is not None:\n            self.regressor.fit(X=X_train, y=y_train, sample_weight=sample_weight, **self.fit_kwargs)\n        else:\n            self.regressor.fit(X=X_train, y=y_train, **self.fit_kwargs)\n        self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n        self.X_train_features_names_out_ = X_train_features_names_out_\n        self.is_fitted = True\n        self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.training_range_ = preprocess_y(y=y, return_values=False)[1][[0, -1]]\n        self.index_type_ = type(X_train.index)\n        if isinstance(X_train.index, pd.DatetimeIndex):\n            self.index_freq_ = X_train.index.freqstr\n        else:\n            self.index_freq_ = X_train.index.step\n        if exog is not None:\n            self.exog_in_ = True\n            self.exog_type_in_ = type(exog)\n            self.exog_names_in_ = exog_names_in_\n            self.exog_dtypes_in_ = exog_dtypes_in_\n            self.X_train_exog_names_out_ = X_train_exog_names_out_\n        if store_in_sample_residuals:\n            self._binning_in_sample_residuals(y_true=y_train.to_numpy(), y_pred=self.regressor.predict(X_train).ravel(), random_state=random_state)\n        if store_last_window:\n            self.last_window_ = y.iloc[-self.window_size:].copy().to_frame(name=y.name if y.name is not None else 'y')\n\n    def _binning_in_sample_residuals(self, y_true: np.ndarray, y_pred: np.ndarray, random_state: int=123) -> None:\n        \"\"\"\n        Binning residuals according to the predicted value each residual is\n        associated with. First a skforecast.preprocessing.QuantileBinner object\n        is fitted to the predicted values. Then, residuals are binned according\n        to the predicted value each residual is associated with. Residuals are\n        stored in the forecaster object as `in_sample_residuals_` and\n        `in_sample_residuals_by_bin_`.\n        If `transformer_y` is not `None`, `y_true` and `y_pred` are transformed\n        before calculating residuals. If `differentiation` is not `None`, `y_true`\n        and `y_pred` are differentiated before calculating residuals. If both,\n        `transformer_y` and `differentiation` are not `None`, transformation is\n        done before differentiation. The number of residuals stored per bin is\n        limited to  `10_000 // self.binner.n_bins_`. The total number of residuals\n        stored is `10_000`.\n        **New in version 0.14.0**\n\n        Parameters\n        ----------\n        y_true : numpy ndarray\n            True values of the time series.\n        y_pred : numpy ndarray\n            Predicted values of the time series.\n        random_state : int, default `123`\n            Set a seed for the random generator so that the stored sample \n            residuals are always deterministic.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        data = pd.DataFrame({'prediction': y_pred, 'residuals': y_true - y_pred})\n        data['bin'] = self.binner.fit_transform(y_pred).astype(int)\n        self.in_sample_residuals_by_bin_ = data.groupby('bin')['residuals'].apply(np.array).to_dict()\n        rng = np.random.default_rng(seed=random_state)\n        max_sample = 10000 // self.binner.n_bins_\n        for k, v in self.in_sample_residuals_by_bin_.items():\n            if len(v) > max_sample:\n                sample = v[rng.integers(low=0, high=len(v), size=max_sample)]\n                self.in_sample_residuals_by_bin_[k] = sample\n        self.in_sample_residuals_ = np.concatenate(list(self.in_sample_residuals_by_bin_.values()))\n        self.binner_intervals_ = self.binner.intervals_\n\n    def _create_predict_inputs(self, steps: Union[int, str, pd.Timestamp], last_window: Optional[Union[pd.Series, pd.DataFrame]]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, predict_boot: bool=False, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False, check_inputs: bool=True) -> Tuple[np.ndarray, Optional[np.ndarray], pd.Index, int]:\n        \"\"\"\n        Create the inputs needed for the first iteration of the prediction \n        process. As this is a recursive process, the last window is updated at \n        each iteration of the prediction process.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        predict_boot : bool, default `False`\n            If `True`, residuals are returned to generate bootstrapping predictions.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n\n        Returns\n        -------\n        last_window_values : numpy ndarray\n            Series values used to create the predictors needed in the first \n            iteration of the prediction (t + 1).\n        exog_values : numpy ndarray, None\n            Exogenous variable/s included as predictor/s.\n        prediction_index : pandas Index\n            Index of the predictions.\n        steps: int\n            Number of future steps predicted.\n        \n        \"\"\"\n        if last_window is None:\n            last_window = self.last_window_\n        if self.is_fitted:\n            steps = date_to_index_position(index=last_window.index, date_input=steps, date_literal='steps')\n        if check_inputs:\n            check_predict_input(forecaster_name=type(self).__name__, steps=steps, is_fitted=self.is_fitted, exog_in_=self.exog_in_, index_type_=self.index_type_, index_freq_=self.index_freq_, window_size=self.window_size, last_window=last_window, exog=exog, exog_type_in_=self.exog_type_in_, exog_names_in_=self.exog_names_in_, interval=None)\n            if predict_boot and (not use_in_sample_residuals):\n                if not use_binned_residuals and self.out_sample_residuals_ is None:\n                    raise ValueError('`forecaster.out_sample_residuals_` is `None`. Use `use_in_sample_residuals=True` or the `set_out_sample_residuals()` method before predicting.')\n                if use_binned_residuals and self.out_sample_residuals_by_bin_ is None:\n                    raise ValueError('`forecaster.out_sample_residuals_by_bin_` is `None`. Use `use_in_sample_residuals=True` or the `set_out_sample_residuals()` method before predicting.')\n        last_window = last_window.iloc[-self.window_size:].copy()\n        last_window_values, last_window_index = preprocess_last_window(last_window=last_window)\n        last_window_values = transform_numpy(array=last_window_values, transformer=self.transformer_y, fit=False, inverse_transform=False)\n        if self.differentiation is not None:\n            last_window_values = self.differentiator.fit_transform(last_window_values)\n        if exog is not None:\n            exog = input_to_frame(data=exog, input_name='exog')\n            exog = exog.loc[:, self.exog_names_in_]\n            exog = transform_dataframe(df=exog, transformer=self.transformer_exog, fit=False, inverse_transform=False)\n            check_exog_dtypes(exog=exog)\n            exog_values = exog.to_numpy()[:steps]\n        else:\n            exog_values = None\n        prediction_index = expand_index(index=last_window_index, steps=steps)\n        return (last_window_values, exog_values, prediction_index, steps)\n\n    def _recursive_predict(self, steps: int, last_window_values: np.ndarray, exog_values: Optional[np.ndarray]=None, residuals: Optional[Union[np.ndarray, dict]]=None, use_binned_residuals: bool=False) -> np.ndarray:\n        \"\"\"\n        Predict n steps ahead. It is an iterative process in which, each prediction,\n        is used as a predictor for the next step.\n        \n        Parameters\n        ----------\n        steps : int\n            Number of future steps predicted.\n        last_window_values : numpy ndarray\n            Series values used to create the predictors needed in the first \n            iteration of the prediction (t + 1).\n        exog_values : numpy ndarray, default `None`\n            Exogenous variable/s included as predictor/s.\n        residuals : numpy ndarray, dict, default `None`\n            Residuals used to generate bootstrapping predictions.\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : numpy ndarray\n            Predicted values.\n        \n        \"\"\"\n        n_lags = len(self.lags) if self.lags is not None else 0\n        n_window_features = len(self.X_train_window_features_names_out_) if self.window_features is not None else 0\n        n_exog = exog_values.shape[1] if exog_values is not None else 0\n        X = np.full(shape=n_lags + n_window_features + n_exog, fill_value=np.nan, dtype=float)\n        predictions = np.full(shape=steps, fill_value=np.nan, dtype=float)\n        last_window = np.concatenate((last_window_values, predictions))\n        for i in range(steps):\n            if self.lags is not None:\n                X[:n_lags] = last_window[-self.lags - (steps - i)]\n            if self.window_features is not None:\n                X[n_lags:n_lags + n_window_features] = np.concatenate([wf.transform(last_window[i:-(steps - i)]) for wf in self.window_features])\n            if exog_values is not None:\n                X[n_lags + n_window_features:] = exog_values[i]\n            pred = self.regressor.predict(X.reshape(1, -1)).ravel()\n            if residuals is not None:\n                if use_binned_residuals:\n                    predicted_bin = self.binner.transform(pred).item()\n                    step_residual = residuals[predicted_bin][i]\n                else:\n                    step_residual = residuals[i]\n                pred += step_residual\n            predictions[i] = pred[0]\n            last_window[-(steps - i)] = pred[0]\n        return predictions\n\n    def create_predict_X(self, steps: int, last_window: Optional[Union[pd.Series, pd.DataFrame]]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> pd.DataFrame:\n        \"\"\"\n        Create the predictors needed to predict `steps` ahead. As it is a recursive\n        process, the predictors are created at each iteration of the prediction \n        process.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n\n        Returns\n        -------\n        X_predict : pandas DataFrame\n            Pandas DataFrame with the predictors for each step. The index \n            is the same as the prediction index.\n        \n        \"\"\"\n        last_window_values, exog_values, prediction_index, steps = self._create_predict_inputs(steps=steps, last_window=last_window, exog=exog)\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', message='X does not have valid feature names', category=UserWarning)\n            predictions = self._recursive_predict(steps=steps, last_window_values=last_window_values, exog_values=exog_values)\n        X_predict = []\n        full_predictors = np.concatenate((last_window_values, predictions))\n        if self.lags is not None:\n            idx = np.arange(-steps, 0)[:, None] - self.lags\n            X_lags = full_predictors[idx + len(full_predictors)]\n            X_predict.append(X_lags)\n        if self.window_features is not None:\n            X_window_features = np.full(shape=(steps, len(self.X_train_window_features_names_out_)), fill_value=np.nan, order='C', dtype=float)\n            for i in range(steps):\n                X_window_features[i, :] = np.concatenate([wf.transform(full_predictors[i:-(steps - i)]) for wf in self.window_features])\n            X_predict.append(X_window_features)\n        if exog is not None:\n            X_predict.append(exog_values)\n        X_predict = pd.DataFrame(data=np.concatenate(X_predict, axis=1), columns=self.X_train_features_names_out_, index=prediction_index)\n        if self.transformer_y is not None or self.differentiation is not None:\n            warnings.warn('The output matrix is in the transformed scale due to the inclusion of transformations or differentiation in the Forecaster. As a result, any predictions generated using this matrix will also be in the transformed scale. Please refer to the documentation for more details: https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html', DataTransformationWarning)\n        return X_predict\n\n    def predict(self, steps: Union[int, str, pd.Timestamp], last_window: Optional[Union[pd.Series, pd.DataFrame]]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, check_inputs: bool=True) -> pd.Series:\n        \"\"\"\n        Predict n steps ahead. It is an recursive process in which, each prediction,\n        is used as a predictor for the next step.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n\n        Returns\n        -------\n        predictions : pandas Series\n            Predicted values.\n        \n        \"\"\"\n        last_window_values, exog_values, prediction_index, steps = self._create_predict_inputs(steps=steps, last_window=last_window, exog=exog, check_inputs=check_inputs)\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', message='X does not have valid feature names', category=UserWarning)\n            predictions = self._recursive_predict(steps=steps, last_window_values=last_window_values, exog_values=exog_values)\n        if self.differentiation is not None:\n            predictions = self.differentiator.inverse_transform_next_window(predictions)\n        predictions = transform_numpy(array=predictions, transformer=self.transformer_y, fit=False, inverse_transform=True)\n        predictions = pd.Series(data=predictions, index=prediction_index, name='pred')\n        return predictions\n\n    def predict_bootstrapping(self, steps: Union[int, str, pd.Timestamp], last_window: Optional[pd.Series]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False) -> pd.DataFrame:\n        \"\"\"\n        Generate multiple forecasting predictions using a bootstrapping process. \n        By sampling from a collection of past observed errors (the residuals),\n        each iteration of bootstrapping generates a different set of predictions. \n        See the Notes section for more information. \n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        boot_predictions : pandas DataFrame\n            Predictions generated by bootstrapping.\n            Shape: (steps, n_boot)\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n        Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n        \"\"\"\n        last_window_values, exog_values, prediction_index, steps = self._create_predict_inputs(steps=steps, last_window=last_window, exog=exog, predict_boot=True, use_in_sample_residuals=use_in_sample_residuals, use_binned_residuals=use_binned_residuals)\n        if use_in_sample_residuals:\n            residuals = self.in_sample_residuals_\n            residuals_by_bin = self.in_sample_residuals_by_bin_\n        else:\n            residuals = self.out_sample_residuals_\n            residuals_by_bin = self.out_sample_residuals_by_bin_\n        rng = np.random.default_rng(seed=random_state)\n        if use_binned_residuals:\n            sampled_residuals = {k: v[rng.integers(low=0, high=len(v), size=(steps, n_boot))] for k, v in residuals_by_bin.items()}\n        else:\n            sampled_residuals = residuals[rng.integers(low=0, high=len(residuals), size=(steps, n_boot))]\n        boot_columns = []\n        boot_predictions = np.full(shape=(steps, n_boot), fill_value=np.nan, order='F', dtype=float)\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', message='X does not have valid feature names', category=UserWarning)\n            for i in range(n_boot):\n                if use_binned_residuals:\n                    boot_sampled_residuals = {k: v[:, i] for k, v in sampled_residuals.items()}\n                else:\n                    boot_sampled_residuals = sampled_residuals[:, i]\n                boot_columns.append(f'pred_boot_{i}')\n                boot_predictions[:, i] = self._recursive_predict(steps=steps, last_window_values=last_window_values, exog_values=exog_values, residuals=boot_sampled_residuals, use_binned_residuals=use_binned_residuals)\n        if self.differentiation is not None:\n            boot_predictions = self.differentiator.inverse_transform_next_window(boot_predictions)\n        if self.transformer_y:\n            boot_predictions = np.apply_along_axis(func1d=transform_numpy, axis=0, arr=boot_predictions, transformer=self.transformer_y, fit=False, inverse_transform=True)\n        boot_predictions = pd.DataFrame(data=boot_predictions, index=prediction_index, columns=boot_columns)\n        return boot_predictions\n\n    def predict_interval(self, steps: Union[int, str, pd.Timestamp], last_window: Optional[pd.Series]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, interval: list=[5, 95], n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False) -> pd.DataFrame:\n        \"\"\"\n        Iterative process in which each prediction is used as a predictor\n        for the next step, and bootstrapping is used to estimate prediction\n        intervals. Both predictions and intervals are returned.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        interval : list, default `[5, 95]`\n            Confidence of the prediction interval estimated. Sequence of \n            percentiles to compute, which must be between 0 and 100 inclusive. \n            For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Values predicted by the forecaster and their estimated interval.\n\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n        check_interval(interval=interval)\n        boot_predictions = self.predict_bootstrapping(steps=steps, last_window=last_window, exog=exog, n_boot=n_boot, random_state=random_state, use_in_sample_residuals=use_in_sample_residuals, use_binned_residuals=use_binned_residuals)\n        predictions = self.predict(steps=steps, last_window=last_window, exog=exog, check_inputs=False)\n        interval = np.array(interval) / 100\n        predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n        predictions_interval.columns = ['lower_bound', 'upper_bound']\n        predictions = pd.concat((predictions, predictions_interval), axis=1)\n        return predictions\n\n    def predict_quantiles(self, steps: Union[int, str, pd.Timestamp], last_window: Optional[pd.Series]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, quantiles: list=[0.05, 0.5, 0.95], n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False) -> pd.DataFrame:\n        \"\"\"\n        Calculate the specified quantiles for each step. After generating \n        multiple forecasting predictions through a bootstrapping process, each \n        quantile is calculated for each step.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        quantiles : list, default `[0.05, 0.5, 0.95]`\n            Sequence of quantiles to compute, which must be between 0 and 1 \n            inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n            `quantiles = [0.05, 0.5, 0.95]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate quantiles.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot quantiles are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create prediction quantiles. If `False`, out of\n            sample residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Quantiles predicted by the forecaster.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n        check_interval(quantiles=quantiles)\n        boot_predictions = self.predict_bootstrapping(steps=steps, last_window=last_window, exog=exog, n_boot=n_boot, random_state=random_state, use_in_sample_residuals=use_in_sample_residuals, use_binned_residuals=use_binned_residuals)\n        predictions = boot_predictions.quantile(q=quantiles, axis=1).transpose()\n        predictions.columns = [f'q_{q}' for q in quantiles]\n        return predictions\n\n    def predict_dist(self, steps: Union[int, str, pd.Timestamp], distribution: object, last_window: Optional[pd.Series]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False) -> pd.DataFrame:\n        \"\"\"\n        Fit a given probability distribution for each step. After generating \n        multiple forecasting predictions through a bootstrapping process, each \n        step is fitted to the given distribution.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        distribution : Object\n            A distribution object from scipy.stats.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).  \n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Distribution parameters estimated for each step.\n\n        \"\"\"\n        boot_samples = self.predict_bootstrapping(steps=steps, last_window=last_window, exog=exog, n_boot=n_boot, random_state=random_state, use_in_sample_residuals=use_in_sample_residuals, use_binned_residuals=use_binned_residuals)\n        param_names = [p for p in inspect.signature(distribution._pdf).parameters if not p == 'x'] + ['loc', 'scale']\n        param_values = np.apply_along_axis(lambda x: distribution.fit(x), axis=1, arr=boot_samples)\n        predictions = pd.DataFrame(data=param_values, columns=param_names, index=boot_samples.index)\n        return predictions\n\n    def set_params(self, params: dict) -> None:\n        \"\"\"\n        Set new values to the parameters of the scikit learn model stored in the\n        forecaster.\n        \n        Parameters\n        ----------\n        params : dict\n            Parameters values.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        self.regressor = clone(self.regressor)\n        self.regressor.set_params(**params)\n\n    def set_fit_kwargs(self, fit_kwargs: dict) -> None:\n        \"\"\"\n        Set new values for the additional keyword arguments passed to the `fit` \n        method of the regressor.\n        \n        Parameters\n        ----------\n        fit_kwargs : dict\n            Dict of the form {\"argument\": new_value}.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n\n    def set_lags(self, lags: Optional[Union[int, list, np.ndarray, range]]=None) -> None:\n        \"\"\"\n        Set new value to the attribute `lags`. Attributes `lags_names`, \n        `max_lag` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        lags : int, list, numpy ndarray, range, default `None`\n            Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. \n        \n            - `int`: include lags from 1 to `lags` (included).\n            - `list`, `1d numpy ndarray` or `range`: include only lags present in \n            `lags`, all elements must be int.\n            - `None`: no lags are included as predictors. \n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        if self.window_features is None and lags is None:\n            raise ValueError('At least one of the arguments `lags` or `window_features` must be different from None. This is required to create the predictors used in training the forecaster.')\n        self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\n        self.window_size = max([ws for ws in [self.max_lag, self.max_size_window_features] if ws is not None])\n        if self.differentiation is not None:\n            self.window_size += self.differentiation\n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_window_features(self, window_features: Optional[Union[object, list]]=None) -> None:\n        \"\"\"\n        Set new value to the attribute `window_features`. Attributes \n        `max_size_window_features`, `window_features_names`, \n        `window_features_class_names` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        window_features : object, list, default `None`\n            Instance or list of instances used to create window features. Window features\n            are created from the original time series and are included as predictors.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        if window_features is None and self.lags is None:\n            raise ValueError('At least one of the arguments `lags` or `window_features` must be different from None. This is required to create the predictors used in training the forecaster.')\n        self.window_features, self.window_features_names, self.max_size_window_features = initialize_window_features(window_features)\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [type(wf).__name__ for wf in self.window_features]\n        self.window_size = max([ws for ws in [self.max_lag, self.max_size_window_features] if ws is not None])\n        if self.differentiation is not None:\n            self.window_size += self.differentiation\n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_out_sample_residuals(self, y_true: Union[pd.Series, np.ndarray], y_pred: Union[pd.Series, np.ndarray], append: bool=False, random_state: int=123) -> None:\n        \"\"\"\n        Set new values to the attribute `out_sample_residuals_`. Out of sample\n        residuals are meant to be calculated using observations that did not\n        participate in the training process. `y_true` and `y_pred` are expected\n        to be in the original scale of the time series. Residuals are calculated\n        as `y_true` - `y_pred`, after applying the necessary transformations and\n        differentiations if the forecaster includes them (`self.transformer_y`\n        and `self.differentiation`). Two internal attributes are updated:\n\n        + `out_sample_residuals_`: residuals stored in a numpy ndarray.\n        + `out_sample_residuals_by_bin_`: residuals are binned according to the\n        predicted value they are associated with and stored in a dictionary, where\n        the keys are the  intervals of the predicted values and the values are\n        the residuals associated with that range. If a bin binning is empty, it\n        is filled with a random sample of residuals from other bins. This is done\n        to ensure that all bins have at least one residual and can be used in the\n        prediction process.\n\n        A total of 10_000 residuals are stored in the attribute `out_sample_residuals_`.\n        If the number of residuals is greater than 10_000, a random sample of\n        10_000 residuals is stored. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n        \n        Parameters\n        ----------\n        y_true : pandas Series, numpy ndarray, default `None`\n            True values of the time series from which the residuals have been\n            calculated.\n        y_pred : pandas Series, numpy ndarray, default `None`\n            Predicted values of the time series.\n        append : bool, default `False`\n            If `True`, new residuals are added to the once already stored in the\n            forecaster. If after appending the new residuals, the limit of\n            `10_000 // self.binner.n_bins_` values per bin is reached, a random\n            sample of residuals is stored.\n        random_state : int, default `123`\n            Sets a seed to the random sampling for reproducible output.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n        if not self.is_fitted:\n            raise NotFittedError('This forecaster is not fitted yet. Call `fit` with appropriate arguments before using `set_out_sample_residuals()`.')\n        if not isinstance(y_true, (np.ndarray, pd.Series)):\n            raise TypeError(f'`y_true` argument must be `numpy ndarray` or `pandas Series`. Got {type(y_true)}.')\n        if not isinstance(y_pred, (np.ndarray, pd.Series)):\n            raise TypeError(f'`y_pred` argument must be `numpy ndarray` or `pandas Series`. Got {type(y_pred)}.')\n        if len(y_true) != len(y_pred):\n            raise ValueError(f'`y_true` and `y_pred` must have the same length. Got {len(y_true)} and {len(y_pred)}.')\n        if isinstance(y_true, pd.Series) and isinstance(y_pred, pd.Series):\n            if not y_true.index.equals(y_pred.index):\n                raise ValueError('`y_true` and `y_pred` must have the same index.')\n        if not isinstance(y_pred, np.ndarray):\n            y_pred = y_pred.to_numpy()\n        if not isinstance(y_true, np.ndarray):\n            y_true = y_true.to_numpy()\n        if self.transformer_y:\n            y_true = transform_numpy(array=y_true, transformer=self.transformer_y, fit=False, inverse_transform=False)\n            y_pred = transform_numpy(array=y_pred, transformer=self.transformer_y, fit=False, inverse_transform=False)\n        if self.differentiation is not None:\n            differentiator = copy(self.differentiator)\n            differentiator.set_params(window_size=None)\n            y_true = differentiator.fit_transform(y_true)[self.differentiation:]\n            y_pred = differentiator.fit_transform(y_pred)[self.differentiation:]\n        residuals = y_true - y_pred\n        data = pd.DataFrame({'prediction': y_pred, 'residuals': residuals})\n        data['bin'] = self.binner.transform(y_pred).astype(int)\n        residuals_by_bin = data.groupby('bin')['residuals'].apply(np.array).to_dict()\n        if append and self.out_sample_residuals_by_bin_ is not None:\n            for k, v in residuals_by_bin.items():\n                if k in self.out_sample_residuals_by_bin_:\n                    self.out_sample_residuals_by_bin_[k] = np.concatenate((self.out_sample_residuals_by_bin_[k], v))\n                else:\n                    self.out_sample_residuals_by_bin_[k] = v\n        else:\n            self.out_sample_residuals_by_bin_ = residuals_by_bin\n        max_samples = 10000 // self.binner.n_bins_\n        rng = np.random.default_rng(seed=random_state)\n        for k, v in self.out_sample_residuals_by_bin_.items():\n            if len(v) > max_samples:\n                sample = rng.choice(a=v, size=max_samples, replace=False)\n                self.out_sample_residuals_by_bin_[k] = sample\n        for k in self.in_sample_residuals_by_bin_.keys():\n            if k not in self.out_sample_residuals_by_bin_:\n                self.out_sample_residuals_by_bin_[k] = np.array([])\n        empty_bins = [k for k, v in self.out_sample_residuals_by_bin_.items() if len(v) == 0]\n        if empty_bins:\n            warnings.warn(f'The following bins have no out of sample residuals: {empty_bins}. No predicted values fall in the interval {[self.binner_intervals_[bin] for bin in empty_bins]}. Empty bins will be filled with a random sample of residuals.')\n            for k in empty_bins:\n                self.out_sample_residuals_by_bin_[k] = rng.choice(a=residuals, size=max_samples, replace=True)\n        self.out_sample_residuals_ = np.concatenate(list(self.out_sample_residuals_by_bin_.values()))\n\n    def get_feature_importances(self, sort_importance: bool=True) -> pd.DataFrame:\n        \"\"\"\n        Return feature importances of the regressor stored in the forecaster.\n        Only valid when regressor stores internally the feature importances in the\n        attribute `feature_importances_` or `coef_`. Otherwise, returns `None`.\n\n        Parameters\n        ----------\n        sort_importance: bool, default `True`\n            If `True`, sorts the feature importances in descending order.\n\n        Returns\n        -------\n        feature_importances : pandas DataFrame\n            Feature importances associated with each predictor.\n\n        \"\"\"\n        if not self.is_fitted:\n            raise NotFittedError('This forecaster is not fitted yet. Call `fit` with appropriate arguments before using `get_feature_importances()`.')\n        if isinstance(self.regressor, Pipeline):\n            estimator = self.regressor[-1]\n        else:\n            estimator = self.regressor\n        if hasattr(estimator, 'feature_importances_'):\n            feature_importances = estimator.feature_importances_\n        elif hasattr(estimator, 'coef_'):\n            feature_importances = estimator.coef_\n        else:\n            warnings.warn(f'Impossible to access feature importances for regressor of type {type(estimator)}. This method is only valid when the regressor stores internally the feature importances in the attribute `feature_importances_` or `coef_`.')\n            feature_importances = None\n        if feature_importances is not None:\n            feature_importances = pd.DataFrame({'feature': self.X_train_features_names_out_, 'importance': feature_importances})\n            if sort_importance:\n                feature_importances = feature_importances.sort_values(by='importance', ascending=False)\n        return feature_importances",
    "skforecast/metrics/metrics.py": "from typing import Union, Callable\nimport numpy as np\nimport pandas as pd\nimport inspect\nfrom functools import wraps\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, mean_squared_log_error, median_absolute_error\n\ndef _get_metric(metric: str) -> Callable:\n    \"\"\"\n    Get the corresponding scikit-learn function to calculate the metric.\n\n    Parameters\n    ----------\n    metric : str\n        Metric used to quantify the goodness of fit of the model.\n\n    Returns\n    -------\n    metric : Callable\n        scikit-learn function to calculate the desired metric.\n\n    \"\"\"\n    allowed_metrics = ['mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error', 'median_absolute_error']\n    if metric not in allowed_metrics:\n        raise ValueError(f'Allowed metrics are: {allowed_metrics}. Got {metric}.')\n    metrics = {'mean_squared_error': mean_squared_error, 'mean_absolute_error': mean_absolute_error, 'mean_absolute_percentage_error': mean_absolute_percentage_error, 'mean_squared_log_error': mean_squared_log_error, 'mean_absolute_scaled_error': mean_absolute_scaled_error, 'root_mean_squared_scaled_error': root_mean_squared_scaled_error, 'median_absolute_error': median_absolute_error}\n    metric = add_y_train_argument(metrics[metric])\n    return metric\n\ndef mean_absolute_scaled_error(y_true: Union[pd.Series, np.ndarray], y_pred: Union[pd.Series, np.ndarray], y_train: Union[list, pd.Series, np.ndarray]) -> float:\n    \"\"\"\n    Mean Absolute Scaled Error (MASE)\n\n    MASE is a scale-independent error metric that measures the accuracy of\n    a forecast. It is the mean absolute error of the forecast divided by the\n    mean absolute error of a naive forecast in the training set. The naive\n    forecast is the one obtained by shifting the time series by one period.\n    If y_train is a list of numpy arrays or pandas Series, it is considered\n    that each element is the true value of the target variable in the training\n    set for each time series. In this case, the naive forecast is calculated\n    for each time series separately.\n\n    Parameters\n    ----------\n    y_true : pandas Series, numpy ndarray\n        True values of the target variable.\n    y_pred : pandas Series, numpy ndarray\n        Predicted values of the target variable.\n    y_train : list, pandas Series, numpy ndarray\n        True values of the target variable in the training set. If `list`, it\n        is consider that each element is the true value of the target variable\n        in the training set for each time series.\n\n    Returns\n    -------\n    mase : float\n        MASE value.\n    \n    \"\"\"\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError('`y_true` must be a pandas Series or numpy ndarray.')\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError('`y_pred` must be a pandas Series or numpy ndarray.')\n    if not isinstance(y_train, (list, pd.Series, np.ndarray)):\n        raise TypeError('`y_train` must be a list, pandas Series or numpy ndarray.')\n    if isinstance(y_train, list):\n        for x in y_train:\n            if not isinstance(x, (pd.Series, np.ndarray)):\n                raise TypeError('When `y_train` is a list, each element must be a pandas Series or numpy ndarray.')\n    if len(y_true) != len(y_pred):\n        raise ValueError('`y_true` and `y_pred` must have the same length.')\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError('`y_true` and `y_pred` must have at least one element.')\n    if isinstance(y_train, list):\n        naive_forecast = np.concatenate([np.diff(x) for x in y_train])\n    else:\n        naive_forecast = np.diff(y_train)\n    mase = np.mean(np.abs(y_true - y_pred)) / np.nanmean(np.abs(naive_forecast))\n    return mase\n\ndef root_mean_squared_scaled_error(y_true: Union[pd.Series, np.ndarray], y_pred: Union[pd.Series, np.ndarray], y_train: Union[list, pd.Series, np.ndarray]) -> float:\n    \"\"\"\n    Root Mean Squared Scaled Error (RMSSE)\n\n    RMSSE is a scale-independent error metric that measures the accuracy of\n    a forecast. It is the root mean squared error of the forecast divided by\n    the root mean squared error of a naive forecast in the training set. The\n    naive forecast is the one obtained by shifting the time series by one period.\n    If y_train is a list of numpy arrays or pandas Series, it is considered\n    that each element is the true value of the target variable in the training\n    set for each time series. In this case, the naive forecast is calculated\n    for each time series separately.\n\n    Parameters\n    ----------\n    y_true : pandas Series, numpy ndarray\n        True values of the target variable.\n    y_pred : pandas Series, numpy ndarray\n        Predicted values of the target variable.\n    y_train : list, pandas Series, numpy ndarray\n        True values of the target variable in the training set. If list, it\n        is consider that each element is the true value of the target variable\n        in the training set for each time series.\n\n    Returns\n    -------\n    rmsse : float\n        RMSSE value.\n    \n    \"\"\"\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError('`y_true` must be a pandas Series or numpy ndarray.')\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError('`y_pred` must be a pandas Series or numpy ndarray.')\n    if not isinstance(y_train, (list, pd.Series, np.ndarray)):\n        raise TypeError('`y_train` must be a list, pandas Series or numpy ndarray.')\n    if isinstance(y_train, list):\n        for x in y_train:\n            if not isinstance(x, (pd.Series, np.ndarray)):\n                raise TypeError('When `y_train` is a list, each element must be a pandas Series or numpy ndarray.')\n    if len(y_true) != len(y_pred):\n        raise ValueError('`y_true` and `y_pred` must have the same length.')\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError('`y_true` and `y_pred` must have at least one element.')\n    if isinstance(y_train, list):\n        naive_forecast = np.concatenate([np.diff(x) for x in y_train])\n    else:\n        naive_forecast = np.diff(y_train)\n    rmsse = np.sqrt(np.mean((y_true - y_pred) ** 2)) / np.sqrt(np.nanmean(naive_forecast ** 2))\n    return rmsse",
    "skforecast/direct/_forecaster_direct.py": "from typing import Union, Tuple, Optional, Callable, Any\nimport warnings\nimport sys\nimport numpy as np\nimport pandas as pd\nimport inspect\nfrom copy import copy\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import clone\nfrom joblib import Parallel, delayed, cpu_count\nimport skforecast\nfrom ..base import ForecasterBase\nfrom ..exceptions import DataTransformationWarning\nfrom ..utils import initialize_lags, initialize_window_features, initialize_weights, check_select_fit_kwargs, check_y, check_exog, get_exog_dtypes, check_exog_dtypes, prepare_steps_direct, check_predict_input, check_interval, preprocess_y, preprocess_last_window, preprocess_exog, input_to_frame, exog_to_direct, exog_to_direct_numpy, expand_index, transform_numpy, transform_dataframe, select_n_jobs_fit_forecaster\nfrom ..preprocessing import TimeSeriesDifferentiator\n\nclass ForecasterDirect(ForecasterBase):\n    \"\"\"\n    This class turns any regressor compatible with the scikit-learn API into a\n    autoregressive direct multi-step forecaster. A separate model is created for\n    each forecast time step. See documentation for more details.\n    \n    Parameters\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n    steps : int\n        Maximum number of future steps the forecaster will predict when using\n        method `predict()`. Since a different model is created for each step,\n        this value should be defined before training.\n    lags : int, list, numpy ndarray, range, default `None`\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n    \n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n        - `None`: no lags are included as predictors. \n    window_features : object, list, default `None`\n        Instance or list of instances used to create window features. Window features\n        are created from the original time series and are included as predictors.\n    transformer_y : object transformer (preprocessor), default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and inverse_transform.\n        ColumnTransformers are not allowed since they do not have inverse_transform method.\n        The transformation is applied to `y` before training the forecaster. \n    transformer_exog : object transformer (preprocessor), default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable, default `None`\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\n        method. The resulting `sample_weight` cannot have negative values.\n    differentiation : int, default `None`\n        Order of differencing applied to the time series before training the forecaster.\n        If `None`, no differencing is applied. The order of differentiation is the number\n        of times the differencing operation is applied to a time series. Differencing\n        involves computing the differences between consecutive data points in the series.\n        Differentiation is reversed in the output of `predict()` and `predict_interval()`.\n        **WARNING: This argument is newly introduced and requires special attention. It\n        is still experimental and may undergo changes.**\n    fit_kwargs : dict, default `None`\n        Additional arguments to be passed to the `fit` method of the regressor.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_fit_forecaster.\n    forecaster_id : str, int, default `None`\n        Name used as an identifier of the forecaster.\n    \n    Attributes\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n        An instance of this regressor is trained for each step. All of them \n        are stored in `self.regressors_`.\n    regressors_ : dict\n        Dictionary with regressors trained for each step. They are initialized \n        as a copy of `regressor`.\n    steps : int\n        Number of future steps the forecaster will predict when using method\n        `predict()`. Since a different model is created for each step, this value\n        should be defined before training.\n    lags : numpy ndarray\n        Lags used as predictors.\n    lags_names : list\n        Names of the lags used as predictors.\n    max_lag : int\n        Maximum lag included in `lags`.\n    window_features : list\n        Class or list of classes used to create window features.\n    window_features_names : list\n        Names of the window features to be included in the `X_train` matrix.\n    window_features_class_names : list\n        Names of the classes used to create the window features.\n    max_size_window_features : int\n        Maximum window size required by the window features.\n    window_size : int\n        The window size needed to create the predictors. It is calculated as the \n        maximum value between `max_lag` and `max_size_window_features`. If \n        differentiation is used, `window_size` is increased by n units equal to \n        the order of differentiation so that predictors can be generated correctly.\n    transformer_y : object transformer (preprocessor)\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and inverse_transform.\n        ColumnTransformers are not allowed since they do not have inverse_transform method.\n        The transformation is applied to `y` before training the forecaster.\n    transformer_exog : object transformer (preprocessor)\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\n        method. The resulting `sample_weight` cannot have negative values.\n    source_code_weight_func : str\n        Source code of the custom function used to create weights.\n    differentiation : int\n        Order of differencing applied to the time series before training the \n        forecaster.\n    differentiator : TimeSeriesDifferentiator\n        Skforecast object used to differentiate the time series.\n    last_window_ : pandas DataFrame\n        This window represents the most recent data observed by the predictor\n        during its training phase. It contains the values needed to predict the\n        next step immediately after the training data. These values are stored\n        in the original scale of the time series before undergoing any transformations\n        or differentiation. When `differentiation` parameter is specified, the\n        dimensions of the `last_window_` are expanded as many values as the order\n        of differentiation. For example, if `lags` = 7 and `differentiation` = 1,\n        `last_window_` will have 8 values.\n    index_type_ : type\n        Type of index of the input used in training.\n    index_freq_ : str\n        Frequency of Index of the input used in training.\n    training_range_ : pandas Index\n        First and last values of index of the data used during training.\n    exog_in_ : bool\n        If the forecaster has been trained using exogenous variable/s.\n    exog_names_in_ : list\n        Names of the exogenous variables used during training.\n    exog_type_in_ : type\n        Type of exogenous data (pandas Series or DataFrame) used in training.\n    exog_dtypes_in_ : dict\n        Type of each exogenous variable/s used in training. If `transformer_exog` \n        is used, the dtypes are calculated after the transformation.\n    X_train_window_features_names_out_ : list\n        Names of the window features included in the matrix `X_train` created\n        internally for training.\n    X_train_exog_names_out_ : list\n        Names of the exogenous variables included in the matrix `X_train` created\n        internally for training. It can be different from `exog_names_in_` if\n        some exogenous variables are transformed during the training process.\n    X_train_direct_exog_names_out_ : list\n        Same as `X_train_exog_names_out_` but using the direct format. The same \n        exogenous variable is repeated for each step.\n    X_train_features_names_out_ : list\n        Names of columns of the matrix created internally for training.\n    fit_kwargs : dict\n        Additional arguments to be passed to the `fit` method of the regressor.\n    in_sample_residuals_ : dict\n        Residuals of the models when predicting training data. Only stored up to\n        1000 values per model in the form `{step: residuals}`. If `transformer_y` \n        is not `None`, residuals are stored in the transformed scale.\n    out_sample_residuals_ : dict\n        Residuals of the models when predicting non training data. Only stored\n        up to 1000 values per model in the form `{step: residuals}`. If `transformer_y` \n        is not `None`, residuals are assumed to be in the transformed scale. Use \n        `set_out_sample_residuals()` method to set values.\n    creation_date : str\n        Date of creation.\n    is_fitted : bool\n        Tag to identify if the regressor has been fitted (trained).\n    fit_date : str\n        Date of last fit.\n    skforecast_version : str\n        Version of skforecast library used to create the forecaster.\n    python_version : str\n        Version of python used to create the forecaster.\n    n_jobs : int, 'auto'\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_fit_forecaster.\n    forecaster_id : str, int\n        Name used as an identifier of the forecaster.\n    binner : Ignored\n        Not used, present here for API consistency by convention.\n    binner_intervals_ : Ignored\n        Not used, present here for API consistency by convention.\n    binner_kwargs : Ignored\n        Not used, present here for API consistency by convention.\n    in_sample_residuals_by_bin_ : Ignored\n        Not used, present here for API consistency by convention.\n    out_sample_residuals_by_bin_ : Ignored\n        Not used, present here for API consistency by convention.\n\n    Notes\n    -----\n    A separate model is created for each forecasting time step. It is important to\n    note that all models share the same parameter and hyperparameter configuration.\n     \n    \"\"\"\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Information displayed when a ForecasterDirect object is printed.\n        \"\"\"\n        params, _, _, exog_names_in_, _ = self._preprocess_repr(regressor=self.regressor, exog_names_in_=self.exog_names_in_)\n        params = self._format_text_repr(params)\n        exog_names_in_ = self._format_text_repr(exog_names_in_)\n        info = f'{'=' * len(type(self).__name__)} \\n{type(self).__name__} \\n{'=' * len(type(self).__name__)} \\nRegressor: {type(self.regressor).__name__} \\nLags: {self.lags} \\nWindow features: {self.window_features_names} \\nWindow size: {self.window_size} \\nMaximum steps to predict: {self.steps} \\nExogenous included: {self.exog_in_} \\nExogenous names: {exog_names_in_} \\nTransformer for y: {self.transformer_y} \\nTransformer for exog: {self.transformer_exog} \\nWeight function included: {(True if self.weight_func is not None else False)} \\nDifferentiation order: {self.differentiation} \\nTraining range: {(self.training_range_.to_list() if self.is_fitted else None)} \\nTraining index type: {(str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None)} \\nTraining index frequency: {(self.index_freq_ if self.is_fitted else None)} \\nRegressor parameters: {params} \\nfit_kwargs: {self.fit_kwargs} \\nCreation date: {self.creation_date} \\nLast fit date: {self.fit_date} \\nSkforecast version: {self.skforecast_version} \\nPython version: {self.python_version} \\nForecaster id: {self.forecaster_id} \\n'\n        return info\n\n    def _repr_html_(self):\n        \"\"\"\n        HTML representation of the object.\n        The \"General Information\" section is expanded by default.\n        \"\"\"\n        params, _, _, exog_names_in_, _ = self._preprocess_repr(regressor=self.regressor, exog_names_in_=self.exog_names_in_)\n        style, unique_id = self._get_style_repr_html(self.is_fitted)\n        content = f'\\n        <div class=\"container-{unique_id}\">\\n            <h2>{type(self).__name__}</h2>\\n            <details open>\\n                <summary>General Information</summary>\\n                <ul>\\n                    <li><strong>Regressor:</strong> {type(self.regressor).__name__}</li>\\n                    <li><strong>Lags:</strong> {self.lags}</li>\\n                    <li><strong>Window features:</strong> {self.window_features_names}</li>\\n                    <li><strong>Window size:</strong> {self.window_size}</li>\\n                    <li><strong>Maximum steps to predict:</strong> {self.steps}</li>\\n                    <li><strong>Exogenous included:</strong> {self.exog_in_}</li>\\n                    <li><strong>Weight function included:</strong> {self.weight_func is not None}</li>\\n                    <li><strong>Differentiation order:</strong> {self.differentiation}</li>\\n                    <li><strong>Creation date:</strong> {self.creation_date}</li>\\n                    <li><strong>Last fit date:</strong> {self.fit_date}</li>\\n                    <li><strong>Skforecast version:</strong> {self.skforecast_version}</li>\\n                    <li><strong>Python version:</strong> {self.python_version}</li>\\n                    <li><strong>Forecaster id:</strong> {self.forecaster_id}</li>\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Exogenous Variables</summary>\\n                <ul>\\n                    {exog_names_in_}\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Data Transformations</summary>\\n                <ul>\\n                    <li><strong>Transformer for y:</strong> {self.transformer_y}</li>\\n                    <li><strong>Transformer for exog:</strong> {self.transformer_exog}</li>\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Training Information</summary>\\n                <ul>\\n                    <li><strong>Training range:</strong> {(self.training_range_.to_list() if self.is_fitted else 'Not fitted')}</li>\\n                    <li><strong>Training index type:</strong> {(str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted')}</li>\\n                    <li><strong>Training index frequency:</strong> {(self.index_freq_ if self.is_fitted else 'Not fitted')}</li>\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Regressor Parameters</summary>\\n                <ul>\\n                    {params}\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Fit Kwargs</summary>\\n                <ul>\\n                    {self.fit_kwargs}\\n                </ul>\\n            </details>\\n            <p>\\n                <a href=\"https://skforecast.org/{skforecast.__version__}/api/forecasterdirect.html\">&#128712 <strong>API Reference</strong></a>\\n                &nbsp;&nbsp;\\n                <a href=\"https://skforecast.org/{skforecast.__version__}/user_guides/direct-multi-step-forecasting.html\">&#128462 <strong>User Guide</strong></a>\\n            </p>\\n        </div>\\n        '\n        return style + content\n\n    def _create_lags(self, y: np.ndarray, X_as_pandas: bool=False, train_index: Optional[pd.Index]=None) -> Tuple[Optional[Union[np.ndarray, pd.DataFrame]], np.ndarray]:\n        \"\"\"\n        Create the lagged values and their target variable from a time series.\n        \n        Note that the returned matrix `X_data` contains the lag 1 in the first \n        column, the lag 2 in the in the second column and so on.\n        \n        Parameters\n        ----------\n        y : numpy ndarray\n            Training time series values.\n        X_as_pandas : bool, default `False`\n            If `True`, the returned matrix `X_data` is a pandas DataFrame.\n        train_index : pandas Index, default `None`\n            Index of the training data. It is used to create the pandas DataFrame\n            `X_data` when `X_as_pandas` is `True`.\n\n        Returns\n        -------\n        X_data : numpy ndarray, pandas DataFrame, None\n            Lagged values (predictors).\n        y_data : numpy ndarray\n            Values of the time series related to each row of `X_data`.\n        \n        \"\"\"\n        n_rows = len(y) - self.window_size - (self.steps - 1)\n        X_data = None\n        if self.lags is not None:\n            X_data = np.full(shape=(n_rows, len(self.lags)), fill_value=np.nan, order='F', dtype=float)\n            for i, lag in enumerate(self.lags):\n                X_data[:, i] = y[self.window_size - lag:-(lag + self.steps - 1)]\n            if X_as_pandas:\n                X_data = pd.DataFrame(data=X_data, columns=self.lags_names, index=train_index)\n        y_data = np.full(shape=(n_rows, self.steps), fill_value=np.nan, order='F', dtype=float)\n        for step in range(self.steps):\n            y_data[:, step] = y[self.window_size + step:self.window_size + step + n_rows]\n        return (X_data, y_data)\n\n    def _create_window_features(self, y: pd.Series, train_index: pd.Index, X_as_pandas: bool=False) -> Tuple[list, list]:\n        \"\"\"\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        train_index : pandas Index\n            Index of the training data. It is used to create the pandas DataFrame\n            `X_train_window_features` when `X_as_pandas` is `True`.\n        X_as_pandas : bool, default `False`\n            If `True`, the returned matrix `X_train_window_features` is a \n            pandas DataFrame.\n\n        Returns\n        -------\n        X_train_window_features : list\n            List of numpy ndarrays or pandas DataFrames with the window features.\n        X_train_window_features_names_out_ : list\n            Names of the window features.\n        \n        \"\"\"\n        len_train_index = len(train_index)\n        X_train_window_features = []\n        X_train_window_features_names_out_ = []\n        for wf in self.window_features:\n            X_train_wf = wf.transform_batch(y)\n            if not isinstance(X_train_wf, pd.DataFrame):\n                raise TypeError(f'The method `transform_batch` of {type(wf).__name__} must return a pandas DataFrame.')\n            X_train_wf = X_train_wf.iloc[-len_train_index:]\n            if not len(X_train_wf) == len_train_index:\n                raise ValueError(f'The method `transform_batch` of {type(wf).__name__} must return a DataFrame with the same number of rows as the input time series - (`window_size` + (`steps` - 1)): {len_train_index}.')\n            X_train_wf.index = train_index\n            X_train_window_features_names_out_.extend(X_train_wf.columns)\n            if not X_as_pandas:\n                X_train_wf = X_train_wf.to_numpy()\n            X_train_window_features.append(X_train_wf)\n        return (X_train_window_features, X_train_window_features_names_out_)\n\n    def _create_train_X_y(self, y: pd.Series, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, dict, list, list, list, dict]:\n        \"\"\"\n        Create training matrices from univariate time series and exogenous\n        variables. The resulting matrices contain the target variable and \n        predictors needed to train all the regressors (one per step).\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors) for each step. Note that the index \n            corresponds to that of the last step. It is updated for the corresponding \n            step in the `filter_train_X_y_for_step` method.\n        y_train : dict\n            Values of the time series related to each row of `X_train` for each \n            step in the form {step: y_step_[i]}.\n        exog_names_in_ : list\n            Names of the exogenous variables used during training.\n        X_train_exog_names_out_ : list\n            Names of the exogenous variables included in the matrix `X_train` created\n            internally for training. It can be different from `exog_names_in_` if\n            some exogenous variables are transformed during the training process.\n        X_train_features_names_out_ : list\n            Names of the columns of the matrix created internally for training.\n        exog_dtypes_in_ : dict\n            Type of each exogenous variable/s used in training. If `transformer_exog` \n            is used, the dtypes are calculated before the transformation.\n        \n        \"\"\"\n        check_y(y=y)\n        y = input_to_frame(data=y, input_name='y')\n        if len(y) < self.window_size + self.steps:\n            raise ValueError(f'Minimum length of `y` for training this forecaster is {self.window_size + self.steps}. Reduce the number of predicted steps, {self.steps}, or the maximum window_size, {self.window_size}, if no more data is available.\\n    Length `y`: {len(y)}.\\n    Max step : {self.steps}.\\n    Max window size: {self.window_size}.\\n    Lags window size: {self.max_lag}.\\n    Window features window size: {self.max_size_window_features}.')\n        fit_transformer = False if self.is_fitted else True\n        y = transform_dataframe(df=y, transformer=self.transformer_y, fit=fit_transformer, inverse_transform=False)\n        y_values, y_index = preprocess_y(y=y)\n        if self.differentiation is not None:\n            if not self.is_fitted:\n                y_values = self.differentiator.fit_transform(y_values)\n            else:\n                differentiator = copy(self.differentiator)\n                y_values = differentiator.fit_transform(y_values)\n        exog_names_in_ = None\n        exog_dtypes_in_ = None\n        categorical_features = False\n        if exog is not None:\n            check_exog(exog=exog, allow_nan=True)\n            exog = input_to_frame(data=exog, input_name='exog')\n            y_index_no_ws = y_index[self.window_size:]\n            len_y = len(y_values)\n            len_y_no_ws = len_y - self.window_size\n            len_exog = len(exog)\n            if not len_exog == len_y and (not len_exog == len_y_no_ws):\n                raise ValueError(f'Length of `exog` must be equal to the length of `y` (if index is fully aligned) or length of `y` - `window_size` (if `exog` starts after the first `window_size` values).\\n    `exog`              : ({exog.index[0]} -- {exog.index[-1]})  (n={len_exog})\\n    `y`                 : ({y.index[0]} -- {y.index[-1]})  (n={len_y})\\n    `y` - `window_size` : ({y_index_no_ws[0]} -- {y_index_no_ws[-1]})  (n={len_y_no_ws})')\n            self.exog_in_ = True\n            exog_names_in_ = exog.columns.to_list()\n            exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n            exog = transform_dataframe(df=exog, transformer=self.transformer_exog, fit=fit_transformer, inverse_transform=False)\n            check_exog_dtypes(exog, call_check_exog=True)\n            categorical_features = exog.select_dtypes(include=np.number).shape[1] != exog.shape[1]\n            _, exog_index = preprocess_exog(exog=exog, return_values=False)\n            if len_exog == len_y:\n                if not (exog_index == y_index).all():\n                    raise ValueError('When `exog` has the same length as `y`, the index of `exog` must be aligned with the index of `y` to ensure the correct alignment of values.')\n                exog = exog.iloc[self.window_size:,]\n            elif not (exog_index == y_index_no_ws).all():\n                raise ValueError(\"When `exog` doesn't contain the first `window_size` observations, the index of `exog` must be aligned with the index of `y` minus the first `window_size` observations to ensure the correct alignment of values.\")\n        X_train = []\n        X_train_features_names_out_ = []\n        train_index = y_index[self.window_size + (self.steps - 1):]\n        len_train_index = len(train_index)\n        X_as_pandas = True if categorical_features else False\n        X_train_lags, y_train = self._create_lags(y=y_values, X_as_pandas=X_as_pandas, train_index=train_index)\n        if X_train_lags is not None:\n            X_train.append(X_train_lags)\n            X_train_features_names_out_.extend(self.lags_names)\n        X_train_window_features_names_out_ = None\n        if self.window_features is not None:\n            n_diff = 0 if self.differentiation is None else self.differentiation\n            end_wf = None if self.steps == 1 else -(self.steps - 1)\n            y_window_features = pd.Series(y_values[n_diff:end_wf], index=y_index[n_diff:end_wf])\n            X_train_window_features, X_train_window_features_names_out_ = self._create_window_features(y=y_window_features, X_as_pandas=X_as_pandas, train_index=train_index)\n            X_train.extend(X_train_window_features)\n            X_train_features_names_out_.extend(X_train_window_features_names_out_)\n        self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n        X_train_exog_names_out_ = None\n        if exog is not None:\n            X_train_exog_names_out_ = exog.columns.to_list()\n            if X_as_pandas:\n                exog_direct, X_train_direct_exog_names_out_ = exog_to_direct(exog=exog, steps=self.steps)\n                exog_direct.index = train_index\n            else:\n                exog_direct, X_train_direct_exog_names_out_ = exog_to_direct_numpy(exog=exog, steps=self.steps)\n            self.X_train_direct_exog_names_out_ = X_train_direct_exog_names_out_\n            X_train_features_names_out_.extend(self.X_train_direct_exog_names_out_)\n            X_train.append(exog_direct)\n        if len(X_train) == 1:\n            X_train = X_train[0]\n        elif X_as_pandas:\n            X_train = pd.concat(X_train, axis=1)\n        else:\n            X_train = np.concatenate(X_train, axis=1)\n        if X_as_pandas:\n            X_train.index = train_index\n        else:\n            X_train = pd.DataFrame(data=X_train, index=train_index, columns=X_train_features_names_out_)\n        y_train = {step: pd.Series(data=y_train[:, step - 1], index=y_index[self.window_size + step - 1:][:len_train_index], name=f'y_step_{step}') for step in range(1, self.steps + 1)}\n        return (X_train, y_train, exog_names_in_, X_train_exog_names_out_, X_train_features_names_out_, exog_dtypes_in_)\n\n    def create_train_X_y(self, y: pd.Series, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, dict]:\n        \"\"\"\n        Create training matrices from univariate time series and exogenous\n        variables. The resulting matrices contain the target variable and \n        predictors needed to train all the regressors (one per step).\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors) for each step. Note that the index \n            corresponds to that of the last step. It is updated for the corresponding \n            step in the `filter_train_X_y_for_step` method.\n        y_train : dict\n            Values of the time series related to each row of `X_train` for each \n            step in the form {step: y_step_[i]}.\n        \n        \"\"\"\n        output = self._create_train_X_y(y=y, exog=exog)\n        X_train = output[0]\n        y_train = output[1]\n        return (X_train, y_train)\n\n    def filter_train_X_y_for_step(self, step: int, X_train: pd.DataFrame, y_train: dict, remove_suffix: bool=False) -> Tuple[pd.DataFrame, pd.Series]:\n        \"\"\"\n        Select the columns needed to train a forecaster for a specific step.  \n        The input matrices should be created using `create_train_X_y` method. \n        This method updates the index of `X_train` to the corresponding one \n        according to `y_train`. If `remove_suffix=True` the suffix \"_step_i\" \n        will be removed from the column names. \n\n        Parameters\n        ----------\n        step : int\n            Step for which columns must be selected selected. Starts at 1.\n        X_train : pandas DataFrame\n            Dataframe created with the `create_train_X_y` method, first return.\n        y_train : dict\n            Dict created with the `create_train_X_y` method, second return.\n        remove_suffix : bool, default `False`\n            If True, suffix \"_step_i\" is removed from the column names.\n\n        Returns\n        -------\n        X_train_step : pandas DataFrame\n            Training values (predictors) for the selected step.\n        y_train_step : pandas Series\n            Values of the time series related to each row of `X_train`.\n\n        \"\"\"\n        if step < 1 or step > self.steps:\n            raise ValueError(f'Invalid value `step`. For this forecaster, minimum value is 1 and the maximum step is {self.steps}.')\n        y_train_step = y_train[step]\n        if not self.exog_in_:\n            X_train_step = X_train\n        else:\n            n_lags = len(self.lags) if self.lags is not None else 0\n            n_window_features = len(self.X_train_window_features_names_out_) if self.window_features is not None else 0\n            idx_columns_autoreg = np.arange(n_lags + n_window_features)\n            n_exog = len(self.X_train_direct_exog_names_out_) / self.steps\n            idx_columns_exog = np.arange((step - 1) * n_exog, step * n_exog) + idx_columns_autoreg[-1] + 1\n            idx_columns = np.concatenate((idx_columns_autoreg, idx_columns_exog))\n            X_train_step = X_train.iloc[:, idx_columns]\n        X_train_step.index = y_train_step.index\n        if remove_suffix:\n            X_train_step.columns = [col_name.replace(f'_step_{step}', '') for col_name in X_train_step.columns]\n            y_train_step.name = y_train_step.name.replace(f'_step_{step}', '')\n        return (X_train_step, y_train_step)\n\n    def create_sample_weights(self, X_train: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Crate weights for each observation according to the forecaster's attribute\n        `weight_func`.\n\n        Parameters\n        ----------\n        X_train : pandas DataFrame\n            Dataframe created with `create_train_X_y` and `filter_train_X_y_for_step`\n            methods, first return.\n\n        Returns\n        -------\n        sample_weight : numpy ndarray\n            Weights to use in `fit` method.\n\n        \"\"\"\n        sample_weight = None\n        if self.weight_func is not None:\n            sample_weight = self.weight_func(X_train.index)\n        if sample_weight is not None:\n            if np.isnan(sample_weight).any():\n                raise ValueError('The resulting `sample_weight` cannot have NaN values.')\n            if np.any(sample_weight < 0):\n                raise ValueError('The resulting `sample_weight` cannot have negative values.')\n            if np.sum(sample_weight) == 0:\n                raise ValueError('The resulting `sample_weight` cannot be normalized because the sum of the weights is zero.')\n        return sample_weight\n\n    def fit(self, y: pd.Series, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, store_last_window: bool=True, store_in_sample_residuals: bool=True) -> None:\n        \"\"\"\n        Training Forecaster.\n\n        Additional arguments to be passed to the `fit` method of the regressor \n        can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned so\n            that y[i] is regressed on exog[i].\n        store_last_window : bool, default `True`\n            Whether or not to store the last window (`last_window_`) of training data.\n        store_in_sample_residuals : bool, default `True`\n            If `True`, in-sample residuals will be stored in the forecaster object\n            after fitting (`in_sample_residuals_` attribute).\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.exog_in_ = False\n        self.exog_names_in_ = None\n        self.exog_type_in_ = None\n        self.exog_dtypes_in_ = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_ = None\n        self.X_train_direct_exog_names_out_ = None\n        self.X_train_features_names_out_ = None\n        self.in_sample_residuals_ = {step: None for step in range(1, self.steps + 1)}\n        self.is_fitted = False\n        self.fit_date = None\n        X_train, y_train, exog_names_in_, X_train_exog_names_out_, X_train_features_names_out_, exog_dtypes_in_ = self._create_train_X_y(y=y, exog=exog)\n\n        def fit_forecaster(regressor, X_train, y_train, step, store_in_sample_residuals):\n            \"\"\"\n            Auxiliary function to fit each of the forecaster's regressors in parallel.\n\n            Parameters\n            ----------\n            regressor : object\n                Regressor to be fitted.\n            X_train : pandas DataFrame\n                Dataframe created with the `create_train_X_y` method, first return.\n            y_train : dict\n                Dict created with the `create_train_X_y` method, second return.\n            step : int\n                Step of the forecaster to be fitted.\n            store_in_sample_residuals : bool\n                If `True`, in-sample residuals will be stored in the forecaster object\n                after fitting (`in_sample_residuals_` attribute).\n            \n            Returns\n            -------\n            Tuple with the step, fitted regressor and in-sample residuals.\n\n            \"\"\"\n            X_train_step, y_train_step = self.filter_train_X_y_for_step(step=step, X_train=X_train, y_train=y_train, remove_suffix=True)\n            sample_weight = self.create_sample_weights(X_train=X_train_step)\n            if sample_weight is not None:\n                regressor.fit(X=X_train_step, y=y_train_step, sample_weight=sample_weight, **self.fit_kwargs)\n            else:\n                regressor.fit(X=X_train_step, y=y_train_step, **self.fit_kwargs)\n            if store_in_sample_residuals:\n                residuals = (y_train_step - regressor.predict(X_train_step)).to_numpy()\n                if len(residuals) > 1000:\n                    rng = np.random.default_rng(seed=123)\n                    residuals = rng.choice(a=residuals, size=1000, replace=False)\n            else:\n                residuals = None\n            return (step, regressor, residuals)\n        results_fit = Parallel(n_jobs=self.n_jobs)((delayed(fit_forecaster)(regressor=copy(self.regressor), X_train=X_train, y_train=y_train, step=step, store_in_sample_residuals=store_in_sample_residuals) for step in range(1, self.steps + 1)))\n        self.regressors_ = {step: regressor for step, regressor, _ in results_fit}\n        if store_in_sample_residuals:\n            self.in_sample_residuals_ = {step: residuals for step, _, residuals in results_fit}\n        self.X_train_features_names_out_ = X_train_features_names_out_\n        self.is_fitted = True\n        self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.training_range_ = preprocess_y(y=y, return_values=False)[1][[0, -1]]\n        self.index_type_ = type(X_train.index)\n        if isinstance(X_train.index, pd.DatetimeIndex):\n            self.index_freq_ = X_train.index.freqstr\n        else:\n            self.index_freq_ = X_train.index.step\n        if exog is not None:\n            self.exog_in_ = True\n            self.exog_type_in_ = type(exog)\n            self.exog_names_in_ = exog_names_in_\n            self.exog_dtypes_in_ = exog_dtypes_in_\n            self.X_train_exog_names_out_ = X_train_exog_names_out_\n        if store_last_window:\n            self.last_window_ = y.iloc[-self.window_size:].copy().to_frame(name=y.name if y.name is not None else 'y')\n\n    def _create_predict_inputs(self, steps: Optional[Union[int, list]]=None, last_window: Optional[Union[pd.Series, pd.DataFrame]]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, check_inputs: bool=True) -> Tuple[list, list, list, pd.Index]:\n        \"\"\"\n        Create the inputs needed for the prediction process.\n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n\n        Returns\n        -------\n        Xs : list\n            List of numpy arrays with the predictors for each step.\n        Xs_col_names : list\n            Names of the columns of the matrix created internally for prediction.\n        steps : list\n            Steps to predict.\n        prediction_index : pandas Index\n            Index of the predictions.\n        \n        \"\"\"\n        steps = prepare_steps_direct(max_step=self.steps, steps=steps)\n        if last_window is None:\n            last_window = self.last_window_\n        if check_inputs:\n            check_predict_input(forecaster_name=type(self).__name__, steps=steps, is_fitted=self.is_fitted, exog_in_=self.exog_in_, index_type_=self.index_type_, index_freq_=self.index_freq_, window_size=self.window_size, last_window=last_window, exog=exog, exog_type_in_=self.exog_type_in_, exog_names_in_=self.exog_names_in_, interval=None, max_steps=self.steps)\n        last_window = last_window.iloc[-self.window_size:].copy()\n        last_window_values, last_window_index = preprocess_last_window(last_window=last_window)\n        last_window_values = transform_numpy(array=last_window_values, transformer=self.transformer_y, fit=False, inverse_transform=False)\n        if self.differentiation is not None:\n            last_window_values = self.differentiator.fit_transform(last_window_values)\n        X_autoreg = []\n        Xs_col_names = []\n        if self.lags is not None:\n            X_lags = last_window_values[-self.lags]\n            X_autoreg.append(X_lags)\n            Xs_col_names.extend(self.lags_names)\n        if self.window_features is not None:\n            n_diff = 0 if self.differentiation is None else self.differentiation\n            X_window_features = np.concatenate([wf.transform(last_window_values[n_diff:]) for wf in self.window_features])\n            X_autoreg.append(X_window_features)\n            Xs_col_names.extend(self.X_train_window_features_names_out_)\n        X_autoreg = np.concatenate(X_autoreg).reshape(1, -1)\n        if exog is not None:\n            exog = input_to_frame(data=exog, input_name='exog')\n            exog = exog.loc[:, self.exog_names_in_]\n            exog = transform_dataframe(df=exog, transformer=self.transformer_exog, fit=False, inverse_transform=False)\n            check_exog_dtypes(exog=exog)\n            exog_values, _ = exog_to_direct_numpy(exog=exog.to_numpy()[:max(steps)], steps=max(steps))\n            exog_values = exog_values[0]\n            n_exog = exog.shape[1]\n            Xs = [np.concatenate([X_autoreg, exog_values[(step - 1) * n_exog:step * n_exog].reshape(1, -1)], axis=1) for step in steps]\n            Xs_col_names = Xs_col_names + exog.columns.to_list()\n        else:\n            Xs = [X_autoreg] * len(steps)\n        prediction_index = expand_index(index=last_window_index, steps=max(steps))[np.array(steps) - 1]\n        if isinstance(last_window_index, pd.DatetimeIndex) and np.array_equal(steps, np.arange(min(steps), max(steps) + 1)):\n            prediction_index.freq = last_window_index.freq\n        return (Xs, Xs_col_names, steps, prediction_index)\n\n    def create_predict_X(self, steps: Optional[Union[int, list]]=None, last_window: Optional[Union[pd.Series, pd.DataFrame]]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> pd.DataFrame:\n        \"\"\"\n        Create the predictors needed to predict `steps` ahead.\n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n\n        Returns\n        -------\n        X_predict : pandas DataFrame\n            Pandas DataFrame with the predictors for each step. The index \n            is the same as the prediction index.\n        \n        \"\"\"\n        Xs, Xs_col_names, steps, prediction_index = self._create_predict_inputs(steps=steps, last_window=last_window, exog=exog)\n        X_predict = pd.DataFrame(data=np.concatenate(Xs, axis=0), columns=Xs_col_names, index=prediction_index)\n        if self.transformer_y is not None or self.differentiation is not None:\n            warnings.warn('The output matrix is in the transformed scale due to the inclusion of transformations or differentiation in the Forecaster. As a result, any predictions generated using this matrix will also be in the transformed scale. Please refer to the documentation for more details: https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html', DataTransformationWarning)\n        return X_predict\n\n    def predict(self, steps: Optional[Union[int, list]]=None, last_window: Optional[Union[pd.Series, pd.DataFrame]]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, check_inputs: bool=True) -> pd.Series:\n        \"\"\"\n        Predict n steps ahead.\n\n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n\n        Returns\n        -------\n        predictions : pandas Series\n            Predicted values.\n\n        \"\"\"\n        Xs, _, steps, prediction_index = self._create_predict_inputs(steps=steps, last_window=last_window, exog=exog, check_inputs=check_inputs)\n        regressors = [self.regressors_[step] for step in steps]\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', message='X does not have valid feature names', category=UserWarning)\n            predictions = np.array([regressor.predict(X).ravel()[0] for regressor, X in zip(regressors, Xs)])\n        if self.differentiation is not None:\n            predictions = self.differentiator.inverse_transform_next_window(predictions)\n        predictions = transform_numpy(array=predictions, transformer=self.transformer_y, fit=False, inverse_transform=True)\n        predictions = pd.Series(data=predictions, index=prediction_index, name='pred')\n        return predictions\n\n    def predict_bootstrapping(self, steps: Optional[Union[int, list]]=None, last_window: Optional[pd.Series]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: Any=None) -> pd.DataFrame:\n        \"\"\"\n        Generate multiple forecasting predictions using a bootstrapping process. \n        By sampling from a collection of past observed errors (the residuals),\n        each iteration of bootstrapping generates a different set of predictions. \n        See the Notes section for more information. \n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        boot_predictions : pandas DataFrame\n            Predictions generated by bootstrapping.\n            Shape: (steps, n_boot)\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n        Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n        \"\"\"\n        if self.is_fitted:\n            steps = prepare_steps_direct(steps=steps, max_step=self.steps)\n            if use_in_sample_residuals:\n                if not set(steps).issubset(set(self.in_sample_residuals_.keys())):\n                    raise ValueError(f'Not `forecaster.in_sample_residuals_` for steps: {set(steps) - set(self.in_sample_residuals_.keys())}.')\n                residuals = self.in_sample_residuals_\n            else:\n                if self.out_sample_residuals_ is None:\n                    raise ValueError('`forecaster.out_sample_residuals_` is `None`. Use `use_in_sample_residuals=True` or the `set_out_sample_residuals()` method before predicting.')\n                elif not set(steps).issubset(set(self.out_sample_residuals_.keys())):\n                    raise ValueError(f'No `forecaster.out_sample_residuals_` for steps: {set(steps) - set(self.out_sample_residuals_.keys())}. Use method `set_out_sample_residuals()`.')\n                residuals = self.out_sample_residuals_\n            check_residuals = 'forecaster.in_sample_residuals_' if use_in_sample_residuals else 'forecaster.out_sample_residuals_'\n            for step in steps:\n                if residuals[step] is None:\n                    raise ValueError(f'forecaster residuals for step {step} are `None`. Check {check_residuals}.')\n                elif any((x is None or np.isnan(x) for x in residuals[step])):\n                    raise ValueError(f'forecaster residuals for step {step} contains `None` values. Check {check_residuals}.')\n        Xs, _, steps, prediction_index = self._create_predict_inputs(steps=steps, last_window=last_window, exog=exog)\n        regressors = [self.regressors_[step] for step in steps]\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', message='X does not have valid feature names', category=UserWarning)\n            predictions = np.array([regressor.predict(X).ravel()[0] for regressor, X in zip(regressors, Xs)])\n        boot_predictions = np.tile(predictions, (n_boot, 1)).T\n        boot_columns = [f'pred_boot_{i}' for i in range(n_boot)]\n        rng = np.random.default_rng(seed=random_state)\n        for i, step in enumerate(steps):\n            sampled_residuals = residuals[step][rng.integers(low=0, high=len(residuals[step]), size=n_boot)]\n            boot_predictions[i, :] = boot_predictions[i, :] + sampled_residuals\n        if self.differentiation is not None:\n            boot_predictions = self.differentiator.inverse_transform_next_window(boot_predictions)\n        if self.transformer_y:\n            boot_predictions = np.apply_along_axis(func1d=transform_numpy, axis=0, arr=boot_predictions, transformer=self.transformer_y, fit=False, inverse_transform=True)\n        boot_predictions = pd.DataFrame(data=boot_predictions, index=prediction_index, columns=boot_columns)\n        return boot_predictions\n\n    def predict_interval(self, steps: Optional[Union[int, list]]=None, last_window: Optional[pd.Series]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, interval: list=[5, 95], n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: Any=None) -> pd.DataFrame:\n        \"\"\"\n        Bootstrapping based predicted intervals.\n        Both predictions and intervals are returned.\n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        interval : list, default `[5, 95]`\n            Confidence of the prediction interval estimated. Sequence of \n            percentiles to compute, which must be between 0 and 100 inclusive. \n            For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Values predicted by the forecaster and their estimated interval.\n\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n        check_interval(interval=interval)\n        boot_predictions = self.predict_bootstrapping(steps=steps, last_window=last_window, exog=exog, n_boot=n_boot, random_state=random_state, use_in_sample_residuals=use_in_sample_residuals)\n        predictions = self.predict(steps=steps, last_window=last_window, exog=exog, check_inputs=False)\n        interval = np.array(interval) / 100\n        predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n        predictions_interval.columns = ['lower_bound', 'upper_bound']\n        predictions = pd.concat((predictions, predictions_interval), axis=1)\n        return predictions\n\n    def predict_quantiles(self, steps: Optional[Union[int, list]]=None, last_window: Optional[pd.Series]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, quantiles: list=[0.05, 0.5, 0.95], n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: Any=None) -> pd.DataFrame:\n        \"\"\"\n        Bootstrapping based predicted quantiles.\n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        quantiles : list, default `[0.05, 0.5, 0.95]`\n            Sequence of quantiles to compute, which must be between 0 and 1 \n            inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n            `quantiles = [0.05, 0.5, 0.95]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate quantiles.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot quantiles are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create prediction quantiles. If `False`, out of\n            sample residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Quantiles predicted by the forecaster.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n        check_interval(quantiles=quantiles)\n        boot_predictions = self.predict_bootstrapping(steps=steps, last_window=last_window, exog=exog, n_boot=n_boot, random_state=random_state, use_in_sample_residuals=use_in_sample_residuals)\n        predictions = boot_predictions.quantile(q=quantiles, axis=1).transpose()\n        predictions.columns = [f'q_{q}' for q in quantiles]\n        return predictions\n\n    def predict_dist(self, distribution: object, steps: Optional[Union[int, list]]=None, last_window: Optional[pd.Series]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: Any=None) -> pd.DataFrame:\n        \"\"\"\n        Fit a given probability distribution for each step. After generating \n        multiple forecasting predictions through a bootstrapping process, each \n        step is fitted to the given distribution.\n        \n        Parameters\n        ----------\n        distribution : Object\n            A distribution object from scipy.stats.\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Distribution parameters estimated for each step.\n\n        \"\"\"\n        boot_samples = self.predict_bootstrapping(steps=steps, last_window=last_window, exog=exog, n_boot=n_boot, random_state=random_state, use_in_sample_residuals=use_in_sample_residuals)\n        param_names = [p for p in inspect.signature(distribution._pdf).parameters if not p == 'x'] + ['loc', 'scale']\n        param_values = np.apply_along_axis(lambda x: distribution.fit(x), axis=1, arr=boot_samples)\n        predictions = pd.DataFrame(data=param_values, columns=param_names, index=boot_samples.index)\n        return predictions\n\n    def set_params(self, params: dict) -> None:\n        \"\"\"\n        Set new values to the parameters of the scikit learn model stored in the\n        forecaster. It is important to note that all models share the same \n        configuration of parameters and hyperparameters.\n        \n        Parameters\n        ----------\n        params : dict\n            Parameters values.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        self.regressor = clone(self.regressor)\n        self.regressor.set_params(**params)\n        self.regressors_ = {step: clone(self.regressor) for step in range(1, self.steps + 1)}\n\n    def set_fit_kwargs(self, fit_kwargs: dict) -> None:\n        \"\"\"\n        Set new values for the additional keyword arguments passed to the `fit` \n        method of the regressor.\n        \n        Parameters\n        ----------\n        fit_kwargs : dict\n            Dict of the form {\"argument\": new_value}.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n\n    def set_lags(self, lags: Optional[Union[int, list, np.ndarray, range]]=None) -> None:\n        \"\"\"\n        Set new value to the attribute `lags`. Attributes `lags_names`, \n        `max_lag` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        lags : int, list, numpy ndarray, range, default `None`\n            Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. \n        \n            - `int`: include lags from 1 to `lags` (included).\n            - `list`, `1d numpy ndarray` or `range`: include only lags present in \n            `lags`, all elements must be int.\n            - `None`: no lags are included as predictors. \n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        if self.window_features is None and lags is None:\n            raise ValueError('At least one of the arguments `lags` or `window_features` must be different from None. This is required to create the predictors used in training the forecaster.')\n        self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\n        self.window_size = max([ws for ws in [self.max_lag, self.max_size_window_features] if ws is not None])\n        if self.differentiation is not None:\n            self.window_size += self.differentiation\n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_window_features(self, window_features: Optional[Union[object, list]]=None) -> None:\n        \"\"\"\n        Set new value to the attribute `window_features`. Attributes \n        `max_size_window_features`, `window_features_names`, \n        `window_features_class_names` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        window_features : object, list, default `None`\n            Instance or list of instances used to create window features. Window features\n            are created from the original time series and are included as predictors.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        if window_features is None and self.lags is None:\n            raise ValueError('At least one of the arguments `lags` or `window_features` must be different from None. This is required to create the predictors used in training the forecaster.')\n        self.window_features, self.window_features_names, self.max_size_window_features = initialize_window_features(window_features)\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [type(wf).__name__ for wf in self.window_features]\n        self.window_size = max([ws for ws in [self.max_lag, self.max_size_window_features] if ws is not None])\n        if self.differentiation is not None:\n            self.window_size += self.differentiation\n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_out_sample_residuals(self, y_true: dict, y_pred: dict, append: bool=False, random_state: int=36987) -> None:\n        \"\"\"\n        Set new values to the attribute `out_sample_residuals_`. Out of sample\n        residuals are meant to be calculated using observations that did not\n        participate in the training process. `y_true` and `y_pred` are expected\n        to be in the original scale of the time series. Residuals are calculated\n        as `y_true` - `y_pred`, after applying the necessary transformations and\n        differentiations if the forecaster includes them (`self.transformer_y`\n        and `self.differentiation`).\n\n        A total of 10_000 residuals are stored in the attribute `out_sample_residuals_`.\n        If the number of residuals is greater than 10_000, a random sample of\n        10_000 residuals is stored.\n        \n        Parameters\n        ----------\n        y_true : dict\n            Dictionary of numpy ndarrays or pandas Series with the true values of\n            the time series for each model in the form {step: y_true}.\n        y_pred : dict\n            Dictionary of numpy ndarrays or pandas Series with the predicted values\n            of the time series for each model in the form {step: y_pred}.\n        append : bool, default `False`\n            If `True`, new residuals are added to the once already stored in the\n            attribute `out_sample_residuals_`. If after appending the new residuals,\n            the limit of 10_000 samples is exceeded, a random sample of 10_000 is\n            kept.\n        random_state : int, default `36987`\n            Sets a seed to the random sampling for reproducible output.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n        if not self.is_fitted:\n            raise NotFittedError('This forecaster is not fitted yet. Call `fit` with appropriate arguments before using `set_out_sample_residuals()`.')\n        if not isinstance(y_true, dict):\n            raise TypeError(f'`y_true` must be a dictionary of numpy ndarrays or pandas Series. Got {type(y_true)}.')\n        if not isinstance(y_pred, dict):\n            raise TypeError(f'`y_pred` must be a dictionary of numpy ndarrays or pandas Series. Got {type(y_pred)}.')\n        if not set(y_true.keys()) == set(y_pred.keys()):\n            raise ValueError(f'`y_true` and `y_pred` must have the same keys. Got {set(y_true.keys())} and {set(y_pred.keys())}.')\n        for k in y_true.keys():\n            if not isinstance(y_true[k], (np.ndarray, pd.Series)):\n                raise TypeError(f'Values of `y_true` must be numpy ndarrays or pandas Series. Got {type(y_true[k])} for step {k}.')\n            if not isinstance(y_pred[k], (np.ndarray, pd.Series)):\n                raise TypeError(f'Values of `y_pred` must be numpy ndarrays or pandas Series. Got {type(y_pred[k])} for step {k}.')\n            if len(y_true[k]) != len(y_pred[k]):\n                raise ValueError(f'`y_true` and `y_pred` must have the same length. Got {len(y_true[k])} and {len(y_pred[k])} for step {k}.')\n            if isinstance(y_true[k], pd.Series) and isinstance(y_pred[k], pd.Series):\n                if not y_true[k].index.equals(y_pred[k].index):\n                    raise ValueError(f'When containing pandas Series, elements in `y_true` and `y_pred` must have the same index. Error in step {k}.')\n        if self.out_sample_residuals_ is None:\n            self.out_sample_residuals_ = {step: None for step in range(1, self.steps + 1)}\n        steps_to_update = set(range(1, self.steps + 1)).intersection(set(y_pred.keys()))\n        if not steps_to_update:\n            raise ValueError('Provided keys in `y_pred` and `y_true` do not match any step. Residuals cannot be updated.')\n        residuals = {}\n        rng = np.random.default_rng(seed=random_state)\n        y_true = y_true.copy()\n        y_pred = y_pred.copy()\n        if self.differentiation is not None:\n            differentiator = copy(self.differentiator)\n            differentiator.set_params(window_size=None)\n        for k in steps_to_update:\n            if isinstance(y_true[k], pd.Series):\n                y_true[k] = y_true[k].to_numpy()\n            if isinstance(y_pred[k], pd.Series):\n                y_pred[k] = y_pred[k].to_numpy()\n            if self.transformer_y:\n                y_true[k] = transform_numpy(array=y_true[k], transformer=self.transformer_y, fit=False, inverse_transform=False)\n                y_pred[k] = transform_numpy(array=y_pred[k], transformer=self.transformer_y, fit=False, inverse_transform=False)\n            if self.differentiation is not None:\n                y_true[k] = differentiator.fit_transform(y_true[k])[self.differentiation:]\n                y_pred[k] = differentiator.fit_transform(y_pred[k])[self.differentiation:]\n            residuals[k] = y_true[k] - y_pred[k]\n        for key, value in residuals.items():\n            if append and self.out_sample_residuals_[key] is not None:\n                value = np.concatenate((self.out_sample_residuals_[key], value))\n            if len(value) > 10000:\n                value = rng.choice(value, size=10000, replace=False)\n            self.out_sample_residuals_[key] = value\n\n    def get_feature_importances(self, step: int, sort_importance: bool=True) -> pd.DataFrame:\n        \"\"\"\n        Return feature importance of the model stored in the forecaster for a\n        specific step. Since a separate model is created for each forecast time\n        step, it is necessary to select the model from which retrieve information.\n        Only valid when regressor stores internally the feature importances in\n        the attribute `feature_importances_` or `coef_`. Otherwise, it returns  \n        `None`.\n\n        Parameters\n        ----------\n        step : int\n            Model from which retrieve information (a separate model is created \n            for each forecast time step). First step is 1.\n        sort_importance: bool, default `True`\n            If `True`, sorts the feature importances in descending order.\n\n        Returns\n        -------\n        feature_importances : pandas DataFrame\n            Feature importances associated with each predictor.\n        \n        \"\"\"\n        if not isinstance(step, int):\n            raise TypeError(f'`step` must be an integer. Got {type(step)}.')\n        if not self.is_fitted:\n            raise NotFittedError('This forecaster is not fitted yet. Call `fit` with appropriate arguments before using `get_feature_importances()`.')\n        if step < 1 or step > self.steps:\n            raise ValueError(f'The step must have a value from 1 to the maximum number of steps ({self.steps}). Got {step}.')\n        if isinstance(self.regressor, Pipeline):\n            estimator = self.regressors_[step][-1]\n        else:\n            estimator = self.regressors_[step]\n        n_lags = len(self.lags) if self.lags is not None else 0\n        n_window_features = len(self.window_features_names) if self.window_features is not None else 0\n        idx_columns_autoreg = np.arange(n_lags + n_window_features)\n        if not self.exog_in_:\n            idx_columns = idx_columns_autoreg\n        else:\n            n_exog = len(self.X_train_direct_exog_names_out_) / self.steps\n            idx_columns_exog = np.arange((step - 1) * n_exog, step * n_exog) + idx_columns_autoreg[-1] + 1\n            idx_columns = np.concatenate((idx_columns_autoreg, idx_columns_exog))\n        idx_columns = [int(x) for x in idx_columns]\n        feature_names = [self.X_train_features_names_out_[i].replace(f'_step_{step}', '') for i in idx_columns]\n        if hasattr(estimator, 'feature_importances_'):\n            feature_importances = estimator.feature_importances_\n        elif hasattr(estimator, 'coef_'):\n            feature_importances = estimator.coef_\n        else:\n            warnings.warn(f'Impossible to access feature importances for regressor of type {type(estimator)}. This method is only valid when the regressor stores internally the feature importances in the attribute `feature_importances_` or `coef_`.')\n            feature_importances = None\n        if feature_importances is not None:\n            feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importances})\n            if sort_importance:\n                feature_importances = feature_importances.sort_values(by='importance', ascending=False)\n        return feature_importances"
  },
  "call_tree": {
    "skforecast/model_selection/tests/tests_utils/test_calculate_metrics_one_step_ahead.py:test_calculate_metrics_one_step_ahead_when_ForecasterRecursive": {
      "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:__init__": {
        "skforecast/utils/utils.py:initialize_lags": {},
        "skforecast/utils/utils.py:initialize_window_features": {},
        "skforecast/preprocessing/preprocessing.py:QuantileBinner:__init__": {
          "skforecast/preprocessing/preprocessing.py:QuantileBinner:_validate_params": {}
        },
        "skforecast/preprocessing/preprocessing.py:TimeSeriesDifferentiator:__init__": {},
        "skforecast/utils/utils.py:initialize_weights": {},
        "skforecast/utils/utils.py:check_select_fit_kwargs": {}
      },
      "skforecast/metrics/metrics.py:add_y_train_argument": {},
      "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:_train_test_split_one_step_ahead": {
        "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:_create_train_X_y": {
          "skforecast/utils/utils.py:check_y": {},
          "skforecast/utils/utils.py:input_to_frame": {},
          "skforecast/utils/utils.py:transform_dataframe": {},
          "skforecast/utils/utils.py:preprocess_y": {},
          "skforecast/preprocessing/preprocessing.py:wrapper": {
            "skforecast/preprocessing/preprocessing.py:TimeSeriesDifferentiator:fit": {},
            "skforecast/preprocessing/preprocessing.py:TimeSeriesDifferentiator:transform": {}
          },
          "skforecast/utils/utils.py:check_exog": {},
          "skforecast/utils/utils.py:get_exog_dtypes": {},
          "skforecast/utils/utils.py:check_exog_dtypes": {
            "skforecast/utils/utils.py:check_exog": {}
          },
          "skforecast/utils/utils.py:preprocess_exog": {},
          "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:_create_lags": {}
        }
      },
      "skforecast/model_selection/_utils.py:_calculate_metrics_one_step_ahead": {
        "skforecast/preprocessing/preprocessing.py:wrapper": {
          "skforecast/preprocessing/preprocessing.py:TimeSeriesDifferentiator:inverse_transform_next_window": {},
          "skforecast/preprocessing/preprocessing.py:TimeSeriesDifferentiator:inverse_transform_training": {}
        },
        "skforecast/metrics/metrics.py:wrapper": {},
        "skforecast/metrics/metrics.py:mean_absolute_scaled_error": {}
      }
    },
    "skforecast/model_selection/tests/tests_utils/test_calculate_metrics_one_step_ahead.py:test_calculate_metrics_one_step_ahead_when_ForecasterDirect": {
      "skforecast/direct/_forecaster_direct.py:ForecasterDirect:__init__": {
        "skforecast/utils/utils.py:initialize_lags": {},
        "skforecast/utils/utils.py:initialize_window_features": {},
        "skforecast/utils/utils.py:initialize_weights": {},
        "skforecast/utils/utils.py:check_select_fit_kwargs": {},
        "skforecast/utils/utils.py:select_n_jobs_fit_forecaster": {}
      },
      "skforecast/metrics/metrics.py:add_y_train_argument": {},
      "skforecast/direct/_forecaster_direct.py:ForecasterDirect:_train_test_split_one_step_ahead": {
        "skforecast/direct/_forecaster_direct.py:ForecasterDirect:_create_train_X_y": {
          "skforecast/utils/utils.py:check_y": {},
          "skforecast/utils/utils.py:input_to_frame": {},
          "skforecast/utils/utils.py:transform_dataframe": {},
          "skforecast/utils/utils.py:preprocess_y": {},
          "skforecast/utils/utils.py:check_exog": {},
          "skforecast/utils/utils.py:get_exog_dtypes": {},
          "skforecast/utils/utils.py:check_exog_dtypes": {
            "skforecast/utils/utils.py:check_exog": {}
          },
          "skforecast/utils/utils.py:preprocess_exog": {},
          "skforecast/direct/_forecaster_direct.py:ForecasterDirect:_create_lags": {},
          "skforecast/utils/utils.py:exog_to_direct_numpy": {}
        }
      },
      "skforecast/model_selection/_utils.py:_calculate_metrics_one_step_ahead": {
        "skforecast/direct/_forecaster_direct.py:ForecasterDirect:filter_train_X_y_for_step": {},
        "skforecast/metrics/metrics.py:wrapper": {},
        "skforecast/metrics/metrics.py:mean_absolute_scaled_error": {}
      }
    }
  }
}