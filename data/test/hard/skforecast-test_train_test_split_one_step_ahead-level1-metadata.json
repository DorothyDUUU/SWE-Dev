{
  "dir_path": "/app/skforecast",
  "package_name": "skforecast",
  "sample_name": "skforecast-test_train_test_split_one_step_ahead",
  "src_dir": "skforecast/",
  "test_dir": "tests/",
  "test_file": "skforecast/direct/tests/tests_forecaster_direct_multivariate/test_train_test_split_one_step_ahead.py",
  "test_code": "# Unit test _train_test_split_one_step_ahead ForecasterDirectMultiVariate\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom skforecast.direct import ForecasterDirectMultiVariate\n\n\ndef test_train_test_split_one_step_ahead_when_y_is_series_and_exog_are_dataframe():\n    \"\"\"\n    Test the output of _train_test_split_one_step_ahead when series and exog are\n    pandas dataframes.\n    \"\"\"\n    series = pd.DataFrame(\n        {\n            \"series_1\": np.arange(15),\n            \"series_2\": np.arange(50, 65),\n        },\n        index=pd.date_range(\"2020-01-01\", periods=15),\n        dtype=float,\n    )\n    exog = pd.DataFrame(\n        {\n            \"exog_1\": np.arange(100, 115, dtype=float),\n            \"exog_2\": np.arange(1000, 1015, dtype=float),\n        },\n        index=pd.date_range(\"2020-01-01\", periods=15),\n    )\n\n    forecaster = ForecasterDirectMultiVariate(\n        LinearRegression(), lags=5, level=\"series_1\", steps=1, transformer_series=None\n    )\n\n    X_train, y_train, X_test, y_test, X_train_encoding, X_test_encoding = (\n        forecaster._train_test_split_one_step_ahead(\n            series=series, exog=exog, initial_train_size=10\n        )\n    )\n\n    expected_X_train = pd.DataFrame(\n        {\n            \"series_1_lag_1\": [4.0, 5.0, 6.0, 7.0, 8.0],\n            \"series_1_lag_2\": [3.0, 4.0, 5.0, 6.0, 7.0],\n            \"series_1_lag_3\": [2.0, 3.0, 4.0, 5.0, 6.0],\n            \"series_1_lag_4\": [1.0, 2.0, 3.0, 4.0, 5.0],\n            \"series_1_lag_5\": [0.0, 1.0, 2.0, 3.0, 4.0],\n            \"series_2_lag_1\": [54.0, 55.0, 56.0, 57.0, 58.0],\n            \"series_2_lag_2\": [53.0, 54.0, 55.0, 56.0, 57.0],\n            \"series_2_lag_3\": [52.0, 53.0, 54.0, 55.0, 56.0],\n            \"series_2_lag_4\": [51.0, 52.0, 53.0, 54.0, 55.0],\n            \"series_2_lag_5\": [50.0, 51.0, 52.0, 53.0, 54.0],\n            \"exog_1_step_1\": [105.0, 106.0, 107.0, 108.0, 109.0],\n            \"exog_2_step_1\": [1005.0, 1006.0, 1007.0, 1008.0, 1009.0],\n        },\n        index=pd.DatetimeIndex(\n            [\n                \"2020-01-06\",\n                \"2020-01-07\",\n                \"2020-01-08\",\n                \"2020-01-09\",\n                \"2020-01-10\",\n            ],\n            freq=\"D\",\n        ),\n    )\n\n    expected_y_train = {\n        1: pd.Series(\n            [5.0, 6.0, 7.0, 8.0, 9.0],\n            index=pd.DatetimeIndex(\n                [\n                    \"2020-01-06\",\n                    \"2020-01-07\",\n                    \"2020-01-08\",\n                    \"2020-01-09\",\n                    \"2020-01-10\",\n                ],\n                freq=\"D\",\n            ),\n            name=\"series_1_step_1\",\n        )\n    }\n\n    expected_X_test = pd.DataFrame(\n        {\n            \"series_1_lag_1\": [9.0, 10.0, 11.0, 12.0, 13.0],\n            \"series_1_lag_2\": [8.0, 9.0, 10.0, 11.0, 12.0],\n            \"series_1_lag_3\": [7.0, 8.0, 9.0, 10.0, 11.0],\n            \"series_1_lag_4\": [6.0, 7.0, 8.0, 9.0, 10.0],\n            \"series_1_lag_5\": [5.0, 6.0, 7.0, 8.0, 9.0],\n            \"series_2_lag_1\": [59.0, 60.0, 61.0, 62.0, 63.0],\n            \"series_2_lag_2\": [58.0, 59.0, 60.0, 61.0, 62.0],\n            \"series_2_lag_3\": [57.0, 58.0, 59.0, 60.0, 61.0],\n            \"series_2_lag_4\": [56.0, 57.0, 58.0, 59.0, 60.0],\n            \"series_2_lag_5\": [55.0, 56.0, 57.0, 58.0, 59.0],\n            \"exog_1_step_1\": [110.0, 111.0, 112.0, 113.0, 114.0],\n            \"exog_2_step_1\": [1010.0, 1011.0, 1012.0, 1013.0, 1014.0],\n        },\n        index=pd.DatetimeIndex(\n            [\n                \"2020-01-11\",\n                \"2020-01-12\",\n                \"2020-01-13\",\n                \"2020-01-14\",\n                \"2020-01-15\",\n            ],\n            freq=\"D\",\n        ),\n    )\n\n    expected_y_test = {\n        1: pd.Series(\n            [10.0, 11.0, 12.0, 13.0, 14.0],\n            index=pd.DatetimeIndex(\n                [\n                    \"2020-01-11\",\n                    \"2020-01-12\",\n                    \"2020-01-13\",\n                    \"2020-01-14\",\n                    \"2020-01-15\",\n                ],\n                freq=\"D\",\n            ),\n            name=\"series_1_step_1\",\n        )\n    }\n\n    expected_X_train_encoding = pd.Series(\n        [\n            \"series_1\",\n            \"series_1\",\n            \"series_1\",\n            \"series_1\",\n            \"series_1\",\n        ],\n        index=pd.DatetimeIndex(\n            [\n                \"2020-01-06\",\n                \"2020-01-07\",\n                \"2020-01-08\",\n                \"2020-01-09\",\n                \"2020-01-10\",\n            ],\n            freq=\"D\",\n        ),\n    )\n\n    expected_X_test_encoding = pd.Series(\n        [\n            \"series_1\",\n            \"series_1\",\n            \"series_1\",\n            \"series_1\",\n            \"series_1\",\n        ],\n        index=pd.DatetimeIndex(\n            [\"2020-01-11\", \"2020-01-12\", \"2020-01-13\", \"2020-01-14\", \"2020-01-15\"],\n            freq=\"D\",\n        ),\n    )\n\n    pd.testing.assert_frame_equal(X_train, expected_X_train)\n    pd.testing.assert_series_equal(y_train[1], expected_y_train[1])\n    pd.testing.assert_frame_equal(X_test, expected_X_test)\n    pd.testing.assert_series_equal(y_test[1], expected_y_test[1])\n    pd.testing.assert_series_equal(X_train_encoding, expected_X_train_encoding)\n    pd.testing.assert_series_equal(X_test_encoding, expected_X_test_encoding)\n",
  "GT_file_code": {
    "skforecast/direct/_forecaster_direct_multivariate.py": "################################################################################\n#                         ForecasterDirectMultiVariate                         #\n#                                                                              #\n# This work by skforecast team is licensed under the BSD 3-Clause License.     #\n################################################################################\n# coding=utf-8\n\nfrom typing import Union, Tuple, Optional, Callable, Any\nimport warnings\nimport sys\nimport numpy as np\nimport pandas as pd\nimport inspect\nfrom copy import copy\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import clone\nfrom joblib import Parallel, delayed, cpu_count\nfrom sklearn.preprocessing import StandardScaler\nfrom itertools import chain\n\nimport skforecast\nfrom ..base import ForecasterBase\nfrom ..exceptions import DataTransformationWarning\nfrom ..utils import (\n    initialize_lags,\n    initialize_window_features,\n    initialize_weights,\n    initialize_transformer_series,\n    check_select_fit_kwargs,\n    check_y,\n    check_exog,\n    prepare_steps_direct,\n    get_exog_dtypes,\n    check_exog_dtypes,\n    check_predict_input,\n    check_interval,\n    preprocess_y,\n    preprocess_last_window,\n    input_to_frame,\n    exog_to_direct,\n    exog_to_direct_numpy,\n    expand_index,\n    transform_numpy,\n    transform_series,\n    transform_dataframe,\n    select_n_jobs_fit_forecaster,\n    set_skforecast_warnings\n)\nfrom ..preprocessing import TimeSeriesDifferentiator\nfrom ..model_selection._utils import _extract_data_folds_multiseries\n\n\nclass ForecasterDirectMultiVariate(ForecasterBase):\n    \"\"\"\n    This class turns any regressor compatible with the scikit-learn API into a\n    autoregressive multivariate direct multi-step forecaster. A separate model \n    is created for each forecast time step. See documentation for more details.\n\n    Parameters\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n    level : str\n        Name of the time series to be predicted.\n    steps : int\n        Maximum number of future steps the forecaster will predict when using\n        method `predict()`. Since a different model is created for each step,\n        this value must be defined before training.\n    lags : int, list, numpy ndarray, range, dict, default `None`\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n\n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n        - `dict`: create different lags for each series. {'series_column_name': lags}.\n        - `None`: no lags are included as predictors. \n    window_features : object, list, default `None`\n        Instance or list of instances used to create window features. Window features\n        are created from the original time series and are included as predictors.\n    transformer_series : transformer (preprocessor), dict, default `sklearn.preprocessing.StandardScaler`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and \n        inverse_transform. Transformation is applied to each `series` before training \n        the forecaster. ColumnTransformers are not allowed since they do not have \n        inverse_transform method.\n\n        - If single transformer: it is cloned and applied to all series. \n        - If `dict` of transformers: a different transformer can be used for each series.\n    transformer_exog : transformer, default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable, default `None`\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\n        method. The resulting `sample_weight` cannot have negative values.\n    fit_kwargs : dict, default `None`\n        Additional arguments to be passed to the `fit` method of the regressor.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_fit_forecaster.\n    forecaster_id : str, int, default `None`\n        Name used as an identifier of the forecaster.\n\n    Attributes\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n        An instance of this regressor is trained for each step. All of them \n        are stored in `self.regressors_`.\n    regressors_ : dict\n        Dictionary with regressors trained for each step. They are initialized \n        as a copy of `regressor`.\n    steps : int\n        Number of future steps the forecaster will predict when using method\n        `predict()`. Since a different model is created for each step, this value\n        should be defined before training.\n    lags : numpy ndarray, dict\n        Lags used as predictors.\n    lags_ : dict\n        Dictionary with the lags of each series. Created from `lags` when \n        creating the training matrices and used internally to avoid overwriting.\n    lags_names : dict\n        Names of the lags of each series.\n    max_lag : int\n        Maximum lag included in `lags`.\n    window_features : list\n        Class or list of classes used to create window features.\n    window_features_names : list\n        Names of the window features to be included in the `X_train` matrix.\n    window_features_class_names : list\n        Names of the classes used to create the window features.\n    max_size_window_features : int\n        Maximum window size required by the window features.\n    window_size : int\n        The window size needed to create the predictors. It is calculated as the \n        maximum value between `max_lag` and `max_size_window_features`. If \n        differentiation is used, `window_size` is increased by n units equal to \n        the order of differentiation so that predictors can be generated correctly.\n    transformer_series : transformer (preprocessor), dict, default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and \n        inverse_transform. Transformation is applied to each `series` before training \n        the forecaster. ColumnTransformers are not allowed since they do not have \n        inverse_transform method.\n\n        - If single transformer: it is cloned and applied to all series. \n        - If `dict` of transformers: a different transformer can be used for each series.\n    transformer_series_ : dict\n        Dictionary with the transformer for each series. It is created cloning the \n        objects in `transformer_series` and is used internally to avoid overwriting.\n    transformer_exog : transformer\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its\n        `fit` method. The resulting `sample_weight` cannot have negative values.\n    source_code_weight_func : str\n        Source code of the custom function used to create weights.\n    differentiation : int\n        Order of differencing applied to the time series before training the \n        forecaster.\n    differentiator : TimeSeriesDifferentiator\n        Skforecast object used to differentiate the time series.\n    differentiator_ : dict\n        Dictionary with the `differentiator` for each series. It is created cloning the\n        objects in `differentiator` and is used internally to avoid overwriting.\n    last_window_ : pandas DataFrame\n        This window represents the most recent data observed by the predictor\n        during its training phase. It contains the values needed to predict the\n        next step immediately after the training data. These values are stored\n        in the original scale of the time series before undergoing any transformations\n        or differentiation. When `differentiation` parameter is specified, the\n        dimensions of the `last_window_` are expanded as many values as the order\n        of differentiation. For example, if `lags` = 7 and `differentiation` = 1,\n        `last_window_` will have 8 values.\n    index_type_ : type\n        Type of index of the input used in training.\n    index_freq_ : str\n        Frequency of Index of the input used in training.\n    training_range_: pandas Index\n        First and last values of index of the data used during training.\n    exog_in_ : bool\n        If the forecaster has been trained using exogenous variable/s.\n    exog_type_in_ : type\n        Type of exogenous variable/s used in training.\n    exog_dtypes_in_ : dict\n        Type of each exogenous variable/s used in training. If `transformer_exog` \n        is used, the dtypes are calculated after the transformation.\n    exog_names_in_ : list\n        Names of the exogenous variables used during training.\n    series_names_in_ : list\n        Names of the series used during training.\n    X_train_series_names_in_ : list\n        Names of the series added to `X_train` when creating the training\n        matrices with `_create_train_X_y` method. It is a subset of \n        `series_names_in_`.\n    X_train_window_features_names_out_ : list\n        Names of the window features included in the matrix `X_train` created\n        internally for training.\n    X_train_exog_names_out_ : list\n        Names of the exogenous variables included in the matrix `X_train` created\n        internally for training. It can be different from `exog_names_in_` if\n        some exogenous variables are transformed during the training process.\n    X_train_direct_exog_names_out_ : list\n        Same as `X_train_exog_names_out_` but using the direct format. The same \n        exogenous variable is repeated for each step.\n    X_train_features_names_out_ : list\n        Names of columns of the matrix created internally for training.\n    fit_kwargs : dict\n        Additional arguments to be passed to the `fit` method of the regressor.\n    in_sample_residuals_ : dict\n        Residuals of the models when predicting training data. Only stored up to\n        1000 values per model in the form `{step: residuals}`. If `transformer_series` \n        is not `None`, residuals are stored in the transformed scale.\n    out_sample_residuals_ : dict\n        Residuals of the models when predicting non training data. Only stored\n        up to 1000 values per model in the form `{step: residuals}`. If `transformer_series` \n        is not `None`, residuals are assumed to be in the transformed scale. Use \n        `set_out_sample_residuals()` method to set values.\n    creation_date : str\n        Date of creation.\n    is_fitted : bool\n        Tag to identify if the regressor has been fitted (trained).\n    fit_date : str\n        Date of last fit.\n    skforecast_version : str\n        Version of skforecast library used to create the forecaster.\n    python_version : str\n        Version of python used to create the forecaster.\n    n_jobs : int, 'auto'\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the fuction\n        skforecast.utils.select_n_jobs_fit_forecaster.\n    forecaster_id : str, int\n        Name used as an identifier of the forecaster.\n    dropna_from_series : Ignored\n        Not used, present here for API consistency by convention.\n    encoding : Ignored\n        Not used, present here for API consistency by convention.\n\n    Notes\n    -----\n    A separate model is created for each forecasting time step. It is important to\n    note that all models share the same parameter and hyperparameter configuration.\n    \n    \"\"\"\n    \n    def __init__(\n        self,\n        regressor: object,\n        level: str,\n        steps: int,\n        lags: Optional[Union[int, list, np.ndarray, range, dict]] = None,\n        window_features: Optional[Union[object, list]] = None,\n        transformer_series: Optional[Union[object, dict]] = StandardScaler(),\n        transformer_exog: Optional[object] = None,\n        weight_func: Optional[Callable] = None,\n        differentiation: Optional[int] = None,\n        fit_kwargs: Optional[dict] = None,\n        n_jobs: Union[int, str] = 'auto',\n        forecaster_id: Optional[Union[str, int]] = None\n    ) -> None:\n        \n        self.regressor                          = copy(regressor)\n        self.level                              = level\n        self.steps                              = steps\n        self.lags_                              = None\n        self.transformer_series                 = transformer_series\n        self.transformer_series_                = None\n        self.transformer_exog                   = transformer_exog\n        self.weight_func                        = weight_func\n        self.source_code_weight_func            = None\n        self.differentiation                    = differentiation\n        self.differentiator                     = None\n        self.differentiator_                    = None\n        self.last_window_                       = None\n        self.index_type_                        = None\n        self.index_freq_                        = None\n        self.training_range_                    = None\n        self.series_names_in_                   = None\n        self.exog_in_                           = False\n        self.exog_names_in_                     = None\n        self.exog_type_in_                      = None\n        self.exog_dtypes_in_                    = None\n        self.X_train_series_names_in_           = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_            = None\n        self.X_train_direct_exog_names_out_     = None\n        self.X_train_features_names_out_        = None\n        self.creation_date                      = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.is_fitted                          = False\n        self.fit_date                           = None\n        self.skforecast_version                 = skforecast.__version__\n        self.python_version                     = sys.version.split(\" \")[0]\n        self.forecaster_id                      = forecaster_id\n        self.dropna_from_series                 = False  # Ignored in this forecaster\n        self.encoding                           = None   # Ignored in this forecaster\n\n        if not isinstance(level, str):\n            raise TypeError(\n                f\"`level` argument must be a str. Got {type(level)}.\"\n            )\n\n        if not isinstance(steps, int):\n            raise TypeError(\n                f\"`steps` argument must be an int greater than or equal to 1. \"\n                f\"Got {type(steps)}.\"\n            )\n\n        if steps < 1:\n            raise ValueError(\n                f\"`steps` argument must be greater than or equal to 1. Got {steps}.\"\n            )\n        \n        self.regressors_ = {step: clone(self.regressor) for step in range(1, steps + 1)}\n\n        if isinstance(lags, dict):\n            self.lags = {}\n            self.lags_names = {}\n            list_max_lags = []\n            for key in lags:\n                if lags[key] is None:\n                    self.lags[key] = None\n                    self.lags_names[key] = None\n                else:\n                    self.lags[key], lags_names, max_lag = initialize_lags(\n                        forecaster_name = type(self).__name__,\n                        lags            = lags[key]\n                    )\n                    self.lags_names[key] = (\n                        [f'{key}_{lag}' for lag in lags_names] \n                         if lags_names is not None \n                         else None\n                    )\n                    if max_lag is not None:\n                        list_max_lags.append(max_lag)\n            \n            self.max_lag = max(list_max_lags) if len(list_max_lags) != 0 else None\n        else:\n            self.lags, self.lags_names, self.max_lag = initialize_lags(\n                forecaster_name = type(self).__name__, \n                lags            = lags\n            )\n\n        self.window_features, self.window_features_names, self.max_size_window_features = (\n            initialize_window_features(window_features)\n        )\n        if self.window_features is None and (self.lags is None or self.max_lag is None):\n            raise ValueError(\n                \"At least one of the arguments `lags` or `window_features` \"\n                \"must be different from None. This is required to create the \"\n                \"predictors used in training the forecaster.\"\n            )\n        \n        self.window_size = max(\n            [ws for ws in [self.max_lag, self.max_size_window_features] \n             if ws is not None]\n        )\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [\n                type(wf).__name__ for wf in self.window_features\n            ]\n\n        if self.differentiation is not None:\n            if not isinstance(differentiation, int) or differentiation < 1:\n                raise ValueError(\n                    f\"Argument `differentiation` must be an integer equal to or \"\n                    f\"greater than 1. Got {differentiation}.\"\n                )\n            self.window_size += self.differentiation\n            self.differentiator = TimeSeriesDifferentiator(\n                order=self.differentiation, window_size=self.window_size\n            )\n            \n        self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n            forecaster_name = type(self).__name__, \n            regressor       = regressor, \n            weight_func     = weight_func, \n            series_weights  = None\n        )\n\n        self.fit_kwargs = check_select_fit_kwargs(\n                              regressor  = regressor,\n                              fit_kwargs = fit_kwargs\n                          )\n\n        self.in_sample_residuals_ = {step: None for step in range(1, steps + 1)}\n        self.out_sample_residuals_ = None\n\n        if n_jobs == 'auto':\n            self.n_jobs = select_n_jobs_fit_forecaster(\n                              forecaster_name = type(self).__name__,\n                              regressor       = self.regressor\n                          )\n        else:\n            if not isinstance(n_jobs, int):\n                raise TypeError(\n                    f\"`n_jobs` must be an integer or `'auto'`. Got {type(n_jobs)}.\"\n                )\n            self.n_jobs = n_jobs if n_jobs > 0 else cpu_count()\n\n    def __repr__(\n        self\n    ) -> str:\n        \"\"\"\n        Information displayed when a ForecasterDirectMultiVariate object is printed.\n        \"\"\"\n        \n        (\n            params,\n            _,\n            series_names_in_,\n            exog_names_in_,\n            transformer_series,\n        ) = [\n            self._format_text_repr(value) \n            for value in self._preprocess_repr(\n                regressor          = self.regressor,\n                series_names_in_   = self.series_names_in_,\n                exog_names_in_     = self.exog_names_in_,\n                transformer_series = self.transformer_series,\n            )\n        ]\n\n        info = (\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"{type(self).__name__} \\n\"\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"Regressor: {type(self.regressor).__name__} \\n\"\n            f\"Target series (level): {self.level} \\n\"\n            f\"Lags: {self.lags} \\n\"\n            f\"Window features: {self.window_features_names} \\n\"\n            f\"Window size: {self.window_size} \\n\"\n            f\"Maximum steps to predict: {self.steps} \\n\"\n            f\"Multivariate series: {series_names_in_} \\n\"\n            f\"Exogenous included: {self.exog_in_} \\n\"\n            f\"Exogenous names: {exog_names_in_} \\n\"\n            f\"Transformer for series: {transformer_series} \\n\"\n            f\"Transformer for exog: {self.transformer_exog} \\n\"\n            f\"Weight function included: {True if self.weight_func is not None else False} \\n\"\n            f\"Differentiation order: {self.differentiation} \\n\"\n            f\"Training range: {self.training_range_.to_list() if self.is_fitted else None} \\n\"\n            f\"Training index type: {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None} \\n\"\n            f\"Training index frequency: {self.index_freq_ if self.is_fitted else None} \\n\"\n            f\"Regressor parameters: {params} \\n\"\n            f\"fit_kwargs: {self.fit_kwargs} \\n\"\n            f\"Creation date: {self.creation_date} \\n\"\n            f\"Last fit date: {self.fit_date} \\n\"\n            f\"Skforecast version: {self.skforecast_version} \\n\"\n            f\"Python version: {self.python_version} \\n\"\n            f\"Forecaster id: {self.forecaster_id} \\n\"\n        )\n\n        return info\n\n\n    def _repr_html_(self):\n        \"\"\"\n        HTML representation of the object.\n        The \"General Information\" section is expanded by default.\n        \"\"\"\n\n        (\n            params,\n            _,\n            series_names_in_,\n            exog_names_in_,\n            transformer_series,\n        ) = self._preprocess_repr(\n                regressor          = self.regressor,\n                series_names_in_   = self.series_names_in_,\n                exog_names_in_     = self.exog_names_in_,\n                transformer_series = self.transformer_series,\n            )\n\n        style, unique_id = self._get_style_repr_html(self.is_fitted)\n        \n        content = f\"\"\"\n        <div class=\"container-{unique_id}\">\n            <h2>{type(self).__name__}</h2>\n            <details open>\n                <summary>General Information</summary>\n                <ul>\n                    <li><strong>Regressor:</strong> {type(self.regressor).__name__}</li>\n                    <li><strong>Target series (level):</strong> {self.level}</li>\n                    <li><strong>Lags:</strong> {self.lags}</li>\n                    <li><strong>Window features:</strong> {self.window_features_names}</li>\n                    <li><strong>Window size:</strong> {self.window_size}</li>\n                    <li><strong>Maximum steps to predict:</strong> {self.steps}</li>\n                    <li><strong>Exogenous included:</strong> {self.exog_in_}</li>\n                    <li><strong>Weight function included:</strong> {self.weight_func is not None}</li>\n                    <li><strong>Differentiation order:</strong> {self.differentiation}</li>\n                    <li><strong>Creation date:</strong> {self.creation_date}</li>\n                    <li><strong>Last fit date:</strong> {self.fit_date}</li>\n                    <li><strong>Skforecast version:</strong> {self.skforecast_version}</li>\n                    <li><strong>Python version:</strong> {self.python_version}</li>\n                    <li><strong>Forecaster id:</strong> {self.forecaster_id}</li>\n                </ul>\n            </details>\n            <details>\n                <summary>Exogenous Variables</summary>\n                <ul>\n                    {exog_names_in_}\n                </ul>\n            </details>\n            <details>\n                <summary>Data Transformations</summary>\n                <ul>\n                    <li><strong>Transformer for series:</strong> {transformer_series}</li>\n                    <li><strong>Transformer for exog:</strong> {self.transformer_exog}</li>\n                </ul>\n            </details>\n            <details>\n                <summary>Training Information</summary>\n                <ul>\n                    <li><strong>Target series (level):</strong> {self.level}</li>\n                    <li><strong>Multivariate series:</strong> {series_names_in_}</li>\n                    <li><strong>Training range:</strong> {self.training_range_.to_list() if self.is_fitted else 'Not fitted'}</li>\n                    <li><strong>Training index type:</strong> {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted'}</li>\n                    <li><strong>Training index frequency:</strong> {self.index_freq_ if self.is_fitted else 'Not fitted'}</li>\n                </ul>\n            </details>\n            <details>\n                <summary>Regressor Parameters</summary>\n                <ul>\n                    {params}\n                </ul>\n            </details>\n            <details>\n                <summary>Fit Kwargs</summary>\n                <ul>\n                    {self.fit_kwargs}\n                </ul>\n            </details>\n            <p>\n                <a href=\"https://skforecast.org/{skforecast.__version__}/api/forecasterdirectmultivariate.html\">&#128712 <strong>API Reference</strong></a>\n                &nbsp;&nbsp;\n                <a href=\"https://skforecast.org/{skforecast.__version__}/user_guides/dependent-multi-series-multivariate-forecasting.html\">&#128462 <strong>User Guide</strong></a>\n            </p>\n        </div>\n        \"\"\"\n\n        # Return the combined style and content\n        return style + content\n\n    \n    def _create_data_to_return_dict(\n        self, \n        series_names_in_: list\n    ) -> Tuple[dict, list]:\n        \"\"\"\n        Create `data_to_return_dict` based on series names and lags configuration.\n        The dictionary contains the information to decide what data to return in \n        the `_create_lags` method.\n        \n        Parameters\n        ----------\n        series_names_in_ : list\n            Names of the series used during training.\n\n        Returns\n        -------\n        data_to_return_dict : dict\n            Dictionary with the information to decide what data to return in the\n            `_create_lags` method.\n        X_train_series_names_in_ : list\n            Names of the series added to `X_train` when creating the training\n            matrices with `_create_train_X_y` method. It is a subset of \n            `series_names_in_`.\n        \n        \"\"\"\n\n        if isinstance(self.lags, dict):\n            lags_keys = list(self.lags.keys())\n            if set(lags_keys) != set(series_names_in_):  # Set to avoid order\n                raise ValueError(\n                    (f\"When `lags` parameter is a `dict`, its keys must be the \"\n                     f\"same as `series` column names. If don't want to include lags, \"\n                      \"add '{column: None}' to the lags dict.\\n\"\n                     f\"  Lags keys        : {lags_keys}.\\n\"\n                     f\"  `series` columns : {series_names_in_}.\")\n                )\n            self.lags_ = copy(self.lags)\n        else:\n            self.lags_ = {serie: self.lags for serie in series_names_in_}\n            if self.lags is not None:\n                # Defined `lags_names` here to avoid overwriting when fit and then create_train_X_y\n                lags_names = [f'lag_{i}' for i in self.lags]\n                self.lags_names = {\n                    serie: [f'{serie}_{lag}' for lag in lags_names]\n                    for serie in series_names_in_\n                }\n            else:\n                self.lags_names = {serie: None for serie in series_names_in_}\n\n        X_train_series_names_in_ = series_names_in_\n        if self.lags is None:\n            data_to_return_dict = {self.level: 'y'}\n        else:\n            # If col is not level and has lags, create 'X' if no lags don't include\n            # If col is level, create 'both' (`X` and `y`)\n            data_to_return_dict = {\n                col: ('both' if col == self.level else 'X')\n                for col in series_names_in_\n                if col == self.level or self.lags_.get(col) is not None\n            }\n\n            # Adjust 'level' in case self.lags_[level] is None\n            if self.lags_.get(self.level) is None:\n                data_to_return_dict[self.level] = 'y'\n\n            if self.window_features is None:\n                # X_train_series_names_in_ include series that will be added to X_train\n                X_train_series_names_in_ = [\n                    col for col in data_to_return_dict.keys()\n                    if data_to_return_dict[col] in ['X', 'both']\n                ]\n\n        return data_to_return_dict, X_train_series_names_in_\n\n\n    def _create_lags(\n        self, \n        y: np.ndarray,\n        lags: np.ndarray,\n        data_to_return: Optional[str] = 'both'\n    ) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n        \"\"\"\n        Create the lagged values and their target variable from a time series.\n        \n        Note that the returned matrix `X_data` contains the lag 1 in the first \n        column, the lag 2 in the in the second column and so on.\n        \n        Parameters\n        ----------\n        y : numpy ndarray\n            Training time series values.\n        lags : numpy ndarray\n            lags to create.\n        data_to_return : str, default 'both'\n            Specifies which data to return. Options are 'X', 'y', 'both' or None.\n\n        Returns\n        -------\n        X_data : numpy ndarray, None\n            Lagged values (predictors).\n        y_data : numpy ndarray, None\n            Values of the time series related to each row of `X_data`.\n        \n        \"\"\"\n\n        X_data = None\n        y_data = None\n        if data_to_return is not None:\n\n            n_rows = len(y) - self.window_size - (self.steps - 1)\n\n            if data_to_return != 'y':\n                # If `data_to_return` is not 'y', it means is 'X' or 'both', X_data is created\n                X_data = np.full(\n                    shape=(n_rows, len(lags)), fill_value=np.nan, order='F', dtype=float\n                )\n                for i, lag in enumerate(lags):\n                    X_data[:, i] = y[self.window_size - lag : -(lag + self.steps - 1)]\n\n            if data_to_return != 'X':\n                # If `data_to_return` is not 'X', it means is 'y' or 'both', y_data is created\n                y_data = np.full(\n                    shape=(n_rows, self.steps), fill_value=np.nan, order='F', dtype=float\n                )\n                for step in range(self.steps):\n                    y_data[:, step] = y[self.window_size + step : self.window_size + step + n_rows]\n        \n        return X_data, y_data\n\n\n    def _create_window_features(\n        self, \n        y: pd.Series,\n        train_index: pd.Index,\n        X_as_pandas: bool = False,\n    ) -> Tuple[list, list]:\n        \"\"\"\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        train_index : pandas Index\n            Index of the training data. It is used to create the pandas DataFrame\n            `X_train_window_features` when `X_as_pandas` is `True`.\n        X_as_pandas : bool, default `False`\n            If `True`, the returned matrix `X_train_window_features` is a \n            pandas DataFrame.\n\n        Returns\n        -------\n        X_train_window_features : list\n            List of numpy ndarrays or pandas DataFrames with the window features.\n        X_train_window_features_names_out_ : list\n            Names of the window features.\n        \n        \"\"\"\n\n        len_train_index = len(train_index)\n        X_train_window_features = []\n        X_train_window_features_names_out_ = []\n        for wf in self.window_features:\n            X_train_wf = wf.transform_batch(y)\n            if not isinstance(X_train_wf, pd.DataFrame):\n                raise TypeError(\n                    (f\"The method `transform_batch` of {type(wf).__name__} \"\n                     f\"must return a pandas DataFrame.\")\n                )\n            X_train_wf = X_train_wf.iloc[-len_train_index:]\n            if not len(X_train_wf) == len_train_index:\n                raise ValueError(\n                    (f\"The method `transform_batch` of {type(wf).__name__} \"\n                     f\"must return a DataFrame with the same number of rows as \"\n                     f\"the input time series - (`window_size` + (`steps` - 1)): {len_train_index}.\")\n                )\n            X_train_wf.index = train_index\n            \n            X_train_wf.columns = [f'{y.name}_{col}' for col in X_train_wf.columns]\n            X_train_window_features_names_out_.extend(X_train_wf.columns)\n            if not X_as_pandas:\n                X_train_wf = X_train_wf.to_numpy()     \n            X_train_window_features.append(X_train_wf)\n\n        return X_train_window_features, X_train_window_features_names_out_\n\n\n    def _create_train_X_y(\n        self,\n        series: pd.DataFrame,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n    ) -> Tuple[pd.DataFrame, dict, list, list, list, list, list, dict]:\n        \"\"\"\n        Create training matrices from multiple time series and exogenous\n        variables. The resulting matrices contain the target variable and predictors\n        needed to train all the regressors (one per step).\n        \n        Parameters\n        ----------\n        series : pandas DataFrame\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `series` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors) for each step. Note that the index \n            corresponds to that of the last step. It is updated for the corresponding \n            step in the filter_train_X_y_for_step method.\n        y_train : dict\n            Values of the time series related to each row of `X_train` for each \n            step in the form {step: y_step_[i]}.\n        series_names_in_ : list\n            Names of the series used during training.\n        X_train_series_names_in_ : list\n            Names of the series added to `X_train` when creating the training\n            matrices with `_create_train_X_y` method. It is a subset of \n            `series_names_in_`.\n        exog_names_in_ : list\n            Names of the exogenous variables included in the training matrices.\n        X_train_exog_names_out_ : list\n            Names of the exogenous variables included in the matrix `X_train` created\n            internally for training. It can be different from `exog_names_in_` if\n            some exogenous variables are transformed during the training process.\n        X_train_features_names_out_ : list\n            Names of the columns of the matrix created internally for training.\n        exog_dtypes_in_ : dict\n            Type of each exogenous variable/s used in training. If `transformer_exog` \n            is used, the dtypes are calculated before the transformation.\n        \n        \"\"\"\n\n        if not isinstance(series, pd.DataFrame):\n            raise TypeError(\n                f\"`series` must be a pandas DataFrame. Got {type(series)}.\"\n            )\n\n        if len(series) < self.window_size + self.steps:\n            raise ValueError(\n                f\"Minimum length of `series` for training this forecaster is \"\n                f\"{self.window_size + self.steps}. Reduce the number of \"\n                f\"predicted steps, {self.steps}, or the maximum \"\n                f\"window_size, {self.window_size}, if no more data is available.\\n\"\n                f\"    Length `series`: {len(series)}.\\n\"\n                f\"    Max step : {self.steps}.\\n\"\n                f\"    Max window size: {self.window_size}.\\n\"\n                f\"    Lags window size: {self.max_lag}.\\n\"\n                f\"    Window features window size: {self.max_size_window_features}.\"\n            )\n        \n        series_names_in_ = list(series.columns)\n\n        if self.level not in series_names_in_:\n            raise ValueError(\n                f\"One of the `series` columns must be named as the `level` of the forecaster.\\n\"\n                f\"  Forecaster `level` : {self.level}.\\n\"\n                f\"  `series` columns   : {series_names_in_}.\"\n            )\n\n        data_to_return_dict, X_train_series_names_in_ = (\n            self._create_data_to_return_dict(series_names_in_=series_names_in_)\n        )\n\n        series_to_create_autoreg_features_and_y = [\n            col for col in series_names_in_ \n            if col in X_train_series_names_in_ + [self.level]\n        ]\n\n        fit_transformer = False\n        if not self.is_fitted:\n            fit_transformer = True\n            self.transformer_series_ = initialize_transformer_series(\n                                           forecaster_name    = type(self).__name__,\n                                           series_names_in_   = series_to_create_autoreg_features_and_y,\n                                           transformer_series = self.transformer_series\n                                       )\n\n        if self.differentiation is None:\n            self.differentiator_ = {\n                serie: None for serie in series_to_create_autoreg_features_and_y\n            }\n        else:\n            if not self.is_fitted:\n                self.differentiator_ = {\n                    serie: copy(self.differentiator)\n                    for serie in series_to_create_autoreg_features_and_y\n                }\n\n        exog_names_in_ = None\n        exog_dtypes_in_ = None\n        categorical_features = False\n        if exog is not None:\n            check_exog(exog=exog, allow_nan=True)\n            exog = input_to_frame(data=exog, input_name='exog')\n            \n            series_index_no_ws = series.index[self.window_size:]\n            len_series = len(series)\n            len_series_no_ws = len_series - self.window_size\n            len_exog = len(exog)\n            if not len_exog == len_series and not len_exog == len_series_no_ws:\n                raise ValueError(\n                    f\"Length of `exog` must be equal to the length of `series` (if \"\n                    f\"index is fully aligned) or length of `seriesy` - `window_size` \"\n                    f\"(if `exog` starts after the first `window_size` values).\\n\"\n                    f\"    `exog`                   : ({exog.index[0]} -- {exog.index[-1]})  (n={len_exog})\\n\"\n                    f\"    `series`                 : ({series.index[0]} -- {series.index[-1]})  (n={len_series})\\n\"\n                    f\"    `series` - `window_size` : ({series_index_no_ws[0]} -- {series_index_no_ws[-1]})  (n={len_series_no_ws})\"\n                )\n            \n            exog_names_in_ = exog.columns.to_list()\n            if len(set(exog_names_in_) - set(series_names_in_)) != len(exog_names_in_):\n                raise ValueError(\n                    f\"`exog` cannot contain a column named the same as one of \"\n                    f\"the series (column names of series).\\n\"\n                    f\"  `series` columns : {series_names_in_}.\\n\"\n                    f\"  `exog`   columns : {exog_names_in_}.\"\n                )\n            \n            # NOTE: Need here for filter_train_X_y_for_step to work without fitting\n            self.exog_in_ = True\n            exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = fit_transformer,\n                       inverse_transform = False\n                   )\n                \n            check_exog_dtypes(exog, call_check_exog=True)\n            categorical_features = (\n                exog.select_dtypes(include=np.number).shape[1] != exog.shape[1]\n            )\n\n            # Use .index as series.index is not yet preprocessed with preprocess_y\n            if len_exog == len_series:\n                if not (exog.index == series.index).all():\n                    raise ValueError(\n                        \"When `exog` has the same length as `series`, the index \"\n                        \"of `exog` must be aligned with the index of `series` \"\n                        \"to ensure the correct alignment of values.\"\n                    )\n                # The first `self.window_size` positions have to be removed from \n                # exog since they are not in X_train.\n                exog = exog.iloc[self.window_size:, ]\n            else:\n                if not (exog.index == series_index_no_ws).all():\n                    raise ValueError(\n                        \"When `exog` doesn't contain the first `window_size` \"\n                        \"observations, the index of `exog` must be aligned with \"\n                        \"the index of `series` minus the first `window_size` \"\n                        \"observations to ensure the correct alignment of values.\"\n                    )\n\n        X_train_autoreg = []\n        X_train_window_features_names_out_ = [] if self.window_features is not None else None\n        X_train_features_names_out_ = []\n        for col in series_to_create_autoreg_features_and_y:\n            y = series[col]\n            check_y(y=y, series_id=f\"Column '{col}'\")\n            y = transform_series(\n                    series            = y,\n                    transformer       = self.transformer_series_[col],\n                    fit               = fit_transformer,\n                    inverse_transform = False\n                )\n            y_values, y_index = preprocess_y(y=y)\n\n            if self.differentiation is not None:\n                if not self.is_fitted:\n                    y_values = self.differentiator_[col].fit_transform(y_values)\n                else:\n                    differentiator = copy(self.differentiator_[col])\n                    y_values = differentiator.fit_transform(y_values)\n\n            X_train_autoreg_col = []\n            train_index = y_index[self.window_size + (self.steps - 1):]\n\n            X_train_lags, y_train_values = self._create_lags(\n                y=y_values, lags=self.lags_[col], data_to_return=data_to_return_dict.get(col, None)\n            )\n            if X_train_lags is not None:\n                X_train_autoreg_col.append(X_train_lags)\n                X_train_features_names_out_.extend(self.lags_names[col])\n\n            if col == self.level:\n                y_train = y_train_values\n\n            if self.window_features is not None:\n                n_diff = 0 if self.differentiation is None else self.differentiation\n                end_wf = None if self.steps == 1 else -(self.steps - 1)\n                y_window_features = pd.Series(\n                    y_values[n_diff:end_wf], index=y_index[n_diff:end_wf], name=col\n                )\n                X_train_window_features, X_train_wf_names_out_ = (\n                    self._create_window_features(\n                        y=y_window_features, X_as_pandas=False, train_index=train_index\n                    )\n                )\n                X_train_autoreg_col.extend(X_train_window_features)\n                X_train_window_features_names_out_.extend(X_train_wf_names_out_)\n                X_train_features_names_out_.extend(X_train_wf_names_out_)\n\n            if X_train_autoreg_col:\n                if len(X_train_autoreg_col) == 1:\n                    X_train_autoreg_col = X_train_autoreg_col[0]\n                else:\n                    X_train_autoreg_col = np.concatenate(X_train_autoreg_col, axis=1)\n\n                X_train_autoreg.append(X_train_autoreg_col)\n\n        X_train = []\n        len_train_index = len(train_index)\n        if categorical_features:\n            if len(X_train_autoreg) == 1:\n                X_train_autoreg = X_train_autoreg[0]\n            else:\n                X_train_autoreg = np.concatenate(X_train_autoreg, axis=1)\n            X_train_autoreg = pd.DataFrame(\n                                  data    = X_train_autoreg,\n                                  columns = X_train_features_names_out_,\n                                  index   = train_index\n                              )\n            X_train.append(X_train_autoreg)\n        else:\n            X_train.extend(X_train_autoreg)\n\n        # NOTE: Need here for filter_train_X_y_for_step to work without fitting\n        self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n\n        X_train_exog_names_out_ = None\n        if exog is not None:\n            X_train_exog_names_out_ = exog.columns.to_list()\n            if categorical_features:\n                exog_direct, X_train_direct_exog_names_out_ = exog_to_direct(\n                    exog=exog, steps=self.steps\n                )\n                exog_direct.index = train_index\n            else:\n                exog_direct, X_train_direct_exog_names_out_ = exog_to_direct_numpy(\n                    exog=exog, steps=self.steps\n                )\n\n            # NOTE: Need here for filter_train_X_y_for_step to work without fitting\n            self.X_train_direct_exog_names_out_ = X_train_direct_exog_names_out_\n\n            X_train_features_names_out_.extend(self.X_train_direct_exog_names_out_)\n            X_train.append(exog_direct)\n        \n        if len(X_train) == 1:\n            X_train = X_train[0]\n        else:\n            if categorical_features:\n                X_train = pd.concat(X_train, axis=1)\n            else:\n                X_train = np.concatenate(X_train, axis=1)\n                \n        if categorical_features:\n            X_train.index = train_index\n        else:\n            X_train = pd.DataFrame(\n                          data    = X_train,\n                          index   = train_index,\n                          columns = X_train_features_names_out_\n                      )\n\n        y_train = {\n            step: pd.Series(\n                      data  = y_train[:, step - 1], \n                      index = y_index[self.window_size + step - 1:][:len_train_index],\n                      name  = f\"{self.level}_step_{step}\"\n                  )\n            for step in range(1, self.steps + 1)\n        }\n\n        return (\n            X_train,\n            y_train,\n            series_names_in_,\n            X_train_series_names_in_,\n            exog_names_in_,\n            X_train_exog_names_out_,\n            X_train_features_names_out_,\n            exog_dtypes_in_\n        )\n\n\n    def create_train_X_y(\n        self,\n        series: pd.DataFrame,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        suppress_warnings: bool = False\n    ) -> Tuple[pd.DataFrame, dict]:\n        \"\"\"\n        Create training matrices from multiple time series and exogenous\n        variables. The resulting matrices contain the target variable and predictors\n        needed to train all the regressors (one per step).\n        \n        Parameters\n        ----------\n        series : pandas DataFrame\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `series` and their indexes must be aligned.\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the creation\n            of the training matrices. See skforecast.exceptions.warn_skforecast_categories \n            for more information.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors) for each step. Note that the index \n            corresponds to that of the last step. It is updated for the corresponding \n            step in the filter_train_X_y_for_step method.\n        y_train : dict\n            Values of the time series related to each row of `X_train` for each \n            step in the form {step: y_step_[i]}.\n        \n        \"\"\"\n\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n\n        output = self._create_train_X_y(\n                     series = series, \n                     exog   = exog\n                 )\n\n        X_train = output[0]\n        y_train = output[1]\n        \n        set_skforecast_warnings(suppress_warnings, action='default')\n\n        return X_train, y_train\n\n\n    def filter_train_X_y_for_step(\n        self,\n        step: int,\n        X_train: pd.DataFrame,\n        y_train: dict,\n        remove_suffix: bool = False\n    ) -> Tuple[pd.DataFrame, pd.Series]:\n        \"\"\"\n        Select the columns needed to train a forecaster for a specific step.  \n        The input matrices should be created using `_create_train_X_y` method. \n        This method updates the index of `X_train` to the corresponding one \n        according to `y_train`. If `remove_suffix=True` the suffix \"_step_i\" \n        will be removed from the column names. \n\n        Parameters\n        ----------\n        step : int\n            step for which columns must be selected selected. Starts at 1.\n        X_train : pandas DataFrame\n            Dataframe created with the `_create_train_X_y` method, first return.\n        y_train : dict\n            Dict created with the `_create_train_X_y` method, second return.\n        remove_suffix : bool, default `False`\n            If True, suffix \"_step_i\" is removed from the column names.\n\n        Returns\n        -------\n        X_train_step : pandas DataFrame\n            Training values (predictors) for the selected step.\n        y_train_step : pandas Series\n            Values of the time series related to each row of `X_train`.\n\n        \"\"\"\n\n        if (step < 1) or (step > self.steps):\n            raise ValueError(\n                (f\"Invalid value `step`. For this forecaster, minimum value is 1 \"\n                 f\"and the maximum step is {self.steps}.\")\n            )\n\n        y_train_step = y_train[step]\n\n        # Matrix X_train starts at index 0.\n        if not self.exog_in_:\n            X_train_step = X_train\n        else:\n            n_lags = len(list(\n                chain(*[v for v in self.lags_.values() if v is not None])\n            ))\n            n_window_features = (\n                len(self.X_train_window_features_names_out_) if self.window_features is not None else 0\n            )\n            idx_columns_autoreg = np.arange(n_lags + n_window_features)\n            n_exog = len(self.X_train_direct_exog_names_out_) / self.steps\n            idx_columns_exog = (\n                np.arange((step - 1) * n_exog, (step) * n_exog) + idx_columns_autoreg[-1] + 1 \n            )\n            idx_columns = np.concatenate((idx_columns_autoreg, idx_columns_exog))\n            X_train_step = X_train.iloc[:, idx_columns]\n\n        X_train_step.index = y_train_step.index\n\n        if remove_suffix:\n            X_train_step.columns = [\n                col_name.replace(f\"_step_{step}\", \"\")\n                for col_name in X_train_step.columns\n            ]\n            y_train_step.name = y_train_step.name.replace(f\"_step_{step}\", \"\")\n\n        return X_train_step, y_train_step\n\n\n    def _train_test_split_one_step_ahead(\n        self,\n        series: pd.DataFrame,\n        initial_train_size: int,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n    ) -> Tuple[pd.DataFrame, dict, pd.DataFrame, dict, pd.Series, pd.Series]:\n        \"\"\"\n        Create matrices needed to train and test the forecaster for one-step-ahead\n        predictions.\n        \n        Parameters\n        ----------\n        series : pandas DataFrame\n            Training time series.\n        initial_train_size : int\n            Initial size of the training set. It is the number of observations used\n            to train the forecaster before making the first prediction.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `series` and their indexes must be aligned.\n        \n        Returns\n        -------\n        X_train : pandas DataFrame\n            Predictor values used to train the model.\n        y_train : dict\n            Values of the time series related to each row of `X_train` for each \n            step in the form {step: y_step_[i]}.\n        X_test : pandas DataFrame\n            Predictor values used to test the model.\n        y_test : dict\n            Values of the time series related to each row of `X_test` for each \n            step in the form {step: y_step_[i]}.\n        X_train_encoding : pandas Series\n            Series identifiers for each row of `X_train`.\n        X_test_encoding : pandas Series\n            Series identifiers for each row of `X_test`.\n        \n        \"\"\"\n\n        span_index = series.index\n\n        fold = [\n            [0, initial_train_size],\n            [initial_train_size - self.window_size, initial_train_size],\n            [initial_train_size - self.window_size, len(span_index)],\n            [0, 0],  # Dummy value\n            True\n        ]\n        data_fold = _extract_data_folds_multiseries(\n                        series             = series,\n                        folds              = [fold],\n                        span_index         = span_index,\n                        window_size        = self.window_size,\n                        exog               = exog,\n                        dropna_last_window = self.dropna_from_series,\n                        externally_fitted  = False\n                    )\n        series_train, _, levels_last_window, exog_train, exog_test, _ = next(data_fold)\n\n        start_test_idx = initial_train_size - self.window_size\n        series_test = series.iloc[start_test_idx:, :]\n        series_test = series_test.loc[:, levels_last_window]\n        series_test = series_test.dropna(axis=1, how='all')\n       \n        _is_fitted = self.is_fitted\n        _series_names_in_ = self.series_names_in_\n        _exog_names_in_ = self.exog_names_in_\n\n        self.is_fitted = False\n        X_train, y_train, series_names_in_, _, exog_names_in_, *_ = (\n            self._create_train_X_y(\n                series = series_train,\n                exog   = exog_train,\n            )\n        )\n        self.series_names_in_ = series_names_in_\n        if exog is not None:\n            self.exog_names_in_ = exog_names_in_\n        self.is_fitted = True\n\n        X_test, y_test, *_ = self._create_train_X_y(\n                                 series = series_test,\n                                 exog   = exog_test,\n                             )\n        self.is_fitted = _is_fitted\n        self.series_names_in_ = _series_names_in_\n        self.exog_names_in_ = _exog_names_in_\n\n        X_train_encoding = pd.Series(self.level, index=X_train.index)\n        X_test_encoding = pd.Series(self.level, index=X_test.index)\n\n        return X_train, y_train, X_test, y_test, X_train_encoding, X_test_encoding\n\n\n    def create_sample_weights(\n        self,\n        X_train: pd.DataFrame\n    ) -> np.ndarray:\n        \"\"\"\n        Crate weights for each observation according to the forecaster's attribute\n        `weight_func`. \n\n        Parameters\n        ----------\n        X_train : pandas DataFrame\n            Dataframe created with `_create_train_X_y` and filter_train_X_y_for_step`\n            methods, first return.\n\n        Returns\n        -------\n        sample_weight : numpy ndarray\n            Weights to use in `fit` method.\n        \n        \"\"\"\n\n        sample_weight = None\n\n        if self.weight_func is not None:\n            sample_weight = self.weight_func(X_train.index)\n\n        if sample_weight is not None:\n            if np.isnan(sample_weight).any():\n                raise ValueError(\n                    \"The resulting `sample_weight` cannot have NaN values.\"\n                )\n            if np.any(sample_weight < 0):\n                raise ValueError(\n                    \"The resulting `sample_weight` cannot have negative values.\"\n                )\n            if np.sum(sample_weight) == 0:\n                raise ValueError(\n                    (\"The resulting `sample_weight` cannot be normalized because \"\n                     \"the sum of the weights is zero.\")\n                )\n\n        return sample_weight\n\n\n    def fit(\n        self,\n        series: pd.DataFrame,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        store_last_window: bool = True,\n        store_in_sample_residuals: bool = True,\n        suppress_warnings: bool = False\n    ) -> None:\n        \"\"\"\n        Training Forecaster.\n\n        Additional arguments to be passed to the `fit` method of the regressor \n        can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n        Parameters\n        ----------\n        series : pandas DataFrame\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `series` and their indexes must be aligned so\n            that series[i] is regressed on exog[i].\n        store_last_window : bool, default `True`\n            Whether or not to store the last window (`last_window_`) of training data.\n        store_in_sample_residuals : bool, default `True`\n            If `True`, in-sample residuals will be stored in the forecaster object\n            after fitting (`in_sample_residuals_` attribute).\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the training \n            process. See skforecast.exceptions.warn_skforecast_categories for more\n            information.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n        \n        # Reset values in case the forecaster has already been fitted.\n        self.lags_                              = None\n        self.last_window_                       = None\n        self.index_type_                        = None\n        self.index_freq_                        = None\n        self.training_range_                    = None\n        self.series_names_in_                   = None\n        self.exog_in_                           = False\n        self.exog_names_in_                     = None\n        self.exog_type_in_                      = None\n        self.exog_dtypes_in_                    = None\n        self.X_train_series_names_in_           = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_            = None\n        self.X_train_direct_exog_names_out_     = None\n        self.X_train_features_names_out_        = None\n        self.in_sample_residuals_               = {step: None for step in range(1, self.steps + 1)}\n        self.is_fitted                          = False\n        self.fit_date                           = None\n\n        (\n            X_train,\n            y_train,\n            series_names_in_,\n            X_train_series_names_in_,\n            exog_names_in_,\n            X_train_exog_names_out_,\n            X_train_features_names_out_,\n            exog_dtypes_in_\n        ) = self._create_train_X_y(series=series, exog=exog)\n\n        def fit_forecaster(regressor, X_train, y_train, step, store_in_sample_residuals):\n            \"\"\"\n            Auxiliary function to fit each of the forecaster's regressors in parallel.\n\n            Parameters\n            ----------\n            regressor : object\n                Regressor to be fitted.\n            X_train : pandas DataFrame\n                Dataframe created with the `_create_train_X_y` method, first return.\n            y_train : dict\n                Dict created with the `_create_train_X_y` method, second return.\n            step : int\n                Step of the forecaster to be fitted.\n            store_in_sample_residuals : bool\n                If `True`, in-sample residuals will be stored in the forecaster object\n                after fitting (`in_sample_residuals_` attribute).\n            \n            Returns\n            -------\n            Tuple with the step, fitted regressor and in-sample residuals.\n\n            \"\"\"\n\n            X_train_step, y_train_step = self.filter_train_X_y_for_step(\n                                             step          = step,\n                                             X_train       = X_train,\n                                             y_train       = y_train,\n                                             remove_suffix = True\n                                         )\n            sample_weight = self.create_sample_weights(X_train=X_train_step)\n            if sample_weight is not None:\n                regressor.fit(\n                    X             = X_train_step,\n                    y             = y_train_step,\n                    sample_weight = sample_weight,\n                    **self.fit_kwargs\n                )\n            else:\n                regressor.fit(\n                    X = X_train_step,\n                    y = y_train_step,\n                    **self.fit_kwargs\n                )\n\n            # This is done to save time during fit in functions such as backtesting()\n            if store_in_sample_residuals:\n                residuals = (\n                    (y_train_step - regressor.predict(X_train_step))\n                ).to_numpy()\n\n                if len(residuals) > 1000:\n                    # Only up to 1000 residuals are stored\n                    rng = np.random.default_rng(seed=123)\n                    residuals = rng.choice(\n                                    a       = residuals, \n                                    size    = 1000, \n                                    replace = False\n                                )\n            else:\n                residuals = None\n\n            return step, regressor, residuals\n\n        results_fit = (\n            Parallel(n_jobs=self.n_jobs)\n            (delayed(fit_forecaster)\n            (\n                regressor                 = copy(self.regressor),\n                X_train                   = X_train,\n                y_train                   = y_train,\n                step                      = step,\n                store_in_sample_residuals = store_in_sample_residuals\n            )\n            for step in range(1, self.steps + 1))\n        )\n\n        self.regressors_ = {step: regressor \n                            for step, regressor, _ in results_fit}\n\n        if store_in_sample_residuals:\n            self.in_sample_residuals_ = {step: residuals \n                                         for step, _, residuals in results_fit}\n        \n        self.series_names_in_ = series_names_in_\n        self.X_train_series_names_in_ = X_train_series_names_in_\n        self.X_train_features_names_out_ = X_train_features_names_out_\n        \n        self.is_fitted = True\n        self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.training_range_ = preprocess_y(y=series[self.level], return_values=False)[1][[0, -1]]\n        self.index_type_ = type(X_train.index)\n        if isinstance(X_train.index, pd.DatetimeIndex):\n            self.index_freq_ = X_train.index.freqstr\n        else: \n            self.index_freq_ = X_train.index.step\n        \n        if exog is not None:\n            self.exog_in_ = True\n            self.exog_names_in_ = exog_names_in_\n            self.exog_type_in_ = type(exog)\n            self.exog_dtypes_in_ = exog_dtypes_in_\n            self.X_train_exog_names_out_ = X_train_exog_names_out_\n\n        if store_last_window:\n            self.last_window_ = series.iloc[-self.window_size:, ][\n                self.X_train_series_names_in_\n            ].copy()\n        \n        set_skforecast_warnings(suppress_warnings, action='default')\n\n\n    def _create_predict_inputs(\n        self,\n        steps: Optional[Union[int, list]] = None,\n        last_window: Optional[pd.DataFrame] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        check_inputs: bool = True\n    ) -> Tuple[list, list, list, pd.Index]:\n        \"\"\"\n        Create the inputs needed for the prediction process.\n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n\n        Returns\n        -------\n        Xs : list\n            List of numpy arrays with the predictors for each step.\n        Xs_col_names : list\n            Names of the columns of the matrix created internally for prediction.\n        steps : list\n            Steps to predict.\n        prediction_index : pandas Index\n            Index of the predictions.\n        \n        \"\"\"\n        \n        steps = prepare_steps_direct(\n                    steps    = steps,\n                    max_step = self.steps\n                )\n\n        if last_window is None:\n            last_window = self.last_window_\n        \n        if check_inputs:\n            check_predict_input(\n                forecaster_name  = type(self).__name__,\n                steps            = steps,\n                is_fitted        = self.is_fitted,\n                exog_in_         = self.exog_in_,\n                index_type_      = self.index_type_,\n                index_freq_      = self.index_freq_,\n                window_size      = self.window_size,\n                last_window      = last_window,\n                exog             = exog,\n                exog_type_in_    = self.exog_type_in_,\n                exog_names_in_   = self.exog_names_in_,\n                interval         = None,\n                max_steps        = self.steps,\n                series_names_in_ = self.X_train_series_names_in_\n            )\n\n        last_window = last_window.iloc[\n            -self.window_size:, last_window.columns.get_indexer(self.X_train_series_names_in_)\n        ].copy()\n        \n        X_autoreg = []\n        Xs_col_names = []\n        for serie in self.X_train_series_names_in_:\n            last_window_serie = transform_numpy(\n                                    array             = last_window[serie].to_numpy(),\n                                    transformer       = self.transformer_series_[serie],\n                                    fit               = False,\n                                    inverse_transform = False\n                                )\n            \n            if self.differentiation is not None:\n                last_window_serie = self.differentiator_[serie].fit_transform(last_window_serie)\n\n            if self.lags is not None:\n                X_lags = last_window_serie[-self.lags_[serie]]\n                X_autoreg.append(X_lags)\n                Xs_col_names.extend(self.lags_names[serie])\n\n            if self.window_features is not None:\n                n_diff = 0 if self.differentiation is None else self.differentiation\n                X_window_features = np.concatenate(\n                    [\n                        wf.transform(last_window_serie[n_diff:]) \n                        for wf in self.window_features\n                    ]\n                )\n                X_autoreg.append(X_window_features)\n                # HACK: This is not the best way to do it. Can have any problem\n                # if the window_features are not in the same order as the\n                # self.window_features_names.\n                Xs_col_names.extend([f\"{serie}_{wf}\" for wf in self.window_features_names])\n            \n        X_autoreg = np.concatenate(X_autoreg).reshape(1, -1)\n        _, last_window_index = preprocess_last_window(\n            last_window=last_window, return_values=False\n        )\n        if exog is not None:\n            exog = input_to_frame(data=exog, input_name='exog')\n            exog = exog.loc[:, self.exog_names_in_]\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n            check_exog_dtypes(exog=exog)\n            exog_values, _ = exog_to_direct_numpy(\n                                 exog  = exog.to_numpy()[:max(steps)],\n                                 steps = max(steps)\n                             )\n            exog_values = exog_values[0]\n            \n            n_exog = exog.shape[1]\n            Xs = [\n                np.concatenate(\n                    [\n                        X_autoreg, \n                        exog_values[(step - 1) * n_exog : step * n_exog].reshape(1, -1)\n                    ],\n                    axis=1\n                )\n                for step in steps\n            ]\n            # HACK: This is not the best way to do it. Can have any problem\n            # if the exog_columns are not in the same order as the\n            # self.window_features_names.\n            Xs_col_names = Xs_col_names + exog.columns.to_list()\n        else:\n            Xs = [X_autoreg] * len(steps)\n\n        prediction_index = expand_index(\n                               index = last_window_index,\n                               steps = max(steps)\n                           )[np.array(steps) - 1]\n        if isinstance(last_window_index, pd.DatetimeIndex) and np.array_equal(\n            steps, np.arange(min(steps), max(steps) + 1)\n        ):\n            prediction_index.freq = last_window_index.freq\n        \n        # HACK: Why no use self.X_train_features_names_out_ as Xs_col_names?\n        return Xs, Xs_col_names, steps, prediction_index\n\n\n    def create_predict_X(\n        self,\n        steps: Optional[Union[int, list]] = None,\n        last_window: Optional[pd.DataFrame] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        suppress_warnings: bool = False\n    ) -> pd.DataFrame:\n        \"\"\"\n        Create the predictors needed to predict `steps` ahead.\n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the prediction \n            process. See skforecast.exceptions.warn_skforecast_categories for more\n            information.\n\n        Returns\n        -------\n        X_predict : pandas DataFrame\n            Pandas DataFrame with the predictors for each step. The index \n            is the same as the prediction index.\n        \n        \"\"\"\n\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n\n        Xs, Xs_col_names, steps, prediction_index = self._create_predict_inputs(\n            steps=steps, last_window=last_window, exog=exog\n        )\n\n        X_predict = pd.DataFrame(\n                        data    = np.concatenate(Xs, axis=0), \n                        columns = Xs_col_names, \n                        index   = prediction_index\n                    )\n        \n        if self.transformer_series is not None or self.differentiation is not None:\n            warnings.warn(\n                \"The output matrix is in the transformed scale due to the \"\n                \"inclusion of transformations or differentiation in the Forecaster. \"\n                \"As a result, any predictions generated using this matrix will also \"\n                \"be in the transformed scale. Please refer to the documentation \"\n                \"for more details: \"\n                \"https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html\",\n                DataTransformationWarning\n            )\n        \n        set_skforecast_warnings(suppress_warnings, action='default')\n\n        return X_predict\n\n\n    def predict(\n        self,\n        steps: Optional[Union[int, list]] = None,\n        last_window: Optional[pd.DataFrame] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        suppress_warnings: bool = False,\n        check_inputs: bool = True,\n        levels: Any = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        Predict n steps ahead\n\n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the prediction \n            process. See skforecast.exceptions.warn_skforecast_categories for more\n            information.\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n        levels : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Predicted values.\n\n        \"\"\"\n\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n\n        Xs, _, steps, prediction_index = self._create_predict_inputs(\n            steps=steps, last_window=last_window, exog=exog, check_inputs=check_inputs\n        )\n\n        regressors = [self.regressors_[step] for step in steps]\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\", \n                message=\"X does not have valid feature names\", \n                category=UserWarning\n            )\n            predictions = np.array([\n                regressor.predict(X).ravel()[0] \n                for regressor, X in zip(regressors, Xs)\n            ])\n\n        if self.differentiation is not None:\n            predictions = self.differentiator_[\n                self.level\n            ].inverse_transform_next_window(predictions)\n        \n        predictions = transform_numpy(\n                          array             = predictions,\n                          transformer       = self.transformer_series_[self.level],\n                          fit               = False,\n                          inverse_transform = True\n                      )\n            \n        predictions = pd.DataFrame(\n                          data    = predictions,\n                          columns = [self.level],\n                          index   = prediction_index\n                      )\n        \n        set_skforecast_warnings(suppress_warnings, action='default')\n\n        return predictions\n\n\n    def predict_bootstrapping(\n        self,\n        steps: Optional[Union[int, list]] = None,\n        last_window: Optional[pd.DataFrame] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        suppress_warnings: bool = False,\n        levels: Any = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        Generate multiple forecasting predictions using a bootstrapping process. \n        By sampling from a collection of past observed errors (the residuals),\n        each iteration of bootstrapping generates a different set of predictions. \n        See the Notes section for more information. \n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.     \n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.               \n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the prediction \n            process. See skforecast.exceptions.warn_skforecast_categories for more\n            information.\n        levels : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        boot_predictions : pandas DataFrame\n            Predictions generated by bootstrapping.\n            Shape: (steps, n_boot)\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n        Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n        \"\"\"\n\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n\n        if self.is_fitted:\n            \n            steps = prepare_steps_direct(\n                        steps    = steps,\n                        max_step = self.steps\n                    )\n\n            if use_in_sample_residuals:\n                if not set(steps).issubset(set(self.in_sample_residuals_.keys())):\n                    raise ValueError(\n                        f\"Not `forecaster.in_sample_residuals_` for steps: \"\n                        f\"{set(steps) - set(self.in_sample_residuals_.keys())}.\"\n                    )\n                residuals = self.in_sample_residuals_\n            else:\n                if self.out_sample_residuals_ is None:\n                    raise ValueError(\n                        \"`forecaster.out_sample_residuals_` is `None`. Use \"\n                        \"`use_in_sample_residuals=True` or the \"\n                        \"`set_out_sample_residuals()` method before predicting.\"\n                    )\n                else:\n                    if not set(steps).issubset(set(self.out_sample_residuals_.keys())):\n                        raise ValueError(\n                            f\"Not `forecaster.out_sample_residuals_` for steps: \"\n                            f\"{set(steps) - set(self.out_sample_residuals_.keys())}. \"\n                            f\"Use method `set_out_sample_residuals()`.\"\n                        )\n                residuals = self.out_sample_residuals_\n            \n            check_residuals = (\n                'forecaster.in_sample_residuals_' if use_in_sample_residuals\n                else 'forecaster.out_sample_residuals_'\n            )\n            for step in steps:\n                if residuals[step] is None:\n                    raise ValueError(\n                        f\"forecaster residuals for step {step} are `None`. \"\n                        f\"Check {check_residuals}.\"\n                    )\n                elif (any(element is None for element in residuals[step]) or\n                      np.any(np.isnan(residuals[step]))):\n                    raise ValueError(\n                        f\"forecaster residuals for step {step} contains `None` \"\n                        f\"or `NaNs` values. Check {check_residuals}.\"\n                    )\n\n        Xs, _, steps, prediction_index = self._create_predict_inputs(\n            steps=steps, last_window=last_window, exog=exog\n        )\n\n        # NOTE: Predictions must be transformed and differenced before adding residuals\n        regressors = [self.regressors_[step] for step in steps]\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\", \n                message=\"X does not have valid feature names\", \n                category=UserWarning\n            )\n            predictions = np.array([\n                regressor.predict(X).ravel()[0] \n                for regressor, X in zip(regressors, Xs)\n            ])\n        \n        boot_predictions = np.tile(predictions, (n_boot, 1)).T\n        boot_columns = [f\"pred_boot_{i}\" for i in range(n_boot)]\n\n        rng = np.random.default_rng(seed=random_state)\n        for i, step in enumerate(steps):\n            sampled_residuals = residuals[step][\n                rng.integers(low=0, high=len(residuals[step]), size=n_boot)\n            ]\n            boot_predictions[i, :] = boot_predictions[i, :] + sampled_residuals\n\n        if self.differentiation is not None:\n            boot_predictions = self.differentiator_[\n                self.level\n            ].inverse_transform_next_window(boot_predictions)\n\n        if self.transformer_series_[self.level]:\n            boot_predictions = np.apply_along_axis(\n                                   func1d            = transform_numpy,\n                                   axis              = 0,\n                                   arr               = boot_predictions,\n                                   transformer       = self.transformer_series_[self.level],\n                                   fit               = False,\n                                   inverse_transform = True\n                               )\n    \n        boot_predictions = pd.DataFrame(\n                               data    = boot_predictions,\n                               index   = prediction_index,\n                               columns = boot_columns\n                           )\n\n        set_skforecast_warnings(suppress_warnings, action='default')\n        \n        return boot_predictions\n\n\n    def predict_interval(\n        self,\n        steps: Optional[Union[int, list]] = None,\n        last_window: Optional[pd.DataFrame] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        interval: list = [5, 95],\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        suppress_warnings: bool = False,\n        levels: Any = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        Bootstrapping based predicted intervals.\n        Both predictions and intervals are returned.\n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        interval : list, default `[5, 95]`\n            Confidence of the prediction interval estimated. Sequence of \n            percentiles to compute, which must be between 0 and 100 inclusive. \n            For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the prediction \n            process. See skforecast.exceptions.warn_skforecast_categories for more\n            information.\n        levels : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Values predicted by the forecaster and their estimated interval.\n\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n\n        check_interval(interval=interval)\n\n        boot_predictions = self.predict_bootstrapping(\n                               steps                   = steps,\n                               last_window             = last_window,\n                               exog                    = exog,\n                               n_boot                  = n_boot,\n                               random_state            = random_state,\n                               use_in_sample_residuals = use_in_sample_residuals\n                           )\n\n        predictions = self.predict(\n                          steps        = steps,\n                          last_window  = last_window,\n                          exog         = exog,\n                          check_inputs = False\n                      )\n\n        interval = np.array(interval) / 100\n        predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n        predictions_interval.columns = [f'{self.level}_lower_bound', f'{self.level}_upper_bound']\n        predictions = pd.concat((predictions, predictions_interval), axis=1)\n\n        set_skforecast_warnings(suppress_warnings, action='default')\n\n        return predictions\n\n\n    def predict_quantiles(\n        self,\n        steps: Optional[Union[int, list]] = None,\n        last_window: Optional[pd.DataFrame] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        quantiles: list = [0.05, 0.5, 0.95],\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        suppress_warnings: bool = False,\n        levels: Any = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        Bootstrapping based predicted quantiles.\n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        quantiles : list, default `[0.05, 0.5, 0.95]`\n            Sequence of quantiles to compute, which must be between 0 and 1 \n            inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n            `quantiles = [0.05, 0.5, 0.95]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate quantiles.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot quantiles are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create quantiles. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the prediction \n            process. See skforecast.exceptions.warn_skforecast_categories for more\n            information.\n        levels : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Quantiles predicted by the forecaster.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n\n        check_interval(quantiles=quantiles)\n\n        boot_predictions = self.predict_bootstrapping(\n                               steps                   = steps,\n                               last_window             = last_window,\n                               exog                    = exog,\n                               n_boot                  = n_boot,\n                               random_state            = random_state,\n                               use_in_sample_residuals = use_in_sample_residuals\n                           )\n\n        predictions = boot_predictions.quantile(q=quantiles, axis=1).transpose()\n        predictions.columns = [f'{self.level}_q_{q}' for q in quantiles]\n\n        set_skforecast_warnings(suppress_warnings, action='default')\n\n        return predictions\n    \n\n    def predict_dist(\n        self,\n        distribution: object,\n        steps: Optional[Union[int, list]] = None,\n        last_window: Optional[pd.DataFrame] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        suppress_warnings: bool = False,\n        levels: Any = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        Fit a given probability distribution for each step. After generating \n        multiple forecasting predictions through a bootstrapping process, each \n        step is fitted to the given distribution.\n        \n        Parameters\n        ----------\n        distribution : Object\n            A distribution object from scipy.stats.\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the prediction \n            process. See skforecast.exceptions.warn_skforecast_categories for more\n            information.\n        levels : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Distribution parameters estimated for each step.\n\n        \"\"\"\n\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n        \n        boot_samples = self.predict_bootstrapping(\n                           steps                   = steps,\n                           last_window             = last_window,\n                           exog                    = exog,\n                           n_boot                  = n_boot,\n                           random_state            = random_state,\n                           use_in_sample_residuals = use_in_sample_residuals\n                       )       \n\n        param_names = [p for p in inspect.signature(distribution._pdf).parameters \n                       if not p == 'x'] + [\"loc\", \"scale\"]\n        param_values = np.apply_along_axis(\n                           lambda x: distribution.fit(x),\n                           axis = 1,\n                           arr  = boot_samples\n                       )\n        \n        level_param_names = [f'{self.level}_{p}' for p in param_names]\n        predictions = pd.DataFrame(\n                          data    = param_values,\n                          columns = level_param_names,\n                          index   = boot_samples.index\n                      )\n\n        set_skforecast_warnings(suppress_warnings, action='default')\n\n        return predictions\n\n\n    def set_params(\n        self, \n        params: dict\n    ) -> None:\n        \"\"\"\n        Set new values to the parameters of the scikit learn model stored in the\n        forecaster. It is important to note that all models share the same \n        configuration of parameters and hyperparameters.\n        \n        Parameters\n        ----------\n        params : dict\n            Parameters values.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        self.regressor = clone(self.regressor)\n        self.regressor.set_params(**params)\n        self.regressors_ = {step: clone(self.regressor)\n                            for step in range(1, self.steps + 1)}\n\n\n    def set_fit_kwargs(\n        self, \n        fit_kwargs: dict\n    ) -> None:\n        \"\"\"\n        Set new values for the additional keyword arguments passed to the `fit` \n        method of the regressor.\n        \n        Parameters\n        ----------\n        fit_kwargs : dict\n            Dict of the form {\"argument\": new_value}.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n\n        \n    def set_lags(\n        self, \n        lags: Optional[Union[int, list, np.ndarray, range, dict]] = None\n    ) -> None:\n        \"\"\"\n        Set new value to the attribute `lags`. Attributes `lags_names`, \n        `max_lag` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        lags : int, list, numpy ndarray, range, dict, default `None`\n            Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n\n            - `int`: include lags from 1 to `lags` (included).\n            - `list`, `1d numpy ndarray` or `range`: include only lags present in \n            `lags`, all elements must be int.\n            - `dict`: create different lags for each series. {'series_column_name': lags}.\n            - `None`: no lags are included as predictors. \n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        if self.window_features is None and lags is None:\n            raise ValueError(\n                \"At least one of the arguments `lags` or `window_features` \"\n                \"must be different from None. This is required to create the \"\n                \"predictors used in training the forecaster.\"\n            )\n\n        if isinstance(lags, dict):\n            self.lags = {}\n            self.lags_names = {}\n            list_max_lags = []\n            for key in lags:\n                if lags[key] is None:\n                    self.lags[key] = None\n                    self.lags_names[key] = None\n                else:\n                    self.lags[key], lags_names, max_lag = initialize_lags(\n                        forecaster_name = type(self).__name__,\n                        lags            = lags[key]\n                    )\n                    self.lags_names[key] = (\n                        [f'{key}_{lag}' for lag in lags_names] \n                         if lags_names is not None \n                         else None\n                    )\n                    if max_lag is not None:\n                        list_max_lags.append(max_lag)\n            \n            self.max_lag = max(list_max_lags) if len(list_max_lags) != 0 else None\n        else:\n            self.lags, self.lags_names, self.max_lag = initialize_lags(\n                forecaster_name = type(self).__name__, \n                lags            = lags\n            )\n\n        # Repeated here in case of lags is a dict with all values as None\n        if self.window_features is None and (lags is None or self.max_lag is None):\n            raise ValueError(\n                \"At least one of the arguments `lags` or `window_features` \"\n                \"must be different from None. This is required to create the \"\n                \"predictors used in training the forecaster.\"\n            )\n        \n        self.window_size = max(\n            [ws for ws in [self.max_lag, self.max_size_window_features] \n             if ws is not None]\n        )\n        if self.differentiation is not None:\n            self.window_size += self.differentiation\n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_window_features(\n        self, \n        window_features: Optional[Union[object, list]] = None\n    ) -> None:\n        \"\"\"\n        Set new value to the attribute `window_features`. Attributes \n        `max_size_window_features`, `window_features_names`, \n        `window_features_class_names` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        window_features : object, list, default `None`\n            Instance or list of instances used to create window features. Window features\n            are created from the original time series and are included as predictors.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        if window_features is None and self.max_lag is None:\n            raise ValueError(\n                (\"At least one of the arguments `lags` or `window_features` \"\n                 \"must be different from None. This is required to create the \"\n                 \"predictors used in training the forecaster.\")\n            )\n        \n        self.window_features, self.window_features_names, self.max_size_window_features = (\n            initialize_window_features(window_features)\n        )\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [\n                type(wf).__name__ for wf in self.window_features\n            ] \n        self.window_size = max(\n            [ws for ws in [self.max_lag, self.max_size_window_features] \n             if ws is not None]\n        )\n        if self.differentiation is not None:\n            self.window_size += self.differentiation\n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_out_sample_residuals(\n        self,\n        y_true: dict,\n        y_pred: dict,\n        append: bool = False,\n        random_state: int = 123\n    ) -> None:\n        \"\"\"\n        Set new values to the attribute `out_sample_residuals_`. Out of sample\n        residuals are meant to be calculated using observations that did not\n        participate in the training process. `y_true` and `y_pred` are expected\n        to be in the original scale of the time series. Residuals are calculated\n        as `y_true` - `y_pred`, after applying the necessary transformations and\n        differentiations if the forecaster includes them (`self.transformer_series`\n        and `self.differentiation`).\n\n        A total of 10_000 residuals are stored in the attribute `out_sample_residuals_`.\n        If the number of residuals is greater than 10_000, a random sample of\n        10_000 residuals is stored.\n        \n        Parameters\n        ----------\n        y_true : dict\n            Dictionary of numpy ndarrays or pandas Series with the true values of\n            the time series for each model in the form {step: y_true}.\n        y_pred : dict\n            Dictionary of numpy ndarrays or pandas Series with the predicted values\n            of the time series for each model in the form {step: y_pred}.\n        append : bool, default `False`\n            If `True`, new residuals are added to the once already stored in the\n            attribute `out_sample_residuals_`. If after appending the new residuals,\n            the limit of 10_000 samples is exceeded, a random sample of 10_000 is\n            kept.\n        random_state : int, default `123`\n            Sets a seed to the random sampling for reproducible output.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n\n        if not self.is_fitted:\n            raise NotFittedError(\n                \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n                \"arguments before using `set_out_sample_residuals()`.\"\n            )\n\n        if not isinstance(y_true, dict):\n            raise TypeError(\n                f\"`y_true` must be a dictionary of numpy ndarrays or pandas Series. \"\n                f\"Got {type(y_true)}.\"\n            )\n  \n        if not isinstance(y_pred, dict):\n            raise TypeError(\n                f\"`y_pred` must be a dictionary of numpy ndarrays or pandas Series. \"\n                f\"Got {type(y_pred)}.\"\n            )\n        \n        if not set(y_true.keys()) == set(y_pred.keys()):\n            raise ValueError(\n                f\"`y_true` and `y_pred` must have the same keys. \"\n                f\"Got {set(y_true.keys())} and {set(y_pred.keys())}.\"\n            )\n        \n        for k in y_true.keys():\n            if not isinstance(y_true[k], (np.ndarray, pd.Series)):\n                raise TypeError(\n                    f\"Values of `y_true` must be numpy ndarrays or pandas Series. \"\n                    f\"Got {type(y_true[k])} for step {k}.\"\n                )\n            if not isinstance(y_pred[k], (np.ndarray, pd.Series)):\n                raise TypeError(\n                    f\"Values of `y_pred` must be numpy ndarrays or pandas Series. \"\n                    f\"Got {type(y_pred[k])} for step {k}.\"\n                )\n            if len(y_true[k]) != len(y_pred[k]):\n                raise ValueError(\n                    f\"`y_true` and `y_pred` must have the same length. \"\n                    f\"Got {len(y_true[k])} and {len(y_pred[k])} for step {k}.\"\n                )\n            if isinstance(y_true[k], pd.Series) and isinstance(y_pred[k], pd.Series):\n                if not y_true[k].index.equals(y_pred[k].index):\n                    raise ValueError(\n                        f\"When containing pandas Series, elements in `y_true` and \"\n                        f\"`y_pred` must have the same index. Error in step {k}.\"\n                    )\n        \n        if self.out_sample_residuals_ is None:\n            self.out_sample_residuals_ = {\n                step: None for step in range(1, self.steps + 1)\n            }\n        \n        steps_to_update = set(range(1, self.steps + 1)).intersection(set(y_pred.keys()))\n        if not steps_to_update:\n            raise ValueError(\n                \"Provided keys in `y_pred` and `y_true` do not match any step. \"\n                \"Residuals cannot be updated.\"\n            )\n\n        residuals = {}\n        rng = np.random.default_rng(seed=random_state)\n        y_true = y_true.copy()\n        y_pred = y_pred.copy()\n        if self.differentiation is not None:\n            differentiator = copy(self.differentiator)\n            differentiator.set_params(window_size=None)\n\n        for k in steps_to_update:\n            if isinstance(y_true[k], pd.Series):\n                y_true[k] = y_true[k].to_numpy()\n            if isinstance(y_pred[k], pd.Series):\n                y_pred[k] = y_pred[k].to_numpy()\n            if self.transformer_series:\n                y_true[k] = transform_numpy(\n                                array             = y_true[k],\n                                transformer       = self.transformer_series_[self.level],\n                                fit               = False,\n                                inverse_transform = False\n                            )\n                y_pred[k] = transform_numpy(\n                                array             = y_pred[k],\n                                transformer       = self.transformer_series_[self.level],\n                                fit               = False,\n                                inverse_transform = False\n                            )\n            if self.differentiation is not None:\n                y_true[k] = differentiator.fit_transform(y_true[k])[self.differentiation:]\n                y_pred[k] = differentiator.fit_transform(y_pred[k])[self.differentiation:]\n\n            residuals[k] = y_true[k] - y_pred[k]\n\n        for key, value in residuals.items():\n            if append and self.out_sample_residuals_[key] is not None:\n                value = np.concatenate((\n                            self.out_sample_residuals_[key],\n                            value\n                        ))\n            if len(value) > 10000:\n                value = rng.choice(value, size=10000, replace=False)\n            self.out_sample_residuals_[key] = value\n    \n    def get_feature_importances(\n        self,\n        step: int,\n        sort_importance: bool = True\n    ) -> pd.DataFrame:\n        \"\"\"\n        Return feature importance of the model stored in the forecaster for a\n        specific step. Since a separate model is created for each forecast time\n        step, it is necessary to select the model from which retrieve information.\n        Only valid when regressor stores internally the feature importances in\n        the attribute `feature_importances_` or `coef_`. Otherwise, it returns  \n        `None`.\n\n        Parameters\n        ----------\n        step : int\n            Model from which retrieve information (a separate model is created \n            for each forecast time step). First step is 1.\n        sort_importance: bool, default `True`\n            If `True`, sorts the feature importances in descending order.\n\n        Returns\n        -------\n        feature_importances : pandas DataFrame\n            Feature importances associated with each predictor.\n        \n        \"\"\"\n\n        if not isinstance(step, int):\n            raise TypeError(\n                f\"`step` must be an integer. Got {type(step)}.\"\n            )\n\n        if not self.is_fitted:\n            raise NotFittedError(\n                (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n                 \"arguments before using `get_feature_importances()`.\")\n            )\n\n        if (step < 1) or (step > self.steps):\n            raise ValueError(\n                (f\"The step must have a value from 1 to the maximum number of steps \"\n                 f\"({self.steps}). Got {step}.\")\n            )\n\n        if isinstance(self.regressor, Pipeline):\n            estimator = self.regressors_[step][-1]\n        else:\n            estimator = self.regressors_[step]\n                \n        n_lags = len(list(\n            chain(*[v for v in self.lags_.values() if v is not None])\n        ))\n        n_window_features = (\n            len(self.X_train_window_features_names_out_) if self.window_features is not None else 0\n        )\n        idx_columns_autoreg = np.arange(n_lags + n_window_features)\n        if self.exog_in_:\n            idx_columns_exog = np.flatnonzero(\n                                   [name.endswith(f\"step_{step}\")\n                                    for name in self.X_train_features_names_out_]\n                               )\n        else:\n            idx_columns_exog = np.array([], dtype=int)\n        \n        idx_columns = np.concatenate((idx_columns_autoreg, idx_columns_exog))\n        idx_columns = [int(x) for x in idx_columns]  # Required since numpy 2.0\n        feature_names = [\n            self.X_train_features_names_out_[i].replace(f\"_step_{step}\", \"\") \n            for i in idx_columns\n        ]\n\n        if hasattr(estimator, 'feature_importances_'):\n            feature_importances = estimator.feature_importances_\n        elif hasattr(estimator, 'coef_'):\n            feature_importances = estimator.coef_\n        else:\n            warnings.warn(\n                (f\"Impossible to access feature importances for regressor of type \"\n                 f\"{type(estimator)}. This method is only valid when the \"\n                 f\"regressor stores internally the feature importances in the \"\n                 f\"attribute `feature_importances_` or `coef_`.\")\n            )\n            feature_importances = None\n\n        if feature_importances is not None:\n            feature_importances = pd.DataFrame({\n                                      'feature': feature_names,\n                                      'importance': feature_importances\n                                  })\n            if sort_importance:\n                feature_importances = feature_importances.sort_values(\n                                          by='importance', ascending=False\n                                      )\n\n        return feature_importances\n",
    "skforecast/model_selection/_utils.py": "################################################################################\n#                     skforecast.model_selection._utils                        #\n#                                                                              #\n# This work by skforecast team is licensed under the BSD 3-Clause License.     #\n################################################################################\n# coding=utf-8\n\nfrom typing import Union, Tuple, Optional, Callable, Generator\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom joblib import cpu_count\nfrom tqdm.auto import tqdm\nfrom sklearn.pipeline import Pipeline\nimport sklearn.linear_model\nfrom sklearn.exceptions import NotFittedError\n\nfrom ..exceptions import IgnoredArgumentWarning\nfrom ..metrics import add_y_train_argument, _get_metric\nfrom ..utils import check_interval\n\n\ndef initialize_lags_grid(\n    forecaster: object, \n    lags_grid: Optional[Union[list, dict]] = None\n) -> Tuple[dict, str]:\n    \"\"\"\n    Initialize lags grid and lags label for model selection. \n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model. ForecasterRecursive, ForecasterDirect, \n        ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate.\n    lags_grid : list, dict, default `None`\n        Lists of lags to try, containing int, lists, numpy ndarray, or range \n        objects. If `dict`, the keys are used as labels in the `results` \n        DataFrame, and the values are used as the lists of lags to try.\n\n    Returns\n    -------\n    lags_grid : dict\n        Dictionary with lags configuration for each iteration.\n    lags_label : str\n        Label for lags representation in the results object.\n\n    \"\"\"\n\n    if not isinstance(lags_grid, (list, dict, type(None))):\n        raise TypeError(\n            (f\"`lags_grid` argument must be a list, dict or None. \"\n             f\"Got {type(lags_grid)}.\")\n        )\n\n    lags_label = 'values'\n    if isinstance(lags_grid, list):\n        lags_grid = {f'{lags}': lags for lags in lags_grid}\n    elif lags_grid is None:\n        lags = [int(lag) for lag in forecaster.lags]  # Required since numpy 2.0\n        lags_grid = {f'{lags}': lags}\n    else:\n        lags_label = 'keys'\n\n    return lags_grid, lags_label\n\n\ndef check_backtesting_input(\n    forecaster: object,\n    cv: object,\n    metric: Union[str, Callable, list],\n    add_aggregated_metric: bool = True,\n    y: Optional[pd.Series] = None,\n    series: Optional[Union[pd.DataFrame, dict]] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    interval: Optional[list] = None,\n    alpha: Optional[float] = None,\n    n_boot: int = 250,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = False,\n    n_jobs: Union[int, str] = 'auto',\n    show_progress: bool = True,\n    suppress_warnings: bool = False,\n    suppress_warnings_fit: bool = False\n) -> None:\n    \"\"\"\n    This is a helper function to check most inputs of backtesting functions in \n    modules `model_selection`.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model.\n    cv : TimeSeriesFold\n        TimeSeriesFold object with the information needed to split the data into folds.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n    add_aggregated_metric : bool, default `True`\n        If `True`, the aggregated metrics (average, weighted average and pooling)\n        over all levels are also returned (only multiseries).\n    y : pandas Series, default `None`\n        Training time series for uni-series forecasters.\n    series : pandas DataFrame, dict, default `None`\n        Training time series for multi-series forecasters.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive.\n    alpha : float, default `None`\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %. \n    n_boot : int, default `250`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of prediction \n        error to create prediction intervals.  If `False`, out_sample_residuals \n        are used if they are already stored inside the forecaster.\n    use_binned_residuals : bool, default `False`\n        If `True`, residuals used in each bootstrapping iteration are selected\n        conditioning on the predicted values. If `False`, residuals are selected\n        randomly without conditioning on the predicted values.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the fuction\n        skforecast.utils.select_n_jobs_fit_forecaster.\n        **New in version 0.9.0**\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n    suppress_warnings: bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the backtesting \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    suppress_warnings_fit : bool, default `False`\n        If `True`, warnings generated during fitting will be ignored. Only \n        `ForecasterSarimax`.\n\n    Returns\n    -------\n    None\n    \n    \"\"\"\n\n    forecaster_name = type(forecaster).__name__\n    cv_name = type(cv).__name__\n\n    if cv_name != \"TimeSeriesFold\":\n        raise TypeError(f\"`cv` must be a TimeSeriesFold object. Got {cv_name}.\")\n\n    steps = cv.steps\n    initial_train_size = cv.initial_train_size\n    gap = cv.gap\n    allow_incomplete_fold = cv.allow_incomplete_fold\n    refit = cv.refit\n\n    forecasters_uni = [\n        \"ForecasterRecursive\",\n        \"ForecasterDirect\",\n        \"ForecasterSarimax\",\n        \"ForecasterEquivalentDate\",\n    ]\n    forecasters_multi = [\n        \"ForecasterDirectMultiVariate\",\n        \"ForecasterRnn\",\n    ]\n    forecasters_multi_dict = [\n        \"ForecasterRecursiveMultiSeries\"\n    ]\n\n    if forecaster_name in forecasters_uni:\n        if not isinstance(y, pd.Series):\n            raise TypeError(\"`y` must be a pandas Series.\")\n        data_name = 'y'\n        data_length = len(y)\n\n    elif forecaster_name in forecasters_multi:\n        if not isinstance(series, pd.DataFrame):\n            raise TypeError(\"`series` must be a pandas DataFrame.\")\n        data_name = 'series'\n        data_length = len(series)\n    \n    elif forecaster_name in forecasters_multi_dict:\n        if not isinstance(series, (pd.DataFrame, dict)):\n            raise TypeError(\n                f\"`series` must be a pandas DataFrame or a dict of DataFrames or Series. \"\n                f\"Got {type(series)}.\"\n            )\n        \n        data_name = 'series'\n        if isinstance(series, dict):\n            not_valid_series = [\n                k \n                for k, v in series.items()\n                if not isinstance(v, (pd.Series, pd.DataFrame))\n            ]\n            if not_valid_series:\n                raise TypeError(\n                    f\"If `series` is a dictionary, all series must be a named \"\n                    f\"pandas Series or a pandas DataFrame with a single column. \"\n                    f\"Review series: {not_valid_series}\"\n                )\n            not_valid_index = [\n                k \n                for k, v in series.items()\n                if not isinstance(v.index, pd.DatetimeIndex)\n            ]\n            if not_valid_index:\n                raise ValueError(\n                    f\"If `series` is a dictionary, all series must have a Pandas \"\n                    f\"DatetimeIndex as index with the same frequency. \"\n                    f\"Review series: {not_valid_index}\"\n                )\n\n            indexes_freq = [f'{v.index.freq}' for v in series.values()]\n            indexes_freq = sorted(set(indexes_freq))\n            if not len(indexes_freq) == 1:\n                raise ValueError(\n                    f\"If `series` is a dictionary, all series must have a Pandas \"\n                    f\"DatetimeIndex as index with the same frequency. \"\n                    f\"Found frequencies: {indexes_freq}\"\n                )\n            data_length = max([len(series[serie]) for serie in series])\n        else:\n            data_length = len(series)\n\n    if exog is not None:\n        if forecaster_name in forecasters_multi_dict:\n            if not isinstance(exog, (pd.Series, pd.DataFrame, dict)):\n                raise TypeError(\n                    f\"`exog` must be a pandas Series, DataFrame, dictionary of pandas \"\n                    f\"Series/DataFrames or None. Got {type(exog)}.\"\n                )\n            if isinstance(exog, dict):\n                not_valid_exog = [\n                    k \n                    for k, v in exog.items()\n                    if not isinstance(v, (pd.Series, pd.DataFrame, type(None)))\n                ]\n                if not_valid_exog:\n                    raise TypeError(\n                        f\"If `exog` is a dictionary, All exog must be a named pandas \"\n                        f\"Series, a pandas DataFrame or None. Review exog: {not_valid_exog}\"\n                    )\n        else:\n            if not isinstance(exog, (pd.Series, pd.DataFrame)):\n                raise TypeError(\n                    f\"`exog` must be a pandas Series, DataFrame or None. Got {type(exog)}.\"\n                )\n\n    if hasattr(forecaster, 'differentiation'):\n        if forecaster.differentiation != cv.differentiation:\n            raise ValueError(\n                f\"The differentiation included in the forecaster \"\n                f\"({forecaster.differentiation}) differs from the differentiation \"\n                f\"included in the cv ({cv.differentiation}). Set the same value \"\n                f\"for both using the `differentiation` argument.\"\n            )\n\n    if not isinstance(metric, (str, Callable, list)):\n        raise TypeError(\n            f\"`metric` must be a string, a callable function, or a list containing \"\n            f\"multiple strings and/or callables. Got {type(metric)}.\"\n        )\n\n    if forecaster_name == \"ForecasterEquivalentDate\" and isinstance(\n        forecaster.offset, pd.tseries.offsets.DateOffset\n    ):\n        if initial_train_size is None:\n            raise ValueError(\n                f\"`initial_train_size` must be an integer greater than \"\n                f\"the `window_size` of the forecaster ({forecaster.window_size}) \"\n                f\"and smaller than the length of `{data_name}` ({data_length}).\"\n            )\n    elif initial_train_size is not None:\n        if initial_train_size < forecaster.window_size or initial_train_size >= data_length:\n            raise ValueError(\n                f\"If used, `initial_train_size` must be an integer greater than \"\n                f\"the `window_size` of the forecaster ({forecaster.window_size}) \"\n                f\"and smaller than the length of `{data_name}` ({data_length}).\"\n            )\n        if initial_train_size + gap >= data_length:\n            raise ValueError(\n                f\"The combination of initial_train_size {initial_train_size} and \"\n                f\"gap {gap} cannot be greater than the length of `{data_name}` \"\n                f\"({data_length}).\"\n            )\n    else:\n        if forecaster_name in ['ForecasterSarimax', 'ForecasterEquivalentDate']:\n            raise ValueError(\n                f\"`initial_train_size` must be an integer smaller than the \"\n                f\"length of `{data_name}` ({data_length}).\"\n            )\n        else:\n            if not forecaster.is_fitted:\n                raise NotFittedError(\n                    \"`forecaster` must be already trained if no `initial_train_size` \"\n                    \"is provided.\"\n                )\n            if refit:\n                raise ValueError(\n                    \"`refit` is only allowed when `initial_train_size` is not `None`.\"\n                )\n\n    if forecaster_name == 'ForecasterSarimax' and cv.skip_folds is not None:\n        raise ValueError(\n            \"`skip_folds` is not allowed for ForecasterSarimax. Set it to `None`.\"\n        )\n\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError(\"`add_aggregated_metric` must be a boolean: `True`, `False`.\")\n    if not isinstance(n_boot, (int, np.integer)) or n_boot < 0:\n        raise TypeError(f\"`n_boot` must be an integer greater than 0. Got {n_boot}.\")\n    if not isinstance(random_state, (int, np.integer)) or random_state < 0:\n        raise TypeError(f\"`random_state` must be an integer greater than 0. Got {random_state}.\")\n    if not isinstance(use_in_sample_residuals, bool):\n        raise TypeError(\"`use_in_sample_residuals` must be a boolean: `True`, `False`.\")\n    if not isinstance(use_binned_residuals, bool):\n        raise TypeError(\"`use_binned_residuals` must be a boolean: `True`, `False`.\")\n    if not isinstance(n_jobs, int) and n_jobs != 'auto':\n        raise TypeError(f\"`n_jobs` must be an integer or `'auto'`. Got {n_jobs}.\")\n    if not isinstance(show_progress, bool):\n        raise TypeError(\"`show_progress` must be a boolean: `True`, `False`.\")\n    if not isinstance(suppress_warnings, bool):\n        raise TypeError(\"`suppress_warnings` must be a boolean: `True`, `False`.\")\n    if not isinstance(suppress_warnings_fit, bool):\n        raise TypeError(\"`suppress_warnings_fit` must be a boolean: `True`, `False`.\")\n\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n\n    if not allow_incomplete_fold and data_length - (initial_train_size + gap) < steps:\n        raise ValueError(\n            f\"There is not enough data to evaluate {steps} steps in a single \"\n            f\"fold. Set `allow_incomplete_fold` to `True` to allow incomplete folds.\\n\"\n            f\"    Data available for test : {data_length - (initial_train_size + gap)}\\n\"\n            f\"    Steps                   : {steps}\"\n        )\n\n\ndef select_n_jobs_backtesting(\n    forecaster: object,\n    refit: Union[bool, int]\n) -> int:\n    \"\"\"\n    Select the optimal number of jobs to use in the backtesting process. This\n    selection is based on heuristics and is not guaranteed to be optimal.\n\n    The number of jobs is chosen as follows:\n\n    - If `refit` is an integer, then `n_jobs = 1`. This is because parallelization doesn't \n    work with intermittent refit.\n    - If forecaster is 'ForecasterRecursive' and regressor is a linear regressor, \n    then `n_jobs = 1`.\n    - If forecaster is 'ForecasterRecursive' and regressor is not a linear \n    regressor then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and `refit = True`, then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and `refit = False`, then `n_jobs = 1`.\n    - If forecaster is 'ForecasterRecursiveMultiSeries', then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterSarimax' or 'ForecasterEquivalentDate', \n    then `n_jobs = 1`.\n    - If regressor is a `LGBMRegressor(n_jobs=1)`, then `n_jobs = cpu_count() - 1`.\n    - If regressor is a `LGBMRegressor` with internal n_jobs != 1, then `n_jobs = 1`.\n    This is because `lightgbm` is highly optimized for gradient boosting and\n    parallelizes operations at a very fine-grained level, making additional\n    parallelization unnecessary and potentially harmful due to resource contention.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model.\n    refit : bool, int\n        If the forecaster is refitted during the backtesting process.\n\n    Returns\n    -------\n    n_jobs : int\n        The number of jobs to run in parallel.\n    \n    \"\"\"\n\n    forecaster_name = type(forecaster).__name__\n\n    if isinstance(forecaster.regressor, Pipeline):\n        regressor = forecaster.regressor[-1]\n        regressor_name = type(regressor).__name__\n    else:\n        regressor = forecaster.regressor\n        regressor_name = type(regressor).__name__\n\n    linear_regressors = [\n        regressor_name\n        for regressor_name in dir(sklearn.linear_model)\n        if not regressor_name.startswith('_')\n    ]\n\n    refit = False if refit == 0 else refit\n    if not isinstance(refit, bool) and refit != 1:\n        n_jobs = 1\n    else:\n        if forecaster_name in ['ForecasterRecursive']:\n            if regressor_name in linear_regressors:\n                n_jobs = 1\n            elif regressor_name == 'LGBMRegressor':\n                n_jobs = cpu_count() - 1 if regressor.n_jobs == 1 else 1\n            else:\n                n_jobs = cpu_count() - 1\n        elif forecaster_name in ['ForecasterDirect', 'ForecasterDirectMultiVariate']:\n            # Parallelization is applied during the fitting process.\n            n_jobs = 1\n        elif forecaster_name in ['ForecasterRecursiveMultiSeries']:\n            if regressor_name == 'LGBMRegressor':\n                n_jobs = cpu_count() - 1 if regressor.n_jobs == 1 else 1\n            else:\n                n_jobs = cpu_count() - 1\n        elif forecaster_name in ['ForecasterSarimax', 'ForecasterEquivalentDate']:\n            n_jobs = 1\n        else:\n            n_jobs = 1\n\n    return n_jobs\n\n\ndef _calculate_metrics_one_step_ahead(\n    forecaster: object,\n    y: pd.Series,\n    metrics: list,\n    X_train: pd.DataFrame,\n    y_train: Union[pd.Series, dict],\n    X_test: pd.DataFrame,\n    y_test: Union[pd.Series, dict]\n) -> list:\n    \"\"\"\n    Calculate metrics when predictions are one-step-ahead. When forecaster is\n    of type ForecasterDirect only the regressor for step 1 is used.\n\n    Parameters\n    ----------\n    forecaster : object\n        Forecaster model.\n    y : pandas Series\n        Time series data used to train and test the model.\n    metrics : list\n        List of metrics.\n    X_train : pandas DataFrame\n        Predictor values used to train the model.\n    y_train : pandas Series\n        Target values related to each row of `X_train`.\n    X_test : pandas DataFrame\n        Predictor values used to test the model.\n    y_test : pandas Series\n        Target values related to each row of `X_test`.\n\n    Returns\n    -------\n    metric_values : list\n        List with metric values.\n    \n    \"\"\"\n\n    if type(forecaster).__name__ == 'ForecasterDirect':\n\n        step = 1  # Only the model for step 1 is optimized.\n        X_train, y_train = forecaster.filter_train_X_y_for_step(\n                               step    = step,\n                               X_train = X_train,\n                               y_train = y_train\n                           )\n        X_test, y_test = forecaster.filter_train_X_y_for_step(\n                             step    = step,  \n                             X_train = X_test,\n                             y_train = y_test\n                         )\n        forecaster.regressors_[step].fit(X_train, y_train)\n        y_pred = forecaster.regressors_[step].predict(X_test)\n\n    else:\n        forecaster.regressor.fit(X_train, y_train)\n        y_pred = forecaster.regressor.predict(X_test)\n\n    y_true = y_test.to_numpy()\n    y_pred = y_pred.ravel()\n    y_train = y_train.to_numpy()\n\n    if forecaster.differentiation is not None:\n        y_true = forecaster.differentiator.inverse_transform_next_window(y_true)\n        y_pred = forecaster.differentiator.inverse_transform_next_window(y_pred)\n        y_train = forecaster.differentiator.inverse_transform_training(y_train)\n\n    if forecaster.transformer_y is not None:\n        y_true = forecaster.transformer_y.inverse_transform(y_true.reshape(-1, 1))\n        y_pred = forecaster.transformer_y.inverse_transform(y_pred.reshape(-1, 1))\n        y_train = forecaster.transformer_y.inverse_transform(y_train.reshape(-1, 1))\n\n    metric_values = []\n    for m in metrics:\n        metric_values.append(\n            m(y_true=y_true.ravel(), y_pred=y_pred.ravel(), y_train=y_train.ravel())\n        )\n\n    return metric_values\n\n\ndef _initialize_levels_model_selection_multiseries(\n    forecaster: object, \n    series: Union[pd.DataFrame, dict],\n    levels: Optional[Union[str, list]] = None\n) -> list:\n    \"\"\"\n    Initialize levels for model_selection multi-series functions.\n\n    Parameters\n    ----------\n    forecaster : ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate, ForecasterRnn\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Training time series.\n    levels : str, list, default `None`\n        level (`str`) or levels (`list`) at which the forecaster is optimized. \n        If `None`, all levels are taken into account. The resulting metric will be\n        the average of the optimization of all levels.\n\n    Returns\n    -------\n    levels : list\n        List of levels to be used in model_selection multi-series functions.\n    \n    \"\"\"\n\n    multi_series_forecasters_with_levels = [\n        'ForecasterRecursiveMultiSeries', \n        'ForecasterRnn'\n    ]\n\n    if type(forecaster).__name__ in multi_series_forecasters_with_levels  \\\n        and not isinstance(levels, (str, list, type(None))):\n        raise TypeError(\n            (f\"`levels` must be a `list` of column names, a `str` of a column \"\n             f\"name or `None` when using a forecaster of type \"\n             f\"{multi_series_forecasters_with_levels}. If the forecaster is of \"\n             f\"type `ForecasterDirectMultiVariate`, this argument is ignored.\")\n        )\n\n    if type(forecaster).__name__ == 'ForecasterDirectMultiVariate':\n        if levels and levels != forecaster.level and levels != [forecaster.level]:\n            warnings.warn(\n                (f\"`levels` argument have no use when the forecaster is of type \"\n                 f\"`ForecasterDirectMultiVariate`. The level of this forecaster \"\n                 f\"is '{forecaster.level}', to predict another level, change \"\n                 f\"the `level` argument when initializing the forecaster. \\n\"),\n                 IgnoredArgumentWarning\n            )\n        levels = [forecaster.level]\n    else:\n        if levels is None:\n            # Forecaster could be untrained, so self.series_col_names cannot be used.\n            if isinstance(series, pd.DataFrame):\n                levels = list(series.columns)\n            else:\n                levels = list(series.keys())\n        elif isinstance(levels, str):\n            levels = [levels]\n\n    return levels\n\n\ndef _extract_data_folds_multiseries(\n    series: Union[pd.Series, pd.DataFrame, dict],\n    folds: list,\n    span_index: Union[pd.DatetimeIndex, pd.RangeIndex],\n    window_size: int,\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    dropna_last_window: bool = False,\n    externally_fitted: bool = False\n) -> Generator[\n        Tuple[\n            Union[pd.Series, pd.DataFrame, dict],\n            pd.DataFrame,\n            list,\n            Optional[Union[pd.Series, pd.DataFrame, dict]],\n            Optional[Union[pd.Series, pd.DataFrame, dict]],\n            list\n        ],\n        None,\n        None\n    ]:\n    \"\"\"\n    Select the data from series and exog that corresponds to each fold created using the\n    skforecast.model_selection._create_backtesting_folds function.\n\n    Parameters\n    ----------\n    series : pandas Series, pandas DataFrame, dict\n        Time series.\n    folds : list\n        Folds created using the skforecast.model_selection._create_backtesting_folds\n        function.\n    span_index : pandas DatetimeIndex, pandas RangeIndex\n        Full index from the minimum to the maximum index among all series.\n    window_size : int\n        Size of the window needed to create the predictors.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    dropna_last_window : bool, default `False`\n        If `True`, drop the columns of the last window that have NaN values.\n    externally_fitted : bool, default `False`\n        Flag indicating whether the forecaster is already trained. Only used when \n        `initial_train_size` is None and `refit` is False.\n\n    Yield\n    -----\n    series_train : pandas Series, pandas DataFrame, dict\n        Time series corresponding to the training set of the fold.\n    series_last_window: pandas DataFrame\n        Time series corresponding to the last window of the fold.\n    levels_last_window: list\n        Levels of the time series present in the last window of the fold.\n    exog_train: pandas Series, pandas DataFrame, dict, None\n        Exogenous variable corresponding to the training set of the fold.\n    exog_test: pandas Series, pandas DataFrame, dict, None\n        Exogenous variable corresponding to the test set of the fold.\n    fold: list\n        Fold created using the skforecast.model_selection._create_backtesting_folds\n\n    \"\"\"\n\n    for fold in folds:\n        train_iloc_start       = fold[0][0]\n        train_iloc_end         = fold[0][1]\n        last_window_iloc_start = fold[1][0]\n        last_window_iloc_end   = fold[1][1]\n        test_iloc_start        = fold[2][0]\n        test_iloc_end          = fold[2][1]\n\n        if isinstance(series, dict) or isinstance(exog, dict):\n            # Substract 1 to the iloc indexes to get the loc indexes\n            train_loc_start       = span_index[train_iloc_start]\n            train_loc_end         = span_index[train_iloc_end - 1]\n            last_window_loc_start = span_index[last_window_iloc_start]\n            last_window_loc_end   = span_index[last_window_iloc_end - 1]\n            test_loc_start        = span_index[test_iloc_start]\n            test_loc_end          = span_index[test_iloc_end - 1]\n\n        if isinstance(series, pd.DataFrame):\n            series_train = series.iloc[train_iloc_start:train_iloc_end, ]\n\n            series_to_drop = []\n            for col in series_train.columns:\n                if series_train[col].isna().all():\n                    series_to_drop.append(col)\n                else:\n                    first_valid_index = series_train[col].first_valid_index()\n                    last_valid_index = series_train[col].last_valid_index()\n                    if (\n                        len(series_train[col].loc[first_valid_index:last_valid_index])\n                        < window_size\n                    ):\n                        series_to_drop.append(col)\n\n            series_last_window = series.iloc[\n                last_window_iloc_start:last_window_iloc_end,\n            ]\n            \n            series_train = series_train.drop(columns=series_to_drop)\n            if not externally_fitted:\n                series_last_window = series_last_window.drop(columns=series_to_drop)\n        else:\n            series_train = {}\n            for k in series.keys():\n                v = series[k].loc[train_loc_start:train_loc_end]\n                if not v.isna().all():\n                    first_valid_index = v.first_valid_index()\n                    last_valid_index  = v.last_valid_index()\n                    if first_valid_index is not None and last_valid_index is not None:\n                        v = v.loc[first_valid_index : last_valid_index]\n                        if len(v) >= window_size:\n                            series_train[k] = v\n\n            series_last_window = {}\n            for k, v in series.items():\n                v = series[k].loc[last_window_loc_start:last_window_loc_end]\n                if ((externally_fitted or k in series_train) and len(v) >= window_size):\n                    series_last_window[k] = v\n\n            series_last_window = pd.DataFrame(series_last_window)\n\n        if dropna_last_window:\n            series_last_window = series_last_window.dropna(axis=1, how=\"any\")\n            # TODO: add the option to drop the series without minimum non NaN values.\n            # Similar to how pandas does in the rolling window function.\n        \n        levels_last_window = list(series_last_window.columns)\n\n        if exog is not None:\n            if isinstance(exog, (pd.Series, pd.DataFrame)):\n                exog_train = exog.iloc[train_iloc_start:train_iloc_end, ]\n                exog_test = exog.iloc[test_iloc_start:test_iloc_end, ]\n            else:\n                exog_train = {\n                    k: v.loc[train_loc_start:train_loc_end] \n                    for k, v in exog.items()\n                }\n                exog_train = {k: v for k, v in exog_train.items() if len(v) > 0}\n\n                exog_test = {\n                    k: v.loc[test_loc_start:test_loc_end]\n                    for k, v in exog.items()\n                    if externally_fitted or k in exog_train\n                }\n\n                exog_test = {k: v for k, v in exog_test.items() if len(v) > 0}\n        else:\n            exog_train = None\n            exog_test = None\n\n        yield series_train, series_last_window, levels_last_window, exog_train, exog_test, fold\n\n\ndef _calculate_metrics_backtesting_multiseries(\n    series: Union[pd.DataFrame, dict],\n    predictions: pd.DataFrame,\n    folds: Union[list, tqdm],\n    span_index: Union[pd.DatetimeIndex, pd.RangeIndex],\n    window_size: int,\n    metrics: list,\n    levels: list,\n    add_aggregated_metric: bool = True\n) -> pd.DataFrame:\n    \"\"\"   \n    Calculate metrics for each level and also for all levels aggregated using\n    average, weighted average or pooling.\n\n    - 'average': the average (arithmetic mean) of all levels.\n    - 'weighted_average': the average of the metrics weighted by the number of\n    predicted values of each level.\n    - 'pooling': the values of all levels are pooled and then the metric is\n    calculated.\n\n    Parameters\n    ----------\n    series : pandas DataFrame, dict\n        Series data used for backtesting.\n    predictions : pandas DataFrame\n        Predictions generated during the backtesting process.\n    folds : list, tqdm\n        Folds created during the backtesting process.\n    span_index : pandas DatetimeIndex, pandas RangeIndex\n        Full index from the minimum to the maximum index among all series.\n    window_size : int\n        Size of the window used by the forecaster to create the predictors.\n        This is used remove the first `window_size` (differentiation included) \n        values from y_train since they are not part of the training matrix.\n    metrics : list\n        List of metrics to calculate.\n    levels : list\n        Levels to calculate the metrics.\n    add_aggregated_metric : bool, default `True`\n        If `True`, and multiple series (`levels`) are predicted, the aggregated\n        metrics (average, weighted average and pooled) are also returned.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s).\n    \n    \"\"\"\n\n    if not isinstance(series, (pd.DataFrame, dict)):\n        raise TypeError(\n            (\"`series` must be a pandas DataFrame or a dictionary of pandas \"\n             \"DataFrames.\")\n        )\n    if not isinstance(predictions, pd.DataFrame):\n        raise TypeError(\"`predictions` must be a pandas DataFrame.\")\n    if not isinstance(folds, (list, tqdm)):\n        raise TypeError(\"`folds` must be a list or a tqdm object.\")\n    if not isinstance(span_index, (pd.DatetimeIndex, pd.RangeIndex)):\n        raise TypeError(\"`span_index` must be a pandas DatetimeIndex or pandas RangeIndex.\")\n    if not isinstance(window_size, (int, np.integer)):\n        raise TypeError(\"`window_size` must be an integer.\")\n    if not isinstance(metrics, list):\n        raise TypeError(\"`metrics` must be a list.\")\n    if not isinstance(levels, list):\n        raise TypeError(\"`levels` must be a list.\")\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError(\"`add_aggregated_metric` must be a boolean.\")\n    \n    metric_names = [(m if isinstance(m, str) else m.__name__) for m in metrics]\n\n    y_true_pred_levels = []\n    y_train_levels = []\n    for level in levels:\n        y_true_pred_level = None\n        y_train = None\n        if level in predictions.columns:\n            # TODO: avoid merges inside the loop, instead merge outside and then filter\n            y_true_pred_level = pd.merge(\n                series[level],\n                predictions[level],\n                left_index  = True,\n                right_index = True,\n                how         = \"inner\",\n            ).dropna(axis=0, how=\"any\")\n            y_true_pred_level.columns = ['y_true', 'y_pred']\n\n            train_indexes = []\n            for i, fold in enumerate(folds):\n                fit_fold = fold[-1]\n                if i == 0 or fit_fold:\n                    train_iloc_start = fold[0][0]\n                    train_iloc_end = fold[0][1]\n                    train_indexes.append(np.arange(train_iloc_start, train_iloc_end))\n            train_indexes = np.unique(np.concatenate(train_indexes))\n            train_indexes = span_index[train_indexes]\n            y_train = series[level].loc[series[level].index.intersection(train_indexes)]\n\n        y_true_pred_levels.append(y_true_pred_level)\n        y_train_levels.append(y_train)\n            \n    metrics_levels = []\n    for i, level in enumerate(levels):\n        if y_true_pred_levels[i] is not None and not y_true_pred_levels[i].empty:\n            metrics_level = [\n                m(\n                    y_true = y_true_pred_levels[i].iloc[:, 0],\n                    y_pred = y_true_pred_levels[i].iloc[:, 1],\n                    y_train = y_train_levels[i].iloc[window_size:]  # Exclude observations used to create predictors\n                )\n                for m in metrics\n            ]\n            metrics_levels.append(metrics_level)\n        else:\n            metrics_levels.append([None for _ in metrics])\n\n    metrics_levels = pd.DataFrame(\n                         data    = metrics_levels,\n                         columns = [m if isinstance(m, str) else m.__name__\n                                    for m in metrics]\n                     )\n    metrics_levels.insert(0, 'levels', levels)\n\n    if len(levels) < 2:\n        add_aggregated_metric = False\n    \n    if add_aggregated_metric:\n\n        # aggragation: average\n        average = metrics_levels.drop(columns='levels').mean(skipna=True)\n        average = average.to_frame().transpose()\n        average['levels'] = 'average'\n\n        # aggregation: weighted_average\n        weighted_averages = {}\n        n_predictions_levels = (\n            predictions\n            .notna()\n            .sum()\n            .to_frame(name='n_predictions')\n            .reset_index(names='levels')\n        )\n        metrics_levels_no_missing = (\n            metrics_levels.merge(n_predictions_levels, on='levels', how='inner')\n        )\n        for col in metric_names:\n            weighted_averages[col] = np.average(\n                metrics_levels_no_missing[col],\n                weights=metrics_levels_no_missing['n_predictions']\n            )\n        weighted_average = pd.DataFrame(weighted_averages, index=[0])\n        weighted_average['levels'] = 'weighted_average'\n\n        # aggregation: pooling\n        y_true_pred_levels, y_train_levels = zip(\n            *[\n                (a, b.iloc[window_size:])  # Exclude observations used to create predictors\n                for a, b in zip(y_true_pred_levels, y_train_levels)\n                if a is not None\n            ]\n        )\n        y_train_levels = list(y_train_levels)\n        y_true_pred_levels = pd.concat(y_true_pred_levels)\n        y_train_levels_concat = pd.concat(y_train_levels)\n\n        pooled = []\n        for m, m_name in zip(metrics, metric_names):\n            if m_name in ['mean_absolute_scaled_error', 'root_mean_squared_scaled_error']:\n                pooled.append(\n                    m(\n                        y_true = y_true_pred_levels.loc[:, 'y_true'],\n                        y_pred = y_true_pred_levels.loc[:, 'y_pred'],\n                        y_train = y_train_levels\n                    )\n                )\n            else:\n                pooled.append(\n                    m(\n                        y_true = y_true_pred_levels.loc[:, 'y_true'],\n                        y_pred = y_true_pred_levels.loc[:, 'y_pred'],\n                        y_train = y_train_levels_concat\n                    )\n                )\n        pooled = pd.DataFrame([pooled], columns=metric_names)\n        pooled['levels'] = 'pooling'\n\n        metrics_levels = pd.concat(\n            [metrics_levels, average, weighted_average, pooled],\n            axis=0,\n            ignore_index=True\n        )\n\n    return metrics_levels\n\n\ndef _predict_and_calculate_metrics_one_step_ahead_multiseries(\n    forecaster: object,\n    series: Union[pd.DataFrame, dict],\n    X_train: pd.DataFrame,\n    y_train: Union[pd.Series, dict],\n    X_test: pd.DataFrame,\n    y_test: Union[pd.Series, dict],\n    X_train_encoding: pd.Series,\n    X_test_encoding: pd.Series,\n    levels: list,\n    metrics: list,\n    add_aggregated_metric: bool = True\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"   \n    One-step-ahead predictions and metrics for each level and also for all levels\n    aggregated using average, weighted average or pooling.\n    Input matrices (X_train, y_train, X_train_encoding, X_test, y_test, X_test_encoding)\n    should have been generated using the forecaster._train_test_split_one_step_ahead().\n\n    - 'average': the average (arithmetic mean) of all levels.\n    - 'weighted_average': the average of the metrics weighted by the number of\n    predicted values of each level.\n    - 'pooling': the values of all levels are pooled and then the metric is\n    calculated.\n\n    Parameters\n    ----------\n    forecaster : object\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Series data used to train and test the forecaster.\n    X_train : pandas DataFrame\n        Training matrix.\n    y_train : pandas Series, dict\n        Target values of the training set.\n    X_test : pandas DataFrame\n        Test matrix.\n    y_test : pandas Series, dict\n        Target values of the test set.\n    X_train_encoding : pandas Series\n        Series identifiers for each row of `X_train`.\n    X_test_encoding : pandas Series\n        Series identifiers for each row of `X_test`.\n    levels : list\n        Levels to calculate the metrics.\n    metrics : list\n        List of metrics to calculate.\n    add_aggregated_metric : bool, default `True`\n        If `True`, and multiple series (`levels`) are predicted, the aggregated\n        metrics (average, weighted average and pooled) are also returned.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s).\n    predictions : pandas DataFrame\n        Value of predictions for each level.\n    \n    \"\"\"\n\n    if not isinstance(series, (pd.DataFrame, dict)):\n        raise TypeError(\n            \"`series` must be a pandas DataFrame or a dictionary of pandas \"\n            \"DataFrames.\"\n        )\n    if not isinstance(X_train, pd.DataFrame):\n        raise TypeError(f\"`X_train` must be a pandas DataFrame. Got: {type(X_train)}\")\n    if not isinstance(y_train, (pd.Series, dict)):\n        raise TypeError(\n            f\"`y_train` must be a pandas Series or a dictionary of pandas Series. \"\n            f\"Got: {type(y_train)}\"\n        )        \n    if not isinstance(X_test, pd.DataFrame):\n        raise TypeError(f\"`X_test` must be a pandas DataFrame. Got: {type(X_test)}\")\n    if not isinstance(y_test, (pd.Series, dict)):\n        raise TypeError(\n            f\"`y_test` must be a pandas Series or a dictionary of pandas Series. \"\n            f\"Got: {type(y_test)}\"\n        )\n    if not isinstance(X_train_encoding, pd.Series):\n        raise TypeError(\n            f\"`X_train_encoding` must be a pandas Series. Got: {type(X_train_encoding)}\"\n        )\n    if not isinstance(X_test_encoding, pd.Series):\n        raise TypeError(\n            f\"`X_test_encoding` must be a pandas Series. Got: {type(X_test_encoding)}\"\n        )\n    if not isinstance(levels, list):\n        raise TypeError(f\"`levels` must be a list. Got: {type(levels)}\")\n    if not isinstance(metrics, list):\n        raise TypeError(f\"`metrics` must be a list. Got: {type(metrics)}\")\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError(\n            f\"`add_aggregated_metric` must be a boolean. Got: {type(add_aggregated_metric)}\"\n        )\n    \n    metrics = [\n        _get_metric(metric=m)\n        if isinstance(m, str)\n        else add_y_train_argument(m) \n        for m in metrics\n    ]\n    metric_names = [(m if isinstance(m, str) else m.__name__) for m in metrics]\n\n    if isinstance(series[levels[0]].index, pd.DatetimeIndex):\n        freq = series[levels[0]].index.freq\n    else:\n        freq = series[levels[0]].index.step\n\n    if type(forecaster).__name__ == 'ForecasterDirectMultiVariate':\n        step = 1\n        X_train, y_train = forecaster.filter_train_X_y_for_step(\n                               step    = step,\n                               X_train = X_train,\n                               y_train = y_train\n                           )\n        X_test, y_test = forecaster.filter_train_X_y_for_step(\n                             step    = step,  \n                             X_train = X_test,\n                             y_train = y_test\n                         )                 \n        forecaster.regressors_[step].fit(X_train, y_train)\n        pred = forecaster.regressors_[step].predict(X_test)\n    else:\n        forecaster.regressor.fit(X_train, y_train)\n        pred = forecaster.regressor.predict(X_test)\n\n    predictions_per_level = pd.DataFrame(\n        {\n            'y_true': y_test,\n            'y_pred': pred,\n            '_level_skforecast': X_test_encoding,\n        },\n        index=y_test.index,\n    ).groupby('_level_skforecast')\n    predictions_per_level = {key: group for key, group in predictions_per_level}\n\n    y_train_per_level = pd.DataFrame(\n        {\"y_train\": y_train, \"_level_skforecast\": X_train_encoding},\n        index=y_train.index,\n    ).groupby(\"_level_skforecast\")\n    # Interleaved Nan values were excluded fom y_train. They are reestored\n    y_train_per_level = {key: group.asfreq(freq) for key, group in y_train_per_level}\n\n    if forecaster.differentiation is not None:\n        for level in predictions_per_level:\n            predictions_per_level[level][\"y_true\"] = (\n                forecaster.differentiator_[level].inverse_transform_next_window(\n                    predictions_per_level[level][\"y_true\"].to_numpy()\n                )\n            )\n            predictions_per_level[level][\"y_pred\"] = (\n                forecaster.differentiator_[level].inverse_transform_next_window(\n                    predictions_per_level[level][\"y_pred\"].to_numpy()\n                )   \n            )\n            y_train_per_level[level][\"y_train\"] = (\n                forecaster.differentiator_[level].inverse_transform_training(\n                    y_train_per_level[level][\"y_train\"].to_numpy()\n                )\n            )\n\n    if forecaster.transformer_series is not None:\n        for level in predictions_per_level:\n            transformer = forecaster.transformer_series_[level]\n            predictions_per_level[level][\"y_true\"] = transformer.inverse_transform(\n                predictions_per_level[level][[\"y_true\"]]\n            )\n            predictions_per_level[level][\"y_pred\"] = transformer.inverse_transform(\n                predictions_per_level[level][[\"y_pred\"]]\n            )\n            y_train_per_level[level][\"y_train\"] = transformer.inverse_transform(\n                y_train_per_level[level][[\"y_train\"]]\n            )\n    \n    metrics_levels = []\n    for level in levels:\n        if level in predictions_per_level:\n            metrics_level = [\n                m(\n                    y_true  = predictions_per_level[level].loc[:, 'y_true'],\n                    y_pred  = predictions_per_level[level].loc[:, 'y_pred'],\n                    y_train = y_train_per_level[level].loc[:, 'y_train']\n                )\n                for m in metrics\n            ]\n            metrics_levels.append(metrics_level)\n        else:\n            metrics_levels.append([None for _ in metrics])\n\n    metrics_levels = pd.DataFrame(\n                         data    = metrics_levels,\n                         columns = [m if isinstance(m, str) else m.__name__\n                                    for m in metrics]\n                     )\n    metrics_levels.insert(0, 'levels', levels)\n\n    if len(levels) < 2:\n        add_aggregated_metric = False\n\n    if add_aggregated_metric:\n\n        # aggragation: average\n        average = metrics_levels.drop(columns='levels').mean(skipna=True)\n        average = average.to_frame().transpose()\n        average['levels'] = 'average'\n\n        # aggregation: weighted_average\n        weighted_averages = {}\n        n_predictions_levels = {\n            k: v['y_pred'].notna().sum()\n            for k, v in predictions_per_level.items()\n        }\n        n_predictions_levels = pd.DataFrame(\n            n_predictions_levels.items(),\n            columns=['levels', 'n_predictions']\n        )\n        metrics_levels_no_missing = (\n            metrics_levels.merge(n_predictions_levels, on='levels', how='inner')\n        )\n        for col in metric_names:\n            weighted_averages[col] = np.average(\n                metrics_levels_no_missing[col],\n                weights=metrics_levels_no_missing['n_predictions']\n            )\n        weighted_average = pd.DataFrame(weighted_averages, index=[0])\n        weighted_average['levels'] = 'weighted_average'\n\n        # aggregation: pooling\n        list_y_train_by_level = [\n            v['y_train'].to_numpy()\n            for k, v in y_train_per_level.items()\n            if k in predictions_per_level\n        ]\n        predictions_pooled = pd.concat(predictions_per_level.values())\n        y_train_pooled = pd.concat(\n            [v for k, v in y_train_per_level.items() if k in predictions_per_level]\n        )\n        pooled = []\n        for m, m_name in zip(metrics, metric_names):\n            if m_name in ['mean_absolute_scaled_error', 'root_mean_squared_scaled_error']:\n                pooled.append(\n                    m(\n                        y_true  = predictions_pooled['y_true'],\n                        y_pred  = predictions_pooled['y_pred'],\n                        y_train = list_y_train_by_level\n                    )\n                )\n            else:\n                pooled.append(\n                    m(\n                        y_true  = predictions_pooled['y_true'],\n                        y_pred  = predictions_pooled['y_pred'],\n                        y_train = y_train_pooled['y_train']\n                    )\n                )\n        pooled = pd.DataFrame([pooled], columns=metric_names)\n        pooled['levels'] = 'pooling'\n\n        metrics_levels = pd.concat(\n            [metrics_levels, average, weighted_average, pooled],\n            axis=0,\n            ignore_index=True\n        )\n\n    predictions = (\n        pd.concat(predictions_per_level.values())\n        .loc[:, [\"y_pred\", \"_level_skforecast\"]]\n        .pivot(columns=\"_level_skforecast\", values=\"y_pred\")\n        .rename_axis(columns=None, index=None)\n    )\n    predictions = predictions.asfreq(X_test.index.freq)\n\n    return metrics_levels, predictions\n"
  },
  "GT_src_dict": {
    "skforecast/direct/_forecaster_direct_multivariate.py": {
      "ForecasterDirectMultiVariate.__init__": {
        "code": "    def __init__(self, regressor: object, level: str, steps: int, lags: Optional[Union[int, list, np.ndarray, range, dict]]=None, window_features: Optional[Union[object, list]]=None, transformer_series: Optional[Union[object, dict]]=StandardScaler(), transformer_exog: Optional[object]=None, weight_func: Optional[Callable]=None, differentiation: Optional[int]=None, fit_kwargs: Optional[dict]=None, n_jobs: Union[int, str]='auto', forecaster_id: Optional[Union[str, int]]=None) -> None:\n        \"\"\"Initializes a `ForecasterDirectMultiVariate` instance, which turns any regressor compatible with the scikit-learn API into an autoregressive multivariate direct multi-step forecaster. This class is designed to create separate models for each forecasting time step.\n\n    Parameters\n    ----------\n    regressor : object\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n    level : str\n        Name of the time series to predict.\n    steps : int\n        Maximum number of future steps to predict.\n    lags : Optional[Union[int, list, np.ndarray, range, dict]], default `None`\n        Lags used as predictors.\n    window_features : Optional[Union[object, list]], default `None`\n        Instances used to create window features.\n    transformer_series : Optional[Union[object, dict]], default `StandardScaler()`\n        A transformer for preprocessing the series data.\n    transformer_exog : Optional[object], default `None`\n        A transformer for preprocessing exogenous variables.\n    weight_func : Optional[Callable], default `None`\n        A function to define sample weights.\n    differentiation : Optional[int], default `None`\n        Order of differencing applied to the time series.\n    fit_kwargs : Optional[dict], default `None`\n        Additional arguments for the regressor's fit method.\n    n_jobs : Union[int, str], default `'auto'`\n        The number of jobs to run in parallel.\n    forecaster_id : Optional[Union[str, int]], default `None`\n        Identifier for the forecaster.\n\n    Attributes\n    ----------\n    regressors_ : dict\n        Dictionary of regressors trained for each forecast step.\n    lags_ : numpy.ndarray, dict\n        Lags used as predictors.\n    window_features_names : list\n        Names of the window features included in the training matrix.\n    is_fitted : bool\n        Indicates whether the regressor has been fitted or not.\n    creation_date : str\n        Date of creation of the forecaster.\n\n    Notes\n    -----\n    The class requires a regressor from the scikit-learn library, among other components, to function properly. It ensures that lags or window features are defined to create predictors for training.\"\"\"\n        self.regressor = copy(regressor)\n        self.level = level\n        self.steps = steps\n        self.lags_ = None\n        self.transformer_series = transformer_series\n        self.transformer_series_ = None\n        self.transformer_exog = transformer_exog\n        self.weight_func = weight_func\n        self.source_code_weight_func = None\n        self.differentiation = differentiation\n        self.differentiator = None\n        self.differentiator_ = None\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.series_names_in_ = None\n        self.exog_in_ = False\n        self.exog_names_in_ = None\n        self.exog_type_in_ = None\n        self.exog_dtypes_in_ = None\n        self.X_train_series_names_in_ = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_ = None\n        self.X_train_direct_exog_names_out_ = None\n        self.X_train_features_names_out_ = None\n        self.creation_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.is_fitted = False\n        self.fit_date = None\n        self.skforecast_version = skforecast.__version__\n        self.python_version = sys.version.split(' ')[0]\n        self.forecaster_id = forecaster_id\n        self.dropna_from_series = False\n        self.encoding = None\n        if not isinstance(level, str):\n            raise TypeError(f'`level` argument must be a str. Got {type(level)}.')\n        if not isinstance(steps, int):\n            raise TypeError(f'`steps` argument must be an int greater than or equal to 1. Got {type(steps)}.')\n        if steps < 1:\n            raise ValueError(f'`steps` argument must be greater than or equal to 1. Got {steps}.')\n        self.regressors_ = {step: clone(self.regressor) for step in range(1, steps + 1)}\n        if isinstance(lags, dict):\n            self.lags = {}\n            self.lags_names = {}\n            list_max_lags = []\n            for key in lags:\n                if lags[key] is None:\n                    self.lags[key] = None\n                    self.lags_names[key] = None\n                else:\n                    self.lags[key], lags_names, max_lag = initialize_lags(forecaster_name=type(self).__name__, lags=lags[key])\n                    self.lags_names[key] = [f'{key}_{lag}' for lag in lags_names] if lags_names is not None else None\n                    if max_lag is not None:\n                        list_max_lags.append(max_lag)\n            self.max_lag = max(list_max_lags) if len(list_max_lags) != 0 else None\n        else:\n            self.lags, self.lags_names, self.max_lag = initialize_lags(forecaster_name=type(self).__name__, lags=lags)\n        self.window_features, self.window_features_names, self.max_size_window_features = initialize_window_features(window_features)\n        if self.window_features is None and (self.lags is None or self.max_lag is None):\n            raise ValueError('At least one of the arguments `lags` or `window_features` must be different from None. This is required to create the predictors used in training the forecaster.')\n        self.window_size = max([ws for ws in [self.max_lag, self.max_size_window_features] if ws is not None])\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [type(wf).__name__ for wf in self.window_features]\n        if self.differentiation is not None:\n            if not isinstance(differentiation, int) or differentiation < 1:\n                raise ValueError(f'Argument `differentiation` must be an integer equal to or greater than 1. Got {differentiation}.')\n            self.window_size += self.differentiation\n            self.differentiator = TimeSeriesDifferentiator(order=self.differentiation, window_size=self.window_size)\n        self.weight_func, self.source_code_weight_func, _ = initialize_weights(forecaster_name=type(self).__name__, regressor=regressor, weight_func=weight_func, series_weights=None)\n        self.fit_kwargs = check_select_fit_kwargs(regressor=regressor, fit_kwargs=fit_kwargs)\n        self.in_sample_residuals_ = {step: None for step in range(1, steps + 1)}\n        self.out_sample_residuals_ = None\n        if n_jobs == 'auto':\n            self.n_jobs = select_n_jobs_fit_forecaster(forecaster_name=type(self).__name__, regressor=self.regressor)\n        else:\n            if not isinstance(n_jobs, int):\n                raise TypeError(f\"`n_jobs` must be an integer or `'auto'`. Got {type(n_jobs)}.\")\n            self.n_jobs = n_jobs if n_jobs > 0 else cpu_count()",
        "docstring": "Initializes a `ForecasterDirectMultiVariate` instance, which turns any regressor compatible with the scikit-learn API into an autoregressive multivariate direct multi-step forecaster. This class is designed to create separate models for each forecasting time step.\n\nParameters\n----------\nregressor : object\n    An instance of a regressor or pipeline compatible with the scikit-learn API.\nlevel : str\n    Name of the time series to predict.\nsteps : int\n    Maximum number of future steps to predict.\nlags : Optional[Union[int, list, np.ndarray, range, dict]], default `None`\n    Lags used as predictors.\nwindow_features : Optional[Union[object, list]], default `None`\n    Instances used to create window features.\ntransformer_series : Optional[Union[object, dict]], default `StandardScaler()`\n    A transformer for preprocessing the series data.\ntransformer_exog : Optional[object], default `None`\n    A transformer for preprocessing exogenous variables.\nweight_func : Optional[Callable], default `None`\n    A function to define sample weights.\ndifferentiation : Optional[int], default `None`\n    Order of differencing applied to the time series.\nfit_kwargs : Optional[dict], default `None`\n    Additional arguments for the regressor's fit method.\nn_jobs : Union[int, str], default `'auto'`\n    The number of jobs to run in parallel.\nforecaster_id : Optional[Union[str, int]], default `None`\n    Identifier for the forecaster.\n\nAttributes\n----------\nregressors_ : dict\n    Dictionary of regressors trained for each forecast step.\nlags_ : numpy.ndarray, dict\n    Lags used as predictors.\nwindow_features_names : list\n    Names of the window features included in the training matrix.\nis_fitted : bool\n    Indicates whether the regressor has been fitted or not.\ncreation_date : str\n    Date of creation of the forecaster.\n\nNotes\n-----\nThe class requires a regressor from the scikit-learn library, among other components, to function properly. It ensures that lags or window features are defined to create predictors for training.",
        "signature": "def __init__(self, regressor: object, level: str, steps: int, lags: Optional[Union[int, list, np.ndarray, range, dict]]=None, window_features: Optional[Union[object, list]]=None, transformer_series: Optional[Union[object, dict]]=StandardScaler(), transformer_exog: Optional[object]=None, weight_func: Optional[Callable]=None, differentiation: Optional[int]=None, fit_kwargs: Optional[dict]=None, n_jobs: Union[int, str]='auto', forecaster_id: Optional[Union[str, int]]=None) -> None:",
        "type": "Method",
        "class_signature": "class ForecasterDirectMultiVariate(ForecasterBase):"
      },
      "ForecasterDirectMultiVariate._train_test_split_one_step_ahead": {
        "code": "    def _train_test_split_one_step_ahead(self, series: pd.DataFrame, initial_train_size: int, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, dict, pd.DataFrame, dict, pd.Series, pd.Series]:\n        \"\"\"Create matrices needed to train and test the forecaster for one-step-ahead predictions.\n    \n    Parameters\n    ----------\n    series : pandas DataFrame\n        Training time series containing multiple series used for forecasting.\n    initial_train_size : int\n        The number of observations used to initially train the forecaster before making predictions.\n    exog : pandas Series, pandas DataFrame, default `None`\n        Exogenous variable/s included as predictor/s. Must match the number of observations in `series` and be aligned to the same index.\n\n    Returns\n    -------\n    X_train : pandas DataFrame\n        Training predictor values used for the model.\n    y_train : dict\n        Dictionary containing the target values associated with each row of `X_train` for each step, formatted as {step: y_step_[i]}.\n    X_test : pandas DataFrame\n        Predictor values used for testing the model.\n    y_test : dict\n        Dictionary containing the target values associated with each row of `X_test` for each step, formatted as {step: y_step_[i]}.\n    X_train_encoding : pandas Series\n        Series identifying each row in `X_train` with the corresponding target series name.\n    X_test_encoding : pandas Series\n        Series identifying each row in `X_test` with the corresponding target series name.\n\n    Notes\n    -----\n    This method utilizes the `_extract_data_folds_multiseries` function to create training and testing folds based on the specified initial training size and window size. The initial part of the series is trained and the subsequent values are reserved for testing. It temporarily disables the `is_fitted` attribute to prevent data leakage during the training phase.\"\"\"\n        '\\n        Create matrices needed to train and test the forecaster for one-step-ahead\\n        predictions.\\n        \\n        Parameters\\n        ----------\\n        series : pandas DataFrame\\n            Training time series.\\n        initial_train_size : int\\n            Initial size of the training set. It is the number of observations used\\n            to train the forecaster before making the first prediction.\\n        exog : pandas Series, pandas DataFrame, default `None`\\n            Exogenous variable/s included as predictor/s. Must have the same\\n            number of observations as `series` and their indexes must be aligned.\\n        \\n        Returns\\n        -------\\n        X_train : pandas DataFrame\\n            Predictor values used to train the model.\\n        y_train : dict\\n            Values of the time series related to each row of `X_train` for each \\n            step in the form {step: y_step_[i]}.\\n        X_test : pandas DataFrame\\n            Predictor values used to test the model.\\n        y_test : dict\\n            Values of the time series related to each row of `X_test` for each \\n            step in the form {step: y_step_[i]}.\\n        X_train_encoding : pandas Series\\n            Series identifiers for each row of `X_train`.\\n        X_test_encoding : pandas Series\\n            Series identifiers for each row of `X_test`.\\n        \\n        '\n        span_index = series.index\n        fold = [[0, initial_train_size], [initial_train_size - self.window_size, initial_train_size], [initial_train_size - self.window_size, len(span_index)], [0, 0], True]\n        data_fold = _extract_data_folds_multiseries(series=series, folds=[fold], span_index=span_index, window_size=self.window_size, exog=exog, dropna_last_window=self.dropna_from_series, externally_fitted=False)\n        series_train, _, levels_last_window, exog_train, exog_test, _ = next(data_fold)\n        start_test_idx = initial_train_size - self.window_size\n        series_test = series.iloc[start_test_idx:, :]\n        series_test = series_test.loc[:, levels_last_window]\n        series_test = series_test.dropna(axis=1, how='all')\n        _is_fitted = self.is_fitted\n        _series_names_in_ = self.series_names_in_\n        _exog_names_in_ = self.exog_names_in_\n        self.is_fitted = False\n        X_train, y_train, series_names_in_, _, exog_names_in_, *_ = self._create_train_X_y(series=series_train, exog=exog_train)\n        self.series_names_in_ = series_names_in_\n        if exog is not None:\n            self.exog_names_in_ = exog_names_in_\n        self.is_fitted = True\n        X_test, y_test, *_ = self._create_train_X_y(series=series_test, exog=exog_test)\n        self.is_fitted = _is_fitted\n        self.series_names_in_ = _series_names_in_\n        self.exog_names_in_ = _exog_names_in_\n        X_train_encoding = pd.Series(self.level, index=X_train.index)\n        X_test_encoding = pd.Series(self.level, index=X_test.index)\n        return (X_train, y_train, X_test, y_test, X_train_encoding, X_test_encoding)",
        "docstring": "Create matrices needed to train and test the forecaster for one-step-ahead predictions.\n\nParameters\n----------\nseries : pandas DataFrame\n    Training time series containing multiple series used for forecasting.\ninitial_train_size : int\n    The number of observations used to initially train the forecaster before making predictions.\nexog : pandas Series, pandas DataFrame, default `None`\n    Exogenous variable/s included as predictor/s. Must match the number of observations in `series` and be aligned to the same index.\n\nReturns\n-------\nX_train : pandas DataFrame\n    Training predictor values used for the model.\ny_train : dict\n    Dictionary containing the target values associated with each row of `X_train` for each step, formatted as {step: y_step_[i]}.\nX_test : pandas DataFrame\n    Predictor values used for testing the model.\ny_test : dict\n    Dictionary containing the target values associated with each row of `X_test` for each step, formatted as {step: y_step_[i]}.\nX_train_encoding : pandas Series\n    Series identifying each row in `X_train` with the corresponding target series name.\nX_test_encoding : pandas Series\n    Series identifying each row in `X_test` with the corresponding target series name.\n\nNotes\n-----\nThis method utilizes the `_extract_data_folds_multiseries` function to create training and testing folds based on the specified initial training size and window size. The initial part of the series is trained and the subsequent values are reserved for testing. It temporarily disables the `is_fitted` attribute to prevent data leakage during the training phase.",
        "signature": "def _train_test_split_one_step_ahead(self, series: pd.DataFrame, initial_train_size: int, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, dict, pd.DataFrame, dict, pd.Series, pd.Series]:",
        "type": "Method",
        "class_signature": "class ForecasterDirectMultiVariate(ForecasterBase):"
      }
    },
    "skforecast/model_selection/_utils.py": {
      "_extract_data_folds_multiseries": {
        "code": "def _extract_data_folds_multiseries(series: Union[pd.Series, pd.DataFrame, dict], folds: list, span_index: Union[pd.DatetimeIndex, pd.RangeIndex], window_size: int, exog: Optional[Union[pd.Series, pd.DataFrame, dict]]=None, dropna_last_window: bool=False, externally_fitted: bool=False) -> Generator[Tuple[Union[pd.Series, pd.DataFrame, dict], pd.DataFrame, list, Optional[Union[pd.Series, pd.DataFrame, dict]], Optional[Union[pd.Series, pd.DataFrame, dict]], list], None, None]:\n    \"\"\"Extract the training and test data for each fold created during time series backtesting, accommodating multiseries data structures.\n\nParameters\n----------\nseries : Union[pd.Series, pd.DataFrame, dict]\n    The time series data from which to extract training and testing sets.\nfolds : list\n    A list of folds, where each fold specifies the indices for training, last window, and testing.\nspan_index : Union[pd.DatetimeIndex, pd.RangeIndex]\n    The complete index covering the entire time span of the series.\nwindow_size : int\n    The size of the window needed to create predictors in the forecaster.\nexog : Optional[Union[pd.Series, pd.DataFrame, dict]], default `None`\n    Exogenous variables corresponding to the time series data.\ndropna_last_window : bool, default `False`\n    If `True`, drop columns with NaN values from the last window of data.\nexternally_fitted : bool, default `False`\n    Flag to indicate if the forecaster has been trained externally, affecting the extraction of training data.\n\nYield\n-----\nseries_train : Union[pd.Series, pd.DataFrame, dict]\n    The training set corresponding to the current fold.\nseries_last_window : pandas DataFrame\n    The last window of the time series for the current fold.\nlevels_last_window : list\n    The levels (columns) present in the last window of the time series.\nexog_train: Optional[Union[pd.Series, pd.DataFrame, dict]], None\n    The exogenous variables for the training set of the fold.\nexog_test: Optional[Union[pd.Series, pd.DataFrame, dict]], None\n    The exogenous variables for the test set of the fold.\nfold: list\n    The current fold specifying the train and test indices.\n\nThis function is intended to be used in conjunction with the \nskforecast.model_selection._create_backtesting_folds function to facilitate the evaluation of forecasting methods on time series data. It handles both univariate and multivariate series, ensuring that data is correctly assigned based on the provided folds.\"\"\"\n    '\\n    Select the data from series and exog that corresponds to each fold created using the\\n    skforecast.model_selection._create_backtesting_folds function.\\n\\n    Parameters\\n    ----------\\n    series : pandas Series, pandas DataFrame, dict\\n        Time series.\\n    folds : list\\n        Folds created using the skforecast.model_selection._create_backtesting_folds\\n        function.\\n    span_index : pandas DatetimeIndex, pandas RangeIndex\\n        Full index from the minimum to the maximum index among all series.\\n    window_size : int\\n        Size of the window needed to create the predictors.\\n    exog : pandas Series, pandas DataFrame, dict, default `None`\\n        Exogenous variables.\\n    dropna_last_window : bool, default `False`\\n        If `True`, drop the columns of the last window that have NaN values.\\n    externally_fitted : bool, default `False`\\n        Flag indicating whether the forecaster is already trained. Only used when \\n        `initial_train_size` is None and `refit` is False.\\n\\n    Yield\\n    -----\\n    series_train : pandas Series, pandas DataFrame, dict\\n        Time series corresponding to the training set of the fold.\\n    series_last_window: pandas DataFrame\\n        Time series corresponding to the last window of the fold.\\n    levels_last_window: list\\n        Levels of the time series present in the last window of the fold.\\n    exog_train: pandas Series, pandas DataFrame, dict, None\\n        Exogenous variable corresponding to the training set of the fold.\\n    exog_test: pandas Series, pandas DataFrame, dict, None\\n        Exogenous variable corresponding to the test set of the fold.\\n    fold: list\\n        Fold created using the skforecast.model_selection._create_backtesting_folds\\n\\n    '\n    for fold in folds:\n        train_iloc_start = fold[0][0]\n        train_iloc_end = fold[0][1]\n        last_window_iloc_start = fold[1][0]\n        last_window_iloc_end = fold[1][1]\n        test_iloc_start = fold[2][0]\n        test_iloc_end = fold[2][1]\n        if isinstance(series, dict) or isinstance(exog, dict):\n            train_loc_start = span_index[train_iloc_start]\n            train_loc_end = span_index[train_iloc_end - 1]\n            last_window_loc_start = span_index[last_window_iloc_start]\n            last_window_loc_end = span_index[last_window_iloc_end - 1]\n            test_loc_start = span_index[test_iloc_start]\n            test_loc_end = span_index[test_iloc_end - 1]\n        if isinstance(series, pd.DataFrame):\n            series_train = series.iloc[train_iloc_start:train_iloc_end,]\n            series_to_drop = []\n            for col in series_train.columns:\n                if series_train[col].isna().all():\n                    series_to_drop.append(col)\n                else:\n                    first_valid_index = series_train[col].first_valid_index()\n                    last_valid_index = series_train[col].last_valid_index()\n                    if len(series_train[col].loc[first_valid_index:last_valid_index]) < window_size:\n                        series_to_drop.append(col)\n            series_last_window = series.iloc[last_window_iloc_start:last_window_iloc_end,]\n            series_train = series_train.drop(columns=series_to_drop)\n            if not externally_fitted:\n                series_last_window = series_last_window.drop(columns=series_to_drop)\n        else:\n            series_train = {}\n            for k in series.keys():\n                v = series[k].loc[train_loc_start:train_loc_end]\n                if not v.isna().all():\n                    first_valid_index = v.first_valid_index()\n                    last_valid_index = v.last_valid_index()\n                    if first_valid_index is not None and last_valid_index is not None:\n                        v = v.loc[first_valid_index:last_valid_index]\n                        if len(v) >= window_size:\n                            series_train[k] = v\n            series_last_window = {}\n            for k, v in series.items():\n                v = series[k].loc[last_window_loc_start:last_window_loc_end]\n                if (externally_fitted or k in series_train) and len(v) >= window_size:\n                    series_last_window[k] = v\n            series_last_window = pd.DataFrame(series_last_window)\n        if dropna_last_window:\n            series_last_window = series_last_window.dropna(axis=1, how='any')\n        levels_last_window = list(series_last_window.columns)\n        if exog is not None:\n            if isinstance(exog, (pd.Series, pd.DataFrame)):\n                exog_train = exog.iloc[train_iloc_start:train_iloc_end,]\n                exog_test = exog.iloc[test_iloc_start:test_iloc_end,]\n            else:\n                exog_train = {k: v.loc[train_loc_start:train_loc_end] for k, v in exog.items()}\n                exog_train = {k: v for k, v in exog_train.items() if len(v) > 0}\n                exog_test = {k: v.loc[test_loc_start:test_loc_end] for k, v in exog.items() if externally_fitted or k in exog_train}\n                exog_test = {k: v for k, v in exog_test.items() if len(v) > 0}\n        else:\n            exog_train = None\n            exog_test = None\n        yield (series_train, series_last_window, levels_last_window, exog_train, exog_test, fold)",
        "docstring": "Extract the training and test data for each fold created during time series backtesting, accommodating multiseries data structures.\n\nParameters\n----------\nseries : Union[pd.Series, pd.DataFrame, dict]\n    The time series data from which to extract training and testing sets.\nfolds : list\n    A list of folds, where each fold specifies the indices for training, last window, and testing.\nspan_index : Union[pd.DatetimeIndex, pd.RangeIndex]\n    The complete index covering the entire time span of the series.\nwindow_size : int\n    The size of the window needed to create predictors in the forecaster.\nexog : Optional[Union[pd.Series, pd.DataFrame, dict]], default `None`\n    Exogenous variables corresponding to the time series data.\ndropna_last_window : bool, default `False`\n    If `True`, drop columns with NaN values from the last window of data.\nexternally_fitted : bool, default `False`\n    Flag to indicate if the forecaster has been trained externally, affecting the extraction of training data.\n\nYield\n-----\nseries_train : Union[pd.Series, pd.DataFrame, dict]\n    The training set corresponding to the current fold.\nseries_last_window : pandas DataFrame\n    The last window of the time series for the current fold.\nlevels_last_window : list\n    The levels (columns) present in the last window of the time series.\nexog_train: Optional[Union[pd.Series, pd.DataFrame, dict]], None\n    The exogenous variables for the training set of the fold.\nexog_test: Optional[Union[pd.Series, pd.DataFrame, dict]], None\n    The exogenous variables for the test set of the fold.\nfold: list\n    The current fold specifying the train and test indices.\n\nThis function is intended to be used in conjunction with the \nskforecast.model_selection._create_backtesting_folds function to facilitate the evaluation of forecasting methods on time series data. It handles both univariate and multivariate series, ensuring that data is correctly assigned based on the provided folds.",
        "signature": "def _extract_data_folds_multiseries(series: Union[pd.Series, pd.DataFrame, dict], folds: list, span_index: Union[pd.DatetimeIndex, pd.RangeIndex], window_size: int, exog: Optional[Union[pd.Series, pd.DataFrame, dict]]=None, dropna_last_window: bool=False, externally_fitted: bool=False) -> Generator[Tuple[Union[pd.Series, pd.DataFrame, dict], pd.DataFrame, list, Optional[Union[pd.Series, pd.DataFrame, dict]], Optional[Union[pd.Series, pd.DataFrame, dict]], list], None, None]:",
        "type": "Function",
        "class_signature": null
      }
    }
  },
  "dependency_dict": {
    "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:__init__": {
      "skforecast/utils/utils.py": {
        "initialize_lags": {
          "code": "def initialize_lags(\n    forecaster_name: str,\n    lags: Any\n) -> Union[Optional[np.ndarray], Optional[list], Optional[int]]:\n    \"\"\"\n    Check lags argument input and generate the corresponding numpy ndarray.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    lags : Any\n        Lags used as predictors.\n\n    Returns\n    -------\n    lags : numpy ndarray, None\n        Lags used as predictors.\n    lags_names : list, None\n        Names of the lags used as predictors.\n    max_lag : int, None\n        Maximum value of the lags.\n    \n    \"\"\"\n\n    lags_names = None\n    max_lag = None\n    if lags is not None:\n        if isinstance(lags, int):\n            if lags < 1:\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n            lags = np.arange(1, lags + 1)\n\n        if isinstance(lags, (list, tuple, range)):\n            lags = np.array(lags)\n        \n        if isinstance(lags, np.ndarray):\n            if lags.size == 0:\n                return None, None, None\n            if lags.ndim != 1:\n                raise ValueError(\"`lags` must be a 1-dimensional array.\")\n            if not np.issubdtype(lags.dtype, np.integer):\n                raise TypeError(\"All values in `lags` must be integers.\")\n            if np.any(lags < 1):\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n        else:\n            if forecaster_name != 'ForecasterDirectMultiVariate':\n                raise TypeError(\n                    (f\"`lags` argument must be an int, 1d numpy ndarray, range, \"\n                     f\"tuple or list. Got {type(lags)}.\")\n                )\n            else:\n                raise TypeError(\n                    (f\"`lags` argument must be a dict, int, 1d numpy ndarray, range, \"\n                     f\"tuple or list. Got {type(lags)}.\")\n                )\n        \n        lags_names = [f'lag_{i}' for i in lags]\n        max_lag = max(lags)\n\n    return lags, lags_names, max_lag",
          "docstring": "Check lags argument input and generate the corresponding numpy ndarray.\n\nParameters\n----------\nforecaster_name : str\n    Forecaster name.\nlags : Any\n    Lags used as predictors.\n\nReturns\n-------\nlags : numpy ndarray, None\n    Lags used as predictors.\nlags_names : list, None\n    Names of the lags used as predictors.\nmax_lag : int, None\n    Maximum value of the lags.",
          "signature": "def initialize_lags(forecaster_name: str, lags: Any) -> Union[Optional[np.ndarray], Optional[list], Optional[int]]:",
          "type": "Function",
          "class_signature": null
        },
        "initialize_window_features": {
          "code": "def initialize_window_features(\n    window_features: Any\n) -> Union[Optional[list], Optional[list], Optional[int]]:\n    \"\"\"\n    Check window_features argument input and generate the corresponding list.\n\n    Parameters\n    ----------\n    window_features : Any\n        Classes used to create window features.\n\n    Returns\n    -------\n    window_features : list, None\n        List of classes used to create window features.\n    window_features_names : list, None\n        List with all the features names of the window features.\n    max_size_window_features : int, None\n        Maximum value of the `window_sizes` attribute of all classes.\n    \n    \"\"\"\n\n    needed_atts = ['window_sizes', 'features_names']\n    needed_methods = ['transform_batch', 'transform']\n\n    max_window_sizes = None\n    window_features_names = None\n    max_size_window_features = None\n    if window_features is not None:\n        if isinstance(window_features, list) and len(window_features) < 1:\n            raise ValueError(\n                \"Argument `window_features` must contain at least one element.\"\n            )\n        if not isinstance(window_features, list):\n            window_features = [window_features]\n\n        link_to_docs = (\n            \"\\nVisit the documentation for more information about how to create \"\n            \"custom window features:\\n\"\n            \"https://skforecast.org/latest/user_guides/window-features-and-custom-features.html#create-your-custom-window-features\"\n        )\n        \n        max_window_sizes = []\n        window_features_names = []\n        for wf in window_features:\n            wf_name = type(wf).__name__\n            atts_methods = set([a for a in dir(wf)])\n            if not set(needed_atts).issubset(atts_methods):\n                raise ValueError(\n                    f\"{wf_name} must have the attributes: {needed_atts}.\" + link_to_docs\n                )\n            if not set(needed_methods).issubset(atts_methods):\n                raise ValueError(\n                    f\"{wf_name} must have the methods: {needed_methods}.\" + link_to_docs\n                )\n            \n            window_sizes = wf.window_sizes\n            if not isinstance(window_sizes, (int, list)):\n                raise TypeError(\n                    f\"Attribute `window_sizes` of {wf_name} must be an int or a list \"\n                    f\"of ints. Got {type(window_sizes)}.\" + link_to_docs\n                )\n            \n            if isinstance(window_sizes, int):\n                if window_sizes < 1:\n                    raise ValueError(\n                        f\"If argument `window_sizes` is an integer, it must be equal to or \"\n                        f\"greater than 1. Got {window_sizes} from {wf_name}.\" + link_to_docs\n                    )\n                max_window_sizes.append(window_sizes)\n            else:\n                if not all(isinstance(ws, int) for ws in window_sizes) or not all(\n                    ws >= 1 for ws in window_sizes\n                ):                    \n                    raise ValueError(\n                        f\"If argument `window_sizes` is a list, all elements must be integers \"\n                        f\"equal to or greater than 1. Got {window_sizes} from {wf_name}.\" + link_to_docs\n                    )\n                max_window_sizes.append(max(window_sizes))\n\n            features_names = wf.features_names\n            if not isinstance(features_names, (str, list)):\n                raise TypeError(\n                    f\"Attribute `features_names` of {wf_name} must be a str or \"\n                    f\"a list of strings. Got {type(features_names)}.\" + link_to_docs\n                )\n            if isinstance(features_names, str):\n                window_features_names.append(features_names)\n            else:\n                if not all(isinstance(fn, str) for fn in features_names):\n                    raise TypeError(\n                        f\"If argument `features_names` is a list, all elements \"\n                        f\"must be strings. Got {features_names} from {wf_name}.\" + link_to_docs\n                    )\n                window_features_names.extend(features_names)\n\n        max_size_window_features = max(max_window_sizes)\n        if len(set(window_features_names)) != len(window_features_names):\n            raise ValueError(\n                f\"All window features names must be unique. Got {window_features_names}.\"\n            )\n\n    return window_features, window_features_names, max_size_window_features",
          "docstring": "Check window_features argument input and generate the corresponding list.\n\nParameters\n----------\nwindow_features : Any\n    Classes used to create window features.\n\nReturns\n-------\nwindow_features : list, None\n    List of classes used to create window features.\nwindow_features_names : list, None\n    List with all the features names of the window features.\nmax_size_window_features : int, None\n    Maximum value of the `window_sizes` attribute of all classes.",
          "signature": "def initialize_window_features(window_features: Any) -> Union[Optional[list], Optional[list], Optional[int]]:",
          "type": "Function",
          "class_signature": null
        },
        "initialize_weights": {
          "code": "def initialize_weights(\n    forecaster_name: str,\n    regressor: object,\n    weight_func: Union[Callable, dict],\n    series_weights: dict\n) -> Tuple[Union[Callable, dict], Union[str, dict], dict]:\n    \"\"\"\n    Check weights arguments, `weight_func` and `series_weights` for the different \n    forecasters. Create `source_code_weight_func`, source code of the custom \n    function(s) used to create weights.\n    \n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        Regressor of the forecaster.\n    weight_func : Callable, dict\n        Argument `weight_func` of the forecaster.\n    series_weights : dict\n        Argument `series_weights` of the forecaster.\n\n    Returns\n    -------\n    weight_func : Callable, dict\n        Argument `weight_func` of the forecaster.\n    source_code_weight_func : str, dict\n        Argument `source_code_weight_func` of the forecaster.\n    series_weights : dict\n        Argument `series_weights` of the forecaster.\n    \n    \"\"\"\n\n    source_code_weight_func = None\n\n    if weight_func is not None:\n\n        if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n            if not isinstance(weight_func, (Callable, dict)):\n                raise TypeError(\n                    (f\"Argument `weight_func` must be a Callable or a dict of \"\n                     f\"Callables. Got {type(weight_func)}.\")\n                )\n        elif not isinstance(weight_func, Callable):\n            raise TypeError(\n                f\"Argument `weight_func` must be a Callable. Got {type(weight_func)}.\"\n            )\n        \n        if isinstance(weight_func, dict):\n            source_code_weight_func = {}\n            for key in weight_func:\n                source_code_weight_func[key] = inspect.getsource(weight_func[key])\n        else:\n            source_code_weight_func = inspect.getsource(weight_func)\n\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(\n                (f\"Argument `weight_func` is ignored since regressor {regressor} \"\n                 f\"does not accept `sample_weight` in its `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n            weight_func = None\n            source_code_weight_func = None\n\n    if series_weights is not None:\n        if not isinstance(series_weights, dict):\n            raise TypeError(\n                (f\"Argument `series_weights` must be a dict of floats or ints.\"\n                 f\"Got {type(series_weights)}.\")\n            )\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(\n                (f\"Argument `series_weights` is ignored since regressor {regressor} \"\n                 f\"does not accept `sample_weight` in its `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n            series_weights = None\n\n    return weight_func, source_code_weight_func, series_weights",
          "docstring": "Check weights arguments, `weight_func` and `series_weights` for the different \nforecasters. Create `source_code_weight_func`, source code of the custom \nfunction(s) used to create weights.\n\nParameters\n----------\nforecaster_name : str\n    Forecaster name.\nregressor : regressor or pipeline compatible with the scikit-learn API\n    Regressor of the forecaster.\nweight_func : Callable, dict\n    Argument `weight_func` of the forecaster.\nseries_weights : dict\n    Argument `series_weights` of the forecaster.\n\nReturns\n-------\nweight_func : Callable, dict\n    Argument `weight_func` of the forecaster.\nsource_code_weight_func : str, dict\n    Argument `source_code_weight_func` of the forecaster.\nseries_weights : dict\n    Argument `series_weights` of the forecaster.",
          "signature": "def initialize_weights(forecaster_name: str, regressor: object, weight_func: Union[Callable, dict], series_weights: dict) -> Tuple[Union[Callable, dict], Union[str, dict], dict]:",
          "type": "Function",
          "class_signature": null
        },
        "check_select_fit_kwargs": {
          "code": "def check_select_fit_kwargs(\n    regressor: object,\n    fit_kwargs: Optional[dict] = None\n) -> dict:\n    \"\"\"\n    Check if `fit_kwargs` is a dict and select only the keys that are used by\n    the `fit` method of the regressor.\n\n    Parameters\n    ----------\n    regressor : object\n        Regressor object.\n    fit_kwargs : dict, default `None`\n        Dictionary with the arguments to pass to the `fit' method of the forecaster.\n\n    Returns\n    -------\n    fit_kwargs : dict\n        Dictionary with the arguments to be passed to the `fit` method of the \n        regressor after removing the unused keys.\n    \n    \"\"\"\n\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    else:\n        if not isinstance(fit_kwargs, dict):\n            raise TypeError(\n                f\"Argument `fit_kwargs` must be a dict. Got {type(fit_kwargs)}.\"\n            )\n\n        # Non used keys\n        non_used_keys = [k for k in fit_kwargs.keys()\n                         if k not in inspect.signature(regressor.fit).parameters]\n        if non_used_keys:\n            warnings.warn(\n                (f\"Argument/s {non_used_keys} ignored since they are not used by the \"\n                 f\"regressor's `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n\n        if 'sample_weight' in fit_kwargs.keys():\n            warnings.warn(\n                (\"The `sample_weight` argument is ignored. Use `weight_func` to pass \"\n                 \"a function that defines the individual weights for each sample \"\n                 \"based on its index.\"),\n                 IgnoredArgumentWarning\n            )\n            del fit_kwargs['sample_weight']\n\n        # Select only the keyword arguments allowed by the regressor's `fit` method.\n        fit_kwargs = {k: v for k, v in fit_kwargs.items()\n                      if k in inspect.signature(regressor.fit).parameters}\n\n    return fit_kwargs",
          "docstring": "Check if `fit_kwargs` is a dict and select only the keys that are used by\nthe `fit` method of the regressor.\n\nParameters\n----------\nregressor : object\n    Regressor object.\nfit_kwargs : dict, default `None`\n    Dictionary with the arguments to pass to the `fit' method of the forecaster.\n\nReturns\n-------\nfit_kwargs : dict\n    Dictionary with the arguments to be passed to the `fit` method of the \n    regressor after removing the unused keys.",
          "signature": "def check_select_fit_kwargs(regressor: object, fit_kwargs: Optional[dict]=None) -> dict:",
          "type": "Function",
          "class_signature": null
        },
        "select_n_jobs_fit_forecaster": {
          "code": "def select_n_jobs_fit_forecaster(\n    forecaster_name: str,\n    regressor: object,\n) -> int:\n    \"\"\"\n    Select the optimal number of jobs to use in the fitting process. This\n    selection is based on heuristics and is not guaranteed to be optimal. \n    \n    The number of jobs is chosen as follows:\n    \n    - If forecaster_name is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and regressor_name is a linear regressor then `n_jobs = 1`, \n    otherwise `n_jobs = cpu_count() - 1`.\n    - If regressor is a `LGBMRegressor(n_jobs=1)`, then `n_jobs = cpu_count() - 1`.\n    - If regressor is a `LGBMRegressor` with internal n_jobs != 1, then `n_jobs = 1`.\n    This is because `lightgbm` is highly optimized for gradient boosting and\n    parallelizes operations at a very fine-grained level, making additional\n    parallelization unnecessary and potentially harmful due to resource contention.\n    \n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n\n    Returns\n    -------\n    n_jobs : int\n        The number of jobs to run in parallel.\n    \n    \"\"\"\n\n    if isinstance(regressor, Pipeline):\n        regressor = regressor[-1]\n        regressor_name = type(regressor).__name__\n    else:\n        regressor_name = type(regressor).__name__\n\n    linear_regressors = [\n        regressor_name\n        for regressor_name in dir(sklearn.linear_model)\n        if not regressor_name.startswith('_')\n    ]\n\n    if forecaster_name in ['ForecasterDirect', \n                           'ForecasterDirectMultiVariate']:\n        if regressor_name in linear_regressors:\n            n_jobs = 1\n        elif regressor_name == 'LGBMRegressor':\n            n_jobs = joblib.cpu_count() - 1 if regressor.n_jobs == 1 else 1\n        else:\n            n_jobs = joblib.cpu_count() - 1\n    else:\n        n_jobs = 1\n\n    return n_jobs",
          "docstring": "Select the optimal number of jobs to use in the fitting process. This\nselection is based on heuristics and is not guaranteed to be optimal. \n\nThe number of jobs is chosen as follows:\n\n- If forecaster_name is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\nand regressor_name is a linear regressor then `n_jobs = 1`, \notherwise `n_jobs = cpu_count() - 1`.\n- If regressor is a `LGBMRegressor(n_jobs=1)`, then `n_jobs = cpu_count() - 1`.\n- If regressor is a `LGBMRegressor` with internal n_jobs != 1, then `n_jobs = 1`.\nThis is because `lightgbm` is highly optimized for gradient boosting and\nparallelizes operations at a very fine-grained level, making additional\nparallelization unnecessary and potentially harmful due to resource contention.\n\nParameters\n----------\nforecaster_name : str\n    Forecaster name.\nregressor : regressor or pipeline compatible with the scikit-learn API\n    An instance of a regressor or pipeline compatible with the scikit-learn API.\n\nReturns\n-------\nn_jobs : int\n    The number of jobs to run in parallel.",
          "signature": "def select_n_jobs_fit_forecaster(forecaster_name: str, regressor: object) -> int:",
          "type": "Function",
          "class_signature": null
        }
      }
    },
    "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:_train_test_split_one_step_ahead": {
      "skforecast/direct/_forecaster_direct_multivariate.py": {
        "ForecasterDirectMultiVariate._create_train_X_y": {
          "code": "    def _create_train_X_y(self, series: pd.DataFrame, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, dict, list, list, list, list, list, dict]:\n        \"\"\"\n        Create training matrices from multiple time series and exogenous\n        variables. The resulting matrices contain the target variable and predictors\n        needed to train all the regressors (one per step).\n        \n        Parameters\n        ----------\n        series : pandas DataFrame\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `series` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors) for each step. Note that the index \n            corresponds to that of the last step. It is updated for the corresponding \n            step in the filter_train_X_y_for_step method.\n        y_train : dict\n            Values of the time series related to each row of `X_train` for each \n            step in the form {step: y_step_[i]}.\n        series_names_in_ : list\n            Names of the series used during training.\n        X_train_series_names_in_ : list\n            Names of the series added to `X_train` when creating the training\n            matrices with `_create_train_X_y` method. It is a subset of \n            `series_names_in_`.\n        exog_names_in_ : list\n            Names of the exogenous variables included in the training matrices.\n        X_train_exog_names_out_ : list\n            Names of the exogenous variables included in the matrix `X_train` created\n            internally for training. It can be different from `exog_names_in_` if\n            some exogenous variables are transformed during the training process.\n        X_train_features_names_out_ : list\n            Names of the columns of the matrix created internally for training.\n        exog_dtypes_in_ : dict\n            Type of each exogenous variable/s used in training. If `transformer_exog` \n            is used, the dtypes are calculated before the transformation.\n        \n        \"\"\"\n        if not isinstance(series, pd.DataFrame):\n            raise TypeError(f'`series` must be a pandas DataFrame. Got {type(series)}.')\n        if len(series) < self.window_size + self.steps:\n            raise ValueError(f'Minimum length of `series` for training this forecaster is {self.window_size + self.steps}. Reduce the number of predicted steps, {self.steps}, or the maximum window_size, {self.window_size}, if no more data is available.\\n    Length `series`: {len(series)}.\\n    Max step : {self.steps}.\\n    Max window size: {self.window_size}.\\n    Lags window size: {self.max_lag}.\\n    Window features window size: {self.max_size_window_features}.')\n        series_names_in_ = list(series.columns)\n        if self.level not in series_names_in_:\n            raise ValueError(f'One of the `series` columns must be named as the `level` of the forecaster.\\n  Forecaster `level` : {self.level}.\\n  `series` columns   : {series_names_in_}.')\n        data_to_return_dict, X_train_series_names_in_ = self._create_data_to_return_dict(series_names_in_=series_names_in_)\n        series_to_create_autoreg_features_and_y = [col for col in series_names_in_ if col in X_train_series_names_in_ + [self.level]]\n        fit_transformer = False\n        if not self.is_fitted:\n            fit_transformer = True\n            self.transformer_series_ = initialize_transformer_series(forecaster_name=type(self).__name__, series_names_in_=series_to_create_autoreg_features_and_y, transformer_series=self.transformer_series)\n        if self.differentiation is None:\n            self.differentiator_ = {serie: None for serie in series_to_create_autoreg_features_and_y}\n        elif not self.is_fitted:\n            self.differentiator_ = {serie: copy(self.differentiator) for serie in series_to_create_autoreg_features_and_y}\n        exog_names_in_ = None\n        exog_dtypes_in_ = None\n        categorical_features = False\n        if exog is not None:\n            check_exog(exog=exog, allow_nan=True)\n            exog = input_to_frame(data=exog, input_name='exog')\n            series_index_no_ws = series.index[self.window_size:]\n            len_series = len(series)\n            len_series_no_ws = len_series - self.window_size\n            len_exog = len(exog)\n            if not len_exog == len_series and (not len_exog == len_series_no_ws):\n                raise ValueError(f'Length of `exog` must be equal to the length of `series` (if index is fully aligned) or length of `seriesy` - `window_size` (if `exog` starts after the first `window_size` values).\\n    `exog`                   : ({exog.index[0]} -- {exog.index[-1]})  (n={len_exog})\\n    `series`                 : ({series.index[0]} -- {series.index[-1]})  (n={len_series})\\n    `series` - `window_size` : ({series_index_no_ws[0]} -- {series_index_no_ws[-1]})  (n={len_series_no_ws})')\n            exog_names_in_ = exog.columns.to_list()\n            if len(set(exog_names_in_) - set(series_names_in_)) != len(exog_names_in_):\n                raise ValueError(f'`exog` cannot contain a column named the same as one of the series (column names of series).\\n  `series` columns : {series_names_in_}.\\n  `exog`   columns : {exog_names_in_}.')\n            self.exog_in_ = True\n            exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n            exog = transform_dataframe(df=exog, transformer=self.transformer_exog, fit=fit_transformer, inverse_transform=False)\n            check_exog_dtypes(exog, call_check_exog=True)\n            categorical_features = exog.select_dtypes(include=np.number).shape[1] != exog.shape[1]\n            if len_exog == len_series:\n                if not (exog.index == series.index).all():\n                    raise ValueError('When `exog` has the same length as `series`, the index of `exog` must be aligned with the index of `series` to ensure the correct alignment of values.')\n                exog = exog.iloc[self.window_size:,]\n            elif not (exog.index == series_index_no_ws).all():\n                raise ValueError(\"When `exog` doesn't contain the first `window_size` observations, the index of `exog` must be aligned with the index of `series` minus the first `window_size` observations to ensure the correct alignment of values.\")\n        X_train_autoreg = []\n        X_train_window_features_names_out_ = [] if self.window_features is not None else None\n        X_train_features_names_out_ = []\n        for col in series_to_create_autoreg_features_and_y:\n            y = series[col]\n            check_y(y=y, series_id=f\"Column '{col}'\")\n            y = transform_series(series=y, transformer=self.transformer_series_[col], fit=fit_transformer, inverse_transform=False)\n            y_values, y_index = preprocess_y(y=y)\n            if self.differentiation is not None:\n                if not self.is_fitted:\n                    y_values = self.differentiator_[col].fit_transform(y_values)\n                else:\n                    differentiator = copy(self.differentiator_[col])\n                    y_values = differentiator.fit_transform(y_values)\n            X_train_autoreg_col = []\n            train_index = y_index[self.window_size + (self.steps - 1):]\n            X_train_lags, y_train_values = self._create_lags(y=y_values, lags=self.lags_[col], data_to_return=data_to_return_dict.get(col, None))\n            if X_train_lags is not None:\n                X_train_autoreg_col.append(X_train_lags)\n                X_train_features_names_out_.extend(self.lags_names[col])\n            if col == self.level:\n                y_train = y_train_values\n            if self.window_features is not None:\n                n_diff = 0 if self.differentiation is None else self.differentiation\n                end_wf = None if self.steps == 1 else -(self.steps - 1)\n                y_window_features = pd.Series(y_values[n_diff:end_wf], index=y_index[n_diff:end_wf], name=col)\n                X_train_window_features, X_train_wf_names_out_ = self._create_window_features(y=y_window_features, X_as_pandas=False, train_index=train_index)\n                X_train_autoreg_col.extend(X_train_window_features)\n                X_train_window_features_names_out_.extend(X_train_wf_names_out_)\n                X_train_features_names_out_.extend(X_train_wf_names_out_)\n            if X_train_autoreg_col:\n                if len(X_train_autoreg_col) == 1:\n                    X_train_autoreg_col = X_train_autoreg_col[0]\n                else:\n                    X_train_autoreg_col = np.concatenate(X_train_autoreg_col, axis=1)\n                X_train_autoreg.append(X_train_autoreg_col)\n        X_train = []\n        len_train_index = len(train_index)\n        if categorical_features:\n            if len(X_train_autoreg) == 1:\n                X_train_autoreg = X_train_autoreg[0]\n            else:\n                X_train_autoreg = np.concatenate(X_train_autoreg, axis=1)\n            X_train_autoreg = pd.DataFrame(data=X_train_autoreg, columns=X_train_features_names_out_, index=train_index)\n            X_train.append(X_train_autoreg)\n        else:\n            X_train.extend(X_train_autoreg)\n        self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n        X_train_exog_names_out_ = None\n        if exog is not None:\n            X_train_exog_names_out_ = exog.columns.to_list()\n            if categorical_features:\n                exog_direct, X_train_direct_exog_names_out_ = exog_to_direct(exog=exog, steps=self.steps)\n                exog_direct.index = train_index\n            else:\n                exog_direct, X_train_direct_exog_names_out_ = exog_to_direct_numpy(exog=exog, steps=self.steps)\n            self.X_train_direct_exog_names_out_ = X_train_direct_exog_names_out_\n            X_train_features_names_out_.extend(self.X_train_direct_exog_names_out_)\n            X_train.append(exog_direct)\n        if len(X_train) == 1:\n            X_train = X_train[0]\n        elif categorical_features:\n            X_train = pd.concat(X_train, axis=1)\n        else:\n            X_train = np.concatenate(X_train, axis=1)\n        if categorical_features:\n            X_train.index = train_index\n        else:\n            X_train = pd.DataFrame(data=X_train, index=train_index, columns=X_train_features_names_out_)\n        y_train = {step: pd.Series(data=y_train[:, step - 1], index=y_index[self.window_size + step - 1:][:len_train_index], name=f'{self.level}_step_{step}') for step in range(1, self.steps + 1)}\n        return (X_train, y_train, series_names_in_, X_train_series_names_in_, exog_names_in_, X_train_exog_names_out_, X_train_features_names_out_, exog_dtypes_in_)",
          "docstring": "Create training matrices from multiple time series and exogenous\nvariables. The resulting matrices contain the target variable and predictors\nneeded to train all the regressors (one per step).\n\nParameters\n----------\nseries : pandas DataFrame\n    Training time series.\nexog : pandas Series, pandas DataFrame, default `None`\n    Exogenous variable/s included as predictor/s. Must have the same\n    number of observations as `series` and their indexes must be aligned.\n\nReturns\n-------\nX_train : pandas DataFrame\n    Training values (predictors) for each step. Note that the index \n    corresponds to that of the last step. It is updated for the corresponding \n    step in the filter_train_X_y_for_step method.\ny_train : dict\n    Values of the time series related to each row of `X_train` for each \n    step in the form {step: y_step_[i]}.\nseries_names_in_ : list\n    Names of the series used during training.\nX_train_series_names_in_ : list\n    Names of the series added to `X_train` when creating the training\n    matrices with `_create_train_X_y` method. It is a subset of \n    `series_names_in_`.\nexog_names_in_ : list\n    Names of the exogenous variables included in the training matrices.\nX_train_exog_names_out_ : list\n    Names of the exogenous variables included in the matrix `X_train` created\n    internally for training. It can be different from `exog_names_in_` if\n    some exogenous variables are transformed during the training process.\nX_train_features_names_out_ : list\n    Names of the columns of the matrix created internally for training.\nexog_dtypes_in_ : dict\n    Type of each exogenous variable/s used in training. If `transformer_exog` \n    is used, the dtypes are calculated before the transformation.",
          "signature": "def _create_train_X_y(self, series: pd.DataFrame, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, dict, list, list, list, list, list, dict]:",
          "type": "Method",
          "class_signature": "class ForecasterDirectMultiVariate(ForecasterBase):"
        }
      }
    }
  },
  "PRD": "# PROJECT NAME: skforecast-test_train_test_split_one_step_ahead\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 skforecast/\n    \u251c\u2500\u2500 direct/\n    \u2502   \u2514\u2500\u2500 _forecaster_direct_multivariate.py\n    \u2502       \u251c\u2500\u2500 ForecasterDirectMultiVariate.__init__\n    \u2502       \u2514\u2500\u2500 ForecasterDirectMultiVariate._train_test_split_one_step_ahead\n    \u2514\u2500\u2500 model_selection/\n        \u2514\u2500\u2500 _utils.py\n            \u2514\u2500\u2500 _extract_data_folds_multiseries\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThis module facilitates train-test data splits for direct multivariate forecasting models by generating feature matrices and target variables tailored for time series prediction tasks. It supports handling input time series data and associated exogenous variables, efficiently creating lagged features and aligning them with corresponding future target values. The module enables developers and data scientists to prepare and validate datasets for one-step-ahead forecasting scenarios with precise encoding of inputs and outputs. By automating this complex preprocessing, it simplifies the integration of multivariate forecasting models and ensures consistency in data preparation, reducing manual effort and potential errors in model development workflows.\n\n## FILE 1: skforecast/direct/_forecaster_direct_multivariate.py\n\n- CLASS METHOD: ForecasterDirectMultiVariate._train_test_split_one_step_ahead\n  - CLASS SIGNATURE: class ForecasterDirectMultiVariate(ForecasterBase):\n  - SIGNATURE: def _train_test_split_one_step_ahead(self, series: pd.DataFrame, initial_train_size: int, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, dict, pd.DataFrame, dict, pd.Series, pd.Series]:\n  - DOCSTRING: \n```python\n\"\"\"\nCreate matrices needed to train and test the forecaster for one-step-ahead predictions.\n\nParameters\n----------\nseries : pandas DataFrame\n    Training time series containing multiple series used for forecasting.\ninitial_train_size : int\n    The number of observations used to initially train the forecaster before making predictions.\nexog : pandas Series, pandas DataFrame, default `None`\n    Exogenous variable/s included as predictor/s. Must match the number of observations in `series` and be aligned to the same index.\n\nReturns\n-------\nX_train : pandas DataFrame\n    Training predictor values used for the model.\ny_train : dict\n    Dictionary containing the target values associated with each row of `X_train` for each step, formatted as {step: y_step_[i]}.\nX_test : pandas DataFrame\n    Predictor values used for testing the model.\ny_test : dict\n    Dictionary containing the target values associated with each row of `X_test` for each step, formatted as {step: y_step_[i]}.\nX_train_encoding : pandas Series\n    Series identifying each row in `X_train` with the corresponding target series name.\nX_test_encoding : pandas Series\n    Series identifying each row in `X_test` with the corresponding target series name.\n\nNotes\n-----\nThis method utilizes the `_extract_data_folds_multiseries` function to create training and testing folds based on the specified initial training size and window size. The initial part of the series is trained and the subsequent values are reserved for testing. It temporarily disables the `is_fitted` attribute to prevent data leakage during the training phase.\n\"\"\"\n```\n\n- CLASS METHOD: ForecasterDirectMultiVariate.__init__\n  - CLASS SIGNATURE: class ForecasterDirectMultiVariate(ForecasterBase):\n  - SIGNATURE: def __init__(self, regressor: object, level: str, steps: int, lags: Optional[Union[int, list, np.ndarray, range, dict]]=None, window_features: Optional[Union[object, list]]=None, transformer_series: Optional[Union[object, dict]]=StandardScaler(), transformer_exog: Optional[object]=None, weight_func: Optional[Callable]=None, differentiation: Optional[int]=None, fit_kwargs: Optional[dict]=None, n_jobs: Union[int, str]='auto', forecaster_id: Optional[Union[str, int]]=None) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nInitializes a `ForecasterDirectMultiVariate` instance, which turns any regressor compatible with the scikit-learn API into an autoregressive multivariate direct multi-step forecaster. This class is designed to create separate models for each forecasting time step.\n\nParameters\n----------\nregressor : object\n    An instance of a regressor or pipeline compatible with the scikit-learn API.\nlevel : str\n    Name of the time series to predict.\nsteps : int\n    Maximum number of future steps to predict.\nlags : Optional[Union[int, list, np.ndarray, range, dict]], default `None`\n    Lags used as predictors.\nwindow_features : Optional[Union[object, list]], default `None`\n    Instances used to create window features.\ntransformer_series : Optional[Union[object, dict]], default `StandardScaler()`\n    A transformer for preprocessing the series data.\ntransformer_exog : Optional[object], default `None`\n    A transformer for preprocessing exogenous variables.\nweight_func : Optional[Callable], default `None`\n    A function to define sample weights.\ndifferentiation : Optional[int], default `None`\n    Order of differencing applied to the time series.\nfit_kwargs : Optional[dict], default `None`\n    Additional arguments for the regressor's fit method.\nn_jobs : Union[int, str], default `'auto'`\n    The number of jobs to run in parallel.\nforecaster_id : Optional[Union[str, int]], default `None`\n    Identifier for the forecaster.\n\nAttributes\n----------\nregressors_ : dict\n    Dictionary of regressors trained for each forecast step.\nlags_ : numpy.ndarray, dict\n    Lags used as predictors.\nwindow_features_names : list\n    Names of the window features included in the training matrix.\nis_fitted : bool\n    Indicates whether the regressor has been fitted or not.\ncreation_date : str\n    Date of creation of the forecaster.\n\nNotes\n-----\nThe class requires a regressor from the scikit-learn library, among other components, to function properly. It ensures that lags or window features are defined to create predictors for training.\n\"\"\"\n```\n\n## FILE 2: skforecast/model_selection/_utils.py\n\n- FUNCTION NAME: _extract_data_folds_multiseries\n  - SIGNATURE: def _extract_data_folds_multiseries(series: Union[pd.Series, pd.DataFrame, dict], folds: list, span_index: Union[pd.DatetimeIndex, pd.RangeIndex], window_size: int, exog: Optional[Union[pd.Series, pd.DataFrame, dict]]=None, dropna_last_window: bool=False, externally_fitted: bool=False) -> Generator[Tuple[Union[pd.Series, pd.DataFrame, dict], pd.DataFrame, list, Optional[Union[pd.Series, pd.DataFrame, dict]], Optional[Union[pd.Series, pd.DataFrame, dict]], list], None, None]:\n  - DOCSTRING: \n```python\n\"\"\"\nExtract the training and test data for each fold created during time series backtesting, accommodating multiseries data structures.\n\nParameters\n----------\nseries : Union[pd.Series, pd.DataFrame, dict]\n    The time series data from which to extract training and testing sets.\nfolds : list\n    A list of folds, where each fold specifies the indices for training, last window, and testing.\nspan_index : Union[pd.DatetimeIndex, pd.RangeIndex]\n    The complete index covering the entire time span of the series.\nwindow_size : int\n    The size of the window needed to create predictors in the forecaster.\nexog : Optional[Union[pd.Series, pd.DataFrame, dict]], default `None`\n    Exogenous variables corresponding to the time series data.\ndropna_last_window : bool, default `False`\n    If `True`, drop columns with NaN values from the last window of data.\nexternally_fitted : bool, default `False`\n    Flag to indicate if the forecaster has been trained externally, affecting the extraction of training data.\n\nYield\n-----\nseries_train : Union[pd.Series, pd.DataFrame, dict]\n    The training set corresponding to the current fold.\nseries_last_window : pandas DataFrame\n    The last window of the time series for the current fold.\nlevels_last_window : list\n    The levels (columns) present in the last window of the time series.\nexog_train: Optional[Union[pd.Series, pd.DataFrame, dict]], None\n    The exogenous variables for the training set of the fold.\nexog_test: Optional[Union[pd.Series, pd.DataFrame, dict]], None\n    The exogenous variables for the test set of the fold.\nfold: list\n    The current fold specifying the train and test indices.\n\nThis function is intended to be used in conjunction with the \nskforecast.model_selection._create_backtesting_folds function to facilitate the evaluation of forecasting methods on time series data. It handles both univariate and multivariate series, ensuring that data is correctly assigned based on the provided folds.\n\"\"\"\n```\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "skforecast/direct/_forecaster_direct_multivariate.py": "from typing import Union, Tuple, Optional, Callable, Any\nimport warnings\nimport sys\nimport numpy as np\nimport pandas as pd\nimport inspect\nfrom copy import copy\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import clone\nfrom joblib import Parallel, delayed, cpu_count\nfrom sklearn.preprocessing import StandardScaler\nfrom itertools import chain\nimport skforecast\nfrom ..base import ForecasterBase\nfrom ..exceptions import DataTransformationWarning\nfrom ..utils import initialize_lags, initialize_window_features, initialize_weights, initialize_transformer_series, check_select_fit_kwargs, check_y, check_exog, prepare_steps_direct, get_exog_dtypes, check_exog_dtypes, check_predict_input, check_interval, preprocess_y, preprocess_last_window, input_to_frame, exog_to_direct, exog_to_direct_numpy, expand_index, transform_numpy, transform_series, transform_dataframe, select_n_jobs_fit_forecaster, set_skforecast_warnings\nfrom ..preprocessing import TimeSeriesDifferentiator\nfrom ..model_selection._utils import _extract_data_folds_multiseries\n\nclass ForecasterDirectMultiVariate(ForecasterBase):\n    \"\"\"\n    This class turns any regressor compatible with the scikit-learn API into a\n    autoregressive multivariate direct multi-step forecaster. A separate model \n    is created for each forecast time step. See documentation for more details.\n\n    Parameters\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n    level : str\n        Name of the time series to be predicted.\n    steps : int\n        Maximum number of future steps the forecaster will predict when using\n        method `predict()`. Since a different model is created for each step,\n        this value must be defined before training.\n    lags : int, list, numpy ndarray, range, dict, default `None`\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n\n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n        - `dict`: create different lags for each series. {'series_column_name': lags}.\n        - `None`: no lags are included as predictors. \n    window_features : object, list, default `None`\n        Instance or list of instances used to create window features. Window features\n        are created from the original time series and are included as predictors.\n    transformer_series : transformer (preprocessor), dict, default `sklearn.preprocessing.StandardScaler`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and \n        inverse_transform. Transformation is applied to each `series` before training \n        the forecaster. ColumnTransformers are not allowed since they do not have \n        inverse_transform method.\n\n        - If single transformer: it is cloned and applied to all series. \n        - If `dict` of transformers: a different transformer can be used for each series.\n    transformer_exog : transformer, default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable, default `None`\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\n        method. The resulting `sample_weight` cannot have negative values.\n    fit_kwargs : dict, default `None`\n        Additional arguments to be passed to the `fit` method of the regressor.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_fit_forecaster.\n    forecaster_id : str, int, default `None`\n        Name used as an identifier of the forecaster.\n\n    Attributes\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n        An instance of this regressor is trained for each step. All of them \n        are stored in `self.regressors_`.\n    regressors_ : dict\n        Dictionary with regressors trained for each step. They are initialized \n        as a copy of `regressor`.\n    steps : int\n        Number of future steps the forecaster will predict when using method\n        `predict()`. Since a different model is created for each step, this value\n        should be defined before training.\n    lags : numpy ndarray, dict\n        Lags used as predictors.\n    lags_ : dict\n        Dictionary with the lags of each series. Created from `lags` when \n        creating the training matrices and used internally to avoid overwriting.\n    lags_names : dict\n        Names of the lags of each series.\n    max_lag : int\n        Maximum lag included in `lags`.\n    window_features : list\n        Class or list of classes used to create window features.\n    window_features_names : list\n        Names of the window features to be included in the `X_train` matrix.\n    window_features_class_names : list\n        Names of the classes used to create the window features.\n    max_size_window_features : int\n        Maximum window size required by the window features.\n    window_size : int\n        The window size needed to create the predictors. It is calculated as the \n        maximum value between `max_lag` and `max_size_window_features`. If \n        differentiation is used, `window_size` is increased by n units equal to \n        the order of differentiation so that predictors can be generated correctly.\n    transformer_series : transformer (preprocessor), dict, default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and \n        inverse_transform. Transformation is applied to each `series` before training \n        the forecaster. ColumnTransformers are not allowed since they do not have \n        inverse_transform method.\n\n        - If single transformer: it is cloned and applied to all series. \n        - If `dict` of transformers: a different transformer can be used for each series.\n    transformer_series_ : dict\n        Dictionary with the transformer for each series. It is created cloning the \n        objects in `transformer_series` and is used internally to avoid overwriting.\n    transformer_exog : transformer\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its\n        `fit` method. The resulting `sample_weight` cannot have negative values.\n    source_code_weight_func : str\n        Source code of the custom function used to create weights.\n    differentiation : int\n        Order of differencing applied to the time series before training the \n        forecaster.\n    differentiator : TimeSeriesDifferentiator\n        Skforecast object used to differentiate the time series.\n    differentiator_ : dict\n        Dictionary with the `differentiator` for each series. It is created cloning the\n        objects in `differentiator` and is used internally to avoid overwriting.\n    last_window_ : pandas DataFrame\n        This window represents the most recent data observed by the predictor\n        during its training phase. It contains the values needed to predict the\n        next step immediately after the training data. These values are stored\n        in the original scale of the time series before undergoing any transformations\n        or differentiation. When `differentiation` parameter is specified, the\n        dimensions of the `last_window_` are expanded as many values as the order\n        of differentiation. For example, if `lags` = 7 and `differentiation` = 1,\n        `last_window_` will have 8 values.\n    index_type_ : type\n        Type of index of the input used in training.\n    index_freq_ : str\n        Frequency of Index of the input used in training.\n    training_range_: pandas Index\n        First and last values of index of the data used during training.\n    exog_in_ : bool\n        If the forecaster has been trained using exogenous variable/s.\n    exog_type_in_ : type\n        Type of exogenous variable/s used in training.\n    exog_dtypes_in_ : dict\n        Type of each exogenous variable/s used in training. If `transformer_exog` \n        is used, the dtypes are calculated after the transformation.\n    exog_names_in_ : list\n        Names of the exogenous variables used during training.\n    series_names_in_ : list\n        Names of the series used during training.\n    X_train_series_names_in_ : list\n        Names of the series added to `X_train` when creating the training\n        matrices with `_create_train_X_y` method. It is a subset of \n        `series_names_in_`.\n    X_train_window_features_names_out_ : list\n        Names of the window features included in the matrix `X_train` created\n        internally for training.\n    X_train_exog_names_out_ : list\n        Names of the exogenous variables included in the matrix `X_train` created\n        internally for training. It can be different from `exog_names_in_` if\n        some exogenous variables are transformed during the training process.\n    X_train_direct_exog_names_out_ : list\n        Same as `X_train_exog_names_out_` but using the direct format. The same \n        exogenous variable is repeated for each step.\n    X_train_features_names_out_ : list\n        Names of columns of the matrix created internally for training.\n    fit_kwargs : dict\n        Additional arguments to be passed to the `fit` method of the regressor.\n    in_sample_residuals_ : dict\n        Residuals of the models when predicting training data. Only stored up to\n        1000 values per model in the form `{step: residuals}`. If `transformer_series` \n        is not `None`, residuals are stored in the transformed scale.\n    out_sample_residuals_ : dict\n        Residuals of the models when predicting non training data. Only stored\n        up to 1000 values per model in the form `{step: residuals}`. If `transformer_series` \n        is not `None`, residuals are assumed to be in the transformed scale. Use \n        `set_out_sample_residuals()` method to set values.\n    creation_date : str\n        Date of creation.\n    is_fitted : bool\n        Tag to identify if the regressor has been fitted (trained).\n    fit_date : str\n        Date of last fit.\n    skforecast_version : str\n        Version of skforecast library used to create the forecaster.\n    python_version : str\n        Version of python used to create the forecaster.\n    n_jobs : int, 'auto'\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the fuction\n        skforecast.utils.select_n_jobs_fit_forecaster.\n    forecaster_id : str, int\n        Name used as an identifier of the forecaster.\n    dropna_from_series : Ignored\n        Not used, present here for API consistency by convention.\n    encoding : Ignored\n        Not used, present here for API consistency by convention.\n\n    Notes\n    -----\n    A separate model is created for each forecasting time step. It is important to\n    note that all models share the same parameter and hyperparameter configuration.\n    \n    \"\"\"\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Information displayed when a ForecasterDirectMultiVariate object is printed.\n        \"\"\"\n        params, _, series_names_in_, exog_names_in_, transformer_series = [self._format_text_repr(value) for value in self._preprocess_repr(regressor=self.regressor, series_names_in_=self.series_names_in_, exog_names_in_=self.exog_names_in_, transformer_series=self.transformer_series)]\n        info = f'{'=' * len(type(self).__name__)} \\n{type(self).__name__} \\n{'=' * len(type(self).__name__)} \\nRegressor: {type(self.regressor).__name__} \\nTarget series (level): {self.level} \\nLags: {self.lags} \\nWindow features: {self.window_features_names} \\nWindow size: {self.window_size} \\nMaximum steps to predict: {self.steps} \\nMultivariate series: {series_names_in_} \\nExogenous included: {self.exog_in_} \\nExogenous names: {exog_names_in_} \\nTransformer for series: {transformer_series} \\nTransformer for exog: {self.transformer_exog} \\nWeight function included: {(True if self.weight_func is not None else False)} \\nDifferentiation order: {self.differentiation} \\nTraining range: {(self.training_range_.to_list() if self.is_fitted else None)} \\nTraining index type: {(str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None)} \\nTraining index frequency: {(self.index_freq_ if self.is_fitted else None)} \\nRegressor parameters: {params} \\nfit_kwargs: {self.fit_kwargs} \\nCreation date: {self.creation_date} \\nLast fit date: {self.fit_date} \\nSkforecast version: {self.skforecast_version} \\nPython version: {self.python_version} \\nForecaster id: {self.forecaster_id} \\n'\n        return info\n\n    def _repr_html_(self):\n        \"\"\"\n        HTML representation of the object.\n        The \"General Information\" section is expanded by default.\n        \"\"\"\n        params, _, series_names_in_, exog_names_in_, transformer_series = self._preprocess_repr(regressor=self.regressor, series_names_in_=self.series_names_in_, exog_names_in_=self.exog_names_in_, transformer_series=self.transformer_series)\n        style, unique_id = self._get_style_repr_html(self.is_fitted)\n        content = f'\\n        <div class=\"container-{unique_id}\">\\n            <h2>{type(self).__name__}</h2>\\n            <details open>\\n                <summary>General Information</summary>\\n                <ul>\\n                    <li><strong>Regressor:</strong> {type(self.regressor).__name__}</li>\\n                    <li><strong>Target series (level):</strong> {self.level}</li>\\n                    <li><strong>Lags:</strong> {self.lags}</li>\\n                    <li><strong>Window features:</strong> {self.window_features_names}</li>\\n                    <li><strong>Window size:</strong> {self.window_size}</li>\\n                    <li><strong>Maximum steps to predict:</strong> {self.steps}</li>\\n                    <li><strong>Exogenous included:</strong> {self.exog_in_}</li>\\n                    <li><strong>Weight function included:</strong> {self.weight_func is not None}</li>\\n                    <li><strong>Differentiation order:</strong> {self.differentiation}</li>\\n                    <li><strong>Creation date:</strong> {self.creation_date}</li>\\n                    <li><strong>Last fit date:</strong> {self.fit_date}</li>\\n                    <li><strong>Skforecast version:</strong> {self.skforecast_version}</li>\\n                    <li><strong>Python version:</strong> {self.python_version}</li>\\n                    <li><strong>Forecaster id:</strong> {self.forecaster_id}</li>\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Exogenous Variables</summary>\\n                <ul>\\n                    {exog_names_in_}\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Data Transformations</summary>\\n                <ul>\\n                    <li><strong>Transformer for series:</strong> {transformer_series}</li>\\n                    <li><strong>Transformer for exog:</strong> {self.transformer_exog}</li>\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Training Information</summary>\\n                <ul>\\n                    <li><strong>Target series (level):</strong> {self.level}</li>\\n                    <li><strong>Multivariate series:</strong> {series_names_in_}</li>\\n                    <li><strong>Training range:</strong> {(self.training_range_.to_list() if self.is_fitted else 'Not fitted')}</li>\\n                    <li><strong>Training index type:</strong> {(str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted')}</li>\\n                    <li><strong>Training index frequency:</strong> {(self.index_freq_ if self.is_fitted else 'Not fitted')}</li>\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Regressor Parameters</summary>\\n                <ul>\\n                    {params}\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Fit Kwargs</summary>\\n                <ul>\\n                    {self.fit_kwargs}\\n                </ul>\\n            </details>\\n            <p>\\n                <a href=\"https://skforecast.org/{skforecast.__version__}/api/forecasterdirectmultivariate.html\">&#128712 <strong>API Reference</strong></a>\\n                &nbsp;&nbsp;\\n                <a href=\"https://skforecast.org/{skforecast.__version__}/user_guides/dependent-multi-series-multivariate-forecasting.html\">&#128462 <strong>User Guide</strong></a>\\n            </p>\\n        </div>\\n        '\n        return style + content\n\n    def _create_data_to_return_dict(self, series_names_in_: list) -> Tuple[dict, list]:\n        \"\"\"\n        Create `data_to_return_dict` based on series names and lags configuration.\n        The dictionary contains the information to decide what data to return in \n        the `_create_lags` method.\n        \n        Parameters\n        ----------\n        series_names_in_ : list\n            Names of the series used during training.\n\n        Returns\n        -------\n        data_to_return_dict : dict\n            Dictionary with the information to decide what data to return in the\n            `_create_lags` method.\n        X_train_series_names_in_ : list\n            Names of the series added to `X_train` when creating the training\n            matrices with `_create_train_X_y` method. It is a subset of \n            `series_names_in_`.\n        \n        \"\"\"\n        if isinstance(self.lags, dict):\n            lags_keys = list(self.lags.keys())\n            if set(lags_keys) != set(series_names_in_):\n                raise ValueError(f\"When `lags` parameter is a `dict`, its keys must be the same as `series` column names. If don't want to include lags, add '{{column: None}}' to the lags dict.\\n  Lags keys        : {lags_keys}.\\n  `series` columns : {series_names_in_}.\")\n            self.lags_ = copy(self.lags)\n        else:\n            self.lags_ = {serie: self.lags for serie in series_names_in_}\n            if self.lags is not None:\n                lags_names = [f'lag_{i}' for i in self.lags]\n                self.lags_names = {serie: [f'{serie}_{lag}' for lag in lags_names] for serie in series_names_in_}\n            else:\n                self.lags_names = {serie: None for serie in series_names_in_}\n        X_train_series_names_in_ = series_names_in_\n        if self.lags is None:\n            data_to_return_dict = {self.level: 'y'}\n        else:\n            data_to_return_dict = {col: 'both' if col == self.level else 'X' for col in series_names_in_ if col == self.level or self.lags_.get(col) is not None}\n            if self.lags_.get(self.level) is None:\n                data_to_return_dict[self.level] = 'y'\n            if self.window_features is None:\n                X_train_series_names_in_ = [col for col in data_to_return_dict.keys() if data_to_return_dict[col] in ['X', 'both']]\n        return (data_to_return_dict, X_train_series_names_in_)\n\n    def _create_lags(self, y: np.ndarray, lags: np.ndarray, data_to_return: Optional[str]='both') -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n        \"\"\"\n        Create the lagged values and their target variable from a time series.\n        \n        Note that the returned matrix `X_data` contains the lag 1 in the first \n        column, the lag 2 in the in the second column and so on.\n        \n        Parameters\n        ----------\n        y : numpy ndarray\n            Training time series values.\n        lags : numpy ndarray\n            lags to create.\n        data_to_return : str, default 'both'\n            Specifies which data to return. Options are 'X', 'y', 'both' or None.\n\n        Returns\n        -------\n        X_data : numpy ndarray, None\n            Lagged values (predictors).\n        y_data : numpy ndarray, None\n            Values of the time series related to each row of `X_data`.\n        \n        \"\"\"\n        X_data = None\n        y_data = None\n        if data_to_return is not None:\n            n_rows = len(y) - self.window_size - (self.steps - 1)\n            if data_to_return != 'y':\n                X_data = np.full(shape=(n_rows, len(lags)), fill_value=np.nan, order='F', dtype=float)\n                for i, lag in enumerate(lags):\n                    X_data[:, i] = y[self.window_size - lag:-(lag + self.steps - 1)]\n            if data_to_return != 'X':\n                y_data = np.full(shape=(n_rows, self.steps), fill_value=np.nan, order='F', dtype=float)\n                for step in range(self.steps):\n                    y_data[:, step] = y[self.window_size + step:self.window_size + step + n_rows]\n        return (X_data, y_data)\n\n    def _create_window_features(self, y: pd.Series, train_index: pd.Index, X_as_pandas: bool=False) -> Tuple[list, list]:\n        \"\"\"\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        train_index : pandas Index\n            Index of the training data. It is used to create the pandas DataFrame\n            `X_train_window_features` when `X_as_pandas` is `True`.\n        X_as_pandas : bool, default `False`\n            If `True`, the returned matrix `X_train_window_features` is a \n            pandas DataFrame.\n\n        Returns\n        -------\n        X_train_window_features : list\n            List of numpy ndarrays or pandas DataFrames with the window features.\n        X_train_window_features_names_out_ : list\n            Names of the window features.\n        \n        \"\"\"\n        len_train_index = len(train_index)\n        X_train_window_features = []\n        X_train_window_features_names_out_ = []\n        for wf in self.window_features:\n            X_train_wf = wf.transform_batch(y)\n            if not isinstance(X_train_wf, pd.DataFrame):\n                raise TypeError(f'The method `transform_batch` of {type(wf).__name__} must return a pandas DataFrame.')\n            X_train_wf = X_train_wf.iloc[-len_train_index:]\n            if not len(X_train_wf) == len_train_index:\n                raise ValueError(f'The method `transform_batch` of {type(wf).__name__} must return a DataFrame with the same number of rows as the input time series - (`window_size` + (`steps` - 1)): {len_train_index}.')\n            X_train_wf.index = train_index\n            X_train_wf.columns = [f'{y.name}_{col}' for col in X_train_wf.columns]\n            X_train_window_features_names_out_.extend(X_train_wf.columns)\n            if not X_as_pandas:\n                X_train_wf = X_train_wf.to_numpy()\n            X_train_window_features.append(X_train_wf)\n        return (X_train_window_features, X_train_window_features_names_out_)\n\n    def _create_train_X_y(self, series: pd.DataFrame, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, dict, list, list, list, list, list, dict]:\n        \"\"\"\n        Create training matrices from multiple time series and exogenous\n        variables. The resulting matrices contain the target variable and predictors\n        needed to train all the regressors (one per step).\n        \n        Parameters\n        ----------\n        series : pandas DataFrame\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `series` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors) for each step. Note that the index \n            corresponds to that of the last step. It is updated for the corresponding \n            step in the filter_train_X_y_for_step method.\n        y_train : dict\n            Values of the time series related to each row of `X_train` for each \n            step in the form {step: y_step_[i]}.\n        series_names_in_ : list\n            Names of the series used during training.\n        X_train_series_names_in_ : list\n            Names of the series added to `X_train` when creating the training\n            matrices with `_create_train_X_y` method. It is a subset of \n            `series_names_in_`.\n        exog_names_in_ : list\n            Names of the exogenous variables included in the training matrices.\n        X_train_exog_names_out_ : list\n            Names of the exogenous variables included in the matrix `X_train` created\n            internally for training. It can be different from `exog_names_in_` if\n            some exogenous variables are transformed during the training process.\n        X_train_features_names_out_ : list\n            Names of the columns of the matrix created internally for training.\n        exog_dtypes_in_ : dict\n            Type of each exogenous variable/s used in training. If `transformer_exog` \n            is used, the dtypes are calculated before the transformation.\n        \n        \"\"\"\n        if not isinstance(series, pd.DataFrame):\n            raise TypeError(f'`series` must be a pandas DataFrame. Got {type(series)}.')\n        if len(series) < self.window_size + self.steps:\n            raise ValueError(f'Minimum length of `series` for training this forecaster is {self.window_size + self.steps}. Reduce the number of predicted steps, {self.steps}, or the maximum window_size, {self.window_size}, if no more data is available.\\n    Length `series`: {len(series)}.\\n    Max step : {self.steps}.\\n    Max window size: {self.window_size}.\\n    Lags window size: {self.max_lag}.\\n    Window features window size: {self.max_size_window_features}.')\n        series_names_in_ = list(series.columns)\n        if self.level not in series_names_in_:\n            raise ValueError(f'One of the `series` columns must be named as the `level` of the forecaster.\\n  Forecaster `level` : {self.level}.\\n  `series` columns   : {series_names_in_}.')\n        data_to_return_dict, X_train_series_names_in_ = self._create_data_to_return_dict(series_names_in_=series_names_in_)\n        series_to_create_autoreg_features_and_y = [col for col in series_names_in_ if col in X_train_series_names_in_ + [self.level]]\n        fit_transformer = False\n        if not self.is_fitted:\n            fit_transformer = True\n            self.transformer_series_ = initialize_transformer_series(forecaster_name=type(self).__name__, series_names_in_=series_to_create_autoreg_features_and_y, transformer_series=self.transformer_series)\n        if self.differentiation is None:\n            self.differentiator_ = {serie: None for serie in series_to_create_autoreg_features_and_y}\n        elif not self.is_fitted:\n            self.differentiator_ = {serie: copy(self.differentiator) for serie in series_to_create_autoreg_features_and_y}\n        exog_names_in_ = None\n        exog_dtypes_in_ = None\n        categorical_features = False\n        if exog is not None:\n            check_exog(exog=exog, allow_nan=True)\n            exog = input_to_frame(data=exog, input_name='exog')\n            series_index_no_ws = series.index[self.window_size:]\n            len_series = len(series)\n            len_series_no_ws = len_series - self.window_size\n            len_exog = len(exog)\n            if not len_exog == len_series and (not len_exog == len_series_no_ws):\n                raise ValueError(f'Length of `exog` must be equal to the length of `series` (if index is fully aligned) or length of `seriesy` - `window_size` (if `exog` starts after the first `window_size` values).\\n    `exog`                   : ({exog.index[0]} -- {exog.index[-1]})  (n={len_exog})\\n    `series`                 : ({series.index[0]} -- {series.index[-1]})  (n={len_series})\\n    `series` - `window_size` : ({series_index_no_ws[0]} -- {series_index_no_ws[-1]})  (n={len_series_no_ws})')\n            exog_names_in_ = exog.columns.to_list()\n            if len(set(exog_names_in_) - set(series_names_in_)) != len(exog_names_in_):\n                raise ValueError(f'`exog` cannot contain a column named the same as one of the series (column names of series).\\n  `series` columns : {series_names_in_}.\\n  `exog`   columns : {exog_names_in_}.')\n            self.exog_in_ = True\n            exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n            exog = transform_dataframe(df=exog, transformer=self.transformer_exog, fit=fit_transformer, inverse_transform=False)\n            check_exog_dtypes(exog, call_check_exog=True)\n            categorical_features = exog.select_dtypes(include=np.number).shape[1] != exog.shape[1]\n            if len_exog == len_series:\n                if not (exog.index == series.index).all():\n                    raise ValueError('When `exog` has the same length as `series`, the index of `exog` must be aligned with the index of `series` to ensure the correct alignment of values.')\n                exog = exog.iloc[self.window_size:,]\n            elif not (exog.index == series_index_no_ws).all():\n                raise ValueError(\"When `exog` doesn't contain the first `window_size` observations, the index of `exog` must be aligned with the index of `series` minus the first `window_size` observations to ensure the correct alignment of values.\")\n        X_train_autoreg = []\n        X_train_window_features_names_out_ = [] if self.window_features is not None else None\n        X_train_features_names_out_ = []\n        for col in series_to_create_autoreg_features_and_y:\n            y = series[col]\n            check_y(y=y, series_id=f\"Column '{col}'\")\n            y = transform_series(series=y, transformer=self.transformer_series_[col], fit=fit_transformer, inverse_transform=False)\n            y_values, y_index = preprocess_y(y=y)\n            if self.differentiation is not None:\n                if not self.is_fitted:\n                    y_values = self.differentiator_[col].fit_transform(y_values)\n                else:\n                    differentiator = copy(self.differentiator_[col])\n                    y_values = differentiator.fit_transform(y_values)\n            X_train_autoreg_col = []\n            train_index = y_index[self.window_size + (self.steps - 1):]\n            X_train_lags, y_train_values = self._create_lags(y=y_values, lags=self.lags_[col], data_to_return=data_to_return_dict.get(col, None))\n            if X_train_lags is not None:\n                X_train_autoreg_col.append(X_train_lags)\n                X_train_features_names_out_.extend(self.lags_names[col])\n            if col == self.level:\n                y_train = y_train_values\n            if self.window_features is not None:\n                n_diff = 0 if self.differentiation is None else self.differentiation\n                end_wf = None if self.steps == 1 else -(self.steps - 1)\n                y_window_features = pd.Series(y_values[n_diff:end_wf], index=y_index[n_diff:end_wf], name=col)\n                X_train_window_features, X_train_wf_names_out_ = self._create_window_features(y=y_window_features, X_as_pandas=False, train_index=train_index)\n                X_train_autoreg_col.extend(X_train_window_features)\n                X_train_window_features_names_out_.extend(X_train_wf_names_out_)\n                X_train_features_names_out_.extend(X_train_wf_names_out_)\n            if X_train_autoreg_col:\n                if len(X_train_autoreg_col) == 1:\n                    X_train_autoreg_col = X_train_autoreg_col[0]\n                else:\n                    X_train_autoreg_col = np.concatenate(X_train_autoreg_col, axis=1)\n                X_train_autoreg.append(X_train_autoreg_col)\n        X_train = []\n        len_train_index = len(train_index)\n        if categorical_features:\n            if len(X_train_autoreg) == 1:\n                X_train_autoreg = X_train_autoreg[0]\n            else:\n                X_train_autoreg = np.concatenate(X_train_autoreg, axis=1)\n            X_train_autoreg = pd.DataFrame(data=X_train_autoreg, columns=X_train_features_names_out_, index=train_index)\n            X_train.append(X_train_autoreg)\n        else:\n            X_train.extend(X_train_autoreg)\n        self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n        X_train_exog_names_out_ = None\n        if exog is not None:\n            X_train_exog_names_out_ = exog.columns.to_list()\n            if categorical_features:\n                exog_direct, X_train_direct_exog_names_out_ = exog_to_direct(exog=exog, steps=self.steps)\n                exog_direct.index = train_index\n            else:\n                exog_direct, X_train_direct_exog_names_out_ = exog_to_direct_numpy(exog=exog, steps=self.steps)\n            self.X_train_direct_exog_names_out_ = X_train_direct_exog_names_out_\n            X_train_features_names_out_.extend(self.X_train_direct_exog_names_out_)\n            X_train.append(exog_direct)\n        if len(X_train) == 1:\n            X_train = X_train[0]\n        elif categorical_features:\n            X_train = pd.concat(X_train, axis=1)\n        else:\n            X_train = np.concatenate(X_train, axis=1)\n        if categorical_features:\n            X_train.index = train_index\n        else:\n            X_train = pd.DataFrame(data=X_train, index=train_index, columns=X_train_features_names_out_)\n        y_train = {step: pd.Series(data=y_train[:, step - 1], index=y_index[self.window_size + step - 1:][:len_train_index], name=f'{self.level}_step_{step}') for step in range(1, self.steps + 1)}\n        return (X_train, y_train, series_names_in_, X_train_series_names_in_, exog_names_in_, X_train_exog_names_out_, X_train_features_names_out_, exog_dtypes_in_)\n\n    def create_train_X_y(self, series: pd.DataFrame, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, suppress_warnings: bool=False) -> Tuple[pd.DataFrame, dict]:\n        \"\"\"\n        Create training matrices from multiple time series and exogenous\n        variables. The resulting matrices contain the target variable and predictors\n        needed to train all the regressors (one per step).\n        \n        Parameters\n        ----------\n        series : pandas DataFrame\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `series` and their indexes must be aligned.\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the creation\n            of the training matrices. See skforecast.exceptions.warn_skforecast_categories \n            for more information.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors) for each step. Note that the index \n            corresponds to that of the last step. It is updated for the corresponding \n            step in the filter_train_X_y_for_step method.\n        y_train : dict\n            Values of the time series related to each row of `X_train` for each \n            step in the form {step: y_step_[i]}.\n        \n        \"\"\"\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n        output = self._create_train_X_y(series=series, exog=exog)\n        X_train = output[0]\n        y_train = output[1]\n        set_skforecast_warnings(suppress_warnings, action='default')\n        return (X_train, y_train)\n\n    def filter_train_X_y_for_step(self, step: int, X_train: pd.DataFrame, y_train: dict, remove_suffix: bool=False) -> Tuple[pd.DataFrame, pd.Series]:\n        \"\"\"\n        Select the columns needed to train a forecaster for a specific step.  \n        The input matrices should be created using `_create_train_X_y` method. \n        This method updates the index of `X_train` to the corresponding one \n        according to `y_train`. If `remove_suffix=True` the suffix \"_step_i\" \n        will be removed from the column names. \n\n        Parameters\n        ----------\n        step : int\n            step for which columns must be selected selected. Starts at 1.\n        X_train : pandas DataFrame\n            Dataframe created with the `_create_train_X_y` method, first return.\n        y_train : dict\n            Dict created with the `_create_train_X_y` method, second return.\n        remove_suffix : bool, default `False`\n            If True, suffix \"_step_i\" is removed from the column names.\n\n        Returns\n        -------\n        X_train_step : pandas DataFrame\n            Training values (predictors) for the selected step.\n        y_train_step : pandas Series\n            Values of the time series related to each row of `X_train`.\n\n        \"\"\"\n        if step < 1 or step > self.steps:\n            raise ValueError(f'Invalid value `step`. For this forecaster, minimum value is 1 and the maximum step is {self.steps}.')\n        y_train_step = y_train[step]\n        if not self.exog_in_:\n            X_train_step = X_train\n        else:\n            n_lags = len(list(chain(*[v for v in self.lags_.values() if v is not None])))\n            n_window_features = len(self.X_train_window_features_names_out_) if self.window_features is not None else 0\n            idx_columns_autoreg = np.arange(n_lags + n_window_features)\n            n_exog = len(self.X_train_direct_exog_names_out_) / self.steps\n            idx_columns_exog = np.arange((step - 1) * n_exog, step * n_exog) + idx_columns_autoreg[-1] + 1\n            idx_columns = np.concatenate((idx_columns_autoreg, idx_columns_exog))\n            X_train_step = X_train.iloc[:, idx_columns]\n        X_train_step.index = y_train_step.index\n        if remove_suffix:\n            X_train_step.columns = [col_name.replace(f'_step_{step}', '') for col_name in X_train_step.columns]\n            y_train_step.name = y_train_step.name.replace(f'_step_{step}', '')\n        return (X_train_step, y_train_step)\n\n    def create_sample_weights(self, X_train: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Crate weights for each observation according to the forecaster's attribute\n        `weight_func`. \n\n        Parameters\n        ----------\n        X_train : pandas DataFrame\n            Dataframe created with `_create_train_X_y` and filter_train_X_y_for_step`\n            methods, first return.\n\n        Returns\n        -------\n        sample_weight : numpy ndarray\n            Weights to use in `fit` method.\n        \n        \"\"\"\n        sample_weight = None\n        if self.weight_func is not None:\n            sample_weight = self.weight_func(X_train.index)\n        if sample_weight is not None:\n            if np.isnan(sample_weight).any():\n                raise ValueError('The resulting `sample_weight` cannot have NaN values.')\n            if np.any(sample_weight < 0):\n                raise ValueError('The resulting `sample_weight` cannot have negative values.')\n            if np.sum(sample_weight) == 0:\n                raise ValueError('The resulting `sample_weight` cannot be normalized because the sum of the weights is zero.')\n        return sample_weight\n\n    def fit(self, series: pd.DataFrame, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, store_last_window: bool=True, store_in_sample_residuals: bool=True, suppress_warnings: bool=False) -> None:\n        \"\"\"\n        Training Forecaster.\n\n        Additional arguments to be passed to the `fit` method of the regressor \n        can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n        Parameters\n        ----------\n        series : pandas DataFrame\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `series` and their indexes must be aligned so\n            that series[i] is regressed on exog[i].\n        store_last_window : bool, default `True`\n            Whether or not to store the last window (`last_window_`) of training data.\n        store_in_sample_residuals : bool, default `True`\n            If `True`, in-sample residuals will be stored in the forecaster object\n            after fitting (`in_sample_residuals_` attribute).\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the training \n            process. See skforecast.exceptions.warn_skforecast_categories for more\n            information.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n        self.lags_ = None\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.series_names_in_ = None\n        self.exog_in_ = False\n        self.exog_names_in_ = None\n        self.exog_type_in_ = None\n        self.exog_dtypes_in_ = None\n        self.X_train_series_names_in_ = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_ = None\n        self.X_train_direct_exog_names_out_ = None\n        self.X_train_features_names_out_ = None\n        self.in_sample_residuals_ = {step: None for step in range(1, self.steps + 1)}\n        self.is_fitted = False\n        self.fit_date = None\n        X_train, y_train, series_names_in_, X_train_series_names_in_, exog_names_in_, X_train_exog_names_out_, X_train_features_names_out_, exog_dtypes_in_ = self._create_train_X_y(series=series, exog=exog)\n\n        def fit_forecaster(regressor, X_train, y_train, step, store_in_sample_residuals):\n            \"\"\"\n            Auxiliary function to fit each of the forecaster's regressors in parallel.\n\n            Parameters\n            ----------\n            regressor : object\n                Regressor to be fitted.\n            X_train : pandas DataFrame\n                Dataframe created with the `_create_train_X_y` method, first return.\n            y_train : dict\n                Dict created with the `_create_train_X_y` method, second return.\n            step : int\n                Step of the forecaster to be fitted.\n            store_in_sample_residuals : bool\n                If `True`, in-sample residuals will be stored in the forecaster object\n                after fitting (`in_sample_residuals_` attribute).\n            \n            Returns\n            -------\n            Tuple with the step, fitted regressor and in-sample residuals.\n\n            \"\"\"\n            X_train_step, y_train_step = self.filter_train_X_y_for_step(step=step, X_train=X_train, y_train=y_train, remove_suffix=True)\n            sample_weight = self.create_sample_weights(X_train=X_train_step)\n            if sample_weight is not None:\n                regressor.fit(X=X_train_step, y=y_train_step, sample_weight=sample_weight, **self.fit_kwargs)\n            else:\n                regressor.fit(X=X_train_step, y=y_train_step, **self.fit_kwargs)\n            if store_in_sample_residuals:\n                residuals = (y_train_step - regressor.predict(X_train_step)).to_numpy()\n                if len(residuals) > 1000:\n                    rng = np.random.default_rng(seed=123)\n                    residuals = rng.choice(a=residuals, size=1000, replace=False)\n            else:\n                residuals = None\n            return (step, regressor, residuals)\n        results_fit = Parallel(n_jobs=self.n_jobs)((delayed(fit_forecaster)(regressor=copy(self.regressor), X_train=X_train, y_train=y_train, step=step, store_in_sample_residuals=store_in_sample_residuals) for step in range(1, self.steps + 1)))\n        self.regressors_ = {step: regressor for step, regressor, _ in results_fit}\n        if store_in_sample_residuals:\n            self.in_sample_residuals_ = {step: residuals for step, _, residuals in results_fit}\n        self.series_names_in_ = series_names_in_\n        self.X_train_series_names_in_ = X_train_series_names_in_\n        self.X_train_features_names_out_ = X_train_features_names_out_\n        self.is_fitted = True\n        self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.training_range_ = preprocess_y(y=series[self.level], return_values=False)[1][[0, -1]]\n        self.index_type_ = type(X_train.index)\n        if isinstance(X_train.index, pd.DatetimeIndex):\n            self.index_freq_ = X_train.index.freqstr\n        else:\n            self.index_freq_ = X_train.index.step\n        if exog is not None:\n            self.exog_in_ = True\n            self.exog_names_in_ = exog_names_in_\n            self.exog_type_in_ = type(exog)\n            self.exog_dtypes_in_ = exog_dtypes_in_\n            self.X_train_exog_names_out_ = X_train_exog_names_out_\n        if store_last_window:\n            self.last_window_ = series.iloc[-self.window_size:,][self.X_train_series_names_in_].copy()\n        set_skforecast_warnings(suppress_warnings, action='default')\n\n    def _create_predict_inputs(self, steps: Optional[Union[int, list]]=None, last_window: Optional[pd.DataFrame]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, check_inputs: bool=True) -> Tuple[list, list, list, pd.Index]:\n        \"\"\"\n        Create the inputs needed for the prediction process.\n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n\n        Returns\n        -------\n        Xs : list\n            List of numpy arrays with the predictors for each step.\n        Xs_col_names : list\n            Names of the columns of the matrix created internally for prediction.\n        steps : list\n            Steps to predict.\n        prediction_index : pandas Index\n            Index of the predictions.\n        \n        \"\"\"\n        steps = prepare_steps_direct(steps=steps, max_step=self.steps)\n        if last_window is None:\n            last_window = self.last_window_\n        if check_inputs:\n            check_predict_input(forecaster_name=type(self).__name__, steps=steps, is_fitted=self.is_fitted, exog_in_=self.exog_in_, index_type_=self.index_type_, index_freq_=self.index_freq_, window_size=self.window_size, last_window=last_window, exog=exog, exog_type_in_=self.exog_type_in_, exog_names_in_=self.exog_names_in_, interval=None, max_steps=self.steps, series_names_in_=self.X_train_series_names_in_)\n        last_window = last_window.iloc[-self.window_size:, last_window.columns.get_indexer(self.X_train_series_names_in_)].copy()\n        X_autoreg = []\n        Xs_col_names = []\n        for serie in self.X_train_series_names_in_:\n            last_window_serie = transform_numpy(array=last_window[serie].to_numpy(), transformer=self.transformer_series_[serie], fit=False, inverse_transform=False)\n            if self.differentiation is not None:\n                last_window_serie = self.differentiator_[serie].fit_transform(last_window_serie)\n            if self.lags is not None:\n                X_lags = last_window_serie[-self.lags_[serie]]\n                X_autoreg.append(X_lags)\n                Xs_col_names.extend(self.lags_names[serie])\n            if self.window_features is not None:\n                n_diff = 0 if self.differentiation is None else self.differentiation\n                X_window_features = np.concatenate([wf.transform(last_window_serie[n_diff:]) for wf in self.window_features])\n                X_autoreg.append(X_window_features)\n                Xs_col_names.extend([f'{serie}_{wf}' for wf in self.window_features_names])\n        X_autoreg = np.concatenate(X_autoreg).reshape(1, -1)\n        _, last_window_index = preprocess_last_window(last_window=last_window, return_values=False)\n        if exog is not None:\n            exog = input_to_frame(data=exog, input_name='exog')\n            exog = exog.loc[:, self.exog_names_in_]\n            exog = transform_dataframe(df=exog, transformer=self.transformer_exog, fit=False, inverse_transform=False)\n            check_exog_dtypes(exog=exog)\n            exog_values, _ = exog_to_direct_numpy(exog=exog.to_numpy()[:max(steps)], steps=max(steps))\n            exog_values = exog_values[0]\n            n_exog = exog.shape[1]\n            Xs = [np.concatenate([X_autoreg, exog_values[(step - 1) * n_exog:step * n_exog].reshape(1, -1)], axis=1) for step in steps]\n            Xs_col_names = Xs_col_names + exog.columns.to_list()\n        else:\n            Xs = [X_autoreg] * len(steps)\n        prediction_index = expand_index(index=last_window_index, steps=max(steps))[np.array(steps) - 1]\n        if isinstance(last_window_index, pd.DatetimeIndex) and np.array_equal(steps, np.arange(min(steps), max(steps) + 1)):\n            prediction_index.freq = last_window_index.freq\n        return (Xs, Xs_col_names, steps, prediction_index)\n\n    def create_predict_X(self, steps: Optional[Union[int, list]]=None, last_window: Optional[pd.DataFrame]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, suppress_warnings: bool=False) -> pd.DataFrame:\n        \"\"\"\n        Create the predictors needed to predict `steps` ahead.\n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the prediction \n            process. See skforecast.exceptions.warn_skforecast_categories for more\n            information.\n\n        Returns\n        -------\n        X_predict : pandas DataFrame\n            Pandas DataFrame with the predictors for each step. The index \n            is the same as the prediction index.\n        \n        \"\"\"\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n        Xs, Xs_col_names, steps, prediction_index = self._create_predict_inputs(steps=steps, last_window=last_window, exog=exog)\n        X_predict = pd.DataFrame(data=np.concatenate(Xs, axis=0), columns=Xs_col_names, index=prediction_index)\n        if self.transformer_series is not None or self.differentiation is not None:\n            warnings.warn('The output matrix is in the transformed scale due to the inclusion of transformations or differentiation in the Forecaster. As a result, any predictions generated using this matrix will also be in the transformed scale. Please refer to the documentation for more details: https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html', DataTransformationWarning)\n        set_skforecast_warnings(suppress_warnings, action='default')\n        return X_predict\n\n    def predict(self, steps: Optional[Union[int, list]]=None, last_window: Optional[pd.DataFrame]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, suppress_warnings: bool=False, check_inputs: bool=True, levels: Any=None) -> pd.DataFrame:\n        \"\"\"\n        Predict n steps ahead\n\n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the prediction \n            process. See skforecast.exceptions.warn_skforecast_categories for more\n            information.\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n        levels : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Predicted values.\n\n        \"\"\"\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n        Xs, _, steps, prediction_index = self._create_predict_inputs(steps=steps, last_window=last_window, exog=exog, check_inputs=check_inputs)\n        regressors = [self.regressors_[step] for step in steps]\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', message='X does not have valid feature names', category=UserWarning)\n            predictions = np.array([regressor.predict(X).ravel()[0] for regressor, X in zip(regressors, Xs)])\n        if self.differentiation is not None:\n            predictions = self.differentiator_[self.level].inverse_transform_next_window(predictions)\n        predictions = transform_numpy(array=predictions, transformer=self.transformer_series_[self.level], fit=False, inverse_transform=True)\n        predictions = pd.DataFrame(data=predictions, columns=[self.level], index=prediction_index)\n        set_skforecast_warnings(suppress_warnings, action='default')\n        return predictions\n\n    def predict_bootstrapping(self, steps: Optional[Union[int, list]]=None, last_window: Optional[pd.DataFrame]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, suppress_warnings: bool=False, levels: Any=None) -> pd.DataFrame:\n        \"\"\"\n        Generate multiple forecasting predictions using a bootstrapping process. \n        By sampling from a collection of past observed errors (the residuals),\n        each iteration of bootstrapping generates a different set of predictions. \n        See the Notes section for more information. \n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.     \n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.               \n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the prediction \n            process. See skforecast.exceptions.warn_skforecast_categories for more\n            information.\n        levels : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        boot_predictions : pandas DataFrame\n            Predictions generated by bootstrapping.\n            Shape: (steps, n_boot)\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n        Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n        \"\"\"\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n        if self.is_fitted:\n            steps = prepare_steps_direct(steps=steps, max_step=self.steps)\n            if use_in_sample_residuals:\n                if not set(steps).issubset(set(self.in_sample_residuals_.keys())):\n                    raise ValueError(f'Not `forecaster.in_sample_residuals_` for steps: {set(steps) - set(self.in_sample_residuals_.keys())}.')\n                residuals = self.in_sample_residuals_\n            else:\n                if self.out_sample_residuals_ is None:\n                    raise ValueError('`forecaster.out_sample_residuals_` is `None`. Use `use_in_sample_residuals=True` or the `set_out_sample_residuals()` method before predicting.')\n                elif not set(steps).issubset(set(self.out_sample_residuals_.keys())):\n                    raise ValueError(f'Not `forecaster.out_sample_residuals_` for steps: {set(steps) - set(self.out_sample_residuals_.keys())}. Use method `set_out_sample_residuals()`.')\n                residuals = self.out_sample_residuals_\n            check_residuals = 'forecaster.in_sample_residuals_' if use_in_sample_residuals else 'forecaster.out_sample_residuals_'\n            for step in steps:\n                if residuals[step] is None:\n                    raise ValueError(f'forecaster residuals for step {step} are `None`. Check {check_residuals}.')\n                elif any((element is None for element in residuals[step])) or np.any(np.isnan(residuals[step])):\n                    raise ValueError(f'forecaster residuals for step {step} contains `None` or `NaNs` values. Check {check_residuals}.')\n        Xs, _, steps, prediction_index = self._create_predict_inputs(steps=steps, last_window=last_window, exog=exog)\n        regressors = [self.regressors_[step] for step in steps]\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', message='X does not have valid feature names', category=UserWarning)\n            predictions = np.array([regressor.predict(X).ravel()[0] for regressor, X in zip(regressors, Xs)])\n        boot_predictions = np.tile(predictions, (n_boot, 1)).T\n        boot_columns = [f'pred_boot_{i}' for i in range(n_boot)]\n        rng = np.random.default_rng(seed=random_state)\n        for i, step in enumerate(steps):\n            sampled_residuals = residuals[step][rng.integers(low=0, high=len(residuals[step]), size=n_boot)]\n            boot_predictions[i, :] = boot_predictions[i, :] + sampled_residuals\n        if self.differentiation is not None:\n            boot_predictions = self.differentiator_[self.level].inverse_transform_next_window(boot_predictions)\n        if self.transformer_series_[self.level]:\n            boot_predictions = np.apply_along_axis(func1d=transform_numpy, axis=0, arr=boot_predictions, transformer=self.transformer_series_[self.level], fit=False, inverse_transform=True)\n        boot_predictions = pd.DataFrame(data=boot_predictions, index=prediction_index, columns=boot_columns)\n        set_skforecast_warnings(suppress_warnings, action='default')\n        return boot_predictions\n\n    def predict_interval(self, steps: Optional[Union[int, list]]=None, last_window: Optional[pd.DataFrame]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, interval: list=[5, 95], n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, suppress_warnings: bool=False, levels: Any=None) -> pd.DataFrame:\n        \"\"\"\n        Bootstrapping based predicted intervals.\n        Both predictions and intervals are returned.\n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        interval : list, default `[5, 95]`\n            Confidence of the prediction interval estimated. Sequence of \n            percentiles to compute, which must be between 0 and 100 inclusive. \n            For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the prediction \n            process. See skforecast.exceptions.warn_skforecast_categories for more\n            information.\n        levels : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Values predicted by the forecaster and their estimated interval.\n\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n        check_interval(interval=interval)\n        boot_predictions = self.predict_bootstrapping(steps=steps, last_window=last_window, exog=exog, n_boot=n_boot, random_state=random_state, use_in_sample_residuals=use_in_sample_residuals)\n        predictions = self.predict(steps=steps, last_window=last_window, exog=exog, check_inputs=False)\n        interval = np.array(interval) / 100\n        predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n        predictions_interval.columns = [f'{self.level}_lower_bound', f'{self.level}_upper_bound']\n        predictions = pd.concat((predictions, predictions_interval), axis=1)\n        set_skforecast_warnings(suppress_warnings, action='default')\n        return predictions\n\n    def predict_quantiles(self, steps: Optional[Union[int, list]]=None, last_window: Optional[pd.DataFrame]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, quantiles: list=[0.05, 0.5, 0.95], n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, suppress_warnings: bool=False, levels: Any=None) -> pd.DataFrame:\n        \"\"\"\n        Bootstrapping based predicted quantiles.\n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        quantiles : list, default `[0.05, 0.5, 0.95]`\n            Sequence of quantiles to compute, which must be between 0 and 1 \n            inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n            `quantiles = [0.05, 0.5, 0.95]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate quantiles.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot quantiles are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create quantiles. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the prediction \n            process. See skforecast.exceptions.warn_skforecast_categories for more\n            information.\n        levels : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Quantiles predicted by the forecaster.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n        check_interval(quantiles=quantiles)\n        boot_predictions = self.predict_bootstrapping(steps=steps, last_window=last_window, exog=exog, n_boot=n_boot, random_state=random_state, use_in_sample_residuals=use_in_sample_residuals)\n        predictions = boot_predictions.quantile(q=quantiles, axis=1).transpose()\n        predictions.columns = [f'{self.level}_q_{q}' for q in quantiles]\n        set_skforecast_warnings(suppress_warnings, action='default')\n        return predictions\n\n    def predict_dist(self, distribution: object, steps: Optional[Union[int, list]]=None, last_window: Optional[pd.DataFrame]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, suppress_warnings: bool=False, levels: Any=None) -> pd.DataFrame:\n        \"\"\"\n        Fit a given probability distribution for each step. After generating \n        multiple forecasting predictions through a bootstrapping process, each \n        step is fitted to the given distribution.\n        \n        Parameters\n        ----------\n        distribution : Object\n            A distribution object from scipy.stats.\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the prediction \n            process. See skforecast.exceptions.warn_skforecast_categories for more\n            information.\n        levels : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Distribution parameters estimated for each step.\n\n        \"\"\"\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n        boot_samples = self.predict_bootstrapping(steps=steps, last_window=last_window, exog=exog, n_boot=n_boot, random_state=random_state, use_in_sample_residuals=use_in_sample_residuals)\n        param_names = [p for p in inspect.signature(distribution._pdf).parameters if not p == 'x'] + ['loc', 'scale']\n        param_values = np.apply_along_axis(lambda x: distribution.fit(x), axis=1, arr=boot_samples)\n        level_param_names = [f'{self.level}_{p}' for p in param_names]\n        predictions = pd.DataFrame(data=param_values, columns=level_param_names, index=boot_samples.index)\n        set_skforecast_warnings(suppress_warnings, action='default')\n        return predictions\n\n    def set_params(self, params: dict) -> None:\n        \"\"\"\n        Set new values to the parameters of the scikit learn model stored in the\n        forecaster. It is important to note that all models share the same \n        configuration of parameters and hyperparameters.\n        \n        Parameters\n        ----------\n        params : dict\n            Parameters values.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        self.regressor = clone(self.regressor)\n        self.regressor.set_params(**params)\n        self.regressors_ = {step: clone(self.regressor) for step in range(1, self.steps + 1)}\n\n    def set_fit_kwargs(self, fit_kwargs: dict) -> None:\n        \"\"\"\n        Set new values for the additional keyword arguments passed to the `fit` \n        method of the regressor.\n        \n        Parameters\n        ----------\n        fit_kwargs : dict\n            Dict of the form {\"argument\": new_value}.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n\n    def set_lags(self, lags: Optional[Union[int, list, np.ndarray, range, dict]]=None) -> None:\n        \"\"\"\n        Set new value to the attribute `lags`. Attributes `lags_names`, \n        `max_lag` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        lags : int, list, numpy ndarray, range, dict, default `None`\n            Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n\n            - `int`: include lags from 1 to `lags` (included).\n            - `list`, `1d numpy ndarray` or `range`: include only lags present in \n            `lags`, all elements must be int.\n            - `dict`: create different lags for each series. {'series_column_name': lags}.\n            - `None`: no lags are included as predictors. \n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        if self.window_features is None and lags is None:\n            raise ValueError('At least one of the arguments `lags` or `window_features` must be different from None. This is required to create the predictors used in training the forecaster.')\n        if isinstance(lags, dict):\n            self.lags = {}\n            self.lags_names = {}\n            list_max_lags = []\n            for key in lags:\n                if lags[key] is None:\n                    self.lags[key] = None\n                    self.lags_names[key] = None\n                else:\n                    self.lags[key], lags_names, max_lag = initialize_lags(forecaster_name=type(self).__name__, lags=lags[key])\n                    self.lags_names[key] = [f'{key}_{lag}' for lag in lags_names] if lags_names is not None else None\n                    if max_lag is not None:\n                        list_max_lags.append(max_lag)\n            self.max_lag = max(list_max_lags) if len(list_max_lags) != 0 else None\n        else:\n            self.lags, self.lags_names, self.max_lag = initialize_lags(forecaster_name=type(self).__name__, lags=lags)\n        if self.window_features is None and (lags is None or self.max_lag is None):\n            raise ValueError('At least one of the arguments `lags` or `window_features` must be different from None. This is required to create the predictors used in training the forecaster.')\n        self.window_size = max([ws for ws in [self.max_lag, self.max_size_window_features] if ws is not None])\n        if self.differentiation is not None:\n            self.window_size += self.differentiation\n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_window_features(self, window_features: Optional[Union[object, list]]=None) -> None:\n        \"\"\"\n        Set new value to the attribute `window_features`. Attributes \n        `max_size_window_features`, `window_features_names`, \n        `window_features_class_names` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        window_features : object, list, default `None`\n            Instance or list of instances used to create window features. Window features\n            are created from the original time series and are included as predictors.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        if window_features is None and self.max_lag is None:\n            raise ValueError('At least one of the arguments `lags` or `window_features` must be different from None. This is required to create the predictors used in training the forecaster.')\n        self.window_features, self.window_features_names, self.max_size_window_features = initialize_window_features(window_features)\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [type(wf).__name__ for wf in self.window_features]\n        self.window_size = max([ws for ws in [self.max_lag, self.max_size_window_features] if ws is not None])\n        if self.differentiation is not None:\n            self.window_size += self.differentiation\n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_out_sample_residuals(self, y_true: dict, y_pred: dict, append: bool=False, random_state: int=123) -> None:\n        \"\"\"\n        Set new values to the attribute `out_sample_residuals_`. Out of sample\n        residuals are meant to be calculated using observations that did not\n        participate in the training process. `y_true` and `y_pred` are expected\n        to be in the original scale of the time series. Residuals are calculated\n        as `y_true` - `y_pred`, after applying the necessary transformations and\n        differentiations if the forecaster includes them (`self.transformer_series`\n        and `self.differentiation`).\n\n        A total of 10_000 residuals are stored in the attribute `out_sample_residuals_`.\n        If the number of residuals is greater than 10_000, a random sample of\n        10_000 residuals is stored.\n        \n        Parameters\n        ----------\n        y_true : dict\n            Dictionary of numpy ndarrays or pandas Series with the true values of\n            the time series for each model in the form {step: y_true}.\n        y_pred : dict\n            Dictionary of numpy ndarrays or pandas Series with the predicted values\n            of the time series for each model in the form {step: y_pred}.\n        append : bool, default `False`\n            If `True`, new residuals are added to the once already stored in the\n            attribute `out_sample_residuals_`. If after appending the new residuals,\n            the limit of 10_000 samples is exceeded, a random sample of 10_000 is\n            kept.\n        random_state : int, default `123`\n            Sets a seed to the random sampling for reproducible output.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n        if not self.is_fitted:\n            raise NotFittedError('This forecaster is not fitted yet. Call `fit` with appropriate arguments before using `set_out_sample_residuals()`.')\n        if not isinstance(y_true, dict):\n            raise TypeError(f'`y_true` must be a dictionary of numpy ndarrays or pandas Series. Got {type(y_true)}.')\n        if not isinstance(y_pred, dict):\n            raise TypeError(f'`y_pred` must be a dictionary of numpy ndarrays or pandas Series. Got {type(y_pred)}.')\n        if not set(y_true.keys()) == set(y_pred.keys()):\n            raise ValueError(f'`y_true` and `y_pred` must have the same keys. Got {set(y_true.keys())} and {set(y_pred.keys())}.')\n        for k in y_true.keys():\n            if not isinstance(y_true[k], (np.ndarray, pd.Series)):\n                raise TypeError(f'Values of `y_true` must be numpy ndarrays or pandas Series. Got {type(y_true[k])} for step {k}.')\n            if not isinstance(y_pred[k], (np.ndarray, pd.Series)):\n                raise TypeError(f'Values of `y_pred` must be numpy ndarrays or pandas Series. Got {type(y_pred[k])} for step {k}.')\n            if len(y_true[k]) != len(y_pred[k]):\n                raise ValueError(f'`y_true` and `y_pred` must have the same length. Got {len(y_true[k])} and {len(y_pred[k])} for step {k}.')\n            if isinstance(y_true[k], pd.Series) and isinstance(y_pred[k], pd.Series):\n                if not y_true[k].index.equals(y_pred[k].index):\n                    raise ValueError(f'When containing pandas Series, elements in `y_true` and `y_pred` must have the same index. Error in step {k}.')\n        if self.out_sample_residuals_ is None:\n            self.out_sample_residuals_ = {step: None for step in range(1, self.steps + 1)}\n        steps_to_update = set(range(1, self.steps + 1)).intersection(set(y_pred.keys()))\n        if not steps_to_update:\n            raise ValueError('Provided keys in `y_pred` and `y_true` do not match any step. Residuals cannot be updated.')\n        residuals = {}\n        rng = np.random.default_rng(seed=random_state)\n        y_true = y_true.copy()\n        y_pred = y_pred.copy()\n        if self.differentiation is not None:\n            differentiator = copy(self.differentiator)\n            differentiator.set_params(window_size=None)\n        for k in steps_to_update:\n            if isinstance(y_true[k], pd.Series):\n                y_true[k] = y_true[k].to_numpy()\n            if isinstance(y_pred[k], pd.Series):\n                y_pred[k] = y_pred[k].to_numpy()\n            if self.transformer_series:\n                y_true[k] = transform_numpy(array=y_true[k], transformer=self.transformer_series_[self.level], fit=False, inverse_transform=False)\n                y_pred[k] = transform_numpy(array=y_pred[k], transformer=self.transformer_series_[self.level], fit=False, inverse_transform=False)\n            if self.differentiation is not None:\n                y_true[k] = differentiator.fit_transform(y_true[k])[self.differentiation:]\n                y_pred[k] = differentiator.fit_transform(y_pred[k])[self.differentiation:]\n            residuals[k] = y_true[k] - y_pred[k]\n        for key, value in residuals.items():\n            if append and self.out_sample_residuals_[key] is not None:\n                value = np.concatenate((self.out_sample_residuals_[key], value))\n            if len(value) > 10000:\n                value = rng.choice(value, size=10000, replace=False)\n            self.out_sample_residuals_[key] = value\n\n    def get_feature_importances(self, step: int, sort_importance: bool=True) -> pd.DataFrame:\n        \"\"\"\n        Return feature importance of the model stored in the forecaster for a\n        specific step. Since a separate model is created for each forecast time\n        step, it is necessary to select the model from which retrieve information.\n        Only valid when regressor stores internally the feature importances in\n        the attribute `feature_importances_` or `coef_`. Otherwise, it returns  \n        `None`.\n\n        Parameters\n        ----------\n        step : int\n            Model from which retrieve information (a separate model is created \n            for each forecast time step). First step is 1.\n        sort_importance: bool, default `True`\n            If `True`, sorts the feature importances in descending order.\n\n        Returns\n        -------\n        feature_importances : pandas DataFrame\n            Feature importances associated with each predictor.\n        \n        \"\"\"\n        if not isinstance(step, int):\n            raise TypeError(f'`step` must be an integer. Got {type(step)}.')\n        if not self.is_fitted:\n            raise NotFittedError('This forecaster is not fitted yet. Call `fit` with appropriate arguments before using `get_feature_importances()`.')\n        if step < 1 or step > self.steps:\n            raise ValueError(f'The step must have a value from 1 to the maximum number of steps ({self.steps}). Got {step}.')\n        if isinstance(self.regressor, Pipeline):\n            estimator = self.regressors_[step][-1]\n        else:\n            estimator = self.regressors_[step]\n        n_lags = len(list(chain(*[v for v in self.lags_.values() if v is not None])))\n        n_window_features = len(self.X_train_window_features_names_out_) if self.window_features is not None else 0\n        idx_columns_autoreg = np.arange(n_lags + n_window_features)\n        if self.exog_in_:\n            idx_columns_exog = np.flatnonzero([name.endswith(f'step_{step}') for name in self.X_train_features_names_out_])\n        else:\n            idx_columns_exog = np.array([], dtype=int)\n        idx_columns = np.concatenate((idx_columns_autoreg, idx_columns_exog))\n        idx_columns = [int(x) for x in idx_columns]\n        feature_names = [self.X_train_features_names_out_[i].replace(f'_step_{step}', '') for i in idx_columns]\n        if hasattr(estimator, 'feature_importances_'):\n            feature_importances = estimator.feature_importances_\n        elif hasattr(estimator, 'coef_'):\n            feature_importances = estimator.coef_\n        else:\n            warnings.warn(f'Impossible to access feature importances for regressor of type {type(estimator)}. This method is only valid when the regressor stores internally the feature importances in the attribute `feature_importances_` or `coef_`.')\n            feature_importances = None\n        if feature_importances is not None:\n            feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importances})\n            if sort_importance:\n                feature_importances = feature_importances.sort_values(by='importance', ascending=False)\n        return feature_importances",
    "skforecast/model_selection/_utils.py": "from typing import Union, Tuple, Optional, Callable, Generator\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom joblib import cpu_count\nfrom tqdm.auto import tqdm\nfrom sklearn.pipeline import Pipeline\nimport sklearn.linear_model\nfrom sklearn.exceptions import NotFittedError\nfrom ..exceptions import IgnoredArgumentWarning\nfrom ..metrics import add_y_train_argument, _get_metric\nfrom ..utils import check_interval\n\ndef initialize_lags_grid(forecaster: object, lags_grid: Optional[Union[list, dict]]=None) -> Tuple[dict, str]:\n    \"\"\"\n    Initialize lags grid and lags label for model selection. \n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model. ForecasterRecursive, ForecasterDirect, \n        ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate.\n    lags_grid : list, dict, default `None`\n        Lists of lags to try, containing int, lists, numpy ndarray, or range \n        objects. If `dict`, the keys are used as labels in the `results` \n        DataFrame, and the values are used as the lists of lags to try.\n\n    Returns\n    -------\n    lags_grid : dict\n        Dictionary with lags configuration for each iteration.\n    lags_label : str\n        Label for lags representation in the results object.\n\n    \"\"\"\n    if not isinstance(lags_grid, (list, dict, type(None))):\n        raise TypeError(f'`lags_grid` argument must be a list, dict or None. Got {type(lags_grid)}.')\n    lags_label = 'values'\n    if isinstance(lags_grid, list):\n        lags_grid = {f'{lags}': lags for lags in lags_grid}\n    elif lags_grid is None:\n        lags = [int(lag) for lag in forecaster.lags]\n        lags_grid = {f'{lags}': lags}\n    else:\n        lags_label = 'keys'\n    return (lags_grid, lags_label)\n\ndef check_backtesting_input(forecaster: object, cv: object, metric: Union[str, Callable, list], add_aggregated_metric: bool=True, y: Optional[pd.Series]=None, series: Optional[Union[pd.DataFrame, dict]]=None, exog: Optional[Union[pd.Series, pd.DataFrame, dict]]=None, interval: Optional[list]=None, alpha: Optional[float]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False, n_jobs: Union[int, str]='auto', show_progress: bool=True, suppress_warnings: bool=False, suppress_warnings_fit: bool=False) -> None:\n    \"\"\"\n    This is a helper function to check most inputs of backtesting functions in \n    modules `model_selection`.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model.\n    cv : TimeSeriesFold\n        TimeSeriesFold object with the information needed to split the data into folds.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n    add_aggregated_metric : bool, default `True`\n        If `True`, the aggregated metrics (average, weighted average and pooling)\n        over all levels are also returned (only multiseries).\n    y : pandas Series, default `None`\n        Training time series for uni-series forecasters.\n    series : pandas DataFrame, dict, default `None`\n        Training time series for multi-series forecasters.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive.\n    alpha : float, default `None`\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %. \n    n_boot : int, default `250`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of prediction \n        error to create prediction intervals.  If `False`, out_sample_residuals \n        are used if they are already stored inside the forecaster.\n    use_binned_residuals : bool, default `False`\n        If `True`, residuals used in each bootstrapping iteration are selected\n        conditioning on the predicted values. If `False`, residuals are selected\n        randomly without conditioning on the predicted values.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the fuction\n        skforecast.utils.select_n_jobs_fit_forecaster.\n        **New in version 0.9.0**\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n    suppress_warnings: bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the backtesting \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    suppress_warnings_fit : bool, default `False`\n        If `True`, warnings generated during fitting will be ignored. Only \n        `ForecasterSarimax`.\n\n    Returns\n    -------\n    None\n    \n    \"\"\"\n    forecaster_name = type(forecaster).__name__\n    cv_name = type(cv).__name__\n    if cv_name != 'TimeSeriesFold':\n        raise TypeError(f'`cv` must be a TimeSeriesFold object. Got {cv_name}.')\n    steps = cv.steps\n    initial_train_size = cv.initial_train_size\n    gap = cv.gap\n    allow_incomplete_fold = cv.allow_incomplete_fold\n    refit = cv.refit\n    forecasters_uni = ['ForecasterRecursive', 'ForecasterDirect', 'ForecasterSarimax', 'ForecasterEquivalentDate']\n    forecasters_multi = ['ForecasterDirectMultiVariate', 'ForecasterRnn']\n    forecasters_multi_dict = ['ForecasterRecursiveMultiSeries']\n    if forecaster_name in forecasters_uni:\n        if not isinstance(y, pd.Series):\n            raise TypeError('`y` must be a pandas Series.')\n        data_name = 'y'\n        data_length = len(y)\n    elif forecaster_name in forecasters_multi:\n        if not isinstance(series, pd.DataFrame):\n            raise TypeError('`series` must be a pandas DataFrame.')\n        data_name = 'series'\n        data_length = len(series)\n    elif forecaster_name in forecasters_multi_dict:\n        if not isinstance(series, (pd.DataFrame, dict)):\n            raise TypeError(f'`series` must be a pandas DataFrame or a dict of DataFrames or Series. Got {type(series)}.')\n        data_name = 'series'\n        if isinstance(series, dict):\n            not_valid_series = [k for k, v in series.items() if not isinstance(v, (pd.Series, pd.DataFrame))]\n            if not_valid_series:\n                raise TypeError(f'If `series` is a dictionary, all series must be a named pandas Series or a pandas DataFrame with a single column. Review series: {not_valid_series}')\n            not_valid_index = [k for k, v in series.items() if not isinstance(v.index, pd.DatetimeIndex)]\n            if not_valid_index:\n                raise ValueError(f'If `series` is a dictionary, all series must have a Pandas DatetimeIndex as index with the same frequency. Review series: {not_valid_index}')\n            indexes_freq = [f'{v.index.freq}' for v in series.values()]\n            indexes_freq = sorted(set(indexes_freq))\n            if not len(indexes_freq) == 1:\n                raise ValueError(f'If `series` is a dictionary, all series must have a Pandas DatetimeIndex as index with the same frequency. Found frequencies: {indexes_freq}')\n            data_length = max([len(series[serie]) for serie in series])\n        else:\n            data_length = len(series)\n    if exog is not None:\n        if forecaster_name in forecasters_multi_dict:\n            if not isinstance(exog, (pd.Series, pd.DataFrame, dict)):\n                raise TypeError(f'`exog` must be a pandas Series, DataFrame, dictionary of pandas Series/DataFrames or None. Got {type(exog)}.')\n            if isinstance(exog, dict):\n                not_valid_exog = [k for k, v in exog.items() if not isinstance(v, (pd.Series, pd.DataFrame, type(None)))]\n                if not_valid_exog:\n                    raise TypeError(f'If `exog` is a dictionary, All exog must be a named pandas Series, a pandas DataFrame or None. Review exog: {not_valid_exog}')\n        elif not isinstance(exog, (pd.Series, pd.DataFrame)):\n            raise TypeError(f'`exog` must be a pandas Series, DataFrame or None. Got {type(exog)}.')\n    if hasattr(forecaster, 'differentiation'):\n        if forecaster.differentiation != cv.differentiation:\n            raise ValueError(f'The differentiation included in the forecaster ({forecaster.differentiation}) differs from the differentiation included in the cv ({cv.differentiation}). Set the same value for both using the `differentiation` argument.')\n    if not isinstance(metric, (str, Callable, list)):\n        raise TypeError(f'`metric` must be a string, a callable function, or a list containing multiple strings and/or callables. Got {type(metric)}.')\n    if forecaster_name == 'ForecasterEquivalentDate' and isinstance(forecaster.offset, pd.tseries.offsets.DateOffset):\n        if initial_train_size is None:\n            raise ValueError(f'`initial_train_size` must be an integer greater than the `window_size` of the forecaster ({forecaster.window_size}) and smaller than the length of `{data_name}` ({data_length}).')\n    elif initial_train_size is not None:\n        if initial_train_size < forecaster.window_size or initial_train_size >= data_length:\n            raise ValueError(f'If used, `initial_train_size` must be an integer greater than the `window_size` of the forecaster ({forecaster.window_size}) and smaller than the length of `{data_name}` ({data_length}).')\n        if initial_train_size + gap >= data_length:\n            raise ValueError(f'The combination of initial_train_size {initial_train_size} and gap {gap} cannot be greater than the length of `{data_name}` ({data_length}).')\n    elif forecaster_name in ['ForecasterSarimax', 'ForecasterEquivalentDate']:\n        raise ValueError(f'`initial_train_size` must be an integer smaller than the length of `{data_name}` ({data_length}).')\n    else:\n        if not forecaster.is_fitted:\n            raise NotFittedError('`forecaster` must be already trained if no `initial_train_size` is provided.')\n        if refit:\n            raise ValueError('`refit` is only allowed when `initial_train_size` is not `None`.')\n    if forecaster_name == 'ForecasterSarimax' and cv.skip_folds is not None:\n        raise ValueError('`skip_folds` is not allowed for ForecasterSarimax. Set it to `None`.')\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError('`add_aggregated_metric` must be a boolean: `True`, `False`.')\n    if not isinstance(n_boot, (int, np.integer)) or n_boot < 0:\n        raise TypeError(f'`n_boot` must be an integer greater than 0. Got {n_boot}.')\n    if not isinstance(random_state, (int, np.integer)) or random_state < 0:\n        raise TypeError(f'`random_state` must be an integer greater than 0. Got {random_state}.')\n    if not isinstance(use_in_sample_residuals, bool):\n        raise TypeError('`use_in_sample_residuals` must be a boolean: `True`, `False`.')\n    if not isinstance(use_binned_residuals, bool):\n        raise TypeError('`use_binned_residuals` must be a boolean: `True`, `False`.')\n    if not isinstance(n_jobs, int) and n_jobs != 'auto':\n        raise TypeError(f\"`n_jobs` must be an integer or `'auto'`. Got {n_jobs}.\")\n    if not isinstance(show_progress, bool):\n        raise TypeError('`show_progress` must be a boolean: `True`, `False`.')\n    if not isinstance(suppress_warnings, bool):\n        raise TypeError('`suppress_warnings` must be a boolean: `True`, `False`.')\n    if not isinstance(suppress_warnings_fit, bool):\n        raise TypeError('`suppress_warnings_fit` must be a boolean: `True`, `False`.')\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n    if not allow_incomplete_fold and data_length - (initial_train_size + gap) < steps:\n        raise ValueError(f'There is not enough data to evaluate {steps} steps in a single fold. Set `allow_incomplete_fold` to `True` to allow incomplete folds.\\n    Data available for test : {data_length - (initial_train_size + gap)}\\n    Steps                   : {steps}')\n\ndef select_n_jobs_backtesting(forecaster: object, refit: Union[bool, int]) -> int:\n    \"\"\"\n    Select the optimal number of jobs to use in the backtesting process. This\n    selection is based on heuristics and is not guaranteed to be optimal.\n\n    The number of jobs is chosen as follows:\n\n    - If `refit` is an integer, then `n_jobs = 1`. This is because parallelization doesn't \n    work with intermittent refit.\n    - If forecaster is 'ForecasterRecursive' and regressor is a linear regressor, \n    then `n_jobs = 1`.\n    - If forecaster is 'ForecasterRecursive' and regressor is not a linear \n    regressor then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and `refit = True`, then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and `refit = False`, then `n_jobs = 1`.\n    - If forecaster is 'ForecasterRecursiveMultiSeries', then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterSarimax' or 'ForecasterEquivalentDate', \n    then `n_jobs = 1`.\n    - If regressor is a `LGBMRegressor(n_jobs=1)`, then `n_jobs = cpu_count() - 1`.\n    - If regressor is a `LGBMRegressor` with internal n_jobs != 1, then `n_jobs = 1`.\n    This is because `lightgbm` is highly optimized for gradient boosting and\n    parallelizes operations at a very fine-grained level, making additional\n    parallelization unnecessary and potentially harmful due to resource contention.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model.\n    refit : bool, int\n        If the forecaster is refitted during the backtesting process.\n\n    Returns\n    -------\n    n_jobs : int\n        The number of jobs to run in parallel.\n    \n    \"\"\"\n    forecaster_name = type(forecaster).__name__\n    if isinstance(forecaster.regressor, Pipeline):\n        regressor = forecaster.regressor[-1]\n        regressor_name = type(regressor).__name__\n    else:\n        regressor = forecaster.regressor\n        regressor_name = type(regressor).__name__\n    linear_regressors = [regressor_name for regressor_name in dir(sklearn.linear_model) if not regressor_name.startswith('_')]\n    refit = False if refit == 0 else refit\n    if not isinstance(refit, bool) and refit != 1:\n        n_jobs = 1\n    elif forecaster_name in ['ForecasterRecursive']:\n        if regressor_name in linear_regressors:\n            n_jobs = 1\n        elif regressor_name == 'LGBMRegressor':\n            n_jobs = cpu_count() - 1 if regressor.n_jobs == 1 else 1\n        else:\n            n_jobs = cpu_count() - 1\n    elif forecaster_name in ['ForecasterDirect', 'ForecasterDirectMultiVariate']:\n        n_jobs = 1\n    elif forecaster_name in ['ForecasterRecursiveMultiSeries']:\n        if regressor_name == 'LGBMRegressor':\n            n_jobs = cpu_count() - 1 if regressor.n_jobs == 1 else 1\n        else:\n            n_jobs = cpu_count() - 1\n    elif forecaster_name in ['ForecasterSarimax', 'ForecasterEquivalentDate']:\n        n_jobs = 1\n    else:\n        n_jobs = 1\n    return n_jobs\n\ndef _calculate_metrics_one_step_ahead(forecaster: object, y: pd.Series, metrics: list, X_train: pd.DataFrame, y_train: Union[pd.Series, dict], X_test: pd.DataFrame, y_test: Union[pd.Series, dict]) -> list:\n    \"\"\"\n    Calculate metrics when predictions are one-step-ahead. When forecaster is\n    of type ForecasterDirect only the regressor for step 1 is used.\n\n    Parameters\n    ----------\n    forecaster : object\n        Forecaster model.\n    y : pandas Series\n        Time series data used to train and test the model.\n    metrics : list\n        List of metrics.\n    X_train : pandas DataFrame\n        Predictor values used to train the model.\n    y_train : pandas Series\n        Target values related to each row of `X_train`.\n    X_test : pandas DataFrame\n        Predictor values used to test the model.\n    y_test : pandas Series\n        Target values related to each row of `X_test`.\n\n    Returns\n    -------\n    metric_values : list\n        List with metric values.\n    \n    \"\"\"\n    if type(forecaster).__name__ == 'ForecasterDirect':\n        step = 1\n        X_train, y_train = forecaster.filter_train_X_y_for_step(step=step, X_train=X_train, y_train=y_train)\n        X_test, y_test = forecaster.filter_train_X_y_for_step(step=step, X_train=X_test, y_train=y_test)\n        forecaster.regressors_[step].fit(X_train, y_train)\n        y_pred = forecaster.regressors_[step].predict(X_test)\n    else:\n        forecaster.regressor.fit(X_train, y_train)\n        y_pred = forecaster.regressor.predict(X_test)\n    y_true = y_test.to_numpy()\n    y_pred = y_pred.ravel()\n    y_train = y_train.to_numpy()\n    if forecaster.differentiation is not None:\n        y_true = forecaster.differentiator.inverse_transform_next_window(y_true)\n        y_pred = forecaster.differentiator.inverse_transform_next_window(y_pred)\n        y_train = forecaster.differentiator.inverse_transform_training(y_train)\n    if forecaster.transformer_y is not None:\n        y_true = forecaster.transformer_y.inverse_transform(y_true.reshape(-1, 1))\n        y_pred = forecaster.transformer_y.inverse_transform(y_pred.reshape(-1, 1))\n        y_train = forecaster.transformer_y.inverse_transform(y_train.reshape(-1, 1))\n    metric_values = []\n    for m in metrics:\n        metric_values.append(m(y_true=y_true.ravel(), y_pred=y_pred.ravel(), y_train=y_train.ravel()))\n    return metric_values\n\ndef _initialize_levels_model_selection_multiseries(forecaster: object, series: Union[pd.DataFrame, dict], levels: Optional[Union[str, list]]=None) -> list:\n    \"\"\"\n    Initialize levels for model_selection multi-series functions.\n\n    Parameters\n    ----------\n    forecaster : ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate, ForecasterRnn\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Training time series.\n    levels : str, list, default `None`\n        level (`str`) or levels (`list`) at which the forecaster is optimized. \n        If `None`, all levels are taken into account. The resulting metric will be\n        the average of the optimization of all levels.\n\n    Returns\n    -------\n    levels : list\n        List of levels to be used in model_selection multi-series functions.\n    \n    \"\"\"\n    multi_series_forecasters_with_levels = ['ForecasterRecursiveMultiSeries', 'ForecasterRnn']\n    if type(forecaster).__name__ in multi_series_forecasters_with_levels and (not isinstance(levels, (str, list, type(None)))):\n        raise TypeError(f'`levels` must be a `list` of column names, a `str` of a column name or `None` when using a forecaster of type {multi_series_forecasters_with_levels}. If the forecaster is of type `ForecasterDirectMultiVariate`, this argument is ignored.')\n    if type(forecaster).__name__ == 'ForecasterDirectMultiVariate':\n        if levels and levels != forecaster.level and (levels != [forecaster.level]):\n            warnings.warn(f\"`levels` argument have no use when the forecaster is of type `ForecasterDirectMultiVariate`. The level of this forecaster is '{forecaster.level}', to predict another level, change the `level` argument when initializing the forecaster. \\n\", IgnoredArgumentWarning)\n        levels = [forecaster.level]\n    elif levels is None:\n        if isinstance(series, pd.DataFrame):\n            levels = list(series.columns)\n        else:\n            levels = list(series.keys())\n    elif isinstance(levels, str):\n        levels = [levels]\n    return levels\n\ndef _calculate_metrics_backtesting_multiseries(series: Union[pd.DataFrame, dict], predictions: pd.DataFrame, folds: Union[list, tqdm], span_index: Union[pd.DatetimeIndex, pd.RangeIndex], window_size: int, metrics: list, levels: list, add_aggregated_metric: bool=True) -> pd.DataFrame:\n    \"\"\"   \n    Calculate metrics for each level and also for all levels aggregated using\n    average, weighted average or pooling.\n\n    - 'average': the average (arithmetic mean) of all levels.\n    - 'weighted_average': the average of the metrics weighted by the number of\n    predicted values of each level.\n    - 'pooling': the values of all levels are pooled and then the metric is\n    calculated.\n\n    Parameters\n    ----------\n    series : pandas DataFrame, dict\n        Series data used for backtesting.\n    predictions : pandas DataFrame\n        Predictions generated during the backtesting process.\n    folds : list, tqdm\n        Folds created during the backtesting process.\n    span_index : pandas DatetimeIndex, pandas RangeIndex\n        Full index from the minimum to the maximum index among all series.\n    window_size : int\n        Size of the window used by the forecaster to create the predictors.\n        This is used remove the first `window_size` (differentiation included) \n        values from y_train since they are not part of the training matrix.\n    metrics : list\n        List of metrics to calculate.\n    levels : list\n        Levels to calculate the metrics.\n    add_aggregated_metric : bool, default `True`\n        If `True`, and multiple series (`levels`) are predicted, the aggregated\n        metrics (average, weighted average and pooled) are also returned.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s).\n    \n    \"\"\"\n    if not isinstance(series, (pd.DataFrame, dict)):\n        raise TypeError('`series` must be a pandas DataFrame or a dictionary of pandas DataFrames.')\n    if not isinstance(predictions, pd.DataFrame):\n        raise TypeError('`predictions` must be a pandas DataFrame.')\n    if not isinstance(folds, (list, tqdm)):\n        raise TypeError('`folds` must be a list or a tqdm object.')\n    if not isinstance(span_index, (pd.DatetimeIndex, pd.RangeIndex)):\n        raise TypeError('`span_index` must be a pandas DatetimeIndex or pandas RangeIndex.')\n    if not isinstance(window_size, (int, np.integer)):\n        raise TypeError('`window_size` must be an integer.')\n    if not isinstance(metrics, list):\n        raise TypeError('`metrics` must be a list.')\n    if not isinstance(levels, list):\n        raise TypeError('`levels` must be a list.')\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError('`add_aggregated_metric` must be a boolean.')\n    metric_names = [m if isinstance(m, str) else m.__name__ for m in metrics]\n    y_true_pred_levels = []\n    y_train_levels = []\n    for level in levels:\n        y_true_pred_level = None\n        y_train = None\n        if level in predictions.columns:\n            y_true_pred_level = pd.merge(series[level], predictions[level], left_index=True, right_index=True, how='inner').dropna(axis=0, how='any')\n            y_true_pred_level.columns = ['y_true', 'y_pred']\n            train_indexes = []\n            for i, fold in enumerate(folds):\n                fit_fold = fold[-1]\n                if i == 0 or fit_fold:\n                    train_iloc_start = fold[0][0]\n                    train_iloc_end = fold[0][1]\n                    train_indexes.append(np.arange(train_iloc_start, train_iloc_end))\n            train_indexes = np.unique(np.concatenate(train_indexes))\n            train_indexes = span_index[train_indexes]\n            y_train = series[level].loc[series[level].index.intersection(train_indexes)]\n        y_true_pred_levels.append(y_true_pred_level)\n        y_train_levels.append(y_train)\n    metrics_levels = []\n    for i, level in enumerate(levels):\n        if y_true_pred_levels[i] is not None and (not y_true_pred_levels[i].empty):\n            metrics_level = [m(y_true=y_true_pred_levels[i].iloc[:, 0], y_pred=y_true_pred_levels[i].iloc[:, 1], y_train=y_train_levels[i].iloc[window_size:]) for m in metrics]\n            metrics_levels.append(metrics_level)\n        else:\n            metrics_levels.append([None for _ in metrics])\n    metrics_levels = pd.DataFrame(data=metrics_levels, columns=[m if isinstance(m, str) else m.__name__ for m in metrics])\n    metrics_levels.insert(0, 'levels', levels)\n    if len(levels) < 2:\n        add_aggregated_metric = False\n    if add_aggregated_metric:\n        average = metrics_levels.drop(columns='levels').mean(skipna=True)\n        average = average.to_frame().transpose()\n        average['levels'] = 'average'\n        weighted_averages = {}\n        n_predictions_levels = predictions.notna().sum().to_frame(name='n_predictions').reset_index(names='levels')\n        metrics_levels_no_missing = metrics_levels.merge(n_predictions_levels, on='levels', how='inner')\n        for col in metric_names:\n            weighted_averages[col] = np.average(metrics_levels_no_missing[col], weights=metrics_levels_no_missing['n_predictions'])\n        weighted_average = pd.DataFrame(weighted_averages, index=[0])\n        weighted_average['levels'] = 'weighted_average'\n        y_true_pred_levels, y_train_levels = zip(*[(a, b.iloc[window_size:]) for a, b in zip(y_true_pred_levels, y_train_levels) if a is not None])\n        y_train_levels = list(y_train_levels)\n        y_true_pred_levels = pd.concat(y_true_pred_levels)\n        y_train_levels_concat = pd.concat(y_train_levels)\n        pooled = []\n        for m, m_name in zip(metrics, metric_names):\n            if m_name in ['mean_absolute_scaled_error', 'root_mean_squared_scaled_error']:\n                pooled.append(m(y_true=y_true_pred_levels.loc[:, 'y_true'], y_pred=y_true_pred_levels.loc[:, 'y_pred'], y_train=y_train_levels))\n            else:\n                pooled.append(m(y_true=y_true_pred_levels.loc[:, 'y_true'], y_pred=y_true_pred_levels.loc[:, 'y_pred'], y_train=y_train_levels_concat))\n        pooled = pd.DataFrame([pooled], columns=metric_names)\n        pooled['levels'] = 'pooling'\n        metrics_levels = pd.concat([metrics_levels, average, weighted_average, pooled], axis=0, ignore_index=True)\n    return metrics_levels\n\ndef _predict_and_calculate_metrics_one_step_ahead_multiseries(forecaster: object, series: Union[pd.DataFrame, dict], X_train: pd.DataFrame, y_train: Union[pd.Series, dict], X_test: pd.DataFrame, y_test: Union[pd.Series, dict], X_train_encoding: pd.Series, X_test_encoding: pd.Series, levels: list, metrics: list, add_aggregated_metric: bool=True) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"   \n    One-step-ahead predictions and metrics for each level and also for all levels\n    aggregated using average, weighted average or pooling.\n    Input matrices (X_train, y_train, X_train_encoding, X_test, y_test, X_test_encoding)\n    should have been generated using the forecaster._train_test_split_one_step_ahead().\n\n    - 'average': the average (arithmetic mean) of all levels.\n    - 'weighted_average': the average of the metrics weighted by the number of\n    predicted values of each level.\n    - 'pooling': the values of all levels are pooled and then the metric is\n    calculated.\n\n    Parameters\n    ----------\n    forecaster : object\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Series data used to train and test the forecaster.\n    X_train : pandas DataFrame\n        Training matrix.\n    y_train : pandas Series, dict\n        Target values of the training set.\n    X_test : pandas DataFrame\n        Test matrix.\n    y_test : pandas Series, dict\n        Target values of the test set.\n    X_train_encoding : pandas Series\n        Series identifiers for each row of `X_train`.\n    X_test_encoding : pandas Series\n        Series identifiers for each row of `X_test`.\n    levels : list\n        Levels to calculate the metrics.\n    metrics : list\n        List of metrics to calculate.\n    add_aggregated_metric : bool, default `True`\n        If `True`, and multiple series (`levels`) are predicted, the aggregated\n        metrics (average, weighted average and pooled) are also returned.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s).\n    predictions : pandas DataFrame\n        Value of predictions for each level.\n    \n    \"\"\"\n    if not isinstance(series, (pd.DataFrame, dict)):\n        raise TypeError('`series` must be a pandas DataFrame or a dictionary of pandas DataFrames.')\n    if not isinstance(X_train, pd.DataFrame):\n        raise TypeError(f'`X_train` must be a pandas DataFrame. Got: {type(X_train)}')\n    if not isinstance(y_train, (pd.Series, dict)):\n        raise TypeError(f'`y_train` must be a pandas Series or a dictionary of pandas Series. Got: {type(y_train)}')\n    if not isinstance(X_test, pd.DataFrame):\n        raise TypeError(f'`X_test` must be a pandas DataFrame. Got: {type(X_test)}')\n    if not isinstance(y_test, (pd.Series, dict)):\n        raise TypeError(f'`y_test` must be a pandas Series or a dictionary of pandas Series. Got: {type(y_test)}')\n    if not isinstance(X_train_encoding, pd.Series):\n        raise TypeError(f'`X_train_encoding` must be a pandas Series. Got: {type(X_train_encoding)}')\n    if not isinstance(X_test_encoding, pd.Series):\n        raise TypeError(f'`X_test_encoding` must be a pandas Series. Got: {type(X_test_encoding)}')\n    if not isinstance(levels, list):\n        raise TypeError(f'`levels` must be a list. Got: {type(levels)}')\n    if not isinstance(metrics, list):\n        raise TypeError(f'`metrics` must be a list. Got: {type(metrics)}')\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError(f'`add_aggregated_metric` must be a boolean. Got: {type(add_aggregated_metric)}')\n    metrics = [_get_metric(metric=m) if isinstance(m, str) else add_y_train_argument(m) for m in metrics]\n    metric_names = [m if isinstance(m, str) else m.__name__ for m in metrics]\n    if isinstance(series[levels[0]].index, pd.DatetimeIndex):\n        freq = series[levels[0]].index.freq\n    else:\n        freq = series[levels[0]].index.step\n    if type(forecaster).__name__ == 'ForecasterDirectMultiVariate':\n        step = 1\n        X_train, y_train = forecaster.filter_train_X_y_for_step(step=step, X_train=X_train, y_train=y_train)\n        X_test, y_test = forecaster.filter_train_X_y_for_step(step=step, X_train=X_test, y_train=y_test)\n        forecaster.regressors_[step].fit(X_train, y_train)\n        pred = forecaster.regressors_[step].predict(X_test)\n    else:\n        forecaster.regressor.fit(X_train, y_train)\n        pred = forecaster.regressor.predict(X_test)\n    predictions_per_level = pd.DataFrame({'y_true': y_test, 'y_pred': pred, '_level_skforecast': X_test_encoding}, index=y_test.index).groupby('_level_skforecast')\n    predictions_per_level = {key: group for key, group in predictions_per_level}\n    y_train_per_level = pd.DataFrame({'y_train': y_train, '_level_skforecast': X_train_encoding}, index=y_train.index).groupby('_level_skforecast')\n    y_train_per_level = {key: group.asfreq(freq) for key, group in y_train_per_level}\n    if forecaster.differentiation is not None:\n        for level in predictions_per_level:\n            predictions_per_level[level]['y_true'] = forecaster.differentiator_[level].inverse_transform_next_window(predictions_per_level[level]['y_true'].to_numpy())\n            predictions_per_level[level]['y_pred'] = forecaster.differentiator_[level].inverse_transform_next_window(predictions_per_level[level]['y_pred'].to_numpy())\n            y_train_per_level[level]['y_train'] = forecaster.differentiator_[level].inverse_transform_training(y_train_per_level[level]['y_train'].to_numpy())\n    if forecaster.transformer_series is not None:\n        for level in predictions_per_level:\n            transformer = forecaster.transformer_series_[level]\n            predictions_per_level[level]['y_true'] = transformer.inverse_transform(predictions_per_level[level][['y_true']])\n            predictions_per_level[level]['y_pred'] = transformer.inverse_transform(predictions_per_level[level][['y_pred']])\n            y_train_per_level[level]['y_train'] = transformer.inverse_transform(y_train_per_level[level][['y_train']])\n    metrics_levels = []\n    for level in levels:\n        if level in predictions_per_level:\n            metrics_level = [m(y_true=predictions_per_level[level].loc[:, 'y_true'], y_pred=predictions_per_level[level].loc[:, 'y_pred'], y_train=y_train_per_level[level].loc[:, 'y_train']) for m in metrics]\n            metrics_levels.append(metrics_level)\n        else:\n            metrics_levels.append([None for _ in metrics])\n    metrics_levels = pd.DataFrame(data=metrics_levels, columns=[m if isinstance(m, str) else m.__name__ for m in metrics])\n    metrics_levels.insert(0, 'levels', levels)\n    if len(levels) < 2:\n        add_aggregated_metric = False\n    if add_aggregated_metric:\n        average = metrics_levels.drop(columns='levels').mean(skipna=True)\n        average = average.to_frame().transpose()\n        average['levels'] = 'average'\n        weighted_averages = {}\n        n_predictions_levels = {k: v['y_pred'].notna().sum() for k, v in predictions_per_level.items()}\n        n_predictions_levels = pd.DataFrame(n_predictions_levels.items(), columns=['levels', 'n_predictions'])\n        metrics_levels_no_missing = metrics_levels.merge(n_predictions_levels, on='levels', how='inner')\n        for col in metric_names:\n            weighted_averages[col] = np.average(metrics_levels_no_missing[col], weights=metrics_levels_no_missing['n_predictions'])\n        weighted_average = pd.DataFrame(weighted_averages, index=[0])\n        weighted_average['levels'] = 'weighted_average'\n        list_y_train_by_level = [v['y_train'].to_numpy() for k, v in y_train_per_level.items() if k in predictions_per_level]\n        predictions_pooled = pd.concat(predictions_per_level.values())\n        y_train_pooled = pd.concat([v for k, v in y_train_per_level.items() if k in predictions_per_level])\n        pooled = []\n        for m, m_name in zip(metrics, metric_names):\n            if m_name in ['mean_absolute_scaled_error', 'root_mean_squared_scaled_error']:\n                pooled.append(m(y_true=predictions_pooled['y_true'], y_pred=predictions_pooled['y_pred'], y_train=list_y_train_by_level))\n            else:\n                pooled.append(m(y_true=predictions_pooled['y_true'], y_pred=predictions_pooled['y_pred'], y_train=y_train_pooled['y_train']))\n        pooled = pd.DataFrame([pooled], columns=metric_names)\n        pooled['levels'] = 'pooling'\n        metrics_levels = pd.concat([metrics_levels, average, weighted_average, pooled], axis=0, ignore_index=True)\n    predictions = pd.concat(predictions_per_level.values()).loc[:, ['y_pred', '_level_skforecast']].pivot(columns='_level_skforecast', values='y_pred').rename_axis(columns=None, index=None)\n    predictions = predictions.asfreq(X_test.index.freq)\n    return (metrics_levels, predictions)"
  },
  "call_tree": {
    "skforecast/direct/tests/tests_forecaster_direct_multivariate/test_train_test_split_one_step_ahead.py:test_train_test_split_one_step_ahead_when_y_is_series_and_exog_are_dataframe": {
      "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:__init__": {
        "skforecast/utils/utils.py:initialize_lags": {},
        "skforecast/utils/utils.py:initialize_window_features": {},
        "skforecast/utils/utils.py:initialize_weights": {},
        "skforecast/utils/utils.py:check_select_fit_kwargs": {},
        "skforecast/utils/utils.py:select_n_jobs_fit_forecaster": {}
      },
      "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:_train_test_split_one_step_ahead": {
        "skforecast/model_selection/_utils.py:_extract_data_folds_multiseries": {},
        "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:_create_train_X_y": {
          "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:_create_data_to_return_dict": {},
          "skforecast/utils/utils.py:initialize_transformer_series": {},
          "skforecast/utils/utils.py:check_exog": {},
          "skforecast/utils/utils.py:input_to_frame": {},
          "skforecast/utils/utils.py:get_exog_dtypes": {},
          "skforecast/utils/utils.py:transform_dataframe": {},
          "skforecast/utils/utils.py:check_exog_dtypes": {
            "skforecast/utils/utils.py:check_exog": {}
          },
          "skforecast/utils/utils.py:check_y": {},
          "skforecast/utils/utils.py:transform_series": {},
          "skforecast/utils/utils.py:preprocess_y": {},
          "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:_create_lags": {},
          "skforecast/utils/utils.py:exog_to_direct_numpy": {}
        }
      },
      "skforecast/model_selection/_utils.py:_extract_data_folds_multiseries": {}
    }
  }
}