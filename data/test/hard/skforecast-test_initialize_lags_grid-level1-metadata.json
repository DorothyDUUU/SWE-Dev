{
  "dir_path": "/app/skforecast",
  "package_name": "skforecast",
  "sample_name": "skforecast-test_initialize_lags_grid",
  "src_dir": "skforecast/",
  "test_dir": "tests/",
  "test_file": "skforecast/model_selection/tests/tests_utils/test_initialize_lags_grid.py",
  "test_code": "# Unit test initialize_lags_grid\n# ==============================================================================\nimport re\nimport pytest\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom skforecast.recursive import ForecasterRecursive\nfrom skforecast.model_selection._utils import initialize_lags_grid\n\n\ndef test_TypeError_initialize_lags__rid_when_not_list_dict_or_None():\n    \"\"\"\n    Test TypeError is raised when lags_grid is not a list, dict or None.\n    \"\"\"\n    forecaster = ForecasterRecursive(regressor=Ridge(random_state=123), lags=2)\n    lags_grid = 'not_valid_type'\n\n    err_msg = re.escape(\n        (f\"`lags_grid` argument must be a list, dict or None. \"\n         f\"Got {type(lags_grid)}.\")\n    )\n    with pytest.raises(TypeError, match = err_msg):\n        initialize_lags_grid(forecaster, lags_grid)\n\n\ndef test_initialize_lags_grid_when_lags_grid_is_a_list():\n    \"\"\"\n    Test initialize_lags_grid when lags_grid is a list.\n    \"\"\"\n    forecaster = ForecasterRecursive(regressor=Ridge(random_state=123), lags=2)\n    lags_grid = [1, [2, 4], range(3, 5), np.array([3, 7])]\n    lags_grid, lags_label = initialize_lags_grid(forecaster, lags_grid)\n\n    lags_grid_expected = {\n        '1': 1, \n        '[2, 4]': [2, 4], \n        'range(3, 5)': range(3, 5), \n        '[3 7]': np.array([3, 7])\n    }\n\n    assert lags_label == 'values'\n    assert lags_grid.keys() == lags_grid_expected.keys()\n    for v, v_expected in zip(lags_grid.values(), lags_grid_expected.values()):\n        if isinstance(v, np.ndarray):\n            assert np.array_equal(v, v_expected)\n        elif isinstance(v, range):\n            assert list(v) == list(v_expected)\n        else:\n            assert v == v_expected\n\n\n@pytest.mark.parametrize(\"lags, lags_grid_expected\",\n                         [(3, {'[1, 2, 3]': [1, 2, 3]}), \n                          ([1, 2, 3], {'[1, 2, 3]': [1, 2, 3]}),\n                          (range(1, 4), {'[1, 2, 3]': [1, 2, 3]}),\n                          (np.array([1, 2, 3]), {'[1, 2, 3]': [1, 2, 3]})],\n                         ids=lambda lags: f'lags, lags_grid_expected: {lags}')\ndef test_initialize_lags_grid_when_lags_grid_is_None(lags, lags_grid_expected):\n    \"\"\"\n    Test initialize_lags_grid when lags_grid is None.\n    \"\"\"\n    forecaster = ForecasterRecursive(regressor=Ridge(random_state=123), lags=lags)\n    lags_grid = None\n    lags_grid, lags_label = initialize_lags_grid(forecaster, lags_grid)\n\n    assert lags_label == 'values'\n    assert lags_grid.keys() == lags_grid_expected.keys()\n    for v, v_expected in zip(lags_grid.values(), lags_grid_expected.values()):\n        assert v == v_expected\n\n\ndef test_initialize_lags_grid_when_lags_grid_is_a_dict():\n    \"\"\"\n    Test initialize_lags_grid when lags_grid is a dict.\n    \"\"\"\n    forecaster = ForecasterRecursive(regressor=Ridge(random_state=123), lags=2)\n    lags_grid = {'1': 1, '[2, 4]': [2, 4], 'range(3, 5)': range(3, 5), '[3 7]': np.array([3, 7])}\n    lags_grid, lags_label = initialize_lags_grid(forecaster, lags_grid)\n\n    lags_grid_expected = {\n        '1': 1, \n        '[2, 4]': [2, 4], \n        'range(3, 5)': range(3, 5), \n        '[3 7]': np.array([3, 7])\n    }\n    \n    assert lags_label == 'keys'\n    assert lags_grid.keys() == lags_grid_expected.keys()\n    for v, v_expected in zip(lags_grid.values(), lags_grid_expected.values()):\n        if isinstance(v, np.ndarray):\n            assert np.array_equal(v, v_expected)\n        elif isinstance(v, range):\n            assert list(v) == list(v_expected)\n        else:\n            assert v == v_expected",
  "GT_file_code": {
    "skforecast/model_selection/_utils.py": "################################################################################\n#                     skforecast.model_selection._utils                        #\n#                                                                              #\n# This work by skforecast team is licensed under the BSD 3-Clause License.     #\n################################################################################\n# coding=utf-8\n\nfrom typing import Union, Tuple, Optional, Callable, Generator\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom joblib import cpu_count\nfrom tqdm.auto import tqdm\nfrom sklearn.pipeline import Pipeline\nimport sklearn.linear_model\nfrom sklearn.exceptions import NotFittedError\n\nfrom ..exceptions import IgnoredArgumentWarning\nfrom ..metrics import add_y_train_argument, _get_metric\nfrom ..utils import check_interval\n\n\ndef initialize_lags_grid(\n    forecaster: object, \n    lags_grid: Optional[Union[list, dict]] = None\n) -> Tuple[dict, str]:\n    \"\"\"\n    Initialize lags grid and lags label for model selection. \n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model. ForecasterRecursive, ForecasterDirect, \n        ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate.\n    lags_grid : list, dict, default `None`\n        Lists of lags to try, containing int, lists, numpy ndarray, or range \n        objects. If `dict`, the keys are used as labels in the `results` \n        DataFrame, and the values are used as the lists of lags to try.\n\n    Returns\n    -------\n    lags_grid : dict\n        Dictionary with lags configuration for each iteration.\n    lags_label : str\n        Label for lags representation in the results object.\n\n    \"\"\"\n\n    if not isinstance(lags_grid, (list, dict, type(None))):\n        raise TypeError(\n            (f\"`lags_grid` argument must be a list, dict or None. \"\n             f\"Got {type(lags_grid)}.\")\n        )\n\n    lags_label = 'values'\n    if isinstance(lags_grid, list):\n        lags_grid = {f'{lags}': lags for lags in lags_grid}\n    elif lags_grid is None:\n        lags = [int(lag) for lag in forecaster.lags]  # Required since numpy 2.0\n        lags_grid = {f'{lags}': lags}\n    else:\n        lags_label = 'keys'\n\n    return lags_grid, lags_label\n\n\ndef check_backtesting_input(\n    forecaster: object,\n    cv: object,\n    metric: Union[str, Callable, list],\n    add_aggregated_metric: bool = True,\n    y: Optional[pd.Series] = None,\n    series: Optional[Union[pd.DataFrame, dict]] = None,\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    interval: Optional[list] = None,\n    alpha: Optional[float] = None,\n    n_boot: int = 250,\n    random_state: int = 123,\n    use_in_sample_residuals: bool = True,\n    use_binned_residuals: bool = False,\n    n_jobs: Union[int, str] = 'auto',\n    show_progress: bool = True,\n    suppress_warnings: bool = False,\n    suppress_warnings_fit: bool = False\n) -> None:\n    \"\"\"\n    This is a helper function to check most inputs of backtesting functions in \n    modules `model_selection`.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model.\n    cv : TimeSeriesFold\n        TimeSeriesFold object with the information needed to split the data into folds.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n    add_aggregated_metric : bool, default `True`\n        If `True`, the aggregated metrics (average, weighted average and pooling)\n        over all levels are also returned (only multiseries).\n    y : pandas Series, default `None`\n        Training time series for uni-series forecasters.\n    series : pandas DataFrame, dict, default `None`\n        Training time series for multi-series forecasters.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive.\n    alpha : float, default `None`\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %. \n    n_boot : int, default `250`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of prediction \n        error to create prediction intervals.  If `False`, out_sample_residuals \n        are used if they are already stored inside the forecaster.\n    use_binned_residuals : bool, default `False`\n        If `True`, residuals used in each bootstrapping iteration are selected\n        conditioning on the predicted values. If `False`, residuals are selected\n        randomly without conditioning on the predicted values.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the fuction\n        skforecast.utils.select_n_jobs_fit_forecaster.\n        **New in version 0.9.0**\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n    suppress_warnings: bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the backtesting \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    suppress_warnings_fit : bool, default `False`\n        If `True`, warnings generated during fitting will be ignored. Only \n        `ForecasterSarimax`.\n\n    Returns\n    -------\n    None\n    \n    \"\"\"\n\n    forecaster_name = type(forecaster).__name__\n    cv_name = type(cv).__name__\n\n    if cv_name != \"TimeSeriesFold\":\n        raise TypeError(f\"`cv` must be a TimeSeriesFold object. Got {cv_name}.\")\n\n    steps = cv.steps\n    initial_train_size = cv.initial_train_size\n    gap = cv.gap\n    allow_incomplete_fold = cv.allow_incomplete_fold\n    refit = cv.refit\n\n    forecasters_uni = [\n        \"ForecasterRecursive\",\n        \"ForecasterDirect\",\n        \"ForecasterSarimax\",\n        \"ForecasterEquivalentDate\",\n    ]\n    forecasters_multi = [\n        \"ForecasterDirectMultiVariate\",\n        \"ForecasterRnn\",\n    ]\n    forecasters_multi_dict = [\n        \"ForecasterRecursiveMultiSeries\"\n    ]\n\n    if forecaster_name in forecasters_uni:\n        if not isinstance(y, pd.Series):\n            raise TypeError(\"`y` must be a pandas Series.\")\n        data_name = 'y'\n        data_length = len(y)\n\n    elif forecaster_name in forecasters_multi:\n        if not isinstance(series, pd.DataFrame):\n            raise TypeError(\"`series` must be a pandas DataFrame.\")\n        data_name = 'series'\n        data_length = len(series)\n    \n    elif forecaster_name in forecasters_multi_dict:\n        if not isinstance(series, (pd.DataFrame, dict)):\n            raise TypeError(\n                f\"`series` must be a pandas DataFrame or a dict of DataFrames or Series. \"\n                f\"Got {type(series)}.\"\n            )\n        \n        data_name = 'series'\n        if isinstance(series, dict):\n            not_valid_series = [\n                k \n                for k, v in series.items()\n                if not isinstance(v, (pd.Series, pd.DataFrame))\n            ]\n            if not_valid_series:\n                raise TypeError(\n                    f\"If `series` is a dictionary, all series must be a named \"\n                    f\"pandas Series or a pandas DataFrame with a single column. \"\n                    f\"Review series: {not_valid_series}\"\n                )\n            not_valid_index = [\n                k \n                for k, v in series.items()\n                if not isinstance(v.index, pd.DatetimeIndex)\n            ]\n            if not_valid_index:\n                raise ValueError(\n                    f\"If `series` is a dictionary, all series must have a Pandas \"\n                    f\"DatetimeIndex as index with the same frequency. \"\n                    f\"Review series: {not_valid_index}\"\n                )\n\n            indexes_freq = [f'{v.index.freq}' for v in series.values()]\n            indexes_freq = sorted(set(indexes_freq))\n            if not len(indexes_freq) == 1:\n                raise ValueError(\n                    f\"If `series` is a dictionary, all series must have a Pandas \"\n                    f\"DatetimeIndex as index with the same frequency. \"\n                    f\"Found frequencies: {indexes_freq}\"\n                )\n            data_length = max([len(series[serie]) for serie in series])\n        else:\n            data_length = len(series)\n\n    if exog is not None:\n        if forecaster_name in forecasters_multi_dict:\n            if not isinstance(exog, (pd.Series, pd.DataFrame, dict)):\n                raise TypeError(\n                    f\"`exog` must be a pandas Series, DataFrame, dictionary of pandas \"\n                    f\"Series/DataFrames or None. Got {type(exog)}.\"\n                )\n            if isinstance(exog, dict):\n                not_valid_exog = [\n                    k \n                    for k, v in exog.items()\n                    if not isinstance(v, (pd.Series, pd.DataFrame, type(None)))\n                ]\n                if not_valid_exog:\n                    raise TypeError(\n                        f\"If `exog` is a dictionary, All exog must be a named pandas \"\n                        f\"Series, a pandas DataFrame or None. Review exog: {not_valid_exog}\"\n                    )\n        else:\n            if not isinstance(exog, (pd.Series, pd.DataFrame)):\n                raise TypeError(\n                    f\"`exog` must be a pandas Series, DataFrame or None. Got {type(exog)}.\"\n                )\n\n    if hasattr(forecaster, 'differentiation'):\n        if forecaster.differentiation != cv.differentiation:\n            raise ValueError(\n                f\"The differentiation included in the forecaster \"\n                f\"({forecaster.differentiation}) differs from the differentiation \"\n                f\"included in the cv ({cv.differentiation}). Set the same value \"\n                f\"for both using the `differentiation` argument.\"\n            )\n\n    if not isinstance(metric, (str, Callable, list)):\n        raise TypeError(\n            f\"`metric` must be a string, a callable function, or a list containing \"\n            f\"multiple strings and/or callables. Got {type(metric)}.\"\n        )\n\n    if forecaster_name == \"ForecasterEquivalentDate\" and isinstance(\n        forecaster.offset, pd.tseries.offsets.DateOffset\n    ):\n        if initial_train_size is None:\n            raise ValueError(\n                f\"`initial_train_size` must be an integer greater than \"\n                f\"the `window_size` of the forecaster ({forecaster.window_size}) \"\n                f\"and smaller than the length of `{data_name}` ({data_length}).\"\n            )\n    elif initial_train_size is not None:\n        if initial_train_size < forecaster.window_size or initial_train_size >= data_length:\n            raise ValueError(\n                f\"If used, `initial_train_size` must be an integer greater than \"\n                f\"the `window_size` of the forecaster ({forecaster.window_size}) \"\n                f\"and smaller than the length of `{data_name}` ({data_length}).\"\n            )\n        if initial_train_size + gap >= data_length:\n            raise ValueError(\n                f\"The combination of initial_train_size {initial_train_size} and \"\n                f\"gap {gap} cannot be greater than the length of `{data_name}` \"\n                f\"({data_length}).\"\n            )\n    else:\n        if forecaster_name in ['ForecasterSarimax', 'ForecasterEquivalentDate']:\n            raise ValueError(\n                f\"`initial_train_size` must be an integer smaller than the \"\n                f\"length of `{data_name}` ({data_length}).\"\n            )\n        else:\n            if not forecaster.is_fitted:\n                raise NotFittedError(\n                    \"`forecaster` must be already trained if no `initial_train_size` \"\n                    \"is provided.\"\n                )\n            if refit:\n                raise ValueError(\n                    \"`refit` is only allowed when `initial_train_size` is not `None`.\"\n                )\n\n    if forecaster_name == 'ForecasterSarimax' and cv.skip_folds is not None:\n        raise ValueError(\n            \"`skip_folds` is not allowed for ForecasterSarimax. Set it to `None`.\"\n        )\n\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError(\"`add_aggregated_metric` must be a boolean: `True`, `False`.\")\n    if not isinstance(n_boot, (int, np.integer)) or n_boot < 0:\n        raise TypeError(f\"`n_boot` must be an integer greater than 0. Got {n_boot}.\")\n    if not isinstance(random_state, (int, np.integer)) or random_state < 0:\n        raise TypeError(f\"`random_state` must be an integer greater than 0. Got {random_state}.\")\n    if not isinstance(use_in_sample_residuals, bool):\n        raise TypeError(\"`use_in_sample_residuals` must be a boolean: `True`, `False`.\")\n    if not isinstance(use_binned_residuals, bool):\n        raise TypeError(\"`use_binned_residuals` must be a boolean: `True`, `False`.\")\n    if not isinstance(n_jobs, int) and n_jobs != 'auto':\n        raise TypeError(f\"`n_jobs` must be an integer or `'auto'`. Got {n_jobs}.\")\n    if not isinstance(show_progress, bool):\n        raise TypeError(\"`show_progress` must be a boolean: `True`, `False`.\")\n    if not isinstance(suppress_warnings, bool):\n        raise TypeError(\"`suppress_warnings` must be a boolean: `True`, `False`.\")\n    if not isinstance(suppress_warnings_fit, bool):\n        raise TypeError(\"`suppress_warnings_fit` must be a boolean: `True`, `False`.\")\n\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n\n    if not allow_incomplete_fold and data_length - (initial_train_size + gap) < steps:\n        raise ValueError(\n            f\"There is not enough data to evaluate {steps} steps in a single \"\n            f\"fold. Set `allow_incomplete_fold` to `True` to allow incomplete folds.\\n\"\n            f\"    Data available for test : {data_length - (initial_train_size + gap)}\\n\"\n            f\"    Steps                   : {steps}\"\n        )\n\n\ndef select_n_jobs_backtesting(\n    forecaster: object,\n    refit: Union[bool, int]\n) -> int:\n    \"\"\"\n    Select the optimal number of jobs to use in the backtesting process. This\n    selection is based on heuristics and is not guaranteed to be optimal.\n\n    The number of jobs is chosen as follows:\n\n    - If `refit` is an integer, then `n_jobs = 1`. This is because parallelization doesn't \n    work with intermittent refit.\n    - If forecaster is 'ForecasterRecursive' and regressor is a linear regressor, \n    then `n_jobs = 1`.\n    - If forecaster is 'ForecasterRecursive' and regressor is not a linear \n    regressor then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and `refit = True`, then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and `refit = False`, then `n_jobs = 1`.\n    - If forecaster is 'ForecasterRecursiveMultiSeries', then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterSarimax' or 'ForecasterEquivalentDate', \n    then `n_jobs = 1`.\n    - If regressor is a `LGBMRegressor(n_jobs=1)`, then `n_jobs = cpu_count() - 1`.\n    - If regressor is a `LGBMRegressor` with internal n_jobs != 1, then `n_jobs = 1`.\n    This is because `lightgbm` is highly optimized for gradient boosting and\n    parallelizes operations at a very fine-grained level, making additional\n    parallelization unnecessary and potentially harmful due to resource contention.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model.\n    refit : bool, int\n        If the forecaster is refitted during the backtesting process.\n\n    Returns\n    -------\n    n_jobs : int\n        The number of jobs to run in parallel.\n    \n    \"\"\"\n\n    forecaster_name = type(forecaster).__name__\n\n    if isinstance(forecaster.regressor, Pipeline):\n        regressor = forecaster.regressor[-1]\n        regressor_name = type(regressor).__name__\n    else:\n        regressor = forecaster.regressor\n        regressor_name = type(regressor).__name__\n\n    linear_regressors = [\n        regressor_name\n        for regressor_name in dir(sklearn.linear_model)\n        if not regressor_name.startswith('_')\n    ]\n\n    refit = False if refit == 0 else refit\n    if not isinstance(refit, bool) and refit != 1:\n        n_jobs = 1\n    else:\n        if forecaster_name in ['ForecasterRecursive']:\n            if regressor_name in linear_regressors:\n                n_jobs = 1\n            elif regressor_name == 'LGBMRegressor':\n                n_jobs = cpu_count() - 1 if regressor.n_jobs == 1 else 1\n            else:\n                n_jobs = cpu_count() - 1\n        elif forecaster_name in ['ForecasterDirect', 'ForecasterDirectMultiVariate']:\n            # Parallelization is applied during the fitting process.\n            n_jobs = 1\n        elif forecaster_name in ['ForecasterRecursiveMultiSeries']:\n            if regressor_name == 'LGBMRegressor':\n                n_jobs = cpu_count() - 1 if regressor.n_jobs == 1 else 1\n            else:\n                n_jobs = cpu_count() - 1\n        elif forecaster_name in ['ForecasterSarimax', 'ForecasterEquivalentDate']:\n            n_jobs = 1\n        else:\n            n_jobs = 1\n\n    return n_jobs\n\n\ndef _calculate_metrics_one_step_ahead(\n    forecaster: object,\n    y: pd.Series,\n    metrics: list,\n    X_train: pd.DataFrame,\n    y_train: Union[pd.Series, dict],\n    X_test: pd.DataFrame,\n    y_test: Union[pd.Series, dict]\n) -> list:\n    \"\"\"\n    Calculate metrics when predictions are one-step-ahead. When forecaster is\n    of type ForecasterDirect only the regressor for step 1 is used.\n\n    Parameters\n    ----------\n    forecaster : object\n        Forecaster model.\n    y : pandas Series\n        Time series data used to train and test the model.\n    metrics : list\n        List of metrics.\n    X_train : pandas DataFrame\n        Predictor values used to train the model.\n    y_train : pandas Series\n        Target values related to each row of `X_train`.\n    X_test : pandas DataFrame\n        Predictor values used to test the model.\n    y_test : pandas Series\n        Target values related to each row of `X_test`.\n\n    Returns\n    -------\n    metric_values : list\n        List with metric values.\n    \n    \"\"\"\n\n    if type(forecaster).__name__ == 'ForecasterDirect':\n\n        step = 1  # Only the model for step 1 is optimized.\n        X_train, y_train = forecaster.filter_train_X_y_for_step(\n                               step    = step,\n                               X_train = X_train,\n                               y_train = y_train\n                           )\n        X_test, y_test = forecaster.filter_train_X_y_for_step(\n                             step    = step,  \n                             X_train = X_test,\n                             y_train = y_test\n                         )\n        forecaster.regressors_[step].fit(X_train, y_train)\n        y_pred = forecaster.regressors_[step].predict(X_test)\n\n    else:\n        forecaster.regressor.fit(X_train, y_train)\n        y_pred = forecaster.regressor.predict(X_test)\n\n    y_true = y_test.to_numpy()\n    y_pred = y_pred.ravel()\n    y_train = y_train.to_numpy()\n\n    if forecaster.differentiation is not None:\n        y_true = forecaster.differentiator.inverse_transform_next_window(y_true)\n        y_pred = forecaster.differentiator.inverse_transform_next_window(y_pred)\n        y_train = forecaster.differentiator.inverse_transform_training(y_train)\n\n    if forecaster.transformer_y is not None:\n        y_true = forecaster.transformer_y.inverse_transform(y_true.reshape(-1, 1))\n        y_pred = forecaster.transformer_y.inverse_transform(y_pred.reshape(-1, 1))\n        y_train = forecaster.transformer_y.inverse_transform(y_train.reshape(-1, 1))\n\n    metric_values = []\n    for m in metrics:\n        metric_values.append(\n            m(y_true=y_true.ravel(), y_pred=y_pred.ravel(), y_train=y_train.ravel())\n        )\n\n    return metric_values\n\n\ndef _initialize_levels_model_selection_multiseries(\n    forecaster: object, \n    series: Union[pd.DataFrame, dict],\n    levels: Optional[Union[str, list]] = None\n) -> list:\n    \"\"\"\n    Initialize levels for model_selection multi-series functions.\n\n    Parameters\n    ----------\n    forecaster : ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate, ForecasterRnn\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Training time series.\n    levels : str, list, default `None`\n        level (`str`) or levels (`list`) at which the forecaster is optimized. \n        If `None`, all levels are taken into account. The resulting metric will be\n        the average of the optimization of all levels.\n\n    Returns\n    -------\n    levels : list\n        List of levels to be used in model_selection multi-series functions.\n    \n    \"\"\"\n\n    multi_series_forecasters_with_levels = [\n        'ForecasterRecursiveMultiSeries', \n        'ForecasterRnn'\n    ]\n\n    if type(forecaster).__name__ in multi_series_forecasters_with_levels  \\\n        and not isinstance(levels, (str, list, type(None))):\n        raise TypeError(\n            (f\"`levels` must be a `list` of column names, a `str` of a column \"\n             f\"name or `None` when using a forecaster of type \"\n             f\"{multi_series_forecasters_with_levels}. If the forecaster is of \"\n             f\"type `ForecasterDirectMultiVariate`, this argument is ignored.\")\n        )\n\n    if type(forecaster).__name__ == 'ForecasterDirectMultiVariate':\n        if levels and levels != forecaster.level and levels != [forecaster.level]:\n            warnings.warn(\n                (f\"`levels` argument have no use when the forecaster is of type \"\n                 f\"`ForecasterDirectMultiVariate`. The level of this forecaster \"\n                 f\"is '{forecaster.level}', to predict another level, change \"\n                 f\"the `level` argument when initializing the forecaster. \\n\"),\n                 IgnoredArgumentWarning\n            )\n        levels = [forecaster.level]\n    else:\n        if levels is None:\n            # Forecaster could be untrained, so self.series_col_names cannot be used.\n            if isinstance(series, pd.DataFrame):\n                levels = list(series.columns)\n            else:\n                levels = list(series.keys())\n        elif isinstance(levels, str):\n            levels = [levels]\n\n    return levels\n\n\ndef _extract_data_folds_multiseries(\n    series: Union[pd.Series, pd.DataFrame, dict],\n    folds: list,\n    span_index: Union[pd.DatetimeIndex, pd.RangeIndex],\n    window_size: int,\n    exog: Optional[Union[pd.Series, pd.DataFrame, dict]] = None,\n    dropna_last_window: bool = False,\n    externally_fitted: bool = False\n) -> Generator[\n        Tuple[\n            Union[pd.Series, pd.DataFrame, dict],\n            pd.DataFrame,\n            list,\n            Optional[Union[pd.Series, pd.DataFrame, dict]],\n            Optional[Union[pd.Series, pd.DataFrame, dict]],\n            list\n        ],\n        None,\n        None\n    ]:\n    \"\"\"\n    Select the data from series and exog that corresponds to each fold created using the\n    skforecast.model_selection._create_backtesting_folds function.\n\n    Parameters\n    ----------\n    series : pandas Series, pandas DataFrame, dict\n        Time series.\n    folds : list\n        Folds created using the skforecast.model_selection._create_backtesting_folds\n        function.\n    span_index : pandas DatetimeIndex, pandas RangeIndex\n        Full index from the minimum to the maximum index among all series.\n    window_size : int\n        Size of the window needed to create the predictors.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    dropna_last_window : bool, default `False`\n        If `True`, drop the columns of the last window that have NaN values.\n    externally_fitted : bool, default `False`\n        Flag indicating whether the forecaster is already trained. Only used when \n        `initial_train_size` is None and `refit` is False.\n\n    Yield\n    -----\n    series_train : pandas Series, pandas DataFrame, dict\n        Time series corresponding to the training set of the fold.\n    series_last_window: pandas DataFrame\n        Time series corresponding to the last window of the fold.\n    levels_last_window: list\n        Levels of the time series present in the last window of the fold.\n    exog_train: pandas Series, pandas DataFrame, dict, None\n        Exogenous variable corresponding to the training set of the fold.\n    exog_test: pandas Series, pandas DataFrame, dict, None\n        Exogenous variable corresponding to the test set of the fold.\n    fold: list\n        Fold created using the skforecast.model_selection._create_backtesting_folds\n\n    \"\"\"\n\n    for fold in folds:\n        train_iloc_start       = fold[0][0]\n        train_iloc_end         = fold[0][1]\n        last_window_iloc_start = fold[1][0]\n        last_window_iloc_end   = fold[1][1]\n        test_iloc_start        = fold[2][0]\n        test_iloc_end          = fold[2][1]\n\n        if isinstance(series, dict) or isinstance(exog, dict):\n            # Substract 1 to the iloc indexes to get the loc indexes\n            train_loc_start       = span_index[train_iloc_start]\n            train_loc_end         = span_index[train_iloc_end - 1]\n            last_window_loc_start = span_index[last_window_iloc_start]\n            last_window_loc_end   = span_index[last_window_iloc_end - 1]\n            test_loc_start        = span_index[test_iloc_start]\n            test_loc_end          = span_index[test_iloc_end - 1]\n\n        if isinstance(series, pd.DataFrame):\n            series_train = series.iloc[train_iloc_start:train_iloc_end, ]\n\n            series_to_drop = []\n            for col in series_train.columns:\n                if series_train[col].isna().all():\n                    series_to_drop.append(col)\n                else:\n                    first_valid_index = series_train[col].first_valid_index()\n                    last_valid_index = series_train[col].last_valid_index()\n                    if (\n                        len(series_train[col].loc[first_valid_index:last_valid_index])\n                        < window_size\n                    ):\n                        series_to_drop.append(col)\n\n            series_last_window = series.iloc[\n                last_window_iloc_start:last_window_iloc_end,\n            ]\n            \n            series_train = series_train.drop(columns=series_to_drop)\n            if not externally_fitted:\n                series_last_window = series_last_window.drop(columns=series_to_drop)\n        else:\n            series_train = {}\n            for k in series.keys():\n                v = series[k].loc[train_loc_start:train_loc_end]\n                if not v.isna().all():\n                    first_valid_index = v.first_valid_index()\n                    last_valid_index  = v.last_valid_index()\n                    if first_valid_index is not None and last_valid_index is not None:\n                        v = v.loc[first_valid_index : last_valid_index]\n                        if len(v) >= window_size:\n                            series_train[k] = v\n\n            series_last_window = {}\n            for k, v in series.items():\n                v = series[k].loc[last_window_loc_start:last_window_loc_end]\n                if ((externally_fitted or k in series_train) and len(v) >= window_size):\n                    series_last_window[k] = v\n\n            series_last_window = pd.DataFrame(series_last_window)\n\n        if dropna_last_window:\n            series_last_window = series_last_window.dropna(axis=1, how=\"any\")\n            # TODO: add the option to drop the series without minimum non NaN values.\n            # Similar to how pandas does in the rolling window function.\n        \n        levels_last_window = list(series_last_window.columns)\n\n        if exog is not None:\n            if isinstance(exog, (pd.Series, pd.DataFrame)):\n                exog_train = exog.iloc[train_iloc_start:train_iloc_end, ]\n                exog_test = exog.iloc[test_iloc_start:test_iloc_end, ]\n            else:\n                exog_train = {\n                    k: v.loc[train_loc_start:train_loc_end] \n                    for k, v in exog.items()\n                }\n                exog_train = {k: v for k, v in exog_train.items() if len(v) > 0}\n\n                exog_test = {\n                    k: v.loc[test_loc_start:test_loc_end]\n                    for k, v in exog.items()\n                    if externally_fitted or k in exog_train\n                }\n\n                exog_test = {k: v for k, v in exog_test.items() if len(v) > 0}\n        else:\n            exog_train = None\n            exog_test = None\n\n        yield series_train, series_last_window, levels_last_window, exog_train, exog_test, fold\n\n\ndef _calculate_metrics_backtesting_multiseries(\n    series: Union[pd.DataFrame, dict],\n    predictions: pd.DataFrame,\n    folds: Union[list, tqdm],\n    span_index: Union[pd.DatetimeIndex, pd.RangeIndex],\n    window_size: int,\n    metrics: list,\n    levels: list,\n    add_aggregated_metric: bool = True\n) -> pd.DataFrame:\n    \"\"\"   \n    Calculate metrics for each level and also for all levels aggregated using\n    average, weighted average or pooling.\n\n    - 'average': the average (arithmetic mean) of all levels.\n    - 'weighted_average': the average of the metrics weighted by the number of\n    predicted values of each level.\n    - 'pooling': the values of all levels are pooled and then the metric is\n    calculated.\n\n    Parameters\n    ----------\n    series : pandas DataFrame, dict\n        Series data used for backtesting.\n    predictions : pandas DataFrame\n        Predictions generated during the backtesting process.\n    folds : list, tqdm\n        Folds created during the backtesting process.\n    span_index : pandas DatetimeIndex, pandas RangeIndex\n        Full index from the minimum to the maximum index among all series.\n    window_size : int\n        Size of the window used by the forecaster to create the predictors.\n        This is used remove the first `window_size` (differentiation included) \n        values from y_train since they are not part of the training matrix.\n    metrics : list\n        List of metrics to calculate.\n    levels : list\n        Levels to calculate the metrics.\n    add_aggregated_metric : bool, default `True`\n        If `True`, and multiple series (`levels`) are predicted, the aggregated\n        metrics (average, weighted average and pooled) are also returned.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s).\n    \n    \"\"\"\n\n    if not isinstance(series, (pd.DataFrame, dict)):\n        raise TypeError(\n            (\"`series` must be a pandas DataFrame or a dictionary of pandas \"\n             \"DataFrames.\")\n        )\n    if not isinstance(predictions, pd.DataFrame):\n        raise TypeError(\"`predictions` must be a pandas DataFrame.\")\n    if not isinstance(folds, (list, tqdm)):\n        raise TypeError(\"`folds` must be a list or a tqdm object.\")\n    if not isinstance(span_index, (pd.DatetimeIndex, pd.RangeIndex)):\n        raise TypeError(\"`span_index` must be a pandas DatetimeIndex or pandas RangeIndex.\")\n    if not isinstance(window_size, (int, np.integer)):\n        raise TypeError(\"`window_size` must be an integer.\")\n    if not isinstance(metrics, list):\n        raise TypeError(\"`metrics` must be a list.\")\n    if not isinstance(levels, list):\n        raise TypeError(\"`levels` must be a list.\")\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError(\"`add_aggregated_metric` must be a boolean.\")\n    \n    metric_names = [(m if isinstance(m, str) else m.__name__) for m in metrics]\n\n    y_true_pred_levels = []\n    y_train_levels = []\n    for level in levels:\n        y_true_pred_level = None\n        y_train = None\n        if level in predictions.columns:\n            # TODO: avoid merges inside the loop, instead merge outside and then filter\n            y_true_pred_level = pd.merge(\n                series[level],\n                predictions[level],\n                left_index  = True,\n                right_index = True,\n                how         = \"inner\",\n            ).dropna(axis=0, how=\"any\")\n            y_true_pred_level.columns = ['y_true', 'y_pred']\n\n            train_indexes = []\n            for i, fold in enumerate(folds):\n                fit_fold = fold[-1]\n                if i == 0 or fit_fold:\n                    train_iloc_start = fold[0][0]\n                    train_iloc_end = fold[0][1]\n                    train_indexes.append(np.arange(train_iloc_start, train_iloc_end))\n            train_indexes = np.unique(np.concatenate(train_indexes))\n            train_indexes = span_index[train_indexes]\n            y_train = series[level].loc[series[level].index.intersection(train_indexes)]\n\n        y_true_pred_levels.append(y_true_pred_level)\n        y_train_levels.append(y_train)\n            \n    metrics_levels = []\n    for i, level in enumerate(levels):\n        if y_true_pred_levels[i] is not None and not y_true_pred_levels[i].empty:\n            metrics_level = [\n                m(\n                    y_true = y_true_pred_levels[i].iloc[:, 0],\n                    y_pred = y_true_pred_levels[i].iloc[:, 1],\n                    y_train = y_train_levels[i].iloc[window_size:]  # Exclude observations used to create predictors\n                )\n                for m in metrics\n            ]\n            metrics_levels.append(metrics_level)\n        else:\n            metrics_levels.append([None for _ in metrics])\n\n    metrics_levels = pd.DataFrame(\n                         data    = metrics_levels,\n                         columns = [m if isinstance(m, str) else m.__name__\n                                    for m in metrics]\n                     )\n    metrics_levels.insert(0, 'levels', levels)\n\n    if len(levels) < 2:\n        add_aggregated_metric = False\n    \n    if add_aggregated_metric:\n\n        # aggragation: average\n        average = metrics_levels.drop(columns='levels').mean(skipna=True)\n        average = average.to_frame().transpose()\n        average['levels'] = 'average'\n\n        # aggregation: weighted_average\n        weighted_averages = {}\n        n_predictions_levels = (\n            predictions\n            .notna()\n            .sum()\n            .to_frame(name='n_predictions')\n            .reset_index(names='levels')\n        )\n        metrics_levels_no_missing = (\n            metrics_levels.merge(n_predictions_levels, on='levels', how='inner')\n        )\n        for col in metric_names:\n            weighted_averages[col] = np.average(\n                metrics_levels_no_missing[col],\n                weights=metrics_levels_no_missing['n_predictions']\n            )\n        weighted_average = pd.DataFrame(weighted_averages, index=[0])\n        weighted_average['levels'] = 'weighted_average'\n\n        # aggregation: pooling\n        y_true_pred_levels, y_train_levels = zip(\n            *[\n                (a, b.iloc[window_size:])  # Exclude observations used to create predictors\n                for a, b in zip(y_true_pred_levels, y_train_levels)\n                if a is not None\n            ]\n        )\n        y_train_levels = list(y_train_levels)\n        y_true_pred_levels = pd.concat(y_true_pred_levels)\n        y_train_levels_concat = pd.concat(y_train_levels)\n\n        pooled = []\n        for m, m_name in zip(metrics, metric_names):\n            if m_name in ['mean_absolute_scaled_error', 'root_mean_squared_scaled_error']:\n                pooled.append(\n                    m(\n                        y_true = y_true_pred_levels.loc[:, 'y_true'],\n                        y_pred = y_true_pred_levels.loc[:, 'y_pred'],\n                        y_train = y_train_levels\n                    )\n                )\n            else:\n                pooled.append(\n                    m(\n                        y_true = y_true_pred_levels.loc[:, 'y_true'],\n                        y_pred = y_true_pred_levels.loc[:, 'y_pred'],\n                        y_train = y_train_levels_concat\n                    )\n                )\n        pooled = pd.DataFrame([pooled], columns=metric_names)\n        pooled['levels'] = 'pooling'\n\n        metrics_levels = pd.concat(\n            [metrics_levels, average, weighted_average, pooled],\n            axis=0,\n            ignore_index=True\n        )\n\n    return metrics_levels\n\n\ndef _predict_and_calculate_metrics_one_step_ahead_multiseries(\n    forecaster: object,\n    series: Union[pd.DataFrame, dict],\n    X_train: pd.DataFrame,\n    y_train: Union[pd.Series, dict],\n    X_test: pd.DataFrame,\n    y_test: Union[pd.Series, dict],\n    X_train_encoding: pd.Series,\n    X_test_encoding: pd.Series,\n    levels: list,\n    metrics: list,\n    add_aggregated_metric: bool = True\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"   \n    One-step-ahead predictions and metrics for each level and also for all levels\n    aggregated using average, weighted average or pooling.\n    Input matrices (X_train, y_train, X_train_encoding, X_test, y_test, X_test_encoding)\n    should have been generated using the forecaster._train_test_split_one_step_ahead().\n\n    - 'average': the average (arithmetic mean) of all levels.\n    - 'weighted_average': the average of the metrics weighted by the number of\n    predicted values of each level.\n    - 'pooling': the values of all levels are pooled and then the metric is\n    calculated.\n\n    Parameters\n    ----------\n    forecaster : object\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Series data used to train and test the forecaster.\n    X_train : pandas DataFrame\n        Training matrix.\n    y_train : pandas Series, dict\n        Target values of the training set.\n    X_test : pandas DataFrame\n        Test matrix.\n    y_test : pandas Series, dict\n        Target values of the test set.\n    X_train_encoding : pandas Series\n        Series identifiers for each row of `X_train`.\n    X_test_encoding : pandas Series\n        Series identifiers for each row of `X_test`.\n    levels : list\n        Levels to calculate the metrics.\n    metrics : list\n        List of metrics to calculate.\n    add_aggregated_metric : bool, default `True`\n        If `True`, and multiple series (`levels`) are predicted, the aggregated\n        metrics (average, weighted average and pooled) are also returned.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s).\n    predictions : pandas DataFrame\n        Value of predictions for each level.\n    \n    \"\"\"\n\n    if not isinstance(series, (pd.DataFrame, dict)):\n        raise TypeError(\n            \"`series` must be a pandas DataFrame or a dictionary of pandas \"\n            \"DataFrames.\"\n        )\n    if not isinstance(X_train, pd.DataFrame):\n        raise TypeError(f\"`X_train` must be a pandas DataFrame. Got: {type(X_train)}\")\n    if not isinstance(y_train, (pd.Series, dict)):\n        raise TypeError(\n            f\"`y_train` must be a pandas Series or a dictionary of pandas Series. \"\n            f\"Got: {type(y_train)}\"\n        )        \n    if not isinstance(X_test, pd.DataFrame):\n        raise TypeError(f\"`X_test` must be a pandas DataFrame. Got: {type(X_test)}\")\n    if not isinstance(y_test, (pd.Series, dict)):\n        raise TypeError(\n            f\"`y_test` must be a pandas Series or a dictionary of pandas Series. \"\n            f\"Got: {type(y_test)}\"\n        )\n    if not isinstance(X_train_encoding, pd.Series):\n        raise TypeError(\n            f\"`X_train_encoding` must be a pandas Series. Got: {type(X_train_encoding)}\"\n        )\n    if not isinstance(X_test_encoding, pd.Series):\n        raise TypeError(\n            f\"`X_test_encoding` must be a pandas Series. Got: {type(X_test_encoding)}\"\n        )\n    if not isinstance(levels, list):\n        raise TypeError(f\"`levels` must be a list. Got: {type(levels)}\")\n    if not isinstance(metrics, list):\n        raise TypeError(f\"`metrics` must be a list. Got: {type(metrics)}\")\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError(\n            f\"`add_aggregated_metric` must be a boolean. Got: {type(add_aggregated_metric)}\"\n        )\n    \n    metrics = [\n        _get_metric(metric=m)\n        if isinstance(m, str)\n        else add_y_train_argument(m) \n        for m in metrics\n    ]\n    metric_names = [(m if isinstance(m, str) else m.__name__) for m in metrics]\n\n    if isinstance(series[levels[0]].index, pd.DatetimeIndex):\n        freq = series[levels[0]].index.freq\n    else:\n        freq = series[levels[0]].index.step\n\n    if type(forecaster).__name__ == 'ForecasterDirectMultiVariate':\n        step = 1\n        X_train, y_train = forecaster.filter_train_X_y_for_step(\n                               step    = step,\n                               X_train = X_train,\n                               y_train = y_train\n                           )\n        X_test, y_test = forecaster.filter_train_X_y_for_step(\n                             step    = step,  \n                             X_train = X_test,\n                             y_train = y_test\n                         )                 \n        forecaster.regressors_[step].fit(X_train, y_train)\n        pred = forecaster.regressors_[step].predict(X_test)\n    else:\n        forecaster.regressor.fit(X_train, y_train)\n        pred = forecaster.regressor.predict(X_test)\n\n    predictions_per_level = pd.DataFrame(\n        {\n            'y_true': y_test,\n            'y_pred': pred,\n            '_level_skforecast': X_test_encoding,\n        },\n        index=y_test.index,\n    ).groupby('_level_skforecast')\n    predictions_per_level = {key: group for key, group in predictions_per_level}\n\n    y_train_per_level = pd.DataFrame(\n        {\"y_train\": y_train, \"_level_skforecast\": X_train_encoding},\n        index=y_train.index,\n    ).groupby(\"_level_skforecast\")\n    # Interleaved Nan values were excluded fom y_train. They are reestored\n    y_train_per_level = {key: group.asfreq(freq) for key, group in y_train_per_level}\n\n    if forecaster.differentiation is not None:\n        for level in predictions_per_level:\n            predictions_per_level[level][\"y_true\"] = (\n                forecaster.differentiator_[level].inverse_transform_next_window(\n                    predictions_per_level[level][\"y_true\"].to_numpy()\n                )\n            )\n            predictions_per_level[level][\"y_pred\"] = (\n                forecaster.differentiator_[level].inverse_transform_next_window(\n                    predictions_per_level[level][\"y_pred\"].to_numpy()\n                )   \n            )\n            y_train_per_level[level][\"y_train\"] = (\n                forecaster.differentiator_[level].inverse_transform_training(\n                    y_train_per_level[level][\"y_train\"].to_numpy()\n                )\n            )\n\n    if forecaster.transformer_series is not None:\n        for level in predictions_per_level:\n            transformer = forecaster.transformer_series_[level]\n            predictions_per_level[level][\"y_true\"] = transformer.inverse_transform(\n                predictions_per_level[level][[\"y_true\"]]\n            )\n            predictions_per_level[level][\"y_pred\"] = transformer.inverse_transform(\n                predictions_per_level[level][[\"y_pred\"]]\n            )\n            y_train_per_level[level][\"y_train\"] = transformer.inverse_transform(\n                y_train_per_level[level][[\"y_train\"]]\n            )\n    \n    metrics_levels = []\n    for level in levels:\n        if level in predictions_per_level:\n            metrics_level = [\n                m(\n                    y_true  = predictions_per_level[level].loc[:, 'y_true'],\n                    y_pred  = predictions_per_level[level].loc[:, 'y_pred'],\n                    y_train = y_train_per_level[level].loc[:, 'y_train']\n                )\n                for m in metrics\n            ]\n            metrics_levels.append(metrics_level)\n        else:\n            metrics_levels.append([None for _ in metrics])\n\n    metrics_levels = pd.DataFrame(\n                         data    = metrics_levels,\n                         columns = [m if isinstance(m, str) else m.__name__\n                                    for m in metrics]\n                     )\n    metrics_levels.insert(0, 'levels', levels)\n\n    if len(levels) < 2:\n        add_aggregated_metric = False\n\n    if add_aggregated_metric:\n\n        # aggragation: average\n        average = metrics_levels.drop(columns='levels').mean(skipna=True)\n        average = average.to_frame().transpose()\n        average['levels'] = 'average'\n\n        # aggregation: weighted_average\n        weighted_averages = {}\n        n_predictions_levels = {\n            k: v['y_pred'].notna().sum()\n            for k, v in predictions_per_level.items()\n        }\n        n_predictions_levels = pd.DataFrame(\n            n_predictions_levels.items(),\n            columns=['levels', 'n_predictions']\n        )\n        metrics_levels_no_missing = (\n            metrics_levels.merge(n_predictions_levels, on='levels', how='inner')\n        )\n        for col in metric_names:\n            weighted_averages[col] = np.average(\n                metrics_levels_no_missing[col],\n                weights=metrics_levels_no_missing['n_predictions']\n            )\n        weighted_average = pd.DataFrame(weighted_averages, index=[0])\n        weighted_average['levels'] = 'weighted_average'\n\n        # aggregation: pooling\n        list_y_train_by_level = [\n            v['y_train'].to_numpy()\n            for k, v in y_train_per_level.items()\n            if k in predictions_per_level\n        ]\n        predictions_pooled = pd.concat(predictions_per_level.values())\n        y_train_pooled = pd.concat(\n            [v for k, v in y_train_per_level.items() if k in predictions_per_level]\n        )\n        pooled = []\n        for m, m_name in zip(metrics, metric_names):\n            if m_name in ['mean_absolute_scaled_error', 'root_mean_squared_scaled_error']:\n                pooled.append(\n                    m(\n                        y_true  = predictions_pooled['y_true'],\n                        y_pred  = predictions_pooled['y_pred'],\n                        y_train = list_y_train_by_level\n                    )\n                )\n            else:\n                pooled.append(\n                    m(\n                        y_true  = predictions_pooled['y_true'],\n                        y_pred  = predictions_pooled['y_pred'],\n                        y_train = y_train_pooled['y_train']\n                    )\n                )\n        pooled = pd.DataFrame([pooled], columns=metric_names)\n        pooled['levels'] = 'pooling'\n\n        metrics_levels = pd.concat(\n            [metrics_levels, average, weighted_average, pooled],\n            axis=0,\n            ignore_index=True\n        )\n\n    predictions = (\n        pd.concat(predictions_per_level.values())\n        .loc[:, [\"y_pred\", \"_level_skforecast\"]]\n        .pivot(columns=\"_level_skforecast\", values=\"y_pred\")\n        .rename_axis(columns=None, index=None)\n    )\n    predictions = predictions.asfreq(X_test.index.freq)\n\n    return metrics_levels, predictions\n",
    "skforecast/recursive/_forecaster_recursive.py": "################################################################################\n#                           ForecasterRecursive                                #\n#                                                                              #\n# This work by skforecast team is licensed under the BSD 3-Clause License.     #\n################################################################################\n# coding=utf-8\n\nfrom typing import Union, Tuple, Optional, Callable\nimport warnings\nimport sys\nimport numpy as np\nimport pandas as pd\nimport inspect\nfrom copy import copy\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import clone\n\nimport skforecast\nfrom ..base import ForecasterBase\nfrom ..exceptions import DataTransformationWarning\nfrom ..utils import (\n    initialize_lags,\n    initialize_window_features,\n    initialize_weights,\n    check_select_fit_kwargs,\n    check_y,\n    check_exog,\n    get_exog_dtypes,\n    check_exog_dtypes,\n    check_predict_input,\n    check_interval,\n    preprocess_y,\n    preprocess_last_window,\n    preprocess_exog,\n    input_to_frame,\n    date_to_index_position,\n    expand_index,\n    transform_numpy,\n    transform_dataframe,\n)\nfrom ..preprocessing import TimeSeriesDifferentiator\nfrom ..preprocessing import QuantileBinner\n\n\nclass ForecasterRecursive(ForecasterBase):\n    \"\"\"\n    This class turns any regressor compatible with the scikit-learn API into a\n    recursive autoregressive (multi-step) forecaster.\n    \n    Parameters\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n    lags : int, list, numpy ndarray, range, default `None`\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n    \n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n        - `None`: no lags are included as predictors. \n    window_features : object, list, default `None`\n        Instance or list of instances used to create window features. Window features\n        are created from the original time series and are included as predictors.\n    transformer_y : object transformer (preprocessor), default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and inverse_transform.\n        ColumnTransformers are not allowed since they do not have inverse_transform method.\n        The transformation is applied to `y` before training the forecaster. \n    transformer_exog : object transformer (preprocessor), default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable, default `None`\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\n        method. The resulting `sample_weight` cannot have negative values.\n    differentiation : int, default `None`\n        Order of differencing applied to the time series before training the forecaster.\n        If `None`, no differencing is applied. The order of differentiation is the number\n        of times the differencing operation is applied to a time series. Differencing\n        involves computing the differences between consecutive data points in the series.\n        Differentiation is reversed in the output of `predict()` and `predict_interval()`.\n        **WARNING: This argument is newly introduced and requires special attention. It\n        is still experimental and may undergo changes.**\n    fit_kwargs : dict, default `None`\n        Additional arguments to be passed to the `fit` method of the regressor.\n    binner_kwargs : dict, default `None`\n        Additional arguments to pass to the `QuantileBinner` used to discretize \n        the residuals into k bins according to the predicted values associated \n        with each residual. Available arguments are: `n_bins`, `method`, `subsample`,\n        `random_state` and `dtype`. Argument `method` is passed internally to the\n        fucntion `numpy.percentile`.\n        **New in version 0.14.0**\n    forecaster_id : str, int, default `None`\n        Name used as an identifier of the forecaster.\n    \n    Attributes\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n    lags : numpy ndarray\n        Lags used as predictors.\n    lags_names : list\n        Names of the lags used as predictors.\n    max_lag : int\n        Maximum lag included in `lags`.\n    window_features : list\n        Class or list of classes used to create window features.\n    window_features_names : list\n        Names of the window features to be included in the `X_train` matrix.\n    window_features_class_names : list\n        Names of the classes used to create the window features.\n    max_size_window_features : int\n        Maximum window size required by the window features.\n    window_size : int\n        The window size needed to create the predictors. It is calculated as the \n        maximum value between `max_lag` and `max_size_window_features`. If \n        differentiation is used, `window_size` is increased by n units equal to \n        the order of differentiation so that predictors can be generated correctly.\n    transformer_y : object transformer (preprocessor)\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and inverse_transform.\n        ColumnTransformers are not allowed since they do not have inverse_transform method.\n        The transformation is applied to `y` before training the forecaster.\n    transformer_exog : object transformer (preprocessor)\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\n        method. The resulting `sample_weight` cannot have negative values.\n    differentiation : int\n        Order of differencing applied to the time series before training the forecaster.\n        If `None`, no differencing is applied. The order of differentiation is the number\n        of times the differencing operation is applied to a time series. Differencing\n        involves computing the differences between consecutive data points in the series.\n        Differentiation is reversed in the output of `predict()` and `predict_interval()`.\n        **WARNING: This argument is newly introduced and requires special attention. It\n        is still experimental and may undergo changes.**\n        **New in version 0.10.0**\n    binner : sklearn.preprocessing.KBinsDiscretizer\n        `KBinsDiscretizer` used to discretize residuals into k bins according \n        to the predicted values associated with each residual.\n        **New in version 0.12.0**\n    binner_intervals_ : dict\n        Intervals used to discretize residuals into k bins according to the predicted\n        values associated with each residual.\n        **New in version 0.12.0**\n    binner_kwargs : dict\n        Additional arguments to pass to the `QuantileBinner` used to discretize \n        the residuals into k bins according to the predicted values associated \n        with each residual. Available arguments are: `n_bins`, `method`, `subsample`,\n        `random_state` and `dtype`. Argument `method` is passed internally to the\n        fucntion `numpy.percentile`.\n        **New in version 0.14.0**\n    source_code_weight_func : str\n        Source code of the custom function used to create weights.\n    differentiation : int\n        Order of differencing applied to the time series before training the \n        forecaster.\n    differentiator : TimeSeriesDifferentiator\n        Skforecast object used to differentiate the time series.\n    last_window_ : pandas DataFrame\n        This window represents the most recent data observed by the predictor\n        during its training phase. It contains the values needed to predict the\n        next step immediately after the training data. These values are stored\n        in the original scale of the time series before undergoing any transformations\n        or differentiation. When `differentiation` parameter is specified, the\n        dimensions of the `last_window_` are expanded as many values as the order\n        of differentiation. For example, if `lags` = 7 and `differentiation` = 1,\n        `last_window_` will have 8 values.\n    index_type_ : type\n        Type of index of the input used in training.\n    index_freq_ : str\n        Frequency of Index of the input used in training.\n    training_range_ : pandas Index\n        First and last values of index of the data used during training.\n    exog_in_ : bool\n        If the forecaster has been trained using exogenous variable/s.\n    exog_names_in_ : list\n        Names of the exogenous variables used during training.\n    exog_type_in_ : type\n        Type of exogenous data (pandas Series or DataFrame) used in training.\n    exog_dtypes_in_ : dict\n        Type of each exogenous variable/s used in training. If `transformer_exog` \n        is used, the dtypes are calculated before the transformation.\n    X_train_window_features_names_out_ : list\n        Names of the window features included in the matrix `X_train` created\n        internally for training.\n    X_train_exog_names_out_ : list\n        Names of the exogenous variables included in the matrix `X_train` created\n        internally for training. It can be different from `exog_names_in_` if\n        some exogenous variables are transformed during the training process.\n    X_train_features_names_out_ : list\n        Names of columns of the matrix created internally for training.\n    fit_kwargs : dict\n        Additional arguments to be passed to the `fit` method of the regressor.\n    in_sample_residuals_ : numpy ndarray\n        Residuals of the model when predicting training data. Only stored up to\n        10_000 values. If `transformer_y` is not `None`, residuals are stored in\n        the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation.\n    in_sample_residuals_by_bin_ : dict\n        In sample residuals binned according to the predicted value each residual\n        is associated with. If `transformer_y` is not `None`, residuals are stored\n        in the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n        **New in version 0.14.0**\n    out_sample_residuals_ : numpy ndarray\n        Residuals of the model when predicting non training data. Only stored up to\n        10_000 values. If `transformer_y` is not `None`, residuals are stored in\n        the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation.\n    out_sample_residuals_by_bin_ : dict\n        Out of sample residuals binned according to the predicted value each residual\n        is associated with. If `transformer_y` is not `None`, residuals are stored\n        in the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n        **New in version 0.12.0**\n    creation_date : str\n        Date of creation.\n    is_fitted : bool\n        Tag to identify if the regressor has been fitted (trained).\n    fit_date : str\n        Date of last fit.\n    skforecast_version : str\n        Version of skforecast library used to create the forecaster.\n    python_version : str\n        Version of python used to create the forecaster.\n    forecaster_id : str, int\n        Name used as an identifier of the forecaster.\n    \n    \"\"\"\n\n    def __init__(\n        self,\n        regressor: object,\n        lags: Optional[Union[int, list, np.ndarray, range]] = None,\n        window_features: Optional[Union[object, list]] = None,\n        transformer_y: Optional[object] = None,\n        transformer_exog: Optional[object] = None,\n        weight_func: Optional[Callable] = None,\n        differentiation: Optional[int] = None,\n        fit_kwargs: Optional[dict] = None,\n        binner_kwargs: Optional[dict] = None,\n        forecaster_id: Optional[Union[str, int]] = None\n    ) -> None:\n        \n        self.regressor                          = copy(regressor)\n        self.transformer_y                      = transformer_y\n        self.transformer_exog                   = transformer_exog\n        self.weight_func                        = weight_func\n        self.source_code_weight_func            = None\n        self.differentiation                    = differentiation\n        self.differentiator                     = None\n        self.last_window_                       = None\n        self.index_type_                        = None\n        self.index_freq_                        = None\n        self.training_range_                    = None\n        self.exog_in_                           = False\n        self.exog_names_in_                     = None\n        self.exog_type_in_                      = None\n        self.exog_dtypes_in_                    = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_            = None\n        self.X_train_features_names_out_        = None\n        self.in_sample_residuals_               = None\n        self.out_sample_residuals_              = None\n        self.in_sample_residuals_by_bin_        = None\n        self.out_sample_residuals_by_bin_       = None\n        self.creation_date                      = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.is_fitted                          = False\n        self.fit_date                           = None\n        self.skforecast_version                 = skforecast.__version__\n        self.python_version                     = sys.version.split(\" \")[0]\n        self.forecaster_id                      = forecaster_id\n\n        self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\n        self.window_features, self.window_features_names, self.max_size_window_features = (\n            initialize_window_features(window_features)\n        )\n        if self.window_features is None and self.lags is None:\n            raise ValueError(\n                \"At least one of the arguments `lags` or `window_features` \"\n                \"must be different from None. This is required to create the \"\n                \"predictors used in training the forecaster.\"\n            )\n        \n        self.window_size = max(\n            [ws for ws in [self.max_lag, self.max_size_window_features] \n             if ws is not None]\n        )\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [\n                type(wf).__name__ for wf in self.window_features\n            ] \n\n        self.binner_kwargs = binner_kwargs\n        if binner_kwargs is None:\n            self.binner_kwargs = {\n                'n_bins': 10, 'method': 'linear', 'subsample': 200000,\n                'random_state': 789654, 'dtype': np.float64\n            }\n        self.binner = QuantileBinner(**self.binner_kwargs)\n        self.binner_intervals_ = None\n\n        if self.differentiation is not None:\n            if not isinstance(differentiation, int) or differentiation < 1:\n                raise ValueError(\n                    f\"Argument `differentiation` must be an integer equal to or \"\n                    f\"greater than 1. Got {differentiation}.\"\n                )\n            self.window_size += self.differentiation\n            self.differentiator = TimeSeriesDifferentiator(\n                order=self.differentiation, window_size=self.window_size\n            )\n\n        self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n            forecaster_name = type(self).__name__, \n            regressor       = regressor, \n            weight_func     = weight_func, \n            series_weights  = None\n        )\n\n        self.fit_kwargs = check_select_fit_kwargs(\n                              regressor  = regressor,\n                              fit_kwargs = fit_kwargs\n                          )\n\n    def __repr__(\n        self\n    ) -> str:\n        \"\"\"\n        Information displayed when a ForecasterRecursive object is printed.\n        \"\"\"\n\n        (\n            params,\n            _,\n            _,\n            exog_names_in_,\n            _,\n        ) = self._preprocess_repr(\n                regressor      = self.regressor,\n                exog_names_in_ = self.exog_names_in_\n            )\n        \n        params = self._format_text_repr(params)\n        exog_names_in_ = self._format_text_repr(exog_names_in_)\n\n        info = (\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"{type(self).__name__} \\n\"\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"Regressor: {type(self.regressor).__name__} \\n\"\n            f\"Lags: {self.lags} \\n\"\n            f\"Window features: {self.window_features_names} \\n\"\n            f\"Window size: {self.window_size} \\n\"\n            f\"Exogenous included: {self.exog_in_} \\n\"\n            f\"Exogenous names: {exog_names_in_} \\n\"\n            f\"Transformer for y: {self.transformer_y} \\n\"\n            f\"Transformer for exog: {self.transformer_exog} \\n\"\n            f\"Weight function included: {True if self.weight_func is not None else False} \\n\"\n            f\"Differentiation order: {self.differentiation} \\n\"\n            f\"Training range: {self.training_range_.to_list() if self.is_fitted else None} \\n\"\n            f\"Training index type: {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None} \\n\"\n            f\"Training index frequency: {self.index_freq_ if self.is_fitted else None} \\n\"\n            f\"Regressor parameters: {params} \\n\"\n            f\"fit_kwargs: {self.fit_kwargs} \\n\"\n            f\"Creation date: {self.creation_date} \\n\"\n            f\"Last fit date: {self.fit_date} \\n\"\n            f\"Skforecast version: {self.skforecast_version} \\n\"\n            f\"Python version: {self.python_version} \\n\"\n            f\"Forecaster id: {self.forecaster_id} \\n\"\n        )\n\n        return info\n\n\n    def _repr_html_(self):\n        \"\"\"\n        HTML representation of the object.\n        The \"General Information\" section is expanded by default.\n        \"\"\"\n\n        (\n            params,\n            _,\n            _,\n            exog_names_in_,\n            _,\n        ) = self._preprocess_repr(\n                regressor      = self.regressor,\n                exog_names_in_ = self.exog_names_in_\n            )\n\n        style, unique_id = self._get_style_repr_html(self.is_fitted)\n        \n        content = f\"\"\"\n        <div class=\"container-{unique_id}\">\n            <h2>{type(self).__name__}</h2>\n            <details open>\n                <summary>General Information</summary>\n                <ul>\n                    <li><strong>Regressor:</strong> {type(self.regressor).__name__}</li>\n                    <li><strong>Lags:</strong> {self.lags}</li>\n                    <li><strong>Window features:</strong> {self.window_features_names}</li>\n                    <li><strong>Window size:</strong> {self.window_size}</li>\n                    <li><strong>Exogenous included:</strong> {self.exog_in_}</li>\n                    <li><strong>Weight function included:</strong> {self.weight_func is not None}</li>\n                    <li><strong>Differentiation order:</strong> {self.differentiation}</li>\n                    <li><strong>Creation date:</strong> {self.creation_date}</li>\n                    <li><strong>Last fit date:</strong> {self.fit_date}</li>\n                    <li><strong>Skforecast version:</strong> {self.skforecast_version}</li>\n                    <li><strong>Python version:</strong> {self.python_version}</li>\n                    <li><strong>Forecaster id:</strong> {self.forecaster_id}</li>\n                </ul>\n            </details>\n            <details>\n                <summary>Exogenous Variables</summary>\n                <ul>\n                    {exog_names_in_}\n                </ul>\n            </details>\n            <details>\n                <summary>Data Transformations</summary>\n                <ul>\n                    <li><strong>Transformer for y:</strong> {self.transformer_y}</li>\n                    <li><strong>Transformer for exog:</strong> {self.transformer_exog}</li>\n                </ul>\n            </details>\n            <details>\n                <summary>Training Information</summary>\n                <ul>\n                    <li><strong>Training range:</strong> {self.training_range_.to_list() if self.is_fitted else 'Not fitted'}</li>\n                    <li><strong>Training index type:</strong> {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted'}</li>\n                    <li><strong>Training index frequency:</strong> {self.index_freq_ if self.is_fitted else 'Not fitted'}</li>\n                </ul>\n            </details>\n            <details>\n                <summary>Regressor Parameters</summary>\n                <ul>\n                    {params}\n                </ul>\n            </details>\n            <details>\n                <summary>Fit Kwargs</summary>\n                <ul>\n                    {self.fit_kwargs}\n                </ul>\n            </details>\n            <p>\n                <a href=\"https://skforecast.org/{skforecast.__version__}/api/forecasterrecursive.html\">&#128712 <strong>API Reference</strong></a>\n                &nbsp;&nbsp;\n                <a href=\"https://skforecast.org/{skforecast.__version__}/user_guides/autoregresive-forecaster.html\">&#128462 <strong>User Guide</strong></a>\n            </p>\n        </div>\n        \"\"\"\n\n        # Return the combined style and content\n        return style + content\n\n\n    def _create_lags(\n        self,\n        y: np.ndarray,\n        X_as_pandas: bool = False,\n        train_index: Optional[pd.Index] = None\n    ) -> Tuple[Optional[Union[np.ndarray, pd.DataFrame]], np.ndarray]:\n        \"\"\"\n        Create the lagged values and their target variable from a time series.\n        \n        Note that the returned matrix `X_data` contains the lag 1 in the first \n        column, the lag 2 in the in the second column and so on.\n        \n        Parameters\n        ----------\n        y : numpy ndarray\n            Training time series values.\n        X_as_pandas : bool, default `False`\n            If `True`, the returned matrix `X_data` is a pandas DataFrame.\n        train_index : pandas Index, default `None`\n            Index of the training data. It is used to create the pandas DataFrame\n            `X_data` when `X_as_pandas` is `True`.\n\n        Returns\n        -------\n        X_data : numpy ndarray, pandas DataFrame, None\n            Lagged values (predictors).\n        y_data : numpy ndarray\n            Values of the time series related to each row of `X_data`.\n        \n        \"\"\"\n\n        X_data = None\n        if self.lags is not None:\n            n_rows = len(y) - self.window_size\n            X_data = np.full(\n                shape=(n_rows, len(self.lags)), fill_value=np.nan, order='F', dtype=float\n            )\n            for i, lag in enumerate(self.lags):\n                X_data[:, i] = y[self.window_size - lag: -lag]\n\n            if X_as_pandas:\n                X_data = pd.DataFrame(\n                             data    = X_data,\n                             columns = self.lags_names,\n                             index   = train_index\n                         )\n\n        y_data = y[self.window_size:]\n\n        return X_data, y_data\n\n\n    def _create_window_features(\n        self, \n        y: pd.Series,\n        train_index: pd.Index,\n        X_as_pandas: bool = False,\n    ) -> Tuple[list, list]:\n        \"\"\"\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        train_index : pandas Index\n            Index of the training data. It is used to create the pandas DataFrame\n            `X_train_window_features` when `X_as_pandas` is `True`.\n        X_as_pandas : bool, default `False`\n            If `True`, the returned matrix `X_train_window_features` is a \n            pandas DataFrame.\n\n        Returns\n        -------\n        X_train_window_features : list\n            List of numpy ndarrays or pandas DataFrames with the window features.\n        X_train_window_features_names_out_ : list\n            Names of the window features.\n        \n        \"\"\"\n\n        len_train_index = len(train_index)\n        X_train_window_features = []\n        X_train_window_features_names_out_ = []\n        for wf in self.window_features:\n            X_train_wf = wf.transform_batch(y)\n            if not isinstance(X_train_wf, pd.DataFrame):\n                raise TypeError(\n                    (f\"The method `transform_batch` of {type(wf).__name__} \"\n                     f\"must return a pandas DataFrame.\")\n                )\n            X_train_wf = X_train_wf.iloc[-len_train_index:]\n            if not len(X_train_wf) == len_train_index:\n                raise ValueError(\n                    (f\"The method `transform_batch` of {type(wf).__name__} \"\n                     f\"must return a DataFrame with the same number of rows as \"\n                     f\"the input time series - `window_size`: {len_train_index}.\")\n                )\n            if not (X_train_wf.index == train_index).all():\n                raise ValueError(\n                    (f\"The method `transform_batch` of {type(wf).__name__} \"\n                     f\"must return a DataFrame with the same index as \"\n                     f\"the input time series - `window_size`.\")\n                )\n            \n            X_train_window_features_names_out_.extend(X_train_wf.columns)\n            if not X_as_pandas:\n                X_train_wf = X_train_wf.to_numpy()     \n            X_train_window_features.append(X_train_wf)\n\n        return X_train_window_features, X_train_window_features_names_out_\n\n\n    def _create_train_X_y(\n        self,\n        y: pd.Series,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n    ) -> Tuple[pd.DataFrame, pd.Series, list, list, list, list, dict]:\n        \"\"\"\n        Create training matrices from univariate time series and exogenous\n        variables.\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors).\n        y_train : pandas Series\n            Values of the time series related to each row of `X_train`.\n        exog_names_in_ : list\n            Names of the exogenous variables used during training.\n        X_train_window_features_names_out_ : list\n            Names of the window features included in the matrix `X_train` created\n            internally for training.\n        X_train_exog_names_out_ : list\n            Names of the exogenous variables included in the matrix `X_train` created\n            internally for training. It can be different from `exog_names_in_` if\n            some exogenous variables are transformed during the training process.\n        X_train_features_names_out_ : list\n            Names of the columns of the matrix created internally for training.\n        exog_dtypes_in_ : dict\n            Type of each exogenous variable/s used in training. If `transformer_exog` \n            is used, the dtypes are calculated before the transformation.\n        \n        \"\"\"\n\n        check_y(y=y)\n        y = input_to_frame(data=y, input_name='y')\n\n        if len(y) <= self.window_size:\n            raise ValueError(\n                (f\"Length of `y` must be greater than the maximum window size \"\n                 f\"needed by the forecaster.\\n\"\n                 f\"    Length `y`: {len(y)}.\\n\"\n                 f\"    Max window size: {self.window_size}.\\n\"\n                 f\"    Lags window size: {self.max_lag}.\\n\"\n                 f\"    Window features window size: {self.max_size_window_features}.\")\n            )\n\n        fit_transformer = False if self.is_fitted else True\n        y = transform_dataframe(\n                df                = y, \n                transformer       = self.transformer_y,\n                fit               = fit_transformer,\n                inverse_transform = False,\n            )\n        y_values, y_index = preprocess_y(y=y)\n        train_index = y_index[self.window_size:]\n\n        if self.differentiation is not None:\n            if not self.is_fitted:\n                y_values = self.differentiator.fit_transform(y_values)\n            else:\n                differentiator = copy(self.differentiator)\n                y_values = differentiator.fit_transform(y_values)\n\n        exog_names_in_ = None\n        exog_dtypes_in_ = None\n        categorical_features = False\n        if exog is not None:\n            check_exog(exog=exog, allow_nan=True)\n            exog = input_to_frame(data=exog, input_name='exog')\n\n            len_y = len(y_values)\n            len_train_index = len(train_index)\n            len_exog = len(exog)\n            if not len_exog == len_y and not len_exog == len_train_index:\n                raise ValueError(\n                    f\"Length of `exog` must be equal to the length of `y` (if index is \"\n                    f\"fully aligned) or length of `y` - `window_size` (if `exog` \"\n                    f\"starts after the first `window_size` values).\\n\"\n                    f\"    `exog`              : ({exog.index[0]} -- {exog.index[-1]})  (n={len_exog})\\n\"\n                    f\"    `y`                 : ({y.index[0]} -- {y.index[-1]})  (n={len_y})\\n\"\n                    f\"    `y` - `window_size` : ({train_index[0]} -- {train_index[-1]})  (n={len_train_index})\"\n                )\n\n            exog_names_in_ = exog.columns.to_list()\n            exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = fit_transformer,\n                       inverse_transform = False\n                   )\n\n            check_exog_dtypes(exog, call_check_exog=True)\n            categorical_features = (\n                exog.select_dtypes(include=np.number).shape[1] != exog.shape[1]\n            )\n\n            _, exog_index = preprocess_exog(exog=exog, return_values=False)\n            if len_exog == len_y:\n                if not (exog_index == y_index).all():\n                    raise ValueError(\n                        \"When `exog` has the same length as `y`, the index of \"\n                        \"`exog` must be aligned with the index of `y` \"\n                        \"to ensure the correct alignment of values.\"\n                    )\n                # The first `self.window_size` positions have to be removed from \n                # exog since they are not in X_train.\n                exog = exog.iloc[self.window_size:, ]\n            else:\n                if not (exog_index == train_index).all():\n                    raise ValueError(\n                        \"When `exog` doesn't contain the first `window_size` observations, \"\n                        \"the index of `exog` must be aligned with the index of `y` minus \"\n                        \"the first `window_size` observations to ensure the correct \"\n                        \"alignment of values.\"\n                    )\n            \n        X_train = []\n        X_train_features_names_out_ = []\n        X_as_pandas = True if categorical_features else False\n\n        X_train_lags, y_train = self._create_lags(\n            y=y_values, X_as_pandas=X_as_pandas, train_index=train_index\n        )\n        if X_train_lags is not None:\n            X_train.append(X_train_lags)\n            X_train_features_names_out_.extend(self.lags_names)\n        \n        X_train_window_features_names_out_ = None\n        if self.window_features is not None:\n            n_diff = 0 if self.differentiation is None else self.differentiation\n            y_window_features = pd.Series(y_values[n_diff:], index=y_index[n_diff:])\n            X_train_window_features, X_train_window_features_names_out_ = (\n                self._create_window_features(\n                    y=y_window_features, X_as_pandas=X_as_pandas, train_index=train_index\n                )\n            )\n            X_train.extend(X_train_window_features)\n            X_train_features_names_out_.extend(X_train_window_features_names_out_)\n\n        X_train_exog_names_out_ = None\n        if exog is not None:\n            X_train_exog_names_out_ = exog.columns.to_list()  \n            if not X_as_pandas:\n                exog = exog.to_numpy()     \n            X_train_features_names_out_.extend(X_train_exog_names_out_)\n            X_train.append(exog)\n        \n        if len(X_train) == 1:\n            X_train = X_train[0]\n        else:\n            if X_as_pandas:\n                X_train = pd.concat(X_train, axis=1)\n            else:\n                X_train = np.concatenate(X_train, axis=1)\n                \n        if X_as_pandas:\n            X_train.index = train_index\n        else:\n            X_train = pd.DataFrame(\n                          data    = X_train,\n                          index   = train_index,\n                          columns = X_train_features_names_out_\n                      )\n        \n        y_train = pd.Series(\n                      data  = y_train,\n                      index = train_index,\n                      name  = 'y'\n                  )\n\n        return (\n            X_train,\n            y_train,\n            exog_names_in_,\n            X_train_window_features_names_out_,\n            X_train_exog_names_out_,\n            X_train_features_names_out_,\n            exog_dtypes_in_\n        )\n\n\n    def create_train_X_y(\n        self,\n        y: pd.Series,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n    ) -> Tuple[pd.DataFrame, pd.Series]:\n        \"\"\"\n        Create training matrices from univariate time series and exogenous\n        variables.\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors).\n        y_train : pandas Series\n            Values of the time series related to each row of `X_data`.\n        \n        \"\"\"\n\n        output = self._create_train_X_y(y=y, exog=exog)\n\n        X_train = output[0]\n        y_train = output[1]\n\n        return X_train, y_train\n\n\n    def _train_test_split_one_step_ahead(\n        self,\n        y: pd.Series,\n        initial_train_size: int,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n    ) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n        \"\"\"\n        Create matrices needed to train and test the forecaster for one-step-ahead\n        predictions.\n\n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        initial_train_size : int\n            Initial size of the training set. It is the number of observations used\n            to train the forecaster before making the first prediction.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n        \n        Returns\n        -------\n        X_train : pandas DataFrame\n            Predictor values used to train the model.\n        y_train : pandas Series\n            Target values related to each row of `X_train`.\n        X_test : pandas DataFrame\n            Predictor values used to test the model.\n        y_test : pandas Series\n            Target values related to each row of `X_test`.\n        \n        \"\"\"\n\n        is_fitted = self.is_fitted\n        self.is_fitted = False\n        X_train, y_train, *_ = self._create_train_X_y(\n            y    = y.iloc[: initial_train_size],\n            exog = exog.iloc[: initial_train_size] if exog is not None else None\n        )\n\n        test_init = initial_train_size - self.window_size\n        self.is_fitted = True\n        X_test, y_test, *_ = self._create_train_X_y(\n            y    = y.iloc[test_init:],\n            exog = exog.iloc[test_init:] if exog is not None else None\n        )\n\n        self.is_fitted = is_fitted\n\n        return X_train, y_train, X_test, y_test\n\n\n    def create_sample_weights(\n        self,\n        X_train: pd.DataFrame,\n    ) -> np.ndarray:\n        \"\"\"\n        Crate weights for each observation according to the forecaster's attribute\n        `weight_func`.\n\n        Parameters\n        ----------\n        X_train : pandas DataFrame\n            Dataframe created with the `create_train_X_y` method, first return.\n\n        Returns\n        -------\n        sample_weight : numpy ndarray\n            Weights to use in `fit` method.\n\n        \"\"\"\n\n        sample_weight = None\n\n        if self.weight_func is not None:\n            sample_weight = self.weight_func(X_train.index)\n\n        if sample_weight is not None:\n            if np.isnan(sample_weight).any():\n                raise ValueError(\n                    \"The resulting `sample_weight` cannot have NaN values.\"\n                )\n            if np.any(sample_weight < 0):\n                raise ValueError(\n                    \"The resulting `sample_weight` cannot have negative values.\"\n                )\n            if np.sum(sample_weight) == 0:\n                raise ValueError(\n                    (\"The resulting `sample_weight` cannot be normalized because \"\n                     \"the sum of the weights is zero.\")\n                )\n\n        return sample_weight\n\n\n    def fit(\n        self,\n        y: pd.Series,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        store_last_window: bool = True,\n        store_in_sample_residuals: bool = True,\n        random_state: int = 123\n    ) -> None:\n        \"\"\"\n        Training Forecaster.\n\n        Additional arguments to be passed to the `fit` method of the regressor \n        can be added with the `fit_kwargs` argument when initializing the forecaster.\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned so\n            that y[i] is regressed on exog[i].\n        store_last_window : bool, default `True`\n            Whether or not to store the last window (`last_window_`) of training data.\n        store_in_sample_residuals : bool, default `True`\n            If `True`, in-sample residuals will be stored in the forecaster object\n            after fitting (`in_sample_residuals_` attribute).\n        random_state : int, default `123`\n            Set a seed for the random generator so that the stored sample \n            residuals are always deterministic.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        # Reset values in case the forecaster has already been fitted.\n        self.last_window_                       = None\n        self.index_type_                        = None\n        self.index_freq_                        = None\n        self.training_range_                    = None\n        self.exog_in_                           = False\n        self.exog_names_in_                     = None\n        self.exog_type_in_                      = None\n        self.exog_dtypes_in_                    = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_            = None\n        self.X_train_features_names_out_        = None\n        self.in_sample_residuals_               = None\n        self.is_fitted                          = False\n        self.fit_date                           = None\n\n        (\n            X_train,\n            y_train,\n            exog_names_in_,\n            X_train_window_features_names_out_,\n            X_train_exog_names_out_,\n            X_train_features_names_out_,\n            exog_dtypes_in_\n        ) = self._create_train_X_y(y=y, exog=exog)\n        sample_weight = self.create_sample_weights(X_train=X_train)\n\n        if sample_weight is not None:\n            self.regressor.fit(\n                X             = X_train,\n                y             = y_train,\n                sample_weight = sample_weight,\n                **self.fit_kwargs\n            )\n        else:\n            self.regressor.fit(X=X_train, y=y_train, **self.fit_kwargs)\n\n        self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n        self.X_train_features_names_out_ = X_train_features_names_out_\n\n        self.is_fitted = True\n        self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.training_range_ = preprocess_y(y=y, return_values=False)[1][[0, -1]]\n        self.index_type_ = type(X_train.index)\n        if isinstance(X_train.index, pd.DatetimeIndex):\n            self.index_freq_ = X_train.index.freqstr\n        else: \n            self.index_freq_ = X_train.index.step\n\n        if exog is not None:\n            self.exog_in_ = True\n            self.exog_type_in_ = type(exog)\n            self.exog_names_in_ = exog_names_in_\n            self.exog_dtypes_in_ = exog_dtypes_in_\n            self.X_train_exog_names_out_ = X_train_exog_names_out_\n\n        # This is done to save time during fit in functions such as backtesting()\n        if store_in_sample_residuals:\n            self._binning_in_sample_residuals(\n                y_true       = y_train.to_numpy(),\n                y_pred       = self.regressor.predict(X_train).ravel(),\n                random_state = random_state\n            )\n\n        # The last time window of training data is stored so that lags needed as\n        # predictors in the first iteration of `predict()` can be calculated. It\n        # also includes the values need to calculate the diferenctiation.\n        if store_last_window:\n            self.last_window_ = (\n                y.iloc[-self.window_size:]\n                .copy()\n                .to_frame(name=y.name if y.name is not None else 'y')\n            )\n\n\n    def _binning_in_sample_residuals(\n        self,\n        y_true: np.ndarray,\n        y_pred: np.ndarray,\n        random_state: int = 123\n    ) -> None:\n        \"\"\"\n        Binning residuals according to the predicted value each residual is\n        associated with. First a skforecast.preprocessing.QuantileBinner object\n        is fitted to the predicted values. Then, residuals are binned according\n        to the predicted value each residual is associated with. Residuals are\n        stored in the forecaster object as `in_sample_residuals_` and\n        `in_sample_residuals_by_bin_`.\n        If `transformer_y` is not `None`, `y_true` and `y_pred` are transformed\n        before calculating residuals. If `differentiation` is not `None`, `y_true`\n        and `y_pred` are differentiated before calculating residuals. If both,\n        `transformer_y` and `differentiation` are not `None`, transformation is\n        done before differentiation. The number of residuals stored per bin is\n        limited to  `10_000 // self.binner.n_bins_`. The total number of residuals\n        stored is `10_000`.\n        **New in version 0.14.0**\n\n        Parameters\n        ----------\n        y_true : numpy ndarray\n            True values of the time series.\n        y_pred : numpy ndarray\n            Predicted values of the time series.\n        random_state : int, default `123`\n            Set a seed for the random generator so that the stored sample \n            residuals are always deterministic.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        data = pd.DataFrame({'prediction': y_pred, 'residuals': (y_true - y_pred)})\n        data['bin'] = self.binner.fit_transform(y_pred).astype(int)\n        self.in_sample_residuals_by_bin_ = (\n            data.groupby('bin')['residuals'].apply(np.array).to_dict()\n        )\n\n        rng = np.random.default_rng(seed=random_state)\n        max_sample = 10_000 // self.binner.n_bins_\n        for k, v in self.in_sample_residuals_by_bin_.items():\n            \n            if len(v) > max_sample:\n                sample = v[rng.integers(low=0, high=len(v), size=max_sample)]\n                self.in_sample_residuals_by_bin_[k] = sample\n\n        self.in_sample_residuals_ = np.concatenate(list(\n            self.in_sample_residuals_by_bin_.values()\n        ))\n\n        self.binner_intervals_ = self.binner.intervals_\n\n\n    def _create_predict_inputs(\n        self,\n        steps: Union[int, str, pd.Timestamp], \n        last_window: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        predict_boot: bool = False,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = False,\n        check_inputs: bool = True,\n    ) -> Tuple[np.ndarray, Optional[np.ndarray], pd.Index, int]:\n        \"\"\"\n        Create the inputs needed for the first iteration of the prediction \n        process. As this is a recursive process, the last window is updated at \n        each iteration of the prediction process.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        predict_boot : bool, default `False`\n            If `True`, residuals are returned to generate bootstrapping predictions.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n\n        Returns\n        -------\n        last_window_values : numpy ndarray\n            Series values used to create the predictors needed in the first \n            iteration of the prediction (t + 1).\n        exog_values : numpy ndarray, None\n            Exogenous variable/s included as predictor/s.\n        prediction_index : pandas Index\n            Index of the predictions.\n        steps: int\n            Number of future steps predicted.\n        \n        \"\"\"\n\n        if last_window is None:\n            last_window = self.last_window_\n\n        if self.is_fitted:\n            steps = date_to_index_position(\n                        index        = last_window.index,\n                        date_input   = steps,\n                        date_literal = 'steps'\n                    )\n\n        if check_inputs:\n            check_predict_input(\n                forecaster_name  = type(self).__name__,\n                steps            = steps,\n                is_fitted        = self.is_fitted,\n                exog_in_         = self.exog_in_,\n                index_type_      = self.index_type_,\n                index_freq_      = self.index_freq_,\n                window_size      = self.window_size,\n                last_window      = last_window,\n                exog             = exog,\n                exog_type_in_    = self.exog_type_in_,\n                exog_names_in_   = self.exog_names_in_,\n                interval         = None\n            )\n        \n            if predict_boot and not use_in_sample_residuals:\n                if not use_binned_residuals and self.out_sample_residuals_ is None:\n                    raise ValueError(\n                        \"`forecaster.out_sample_residuals_` is `None`. Use \"\n                        \"`use_in_sample_residuals=True` or the \"\n                        \"`set_out_sample_residuals()` method before predicting.\"\n                    )\n                if use_binned_residuals and self.out_sample_residuals_by_bin_ is None:\n                    raise ValueError(\n                        \"`forecaster.out_sample_residuals_by_bin_` is `None`. Use \"\n                        \"`use_in_sample_residuals=True` or the \"\n                        \"`set_out_sample_residuals()` method before predicting.\"\n                    )\n\n        last_window = last_window.iloc[-self.window_size:].copy()\n        last_window_values, last_window_index = preprocess_last_window(\n                                                    last_window = last_window\n                                                )\n\n        last_window_values = transform_numpy(\n                                 array             = last_window_values,\n                                 transformer       = self.transformer_y,\n                                 fit               = False,\n                                 inverse_transform = False\n                             )\n        if self.differentiation is not None:\n            last_window_values = self.differentiator.fit_transform(last_window_values)\n\n        if exog is not None:\n            exog = input_to_frame(data=exog, input_name='exog')\n            exog = exog.loc[:, self.exog_names_in_]\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n            check_exog_dtypes(exog=exog)\n            exog_values = exog.to_numpy()[:steps]\n        else:\n            exog_values = None\n\n        prediction_index = expand_index(\n                               index = last_window_index,\n                               steps = steps,\n                           )\n\n        return last_window_values, exog_values, prediction_index, steps\n\n\n    def _recursive_predict(\n        self,\n        steps: int,\n        last_window_values: np.ndarray,\n        exog_values: Optional[np.ndarray] = None,\n        residuals: Optional[Union[np.ndarray, dict]] = None,\n        use_binned_residuals: bool = False,\n    ) -> np.ndarray:\n        \"\"\"\n        Predict n steps ahead. It is an iterative process in which, each prediction,\n        is used as a predictor for the next step.\n        \n        Parameters\n        ----------\n        steps : int\n            Number of future steps predicted.\n        last_window_values : numpy ndarray\n            Series values used to create the predictors needed in the first \n            iteration of the prediction (t + 1).\n        exog_values : numpy ndarray, default `None`\n            Exogenous variable/s included as predictor/s.\n        residuals : numpy ndarray, dict, default `None`\n            Residuals used to generate bootstrapping predictions.\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : numpy ndarray\n            Predicted values.\n        \n        \"\"\"\n\n        n_lags = len(self.lags) if self.lags is not None else 0\n        n_window_features = (\n            len(self.X_train_window_features_names_out_)\n            if self.window_features is not None\n            else 0\n        )\n        n_exog = exog_values.shape[1] if exog_values is not None else 0\n\n        X = np.full(\n            shape=(n_lags + n_window_features + n_exog), fill_value=np.nan, dtype=float\n        )\n        predictions = np.full(shape=steps, fill_value=np.nan, dtype=float)\n        last_window = np.concatenate((last_window_values, predictions))\n\n        for i in range(steps):\n\n            if self.lags is not None:\n                X[:n_lags] = last_window[-self.lags - (steps - i)]\n            if self.window_features is not None:\n                X[n_lags : n_lags + n_window_features] = np.concatenate(\n                    [\n                        wf.transform(last_window[i : -(steps - i)])\n                        for wf in self.window_features\n                    ]\n                )\n            if exog_values is not None:\n                X[n_lags + n_window_features:] = exog_values[i]\n        \n            pred = self.regressor.predict(X.reshape(1, -1)).ravel()\n            \n            if residuals is not None:\n                if use_binned_residuals:\n                    predicted_bin = (\n                        self.binner.transform(pred).item()\n                    )\n                    step_residual = residuals[predicted_bin][i]\n                else:\n                    step_residual = residuals[i]\n                \n                pred += step_residual\n            \n            predictions[i] = pred[0]\n\n            # Update `last_window` values. The first position is discarded and \n            # the new prediction is added at the end.\n            last_window[-(steps - i)] = pred[0]\n\n        return predictions\n\n\n    def create_predict_X(\n        self,\n        steps: int,\n        last_window: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        Create the predictors needed to predict `steps` ahead. As it is a recursive\n        process, the predictors are created at each iteration of the prediction \n        process.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n\n        Returns\n        -------\n        X_predict : pandas DataFrame\n            Pandas DataFrame with the predictors for each step. The index \n            is the same as the prediction index.\n        \n        \"\"\"\n\n        last_window_values, exog_values, prediction_index, steps = (\n            self._create_predict_inputs(steps=steps, last_window=last_window, exog=exog)\n        )\n        \n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\", \n                message=\"X does not have valid feature names\", \n                category=UserWarning\n            )\n            predictions = self._recursive_predict(\n                              steps              = steps,\n                              last_window_values = last_window_values,\n                              exog_values        = exog_values\n                          )\n\n        X_predict = []\n        full_predictors = np.concatenate((last_window_values, predictions))\n\n        if self.lags is not None:\n            idx = np.arange(-steps, 0)[:, None] - self.lags\n            X_lags = full_predictors[idx + len(full_predictors)]\n            X_predict.append(X_lags)\n\n        if self.window_features is not None:\n            X_window_features = np.full(\n                shape      = (steps, len(self.X_train_window_features_names_out_)), \n                fill_value = np.nan, \n                order      = 'C',\n                dtype      = float\n            )\n            for i in range(steps):\n                X_window_features[i, :] = np.concatenate(\n                    [wf.transform(full_predictors[i:-(steps - i)]) \n                     for wf in self.window_features]\n                )\n            X_predict.append(X_window_features)\n\n        if exog is not None:\n            X_predict.append(exog_values)\n\n        X_predict = pd.DataFrame(\n                        data    = np.concatenate(X_predict, axis=1),\n                        columns = self.X_train_features_names_out_,\n                        index   = prediction_index\n                    )\n        \n        if self.transformer_y is not None or self.differentiation is not None:\n            warnings.warn(\n                \"The output matrix is in the transformed scale due to the \"\n                \"inclusion of transformations or differentiation in the Forecaster. \"\n                \"As a result, any predictions generated using this matrix will also \"\n                \"be in the transformed scale. Please refer to the documentation \"\n                \"for more details: \"\n                \"https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html\",\n                DataTransformationWarning\n            )\n\n        return X_predict\n\n\n    def predict(\n        self,\n        steps: Union[int, str, pd.Timestamp],\n        last_window: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        check_inputs: bool = True\n    ) -> pd.Series:\n        \"\"\"\n        Predict n steps ahead. It is an recursive process in which, each prediction,\n        is used as a predictor for the next step.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n\n        Returns\n        -------\n        predictions : pandas Series\n            Predicted values.\n        \n        \"\"\"\n\n        last_window_values, exog_values, prediction_index, steps = (\n            self._create_predict_inputs(\n                steps=steps,\n                last_window=last_window,\n                exog=exog,\n                check_inputs=check_inputs,\n            )\n        )\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\", \n                message=\"X does not have valid feature names\", \n                category=UserWarning\n            )\n            predictions = self._recursive_predict(\n                              steps              = steps,\n                              last_window_values = last_window_values,\n                              exog_values        = exog_values\n                          )\n\n        if self.differentiation is not None:\n            predictions = self.differentiator.inverse_transform_next_window(predictions)\n\n        predictions = transform_numpy(\n                          array             = predictions,\n                          transformer       = self.transformer_y,\n                          fit               = False,\n                          inverse_transform = True\n                      )\n\n        predictions = pd.Series(\n                          data  = predictions,\n                          index = prediction_index,\n                          name  = 'pred'\n                      )\n\n        return predictions\n\n\n    def predict_bootstrapping(\n        self,\n        steps: Union[int, str, pd.Timestamp],\n        last_window: Optional[pd.Series] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = False\n    ) -> pd.DataFrame:\n        \"\"\"\n        Generate multiple forecasting predictions using a bootstrapping process. \n        By sampling from a collection of past observed errors (the residuals),\n        each iteration of bootstrapping generates a different set of predictions. \n        See the Notes section for more information. \n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        boot_predictions : pandas DataFrame\n            Predictions generated by bootstrapping.\n            Shape: (steps, n_boot)\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n        Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n        \"\"\"\n\n        (\n            last_window_values,\n            exog_values,\n            prediction_index,\n            steps\n        ) = self._create_predict_inputs(\n            steps                   = steps, \n            last_window             = last_window, \n            exog                    = exog,\n            predict_boot            = True, \n            use_in_sample_residuals = use_in_sample_residuals,\n            use_binned_residuals    = use_binned_residuals\n        )\n\n        if use_in_sample_residuals:\n            residuals = self.in_sample_residuals_\n            residuals_by_bin = self.in_sample_residuals_by_bin_\n        else:\n            residuals = self.out_sample_residuals_\n            residuals_by_bin = self.out_sample_residuals_by_bin_\n\n        rng = np.random.default_rng(seed=random_state)\n        if use_binned_residuals:\n            sampled_residuals = {\n                k: v[rng.integers(low=0, high=len(v), size=(steps, n_boot))]\n                for k, v in residuals_by_bin.items()\n            }\n        else:\n            sampled_residuals = residuals[\n                rng.integers(low=0, high=len(residuals), size=(steps, n_boot))\n            ]\n        \n        boot_columns = []\n        boot_predictions = np.full(\n                               shape      = (steps, n_boot),\n                               fill_value = np.nan,\n                               order      = 'F',\n                               dtype      = float\n                           )\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\", \n                message=\"X does not have valid feature names\", \n                category=UserWarning\n            )\n            for i in range(n_boot):\n\n                if use_binned_residuals:\n                    boot_sampled_residuals = {\n                        k: v[:, i]\n                        for k, v in sampled_residuals.items()\n                    }\n                else:\n                    boot_sampled_residuals = sampled_residuals[:, i]\n\n                boot_columns.append(f\"pred_boot_{i}\")\n                boot_predictions[:, i] = self._recursive_predict(\n                    steps                = steps,\n                    last_window_values   = last_window_values,\n                    exog_values          = exog_values,\n                    residuals            = boot_sampled_residuals,\n                    use_binned_residuals = use_binned_residuals,\n                )\n\n        if self.differentiation is not None:\n            boot_predictions = (\n                self.differentiator.inverse_transform_next_window(boot_predictions)\n            )\n        \n        if self.transformer_y:\n            boot_predictions = np.apply_along_axis(\n                                   func1d            = transform_numpy,\n                                   axis              = 0,\n                                   arr               = boot_predictions,\n                                   transformer       = self.transformer_y,\n                                   fit               = False,\n                                   inverse_transform = True\n                               )\n\n        boot_predictions = pd.DataFrame(\n                               data    = boot_predictions,\n                               index   = prediction_index,\n                               columns = boot_columns\n                           )\n\n        return boot_predictions\n\n\n    def predict_interval(\n        self,\n        steps: Union[int, str, pd.Timestamp],\n        last_window: Optional[pd.Series] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        interval: list = [5, 95],\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = False\n    ) -> pd.DataFrame:\n        \"\"\"\n        Iterative process in which each prediction is used as a predictor\n        for the next step, and bootstrapping is used to estimate prediction\n        intervals. Both predictions and intervals are returned.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        interval : list, default `[5, 95]`\n            Confidence of the prediction interval estimated. Sequence of \n            percentiles to compute, which must be between 0 and 100 inclusive. \n            For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Values predicted by the forecaster and their estimated interval.\n\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n\n        check_interval(interval=interval)\n\n        boot_predictions = self.predict_bootstrapping(\n                               steps                   = steps,\n                               last_window             = last_window,\n                               exog                    = exog,\n                               n_boot                  = n_boot,\n                               random_state            = random_state,\n                               use_in_sample_residuals = use_in_sample_residuals,\n                               use_binned_residuals    = use_binned_residuals\n                           )\n\n        predictions = self.predict(\n                          steps        = steps,\n                          last_window  = last_window,\n                          exog         = exog,\n                          check_inputs = False\n                      )\n\n        interval = np.array(interval) / 100\n        predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n        predictions_interval.columns = ['lower_bound', 'upper_bound']\n        predictions = pd.concat((predictions, predictions_interval), axis=1)\n\n        return predictions\n\n\n    def predict_quantiles(\n        self,\n        steps: Union[int, str, pd.Timestamp],\n        last_window: Optional[pd.Series] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        quantiles: list = [0.05, 0.5, 0.95],\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = False\n    ) -> pd.DataFrame:\n        \"\"\"\n        Calculate the specified quantiles for each step. After generating \n        multiple forecasting predictions through a bootstrapping process, each \n        quantile is calculated for each step.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        quantiles : list, default `[0.05, 0.5, 0.95]`\n            Sequence of quantiles to compute, which must be between 0 and 1 \n            inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n            `quantiles = [0.05, 0.5, 0.95]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate quantiles.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot quantiles are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create prediction quantiles. If `False`, out of\n            sample residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Quantiles predicted by the forecaster.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n\n        check_interval(quantiles=quantiles)\n\n        boot_predictions = self.predict_bootstrapping(\n                               steps                   = steps,\n                               last_window             = last_window,\n                               exog                    = exog,\n                               n_boot                  = n_boot,\n                               random_state            = random_state,\n                               use_in_sample_residuals = use_in_sample_residuals,\n                               use_binned_residuals    = use_binned_residuals\n                           )\n\n        predictions = boot_predictions.quantile(q=quantiles, axis=1).transpose()\n        predictions.columns = [f'q_{q}' for q in quantiles]\n\n        return predictions\n\n\n    def predict_dist(\n        self,\n        steps: Union[int, str, pd.Timestamp],\n        distribution: object,\n        last_window: Optional[pd.Series] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = False\n    ) -> pd.DataFrame:\n        \"\"\"\n        Fit a given probability distribution for each step. After generating \n        multiple forecasting predictions through a bootstrapping process, each \n        step is fitted to the given distribution.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        distribution : Object\n            A distribution object from scipy.stats.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).  \n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Distribution parameters estimated for each step.\n\n        \"\"\"\n\n        boot_samples = self.predict_bootstrapping(\n                           steps                   = steps,\n                           last_window             = last_window,\n                           exog                    = exog,\n                           n_boot                  = n_boot,\n                           random_state            = random_state,\n                           use_in_sample_residuals = use_in_sample_residuals,\n                           use_binned_residuals    = use_binned_residuals\n                       )       \n\n        param_names = [p for p in inspect.signature(distribution._pdf).parameters\n                       if not p == 'x'] + [\"loc\", \"scale\"]\n        param_values = np.apply_along_axis(\n                           lambda x: distribution.fit(x),\n                           axis = 1,\n                           arr  = boot_samples\n                       )\n        predictions = pd.DataFrame(\n                          data    = param_values,\n                          columns = param_names,\n                          index   = boot_samples.index\n                      )\n\n        return predictions\n\n    def set_params(\n        self, \n        params: dict\n    ) -> None:\n        \"\"\"\n        Set new values to the parameters of the scikit learn model stored in the\n        forecaster.\n        \n        Parameters\n        ----------\n        params : dict\n            Parameters values.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        self.regressor = clone(self.regressor)\n        self.regressor.set_params(**params)\n\n    def set_fit_kwargs(\n        self, \n        fit_kwargs: dict\n    ) -> None:\n        \"\"\"\n        Set new values for the additional keyword arguments passed to the `fit` \n        method of the regressor.\n        \n        Parameters\n        ----------\n        fit_kwargs : dict\n            Dict of the form {\"argument\": new_value}.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n\n    def set_lags(\n        self, \n        lags: Optional[Union[int, list, np.ndarray, range]] = None\n    ) -> None:\n        \"\"\"\n        Set new value to the attribute `lags`. Attributes `lags_names`, \n        `max_lag` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        lags : int, list, numpy ndarray, range, default `None`\n            Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. \n        \n            - `int`: include lags from 1 to `lags` (included).\n            - `list`, `1d numpy ndarray` or `range`: include only lags present in \n            `lags`, all elements must be int.\n            - `None`: no lags are included as predictors. \n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        if self.window_features is None and lags is None:\n            raise ValueError(\n                \"At least one of the arguments `lags` or `window_features` \"\n                \"must be different from None. This is required to create the \"\n                \"predictors used in training the forecaster.\"\n            )\n        \n        self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\n        self.window_size = max(\n            [ws for ws in [self.max_lag, self.max_size_window_features] \n             if ws is not None]\n        )\n        if self.differentiation is not None:\n            self.window_size += self.differentiation\n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_window_features(\n        self, \n        window_features: Optional[Union[object, list]] = None\n    ) -> None:\n        \"\"\"\n        Set new value to the attribute `window_features`. Attributes \n        `max_size_window_features`, `window_features_names`, \n        `window_features_class_names` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        window_features : object, list, default `None`\n            Instance or list of instances used to create window features. Window features\n            are created from the original time series and are included as predictors.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        if window_features is None and self.lags is None:\n            raise ValueError(\n                \"At least one of the arguments `lags` or `window_features` \"\n                \"must be different from None. This is required to create the \"\n                \"predictors used in training the forecaster.\"\n            )\n        \n        self.window_features, self.window_features_names, self.max_size_window_features = (\n            initialize_window_features(window_features)\n        )\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [\n                type(wf).__name__ for wf in self.window_features\n            ] \n        self.window_size = max(\n            [ws for ws in [self.max_lag, self.max_size_window_features] \n             if ws is not None]\n        )\n        if self.differentiation is not None:\n            self.window_size += self.differentiation\n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_out_sample_residuals(\n        self,\n        y_true: Union[pd.Series, np.ndarray],\n        y_pred: Union[pd.Series, np.ndarray],\n        append: bool = False,\n        random_state: int = 123\n    ) -> None:\n        \"\"\"\n        Set new values to the attribute `out_sample_residuals_`. Out of sample\n        residuals are meant to be calculated using observations that did not\n        participate in the training process. `y_true` and `y_pred` are expected\n        to be in the original scale of the time series. Residuals are calculated\n        as `y_true` - `y_pred`, after applying the necessary transformations and\n        differentiations if the forecaster includes them (`self.transformer_y`\n        and `self.differentiation`). Two internal attributes are updated:\n\n        + `out_sample_residuals_`: residuals stored in a numpy ndarray.\n        + `out_sample_residuals_by_bin_`: residuals are binned according to the\n        predicted value they are associated with and stored in a dictionary, where\n        the keys are the  intervals of the predicted values and the values are\n        the residuals associated with that range. If a bin binning is empty, it\n        is filled with a random sample of residuals from other bins. This is done\n        to ensure that all bins have at least one residual and can be used in the\n        prediction process.\n\n        A total of 10_000 residuals are stored in the attribute `out_sample_residuals_`.\n        If the number of residuals is greater than 10_000, a random sample of\n        10_000 residuals is stored. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n        \n        Parameters\n        ----------\n        y_true : pandas Series, numpy ndarray, default `None`\n            True values of the time series from which the residuals have been\n            calculated.\n        y_pred : pandas Series, numpy ndarray, default `None`\n            Predicted values of the time series.\n        append : bool, default `False`\n            If `True`, new residuals are added to the once already stored in the\n            forecaster. If after appending the new residuals, the limit of\n            `10_000 // self.binner.n_bins_` values per bin is reached, a random\n            sample of residuals is stored.\n        random_state : int, default `123`\n            Sets a seed to the random sampling for reproducible output.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n\n        if not self.is_fitted:\n            raise NotFittedError(\n                \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n                \"arguments before using `set_out_sample_residuals()`.\"\n            )\n\n        if not isinstance(y_true, (np.ndarray, pd.Series)):\n            raise TypeError(\n                f\"`y_true` argument must be `numpy ndarray` or `pandas Series`. \"\n                f\"Got {type(y_true)}.\"\n            )\n        \n        if not isinstance(y_pred, (np.ndarray, pd.Series)):\n            raise TypeError(\n                f\"`y_pred` argument must be `numpy ndarray` or `pandas Series`. \"\n                f\"Got {type(y_pred)}.\"\n            )\n        \n        if len(y_true) != len(y_pred):\n            raise ValueError(\n                f\"`y_true` and `y_pred` must have the same length. \"\n                f\"Got {len(y_true)} and {len(y_pred)}.\"\n            )\n        \n        if isinstance(y_true, pd.Series) and isinstance(y_pred, pd.Series):\n            if not y_true.index.equals(y_pred.index):\n                raise ValueError(\n                    \"`y_true` and `y_pred` must have the same index.\"\n                )\n\n        if not isinstance(y_pred, np.ndarray):\n            y_pred = y_pred.to_numpy()\n\n        if not isinstance(y_true, np.ndarray):\n            y_true = y_true.to_numpy()\n\n        if self.transformer_y:\n            y_true = transform_numpy(\n                         array             = y_true,\n                         transformer       = self.transformer_y,\n                         fit               = False,\n                         inverse_transform = False\n                     )\n            y_pred = transform_numpy(\n                         array             = y_pred,\n                         transformer       = self.transformer_y,\n                         fit               = False,\n                         inverse_transform = False\n                     )\n        \n        if self.differentiation is not None:\n            differentiator = copy(self.differentiator)\n            differentiator.set_params(window_size=None)\n            y_true = differentiator.fit_transform(y_true)[self.differentiation:]\n            y_pred = differentiator.fit_transform(y_pred)[self.differentiation:]\n        \n        residuals = y_true - y_pred\n        data = pd.DataFrame({'prediction': y_pred, 'residuals': residuals})\n        data['bin'] = self.binner.transform(y_pred).astype(int)\n        residuals_by_bin = data.groupby('bin')['residuals'].apply(np.array).to_dict()\n\n        if append and self.out_sample_residuals_by_bin_ is not None:\n            for k, v in residuals_by_bin.items():\n                if k in self.out_sample_residuals_by_bin_:\n                    self.out_sample_residuals_by_bin_[k] = np.concatenate((\n                        self.out_sample_residuals_by_bin_[k], v)\n                    )\n                else:\n                    self.out_sample_residuals_by_bin_[k] = v\n        else:\n            self.out_sample_residuals_by_bin_ = residuals_by_bin\n\n        max_samples = 10_000 // self.binner.n_bins_\n        rng = np.random.default_rng(seed=random_state)\n        for k, v in self.out_sample_residuals_by_bin_.items():\n            if len(v) > max_samples:\n                sample = rng.choice(a=v, size=max_samples, replace=False)\n                self.out_sample_residuals_by_bin_[k] = sample\n\n        for k in self.in_sample_residuals_by_bin_.keys():\n            if k not in self.out_sample_residuals_by_bin_:\n                self.out_sample_residuals_by_bin_[k] = np.array([])\n\n        empty_bins = [\n            k for k, v in self.out_sample_residuals_by_bin_.items() \n            if len(v) == 0\n        ]\n        if empty_bins:\n            warnings.warn(\n                f\"The following bins have no out of sample residuals: {empty_bins}. \"\n                f\"No predicted values fall in the interval \"\n                f\"{[self.binner_intervals_[bin] for bin in empty_bins]}. \"\n                f\"Empty bins will be filled with a random sample of residuals.\"\n            )\n            for k in empty_bins:\n                self.out_sample_residuals_by_bin_[k] = rng.choice(\n                    a       = residuals,\n                    size    = max_samples,\n                    replace = True\n                )\n\n        self.out_sample_residuals_ = np.concatenate(list(\n                                         self.out_sample_residuals_by_bin_.values()\n                                     ))\n\n    def get_feature_importances(\n        self,\n        sort_importance: bool = True\n    ) -> pd.DataFrame:\n        \"\"\"\n        Return feature importances of the regressor stored in the forecaster.\n        Only valid when regressor stores internally the feature importances in the\n        attribute `feature_importances_` or `coef_`. Otherwise, returns `None`.\n\n        Parameters\n        ----------\n        sort_importance: bool, default `True`\n            If `True`, sorts the feature importances in descending order.\n\n        Returns\n        -------\n        feature_importances : pandas DataFrame\n            Feature importances associated with each predictor.\n\n        \"\"\"\n\n        if not self.is_fitted:\n            raise NotFittedError(\n                \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n                \"arguments before using `get_feature_importances()`.\"\n            )\n\n        if isinstance(self.regressor, Pipeline):\n            estimator = self.regressor[-1]\n        else:\n            estimator = self.regressor\n\n        if hasattr(estimator, 'feature_importances_'):\n            feature_importances = estimator.feature_importances_\n        elif hasattr(estimator, 'coef_'):\n            feature_importances = estimator.coef_\n        else:\n            warnings.warn(\n                f\"Impossible to access feature importances for regressor of type \"\n                f\"{type(estimator)}. This method is only valid when the \"\n                f\"regressor stores internally the feature importances in the \"\n                f\"attribute `feature_importances_` or `coef_`.\"\n            )\n            feature_importances = None\n\n        if feature_importances is not None:\n            feature_importances = pd.DataFrame({\n                                      'feature': self.X_train_features_names_out_,\n                                      'importance': feature_importances\n                                  })\n            if sort_importance:\n                feature_importances = feature_importances.sort_values(\n                                          by='importance', ascending=False\n                                      )\n\n        return feature_importances\n"
  },
  "GT_src_dict": {
    "skforecast/model_selection/_utils.py": {
      "initialize_lags_grid": {
        "code": "def initialize_lags_grid(forecaster: object, lags_grid: Optional[Union[list, dict]]=None) -> Tuple[dict, str]:\n    \"\"\"Initialize the lags grid for model selection in forecasting.\n\nThis function prepares a configuration of lags to be evaluated during the model selection process. It takes a forecaster instance and an optional lags grid as inputs. If the lags grid is not provided, it uses the default lags from the forecaster.\n\nParameters\n----------\nforecaster : object\n    An instance of a forecaster model (e.g., ForecasterRecursive, ForecasterDirect, etc.) which dictates the lags available for selection.\nlags_grid : Optional[Union[list, dict]], default `None`\n    Either a list or a dictionary specifying the lags to try. If `dict`, keys represent labels for the results DataFrame, whereas values are the lists of lags.\n\nReturns\n-------\nTuple[dict, str]\n    - lags_grid : dict\n        A dictionary configured with the lags for each iteration, with keys as labels for results if dict input was provided.\n    - lags_label : str\n        A label indicating the representation of lags in the results (either 'values' or 'keys').\n\nRaises\n------\nTypeError\n    If `lags_grid` is not a list, dict, or None.\n\nThis function interacts with the `forecaster` parameter to extract default lags when `lags_grid` is None, ensuring that the lags used for model fitting are relevant to the specific forecasting model employed.\"\"\"\n    '\\n    Initialize lags grid and lags label for model selection. \\n\\n    Parameters\\n    ----------\\n    forecaster : Forecaster\\n        Forecaster model. ForecasterRecursive, ForecasterDirect, \\n        ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate.\\n    lags_grid : list, dict, default `None`\\n        Lists of lags to try, containing int, lists, numpy ndarray, or range \\n        objects. If `dict`, the keys are used as labels in the `results` \\n        DataFrame, and the values are used as the lists of lags to try.\\n\\n    Returns\\n    -------\\n    lags_grid : dict\\n        Dictionary with lags configuration for each iteration.\\n    lags_label : str\\n        Label for lags representation in the results object.\\n\\n    '\n    if not isinstance(lags_grid, (list, dict, type(None))):\n        raise TypeError(f'`lags_grid` argument must be a list, dict or None. Got {type(lags_grid)}.')\n    lags_label = 'values'\n    if isinstance(lags_grid, list):\n        lags_grid = {f'{lags}': lags for lags in lags_grid}\n    elif lags_grid is None:\n        lags = [int(lag) for lag in forecaster.lags]\n        lags_grid = {f'{lags}': lags}\n    else:\n        lags_label = 'keys'\n    return (lags_grid, lags_label)",
        "docstring": "Initialize the lags grid for model selection in forecasting.\n\nThis function prepares a configuration of lags to be evaluated during the model selection process. It takes a forecaster instance and an optional lags grid as inputs. If the lags grid is not provided, it uses the default lags from the forecaster.\n\nParameters\n----------\nforecaster : object\n    An instance of a forecaster model (e.g., ForecasterRecursive, ForecasterDirect, etc.) which dictates the lags available for selection.\nlags_grid : Optional[Union[list, dict]], default `None`\n    Either a list or a dictionary specifying the lags to try. If `dict`, keys represent labels for the results DataFrame, whereas values are the lists of lags.\n\nReturns\n-------\nTuple[dict, str]\n    - lags_grid : dict\n        A dictionary configured with the lags for each iteration, with keys as labels for results if dict input was provided.\n    - lags_label : str\n        A label indicating the representation of lags in the results (either 'values' or 'keys').\n\nRaises\n------\nTypeError\n    If `lags_grid` is not a list, dict, or None.\n\nThis function interacts with the `forecaster` parameter to extract default lags when `lags_grid` is None, ensuring that the lags used for model fitting are relevant to the specific forecasting model employed.",
        "signature": "def initialize_lags_grid(forecaster: object, lags_grid: Optional[Union[list, dict]]=None) -> Tuple[dict, str]:",
        "type": "Function",
        "class_signature": null
      }
    },
    "skforecast/recursive/_forecaster_recursive.py": {
      "ForecasterRecursive.__init__": {
        "code": "    def __init__(self, regressor: object, lags: Optional[Union[int, list, np.ndarray, range]]=None, window_features: Optional[Union[object, list]]=None, transformer_y: Optional[object]=None, transformer_exog: Optional[object]=None, weight_func: Optional[Callable]=None, differentiation: Optional[int]=None, fit_kwargs: Optional[dict]=None, binner_kwargs: Optional[dict]=None, forecaster_id: Optional[Union[str, int]]=None) -> None:\n        \"\"\"Initializes a ForecasterRecursive object that enables multi-step time series forecasting using \n    a given regressor compatible with the scikit-learn API. This constructor prepares the essential \n    parameters and attributes required for training and prediction, including the setup for lagged values, \n    window features, data transformers, residuals handling, and additional fitting parameters.\n\n    Parameters\n    ----------\n    regressor : object\n        A regressor or pipeline compatible with the scikit-learn API, used for making predictions.\n    lags : Optional[Union[int, list, np.ndarray, range]], default `None`\n        Defines the lagged values used as predictors; can be specified as an integer, list, \n        numpy array, or range.\n    window_features : Optional[Union[object, list]], default `None`\n        Instances (or list of instances) used for creating additional window features from the time series.\n    transformer_y : Optional[object], default `None`\n        Preprocessor compatible with scikit-learn for transforming the target variable `y` \n        before training.\n    transformer_exog : Optional[object], default `None`\n        Preprocessor compatible with scikit-learn for transforming exogenous variables before training.\n    weight_func : Optional[Callable], default `None`\n        A function to define individual sample weights based on the index, utilized when the regressor supports it.\n    differentiation : Optional[int], default `None`\n        Defines the order of differencing to apply to the time series. If specified, enhances the \n        window size accordingly.\n    fit_kwargs : Optional[dict], default `None`\n        Additional arguments for the regressor's `fit` method.\n    binner_kwargs : Optional[dict], default `None`\n        Configurations for the `QuantileBinner` to discretize the residuals; defaults defined internally.\n    forecaster_id : Optional[Union[str, int]], default `None`\n        An identifier for the forecaster instance.\n\n    Returns\n    -------\n    None\n        This constructor does not return a value. It sets up the internal state of the ForecasterRecursive \n        instance based on the provided parameters.\n\n    Attributes\n    ----------\n    Several attributes are initialized, including:\n        - `self.last_window_`\n        - `self.index_type_`\n        - `self.index_freq_`\n        - `self.training_range_`\n        - `self.is_fitted`\n        - among others related to the features and transformations.\n\n    Raises\n    ------\n    ValueError\n        If both `lags` and `window_features` are `None`, a ValueError is raised, as predictors are required.\n    TypeError\n        If the parameter types do not conform to expectations.\n\n    Dependencies\n    ------------\n    This method utilizes helper functions such as `initialize_lags`, `initialize_window_features`, \n    `initialize_weights`, and `check_select_fit_kwargs`, which are necessary for preparing the \n    forecaster's parameters and ensuring correctness.\"\"\"\n        self.regressor = copy(regressor)\n        self.transformer_y = transformer_y\n        self.transformer_exog = transformer_exog\n        self.weight_func = weight_func\n        self.source_code_weight_func = None\n        self.differentiation = differentiation\n        self.differentiator = None\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.exog_in_ = False\n        self.exog_names_in_ = None\n        self.exog_type_in_ = None\n        self.exog_dtypes_in_ = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_ = None\n        self.X_train_features_names_out_ = None\n        self.in_sample_residuals_ = None\n        self.out_sample_residuals_ = None\n        self.in_sample_residuals_by_bin_ = None\n        self.out_sample_residuals_by_bin_ = None\n        self.creation_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.is_fitted = False\n        self.fit_date = None\n        self.skforecast_version = skforecast.__version__\n        self.python_version = sys.version.split(' ')[0]\n        self.forecaster_id = forecaster_id\n        self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\n        self.window_features, self.window_features_names, self.max_size_window_features = initialize_window_features(window_features)\n        if self.window_features is None and self.lags is None:\n            raise ValueError('At least one of the arguments `lags` or `window_features` must be different from None. This is required to create the predictors used in training the forecaster.')\n        self.window_size = max([ws for ws in [self.max_lag, self.max_size_window_features] if ws is not None])\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [type(wf).__name__ for wf in self.window_features]\n        self.binner_kwargs = binner_kwargs\n        if binner_kwargs is None:\n            self.binner_kwargs = {'n_bins': 10, 'method': 'linear', 'subsample': 200000, 'random_state': 789654, 'dtype': np.float64}\n        self.binner = QuantileBinner(**self.binner_kwargs)\n        self.binner_intervals_ = None\n        if self.differentiation is not None:\n            if not isinstance(differentiation, int) or differentiation < 1:\n                raise ValueError(f'Argument `differentiation` must be an integer equal to or greater than 1. Got {differentiation}.')\n            self.window_size += self.differentiation\n            self.differentiator = TimeSeriesDifferentiator(order=self.differentiation, window_size=self.window_size)\n        self.weight_func, self.source_code_weight_func, _ = initialize_weights(forecaster_name=type(self).__name__, regressor=regressor, weight_func=weight_func, series_weights=None)\n        self.fit_kwargs = check_select_fit_kwargs(regressor=regressor, fit_kwargs=fit_kwargs)",
        "docstring": "Initializes a ForecasterRecursive object that enables multi-step time series forecasting using \na given regressor compatible with the scikit-learn API. This constructor prepares the essential \nparameters and attributes required for training and prediction, including the setup for lagged values, \nwindow features, data transformers, residuals handling, and additional fitting parameters.\n\nParameters\n----------\nregressor : object\n    A regressor or pipeline compatible with the scikit-learn API, used for making predictions.\nlags : Optional[Union[int, list, np.ndarray, range]], default `None`\n    Defines the lagged values used as predictors; can be specified as an integer, list, \n    numpy array, or range.\nwindow_features : Optional[Union[object, list]], default `None`\n    Instances (or list of instances) used for creating additional window features from the time series.\ntransformer_y : Optional[object], default `None`\n    Preprocessor compatible with scikit-learn for transforming the target variable `y` \n    before training.\ntransformer_exog : Optional[object], default `None`\n    Preprocessor compatible with scikit-learn for transforming exogenous variables before training.\nweight_func : Optional[Callable], default `None`\n    A function to define individual sample weights based on the index, utilized when the regressor supports it.\ndifferentiation : Optional[int], default `None`\n    Defines the order of differencing to apply to the time series. If specified, enhances the \n    window size accordingly.\nfit_kwargs : Optional[dict], default `None`\n    Additional arguments for the regressor's `fit` method.\nbinner_kwargs : Optional[dict], default `None`\n    Configurations for the `QuantileBinner` to discretize the residuals; defaults defined internally.\nforecaster_id : Optional[Union[str, int]], default `None`\n    An identifier for the forecaster instance.\n\nReturns\n-------\nNone\n    This constructor does not return a value. It sets up the internal state of the ForecasterRecursive \n    instance based on the provided parameters.\n\nAttributes\n----------\nSeveral attributes are initialized, including:\n    - `self.last_window_`\n    - `self.index_type_`\n    - `self.index_freq_`\n    - `self.training_range_`\n    - `self.is_fitted`\n    - among others related to the features and transformations.\n\nRaises\n------\nValueError\n    If both `lags` and `window_features` are `None`, a ValueError is raised, as predictors are required.\nTypeError\n    If the parameter types do not conform to expectations.\n\nDependencies\n------------\nThis method utilizes helper functions such as `initialize_lags`, `initialize_window_features`, \n`initialize_weights`, and `check_select_fit_kwargs`, which are necessary for preparing the \nforecaster's parameters and ensuring correctness.",
        "signature": "def __init__(self, regressor: object, lags: Optional[Union[int, list, np.ndarray, range]]=None, window_features: Optional[Union[object, list]]=None, transformer_y: Optional[object]=None, transformer_exog: Optional[object]=None, weight_func: Optional[Callable]=None, differentiation: Optional[int]=None, fit_kwargs: Optional[dict]=None, binner_kwargs: Optional[dict]=None, forecaster_id: Optional[Union[str, int]]=None) -> None:",
        "type": "Method",
        "class_signature": "class ForecasterRecursive(ForecasterBase):"
      }
    }
  },
  "dependency_dict": {
    "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:__init__": {
      "skforecast/utils/utils.py": {
        "initialize_lags": {
          "code": "def initialize_lags(\n    forecaster_name: str,\n    lags: Any\n) -> Union[Optional[np.ndarray], Optional[list], Optional[int]]:\n    \"\"\"\n    Check lags argument input and generate the corresponding numpy ndarray.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    lags : Any\n        Lags used as predictors.\n\n    Returns\n    -------\n    lags : numpy ndarray, None\n        Lags used as predictors.\n    lags_names : list, None\n        Names of the lags used as predictors.\n    max_lag : int, None\n        Maximum value of the lags.\n    \n    \"\"\"\n\n    lags_names = None\n    max_lag = None\n    if lags is not None:\n        if isinstance(lags, int):\n            if lags < 1:\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n            lags = np.arange(1, lags + 1)\n\n        if isinstance(lags, (list, tuple, range)):\n            lags = np.array(lags)\n        \n        if isinstance(lags, np.ndarray):\n            if lags.size == 0:\n                return None, None, None\n            if lags.ndim != 1:\n                raise ValueError(\"`lags` must be a 1-dimensional array.\")\n            if not np.issubdtype(lags.dtype, np.integer):\n                raise TypeError(\"All values in `lags` must be integers.\")\n            if np.any(lags < 1):\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n        else:\n            if forecaster_name != 'ForecasterDirectMultiVariate':\n                raise TypeError(\n                    (f\"`lags` argument must be an int, 1d numpy ndarray, range, \"\n                     f\"tuple or list. Got {type(lags)}.\")\n                )\n            else:\n                raise TypeError(\n                    (f\"`lags` argument must be a dict, int, 1d numpy ndarray, range, \"\n                     f\"tuple or list. Got {type(lags)}.\")\n                )\n        \n        lags_names = [f'lag_{i}' for i in lags]\n        max_lag = max(lags)\n\n    return lags, lags_names, max_lag",
          "docstring": "Check lags argument input and generate the corresponding numpy ndarray.\n\nParameters\n----------\nforecaster_name : str\n    Forecaster name.\nlags : Any\n    Lags used as predictors.\n\nReturns\n-------\nlags : numpy ndarray, None\n    Lags used as predictors.\nlags_names : list, None\n    Names of the lags used as predictors.\nmax_lag : int, None\n    Maximum value of the lags.",
          "signature": "def initialize_lags(forecaster_name: str, lags: Any) -> Union[Optional[np.ndarray], Optional[list], Optional[int]]:",
          "type": "Function",
          "class_signature": null
        },
        "initialize_window_features": {
          "code": "def initialize_window_features(\n    window_features: Any\n) -> Union[Optional[list], Optional[list], Optional[int]]:\n    \"\"\"\n    Check window_features argument input and generate the corresponding list.\n\n    Parameters\n    ----------\n    window_features : Any\n        Classes used to create window features.\n\n    Returns\n    -------\n    window_features : list, None\n        List of classes used to create window features.\n    window_features_names : list, None\n        List with all the features names of the window features.\n    max_size_window_features : int, None\n        Maximum value of the `window_sizes` attribute of all classes.\n    \n    \"\"\"\n\n    needed_atts = ['window_sizes', 'features_names']\n    needed_methods = ['transform_batch', 'transform']\n\n    max_window_sizes = None\n    window_features_names = None\n    max_size_window_features = None\n    if window_features is not None:\n        if isinstance(window_features, list) and len(window_features) < 1:\n            raise ValueError(\n                \"Argument `window_features` must contain at least one element.\"\n            )\n        if not isinstance(window_features, list):\n            window_features = [window_features]\n\n        link_to_docs = (\n            \"\\nVisit the documentation for more information about how to create \"\n            \"custom window features:\\n\"\n            \"https://skforecast.org/latest/user_guides/window-features-and-custom-features.html#create-your-custom-window-features\"\n        )\n        \n        max_window_sizes = []\n        window_features_names = []\n        for wf in window_features:\n            wf_name = type(wf).__name__\n            atts_methods = set([a for a in dir(wf)])\n            if not set(needed_atts).issubset(atts_methods):\n                raise ValueError(\n                    f\"{wf_name} must have the attributes: {needed_atts}.\" + link_to_docs\n                )\n            if not set(needed_methods).issubset(atts_methods):\n                raise ValueError(\n                    f\"{wf_name} must have the methods: {needed_methods}.\" + link_to_docs\n                )\n            \n            window_sizes = wf.window_sizes\n            if not isinstance(window_sizes, (int, list)):\n                raise TypeError(\n                    f\"Attribute `window_sizes` of {wf_name} must be an int or a list \"\n                    f\"of ints. Got {type(window_sizes)}.\" + link_to_docs\n                )\n            \n            if isinstance(window_sizes, int):\n                if window_sizes < 1:\n                    raise ValueError(\n                        f\"If argument `window_sizes` is an integer, it must be equal to or \"\n                        f\"greater than 1. Got {window_sizes} from {wf_name}.\" + link_to_docs\n                    )\n                max_window_sizes.append(window_sizes)\n            else:\n                if not all(isinstance(ws, int) for ws in window_sizes) or not all(\n                    ws >= 1 for ws in window_sizes\n                ):                    \n                    raise ValueError(\n                        f\"If argument `window_sizes` is a list, all elements must be integers \"\n                        f\"equal to or greater than 1. Got {window_sizes} from {wf_name}.\" + link_to_docs\n                    )\n                max_window_sizes.append(max(window_sizes))\n\n            features_names = wf.features_names\n            if not isinstance(features_names, (str, list)):\n                raise TypeError(\n                    f\"Attribute `features_names` of {wf_name} must be a str or \"\n                    f\"a list of strings. Got {type(features_names)}.\" + link_to_docs\n                )\n            if isinstance(features_names, str):\n                window_features_names.append(features_names)\n            else:\n                if not all(isinstance(fn, str) for fn in features_names):\n                    raise TypeError(\n                        f\"If argument `features_names` is a list, all elements \"\n                        f\"must be strings. Got {features_names} from {wf_name}.\" + link_to_docs\n                    )\n                window_features_names.extend(features_names)\n\n        max_size_window_features = max(max_window_sizes)\n        if len(set(window_features_names)) != len(window_features_names):\n            raise ValueError(\n                f\"All window features names must be unique. Got {window_features_names}.\"\n            )\n\n    return window_features, window_features_names, max_size_window_features",
          "docstring": "Check window_features argument input and generate the corresponding list.\n\nParameters\n----------\nwindow_features : Any\n    Classes used to create window features.\n\nReturns\n-------\nwindow_features : list, None\n    List of classes used to create window features.\nwindow_features_names : list, None\n    List with all the features names of the window features.\nmax_size_window_features : int, None\n    Maximum value of the `window_sizes` attribute of all classes.",
          "signature": "def initialize_window_features(window_features: Any) -> Union[Optional[list], Optional[list], Optional[int]]:",
          "type": "Function",
          "class_signature": null
        },
        "initialize_weights": {
          "code": "def initialize_weights(\n    forecaster_name: str,\n    regressor: object,\n    weight_func: Union[Callable, dict],\n    series_weights: dict\n) -> Tuple[Union[Callable, dict], Union[str, dict], dict]:\n    \"\"\"\n    Check weights arguments, `weight_func` and `series_weights` for the different \n    forecasters. Create `source_code_weight_func`, source code of the custom \n    function(s) used to create weights.\n    \n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        Regressor of the forecaster.\n    weight_func : Callable, dict\n        Argument `weight_func` of the forecaster.\n    series_weights : dict\n        Argument `series_weights` of the forecaster.\n\n    Returns\n    -------\n    weight_func : Callable, dict\n        Argument `weight_func` of the forecaster.\n    source_code_weight_func : str, dict\n        Argument `source_code_weight_func` of the forecaster.\n    series_weights : dict\n        Argument `series_weights` of the forecaster.\n    \n    \"\"\"\n\n    source_code_weight_func = None\n\n    if weight_func is not None:\n\n        if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n            if not isinstance(weight_func, (Callable, dict)):\n                raise TypeError(\n                    (f\"Argument `weight_func` must be a Callable or a dict of \"\n                     f\"Callables. Got {type(weight_func)}.\")\n                )\n        elif not isinstance(weight_func, Callable):\n            raise TypeError(\n                f\"Argument `weight_func` must be a Callable. Got {type(weight_func)}.\"\n            )\n        \n        if isinstance(weight_func, dict):\n            source_code_weight_func = {}\n            for key in weight_func:\n                source_code_weight_func[key] = inspect.getsource(weight_func[key])\n        else:\n            source_code_weight_func = inspect.getsource(weight_func)\n\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(\n                (f\"Argument `weight_func` is ignored since regressor {regressor} \"\n                 f\"does not accept `sample_weight` in its `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n            weight_func = None\n            source_code_weight_func = None\n\n    if series_weights is not None:\n        if not isinstance(series_weights, dict):\n            raise TypeError(\n                (f\"Argument `series_weights` must be a dict of floats or ints.\"\n                 f\"Got {type(series_weights)}.\")\n            )\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(\n                (f\"Argument `series_weights` is ignored since regressor {regressor} \"\n                 f\"does not accept `sample_weight` in its `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n            series_weights = None\n\n    return weight_func, source_code_weight_func, series_weights",
          "docstring": "Check weights arguments, `weight_func` and `series_weights` for the different \nforecasters. Create `source_code_weight_func`, source code of the custom \nfunction(s) used to create weights.\n\nParameters\n----------\nforecaster_name : str\n    Forecaster name.\nregressor : regressor or pipeline compatible with the scikit-learn API\n    Regressor of the forecaster.\nweight_func : Callable, dict\n    Argument `weight_func` of the forecaster.\nseries_weights : dict\n    Argument `series_weights` of the forecaster.\n\nReturns\n-------\nweight_func : Callable, dict\n    Argument `weight_func` of the forecaster.\nsource_code_weight_func : str, dict\n    Argument `source_code_weight_func` of the forecaster.\nseries_weights : dict\n    Argument `series_weights` of the forecaster.",
          "signature": "def initialize_weights(forecaster_name: str, regressor: object, weight_func: Union[Callable, dict], series_weights: dict) -> Tuple[Union[Callable, dict], Union[str, dict], dict]:",
          "type": "Function",
          "class_signature": null
        },
        "check_select_fit_kwargs": {
          "code": "def check_select_fit_kwargs(\n    regressor: object,\n    fit_kwargs: Optional[dict] = None\n) -> dict:\n    \"\"\"\n    Check if `fit_kwargs` is a dict and select only the keys that are used by\n    the `fit` method of the regressor.\n\n    Parameters\n    ----------\n    regressor : object\n        Regressor object.\n    fit_kwargs : dict, default `None`\n        Dictionary with the arguments to pass to the `fit' method of the forecaster.\n\n    Returns\n    -------\n    fit_kwargs : dict\n        Dictionary with the arguments to be passed to the `fit` method of the \n        regressor after removing the unused keys.\n    \n    \"\"\"\n\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    else:\n        if not isinstance(fit_kwargs, dict):\n            raise TypeError(\n                f\"Argument `fit_kwargs` must be a dict. Got {type(fit_kwargs)}.\"\n            )\n\n        # Non used keys\n        non_used_keys = [k for k in fit_kwargs.keys()\n                         if k not in inspect.signature(regressor.fit).parameters]\n        if non_used_keys:\n            warnings.warn(\n                (f\"Argument/s {non_used_keys} ignored since they are not used by the \"\n                 f\"regressor's `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n\n        if 'sample_weight' in fit_kwargs.keys():\n            warnings.warn(\n                (\"The `sample_weight` argument is ignored. Use `weight_func` to pass \"\n                 \"a function that defines the individual weights for each sample \"\n                 \"based on its index.\"),\n                 IgnoredArgumentWarning\n            )\n            del fit_kwargs['sample_weight']\n\n        # Select only the keyword arguments allowed by the regressor's `fit` method.\n        fit_kwargs = {k: v for k, v in fit_kwargs.items()\n                      if k in inspect.signature(regressor.fit).parameters}\n\n    return fit_kwargs",
          "docstring": "Check if `fit_kwargs` is a dict and select only the keys that are used by\nthe `fit` method of the regressor.\n\nParameters\n----------\nregressor : object\n    Regressor object.\nfit_kwargs : dict, default `None`\n    Dictionary with the arguments to pass to the `fit' method of the forecaster.\n\nReturns\n-------\nfit_kwargs : dict\n    Dictionary with the arguments to be passed to the `fit` method of the \n    regressor after removing the unused keys.",
          "signature": "def check_select_fit_kwargs(regressor: object, fit_kwargs: Optional[dict]=None) -> dict:",
          "type": "Function",
          "class_signature": null
        }
      },
      "skforecast/preprocessing/preprocessing.py": {
        "QuantileBinner.__init__": {
          "code": "    def __init__(\n        self,\n        n_bins: int,\n        method: Optional[str] = \"linear\",\n        subsample: int = 200000,\n        dtype: Optional[type] = np.float64,\n        random_state: Optional[int] = 789654\n    ):\n        \n        self._validate_params(\n            n_bins,\n            method,\n            subsample,\n            dtype,\n            random_state\n        )\n\n        self.n_bins       = n_bins\n        self.method       = method\n        self.subsample    = subsample\n        self.random_state = random_state\n        self.dtype        = dtype\n        self.n_bins_      = None\n        self.bin_edges_   = None\n        self.intervals_   = None",
          "docstring": "",
          "signature": "def __init__(self, n_bins: int, method: Optional[str]='linear', subsample: int=200000, dtype: Optional[type]=np.float64, random_state: Optional[int]=789654):",
          "type": "Method",
          "class_signature": "class QuantileBinner:"
        }
      }
    }
  },
  "PRD": "# PROJECT NAME: skforecast-test_initialize_lags_grid\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 skforecast/\n    \u251c\u2500\u2500 model_selection/\n    \u2502   \u2514\u2500\u2500 _utils.py\n    \u2502       \u2514\u2500\u2500 initialize_lags_grid\n    \u2514\u2500\u2500 recursive/\n        \u2514\u2500\u2500 _forecaster_recursive.py\n            \u2514\u2500\u2500 ForecasterRecursive.__init__\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThe module facilitates the initialization and validation of lag configurations for a forecasting system using a recursive forecaster with machine learning regressors. Its primary purpose is to accept various lag structures, including lists, dictionaries, ranges, and arrays, and standardize them into a consistent format for seamless integration within the forecasting workflow. The module provides functionality to validate input lag configurations, handle edge cases such as invalid types, and correctly interpret whether the lags are specified as keys or values. By ensuring flexibility with lag inputs and maintaining rigorous validation, it streamlines the setup process for developers, reduces the likelihood of configuration errors, and enhances the reliability of time series forecasting models.\n\n## FILE 1: skforecast/model_selection/_utils.py\n\n- FUNCTION NAME: initialize_lags_grid\n  - SIGNATURE: def initialize_lags_grid(forecaster: object, lags_grid: Optional[Union[list, dict]]=None) -> Tuple[dict, str]:\n  - DOCSTRING: \n```python\n\"\"\"\nInitialize the lags grid for model selection in forecasting.\n\nThis function prepares a configuration of lags to be evaluated during the model selection process. It takes a forecaster instance and an optional lags grid as inputs. If the lags grid is not provided, it uses the default lags from the forecaster.\n\nParameters\n----------\nforecaster : object\n    An instance of a forecaster model (e.g., ForecasterRecursive, ForecasterDirect, etc.) which dictates the lags available for selection.\nlags_grid : Optional[Union[list, dict]], default `None`\n    Either a list or a dictionary specifying the lags to try. If `dict`, keys represent labels for the results DataFrame, whereas values are the lists of lags.\n\nReturns\n-------\nTuple[dict, str]\n    - lags_grid : dict\n        A dictionary configured with the lags for each iteration, with keys as labels for results if dict input was provided.\n    - lags_label : str\n        A label indicating the representation of lags in the results (either 'values' or 'keys').\n\nRaises\n------\nTypeError\n    If `lags_grid` is not a list, dict, or None.\n\nThis function interacts with the `forecaster` parameter to extract default lags when `lags_grid` is None, ensuring that the lags used for model fitting are relevant to the specific forecasting model employed.\n\"\"\"\n```\n\n## FILE 2: skforecast/recursive/_forecaster_recursive.py\n\n- CLASS METHOD: ForecasterRecursive.__init__\n  - CLASS SIGNATURE: class ForecasterRecursive(ForecasterBase):\n  - SIGNATURE: def __init__(self, regressor: object, lags: Optional[Union[int, list, np.ndarray, range]]=None, window_features: Optional[Union[object, list]]=None, transformer_y: Optional[object]=None, transformer_exog: Optional[object]=None, weight_func: Optional[Callable]=None, differentiation: Optional[int]=None, fit_kwargs: Optional[dict]=None, binner_kwargs: Optional[dict]=None, forecaster_id: Optional[Union[str, int]]=None) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nInitializes a ForecasterRecursive object that enables multi-step time series forecasting using \na given regressor compatible with the scikit-learn API. This constructor prepares the essential \nparameters and attributes required for training and prediction, including the setup for lagged values, \nwindow features, data transformers, residuals handling, and additional fitting parameters.\n\nParameters\n----------\nregressor : object\n    A regressor or pipeline compatible with the scikit-learn API, used for making predictions.\nlags : Optional[Union[int, list, np.ndarray, range]], default `None`\n    Defines the lagged values used as predictors; can be specified as an integer, list, \n    numpy array, or range.\nwindow_features : Optional[Union[object, list]], default `None`\n    Instances (or list of instances) used for creating additional window features from the time series.\ntransformer_y : Optional[object], default `None`\n    Preprocessor compatible with scikit-learn for transforming the target variable `y` \n    before training.\ntransformer_exog : Optional[object], default `None`\n    Preprocessor compatible with scikit-learn for transforming exogenous variables before training.\nweight_func : Optional[Callable], default `None`\n    A function to define individual sample weights based on the index, utilized when the regressor supports it.\ndifferentiation : Optional[int], default `None`\n    Defines the order of differencing to apply to the time series. If specified, enhances the \n    window size accordingly.\nfit_kwargs : Optional[dict], default `None`\n    Additional arguments for the regressor's `fit` method.\nbinner_kwargs : Optional[dict], default `None`\n    Configurations for the `QuantileBinner` to discretize the residuals; defaults defined internally.\nforecaster_id : Optional[Union[str, int]], default `None`\n    An identifier for the forecaster instance.\n\nReturns\n-------\nNone\n    This constructor does not return a value. It sets up the internal state of the ForecasterRecursive \n    instance based on the provided parameters.\n\nAttributes\n----------\nSeveral attributes are initialized, including:\n    - `self.last_window_`\n    - `self.index_type_`\n    - `self.index_freq_`\n    - `self.training_range_`\n    - `self.is_fitted`\n    - among others related to the features and transformations.\n\nRaises\n------\nValueError\n    If both `lags` and `window_features` are `None`, a ValueError is raised, as predictors are required.\nTypeError\n    If the parameter types do not conform to expectations.\n\nDependencies\n------------\nThis method utilizes helper functions such as `initialize_lags`, `initialize_window_features`, \n`initialize_weights`, and `check_select_fit_kwargs`, which are necessary for preparing the \nforecaster's parameters and ensuring correctness.\n\"\"\"\n```\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "skforecast/model_selection/_utils.py": "from typing import Union, Tuple, Optional, Callable, Generator\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom joblib import cpu_count\nfrom tqdm.auto import tqdm\nfrom sklearn.pipeline import Pipeline\nimport sklearn.linear_model\nfrom sklearn.exceptions import NotFittedError\nfrom ..exceptions import IgnoredArgumentWarning\nfrom ..metrics import add_y_train_argument, _get_metric\nfrom ..utils import check_interval\n\ndef check_backtesting_input(forecaster: object, cv: object, metric: Union[str, Callable, list], add_aggregated_metric: bool=True, y: Optional[pd.Series]=None, series: Optional[Union[pd.DataFrame, dict]]=None, exog: Optional[Union[pd.Series, pd.DataFrame, dict]]=None, interval: Optional[list]=None, alpha: Optional[float]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False, n_jobs: Union[int, str]='auto', show_progress: bool=True, suppress_warnings: bool=False, suppress_warnings_fit: bool=False) -> None:\n    \"\"\"\n    This is a helper function to check most inputs of backtesting functions in \n    modules `model_selection`.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model.\n    cv : TimeSeriesFold\n        TimeSeriesFold object with the information needed to split the data into folds.\n    metric : str, Callable, list\n        Metric used to quantify the goodness of fit of the model.\n    add_aggregated_metric : bool, default `True`\n        If `True`, the aggregated metrics (average, weighted average and pooling)\n        over all levels are also returned (only multiseries).\n    y : pandas Series, default `None`\n        Training time series for uni-series forecasters.\n    series : pandas DataFrame, dict, default `None`\n        Training time series for multi-series forecasters.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    interval : list, default `None`\n        Confidence of the prediction interval estimated. Sequence of percentiles\n        to compute, which must be between 0 and 100 inclusive.\n    alpha : float, default `None`\n        The confidence intervals used in ForecasterSarimax are (1 - alpha) %. \n    n_boot : int, default `250`\n        Number of bootstrapping iterations used to estimate prediction\n        intervals.\n    random_state : int, default `123`\n        Sets a seed to the random generator, so that boot intervals are always \n        deterministic.\n    use_in_sample_residuals : bool, default `True`\n        If `True`, residuals from the training data are used as proxy of prediction \n        error to create prediction intervals.  If `False`, out_sample_residuals \n        are used if they are already stored inside the forecaster.\n    use_binned_residuals : bool, default `False`\n        If `True`, residuals used in each bootstrapping iteration are selected\n        conditioning on the predicted values. If `False`, residuals are selected\n        randomly without conditioning on the predicted values.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the fuction\n        skforecast.utils.select_n_jobs_fit_forecaster.\n        **New in version 0.9.0**\n    show_progress : bool, default `True`\n        Whether to show a progress bar.\n    suppress_warnings: bool, default `False`\n        If `True`, skforecast warnings will be suppressed during the backtesting \n        process. See skforecast.exceptions.warn_skforecast_categories for more\n        information.\n    suppress_warnings_fit : bool, default `False`\n        If `True`, warnings generated during fitting will be ignored. Only \n        `ForecasterSarimax`.\n\n    Returns\n    -------\n    None\n    \n    \"\"\"\n    forecaster_name = type(forecaster).__name__\n    cv_name = type(cv).__name__\n    if cv_name != 'TimeSeriesFold':\n        raise TypeError(f'`cv` must be a TimeSeriesFold object. Got {cv_name}.')\n    steps = cv.steps\n    initial_train_size = cv.initial_train_size\n    gap = cv.gap\n    allow_incomplete_fold = cv.allow_incomplete_fold\n    refit = cv.refit\n    forecasters_uni = ['ForecasterRecursive', 'ForecasterDirect', 'ForecasterSarimax', 'ForecasterEquivalentDate']\n    forecasters_multi = ['ForecasterDirectMultiVariate', 'ForecasterRnn']\n    forecasters_multi_dict = ['ForecasterRecursiveMultiSeries']\n    if forecaster_name in forecasters_uni:\n        if not isinstance(y, pd.Series):\n            raise TypeError('`y` must be a pandas Series.')\n        data_name = 'y'\n        data_length = len(y)\n    elif forecaster_name in forecasters_multi:\n        if not isinstance(series, pd.DataFrame):\n            raise TypeError('`series` must be a pandas DataFrame.')\n        data_name = 'series'\n        data_length = len(series)\n    elif forecaster_name in forecasters_multi_dict:\n        if not isinstance(series, (pd.DataFrame, dict)):\n            raise TypeError(f'`series` must be a pandas DataFrame or a dict of DataFrames or Series. Got {type(series)}.')\n        data_name = 'series'\n        if isinstance(series, dict):\n            not_valid_series = [k for k, v in series.items() if not isinstance(v, (pd.Series, pd.DataFrame))]\n            if not_valid_series:\n                raise TypeError(f'If `series` is a dictionary, all series must be a named pandas Series or a pandas DataFrame with a single column. Review series: {not_valid_series}')\n            not_valid_index = [k for k, v in series.items() if not isinstance(v.index, pd.DatetimeIndex)]\n            if not_valid_index:\n                raise ValueError(f'If `series` is a dictionary, all series must have a Pandas DatetimeIndex as index with the same frequency. Review series: {not_valid_index}')\n            indexes_freq = [f'{v.index.freq}' for v in series.values()]\n            indexes_freq = sorted(set(indexes_freq))\n            if not len(indexes_freq) == 1:\n                raise ValueError(f'If `series` is a dictionary, all series must have a Pandas DatetimeIndex as index with the same frequency. Found frequencies: {indexes_freq}')\n            data_length = max([len(series[serie]) for serie in series])\n        else:\n            data_length = len(series)\n    if exog is not None:\n        if forecaster_name in forecasters_multi_dict:\n            if not isinstance(exog, (pd.Series, pd.DataFrame, dict)):\n                raise TypeError(f'`exog` must be a pandas Series, DataFrame, dictionary of pandas Series/DataFrames or None. Got {type(exog)}.')\n            if isinstance(exog, dict):\n                not_valid_exog = [k for k, v in exog.items() if not isinstance(v, (pd.Series, pd.DataFrame, type(None)))]\n                if not_valid_exog:\n                    raise TypeError(f'If `exog` is a dictionary, All exog must be a named pandas Series, a pandas DataFrame or None. Review exog: {not_valid_exog}')\n        elif not isinstance(exog, (pd.Series, pd.DataFrame)):\n            raise TypeError(f'`exog` must be a pandas Series, DataFrame or None. Got {type(exog)}.')\n    if hasattr(forecaster, 'differentiation'):\n        if forecaster.differentiation != cv.differentiation:\n            raise ValueError(f'The differentiation included in the forecaster ({forecaster.differentiation}) differs from the differentiation included in the cv ({cv.differentiation}). Set the same value for both using the `differentiation` argument.')\n    if not isinstance(metric, (str, Callable, list)):\n        raise TypeError(f'`metric` must be a string, a callable function, or a list containing multiple strings and/or callables. Got {type(metric)}.')\n    if forecaster_name == 'ForecasterEquivalentDate' and isinstance(forecaster.offset, pd.tseries.offsets.DateOffset):\n        if initial_train_size is None:\n            raise ValueError(f'`initial_train_size` must be an integer greater than the `window_size` of the forecaster ({forecaster.window_size}) and smaller than the length of `{data_name}` ({data_length}).')\n    elif initial_train_size is not None:\n        if initial_train_size < forecaster.window_size or initial_train_size >= data_length:\n            raise ValueError(f'If used, `initial_train_size` must be an integer greater than the `window_size` of the forecaster ({forecaster.window_size}) and smaller than the length of `{data_name}` ({data_length}).')\n        if initial_train_size + gap >= data_length:\n            raise ValueError(f'The combination of initial_train_size {initial_train_size} and gap {gap} cannot be greater than the length of `{data_name}` ({data_length}).')\n    elif forecaster_name in ['ForecasterSarimax', 'ForecasterEquivalentDate']:\n        raise ValueError(f'`initial_train_size` must be an integer smaller than the length of `{data_name}` ({data_length}).')\n    else:\n        if not forecaster.is_fitted:\n            raise NotFittedError('`forecaster` must be already trained if no `initial_train_size` is provided.')\n        if refit:\n            raise ValueError('`refit` is only allowed when `initial_train_size` is not `None`.')\n    if forecaster_name == 'ForecasterSarimax' and cv.skip_folds is not None:\n        raise ValueError('`skip_folds` is not allowed for ForecasterSarimax. Set it to `None`.')\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError('`add_aggregated_metric` must be a boolean: `True`, `False`.')\n    if not isinstance(n_boot, (int, np.integer)) or n_boot < 0:\n        raise TypeError(f'`n_boot` must be an integer greater than 0. Got {n_boot}.')\n    if not isinstance(random_state, (int, np.integer)) or random_state < 0:\n        raise TypeError(f'`random_state` must be an integer greater than 0. Got {random_state}.')\n    if not isinstance(use_in_sample_residuals, bool):\n        raise TypeError('`use_in_sample_residuals` must be a boolean: `True`, `False`.')\n    if not isinstance(use_binned_residuals, bool):\n        raise TypeError('`use_binned_residuals` must be a boolean: `True`, `False`.')\n    if not isinstance(n_jobs, int) and n_jobs != 'auto':\n        raise TypeError(f\"`n_jobs` must be an integer or `'auto'`. Got {n_jobs}.\")\n    if not isinstance(show_progress, bool):\n        raise TypeError('`show_progress` must be a boolean: `True`, `False`.')\n    if not isinstance(suppress_warnings, bool):\n        raise TypeError('`suppress_warnings` must be a boolean: `True`, `False`.')\n    if not isinstance(suppress_warnings_fit, bool):\n        raise TypeError('`suppress_warnings_fit` must be a boolean: `True`, `False`.')\n    if interval is not None or alpha is not None:\n        check_interval(interval=interval, alpha=alpha)\n    if not allow_incomplete_fold and data_length - (initial_train_size + gap) < steps:\n        raise ValueError(f'There is not enough data to evaluate {steps} steps in a single fold. Set `allow_incomplete_fold` to `True` to allow incomplete folds.\\n    Data available for test : {data_length - (initial_train_size + gap)}\\n    Steps                   : {steps}')\n\ndef select_n_jobs_backtesting(forecaster: object, refit: Union[bool, int]) -> int:\n    \"\"\"\n    Select the optimal number of jobs to use in the backtesting process. This\n    selection is based on heuristics and is not guaranteed to be optimal.\n\n    The number of jobs is chosen as follows:\n\n    - If `refit` is an integer, then `n_jobs = 1`. This is because parallelization doesn't \n    work with intermittent refit.\n    - If forecaster is 'ForecasterRecursive' and regressor is a linear regressor, \n    then `n_jobs = 1`.\n    - If forecaster is 'ForecasterRecursive' and regressor is not a linear \n    regressor then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and `refit = True`, then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and `refit = False`, then `n_jobs = 1`.\n    - If forecaster is 'ForecasterRecursiveMultiSeries', then `n_jobs = cpu_count() - 1`.\n    - If forecaster is 'ForecasterSarimax' or 'ForecasterEquivalentDate', \n    then `n_jobs = 1`.\n    - If regressor is a `LGBMRegressor(n_jobs=1)`, then `n_jobs = cpu_count() - 1`.\n    - If regressor is a `LGBMRegressor` with internal n_jobs != 1, then `n_jobs = 1`.\n    This is because `lightgbm` is highly optimized for gradient boosting and\n    parallelizes operations at a very fine-grained level, making additional\n    parallelization unnecessary and potentially harmful due to resource contention.\n\n    Parameters\n    ----------\n    forecaster : Forecaster\n        Forecaster model.\n    refit : bool, int\n        If the forecaster is refitted during the backtesting process.\n\n    Returns\n    -------\n    n_jobs : int\n        The number of jobs to run in parallel.\n    \n    \"\"\"\n    forecaster_name = type(forecaster).__name__\n    if isinstance(forecaster.regressor, Pipeline):\n        regressor = forecaster.regressor[-1]\n        regressor_name = type(regressor).__name__\n    else:\n        regressor = forecaster.regressor\n        regressor_name = type(regressor).__name__\n    linear_regressors = [regressor_name for regressor_name in dir(sklearn.linear_model) if not regressor_name.startswith('_')]\n    refit = False if refit == 0 else refit\n    if not isinstance(refit, bool) and refit != 1:\n        n_jobs = 1\n    elif forecaster_name in ['ForecasterRecursive']:\n        if regressor_name in linear_regressors:\n            n_jobs = 1\n        elif regressor_name == 'LGBMRegressor':\n            n_jobs = cpu_count() - 1 if regressor.n_jobs == 1 else 1\n        else:\n            n_jobs = cpu_count() - 1\n    elif forecaster_name in ['ForecasterDirect', 'ForecasterDirectMultiVariate']:\n        n_jobs = 1\n    elif forecaster_name in ['ForecasterRecursiveMultiSeries']:\n        if regressor_name == 'LGBMRegressor':\n            n_jobs = cpu_count() - 1 if regressor.n_jobs == 1 else 1\n        else:\n            n_jobs = cpu_count() - 1\n    elif forecaster_name in ['ForecasterSarimax', 'ForecasterEquivalentDate']:\n        n_jobs = 1\n    else:\n        n_jobs = 1\n    return n_jobs\n\ndef _calculate_metrics_one_step_ahead(forecaster: object, y: pd.Series, metrics: list, X_train: pd.DataFrame, y_train: Union[pd.Series, dict], X_test: pd.DataFrame, y_test: Union[pd.Series, dict]) -> list:\n    \"\"\"\n    Calculate metrics when predictions are one-step-ahead. When forecaster is\n    of type ForecasterDirect only the regressor for step 1 is used.\n\n    Parameters\n    ----------\n    forecaster : object\n        Forecaster model.\n    y : pandas Series\n        Time series data used to train and test the model.\n    metrics : list\n        List of metrics.\n    X_train : pandas DataFrame\n        Predictor values used to train the model.\n    y_train : pandas Series\n        Target values related to each row of `X_train`.\n    X_test : pandas DataFrame\n        Predictor values used to test the model.\n    y_test : pandas Series\n        Target values related to each row of `X_test`.\n\n    Returns\n    -------\n    metric_values : list\n        List with metric values.\n    \n    \"\"\"\n    if type(forecaster).__name__ == 'ForecasterDirect':\n        step = 1\n        X_train, y_train = forecaster.filter_train_X_y_for_step(step=step, X_train=X_train, y_train=y_train)\n        X_test, y_test = forecaster.filter_train_X_y_for_step(step=step, X_train=X_test, y_train=y_test)\n        forecaster.regressors_[step].fit(X_train, y_train)\n        y_pred = forecaster.regressors_[step].predict(X_test)\n    else:\n        forecaster.regressor.fit(X_train, y_train)\n        y_pred = forecaster.regressor.predict(X_test)\n    y_true = y_test.to_numpy()\n    y_pred = y_pred.ravel()\n    y_train = y_train.to_numpy()\n    if forecaster.differentiation is not None:\n        y_true = forecaster.differentiator.inverse_transform_next_window(y_true)\n        y_pred = forecaster.differentiator.inverse_transform_next_window(y_pred)\n        y_train = forecaster.differentiator.inverse_transform_training(y_train)\n    if forecaster.transformer_y is not None:\n        y_true = forecaster.transformer_y.inverse_transform(y_true.reshape(-1, 1))\n        y_pred = forecaster.transformer_y.inverse_transform(y_pred.reshape(-1, 1))\n        y_train = forecaster.transformer_y.inverse_transform(y_train.reshape(-1, 1))\n    metric_values = []\n    for m in metrics:\n        metric_values.append(m(y_true=y_true.ravel(), y_pred=y_pred.ravel(), y_train=y_train.ravel()))\n    return metric_values\n\ndef _initialize_levels_model_selection_multiseries(forecaster: object, series: Union[pd.DataFrame, dict], levels: Optional[Union[str, list]]=None) -> list:\n    \"\"\"\n    Initialize levels for model_selection multi-series functions.\n\n    Parameters\n    ----------\n    forecaster : ForecasterRecursiveMultiSeries, ForecasterDirectMultiVariate, ForecasterRnn\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Training time series.\n    levels : str, list, default `None`\n        level (`str`) or levels (`list`) at which the forecaster is optimized. \n        If `None`, all levels are taken into account. The resulting metric will be\n        the average of the optimization of all levels.\n\n    Returns\n    -------\n    levels : list\n        List of levels to be used in model_selection multi-series functions.\n    \n    \"\"\"\n    multi_series_forecasters_with_levels = ['ForecasterRecursiveMultiSeries', 'ForecasterRnn']\n    if type(forecaster).__name__ in multi_series_forecasters_with_levels and (not isinstance(levels, (str, list, type(None)))):\n        raise TypeError(f'`levels` must be a `list` of column names, a `str` of a column name or `None` when using a forecaster of type {multi_series_forecasters_with_levels}. If the forecaster is of type `ForecasterDirectMultiVariate`, this argument is ignored.')\n    if type(forecaster).__name__ == 'ForecasterDirectMultiVariate':\n        if levels and levels != forecaster.level and (levels != [forecaster.level]):\n            warnings.warn(f\"`levels` argument have no use when the forecaster is of type `ForecasterDirectMultiVariate`. The level of this forecaster is '{forecaster.level}', to predict another level, change the `level` argument when initializing the forecaster. \\n\", IgnoredArgumentWarning)\n        levels = [forecaster.level]\n    elif levels is None:\n        if isinstance(series, pd.DataFrame):\n            levels = list(series.columns)\n        else:\n            levels = list(series.keys())\n    elif isinstance(levels, str):\n        levels = [levels]\n    return levels\n\ndef _extract_data_folds_multiseries(series: Union[pd.Series, pd.DataFrame, dict], folds: list, span_index: Union[pd.DatetimeIndex, pd.RangeIndex], window_size: int, exog: Optional[Union[pd.Series, pd.DataFrame, dict]]=None, dropna_last_window: bool=False, externally_fitted: bool=False) -> Generator[Tuple[Union[pd.Series, pd.DataFrame, dict], pd.DataFrame, list, Optional[Union[pd.Series, pd.DataFrame, dict]], Optional[Union[pd.Series, pd.DataFrame, dict]], list], None, None]:\n    \"\"\"\n    Select the data from series and exog that corresponds to each fold created using the\n    skforecast.model_selection._create_backtesting_folds function.\n\n    Parameters\n    ----------\n    series : pandas Series, pandas DataFrame, dict\n        Time series.\n    folds : list\n        Folds created using the skforecast.model_selection._create_backtesting_folds\n        function.\n    span_index : pandas DatetimeIndex, pandas RangeIndex\n        Full index from the minimum to the maximum index among all series.\n    window_size : int\n        Size of the window needed to create the predictors.\n    exog : pandas Series, pandas DataFrame, dict, default `None`\n        Exogenous variables.\n    dropna_last_window : bool, default `False`\n        If `True`, drop the columns of the last window that have NaN values.\n    externally_fitted : bool, default `False`\n        Flag indicating whether the forecaster is already trained. Only used when \n        `initial_train_size` is None and `refit` is False.\n\n    Yield\n    -----\n    series_train : pandas Series, pandas DataFrame, dict\n        Time series corresponding to the training set of the fold.\n    series_last_window: pandas DataFrame\n        Time series corresponding to the last window of the fold.\n    levels_last_window: list\n        Levels of the time series present in the last window of the fold.\n    exog_train: pandas Series, pandas DataFrame, dict, None\n        Exogenous variable corresponding to the training set of the fold.\n    exog_test: pandas Series, pandas DataFrame, dict, None\n        Exogenous variable corresponding to the test set of the fold.\n    fold: list\n        Fold created using the skforecast.model_selection._create_backtesting_folds\n\n    \"\"\"\n    for fold in folds:\n        train_iloc_start = fold[0][0]\n        train_iloc_end = fold[0][1]\n        last_window_iloc_start = fold[1][0]\n        last_window_iloc_end = fold[1][1]\n        test_iloc_start = fold[2][0]\n        test_iloc_end = fold[2][1]\n        if isinstance(series, dict) or isinstance(exog, dict):\n            train_loc_start = span_index[train_iloc_start]\n            train_loc_end = span_index[train_iloc_end - 1]\n            last_window_loc_start = span_index[last_window_iloc_start]\n            last_window_loc_end = span_index[last_window_iloc_end - 1]\n            test_loc_start = span_index[test_iloc_start]\n            test_loc_end = span_index[test_iloc_end - 1]\n        if isinstance(series, pd.DataFrame):\n            series_train = series.iloc[train_iloc_start:train_iloc_end,]\n            series_to_drop = []\n            for col in series_train.columns:\n                if series_train[col].isna().all():\n                    series_to_drop.append(col)\n                else:\n                    first_valid_index = series_train[col].first_valid_index()\n                    last_valid_index = series_train[col].last_valid_index()\n                    if len(series_train[col].loc[first_valid_index:last_valid_index]) < window_size:\n                        series_to_drop.append(col)\n            series_last_window = series.iloc[last_window_iloc_start:last_window_iloc_end,]\n            series_train = series_train.drop(columns=series_to_drop)\n            if not externally_fitted:\n                series_last_window = series_last_window.drop(columns=series_to_drop)\n        else:\n            series_train = {}\n            for k in series.keys():\n                v = series[k].loc[train_loc_start:train_loc_end]\n                if not v.isna().all():\n                    first_valid_index = v.first_valid_index()\n                    last_valid_index = v.last_valid_index()\n                    if first_valid_index is not None and last_valid_index is not None:\n                        v = v.loc[first_valid_index:last_valid_index]\n                        if len(v) >= window_size:\n                            series_train[k] = v\n            series_last_window = {}\n            for k, v in series.items():\n                v = series[k].loc[last_window_loc_start:last_window_loc_end]\n                if (externally_fitted or k in series_train) and len(v) >= window_size:\n                    series_last_window[k] = v\n            series_last_window = pd.DataFrame(series_last_window)\n        if dropna_last_window:\n            series_last_window = series_last_window.dropna(axis=1, how='any')\n        levels_last_window = list(series_last_window.columns)\n        if exog is not None:\n            if isinstance(exog, (pd.Series, pd.DataFrame)):\n                exog_train = exog.iloc[train_iloc_start:train_iloc_end,]\n                exog_test = exog.iloc[test_iloc_start:test_iloc_end,]\n            else:\n                exog_train = {k: v.loc[train_loc_start:train_loc_end] for k, v in exog.items()}\n                exog_train = {k: v for k, v in exog_train.items() if len(v) > 0}\n                exog_test = {k: v.loc[test_loc_start:test_loc_end] for k, v in exog.items() if externally_fitted or k in exog_train}\n                exog_test = {k: v for k, v in exog_test.items() if len(v) > 0}\n        else:\n            exog_train = None\n            exog_test = None\n        yield (series_train, series_last_window, levels_last_window, exog_train, exog_test, fold)\n\ndef _calculate_metrics_backtesting_multiseries(series: Union[pd.DataFrame, dict], predictions: pd.DataFrame, folds: Union[list, tqdm], span_index: Union[pd.DatetimeIndex, pd.RangeIndex], window_size: int, metrics: list, levels: list, add_aggregated_metric: bool=True) -> pd.DataFrame:\n    \"\"\"   \n    Calculate metrics for each level and also for all levels aggregated using\n    average, weighted average or pooling.\n\n    - 'average': the average (arithmetic mean) of all levels.\n    - 'weighted_average': the average of the metrics weighted by the number of\n    predicted values of each level.\n    - 'pooling': the values of all levels are pooled and then the metric is\n    calculated.\n\n    Parameters\n    ----------\n    series : pandas DataFrame, dict\n        Series data used for backtesting.\n    predictions : pandas DataFrame\n        Predictions generated during the backtesting process.\n    folds : list, tqdm\n        Folds created during the backtesting process.\n    span_index : pandas DatetimeIndex, pandas RangeIndex\n        Full index from the minimum to the maximum index among all series.\n    window_size : int\n        Size of the window used by the forecaster to create the predictors.\n        This is used remove the first `window_size` (differentiation included) \n        values from y_train since they are not part of the training matrix.\n    metrics : list\n        List of metrics to calculate.\n    levels : list\n        Levels to calculate the metrics.\n    add_aggregated_metric : bool, default `True`\n        If `True`, and multiple series (`levels`) are predicted, the aggregated\n        metrics (average, weighted average and pooled) are also returned.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s).\n    \n    \"\"\"\n    if not isinstance(series, (pd.DataFrame, dict)):\n        raise TypeError('`series` must be a pandas DataFrame or a dictionary of pandas DataFrames.')\n    if not isinstance(predictions, pd.DataFrame):\n        raise TypeError('`predictions` must be a pandas DataFrame.')\n    if not isinstance(folds, (list, tqdm)):\n        raise TypeError('`folds` must be a list or a tqdm object.')\n    if not isinstance(span_index, (pd.DatetimeIndex, pd.RangeIndex)):\n        raise TypeError('`span_index` must be a pandas DatetimeIndex or pandas RangeIndex.')\n    if not isinstance(window_size, (int, np.integer)):\n        raise TypeError('`window_size` must be an integer.')\n    if not isinstance(metrics, list):\n        raise TypeError('`metrics` must be a list.')\n    if not isinstance(levels, list):\n        raise TypeError('`levels` must be a list.')\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError('`add_aggregated_metric` must be a boolean.')\n    metric_names = [m if isinstance(m, str) else m.__name__ for m in metrics]\n    y_true_pred_levels = []\n    y_train_levels = []\n    for level in levels:\n        y_true_pred_level = None\n        y_train = None\n        if level in predictions.columns:\n            y_true_pred_level = pd.merge(series[level], predictions[level], left_index=True, right_index=True, how='inner').dropna(axis=0, how='any')\n            y_true_pred_level.columns = ['y_true', 'y_pred']\n            train_indexes = []\n            for i, fold in enumerate(folds):\n                fit_fold = fold[-1]\n                if i == 0 or fit_fold:\n                    train_iloc_start = fold[0][0]\n                    train_iloc_end = fold[0][1]\n                    train_indexes.append(np.arange(train_iloc_start, train_iloc_end))\n            train_indexes = np.unique(np.concatenate(train_indexes))\n            train_indexes = span_index[train_indexes]\n            y_train = series[level].loc[series[level].index.intersection(train_indexes)]\n        y_true_pred_levels.append(y_true_pred_level)\n        y_train_levels.append(y_train)\n    metrics_levels = []\n    for i, level in enumerate(levels):\n        if y_true_pred_levels[i] is not None and (not y_true_pred_levels[i].empty):\n            metrics_level = [m(y_true=y_true_pred_levels[i].iloc[:, 0], y_pred=y_true_pred_levels[i].iloc[:, 1], y_train=y_train_levels[i].iloc[window_size:]) for m in metrics]\n            metrics_levels.append(metrics_level)\n        else:\n            metrics_levels.append([None for _ in metrics])\n    metrics_levels = pd.DataFrame(data=metrics_levels, columns=[m if isinstance(m, str) else m.__name__ for m in metrics])\n    metrics_levels.insert(0, 'levels', levels)\n    if len(levels) < 2:\n        add_aggregated_metric = False\n    if add_aggregated_metric:\n        average = metrics_levels.drop(columns='levels').mean(skipna=True)\n        average = average.to_frame().transpose()\n        average['levels'] = 'average'\n        weighted_averages = {}\n        n_predictions_levels = predictions.notna().sum().to_frame(name='n_predictions').reset_index(names='levels')\n        metrics_levels_no_missing = metrics_levels.merge(n_predictions_levels, on='levels', how='inner')\n        for col in metric_names:\n            weighted_averages[col] = np.average(metrics_levels_no_missing[col], weights=metrics_levels_no_missing['n_predictions'])\n        weighted_average = pd.DataFrame(weighted_averages, index=[0])\n        weighted_average['levels'] = 'weighted_average'\n        y_true_pred_levels, y_train_levels = zip(*[(a, b.iloc[window_size:]) for a, b in zip(y_true_pred_levels, y_train_levels) if a is not None])\n        y_train_levels = list(y_train_levels)\n        y_true_pred_levels = pd.concat(y_true_pred_levels)\n        y_train_levels_concat = pd.concat(y_train_levels)\n        pooled = []\n        for m, m_name in zip(metrics, metric_names):\n            if m_name in ['mean_absolute_scaled_error', 'root_mean_squared_scaled_error']:\n                pooled.append(m(y_true=y_true_pred_levels.loc[:, 'y_true'], y_pred=y_true_pred_levels.loc[:, 'y_pred'], y_train=y_train_levels))\n            else:\n                pooled.append(m(y_true=y_true_pred_levels.loc[:, 'y_true'], y_pred=y_true_pred_levels.loc[:, 'y_pred'], y_train=y_train_levels_concat))\n        pooled = pd.DataFrame([pooled], columns=metric_names)\n        pooled['levels'] = 'pooling'\n        metrics_levels = pd.concat([metrics_levels, average, weighted_average, pooled], axis=0, ignore_index=True)\n    return metrics_levels\n\ndef _predict_and_calculate_metrics_one_step_ahead_multiseries(forecaster: object, series: Union[pd.DataFrame, dict], X_train: pd.DataFrame, y_train: Union[pd.Series, dict], X_test: pd.DataFrame, y_test: Union[pd.Series, dict], X_train_encoding: pd.Series, X_test_encoding: pd.Series, levels: list, metrics: list, add_aggregated_metric: bool=True) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"   \n    One-step-ahead predictions and metrics for each level and also for all levels\n    aggregated using average, weighted average or pooling.\n    Input matrices (X_train, y_train, X_train_encoding, X_test, y_test, X_test_encoding)\n    should have been generated using the forecaster._train_test_split_one_step_ahead().\n\n    - 'average': the average (arithmetic mean) of all levels.\n    - 'weighted_average': the average of the metrics weighted by the number of\n    predicted values of each level.\n    - 'pooling': the values of all levels are pooled and then the metric is\n    calculated.\n\n    Parameters\n    ----------\n    forecaster : object\n        Forecaster model.\n    series : pandas DataFrame, dict\n        Series data used to train and test the forecaster.\n    X_train : pandas DataFrame\n        Training matrix.\n    y_train : pandas Series, dict\n        Target values of the training set.\n    X_test : pandas DataFrame\n        Test matrix.\n    y_test : pandas Series, dict\n        Target values of the test set.\n    X_train_encoding : pandas Series\n        Series identifiers for each row of `X_train`.\n    X_test_encoding : pandas Series\n        Series identifiers for each row of `X_test`.\n    levels : list\n        Levels to calculate the metrics.\n    metrics : list\n        List of metrics to calculate.\n    add_aggregated_metric : bool, default `True`\n        If `True`, and multiple series (`levels`) are predicted, the aggregated\n        metrics (average, weighted average and pooled) are also returned.\n\n        - 'average': the average (arithmetic mean) of all levels.\n        - 'weighted_average': the average of the metrics weighted by the number of\n        predicted values of each level.\n        - 'pooling': the values of all levels are pooled and then the metric is\n        calculated.\n\n    Returns\n    -------\n    metrics_levels : pandas DataFrame\n        Value(s) of the metric(s).\n    predictions : pandas DataFrame\n        Value of predictions for each level.\n    \n    \"\"\"\n    if not isinstance(series, (pd.DataFrame, dict)):\n        raise TypeError('`series` must be a pandas DataFrame or a dictionary of pandas DataFrames.')\n    if not isinstance(X_train, pd.DataFrame):\n        raise TypeError(f'`X_train` must be a pandas DataFrame. Got: {type(X_train)}')\n    if not isinstance(y_train, (pd.Series, dict)):\n        raise TypeError(f'`y_train` must be a pandas Series or a dictionary of pandas Series. Got: {type(y_train)}')\n    if not isinstance(X_test, pd.DataFrame):\n        raise TypeError(f'`X_test` must be a pandas DataFrame. Got: {type(X_test)}')\n    if not isinstance(y_test, (pd.Series, dict)):\n        raise TypeError(f'`y_test` must be a pandas Series or a dictionary of pandas Series. Got: {type(y_test)}')\n    if not isinstance(X_train_encoding, pd.Series):\n        raise TypeError(f'`X_train_encoding` must be a pandas Series. Got: {type(X_train_encoding)}')\n    if not isinstance(X_test_encoding, pd.Series):\n        raise TypeError(f'`X_test_encoding` must be a pandas Series. Got: {type(X_test_encoding)}')\n    if not isinstance(levels, list):\n        raise TypeError(f'`levels` must be a list. Got: {type(levels)}')\n    if not isinstance(metrics, list):\n        raise TypeError(f'`metrics` must be a list. Got: {type(metrics)}')\n    if not isinstance(add_aggregated_metric, bool):\n        raise TypeError(f'`add_aggregated_metric` must be a boolean. Got: {type(add_aggregated_metric)}')\n    metrics = [_get_metric(metric=m) if isinstance(m, str) else add_y_train_argument(m) for m in metrics]\n    metric_names = [m if isinstance(m, str) else m.__name__ for m in metrics]\n    if isinstance(series[levels[0]].index, pd.DatetimeIndex):\n        freq = series[levels[0]].index.freq\n    else:\n        freq = series[levels[0]].index.step\n    if type(forecaster).__name__ == 'ForecasterDirectMultiVariate':\n        step = 1\n        X_train, y_train = forecaster.filter_train_X_y_for_step(step=step, X_train=X_train, y_train=y_train)\n        X_test, y_test = forecaster.filter_train_X_y_for_step(step=step, X_train=X_test, y_train=y_test)\n        forecaster.regressors_[step].fit(X_train, y_train)\n        pred = forecaster.regressors_[step].predict(X_test)\n    else:\n        forecaster.regressor.fit(X_train, y_train)\n        pred = forecaster.regressor.predict(X_test)\n    predictions_per_level = pd.DataFrame({'y_true': y_test, 'y_pred': pred, '_level_skforecast': X_test_encoding}, index=y_test.index).groupby('_level_skforecast')\n    predictions_per_level = {key: group for key, group in predictions_per_level}\n    y_train_per_level = pd.DataFrame({'y_train': y_train, '_level_skforecast': X_train_encoding}, index=y_train.index).groupby('_level_skforecast')\n    y_train_per_level = {key: group.asfreq(freq) for key, group in y_train_per_level}\n    if forecaster.differentiation is not None:\n        for level in predictions_per_level:\n            predictions_per_level[level]['y_true'] = forecaster.differentiator_[level].inverse_transform_next_window(predictions_per_level[level]['y_true'].to_numpy())\n            predictions_per_level[level]['y_pred'] = forecaster.differentiator_[level].inverse_transform_next_window(predictions_per_level[level]['y_pred'].to_numpy())\n            y_train_per_level[level]['y_train'] = forecaster.differentiator_[level].inverse_transform_training(y_train_per_level[level]['y_train'].to_numpy())\n    if forecaster.transformer_series is not None:\n        for level in predictions_per_level:\n            transformer = forecaster.transformer_series_[level]\n            predictions_per_level[level]['y_true'] = transformer.inverse_transform(predictions_per_level[level][['y_true']])\n            predictions_per_level[level]['y_pred'] = transformer.inverse_transform(predictions_per_level[level][['y_pred']])\n            y_train_per_level[level]['y_train'] = transformer.inverse_transform(y_train_per_level[level][['y_train']])\n    metrics_levels = []\n    for level in levels:\n        if level in predictions_per_level:\n            metrics_level = [m(y_true=predictions_per_level[level].loc[:, 'y_true'], y_pred=predictions_per_level[level].loc[:, 'y_pred'], y_train=y_train_per_level[level].loc[:, 'y_train']) for m in metrics]\n            metrics_levels.append(metrics_level)\n        else:\n            metrics_levels.append([None for _ in metrics])\n    metrics_levels = pd.DataFrame(data=metrics_levels, columns=[m if isinstance(m, str) else m.__name__ for m in metrics])\n    metrics_levels.insert(0, 'levels', levels)\n    if len(levels) < 2:\n        add_aggregated_metric = False\n    if add_aggregated_metric:\n        average = metrics_levels.drop(columns='levels').mean(skipna=True)\n        average = average.to_frame().transpose()\n        average['levels'] = 'average'\n        weighted_averages = {}\n        n_predictions_levels = {k: v['y_pred'].notna().sum() for k, v in predictions_per_level.items()}\n        n_predictions_levels = pd.DataFrame(n_predictions_levels.items(), columns=['levels', 'n_predictions'])\n        metrics_levels_no_missing = metrics_levels.merge(n_predictions_levels, on='levels', how='inner')\n        for col in metric_names:\n            weighted_averages[col] = np.average(metrics_levels_no_missing[col], weights=metrics_levels_no_missing['n_predictions'])\n        weighted_average = pd.DataFrame(weighted_averages, index=[0])\n        weighted_average['levels'] = 'weighted_average'\n        list_y_train_by_level = [v['y_train'].to_numpy() for k, v in y_train_per_level.items() if k in predictions_per_level]\n        predictions_pooled = pd.concat(predictions_per_level.values())\n        y_train_pooled = pd.concat([v for k, v in y_train_per_level.items() if k in predictions_per_level])\n        pooled = []\n        for m, m_name in zip(metrics, metric_names):\n            if m_name in ['mean_absolute_scaled_error', 'root_mean_squared_scaled_error']:\n                pooled.append(m(y_true=predictions_pooled['y_true'], y_pred=predictions_pooled['y_pred'], y_train=list_y_train_by_level))\n            else:\n                pooled.append(m(y_true=predictions_pooled['y_true'], y_pred=predictions_pooled['y_pred'], y_train=y_train_pooled['y_train']))\n        pooled = pd.DataFrame([pooled], columns=metric_names)\n        pooled['levels'] = 'pooling'\n        metrics_levels = pd.concat([metrics_levels, average, weighted_average, pooled], axis=0, ignore_index=True)\n    predictions = pd.concat(predictions_per_level.values()).loc[:, ['y_pred', '_level_skforecast']].pivot(columns='_level_skforecast', values='y_pred').rename_axis(columns=None, index=None)\n    predictions = predictions.asfreq(X_test.index.freq)\n    return (metrics_levels, predictions)",
    "skforecast/recursive/_forecaster_recursive.py": "from typing import Union, Tuple, Optional, Callable\nimport warnings\nimport sys\nimport numpy as np\nimport pandas as pd\nimport inspect\nfrom copy import copy\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import clone\nimport skforecast\nfrom ..base import ForecasterBase\nfrom ..exceptions import DataTransformationWarning\nfrom ..utils import initialize_lags, initialize_window_features, initialize_weights, check_select_fit_kwargs, check_y, check_exog, get_exog_dtypes, check_exog_dtypes, check_predict_input, check_interval, preprocess_y, preprocess_last_window, preprocess_exog, input_to_frame, date_to_index_position, expand_index, transform_numpy, transform_dataframe\nfrom ..preprocessing import TimeSeriesDifferentiator\nfrom ..preprocessing import QuantileBinner\n\nclass ForecasterRecursive(ForecasterBase):\n    \"\"\"\n    This class turns any regressor compatible with the scikit-learn API into a\n    recursive autoregressive (multi-step) forecaster.\n    \n    Parameters\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n    lags : int, list, numpy ndarray, range, default `None`\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n    \n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n        - `None`: no lags are included as predictors. \n    window_features : object, list, default `None`\n        Instance or list of instances used to create window features. Window features\n        are created from the original time series and are included as predictors.\n    transformer_y : object transformer (preprocessor), default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and inverse_transform.\n        ColumnTransformers are not allowed since they do not have inverse_transform method.\n        The transformation is applied to `y` before training the forecaster. \n    transformer_exog : object transformer (preprocessor), default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable, default `None`\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\n        method. The resulting `sample_weight` cannot have negative values.\n    differentiation : int, default `None`\n        Order of differencing applied to the time series before training the forecaster.\n        If `None`, no differencing is applied. The order of differentiation is the number\n        of times the differencing operation is applied to a time series. Differencing\n        involves computing the differences between consecutive data points in the series.\n        Differentiation is reversed in the output of `predict()` and `predict_interval()`.\n        **WARNING: This argument is newly introduced and requires special attention. It\n        is still experimental and may undergo changes.**\n    fit_kwargs : dict, default `None`\n        Additional arguments to be passed to the `fit` method of the regressor.\n    binner_kwargs : dict, default `None`\n        Additional arguments to pass to the `QuantileBinner` used to discretize \n        the residuals into k bins according to the predicted values associated \n        with each residual. Available arguments are: `n_bins`, `method`, `subsample`,\n        `random_state` and `dtype`. Argument `method` is passed internally to the\n        fucntion `numpy.percentile`.\n        **New in version 0.14.0**\n    forecaster_id : str, int, default `None`\n        Name used as an identifier of the forecaster.\n    \n    Attributes\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n    lags : numpy ndarray\n        Lags used as predictors.\n    lags_names : list\n        Names of the lags used as predictors.\n    max_lag : int\n        Maximum lag included in `lags`.\n    window_features : list\n        Class or list of classes used to create window features.\n    window_features_names : list\n        Names of the window features to be included in the `X_train` matrix.\n    window_features_class_names : list\n        Names of the classes used to create the window features.\n    max_size_window_features : int\n        Maximum window size required by the window features.\n    window_size : int\n        The window size needed to create the predictors. It is calculated as the \n        maximum value between `max_lag` and `max_size_window_features`. If \n        differentiation is used, `window_size` is increased by n units equal to \n        the order of differentiation so that predictors can be generated correctly.\n    transformer_y : object transformer (preprocessor)\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and inverse_transform.\n        ColumnTransformers are not allowed since they do not have inverse_transform method.\n        The transformation is applied to `y` before training the forecaster.\n    transformer_exog : object transformer (preprocessor)\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\n        method. The resulting `sample_weight` cannot have negative values.\n    differentiation : int\n        Order of differencing applied to the time series before training the forecaster.\n        If `None`, no differencing is applied. The order of differentiation is the number\n        of times the differencing operation is applied to a time series. Differencing\n        involves computing the differences between consecutive data points in the series.\n        Differentiation is reversed in the output of `predict()` and `predict_interval()`.\n        **WARNING: This argument is newly introduced and requires special attention. It\n        is still experimental and may undergo changes.**\n        **New in version 0.10.0**\n    binner : sklearn.preprocessing.KBinsDiscretizer\n        `KBinsDiscretizer` used to discretize residuals into k bins according \n        to the predicted values associated with each residual.\n        **New in version 0.12.0**\n    binner_intervals_ : dict\n        Intervals used to discretize residuals into k bins according to the predicted\n        values associated with each residual.\n        **New in version 0.12.0**\n    binner_kwargs : dict\n        Additional arguments to pass to the `QuantileBinner` used to discretize \n        the residuals into k bins according to the predicted values associated \n        with each residual. Available arguments are: `n_bins`, `method`, `subsample`,\n        `random_state` and `dtype`. Argument `method` is passed internally to the\n        fucntion `numpy.percentile`.\n        **New in version 0.14.0**\n    source_code_weight_func : str\n        Source code of the custom function used to create weights.\n    differentiation : int\n        Order of differencing applied to the time series before training the \n        forecaster.\n    differentiator : TimeSeriesDifferentiator\n        Skforecast object used to differentiate the time series.\n    last_window_ : pandas DataFrame\n        This window represents the most recent data observed by the predictor\n        during its training phase. It contains the values needed to predict the\n        next step immediately after the training data. These values are stored\n        in the original scale of the time series before undergoing any transformations\n        or differentiation. When `differentiation` parameter is specified, the\n        dimensions of the `last_window_` are expanded as many values as the order\n        of differentiation. For example, if `lags` = 7 and `differentiation` = 1,\n        `last_window_` will have 8 values.\n    index_type_ : type\n        Type of index of the input used in training.\n    index_freq_ : str\n        Frequency of Index of the input used in training.\n    training_range_ : pandas Index\n        First and last values of index of the data used during training.\n    exog_in_ : bool\n        If the forecaster has been trained using exogenous variable/s.\n    exog_names_in_ : list\n        Names of the exogenous variables used during training.\n    exog_type_in_ : type\n        Type of exogenous data (pandas Series or DataFrame) used in training.\n    exog_dtypes_in_ : dict\n        Type of each exogenous variable/s used in training. If `transformer_exog` \n        is used, the dtypes are calculated before the transformation.\n    X_train_window_features_names_out_ : list\n        Names of the window features included in the matrix `X_train` created\n        internally for training.\n    X_train_exog_names_out_ : list\n        Names of the exogenous variables included in the matrix `X_train` created\n        internally for training. It can be different from `exog_names_in_` if\n        some exogenous variables are transformed during the training process.\n    X_train_features_names_out_ : list\n        Names of columns of the matrix created internally for training.\n    fit_kwargs : dict\n        Additional arguments to be passed to the `fit` method of the regressor.\n    in_sample_residuals_ : numpy ndarray\n        Residuals of the model when predicting training data. Only stored up to\n        10_000 values. If `transformer_y` is not `None`, residuals are stored in\n        the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation.\n    in_sample_residuals_by_bin_ : dict\n        In sample residuals binned according to the predicted value each residual\n        is associated with. If `transformer_y` is not `None`, residuals are stored\n        in the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n        **New in version 0.14.0**\n    out_sample_residuals_ : numpy ndarray\n        Residuals of the model when predicting non training data. Only stored up to\n        10_000 values. If `transformer_y` is not `None`, residuals are stored in\n        the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation.\n    out_sample_residuals_by_bin_ : dict\n        Out of sample residuals binned according to the predicted value each residual\n        is associated with. If `transformer_y` is not `None`, residuals are stored\n        in the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n        **New in version 0.12.0**\n    creation_date : str\n        Date of creation.\n    is_fitted : bool\n        Tag to identify if the regressor has been fitted (trained).\n    fit_date : str\n        Date of last fit.\n    skforecast_version : str\n        Version of skforecast library used to create the forecaster.\n    python_version : str\n        Version of python used to create the forecaster.\n    forecaster_id : str, int\n        Name used as an identifier of the forecaster.\n    \n    \"\"\"\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Information displayed when a ForecasterRecursive object is printed.\n        \"\"\"\n        params, _, _, exog_names_in_, _ = self._preprocess_repr(regressor=self.regressor, exog_names_in_=self.exog_names_in_)\n        params = self._format_text_repr(params)\n        exog_names_in_ = self._format_text_repr(exog_names_in_)\n        info = f'{'=' * len(type(self).__name__)} \\n{type(self).__name__} \\n{'=' * len(type(self).__name__)} \\nRegressor: {type(self.regressor).__name__} \\nLags: {self.lags} \\nWindow features: {self.window_features_names} \\nWindow size: {self.window_size} \\nExogenous included: {self.exog_in_} \\nExogenous names: {exog_names_in_} \\nTransformer for y: {self.transformer_y} \\nTransformer for exog: {self.transformer_exog} \\nWeight function included: {(True if self.weight_func is not None else False)} \\nDifferentiation order: {self.differentiation} \\nTraining range: {(self.training_range_.to_list() if self.is_fitted else None)} \\nTraining index type: {(str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None)} \\nTraining index frequency: {(self.index_freq_ if self.is_fitted else None)} \\nRegressor parameters: {params} \\nfit_kwargs: {self.fit_kwargs} \\nCreation date: {self.creation_date} \\nLast fit date: {self.fit_date} \\nSkforecast version: {self.skforecast_version} \\nPython version: {self.python_version} \\nForecaster id: {self.forecaster_id} \\n'\n        return info\n\n    def _repr_html_(self):\n        \"\"\"\n        HTML representation of the object.\n        The \"General Information\" section is expanded by default.\n        \"\"\"\n        params, _, _, exog_names_in_, _ = self._preprocess_repr(regressor=self.regressor, exog_names_in_=self.exog_names_in_)\n        style, unique_id = self._get_style_repr_html(self.is_fitted)\n        content = f'\\n        <div class=\"container-{unique_id}\">\\n            <h2>{type(self).__name__}</h2>\\n            <details open>\\n                <summary>General Information</summary>\\n                <ul>\\n                    <li><strong>Regressor:</strong> {type(self.regressor).__name__}</li>\\n                    <li><strong>Lags:</strong> {self.lags}</li>\\n                    <li><strong>Window features:</strong> {self.window_features_names}</li>\\n                    <li><strong>Window size:</strong> {self.window_size}</li>\\n                    <li><strong>Exogenous included:</strong> {self.exog_in_}</li>\\n                    <li><strong>Weight function included:</strong> {self.weight_func is not None}</li>\\n                    <li><strong>Differentiation order:</strong> {self.differentiation}</li>\\n                    <li><strong>Creation date:</strong> {self.creation_date}</li>\\n                    <li><strong>Last fit date:</strong> {self.fit_date}</li>\\n                    <li><strong>Skforecast version:</strong> {self.skforecast_version}</li>\\n                    <li><strong>Python version:</strong> {self.python_version}</li>\\n                    <li><strong>Forecaster id:</strong> {self.forecaster_id}</li>\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Exogenous Variables</summary>\\n                <ul>\\n                    {exog_names_in_}\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Data Transformations</summary>\\n                <ul>\\n                    <li><strong>Transformer for y:</strong> {self.transformer_y}</li>\\n                    <li><strong>Transformer for exog:</strong> {self.transformer_exog}</li>\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Training Information</summary>\\n                <ul>\\n                    <li><strong>Training range:</strong> {(self.training_range_.to_list() if self.is_fitted else 'Not fitted')}</li>\\n                    <li><strong>Training index type:</strong> {(str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted')}</li>\\n                    <li><strong>Training index frequency:</strong> {(self.index_freq_ if self.is_fitted else 'Not fitted')}</li>\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Regressor Parameters</summary>\\n                <ul>\\n                    {params}\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Fit Kwargs</summary>\\n                <ul>\\n                    {self.fit_kwargs}\\n                </ul>\\n            </details>\\n            <p>\\n                <a href=\"https://skforecast.org/{skforecast.__version__}/api/forecasterrecursive.html\">&#128712 <strong>API Reference</strong></a>\\n                &nbsp;&nbsp;\\n                <a href=\"https://skforecast.org/{skforecast.__version__}/user_guides/autoregresive-forecaster.html\">&#128462 <strong>User Guide</strong></a>\\n            </p>\\n        </div>\\n        '\n        return style + content\n\n    def _create_lags(self, y: np.ndarray, X_as_pandas: bool=False, train_index: Optional[pd.Index]=None) -> Tuple[Optional[Union[np.ndarray, pd.DataFrame]], np.ndarray]:\n        \"\"\"\n        Create the lagged values and their target variable from a time series.\n        \n        Note that the returned matrix `X_data` contains the lag 1 in the first \n        column, the lag 2 in the in the second column and so on.\n        \n        Parameters\n        ----------\n        y : numpy ndarray\n            Training time series values.\n        X_as_pandas : bool, default `False`\n            If `True`, the returned matrix `X_data` is a pandas DataFrame.\n        train_index : pandas Index, default `None`\n            Index of the training data. It is used to create the pandas DataFrame\n            `X_data` when `X_as_pandas` is `True`.\n\n        Returns\n        -------\n        X_data : numpy ndarray, pandas DataFrame, None\n            Lagged values (predictors).\n        y_data : numpy ndarray\n            Values of the time series related to each row of `X_data`.\n        \n        \"\"\"\n        X_data = None\n        if self.lags is not None:\n            n_rows = len(y) - self.window_size\n            X_data = np.full(shape=(n_rows, len(self.lags)), fill_value=np.nan, order='F', dtype=float)\n            for i, lag in enumerate(self.lags):\n                X_data[:, i] = y[self.window_size - lag:-lag]\n            if X_as_pandas:\n                X_data = pd.DataFrame(data=X_data, columns=self.lags_names, index=train_index)\n        y_data = y[self.window_size:]\n        return (X_data, y_data)\n\n    def _create_window_features(self, y: pd.Series, train_index: pd.Index, X_as_pandas: bool=False) -> Tuple[list, list]:\n        \"\"\"\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        train_index : pandas Index\n            Index of the training data. It is used to create the pandas DataFrame\n            `X_train_window_features` when `X_as_pandas` is `True`.\n        X_as_pandas : bool, default `False`\n            If `True`, the returned matrix `X_train_window_features` is a \n            pandas DataFrame.\n\n        Returns\n        -------\n        X_train_window_features : list\n            List of numpy ndarrays or pandas DataFrames with the window features.\n        X_train_window_features_names_out_ : list\n            Names of the window features.\n        \n        \"\"\"\n        len_train_index = len(train_index)\n        X_train_window_features = []\n        X_train_window_features_names_out_ = []\n        for wf in self.window_features:\n            X_train_wf = wf.transform_batch(y)\n            if not isinstance(X_train_wf, pd.DataFrame):\n                raise TypeError(f'The method `transform_batch` of {type(wf).__name__} must return a pandas DataFrame.')\n            X_train_wf = X_train_wf.iloc[-len_train_index:]\n            if not len(X_train_wf) == len_train_index:\n                raise ValueError(f'The method `transform_batch` of {type(wf).__name__} must return a DataFrame with the same number of rows as the input time series - `window_size`: {len_train_index}.')\n            if not (X_train_wf.index == train_index).all():\n                raise ValueError(f'The method `transform_batch` of {type(wf).__name__} must return a DataFrame with the same index as the input time series - `window_size`.')\n            X_train_window_features_names_out_.extend(X_train_wf.columns)\n            if not X_as_pandas:\n                X_train_wf = X_train_wf.to_numpy()\n            X_train_window_features.append(X_train_wf)\n        return (X_train_window_features, X_train_window_features_names_out_)\n\n    def _create_train_X_y(self, y: pd.Series, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, pd.Series, list, list, list, list, dict]:\n        \"\"\"\n        Create training matrices from univariate time series and exogenous\n        variables.\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors).\n        y_train : pandas Series\n            Values of the time series related to each row of `X_train`.\n        exog_names_in_ : list\n            Names of the exogenous variables used during training.\n        X_train_window_features_names_out_ : list\n            Names of the window features included in the matrix `X_train` created\n            internally for training.\n        X_train_exog_names_out_ : list\n            Names of the exogenous variables included in the matrix `X_train` created\n            internally for training. It can be different from `exog_names_in_` if\n            some exogenous variables are transformed during the training process.\n        X_train_features_names_out_ : list\n            Names of the columns of the matrix created internally for training.\n        exog_dtypes_in_ : dict\n            Type of each exogenous variable/s used in training. If `transformer_exog` \n            is used, the dtypes are calculated before the transformation.\n        \n        \"\"\"\n        check_y(y=y)\n        y = input_to_frame(data=y, input_name='y')\n        if len(y) <= self.window_size:\n            raise ValueError(f'Length of `y` must be greater than the maximum window size needed by the forecaster.\\n    Length `y`: {len(y)}.\\n    Max window size: {self.window_size}.\\n    Lags window size: {self.max_lag}.\\n    Window features window size: {self.max_size_window_features}.')\n        fit_transformer = False if self.is_fitted else True\n        y = transform_dataframe(df=y, transformer=self.transformer_y, fit=fit_transformer, inverse_transform=False)\n        y_values, y_index = preprocess_y(y=y)\n        train_index = y_index[self.window_size:]\n        if self.differentiation is not None:\n            if not self.is_fitted:\n                y_values = self.differentiator.fit_transform(y_values)\n            else:\n                differentiator = copy(self.differentiator)\n                y_values = differentiator.fit_transform(y_values)\n        exog_names_in_ = None\n        exog_dtypes_in_ = None\n        categorical_features = False\n        if exog is not None:\n            check_exog(exog=exog, allow_nan=True)\n            exog = input_to_frame(data=exog, input_name='exog')\n            len_y = len(y_values)\n            len_train_index = len(train_index)\n            len_exog = len(exog)\n            if not len_exog == len_y and (not len_exog == len_train_index):\n                raise ValueError(f'Length of `exog` must be equal to the length of `y` (if index is fully aligned) or length of `y` - `window_size` (if `exog` starts after the first `window_size` values).\\n    `exog`              : ({exog.index[0]} -- {exog.index[-1]})  (n={len_exog})\\n    `y`                 : ({y.index[0]} -- {y.index[-1]})  (n={len_y})\\n    `y` - `window_size` : ({train_index[0]} -- {train_index[-1]})  (n={len_train_index})')\n            exog_names_in_ = exog.columns.to_list()\n            exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n            exog = transform_dataframe(df=exog, transformer=self.transformer_exog, fit=fit_transformer, inverse_transform=False)\n            check_exog_dtypes(exog, call_check_exog=True)\n            categorical_features = exog.select_dtypes(include=np.number).shape[1] != exog.shape[1]\n            _, exog_index = preprocess_exog(exog=exog, return_values=False)\n            if len_exog == len_y:\n                if not (exog_index == y_index).all():\n                    raise ValueError('When `exog` has the same length as `y`, the index of `exog` must be aligned with the index of `y` to ensure the correct alignment of values.')\n                exog = exog.iloc[self.window_size:,]\n            elif not (exog_index == train_index).all():\n                raise ValueError(\"When `exog` doesn't contain the first `window_size` observations, the index of `exog` must be aligned with the index of `y` minus the first `window_size` observations to ensure the correct alignment of values.\")\n        X_train = []\n        X_train_features_names_out_ = []\n        X_as_pandas = True if categorical_features else False\n        X_train_lags, y_train = self._create_lags(y=y_values, X_as_pandas=X_as_pandas, train_index=train_index)\n        if X_train_lags is not None:\n            X_train.append(X_train_lags)\n            X_train_features_names_out_.extend(self.lags_names)\n        X_train_window_features_names_out_ = None\n        if self.window_features is not None:\n            n_diff = 0 if self.differentiation is None else self.differentiation\n            y_window_features = pd.Series(y_values[n_diff:], index=y_index[n_diff:])\n            X_train_window_features, X_train_window_features_names_out_ = self._create_window_features(y=y_window_features, X_as_pandas=X_as_pandas, train_index=train_index)\n            X_train.extend(X_train_window_features)\n            X_train_features_names_out_.extend(X_train_window_features_names_out_)\n        X_train_exog_names_out_ = None\n        if exog is not None:\n            X_train_exog_names_out_ = exog.columns.to_list()\n            if not X_as_pandas:\n                exog = exog.to_numpy()\n            X_train_features_names_out_.extend(X_train_exog_names_out_)\n            X_train.append(exog)\n        if len(X_train) == 1:\n            X_train = X_train[0]\n        elif X_as_pandas:\n            X_train = pd.concat(X_train, axis=1)\n        else:\n            X_train = np.concatenate(X_train, axis=1)\n        if X_as_pandas:\n            X_train.index = train_index\n        else:\n            X_train = pd.DataFrame(data=X_train, index=train_index, columns=X_train_features_names_out_)\n        y_train = pd.Series(data=y_train, index=train_index, name='y')\n        return (X_train, y_train, exog_names_in_, X_train_window_features_names_out_, X_train_exog_names_out_, X_train_features_names_out_, exog_dtypes_in_)\n\n    def create_train_X_y(self, y: pd.Series, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, pd.Series]:\n        \"\"\"\n        Create training matrices from univariate time series and exogenous\n        variables.\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors).\n        y_train : pandas Series\n            Values of the time series related to each row of `X_data`.\n        \n        \"\"\"\n        output = self._create_train_X_y(y=y, exog=exog)\n        X_train = output[0]\n        y_train = output[1]\n        return (X_train, y_train)\n\n    def _train_test_split_one_step_ahead(self, y: pd.Series, initial_train_size: int, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n        \"\"\"\n        Create matrices needed to train and test the forecaster for one-step-ahead\n        predictions.\n\n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        initial_train_size : int\n            Initial size of the training set. It is the number of observations used\n            to train the forecaster before making the first prediction.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n        \n        Returns\n        -------\n        X_train : pandas DataFrame\n            Predictor values used to train the model.\n        y_train : pandas Series\n            Target values related to each row of `X_train`.\n        X_test : pandas DataFrame\n            Predictor values used to test the model.\n        y_test : pandas Series\n            Target values related to each row of `X_test`.\n        \n        \"\"\"\n        is_fitted = self.is_fitted\n        self.is_fitted = False\n        X_train, y_train, *_ = self._create_train_X_y(y=y.iloc[:initial_train_size], exog=exog.iloc[:initial_train_size] if exog is not None else None)\n        test_init = initial_train_size - self.window_size\n        self.is_fitted = True\n        X_test, y_test, *_ = self._create_train_X_y(y=y.iloc[test_init:], exog=exog.iloc[test_init:] if exog is not None else None)\n        self.is_fitted = is_fitted\n        return (X_train, y_train, X_test, y_test)\n\n    def create_sample_weights(self, X_train: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Crate weights for each observation according to the forecaster's attribute\n        `weight_func`.\n\n        Parameters\n        ----------\n        X_train : pandas DataFrame\n            Dataframe created with the `create_train_X_y` method, first return.\n\n        Returns\n        -------\n        sample_weight : numpy ndarray\n            Weights to use in `fit` method.\n\n        \"\"\"\n        sample_weight = None\n        if self.weight_func is not None:\n            sample_weight = self.weight_func(X_train.index)\n        if sample_weight is not None:\n            if np.isnan(sample_weight).any():\n                raise ValueError('The resulting `sample_weight` cannot have NaN values.')\n            if np.any(sample_weight < 0):\n                raise ValueError('The resulting `sample_weight` cannot have negative values.')\n            if np.sum(sample_weight) == 0:\n                raise ValueError('The resulting `sample_weight` cannot be normalized because the sum of the weights is zero.')\n        return sample_weight\n\n    def fit(self, y: pd.Series, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, store_last_window: bool=True, store_in_sample_residuals: bool=True, random_state: int=123) -> None:\n        \"\"\"\n        Training Forecaster.\n\n        Additional arguments to be passed to the `fit` method of the regressor \n        can be added with the `fit_kwargs` argument when initializing the forecaster.\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned so\n            that y[i] is regressed on exog[i].\n        store_last_window : bool, default `True`\n            Whether or not to store the last window (`last_window_`) of training data.\n        store_in_sample_residuals : bool, default `True`\n            If `True`, in-sample residuals will be stored in the forecaster object\n            after fitting (`in_sample_residuals_` attribute).\n        random_state : int, default `123`\n            Set a seed for the random generator so that the stored sample \n            residuals are always deterministic.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.exog_in_ = False\n        self.exog_names_in_ = None\n        self.exog_type_in_ = None\n        self.exog_dtypes_in_ = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_ = None\n        self.X_train_features_names_out_ = None\n        self.in_sample_residuals_ = None\n        self.is_fitted = False\n        self.fit_date = None\n        X_train, y_train, exog_names_in_, X_train_window_features_names_out_, X_train_exog_names_out_, X_train_features_names_out_, exog_dtypes_in_ = self._create_train_X_y(y=y, exog=exog)\n        sample_weight = self.create_sample_weights(X_train=X_train)\n        if sample_weight is not None:\n            self.regressor.fit(X=X_train, y=y_train, sample_weight=sample_weight, **self.fit_kwargs)\n        else:\n            self.regressor.fit(X=X_train, y=y_train, **self.fit_kwargs)\n        self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n        self.X_train_features_names_out_ = X_train_features_names_out_\n        self.is_fitted = True\n        self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.training_range_ = preprocess_y(y=y, return_values=False)[1][[0, -1]]\n        self.index_type_ = type(X_train.index)\n        if isinstance(X_train.index, pd.DatetimeIndex):\n            self.index_freq_ = X_train.index.freqstr\n        else:\n            self.index_freq_ = X_train.index.step\n        if exog is not None:\n            self.exog_in_ = True\n            self.exog_type_in_ = type(exog)\n            self.exog_names_in_ = exog_names_in_\n            self.exog_dtypes_in_ = exog_dtypes_in_\n            self.X_train_exog_names_out_ = X_train_exog_names_out_\n        if store_in_sample_residuals:\n            self._binning_in_sample_residuals(y_true=y_train.to_numpy(), y_pred=self.regressor.predict(X_train).ravel(), random_state=random_state)\n        if store_last_window:\n            self.last_window_ = y.iloc[-self.window_size:].copy().to_frame(name=y.name if y.name is not None else 'y')\n\n    def _binning_in_sample_residuals(self, y_true: np.ndarray, y_pred: np.ndarray, random_state: int=123) -> None:\n        \"\"\"\n        Binning residuals according to the predicted value each residual is\n        associated with. First a skforecast.preprocessing.QuantileBinner object\n        is fitted to the predicted values. Then, residuals are binned according\n        to the predicted value each residual is associated with. Residuals are\n        stored in the forecaster object as `in_sample_residuals_` and\n        `in_sample_residuals_by_bin_`.\n        If `transformer_y` is not `None`, `y_true` and `y_pred` are transformed\n        before calculating residuals. If `differentiation` is not `None`, `y_true`\n        and `y_pred` are differentiated before calculating residuals. If both,\n        `transformer_y` and `differentiation` are not `None`, transformation is\n        done before differentiation. The number of residuals stored per bin is\n        limited to  `10_000 // self.binner.n_bins_`. The total number of residuals\n        stored is `10_000`.\n        **New in version 0.14.0**\n\n        Parameters\n        ----------\n        y_true : numpy ndarray\n            True values of the time series.\n        y_pred : numpy ndarray\n            Predicted values of the time series.\n        random_state : int, default `123`\n            Set a seed for the random generator so that the stored sample \n            residuals are always deterministic.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        data = pd.DataFrame({'prediction': y_pred, 'residuals': y_true - y_pred})\n        data['bin'] = self.binner.fit_transform(y_pred).astype(int)\n        self.in_sample_residuals_by_bin_ = data.groupby('bin')['residuals'].apply(np.array).to_dict()\n        rng = np.random.default_rng(seed=random_state)\n        max_sample = 10000 // self.binner.n_bins_\n        for k, v in self.in_sample_residuals_by_bin_.items():\n            if len(v) > max_sample:\n                sample = v[rng.integers(low=0, high=len(v), size=max_sample)]\n                self.in_sample_residuals_by_bin_[k] = sample\n        self.in_sample_residuals_ = np.concatenate(list(self.in_sample_residuals_by_bin_.values()))\n        self.binner_intervals_ = self.binner.intervals_\n\n    def _create_predict_inputs(self, steps: Union[int, str, pd.Timestamp], last_window: Optional[Union[pd.Series, pd.DataFrame]]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, predict_boot: bool=False, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False, check_inputs: bool=True) -> Tuple[np.ndarray, Optional[np.ndarray], pd.Index, int]:\n        \"\"\"\n        Create the inputs needed for the first iteration of the prediction \n        process. As this is a recursive process, the last window is updated at \n        each iteration of the prediction process.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        predict_boot : bool, default `False`\n            If `True`, residuals are returned to generate bootstrapping predictions.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n\n        Returns\n        -------\n        last_window_values : numpy ndarray\n            Series values used to create the predictors needed in the first \n            iteration of the prediction (t + 1).\n        exog_values : numpy ndarray, None\n            Exogenous variable/s included as predictor/s.\n        prediction_index : pandas Index\n            Index of the predictions.\n        steps: int\n            Number of future steps predicted.\n        \n        \"\"\"\n        if last_window is None:\n            last_window = self.last_window_\n        if self.is_fitted:\n            steps = date_to_index_position(index=last_window.index, date_input=steps, date_literal='steps')\n        if check_inputs:\n            check_predict_input(forecaster_name=type(self).__name__, steps=steps, is_fitted=self.is_fitted, exog_in_=self.exog_in_, index_type_=self.index_type_, index_freq_=self.index_freq_, window_size=self.window_size, last_window=last_window, exog=exog, exog_type_in_=self.exog_type_in_, exog_names_in_=self.exog_names_in_, interval=None)\n            if predict_boot and (not use_in_sample_residuals):\n                if not use_binned_residuals and self.out_sample_residuals_ is None:\n                    raise ValueError('`forecaster.out_sample_residuals_` is `None`. Use `use_in_sample_residuals=True` or the `set_out_sample_residuals()` method before predicting.')\n                if use_binned_residuals and self.out_sample_residuals_by_bin_ is None:\n                    raise ValueError('`forecaster.out_sample_residuals_by_bin_` is `None`. Use `use_in_sample_residuals=True` or the `set_out_sample_residuals()` method before predicting.')\n        last_window = last_window.iloc[-self.window_size:].copy()\n        last_window_values, last_window_index = preprocess_last_window(last_window=last_window)\n        last_window_values = transform_numpy(array=last_window_values, transformer=self.transformer_y, fit=False, inverse_transform=False)\n        if self.differentiation is not None:\n            last_window_values = self.differentiator.fit_transform(last_window_values)\n        if exog is not None:\n            exog = input_to_frame(data=exog, input_name='exog')\n            exog = exog.loc[:, self.exog_names_in_]\n            exog = transform_dataframe(df=exog, transformer=self.transformer_exog, fit=False, inverse_transform=False)\n            check_exog_dtypes(exog=exog)\n            exog_values = exog.to_numpy()[:steps]\n        else:\n            exog_values = None\n        prediction_index = expand_index(index=last_window_index, steps=steps)\n        return (last_window_values, exog_values, prediction_index, steps)\n\n    def _recursive_predict(self, steps: int, last_window_values: np.ndarray, exog_values: Optional[np.ndarray]=None, residuals: Optional[Union[np.ndarray, dict]]=None, use_binned_residuals: bool=False) -> np.ndarray:\n        \"\"\"\n        Predict n steps ahead. It is an iterative process in which, each prediction,\n        is used as a predictor for the next step.\n        \n        Parameters\n        ----------\n        steps : int\n            Number of future steps predicted.\n        last_window_values : numpy ndarray\n            Series values used to create the predictors needed in the first \n            iteration of the prediction (t + 1).\n        exog_values : numpy ndarray, default `None`\n            Exogenous variable/s included as predictor/s.\n        residuals : numpy ndarray, dict, default `None`\n            Residuals used to generate bootstrapping predictions.\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : numpy ndarray\n            Predicted values.\n        \n        \"\"\"\n        n_lags = len(self.lags) if self.lags is not None else 0\n        n_window_features = len(self.X_train_window_features_names_out_) if self.window_features is not None else 0\n        n_exog = exog_values.shape[1] if exog_values is not None else 0\n        X = np.full(shape=n_lags + n_window_features + n_exog, fill_value=np.nan, dtype=float)\n        predictions = np.full(shape=steps, fill_value=np.nan, dtype=float)\n        last_window = np.concatenate((last_window_values, predictions))\n        for i in range(steps):\n            if self.lags is not None:\n                X[:n_lags] = last_window[-self.lags - (steps - i)]\n            if self.window_features is not None:\n                X[n_lags:n_lags + n_window_features] = np.concatenate([wf.transform(last_window[i:-(steps - i)]) for wf in self.window_features])\n            if exog_values is not None:\n                X[n_lags + n_window_features:] = exog_values[i]\n            pred = self.regressor.predict(X.reshape(1, -1)).ravel()\n            if residuals is not None:\n                if use_binned_residuals:\n                    predicted_bin = self.binner.transform(pred).item()\n                    step_residual = residuals[predicted_bin][i]\n                else:\n                    step_residual = residuals[i]\n                pred += step_residual\n            predictions[i] = pred[0]\n            last_window[-(steps - i)] = pred[0]\n        return predictions\n\n    def create_predict_X(self, steps: int, last_window: Optional[Union[pd.Series, pd.DataFrame]]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> pd.DataFrame:\n        \"\"\"\n        Create the predictors needed to predict `steps` ahead. As it is a recursive\n        process, the predictors are created at each iteration of the prediction \n        process.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n\n        Returns\n        -------\n        X_predict : pandas DataFrame\n            Pandas DataFrame with the predictors for each step. The index \n            is the same as the prediction index.\n        \n        \"\"\"\n        last_window_values, exog_values, prediction_index, steps = self._create_predict_inputs(steps=steps, last_window=last_window, exog=exog)\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', message='X does not have valid feature names', category=UserWarning)\n            predictions = self._recursive_predict(steps=steps, last_window_values=last_window_values, exog_values=exog_values)\n        X_predict = []\n        full_predictors = np.concatenate((last_window_values, predictions))\n        if self.lags is not None:\n            idx = np.arange(-steps, 0)[:, None] - self.lags\n            X_lags = full_predictors[idx + len(full_predictors)]\n            X_predict.append(X_lags)\n        if self.window_features is not None:\n            X_window_features = np.full(shape=(steps, len(self.X_train_window_features_names_out_)), fill_value=np.nan, order='C', dtype=float)\n            for i in range(steps):\n                X_window_features[i, :] = np.concatenate([wf.transform(full_predictors[i:-(steps - i)]) for wf in self.window_features])\n            X_predict.append(X_window_features)\n        if exog is not None:\n            X_predict.append(exog_values)\n        X_predict = pd.DataFrame(data=np.concatenate(X_predict, axis=1), columns=self.X_train_features_names_out_, index=prediction_index)\n        if self.transformer_y is not None or self.differentiation is not None:\n            warnings.warn('The output matrix is in the transformed scale due to the inclusion of transformations or differentiation in the Forecaster. As a result, any predictions generated using this matrix will also be in the transformed scale. Please refer to the documentation for more details: https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html', DataTransformationWarning)\n        return X_predict\n\n    def predict(self, steps: Union[int, str, pd.Timestamp], last_window: Optional[Union[pd.Series, pd.DataFrame]]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, check_inputs: bool=True) -> pd.Series:\n        \"\"\"\n        Predict n steps ahead. It is an recursive process in which, each prediction,\n        is used as a predictor for the next step.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n\n        Returns\n        -------\n        predictions : pandas Series\n            Predicted values.\n        \n        \"\"\"\n        last_window_values, exog_values, prediction_index, steps = self._create_predict_inputs(steps=steps, last_window=last_window, exog=exog, check_inputs=check_inputs)\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', message='X does not have valid feature names', category=UserWarning)\n            predictions = self._recursive_predict(steps=steps, last_window_values=last_window_values, exog_values=exog_values)\n        if self.differentiation is not None:\n            predictions = self.differentiator.inverse_transform_next_window(predictions)\n        predictions = transform_numpy(array=predictions, transformer=self.transformer_y, fit=False, inverse_transform=True)\n        predictions = pd.Series(data=predictions, index=prediction_index, name='pred')\n        return predictions\n\n    def predict_bootstrapping(self, steps: Union[int, str, pd.Timestamp], last_window: Optional[pd.Series]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False) -> pd.DataFrame:\n        \"\"\"\n        Generate multiple forecasting predictions using a bootstrapping process. \n        By sampling from a collection of past observed errors (the residuals),\n        each iteration of bootstrapping generates a different set of predictions. \n        See the Notes section for more information. \n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        boot_predictions : pandas DataFrame\n            Predictions generated by bootstrapping.\n            Shape: (steps, n_boot)\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n        Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n        \"\"\"\n        last_window_values, exog_values, prediction_index, steps = self._create_predict_inputs(steps=steps, last_window=last_window, exog=exog, predict_boot=True, use_in_sample_residuals=use_in_sample_residuals, use_binned_residuals=use_binned_residuals)\n        if use_in_sample_residuals:\n            residuals = self.in_sample_residuals_\n            residuals_by_bin = self.in_sample_residuals_by_bin_\n        else:\n            residuals = self.out_sample_residuals_\n            residuals_by_bin = self.out_sample_residuals_by_bin_\n        rng = np.random.default_rng(seed=random_state)\n        if use_binned_residuals:\n            sampled_residuals = {k: v[rng.integers(low=0, high=len(v), size=(steps, n_boot))] for k, v in residuals_by_bin.items()}\n        else:\n            sampled_residuals = residuals[rng.integers(low=0, high=len(residuals), size=(steps, n_boot))]\n        boot_columns = []\n        boot_predictions = np.full(shape=(steps, n_boot), fill_value=np.nan, order='F', dtype=float)\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', message='X does not have valid feature names', category=UserWarning)\n            for i in range(n_boot):\n                if use_binned_residuals:\n                    boot_sampled_residuals = {k: v[:, i] for k, v in sampled_residuals.items()}\n                else:\n                    boot_sampled_residuals = sampled_residuals[:, i]\n                boot_columns.append(f'pred_boot_{i}')\n                boot_predictions[:, i] = self._recursive_predict(steps=steps, last_window_values=last_window_values, exog_values=exog_values, residuals=boot_sampled_residuals, use_binned_residuals=use_binned_residuals)\n        if self.differentiation is not None:\n            boot_predictions = self.differentiator.inverse_transform_next_window(boot_predictions)\n        if self.transformer_y:\n            boot_predictions = np.apply_along_axis(func1d=transform_numpy, axis=0, arr=boot_predictions, transformer=self.transformer_y, fit=False, inverse_transform=True)\n        boot_predictions = pd.DataFrame(data=boot_predictions, index=prediction_index, columns=boot_columns)\n        return boot_predictions\n\n    def predict_interval(self, steps: Union[int, str, pd.Timestamp], last_window: Optional[pd.Series]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, interval: list=[5, 95], n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False) -> pd.DataFrame:\n        \"\"\"\n        Iterative process in which each prediction is used as a predictor\n        for the next step, and bootstrapping is used to estimate prediction\n        intervals. Both predictions and intervals are returned.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        interval : list, default `[5, 95]`\n            Confidence of the prediction interval estimated. Sequence of \n            percentiles to compute, which must be between 0 and 100 inclusive. \n            For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Values predicted by the forecaster and their estimated interval.\n\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n        check_interval(interval=interval)\n        boot_predictions = self.predict_bootstrapping(steps=steps, last_window=last_window, exog=exog, n_boot=n_boot, random_state=random_state, use_in_sample_residuals=use_in_sample_residuals, use_binned_residuals=use_binned_residuals)\n        predictions = self.predict(steps=steps, last_window=last_window, exog=exog, check_inputs=False)\n        interval = np.array(interval) / 100\n        predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n        predictions_interval.columns = ['lower_bound', 'upper_bound']\n        predictions = pd.concat((predictions, predictions_interval), axis=1)\n        return predictions\n\n    def predict_quantiles(self, steps: Union[int, str, pd.Timestamp], last_window: Optional[pd.Series]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, quantiles: list=[0.05, 0.5, 0.95], n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False) -> pd.DataFrame:\n        \"\"\"\n        Calculate the specified quantiles for each step. After generating \n        multiple forecasting predictions through a bootstrapping process, each \n        quantile is calculated for each step.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        quantiles : list, default `[0.05, 0.5, 0.95]`\n            Sequence of quantiles to compute, which must be between 0 and 1 \n            inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n            `quantiles = [0.05, 0.5, 0.95]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate quantiles.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot quantiles are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create prediction quantiles. If `False`, out of\n            sample residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Quantiles predicted by the forecaster.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n        check_interval(quantiles=quantiles)\n        boot_predictions = self.predict_bootstrapping(steps=steps, last_window=last_window, exog=exog, n_boot=n_boot, random_state=random_state, use_in_sample_residuals=use_in_sample_residuals, use_binned_residuals=use_binned_residuals)\n        predictions = boot_predictions.quantile(q=quantiles, axis=1).transpose()\n        predictions.columns = [f'q_{q}' for q in quantiles]\n        return predictions\n\n    def predict_dist(self, steps: Union[int, str, pd.Timestamp], distribution: object, last_window: Optional[pd.Series]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False) -> pd.DataFrame:\n        \"\"\"\n        Fit a given probability distribution for each step. After generating \n        multiple forecasting predictions through a bootstrapping process, each \n        step is fitted to the given distribution.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        distribution : Object\n            A distribution object from scipy.stats.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).  \n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Distribution parameters estimated for each step.\n\n        \"\"\"\n        boot_samples = self.predict_bootstrapping(steps=steps, last_window=last_window, exog=exog, n_boot=n_boot, random_state=random_state, use_in_sample_residuals=use_in_sample_residuals, use_binned_residuals=use_binned_residuals)\n        param_names = [p for p in inspect.signature(distribution._pdf).parameters if not p == 'x'] + ['loc', 'scale']\n        param_values = np.apply_along_axis(lambda x: distribution.fit(x), axis=1, arr=boot_samples)\n        predictions = pd.DataFrame(data=param_values, columns=param_names, index=boot_samples.index)\n        return predictions\n\n    def set_params(self, params: dict) -> None:\n        \"\"\"\n        Set new values to the parameters of the scikit learn model stored in the\n        forecaster.\n        \n        Parameters\n        ----------\n        params : dict\n            Parameters values.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        self.regressor = clone(self.regressor)\n        self.regressor.set_params(**params)\n\n    def set_fit_kwargs(self, fit_kwargs: dict) -> None:\n        \"\"\"\n        Set new values for the additional keyword arguments passed to the `fit` \n        method of the regressor.\n        \n        Parameters\n        ----------\n        fit_kwargs : dict\n            Dict of the form {\"argument\": new_value}.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n\n    def set_lags(self, lags: Optional[Union[int, list, np.ndarray, range]]=None) -> None:\n        \"\"\"\n        Set new value to the attribute `lags`. Attributes `lags_names`, \n        `max_lag` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        lags : int, list, numpy ndarray, range, default `None`\n            Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. \n        \n            - `int`: include lags from 1 to `lags` (included).\n            - `list`, `1d numpy ndarray` or `range`: include only lags present in \n            `lags`, all elements must be int.\n            - `None`: no lags are included as predictors. \n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        if self.window_features is None and lags is None:\n            raise ValueError('At least one of the arguments `lags` or `window_features` must be different from None. This is required to create the predictors used in training the forecaster.')\n        self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\n        self.window_size = max([ws for ws in [self.max_lag, self.max_size_window_features] if ws is not None])\n        if self.differentiation is not None:\n            self.window_size += self.differentiation\n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_window_features(self, window_features: Optional[Union[object, list]]=None) -> None:\n        \"\"\"\n        Set new value to the attribute `window_features`. Attributes \n        `max_size_window_features`, `window_features_names`, \n        `window_features_class_names` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        window_features : object, list, default `None`\n            Instance or list of instances used to create window features. Window features\n            are created from the original time series and are included as predictors.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        if window_features is None and self.lags is None:\n            raise ValueError('At least one of the arguments `lags` or `window_features` must be different from None. This is required to create the predictors used in training the forecaster.')\n        self.window_features, self.window_features_names, self.max_size_window_features = initialize_window_features(window_features)\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [type(wf).__name__ for wf in self.window_features]\n        self.window_size = max([ws for ws in [self.max_lag, self.max_size_window_features] if ws is not None])\n        if self.differentiation is not None:\n            self.window_size += self.differentiation\n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_out_sample_residuals(self, y_true: Union[pd.Series, np.ndarray], y_pred: Union[pd.Series, np.ndarray], append: bool=False, random_state: int=123) -> None:\n        \"\"\"\n        Set new values to the attribute `out_sample_residuals_`. Out of sample\n        residuals are meant to be calculated using observations that did not\n        participate in the training process. `y_true` and `y_pred` are expected\n        to be in the original scale of the time series. Residuals are calculated\n        as `y_true` - `y_pred`, after applying the necessary transformations and\n        differentiations if the forecaster includes them (`self.transformer_y`\n        and `self.differentiation`). Two internal attributes are updated:\n\n        + `out_sample_residuals_`: residuals stored in a numpy ndarray.\n        + `out_sample_residuals_by_bin_`: residuals are binned according to the\n        predicted value they are associated with and stored in a dictionary, where\n        the keys are the  intervals of the predicted values and the values are\n        the residuals associated with that range. If a bin binning is empty, it\n        is filled with a random sample of residuals from other bins. This is done\n        to ensure that all bins have at least one residual and can be used in the\n        prediction process.\n\n        A total of 10_000 residuals are stored in the attribute `out_sample_residuals_`.\n        If the number of residuals is greater than 10_000, a random sample of\n        10_000 residuals is stored. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n        \n        Parameters\n        ----------\n        y_true : pandas Series, numpy ndarray, default `None`\n            True values of the time series from which the residuals have been\n            calculated.\n        y_pred : pandas Series, numpy ndarray, default `None`\n            Predicted values of the time series.\n        append : bool, default `False`\n            If `True`, new residuals are added to the once already stored in the\n            forecaster. If after appending the new residuals, the limit of\n            `10_000 // self.binner.n_bins_` values per bin is reached, a random\n            sample of residuals is stored.\n        random_state : int, default `123`\n            Sets a seed to the random sampling for reproducible output.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n        if not self.is_fitted:\n            raise NotFittedError('This forecaster is not fitted yet. Call `fit` with appropriate arguments before using `set_out_sample_residuals()`.')\n        if not isinstance(y_true, (np.ndarray, pd.Series)):\n            raise TypeError(f'`y_true` argument must be `numpy ndarray` or `pandas Series`. Got {type(y_true)}.')\n        if not isinstance(y_pred, (np.ndarray, pd.Series)):\n            raise TypeError(f'`y_pred` argument must be `numpy ndarray` or `pandas Series`. Got {type(y_pred)}.')\n        if len(y_true) != len(y_pred):\n            raise ValueError(f'`y_true` and `y_pred` must have the same length. Got {len(y_true)} and {len(y_pred)}.')\n        if isinstance(y_true, pd.Series) and isinstance(y_pred, pd.Series):\n            if not y_true.index.equals(y_pred.index):\n                raise ValueError('`y_true` and `y_pred` must have the same index.')\n        if not isinstance(y_pred, np.ndarray):\n            y_pred = y_pred.to_numpy()\n        if not isinstance(y_true, np.ndarray):\n            y_true = y_true.to_numpy()\n        if self.transformer_y:\n            y_true = transform_numpy(array=y_true, transformer=self.transformer_y, fit=False, inverse_transform=False)\n            y_pred = transform_numpy(array=y_pred, transformer=self.transformer_y, fit=False, inverse_transform=False)\n        if self.differentiation is not None:\n            differentiator = copy(self.differentiator)\n            differentiator.set_params(window_size=None)\n            y_true = differentiator.fit_transform(y_true)[self.differentiation:]\n            y_pred = differentiator.fit_transform(y_pred)[self.differentiation:]\n        residuals = y_true - y_pred\n        data = pd.DataFrame({'prediction': y_pred, 'residuals': residuals})\n        data['bin'] = self.binner.transform(y_pred).astype(int)\n        residuals_by_bin = data.groupby('bin')['residuals'].apply(np.array).to_dict()\n        if append and self.out_sample_residuals_by_bin_ is not None:\n            for k, v in residuals_by_bin.items():\n                if k in self.out_sample_residuals_by_bin_:\n                    self.out_sample_residuals_by_bin_[k] = np.concatenate((self.out_sample_residuals_by_bin_[k], v))\n                else:\n                    self.out_sample_residuals_by_bin_[k] = v\n        else:\n            self.out_sample_residuals_by_bin_ = residuals_by_bin\n        max_samples = 10000 // self.binner.n_bins_\n        rng = np.random.default_rng(seed=random_state)\n        for k, v in self.out_sample_residuals_by_bin_.items():\n            if len(v) > max_samples:\n                sample = rng.choice(a=v, size=max_samples, replace=False)\n                self.out_sample_residuals_by_bin_[k] = sample\n        for k in self.in_sample_residuals_by_bin_.keys():\n            if k not in self.out_sample_residuals_by_bin_:\n                self.out_sample_residuals_by_bin_[k] = np.array([])\n        empty_bins = [k for k, v in self.out_sample_residuals_by_bin_.items() if len(v) == 0]\n        if empty_bins:\n            warnings.warn(f'The following bins have no out of sample residuals: {empty_bins}. No predicted values fall in the interval {[self.binner_intervals_[bin] for bin in empty_bins]}. Empty bins will be filled with a random sample of residuals.')\n            for k in empty_bins:\n                self.out_sample_residuals_by_bin_[k] = rng.choice(a=residuals, size=max_samples, replace=True)\n        self.out_sample_residuals_ = np.concatenate(list(self.out_sample_residuals_by_bin_.values()))\n\n    def get_feature_importances(self, sort_importance: bool=True) -> pd.DataFrame:\n        \"\"\"\n        Return feature importances of the regressor stored in the forecaster.\n        Only valid when regressor stores internally the feature importances in the\n        attribute `feature_importances_` or `coef_`. Otherwise, returns `None`.\n\n        Parameters\n        ----------\n        sort_importance: bool, default `True`\n            If `True`, sorts the feature importances in descending order.\n\n        Returns\n        -------\n        feature_importances : pandas DataFrame\n            Feature importances associated with each predictor.\n\n        \"\"\"\n        if not self.is_fitted:\n            raise NotFittedError('This forecaster is not fitted yet. Call `fit` with appropriate arguments before using `get_feature_importances()`.')\n        if isinstance(self.regressor, Pipeline):\n            estimator = self.regressor[-1]\n        else:\n            estimator = self.regressor\n        if hasattr(estimator, 'feature_importances_'):\n            feature_importances = estimator.feature_importances_\n        elif hasattr(estimator, 'coef_'):\n            feature_importances = estimator.coef_\n        else:\n            warnings.warn(f'Impossible to access feature importances for regressor of type {type(estimator)}. This method is only valid when the regressor stores internally the feature importances in the attribute `feature_importances_` or `coef_`.')\n            feature_importances = None\n        if feature_importances is not None:\n            feature_importances = pd.DataFrame({'feature': self.X_train_features_names_out_, 'importance': feature_importances})\n            if sort_importance:\n                feature_importances = feature_importances.sort_values(by='importance', ascending=False)\n        return feature_importances"
  },
  "call_tree": {
    "skforecast/model_selection/tests/tests_utils/test_initialize_lags_grid.py:test_TypeError_initialize_lags__rid_when_not_list_dict_or_None": {
      "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:__init__": {
        "skforecast/utils/utils.py:initialize_lags": {},
        "skforecast/utils/utils.py:initialize_window_features": {},
        "skforecast/preprocessing/preprocessing.py:QuantileBinner:__init__": {
          "skforecast/preprocessing/preprocessing.py:QuantileBinner:_validate_params": {}
        },
        "skforecast/utils/utils.py:initialize_weights": {},
        "skforecast/utils/utils.py:check_select_fit_kwargs": {}
      },
      "skforecast/model_selection/_utils.py:initialize_lags_grid": {}
    },
    "skforecast/model_selection/tests/tests_utils/test_initialize_lags_grid.py:test_initialize_lags_grid_when_lags_grid_is_a_list": {
      "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:__init__": {
        "skforecast/utils/utils.py:initialize_lags": {},
        "skforecast/utils/utils.py:initialize_window_features": {},
        "skforecast/preprocessing/preprocessing.py:QuantileBinner:__init__": {
          "skforecast/preprocessing/preprocessing.py:QuantileBinner:_validate_params": {}
        },
        "skforecast/utils/utils.py:initialize_weights": {},
        "skforecast/utils/utils.py:check_select_fit_kwargs": {}
      },
      "skforecast/model_selection/_utils.py:initialize_lags_grid": {}
    },
    "skforecast/model_selection/tests/tests_utils/test_initialize_lags_grid.py:test_initialize_lags_grid_when_lags_grid_is_None": {
      "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:__init__": {
        "skforecast/utils/utils.py:initialize_lags": {},
        "skforecast/utils/utils.py:initialize_window_features": {},
        "skforecast/preprocessing/preprocessing.py:QuantileBinner:__init__": {
          "skforecast/preprocessing/preprocessing.py:QuantileBinner:_validate_params": {}
        },
        "skforecast/utils/utils.py:initialize_weights": {},
        "skforecast/utils/utils.py:check_select_fit_kwargs": {}
      },
      "skforecast/model_selection/_utils.py:initialize_lags_grid": {}
    },
    "skforecast/model_selection/tests/tests_utils/test_initialize_lags_grid.py:test_initialize_lags_grid_when_lags_grid_is_a_dict": {
      "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:__init__": {
        "skforecast/utils/utils.py:initialize_lags": {},
        "skforecast/utils/utils.py:initialize_window_features": {},
        "skforecast/preprocessing/preprocessing.py:QuantileBinner:__init__": {
          "skforecast/preprocessing/preprocessing.py:QuantileBinner:_validate_params": {}
        },
        "skforecast/utils/utils.py:initialize_weights": {},
        "skforecast/utils/utils.py:check_select_fit_kwargs": {}
      },
      "skforecast/model_selection/_utils.py:initialize_lags_grid": {}
    }
  }
}