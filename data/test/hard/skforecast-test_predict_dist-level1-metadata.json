{
  "dir_path": "/app/skforecast",
  "package_name": "skforecast",
  "sample_name": "skforecast-test_predict_dist",
  "src_dir": "skforecast/",
  "test_dir": "tests/",
  "test_file": "skforecast/direct/tests/tests_forecaster_direct_multivariate/test_predict_dist.py",
  "test_code": "# Unit test predict_dist ForecasterDirectMultiVariate\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom skforecast.direct import ForecasterDirectMultiVariate\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom scipy.stats import norm\n\n# Fixtures\nfrom .fixtures_forecaster_direct_multivariate import series\nfrom .fixtures_forecaster_direct_multivariate import exog\nfrom .fixtures_forecaster_direct_multivariate import exog_predict\n\ntransformer_exog = ColumnTransformer(\n                       [('scale', StandardScaler(), ['exog_1']),\n                        ('onehot', OneHotEncoder(), ['exog_2'])],\n                       remainder = 'passthrough',\n                       verbose_feature_names_out = False\n                   )\n\n\ndef test_predict_dist_output_when_forecaster_is_LinearRegression_steps_is_2_in_sample_residuals_True_exog_and_transformer():\n    \"\"\"\n    Test output of predict_dist when regressor is LinearRegression,\n    2 steps are predicted, using in-sample residuals, exog is included and both\n    inputs are transformed.\n    \"\"\"\n    forecaster = ForecasterDirectMultiVariate(\n                     regressor          = LinearRegression(),\n                     steps              = 2,\n                     level              = 'l1',\n                     lags               = 3,\n                     transformer_series = StandardScaler(),\n                     transformer_exog   = transformer_exog\n                 )\n    \n    forecaster.fit(series=series, exog=exog)\n    results = forecaster.predict_dist(\n                  steps                   = 2,\n                  exog                    = exog_predict,\n                  distribution            = norm,\n                  n_boot                  = 4,\n                  use_in_sample_residuals = True\n              )\n    \n    expected = pd.DataFrame(\n                   data    = np.array([[0.53808076, 0.11721631],\n                                       [0.32552242, 0.1713172]]),\n                   columns = ['l1_loc', 'l1_scale'],\n                   index   = pd.RangeIndex(start=50, stop=52)\n               )\n    \n    pd.testing.assert_frame_equal(expected, results)\n\n\ndef test_predict_dist_output_when_forecaster_is_LinearRegression_steps_is_2_in_sample_residuals_False_exog_and_transformer():\n    \"\"\"\n    Test output of predict_dist when regressor is LinearRegression,\n    2 steps are predicted, using out-sample residuals, exog is included and both\n    inputs are transformed.\n    \"\"\"\n    forecaster = ForecasterDirectMultiVariate(\n                     regressor          = LinearRegression(),\n                     steps              = 2,\n                     level              = 'l1',\n                     lags               = 3,\n                     transformer_series = StandardScaler(),\n                     transformer_exog   = transformer_exog\n                 )\n    \n    forecaster.fit(series=series, exog=exog)\n    forecaster.out_sample_residuals_ = forecaster.in_sample_residuals_\n    results = forecaster.predict_dist(\n                  steps                   = 2,\n                  exog                    = exog_predict,\n                  distribution            = norm,\n                  n_boot                  = 4,\n                  use_in_sample_residuals = False\n              )\n    \n    expected = pd.DataFrame(\n                   data    = np.array([[0.53808076, 0.11721631],\n                                       [0.32552242, 0.1713172]]),\n                   columns = ['l1_loc', 'l1_scale'],\n                   index   = pd.RangeIndex(start=50, stop=52)\n               )\n\n    pd.testing.assert_frame_equal(expected, results)",
  "GT_file_code": {
    "skforecast/direct/_forecaster_direct_multivariate.py": "################################################################################\n#                         ForecasterDirectMultiVariate                         #\n#                                                                              #\n# This work by skforecast team is licensed under the BSD 3-Clause License.     #\n################################################################################\n# coding=utf-8\n\nfrom typing import Union, Tuple, Optional, Callable, Any\nimport warnings\nimport sys\nimport numpy as np\nimport pandas as pd\nimport inspect\nfrom copy import copy\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import clone\nfrom joblib import Parallel, delayed, cpu_count\nfrom sklearn.preprocessing import StandardScaler\nfrom itertools import chain\n\nimport skforecast\nfrom ..base import ForecasterBase\nfrom ..exceptions import DataTransformationWarning\nfrom ..utils import (\n    initialize_lags,\n    initialize_window_features,\n    initialize_weights,\n    initialize_transformer_series,\n    check_select_fit_kwargs,\n    check_y,\n    check_exog,\n    prepare_steps_direct,\n    get_exog_dtypes,\n    check_exog_dtypes,\n    check_predict_input,\n    check_interval,\n    preprocess_y,\n    preprocess_last_window,\n    input_to_frame,\n    exog_to_direct,\n    exog_to_direct_numpy,\n    expand_index,\n    transform_numpy,\n    transform_series,\n    transform_dataframe,\n    select_n_jobs_fit_forecaster,\n    set_skforecast_warnings\n)\nfrom ..preprocessing import TimeSeriesDifferentiator\nfrom ..model_selection._utils import _extract_data_folds_multiseries\n\n\nclass ForecasterDirectMultiVariate(ForecasterBase):\n    \"\"\"\n    This class turns any regressor compatible with the scikit-learn API into a\n    autoregressive multivariate direct multi-step forecaster. A separate model \n    is created for each forecast time step. See documentation for more details.\n\n    Parameters\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n    level : str\n        Name of the time series to be predicted.\n    steps : int\n        Maximum number of future steps the forecaster will predict when using\n        method `predict()`. Since a different model is created for each step,\n        this value must be defined before training.\n    lags : int, list, numpy ndarray, range, dict, default `None`\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n\n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n        - `dict`: create different lags for each series. {'series_column_name': lags}.\n        - `None`: no lags are included as predictors. \n    window_features : object, list, default `None`\n        Instance or list of instances used to create window features. Window features\n        are created from the original time series and are included as predictors.\n    transformer_series : transformer (preprocessor), dict, default `sklearn.preprocessing.StandardScaler`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and \n        inverse_transform. Transformation is applied to each `series` before training \n        the forecaster. ColumnTransformers are not allowed since they do not have \n        inverse_transform method.\n\n        - If single transformer: it is cloned and applied to all series. \n        - If `dict` of transformers: a different transformer can be used for each series.\n    transformer_exog : transformer, default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable, default `None`\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\n        method. The resulting `sample_weight` cannot have negative values.\n    fit_kwargs : dict, default `None`\n        Additional arguments to be passed to the `fit` method of the regressor.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_fit_forecaster.\n    forecaster_id : str, int, default `None`\n        Name used as an identifier of the forecaster.\n\n    Attributes\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n        An instance of this regressor is trained for each step. All of them \n        are stored in `self.regressors_`.\n    regressors_ : dict\n        Dictionary with regressors trained for each step. They are initialized \n        as a copy of `regressor`.\n    steps : int\n        Number of future steps the forecaster will predict when using method\n        `predict()`. Since a different model is created for each step, this value\n        should be defined before training.\n    lags : numpy ndarray, dict\n        Lags used as predictors.\n    lags_ : dict\n        Dictionary with the lags of each series. Created from `lags` when \n        creating the training matrices and used internally to avoid overwriting.\n    lags_names : dict\n        Names of the lags of each series.\n    max_lag : int\n        Maximum lag included in `lags`.\n    window_features : list\n        Class or list of classes used to create window features.\n    window_features_names : list\n        Names of the window features to be included in the `X_train` matrix.\n    window_features_class_names : list\n        Names of the classes used to create the window features.\n    max_size_window_features : int\n        Maximum window size required by the window features.\n    window_size : int\n        The window size needed to create the predictors. It is calculated as the \n        maximum value between `max_lag` and `max_size_window_features`. If \n        differentiation is used, `window_size` is increased by n units equal to \n        the order of differentiation so that predictors can be generated correctly.\n    transformer_series : transformer (preprocessor), dict, default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and \n        inverse_transform. Transformation is applied to each `series` before training \n        the forecaster. ColumnTransformers are not allowed since they do not have \n        inverse_transform method.\n\n        - If single transformer: it is cloned and applied to all series. \n        - If `dict` of transformers: a different transformer can be used for each series.\n    transformer_series_ : dict\n        Dictionary with the transformer for each series. It is created cloning the \n        objects in `transformer_series` and is used internally to avoid overwriting.\n    transformer_exog : transformer\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its\n        `fit` method. The resulting `sample_weight` cannot have negative values.\n    source_code_weight_func : str\n        Source code of the custom function used to create weights.\n    differentiation : int\n        Order of differencing applied to the time series before training the \n        forecaster.\n    differentiator : TimeSeriesDifferentiator\n        Skforecast object used to differentiate the time series.\n    differentiator_ : dict\n        Dictionary with the `differentiator` for each series. It is created cloning the\n        objects in `differentiator` and is used internally to avoid overwriting.\n    last_window_ : pandas DataFrame\n        This window represents the most recent data observed by the predictor\n        during its training phase. It contains the values needed to predict the\n        next step immediately after the training data. These values are stored\n        in the original scale of the time series before undergoing any transformations\n        or differentiation. When `differentiation` parameter is specified, the\n        dimensions of the `last_window_` are expanded as many values as the order\n        of differentiation. For example, if `lags` = 7 and `differentiation` = 1,\n        `last_window_` will have 8 values.\n    index_type_ : type\n        Type of index of the input used in training.\n    index_freq_ : str\n        Frequency of Index of the input used in training.\n    training_range_: pandas Index\n        First and last values of index of the data used during training.\n    exog_in_ : bool\n        If the forecaster has been trained using exogenous variable/s.\n    exog_type_in_ : type\n        Type of exogenous variable/s used in training.\n    exog_dtypes_in_ : dict\n        Type of each exogenous variable/s used in training. If `transformer_exog` \n        is used, the dtypes are calculated after the transformation.\n    exog_names_in_ : list\n        Names of the exogenous variables used during training.\n    series_names_in_ : list\n        Names of the series used during training.\n    X_train_series_names_in_ : list\n        Names of the series added to `X_train` when creating the training\n        matrices with `_create_train_X_y` method. It is a subset of \n        `series_names_in_`.\n    X_train_window_features_names_out_ : list\n        Names of the window features included in the matrix `X_train` created\n        internally for training.\n    X_train_exog_names_out_ : list\n        Names of the exogenous variables included in the matrix `X_train` created\n        internally for training. It can be different from `exog_names_in_` if\n        some exogenous variables are transformed during the training process.\n    X_train_direct_exog_names_out_ : list\n        Same as `X_train_exog_names_out_` but using the direct format. The same \n        exogenous variable is repeated for each step.\n    X_train_features_names_out_ : list\n        Names of columns of the matrix created internally for training.\n    fit_kwargs : dict\n        Additional arguments to be passed to the `fit` method of the regressor.\n    in_sample_residuals_ : dict\n        Residuals of the models when predicting training data. Only stored up to\n        1000 values per model in the form `{step: residuals}`. If `transformer_series` \n        is not `None`, residuals are stored in the transformed scale.\n    out_sample_residuals_ : dict\n        Residuals of the models when predicting non training data. Only stored\n        up to 1000 values per model in the form `{step: residuals}`. If `transformer_series` \n        is not `None`, residuals are assumed to be in the transformed scale. Use \n        `set_out_sample_residuals()` method to set values.\n    creation_date : str\n        Date of creation.\n    is_fitted : bool\n        Tag to identify if the regressor has been fitted (trained).\n    fit_date : str\n        Date of last fit.\n    skforecast_version : str\n        Version of skforecast library used to create the forecaster.\n    python_version : str\n        Version of python used to create the forecaster.\n    n_jobs : int, 'auto'\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the fuction\n        skforecast.utils.select_n_jobs_fit_forecaster.\n    forecaster_id : str, int\n        Name used as an identifier of the forecaster.\n    dropna_from_series : Ignored\n        Not used, present here for API consistency by convention.\n    encoding : Ignored\n        Not used, present here for API consistency by convention.\n\n    Notes\n    -----\n    A separate model is created for each forecasting time step. It is important to\n    note that all models share the same parameter and hyperparameter configuration.\n    \n    \"\"\"\n    \n    def __init__(\n        self,\n        regressor: object,\n        level: str,\n        steps: int,\n        lags: Optional[Union[int, list, np.ndarray, range, dict]] = None,\n        window_features: Optional[Union[object, list]] = None,\n        transformer_series: Optional[Union[object, dict]] = StandardScaler(),\n        transformer_exog: Optional[object] = None,\n        weight_func: Optional[Callable] = None,\n        differentiation: Optional[int] = None,\n        fit_kwargs: Optional[dict] = None,\n        n_jobs: Union[int, str] = 'auto',\n        forecaster_id: Optional[Union[str, int]] = None\n    ) -> None:\n        \n        self.regressor                          = copy(regressor)\n        self.level                              = level\n        self.steps                              = steps\n        self.lags_                              = None\n        self.transformer_series                 = transformer_series\n        self.transformer_series_                = None\n        self.transformer_exog                   = transformer_exog\n        self.weight_func                        = weight_func\n        self.source_code_weight_func            = None\n        self.differentiation                    = differentiation\n        self.differentiator                     = None\n        self.differentiator_                    = None\n        self.last_window_                       = None\n        self.index_type_                        = None\n        self.index_freq_                        = None\n        self.training_range_                    = None\n        self.series_names_in_                   = None\n        self.exog_in_                           = False\n        self.exog_names_in_                     = None\n        self.exog_type_in_                      = None\n        self.exog_dtypes_in_                    = None\n        self.X_train_series_names_in_           = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_            = None\n        self.X_train_direct_exog_names_out_     = None\n        self.X_train_features_names_out_        = None\n        self.creation_date                      = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.is_fitted                          = False\n        self.fit_date                           = None\n        self.skforecast_version                 = skforecast.__version__\n        self.python_version                     = sys.version.split(\" \")[0]\n        self.forecaster_id                      = forecaster_id\n        self.dropna_from_series                 = False  # Ignored in this forecaster\n        self.encoding                           = None   # Ignored in this forecaster\n\n        if not isinstance(level, str):\n            raise TypeError(\n                f\"`level` argument must be a str. Got {type(level)}.\"\n            )\n\n        if not isinstance(steps, int):\n            raise TypeError(\n                f\"`steps` argument must be an int greater than or equal to 1. \"\n                f\"Got {type(steps)}.\"\n            )\n\n        if steps < 1:\n            raise ValueError(\n                f\"`steps` argument must be greater than or equal to 1. Got {steps}.\"\n            )\n        \n        self.regressors_ = {step: clone(self.regressor) for step in range(1, steps + 1)}\n\n        if isinstance(lags, dict):\n            self.lags = {}\n            self.lags_names = {}\n            list_max_lags = []\n            for key in lags:\n                if lags[key] is None:\n                    self.lags[key] = None\n                    self.lags_names[key] = None\n                else:\n                    self.lags[key], lags_names, max_lag = initialize_lags(\n                        forecaster_name = type(self).__name__,\n                        lags            = lags[key]\n                    )\n                    self.lags_names[key] = (\n                        [f'{key}_{lag}' for lag in lags_names] \n                         if lags_names is not None \n                         else None\n                    )\n                    if max_lag is not None:\n                        list_max_lags.append(max_lag)\n            \n            self.max_lag = max(list_max_lags) if len(list_max_lags) != 0 else None\n        else:\n            self.lags, self.lags_names, self.max_lag = initialize_lags(\n                forecaster_name = type(self).__name__, \n                lags            = lags\n            )\n\n        self.window_features, self.window_features_names, self.max_size_window_features = (\n            initialize_window_features(window_features)\n        )\n        if self.window_features is None and (self.lags is None or self.max_lag is None):\n            raise ValueError(\n                \"At least one of the arguments `lags` or `window_features` \"\n                \"must be different from None. This is required to create the \"\n                \"predictors used in training the forecaster.\"\n            )\n        \n        self.window_size = max(\n            [ws for ws in [self.max_lag, self.max_size_window_features] \n             if ws is not None]\n        )\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [\n                type(wf).__name__ for wf in self.window_features\n            ]\n\n        if self.differentiation is not None:\n            if not isinstance(differentiation, int) or differentiation < 1:\n                raise ValueError(\n                    f\"Argument `differentiation` must be an integer equal to or \"\n                    f\"greater than 1. Got {differentiation}.\"\n                )\n            self.window_size += self.differentiation\n            self.differentiator = TimeSeriesDifferentiator(\n                order=self.differentiation, window_size=self.window_size\n            )\n            \n        self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n            forecaster_name = type(self).__name__, \n            regressor       = regressor, \n            weight_func     = weight_func, \n            series_weights  = None\n        )\n\n        self.fit_kwargs = check_select_fit_kwargs(\n                              regressor  = regressor,\n                              fit_kwargs = fit_kwargs\n                          )\n\n        self.in_sample_residuals_ = {step: None for step in range(1, steps + 1)}\n        self.out_sample_residuals_ = None\n\n        if n_jobs == 'auto':\n            self.n_jobs = select_n_jobs_fit_forecaster(\n                              forecaster_name = type(self).__name__,\n                              regressor       = self.regressor\n                          )\n        else:\n            if not isinstance(n_jobs, int):\n                raise TypeError(\n                    f\"`n_jobs` must be an integer or `'auto'`. Got {type(n_jobs)}.\"\n                )\n            self.n_jobs = n_jobs if n_jobs > 0 else cpu_count()\n\n    def __repr__(\n        self\n    ) -> str:\n        \"\"\"\n        Information displayed when a ForecasterDirectMultiVariate object is printed.\n        \"\"\"\n        \n        (\n            params,\n            _,\n            series_names_in_,\n            exog_names_in_,\n            transformer_series,\n        ) = [\n            self._format_text_repr(value) \n            for value in self._preprocess_repr(\n                regressor          = self.regressor,\n                series_names_in_   = self.series_names_in_,\n                exog_names_in_     = self.exog_names_in_,\n                transformer_series = self.transformer_series,\n            )\n        ]\n\n        info = (\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"{type(self).__name__} \\n\"\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"Regressor: {type(self.regressor).__name__} \\n\"\n            f\"Target series (level): {self.level} \\n\"\n            f\"Lags: {self.lags} \\n\"\n            f\"Window features: {self.window_features_names} \\n\"\n            f\"Window size: {self.window_size} \\n\"\n            f\"Maximum steps to predict: {self.steps} \\n\"\n            f\"Multivariate series: {series_names_in_} \\n\"\n            f\"Exogenous included: {self.exog_in_} \\n\"\n            f\"Exogenous names: {exog_names_in_} \\n\"\n            f\"Transformer for series: {transformer_series} \\n\"\n            f\"Transformer for exog: {self.transformer_exog} \\n\"\n            f\"Weight function included: {True if self.weight_func is not None else False} \\n\"\n            f\"Differentiation order: {self.differentiation} \\n\"\n            f\"Training range: {self.training_range_.to_list() if self.is_fitted else None} \\n\"\n            f\"Training index type: {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None} \\n\"\n            f\"Training index frequency: {self.index_freq_ if self.is_fitted else None} \\n\"\n            f\"Regressor parameters: {params} \\n\"\n            f\"fit_kwargs: {self.fit_kwargs} \\n\"\n            f\"Creation date: {self.creation_date} \\n\"\n            f\"Last fit date: {self.fit_date} \\n\"\n            f\"Skforecast version: {self.skforecast_version} \\n\"\n            f\"Python version: {self.python_version} \\n\"\n            f\"Forecaster id: {self.forecaster_id} \\n\"\n        )\n\n        return info\n\n\n    def _repr_html_(self):\n        \"\"\"\n        HTML representation of the object.\n        The \"General Information\" section is expanded by default.\n        \"\"\"\n\n        (\n            params,\n            _,\n            series_names_in_,\n            exog_names_in_,\n            transformer_series,\n        ) = self._preprocess_repr(\n                regressor          = self.regressor,\n                series_names_in_   = self.series_names_in_,\n                exog_names_in_     = self.exog_names_in_,\n                transformer_series = self.transformer_series,\n            )\n\n        style, unique_id = self._get_style_repr_html(self.is_fitted)\n        \n        content = f\"\"\"\n        <div class=\"container-{unique_id}\">\n            <h2>{type(self).__name__}</h2>\n            <details open>\n                <summary>General Information</summary>\n                <ul>\n                    <li><strong>Regressor:</strong> {type(self.regressor).__name__}</li>\n                    <li><strong>Target series (level):</strong> {self.level}</li>\n                    <li><strong>Lags:</strong> {self.lags}</li>\n                    <li><strong>Window features:</strong> {self.window_features_names}</li>\n                    <li><strong>Window size:</strong> {self.window_size}</li>\n                    <li><strong>Maximum steps to predict:</strong> {self.steps}</li>\n                    <li><strong>Exogenous included:</strong> {self.exog_in_}</li>\n                    <li><strong>Weight function included:</strong> {self.weight_func is not None}</li>\n                    <li><strong>Differentiation order:</strong> {self.differentiation}</li>\n                    <li><strong>Creation date:</strong> {self.creation_date}</li>\n                    <li><strong>Last fit date:</strong> {self.fit_date}</li>\n                    <li><strong>Skforecast version:</strong> {self.skforecast_version}</li>\n                    <li><strong>Python version:</strong> {self.python_version}</li>\n                    <li><strong>Forecaster id:</strong> {self.forecaster_id}</li>\n                </ul>\n            </details>\n            <details>\n                <summary>Exogenous Variables</summary>\n                <ul>\n                    {exog_names_in_}\n                </ul>\n            </details>\n            <details>\n                <summary>Data Transformations</summary>\n                <ul>\n                    <li><strong>Transformer for series:</strong> {transformer_series}</li>\n                    <li><strong>Transformer for exog:</strong> {self.transformer_exog}</li>\n                </ul>\n            </details>\n            <details>\n                <summary>Training Information</summary>\n                <ul>\n                    <li><strong>Target series (level):</strong> {self.level}</li>\n                    <li><strong>Multivariate series:</strong> {series_names_in_}</li>\n                    <li><strong>Training range:</strong> {self.training_range_.to_list() if self.is_fitted else 'Not fitted'}</li>\n                    <li><strong>Training index type:</strong> {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted'}</li>\n                    <li><strong>Training index frequency:</strong> {self.index_freq_ if self.is_fitted else 'Not fitted'}</li>\n                </ul>\n            </details>\n            <details>\n                <summary>Regressor Parameters</summary>\n                <ul>\n                    {params}\n                </ul>\n            </details>\n            <details>\n                <summary>Fit Kwargs</summary>\n                <ul>\n                    {self.fit_kwargs}\n                </ul>\n            </details>\n            <p>\n                <a href=\"https://skforecast.org/{skforecast.__version__}/api/forecasterdirectmultivariate.html\">&#128712 <strong>API Reference</strong></a>\n                &nbsp;&nbsp;\n                <a href=\"https://skforecast.org/{skforecast.__version__}/user_guides/dependent-multi-series-multivariate-forecasting.html\">&#128462 <strong>User Guide</strong></a>\n            </p>\n        </div>\n        \"\"\"\n\n        # Return the combined style and content\n        return style + content\n\n    \n    def _create_data_to_return_dict(\n        self, \n        series_names_in_: list\n    ) -> Tuple[dict, list]:\n        \"\"\"\n        Create `data_to_return_dict` based on series names and lags configuration.\n        The dictionary contains the information to decide what data to return in \n        the `_create_lags` method.\n        \n        Parameters\n        ----------\n        series_names_in_ : list\n            Names of the series used during training.\n\n        Returns\n        -------\n        data_to_return_dict : dict\n            Dictionary with the information to decide what data to return in the\n            `_create_lags` method.\n        X_train_series_names_in_ : list\n            Names of the series added to `X_train` when creating the training\n            matrices with `_create_train_X_y` method. It is a subset of \n            `series_names_in_`.\n        \n        \"\"\"\n\n        if isinstance(self.lags, dict):\n            lags_keys = list(self.lags.keys())\n            if set(lags_keys) != set(series_names_in_):  # Set to avoid order\n                raise ValueError(\n                    (f\"When `lags` parameter is a `dict`, its keys must be the \"\n                     f\"same as `series` column names. If don't want to include lags, \"\n                      \"add '{column: None}' to the lags dict.\\n\"\n                     f\"  Lags keys        : {lags_keys}.\\n\"\n                     f\"  `series` columns : {series_names_in_}.\")\n                )\n            self.lags_ = copy(self.lags)\n        else:\n            self.lags_ = {serie: self.lags for serie in series_names_in_}\n            if self.lags is not None:\n                # Defined `lags_names` here to avoid overwriting when fit and then create_train_X_y\n                lags_names = [f'lag_{i}' for i in self.lags]\n                self.lags_names = {\n                    serie: [f'{serie}_{lag}' for lag in lags_names]\n                    for serie in series_names_in_\n                }\n            else:\n                self.lags_names = {serie: None for serie in series_names_in_}\n\n        X_train_series_names_in_ = series_names_in_\n        if self.lags is None:\n            data_to_return_dict = {self.level: 'y'}\n        else:\n            # If col is not level and has lags, create 'X' if no lags don't include\n            # If col is level, create 'both' (`X` and `y`)\n            data_to_return_dict = {\n                col: ('both' if col == self.level else 'X')\n                for col in series_names_in_\n                if col == self.level or self.lags_.get(col) is not None\n            }\n\n            # Adjust 'level' in case self.lags_[level] is None\n            if self.lags_.get(self.level) is None:\n                data_to_return_dict[self.level] = 'y'\n\n            if self.window_features is None:\n                # X_train_series_names_in_ include series that will be added to X_train\n                X_train_series_names_in_ = [\n                    col for col in data_to_return_dict.keys()\n                    if data_to_return_dict[col] in ['X', 'both']\n                ]\n\n        return data_to_return_dict, X_train_series_names_in_\n\n\n    def _create_lags(\n        self, \n        y: np.ndarray,\n        lags: np.ndarray,\n        data_to_return: Optional[str] = 'both'\n    ) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n        \"\"\"\n        Create the lagged values and their target variable from a time series.\n        \n        Note that the returned matrix `X_data` contains the lag 1 in the first \n        column, the lag 2 in the in the second column and so on.\n        \n        Parameters\n        ----------\n        y : numpy ndarray\n            Training time series values.\n        lags : numpy ndarray\n            lags to create.\n        data_to_return : str, default 'both'\n            Specifies which data to return. Options are 'X', 'y', 'both' or None.\n\n        Returns\n        -------\n        X_data : numpy ndarray, None\n            Lagged values (predictors).\n        y_data : numpy ndarray, None\n            Values of the time series related to each row of `X_data`.\n        \n        \"\"\"\n\n        X_data = None\n        y_data = None\n        if data_to_return is not None:\n\n            n_rows = len(y) - self.window_size - (self.steps - 1)\n\n            if data_to_return != 'y':\n                # If `data_to_return` is not 'y', it means is 'X' or 'both', X_data is created\n                X_data = np.full(\n                    shape=(n_rows, len(lags)), fill_value=np.nan, order='F', dtype=float\n                )\n                for i, lag in enumerate(lags):\n                    X_data[:, i] = y[self.window_size - lag : -(lag + self.steps - 1)]\n\n            if data_to_return != 'X':\n                # If `data_to_return` is not 'X', it means is 'y' or 'both', y_data is created\n                y_data = np.full(\n                    shape=(n_rows, self.steps), fill_value=np.nan, order='F', dtype=float\n                )\n                for step in range(self.steps):\n                    y_data[:, step] = y[self.window_size + step : self.window_size + step + n_rows]\n        \n        return X_data, y_data\n\n\n    def _create_window_features(\n        self, \n        y: pd.Series,\n        train_index: pd.Index,\n        X_as_pandas: bool = False,\n    ) -> Tuple[list, list]:\n        \"\"\"\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        train_index : pandas Index\n            Index of the training data. It is used to create the pandas DataFrame\n            `X_train_window_features` when `X_as_pandas` is `True`.\n        X_as_pandas : bool, default `False`\n            If `True`, the returned matrix `X_train_window_features` is a \n            pandas DataFrame.\n\n        Returns\n        -------\n        X_train_window_features : list\n            List of numpy ndarrays or pandas DataFrames with the window features.\n        X_train_window_features_names_out_ : list\n            Names of the window features.\n        \n        \"\"\"\n\n        len_train_index = len(train_index)\n        X_train_window_features = []\n        X_train_window_features_names_out_ = []\n        for wf in self.window_features:\n            X_train_wf = wf.transform_batch(y)\n            if not isinstance(X_train_wf, pd.DataFrame):\n                raise TypeError(\n                    (f\"The method `transform_batch` of {type(wf).__name__} \"\n                     f\"must return a pandas DataFrame.\")\n                )\n            X_train_wf = X_train_wf.iloc[-len_train_index:]\n            if not len(X_train_wf) == len_train_index:\n                raise ValueError(\n                    (f\"The method `transform_batch` of {type(wf).__name__} \"\n                     f\"must return a DataFrame with the same number of rows as \"\n                     f\"the input time series - (`window_size` + (`steps` - 1)): {len_train_index}.\")\n                )\n            X_train_wf.index = train_index\n            \n            X_train_wf.columns = [f'{y.name}_{col}' for col in X_train_wf.columns]\n            X_train_window_features_names_out_.extend(X_train_wf.columns)\n            if not X_as_pandas:\n                X_train_wf = X_train_wf.to_numpy()     \n            X_train_window_features.append(X_train_wf)\n\n        return X_train_window_features, X_train_window_features_names_out_\n\n\n    def _create_train_X_y(\n        self,\n        series: pd.DataFrame,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n    ) -> Tuple[pd.DataFrame, dict, list, list, list, list, list, dict]:\n        \"\"\"\n        Create training matrices from multiple time series and exogenous\n        variables. The resulting matrices contain the target variable and predictors\n        needed to train all the regressors (one per step).\n        \n        Parameters\n        ----------\n        series : pandas DataFrame\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `series` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors) for each step. Note that the index \n            corresponds to that of the last step. It is updated for the corresponding \n            step in the filter_train_X_y_for_step method.\n        y_train : dict\n            Values of the time series related to each row of `X_train` for each \n            step in the form {step: y_step_[i]}.\n        series_names_in_ : list\n            Names of the series used during training.\n        X_train_series_names_in_ : list\n            Names of the series added to `X_train` when creating the training\n            matrices with `_create_train_X_y` method. It is a subset of \n            `series_names_in_`.\n        exog_names_in_ : list\n            Names of the exogenous variables included in the training matrices.\n        X_train_exog_names_out_ : list\n            Names of the exogenous variables included in the matrix `X_train` created\n            internally for training. It can be different from `exog_names_in_` if\n            some exogenous variables are transformed during the training process.\n        X_train_features_names_out_ : list\n            Names of the columns of the matrix created internally for training.\n        exog_dtypes_in_ : dict\n            Type of each exogenous variable/s used in training. If `transformer_exog` \n            is used, the dtypes are calculated before the transformation.\n        \n        \"\"\"\n\n        if not isinstance(series, pd.DataFrame):\n            raise TypeError(\n                f\"`series` must be a pandas DataFrame. Got {type(series)}.\"\n            )\n\n        if len(series) < self.window_size + self.steps:\n            raise ValueError(\n                f\"Minimum length of `series` for training this forecaster is \"\n                f\"{self.window_size + self.steps}. Reduce the number of \"\n                f\"predicted steps, {self.steps}, or the maximum \"\n                f\"window_size, {self.window_size}, if no more data is available.\\n\"\n                f\"    Length `series`: {len(series)}.\\n\"\n                f\"    Max step : {self.steps}.\\n\"\n                f\"    Max window size: {self.window_size}.\\n\"\n                f\"    Lags window size: {self.max_lag}.\\n\"\n                f\"    Window features window size: {self.max_size_window_features}.\"\n            )\n        \n        series_names_in_ = list(series.columns)\n\n        if self.level not in series_names_in_:\n            raise ValueError(\n                f\"One of the `series` columns must be named as the `level` of the forecaster.\\n\"\n                f\"  Forecaster `level` : {self.level}.\\n\"\n                f\"  `series` columns   : {series_names_in_}.\"\n            )\n\n        data_to_return_dict, X_train_series_names_in_ = (\n            self._create_data_to_return_dict(series_names_in_=series_names_in_)\n        )\n\n        series_to_create_autoreg_features_and_y = [\n            col for col in series_names_in_ \n            if col in X_train_series_names_in_ + [self.level]\n        ]\n\n        fit_transformer = False\n        if not self.is_fitted:\n            fit_transformer = True\n            self.transformer_series_ = initialize_transformer_series(\n                                           forecaster_name    = type(self).__name__,\n                                           series_names_in_   = series_to_create_autoreg_features_and_y,\n                                           transformer_series = self.transformer_series\n                                       )\n\n        if self.differentiation is None:\n            self.differentiator_ = {\n                serie: None for serie in series_to_create_autoreg_features_and_y\n            }\n        else:\n            if not self.is_fitted:\n                self.differentiator_ = {\n                    serie: copy(self.differentiator)\n                    for serie in series_to_create_autoreg_features_and_y\n                }\n\n        exog_names_in_ = None\n        exog_dtypes_in_ = None\n        categorical_features = False\n        if exog is not None:\n            check_exog(exog=exog, allow_nan=True)\n            exog = input_to_frame(data=exog, input_name='exog')\n            \n            series_index_no_ws = series.index[self.window_size:]\n            len_series = len(series)\n            len_series_no_ws = len_series - self.window_size\n            len_exog = len(exog)\n            if not len_exog == len_series and not len_exog == len_series_no_ws:\n                raise ValueError(\n                    f\"Length of `exog` must be equal to the length of `series` (if \"\n                    f\"index is fully aligned) or length of `seriesy` - `window_size` \"\n                    f\"(if `exog` starts after the first `window_size` values).\\n\"\n                    f\"    `exog`                   : ({exog.index[0]} -- {exog.index[-1]})  (n={len_exog})\\n\"\n                    f\"    `series`                 : ({series.index[0]} -- {series.index[-1]})  (n={len_series})\\n\"\n                    f\"    `series` - `window_size` : ({series_index_no_ws[0]} -- {series_index_no_ws[-1]})  (n={len_series_no_ws})\"\n                )\n            \n            exog_names_in_ = exog.columns.to_list()\n            if len(set(exog_names_in_) - set(series_names_in_)) != len(exog_names_in_):\n                raise ValueError(\n                    f\"`exog` cannot contain a column named the same as one of \"\n                    f\"the series (column names of series).\\n\"\n                    f\"  `series` columns : {series_names_in_}.\\n\"\n                    f\"  `exog`   columns : {exog_names_in_}.\"\n                )\n            \n            # NOTE: Need here for filter_train_X_y_for_step to work without fitting\n            self.exog_in_ = True\n            exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = fit_transformer,\n                       inverse_transform = False\n                   )\n                \n            check_exog_dtypes(exog, call_check_exog=True)\n            categorical_features = (\n                exog.select_dtypes(include=np.number).shape[1] != exog.shape[1]\n            )\n\n            # Use .index as series.index is not yet preprocessed with preprocess_y\n            if len_exog == len_series:\n                if not (exog.index == series.index).all():\n                    raise ValueError(\n                        \"When `exog` has the same length as `series`, the index \"\n                        \"of `exog` must be aligned with the index of `series` \"\n                        \"to ensure the correct alignment of values.\"\n                    )\n                # The first `self.window_size` positions have to be removed from \n                # exog since they are not in X_train.\n                exog = exog.iloc[self.window_size:, ]\n            else:\n                if not (exog.index == series_index_no_ws).all():\n                    raise ValueError(\n                        \"When `exog` doesn't contain the first `window_size` \"\n                        \"observations, the index of `exog` must be aligned with \"\n                        \"the index of `series` minus the first `window_size` \"\n                        \"observations to ensure the correct alignment of values.\"\n                    )\n\n        X_train_autoreg = []\n        X_train_window_features_names_out_ = [] if self.window_features is not None else None\n        X_train_features_names_out_ = []\n        for col in series_to_create_autoreg_features_and_y:\n            y = series[col]\n            check_y(y=y, series_id=f\"Column '{col}'\")\n            y = transform_series(\n                    series            = y,\n                    transformer       = self.transformer_series_[col],\n                    fit               = fit_transformer,\n                    inverse_transform = False\n                )\n            y_values, y_index = preprocess_y(y=y)\n\n            if self.differentiation is not None:\n                if not self.is_fitted:\n                    y_values = self.differentiator_[col].fit_transform(y_values)\n                else:\n                    differentiator = copy(self.differentiator_[col])\n                    y_values = differentiator.fit_transform(y_values)\n\n            X_train_autoreg_col = []\n            train_index = y_index[self.window_size + (self.steps - 1):]\n\n            X_train_lags, y_train_values = self._create_lags(\n                y=y_values, lags=self.lags_[col], data_to_return=data_to_return_dict.get(col, None)\n            )\n            if X_train_lags is not None:\n                X_train_autoreg_col.append(X_train_lags)\n                X_train_features_names_out_.extend(self.lags_names[col])\n\n            if col == self.level:\n                y_train = y_train_values\n\n            if self.window_features is not None:\n                n_diff = 0 if self.differentiation is None else self.differentiation\n                end_wf = None if self.steps == 1 else -(self.steps - 1)\n                y_window_features = pd.Series(\n                    y_values[n_diff:end_wf], index=y_index[n_diff:end_wf], name=col\n                )\n                X_train_window_features, X_train_wf_names_out_ = (\n                    self._create_window_features(\n                        y=y_window_features, X_as_pandas=False, train_index=train_index\n                    )\n                )\n                X_train_autoreg_col.extend(X_train_window_features)\n                X_train_window_features_names_out_.extend(X_train_wf_names_out_)\n                X_train_features_names_out_.extend(X_train_wf_names_out_)\n\n            if X_train_autoreg_col:\n                if len(X_train_autoreg_col) == 1:\n                    X_train_autoreg_col = X_train_autoreg_col[0]\n                else:\n                    X_train_autoreg_col = np.concatenate(X_train_autoreg_col, axis=1)\n\n                X_train_autoreg.append(X_train_autoreg_col)\n\n        X_train = []\n        len_train_index = len(train_index)\n        if categorical_features:\n            if len(X_train_autoreg) == 1:\n                X_train_autoreg = X_train_autoreg[0]\n            else:\n                X_train_autoreg = np.concatenate(X_train_autoreg, axis=1)\n            X_train_autoreg = pd.DataFrame(\n                                  data    = X_train_autoreg,\n                                  columns = X_train_features_names_out_,\n                                  index   = train_index\n                              )\n            X_train.append(X_train_autoreg)\n        else:\n            X_train.extend(X_train_autoreg)\n\n        # NOTE: Need here for filter_train_X_y_for_step to work without fitting\n        self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n\n        X_train_exog_names_out_ = None\n        if exog is not None:\n            X_train_exog_names_out_ = exog.columns.to_list()\n            if categorical_features:\n                exog_direct, X_train_direct_exog_names_out_ = exog_to_direct(\n                    exog=exog, steps=self.steps\n                )\n                exog_direct.index = train_index\n            else:\n                exog_direct, X_train_direct_exog_names_out_ = exog_to_direct_numpy(\n                    exog=exog, steps=self.steps\n                )\n\n            # NOTE: Need here for filter_train_X_y_for_step to work without fitting\n            self.X_train_direct_exog_names_out_ = X_train_direct_exog_names_out_\n\n            X_train_features_names_out_.extend(self.X_train_direct_exog_names_out_)\n            X_train.append(exog_direct)\n        \n        if len(X_train) == 1:\n            X_train = X_train[0]\n        else:\n            if categorical_features:\n                X_train = pd.concat(X_train, axis=1)\n            else:\n                X_train = np.concatenate(X_train, axis=1)\n                \n        if categorical_features:\n            X_train.index = train_index\n        else:\n            X_train = pd.DataFrame(\n                          data    = X_train,\n                          index   = train_index,\n                          columns = X_train_features_names_out_\n                      )\n\n        y_train = {\n            step: pd.Series(\n                      data  = y_train[:, step - 1], \n                      index = y_index[self.window_size + step - 1:][:len_train_index],\n                      name  = f\"{self.level}_step_{step}\"\n                  )\n            for step in range(1, self.steps + 1)\n        }\n\n        return (\n            X_train,\n            y_train,\n            series_names_in_,\n            X_train_series_names_in_,\n            exog_names_in_,\n            X_train_exog_names_out_,\n            X_train_features_names_out_,\n            exog_dtypes_in_\n        )\n\n\n    def create_train_X_y(\n        self,\n        series: pd.DataFrame,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        suppress_warnings: bool = False\n    ) -> Tuple[pd.DataFrame, dict]:\n        \"\"\"\n        Create training matrices from multiple time series and exogenous\n        variables. The resulting matrices contain the target variable and predictors\n        needed to train all the regressors (one per step).\n        \n        Parameters\n        ----------\n        series : pandas DataFrame\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `series` and their indexes must be aligned.\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the creation\n            of the training matrices. See skforecast.exceptions.warn_skforecast_categories \n            for more information.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors) for each step. Note that the index \n            corresponds to that of the last step. It is updated for the corresponding \n            step in the filter_train_X_y_for_step method.\n        y_train : dict\n            Values of the time series related to each row of `X_train` for each \n            step in the form {step: y_step_[i]}.\n        \n        \"\"\"\n\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n\n        output = self._create_train_X_y(\n                     series = series, \n                     exog   = exog\n                 )\n\n        X_train = output[0]\n        y_train = output[1]\n        \n        set_skforecast_warnings(suppress_warnings, action='default')\n\n        return X_train, y_train\n\n\n    def filter_train_X_y_for_step(\n        self,\n        step: int,\n        X_train: pd.DataFrame,\n        y_train: dict,\n        remove_suffix: bool = False\n    ) -> Tuple[pd.DataFrame, pd.Series]:\n        \"\"\"\n        Select the columns needed to train a forecaster for a specific step.  \n        The input matrices should be created using `_create_train_X_y` method. \n        This method updates the index of `X_train` to the corresponding one \n        according to `y_train`. If `remove_suffix=True` the suffix \"_step_i\" \n        will be removed from the column names. \n\n        Parameters\n        ----------\n        step : int\n            step for which columns must be selected selected. Starts at 1.\n        X_train : pandas DataFrame\n            Dataframe created with the `_create_train_X_y` method, first return.\n        y_train : dict\n            Dict created with the `_create_train_X_y` method, second return.\n        remove_suffix : bool, default `False`\n            If True, suffix \"_step_i\" is removed from the column names.\n\n        Returns\n        -------\n        X_train_step : pandas DataFrame\n            Training values (predictors) for the selected step.\n        y_train_step : pandas Series\n            Values of the time series related to each row of `X_train`.\n\n        \"\"\"\n\n        if (step < 1) or (step > self.steps):\n            raise ValueError(\n                (f\"Invalid value `step`. For this forecaster, minimum value is 1 \"\n                 f\"and the maximum step is {self.steps}.\")\n            )\n\n        y_train_step = y_train[step]\n\n        # Matrix X_train starts at index 0.\n        if not self.exog_in_:\n            X_train_step = X_train\n        else:\n            n_lags = len(list(\n                chain(*[v for v in self.lags_.values() if v is not None])\n            ))\n            n_window_features = (\n                len(self.X_train_window_features_names_out_) if self.window_features is not None else 0\n            )\n            idx_columns_autoreg = np.arange(n_lags + n_window_features)\n            n_exog = len(self.X_train_direct_exog_names_out_) / self.steps\n            idx_columns_exog = (\n                np.arange((step - 1) * n_exog, (step) * n_exog) + idx_columns_autoreg[-1] + 1 \n            )\n            idx_columns = np.concatenate((idx_columns_autoreg, idx_columns_exog))\n            X_train_step = X_train.iloc[:, idx_columns]\n\n        X_train_step.index = y_train_step.index\n\n        if remove_suffix:\n            X_train_step.columns = [\n                col_name.replace(f\"_step_{step}\", \"\")\n                for col_name in X_train_step.columns\n            ]\n            y_train_step.name = y_train_step.name.replace(f\"_step_{step}\", \"\")\n\n        return X_train_step, y_train_step\n\n\n    def _train_test_split_one_step_ahead(\n        self,\n        series: pd.DataFrame,\n        initial_train_size: int,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n    ) -> Tuple[pd.DataFrame, dict, pd.DataFrame, dict, pd.Series, pd.Series]:\n        \"\"\"\n        Create matrices needed to train and test the forecaster for one-step-ahead\n        predictions.\n        \n        Parameters\n        ----------\n        series : pandas DataFrame\n            Training time series.\n        initial_train_size : int\n            Initial size of the training set. It is the number of observations used\n            to train the forecaster before making the first prediction.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `series` and their indexes must be aligned.\n        \n        Returns\n        -------\n        X_train : pandas DataFrame\n            Predictor values used to train the model.\n        y_train : dict\n            Values of the time series related to each row of `X_train` for each \n            step in the form {step: y_step_[i]}.\n        X_test : pandas DataFrame\n            Predictor values used to test the model.\n        y_test : dict\n            Values of the time series related to each row of `X_test` for each \n            step in the form {step: y_step_[i]}.\n        X_train_encoding : pandas Series\n            Series identifiers for each row of `X_train`.\n        X_test_encoding : pandas Series\n            Series identifiers for each row of `X_test`.\n        \n        \"\"\"\n\n        span_index = series.index\n\n        fold = [\n            [0, initial_train_size],\n            [initial_train_size - self.window_size, initial_train_size],\n            [initial_train_size - self.window_size, len(span_index)],\n            [0, 0],  # Dummy value\n            True\n        ]\n        data_fold = _extract_data_folds_multiseries(\n                        series             = series,\n                        folds              = [fold],\n                        span_index         = span_index,\n                        window_size        = self.window_size,\n                        exog               = exog,\n                        dropna_last_window = self.dropna_from_series,\n                        externally_fitted  = False\n                    )\n        series_train, _, levels_last_window, exog_train, exog_test, _ = next(data_fold)\n\n        start_test_idx = initial_train_size - self.window_size\n        series_test = series.iloc[start_test_idx:, :]\n        series_test = series_test.loc[:, levels_last_window]\n        series_test = series_test.dropna(axis=1, how='all')\n       \n        _is_fitted = self.is_fitted\n        _series_names_in_ = self.series_names_in_\n        _exog_names_in_ = self.exog_names_in_\n\n        self.is_fitted = False\n        X_train, y_train, series_names_in_, _, exog_names_in_, *_ = (\n            self._create_train_X_y(\n                series = series_train,\n                exog   = exog_train,\n            )\n        )\n        self.series_names_in_ = series_names_in_\n        if exog is not None:\n            self.exog_names_in_ = exog_names_in_\n        self.is_fitted = True\n\n        X_test, y_test, *_ = self._create_train_X_y(\n                                 series = series_test,\n                                 exog   = exog_test,\n                             )\n        self.is_fitted = _is_fitted\n        self.series_names_in_ = _series_names_in_\n        self.exog_names_in_ = _exog_names_in_\n\n        X_train_encoding = pd.Series(self.level, index=X_train.index)\n        X_test_encoding = pd.Series(self.level, index=X_test.index)\n\n        return X_train, y_train, X_test, y_test, X_train_encoding, X_test_encoding\n\n\n    def create_sample_weights(\n        self,\n        X_train: pd.DataFrame\n    ) -> np.ndarray:\n        \"\"\"\n        Crate weights for each observation according to the forecaster's attribute\n        `weight_func`. \n\n        Parameters\n        ----------\n        X_train : pandas DataFrame\n            Dataframe created with `_create_train_X_y` and filter_train_X_y_for_step`\n            methods, first return.\n\n        Returns\n        -------\n        sample_weight : numpy ndarray\n            Weights to use in `fit` method.\n        \n        \"\"\"\n\n        sample_weight = None\n\n        if self.weight_func is not None:\n            sample_weight = self.weight_func(X_train.index)\n\n        if sample_weight is not None:\n            if np.isnan(sample_weight).any():\n                raise ValueError(\n                    \"The resulting `sample_weight` cannot have NaN values.\"\n                )\n            if np.any(sample_weight < 0):\n                raise ValueError(\n                    \"The resulting `sample_weight` cannot have negative values.\"\n                )\n            if np.sum(sample_weight) == 0:\n                raise ValueError(\n                    (\"The resulting `sample_weight` cannot be normalized because \"\n                     \"the sum of the weights is zero.\")\n                )\n\n        return sample_weight\n\n\n    def fit(\n        self,\n        series: pd.DataFrame,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        store_last_window: bool = True,\n        store_in_sample_residuals: bool = True,\n        suppress_warnings: bool = False\n    ) -> None:\n        \"\"\"\n        Training Forecaster.\n\n        Additional arguments to be passed to the `fit` method of the regressor \n        can be added with the `fit_kwargs` argument when initializing the forecaster.\n\n        Parameters\n        ----------\n        series : pandas DataFrame\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `series` and their indexes must be aligned so\n            that series[i] is regressed on exog[i].\n        store_last_window : bool, default `True`\n            Whether or not to store the last window (`last_window_`) of training data.\n        store_in_sample_residuals : bool, default `True`\n            If `True`, in-sample residuals will be stored in the forecaster object\n            after fitting (`in_sample_residuals_` attribute).\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the training \n            process. See skforecast.exceptions.warn_skforecast_categories for more\n            information.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n        \n        # Reset values in case the forecaster has already been fitted.\n        self.lags_                              = None\n        self.last_window_                       = None\n        self.index_type_                        = None\n        self.index_freq_                        = None\n        self.training_range_                    = None\n        self.series_names_in_                   = None\n        self.exog_in_                           = False\n        self.exog_names_in_                     = None\n        self.exog_type_in_                      = None\n        self.exog_dtypes_in_                    = None\n        self.X_train_series_names_in_           = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_            = None\n        self.X_train_direct_exog_names_out_     = None\n        self.X_train_features_names_out_        = None\n        self.in_sample_residuals_               = {step: None for step in range(1, self.steps + 1)}\n        self.is_fitted                          = False\n        self.fit_date                           = None\n\n        (\n            X_train,\n            y_train,\n            series_names_in_,\n            X_train_series_names_in_,\n            exog_names_in_,\n            X_train_exog_names_out_,\n            X_train_features_names_out_,\n            exog_dtypes_in_\n        ) = self._create_train_X_y(series=series, exog=exog)\n\n        def fit_forecaster(regressor, X_train, y_train, step, store_in_sample_residuals):\n            \"\"\"\n            Auxiliary function to fit each of the forecaster's regressors in parallel.\n\n            Parameters\n            ----------\n            regressor : object\n                Regressor to be fitted.\n            X_train : pandas DataFrame\n                Dataframe created with the `_create_train_X_y` method, first return.\n            y_train : dict\n                Dict created with the `_create_train_X_y` method, second return.\n            step : int\n                Step of the forecaster to be fitted.\n            store_in_sample_residuals : bool\n                If `True`, in-sample residuals will be stored in the forecaster object\n                after fitting (`in_sample_residuals_` attribute).\n            \n            Returns\n            -------\n            Tuple with the step, fitted regressor and in-sample residuals.\n\n            \"\"\"\n\n            X_train_step, y_train_step = self.filter_train_X_y_for_step(\n                                             step          = step,\n                                             X_train       = X_train,\n                                             y_train       = y_train,\n                                             remove_suffix = True\n                                         )\n            sample_weight = self.create_sample_weights(X_train=X_train_step)\n            if sample_weight is not None:\n                regressor.fit(\n                    X             = X_train_step,\n                    y             = y_train_step,\n                    sample_weight = sample_weight,\n                    **self.fit_kwargs\n                )\n            else:\n                regressor.fit(\n                    X = X_train_step,\n                    y = y_train_step,\n                    **self.fit_kwargs\n                )\n\n            # This is done to save time during fit in functions such as backtesting()\n            if store_in_sample_residuals:\n                residuals = (\n                    (y_train_step - regressor.predict(X_train_step))\n                ).to_numpy()\n\n                if len(residuals) > 1000:\n                    # Only up to 1000 residuals are stored\n                    rng = np.random.default_rng(seed=123)\n                    residuals = rng.choice(\n                                    a       = residuals, \n                                    size    = 1000, \n                                    replace = False\n                                )\n            else:\n                residuals = None\n\n            return step, regressor, residuals\n\n        results_fit = (\n            Parallel(n_jobs=self.n_jobs)\n            (delayed(fit_forecaster)\n            (\n                regressor                 = copy(self.regressor),\n                X_train                   = X_train,\n                y_train                   = y_train,\n                step                      = step,\n                store_in_sample_residuals = store_in_sample_residuals\n            )\n            for step in range(1, self.steps + 1))\n        )\n\n        self.regressors_ = {step: regressor \n                            for step, regressor, _ in results_fit}\n\n        if store_in_sample_residuals:\n            self.in_sample_residuals_ = {step: residuals \n                                         for step, _, residuals in results_fit}\n        \n        self.series_names_in_ = series_names_in_\n        self.X_train_series_names_in_ = X_train_series_names_in_\n        self.X_train_features_names_out_ = X_train_features_names_out_\n        \n        self.is_fitted = True\n        self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.training_range_ = preprocess_y(y=series[self.level], return_values=False)[1][[0, -1]]\n        self.index_type_ = type(X_train.index)\n        if isinstance(X_train.index, pd.DatetimeIndex):\n            self.index_freq_ = X_train.index.freqstr\n        else: \n            self.index_freq_ = X_train.index.step\n        \n        if exog is not None:\n            self.exog_in_ = True\n            self.exog_names_in_ = exog_names_in_\n            self.exog_type_in_ = type(exog)\n            self.exog_dtypes_in_ = exog_dtypes_in_\n            self.X_train_exog_names_out_ = X_train_exog_names_out_\n\n        if store_last_window:\n            self.last_window_ = series.iloc[-self.window_size:, ][\n                self.X_train_series_names_in_\n            ].copy()\n        \n        set_skforecast_warnings(suppress_warnings, action='default')\n\n\n    def _create_predict_inputs(\n        self,\n        steps: Optional[Union[int, list]] = None,\n        last_window: Optional[pd.DataFrame] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        check_inputs: bool = True\n    ) -> Tuple[list, list, list, pd.Index]:\n        \"\"\"\n        Create the inputs needed for the prediction process.\n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n\n        Returns\n        -------\n        Xs : list\n            List of numpy arrays with the predictors for each step.\n        Xs_col_names : list\n            Names of the columns of the matrix created internally for prediction.\n        steps : list\n            Steps to predict.\n        prediction_index : pandas Index\n            Index of the predictions.\n        \n        \"\"\"\n        \n        steps = prepare_steps_direct(\n                    steps    = steps,\n                    max_step = self.steps\n                )\n\n        if last_window is None:\n            last_window = self.last_window_\n        \n        if check_inputs:\n            check_predict_input(\n                forecaster_name  = type(self).__name__,\n                steps            = steps,\n                is_fitted        = self.is_fitted,\n                exog_in_         = self.exog_in_,\n                index_type_      = self.index_type_,\n                index_freq_      = self.index_freq_,\n                window_size      = self.window_size,\n                last_window      = last_window,\n                exog             = exog,\n                exog_type_in_    = self.exog_type_in_,\n                exog_names_in_   = self.exog_names_in_,\n                interval         = None,\n                max_steps        = self.steps,\n                series_names_in_ = self.X_train_series_names_in_\n            )\n\n        last_window = last_window.iloc[\n            -self.window_size:, last_window.columns.get_indexer(self.X_train_series_names_in_)\n        ].copy()\n        \n        X_autoreg = []\n        Xs_col_names = []\n        for serie in self.X_train_series_names_in_:\n            last_window_serie = transform_numpy(\n                                    array             = last_window[serie].to_numpy(),\n                                    transformer       = self.transformer_series_[serie],\n                                    fit               = False,\n                                    inverse_transform = False\n                                )\n            \n            if self.differentiation is not None:\n                last_window_serie = self.differentiator_[serie].fit_transform(last_window_serie)\n\n            if self.lags is not None:\n                X_lags = last_window_serie[-self.lags_[serie]]\n                X_autoreg.append(X_lags)\n                Xs_col_names.extend(self.lags_names[serie])\n\n            if self.window_features is not None:\n                n_diff = 0 if self.differentiation is None else self.differentiation\n                X_window_features = np.concatenate(\n                    [\n                        wf.transform(last_window_serie[n_diff:]) \n                        for wf in self.window_features\n                    ]\n                )\n                X_autoreg.append(X_window_features)\n                # HACK: This is not the best way to do it. Can have any problem\n                # if the window_features are not in the same order as the\n                # self.window_features_names.\n                Xs_col_names.extend([f\"{serie}_{wf}\" for wf in self.window_features_names])\n            \n        X_autoreg = np.concatenate(X_autoreg).reshape(1, -1)\n        _, last_window_index = preprocess_last_window(\n            last_window=last_window, return_values=False\n        )\n        if exog is not None:\n            exog = input_to_frame(data=exog, input_name='exog')\n            exog = exog.loc[:, self.exog_names_in_]\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n            check_exog_dtypes(exog=exog)\n            exog_values, _ = exog_to_direct_numpy(\n                                 exog  = exog.to_numpy()[:max(steps)],\n                                 steps = max(steps)\n                             )\n            exog_values = exog_values[0]\n            \n            n_exog = exog.shape[1]\n            Xs = [\n                np.concatenate(\n                    [\n                        X_autoreg, \n                        exog_values[(step - 1) * n_exog : step * n_exog].reshape(1, -1)\n                    ],\n                    axis=1\n                )\n                for step in steps\n            ]\n            # HACK: This is not the best way to do it. Can have any problem\n            # if the exog_columns are not in the same order as the\n            # self.window_features_names.\n            Xs_col_names = Xs_col_names + exog.columns.to_list()\n        else:\n            Xs = [X_autoreg] * len(steps)\n\n        prediction_index = expand_index(\n                               index = last_window_index,\n                               steps = max(steps)\n                           )[np.array(steps) - 1]\n        if isinstance(last_window_index, pd.DatetimeIndex) and np.array_equal(\n            steps, np.arange(min(steps), max(steps) + 1)\n        ):\n            prediction_index.freq = last_window_index.freq\n        \n        # HACK: Why no use self.X_train_features_names_out_ as Xs_col_names?\n        return Xs, Xs_col_names, steps, prediction_index\n\n\n    def create_predict_X(\n        self,\n        steps: Optional[Union[int, list]] = None,\n        last_window: Optional[pd.DataFrame] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        suppress_warnings: bool = False\n    ) -> pd.DataFrame:\n        \"\"\"\n        Create the predictors needed to predict `steps` ahead.\n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the prediction \n            process. See skforecast.exceptions.warn_skforecast_categories for more\n            information.\n\n        Returns\n        -------\n        X_predict : pandas DataFrame\n            Pandas DataFrame with the predictors for each step. The index \n            is the same as the prediction index.\n        \n        \"\"\"\n\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n\n        Xs, Xs_col_names, steps, prediction_index = self._create_predict_inputs(\n            steps=steps, last_window=last_window, exog=exog\n        )\n\n        X_predict = pd.DataFrame(\n                        data    = np.concatenate(Xs, axis=0), \n                        columns = Xs_col_names, \n                        index   = prediction_index\n                    )\n        \n        if self.transformer_series is not None or self.differentiation is not None:\n            warnings.warn(\n                \"The output matrix is in the transformed scale due to the \"\n                \"inclusion of transformations or differentiation in the Forecaster. \"\n                \"As a result, any predictions generated using this matrix will also \"\n                \"be in the transformed scale. Please refer to the documentation \"\n                \"for more details: \"\n                \"https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html\",\n                DataTransformationWarning\n            )\n        \n        set_skforecast_warnings(suppress_warnings, action='default')\n\n        return X_predict\n\n\n    def predict(\n        self,\n        steps: Optional[Union[int, list]] = None,\n        last_window: Optional[pd.DataFrame] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        suppress_warnings: bool = False,\n        check_inputs: bool = True,\n        levels: Any = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        Predict n steps ahead\n\n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the prediction \n            process. See skforecast.exceptions.warn_skforecast_categories for more\n            information.\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n        levels : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Predicted values.\n\n        \"\"\"\n\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n\n        Xs, _, steps, prediction_index = self._create_predict_inputs(\n            steps=steps, last_window=last_window, exog=exog, check_inputs=check_inputs\n        )\n\n        regressors = [self.regressors_[step] for step in steps]\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\", \n                message=\"X does not have valid feature names\", \n                category=UserWarning\n            )\n            predictions = np.array([\n                regressor.predict(X).ravel()[0] \n                for regressor, X in zip(regressors, Xs)\n            ])\n\n        if self.differentiation is not None:\n            predictions = self.differentiator_[\n                self.level\n            ].inverse_transform_next_window(predictions)\n        \n        predictions = transform_numpy(\n                          array             = predictions,\n                          transformer       = self.transformer_series_[self.level],\n                          fit               = False,\n                          inverse_transform = True\n                      )\n            \n        predictions = pd.DataFrame(\n                          data    = predictions,\n                          columns = [self.level],\n                          index   = prediction_index\n                      )\n        \n        set_skforecast_warnings(suppress_warnings, action='default')\n\n        return predictions\n\n\n    def predict_bootstrapping(\n        self,\n        steps: Optional[Union[int, list]] = None,\n        last_window: Optional[pd.DataFrame] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        suppress_warnings: bool = False,\n        levels: Any = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        Generate multiple forecasting predictions using a bootstrapping process. \n        By sampling from a collection of past observed errors (the residuals),\n        each iteration of bootstrapping generates a different set of predictions. \n        See the Notes section for more information. \n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.     \n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.               \n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the prediction \n            process. See skforecast.exceptions.warn_skforecast_categories for more\n            information.\n        levels : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        boot_predictions : pandas DataFrame\n            Predictions generated by bootstrapping.\n            Shape: (steps, n_boot)\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n        Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n        \"\"\"\n\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n\n        if self.is_fitted:\n            \n            steps = prepare_steps_direct(\n                        steps    = steps,\n                        max_step = self.steps\n                    )\n\n            if use_in_sample_residuals:\n                if not set(steps).issubset(set(self.in_sample_residuals_.keys())):\n                    raise ValueError(\n                        f\"Not `forecaster.in_sample_residuals_` for steps: \"\n                        f\"{set(steps) - set(self.in_sample_residuals_.keys())}.\"\n                    )\n                residuals = self.in_sample_residuals_\n            else:\n                if self.out_sample_residuals_ is None:\n                    raise ValueError(\n                        \"`forecaster.out_sample_residuals_` is `None`. Use \"\n                        \"`use_in_sample_residuals=True` or the \"\n                        \"`set_out_sample_residuals()` method before predicting.\"\n                    )\n                else:\n                    if not set(steps).issubset(set(self.out_sample_residuals_.keys())):\n                        raise ValueError(\n                            f\"Not `forecaster.out_sample_residuals_` for steps: \"\n                            f\"{set(steps) - set(self.out_sample_residuals_.keys())}. \"\n                            f\"Use method `set_out_sample_residuals()`.\"\n                        )\n                residuals = self.out_sample_residuals_\n            \n            check_residuals = (\n                'forecaster.in_sample_residuals_' if use_in_sample_residuals\n                else 'forecaster.out_sample_residuals_'\n            )\n            for step in steps:\n                if residuals[step] is None:\n                    raise ValueError(\n                        f\"forecaster residuals for step {step} are `None`. \"\n                        f\"Check {check_residuals}.\"\n                    )\n                elif (any(element is None for element in residuals[step]) or\n                      np.any(np.isnan(residuals[step]))):\n                    raise ValueError(\n                        f\"forecaster residuals for step {step} contains `None` \"\n                        f\"or `NaNs` values. Check {check_residuals}.\"\n                    )\n\n        Xs, _, steps, prediction_index = self._create_predict_inputs(\n            steps=steps, last_window=last_window, exog=exog\n        )\n\n        # NOTE: Predictions must be transformed and differenced before adding residuals\n        regressors = [self.regressors_[step] for step in steps]\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\", \n                message=\"X does not have valid feature names\", \n                category=UserWarning\n            )\n            predictions = np.array([\n                regressor.predict(X).ravel()[0] \n                for regressor, X in zip(regressors, Xs)\n            ])\n        \n        boot_predictions = np.tile(predictions, (n_boot, 1)).T\n        boot_columns = [f\"pred_boot_{i}\" for i in range(n_boot)]\n\n        rng = np.random.default_rng(seed=random_state)\n        for i, step in enumerate(steps):\n            sampled_residuals = residuals[step][\n                rng.integers(low=0, high=len(residuals[step]), size=n_boot)\n            ]\n            boot_predictions[i, :] = boot_predictions[i, :] + sampled_residuals\n\n        if self.differentiation is not None:\n            boot_predictions = self.differentiator_[\n                self.level\n            ].inverse_transform_next_window(boot_predictions)\n\n        if self.transformer_series_[self.level]:\n            boot_predictions = np.apply_along_axis(\n                                   func1d            = transform_numpy,\n                                   axis              = 0,\n                                   arr               = boot_predictions,\n                                   transformer       = self.transformer_series_[self.level],\n                                   fit               = False,\n                                   inverse_transform = True\n                               )\n    \n        boot_predictions = pd.DataFrame(\n                               data    = boot_predictions,\n                               index   = prediction_index,\n                               columns = boot_columns\n                           )\n\n        set_skforecast_warnings(suppress_warnings, action='default')\n        \n        return boot_predictions\n\n\n    def predict_interval(\n        self,\n        steps: Optional[Union[int, list]] = None,\n        last_window: Optional[pd.DataFrame] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        interval: list = [5, 95],\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        suppress_warnings: bool = False,\n        levels: Any = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        Bootstrapping based predicted intervals.\n        Both predictions and intervals are returned.\n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        interval : list, default `[5, 95]`\n            Confidence of the prediction interval estimated. Sequence of \n            percentiles to compute, which must be between 0 and 100 inclusive. \n            For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the prediction \n            process. See skforecast.exceptions.warn_skforecast_categories for more\n            information.\n        levels : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Values predicted by the forecaster and their estimated interval.\n\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n\n        check_interval(interval=interval)\n\n        boot_predictions = self.predict_bootstrapping(\n                               steps                   = steps,\n                               last_window             = last_window,\n                               exog                    = exog,\n                               n_boot                  = n_boot,\n                               random_state            = random_state,\n                               use_in_sample_residuals = use_in_sample_residuals\n                           )\n\n        predictions = self.predict(\n                          steps        = steps,\n                          last_window  = last_window,\n                          exog         = exog,\n                          check_inputs = False\n                      )\n\n        interval = np.array(interval) / 100\n        predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n        predictions_interval.columns = [f'{self.level}_lower_bound', f'{self.level}_upper_bound']\n        predictions = pd.concat((predictions, predictions_interval), axis=1)\n\n        set_skforecast_warnings(suppress_warnings, action='default')\n\n        return predictions\n\n\n    def predict_quantiles(\n        self,\n        steps: Optional[Union[int, list]] = None,\n        last_window: Optional[pd.DataFrame] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        quantiles: list = [0.05, 0.5, 0.95],\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        suppress_warnings: bool = False,\n        levels: Any = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        Bootstrapping based predicted quantiles.\n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        quantiles : list, default `[0.05, 0.5, 0.95]`\n            Sequence of quantiles to compute, which must be between 0 and 1 \n            inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n            `quantiles = [0.05, 0.5, 0.95]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate quantiles.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot quantiles are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create quantiles. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the prediction \n            process. See skforecast.exceptions.warn_skforecast_categories for more\n            information.\n        levels : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Quantiles predicted by the forecaster.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n\n        check_interval(quantiles=quantiles)\n\n        boot_predictions = self.predict_bootstrapping(\n                               steps                   = steps,\n                               last_window             = last_window,\n                               exog                    = exog,\n                               n_boot                  = n_boot,\n                               random_state            = random_state,\n                               use_in_sample_residuals = use_in_sample_residuals\n                           )\n\n        predictions = boot_predictions.quantile(q=quantiles, axis=1).transpose()\n        predictions.columns = [f'{self.level}_q_{q}' for q in quantiles]\n\n        set_skforecast_warnings(suppress_warnings, action='default')\n\n        return predictions\n    \n\n    def predict_dist(\n        self,\n        distribution: object,\n        steps: Optional[Union[int, list]] = None,\n        last_window: Optional[pd.DataFrame] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        suppress_warnings: bool = False,\n        levels: Any = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        Fit a given probability distribution for each step. After generating \n        multiple forecasting predictions through a bootstrapping process, each \n        step is fitted to the given distribution.\n        \n        Parameters\n        ----------\n        distribution : Object\n            A distribution object from scipy.stats.\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the prediction \n            process. See skforecast.exceptions.warn_skforecast_categories for more\n            information.\n        levels : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Distribution parameters estimated for each step.\n\n        \"\"\"\n\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n        \n        boot_samples = self.predict_bootstrapping(\n                           steps                   = steps,\n                           last_window             = last_window,\n                           exog                    = exog,\n                           n_boot                  = n_boot,\n                           random_state            = random_state,\n                           use_in_sample_residuals = use_in_sample_residuals\n                       )       \n\n        param_names = [p for p in inspect.signature(distribution._pdf).parameters \n                       if not p == 'x'] + [\"loc\", \"scale\"]\n        param_values = np.apply_along_axis(\n                           lambda x: distribution.fit(x),\n                           axis = 1,\n                           arr  = boot_samples\n                       )\n        \n        level_param_names = [f'{self.level}_{p}' for p in param_names]\n        predictions = pd.DataFrame(\n                          data    = param_values,\n                          columns = level_param_names,\n                          index   = boot_samples.index\n                      )\n\n        set_skforecast_warnings(suppress_warnings, action='default')\n\n        return predictions\n\n\n    def set_params(\n        self, \n        params: dict\n    ) -> None:\n        \"\"\"\n        Set new values to the parameters of the scikit learn model stored in the\n        forecaster. It is important to note that all models share the same \n        configuration of parameters and hyperparameters.\n        \n        Parameters\n        ----------\n        params : dict\n            Parameters values.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        self.regressor = clone(self.regressor)\n        self.regressor.set_params(**params)\n        self.regressors_ = {step: clone(self.regressor)\n                            for step in range(1, self.steps + 1)}\n\n\n    def set_fit_kwargs(\n        self, \n        fit_kwargs: dict\n    ) -> None:\n        \"\"\"\n        Set new values for the additional keyword arguments passed to the `fit` \n        method of the regressor.\n        \n        Parameters\n        ----------\n        fit_kwargs : dict\n            Dict of the form {\"argument\": new_value}.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n\n        \n    def set_lags(\n        self, \n        lags: Optional[Union[int, list, np.ndarray, range, dict]] = None\n    ) -> None:\n        \"\"\"\n        Set new value to the attribute `lags`. Attributes `lags_names`, \n        `max_lag` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        lags : int, list, numpy ndarray, range, dict, default `None`\n            Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n\n            - `int`: include lags from 1 to `lags` (included).\n            - `list`, `1d numpy ndarray` or `range`: include only lags present in \n            `lags`, all elements must be int.\n            - `dict`: create different lags for each series. {'series_column_name': lags}.\n            - `None`: no lags are included as predictors. \n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        if self.window_features is None and lags is None:\n            raise ValueError(\n                \"At least one of the arguments `lags` or `window_features` \"\n                \"must be different from None. This is required to create the \"\n                \"predictors used in training the forecaster.\"\n            )\n\n        if isinstance(lags, dict):\n            self.lags = {}\n            self.lags_names = {}\n            list_max_lags = []\n            for key in lags:\n                if lags[key] is None:\n                    self.lags[key] = None\n                    self.lags_names[key] = None\n                else:\n                    self.lags[key], lags_names, max_lag = initialize_lags(\n                        forecaster_name = type(self).__name__,\n                        lags            = lags[key]\n                    )\n                    self.lags_names[key] = (\n                        [f'{key}_{lag}' for lag in lags_names] \n                         if lags_names is not None \n                         else None\n                    )\n                    if max_lag is not None:\n                        list_max_lags.append(max_lag)\n            \n            self.max_lag = max(list_max_lags) if len(list_max_lags) != 0 else None\n        else:\n            self.lags, self.lags_names, self.max_lag = initialize_lags(\n                forecaster_name = type(self).__name__, \n                lags            = lags\n            )\n\n        # Repeated here in case of lags is a dict with all values as None\n        if self.window_features is None and (lags is None or self.max_lag is None):\n            raise ValueError(\n                \"At least one of the arguments `lags` or `window_features` \"\n                \"must be different from None. This is required to create the \"\n                \"predictors used in training the forecaster.\"\n            )\n        \n        self.window_size = max(\n            [ws for ws in [self.max_lag, self.max_size_window_features] \n             if ws is not None]\n        )\n        if self.differentiation is not None:\n            self.window_size += self.differentiation\n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_window_features(\n        self, \n        window_features: Optional[Union[object, list]] = None\n    ) -> None:\n        \"\"\"\n        Set new value to the attribute `window_features`. Attributes \n        `max_size_window_features`, `window_features_names`, \n        `window_features_class_names` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        window_features : object, list, default `None`\n            Instance or list of instances used to create window features. Window features\n            are created from the original time series and are included as predictors.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        if window_features is None and self.max_lag is None:\n            raise ValueError(\n                (\"At least one of the arguments `lags` or `window_features` \"\n                 \"must be different from None. This is required to create the \"\n                 \"predictors used in training the forecaster.\")\n            )\n        \n        self.window_features, self.window_features_names, self.max_size_window_features = (\n            initialize_window_features(window_features)\n        )\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [\n                type(wf).__name__ for wf in self.window_features\n            ] \n        self.window_size = max(\n            [ws for ws in [self.max_lag, self.max_size_window_features] \n             if ws is not None]\n        )\n        if self.differentiation is not None:\n            self.window_size += self.differentiation\n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_out_sample_residuals(\n        self,\n        y_true: dict,\n        y_pred: dict,\n        append: bool = False,\n        random_state: int = 123\n    ) -> None:\n        \"\"\"\n        Set new values to the attribute `out_sample_residuals_`. Out of sample\n        residuals are meant to be calculated using observations that did not\n        participate in the training process. `y_true` and `y_pred` are expected\n        to be in the original scale of the time series. Residuals are calculated\n        as `y_true` - `y_pred`, after applying the necessary transformations and\n        differentiations if the forecaster includes them (`self.transformer_series`\n        and `self.differentiation`).\n\n        A total of 10_000 residuals are stored in the attribute `out_sample_residuals_`.\n        If the number of residuals is greater than 10_000, a random sample of\n        10_000 residuals is stored.\n        \n        Parameters\n        ----------\n        y_true : dict\n            Dictionary of numpy ndarrays or pandas Series with the true values of\n            the time series for each model in the form {step: y_true}.\n        y_pred : dict\n            Dictionary of numpy ndarrays or pandas Series with the predicted values\n            of the time series for each model in the form {step: y_pred}.\n        append : bool, default `False`\n            If `True`, new residuals are added to the once already stored in the\n            attribute `out_sample_residuals_`. If after appending the new residuals,\n            the limit of 10_000 samples is exceeded, a random sample of 10_000 is\n            kept.\n        random_state : int, default `123`\n            Sets a seed to the random sampling for reproducible output.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n\n        if not self.is_fitted:\n            raise NotFittedError(\n                \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n                \"arguments before using `set_out_sample_residuals()`.\"\n            )\n\n        if not isinstance(y_true, dict):\n            raise TypeError(\n                f\"`y_true` must be a dictionary of numpy ndarrays or pandas Series. \"\n                f\"Got {type(y_true)}.\"\n            )\n  \n        if not isinstance(y_pred, dict):\n            raise TypeError(\n                f\"`y_pred` must be a dictionary of numpy ndarrays or pandas Series. \"\n                f\"Got {type(y_pred)}.\"\n            )\n        \n        if not set(y_true.keys()) == set(y_pred.keys()):\n            raise ValueError(\n                f\"`y_true` and `y_pred` must have the same keys. \"\n                f\"Got {set(y_true.keys())} and {set(y_pred.keys())}.\"\n            )\n        \n        for k in y_true.keys():\n            if not isinstance(y_true[k], (np.ndarray, pd.Series)):\n                raise TypeError(\n                    f\"Values of `y_true` must be numpy ndarrays or pandas Series. \"\n                    f\"Got {type(y_true[k])} for step {k}.\"\n                )\n            if not isinstance(y_pred[k], (np.ndarray, pd.Series)):\n                raise TypeError(\n                    f\"Values of `y_pred` must be numpy ndarrays or pandas Series. \"\n                    f\"Got {type(y_pred[k])} for step {k}.\"\n                )\n            if len(y_true[k]) != len(y_pred[k]):\n                raise ValueError(\n                    f\"`y_true` and `y_pred` must have the same length. \"\n                    f\"Got {len(y_true[k])} and {len(y_pred[k])} for step {k}.\"\n                )\n            if isinstance(y_true[k], pd.Series) and isinstance(y_pred[k], pd.Series):\n                if not y_true[k].index.equals(y_pred[k].index):\n                    raise ValueError(\n                        f\"When containing pandas Series, elements in `y_true` and \"\n                        f\"`y_pred` must have the same index. Error in step {k}.\"\n                    )\n        \n        if self.out_sample_residuals_ is None:\n            self.out_sample_residuals_ = {\n                step: None for step in range(1, self.steps + 1)\n            }\n        \n        steps_to_update = set(range(1, self.steps + 1)).intersection(set(y_pred.keys()))\n        if not steps_to_update:\n            raise ValueError(\n                \"Provided keys in `y_pred` and `y_true` do not match any step. \"\n                \"Residuals cannot be updated.\"\n            )\n\n        residuals = {}\n        rng = np.random.default_rng(seed=random_state)\n        y_true = y_true.copy()\n        y_pred = y_pred.copy()\n        if self.differentiation is not None:\n            differentiator = copy(self.differentiator)\n            differentiator.set_params(window_size=None)\n\n        for k in steps_to_update:\n            if isinstance(y_true[k], pd.Series):\n                y_true[k] = y_true[k].to_numpy()\n            if isinstance(y_pred[k], pd.Series):\n                y_pred[k] = y_pred[k].to_numpy()\n            if self.transformer_series:\n                y_true[k] = transform_numpy(\n                                array             = y_true[k],\n                                transformer       = self.transformer_series_[self.level],\n                                fit               = False,\n                                inverse_transform = False\n                            )\n                y_pred[k] = transform_numpy(\n                                array             = y_pred[k],\n                                transformer       = self.transformer_series_[self.level],\n                                fit               = False,\n                                inverse_transform = False\n                            )\n            if self.differentiation is not None:\n                y_true[k] = differentiator.fit_transform(y_true[k])[self.differentiation:]\n                y_pred[k] = differentiator.fit_transform(y_pred[k])[self.differentiation:]\n\n            residuals[k] = y_true[k] - y_pred[k]\n\n        for key, value in residuals.items():\n            if append and self.out_sample_residuals_[key] is not None:\n                value = np.concatenate((\n                            self.out_sample_residuals_[key],\n                            value\n                        ))\n            if len(value) > 10000:\n                value = rng.choice(value, size=10000, replace=False)\n            self.out_sample_residuals_[key] = value\n    \n    def get_feature_importances(\n        self,\n        step: int,\n        sort_importance: bool = True\n    ) -> pd.DataFrame:\n        \"\"\"\n        Return feature importance of the model stored in the forecaster for a\n        specific step. Since a separate model is created for each forecast time\n        step, it is necessary to select the model from which retrieve information.\n        Only valid when regressor stores internally the feature importances in\n        the attribute `feature_importances_` or `coef_`. Otherwise, it returns  \n        `None`.\n\n        Parameters\n        ----------\n        step : int\n            Model from which retrieve information (a separate model is created \n            for each forecast time step). First step is 1.\n        sort_importance: bool, default `True`\n            If `True`, sorts the feature importances in descending order.\n\n        Returns\n        -------\n        feature_importances : pandas DataFrame\n            Feature importances associated with each predictor.\n        \n        \"\"\"\n\n        if not isinstance(step, int):\n            raise TypeError(\n                f\"`step` must be an integer. Got {type(step)}.\"\n            )\n\n        if not self.is_fitted:\n            raise NotFittedError(\n                (\"This forecaster is not fitted yet. Call `fit` with appropriate \"\n                 \"arguments before using `get_feature_importances()`.\")\n            )\n\n        if (step < 1) or (step > self.steps):\n            raise ValueError(\n                (f\"The step must have a value from 1 to the maximum number of steps \"\n                 f\"({self.steps}). Got {step}.\")\n            )\n\n        if isinstance(self.regressor, Pipeline):\n            estimator = self.regressors_[step][-1]\n        else:\n            estimator = self.regressors_[step]\n                \n        n_lags = len(list(\n            chain(*[v for v in self.lags_.values() if v is not None])\n        ))\n        n_window_features = (\n            len(self.X_train_window_features_names_out_) if self.window_features is not None else 0\n        )\n        idx_columns_autoreg = np.arange(n_lags + n_window_features)\n        if self.exog_in_:\n            idx_columns_exog = np.flatnonzero(\n                                   [name.endswith(f\"step_{step}\")\n                                    for name in self.X_train_features_names_out_]\n                               )\n        else:\n            idx_columns_exog = np.array([], dtype=int)\n        \n        idx_columns = np.concatenate((idx_columns_autoreg, idx_columns_exog))\n        idx_columns = [int(x) for x in idx_columns]  # Required since numpy 2.0\n        feature_names = [\n            self.X_train_features_names_out_[i].replace(f\"_step_{step}\", \"\") \n            for i in idx_columns\n        ]\n\n        if hasattr(estimator, 'feature_importances_'):\n            feature_importances = estimator.feature_importances_\n        elif hasattr(estimator, 'coef_'):\n            feature_importances = estimator.coef_\n        else:\n            warnings.warn(\n                (f\"Impossible to access feature importances for regressor of type \"\n                 f\"{type(estimator)}. This method is only valid when the \"\n                 f\"regressor stores internally the feature importances in the \"\n                 f\"attribute `feature_importances_` or `coef_`.\")\n            )\n            feature_importances = None\n\n        if feature_importances is not None:\n            feature_importances = pd.DataFrame({\n                                      'feature': feature_names,\n                                      'importance': feature_importances\n                                  })\n            if sort_importance:\n                feature_importances = feature_importances.sort_values(\n                                          by='importance', ascending=False\n                                      )\n\n        return feature_importances\n"
  },
  "GT_src_dict": {
    "skforecast/direct/_forecaster_direct_multivariate.py": {
      "ForecasterDirectMultiVariate.__init__": {
        "code": "    def __init__(self, regressor: object, level: str, steps: int, lags: Optional[Union[int, list, np.ndarray, range, dict]]=None, window_features: Optional[Union[object, list]]=None, transformer_series: Optional[Union[object, dict]]=StandardScaler(), transformer_exog: Optional[object]=None, weight_func: Optional[Callable]=None, differentiation: Optional[int]=None, fit_kwargs: Optional[dict]=None, n_jobs: Union[int, str]='auto', forecaster_id: Optional[Union[str, int]]=None) -> None:\n        \"\"\"Initializes the `ForecasterDirectMultiVariate` class, which enables multivariate direct multi-step forecasting using a scikit-learn compatible regressor. Each forecast step is modeled separately, allowing for flexible predictions with a shared configuration.\n\n    Parameters\n    ----------\n    regressor : object\n        A scikit-learn compatible regressor or pipeline used for forecasting.\n    level : str\n        The name of the time series to be predicted.\n    steps : int\n        Number of future steps to predict (must be >= 1).\n    lags : Optional[Union[int, list, np.ndarray, range, dict]], default=None\n        Lags used as predictors. Can be an integer specifying the maximum lag, a list of specific lags, a dictionary assigning lags to specific series, or None to omit lags.\n    window_features : Optional[Union[object, list]], default=None\n        Instances or lists of instances used to create additional window features to include as predictors.\n    transformer_series : Optional[Union[object, dict]], default=StandardScaler()\n        A transformer for preprocessing the series, applied before training. A separate transformer can be provided for each series if a dictionary is given.\n    transformer_exog : Optional[object], default=None\n        A transformer for preprocessing exogenous variables, applied before training.\n    weight_func : Optional[Callable], default=None\n        A function that generates individual sample weights based on the input index.\n    differentiation: Optional[int], default=None\n        The order of differencing to apply to the time series for making the data stationary.\n    fit_kwargs : Optional[dict], default=None\n        Additional arguments to be passed to the `fit` method of the regressor.\n    n_jobs : Union[int, str], default='auto'\n        Number of jobs for parallel execution during fitting. Can be an integer or 'auto'.\n    forecaster_id : Optional[Union[str, int]], default=None\n        An identifier for the forecaster instance.\n\n    Attributes\n    ----------\n    regressor : object\n        A clone of the provided regressor, which will be used for training.\n    regressors_ : dict\n        A dictionary of cloned regressors, one for each forecasting step.\n    lags : numpy ndarray, dict\n        The lags used as predictors.\n    max_lag : int\n        Maximum lag from the provided lags.\n    window_size : int\n        The size of the window used for creating predictors, determined by the maximum lag and window features.\n    is_fitted : bool\n        Indicates whether the forecaster has been fitted.\n    creation_date : str\n        The timestamp of when the instance was created.\n    fit_kwargs : dict\n        Stores additional arguments for fitting, validated through `check_select_fit_kwargs`.\n\n    Notes\n    -----\n    The class expects the input data to be in a pd.DataFrame format and manages preprocessing automatically. The lags and window features must be set appropriately to create training data for the respective models.\"\"\"\n        self.regressor = copy(regressor)\n        self.level = level\n        self.steps = steps\n        self.lags_ = None\n        self.transformer_series = transformer_series\n        self.transformer_series_ = None\n        self.transformer_exog = transformer_exog\n        self.weight_func = weight_func\n        self.source_code_weight_func = None\n        self.differentiation = differentiation\n        self.differentiator = None\n        self.differentiator_ = None\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.series_names_in_ = None\n        self.exog_in_ = False\n        self.exog_names_in_ = None\n        self.exog_type_in_ = None\n        self.exog_dtypes_in_ = None\n        self.X_train_series_names_in_ = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_ = None\n        self.X_train_direct_exog_names_out_ = None\n        self.X_train_features_names_out_ = None\n        self.creation_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.is_fitted = False\n        self.fit_date = None\n        self.skforecast_version = skforecast.__version__\n        self.python_version = sys.version.split(' ')[0]\n        self.forecaster_id = forecaster_id\n        self.dropna_from_series = False\n        self.encoding = None\n        if not isinstance(level, str):\n            raise TypeError(f'`level` argument must be a str. Got {type(level)}.')\n        if not isinstance(steps, int):\n            raise TypeError(f'`steps` argument must be an int greater than or equal to 1. Got {type(steps)}.')\n        if steps < 1:\n            raise ValueError(f'`steps` argument must be greater than or equal to 1. Got {steps}.')\n        self.regressors_ = {step: clone(self.regressor) for step in range(1, steps + 1)}\n        if isinstance(lags, dict):\n            self.lags = {}\n            self.lags_names = {}\n            list_max_lags = []\n            for key in lags:\n                if lags[key] is None:\n                    self.lags[key] = None\n                    self.lags_names[key] = None\n                else:\n                    self.lags[key], lags_names, max_lag = initialize_lags(forecaster_name=type(self).__name__, lags=lags[key])\n                    self.lags_names[key] = [f'{key}_{lag}' for lag in lags_names] if lags_names is not None else None\n                    if max_lag is not None:\n                        list_max_lags.append(max_lag)\n            self.max_lag = max(list_max_lags) if len(list_max_lags) != 0 else None\n        else:\n            self.lags, self.lags_names, self.max_lag = initialize_lags(forecaster_name=type(self).__name__, lags=lags)\n        self.window_features, self.window_features_names, self.max_size_window_features = initialize_window_features(window_features)\n        if self.window_features is None and (self.lags is None or self.max_lag is None):\n            raise ValueError('At least one of the arguments `lags` or `window_features` must be different from None. This is required to create the predictors used in training the forecaster.')\n        self.window_size = max([ws for ws in [self.max_lag, self.max_size_window_features] if ws is not None])\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [type(wf).__name__ for wf in self.window_features]\n        if self.differentiation is not None:\n            if not isinstance(differentiation, int) or differentiation < 1:\n                raise ValueError(f'Argument `differentiation` must be an integer equal to or greater than 1. Got {differentiation}.')\n            self.window_size += self.differentiation\n            self.differentiator = TimeSeriesDifferentiator(order=self.differentiation, window_size=self.window_size)\n        self.weight_func, self.source_code_weight_func, _ = initialize_weights(forecaster_name=type(self).__name__, regressor=regressor, weight_func=weight_func, series_weights=None)\n        self.fit_kwargs = check_select_fit_kwargs(regressor=regressor, fit_kwargs=fit_kwargs)\n        self.in_sample_residuals_ = {step: None for step in range(1, steps + 1)}\n        self.out_sample_residuals_ = None\n        if n_jobs == 'auto':\n            self.n_jobs = select_n_jobs_fit_forecaster(forecaster_name=type(self).__name__, regressor=self.regressor)\n        else:\n            if not isinstance(n_jobs, int):\n                raise TypeError(f\"`n_jobs` must be an integer or `'auto'`. Got {type(n_jobs)}.\")\n            self.n_jobs = n_jobs if n_jobs > 0 else cpu_count()",
        "docstring": "Initializes the `ForecasterDirectMultiVariate` class, which enables multivariate direct multi-step forecasting using a scikit-learn compatible regressor. Each forecast step is modeled separately, allowing for flexible predictions with a shared configuration.\n\nParameters\n----------\nregressor : object\n    A scikit-learn compatible regressor or pipeline used for forecasting.\nlevel : str\n    The name of the time series to be predicted.\nsteps : int\n    Number of future steps to predict (must be >= 1).\nlags : Optional[Union[int, list, np.ndarray, range, dict]], default=None\n    Lags used as predictors. Can be an integer specifying the maximum lag, a list of specific lags, a dictionary assigning lags to specific series, or None to omit lags.\nwindow_features : Optional[Union[object, list]], default=None\n    Instances or lists of instances used to create additional window features to include as predictors.\ntransformer_series : Optional[Union[object, dict]], default=StandardScaler()\n    A transformer for preprocessing the series, applied before training. A separate transformer can be provided for each series if a dictionary is given.\ntransformer_exog : Optional[object], default=None\n    A transformer for preprocessing exogenous variables, applied before training.\nweight_func : Optional[Callable], default=None\n    A function that generates individual sample weights based on the input index.\ndifferentiation: Optional[int], default=None\n    The order of differencing to apply to the time series for making the data stationary.\nfit_kwargs : Optional[dict], default=None\n    Additional arguments to be passed to the `fit` method of the regressor.\nn_jobs : Union[int, str], default='auto'\n    Number of jobs for parallel execution during fitting. Can be an integer or 'auto'.\nforecaster_id : Optional[Union[str, int]], default=None\n    An identifier for the forecaster instance.\n\nAttributes\n----------\nregressor : object\n    A clone of the provided regressor, which will be used for training.\nregressors_ : dict\n    A dictionary of cloned regressors, one for each forecasting step.\nlags : numpy ndarray, dict\n    The lags used as predictors.\nmax_lag : int\n    Maximum lag from the provided lags.\nwindow_size : int\n    The size of the window used for creating predictors, determined by the maximum lag and window features.\nis_fitted : bool\n    Indicates whether the forecaster has been fitted.\ncreation_date : str\n    The timestamp of when the instance was created.\nfit_kwargs : dict\n    Stores additional arguments for fitting, validated through `check_select_fit_kwargs`.\n\nNotes\n-----\nThe class expects the input data to be in a pd.DataFrame format and manages preprocessing automatically. The lags and window features must be set appropriately to create training data for the respective models.",
        "signature": "def __init__(self, regressor: object, level: str, steps: int, lags: Optional[Union[int, list, np.ndarray, range, dict]]=None, window_features: Optional[Union[object, list]]=None, transformer_series: Optional[Union[object, dict]]=StandardScaler(), transformer_exog: Optional[object]=None, weight_func: Optional[Callable]=None, differentiation: Optional[int]=None, fit_kwargs: Optional[dict]=None, n_jobs: Union[int, str]='auto', forecaster_id: Optional[Union[str, int]]=None) -> None:",
        "type": "Method",
        "class_signature": "class ForecasterDirectMultiVariate(ForecasterBase):"
      },
      "ForecasterDirectMultiVariate.fit": {
        "code": "    def fit(self, series: pd.DataFrame, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, store_last_window: bool=True, store_in_sample_residuals: bool=True, suppress_warnings: bool=False) -> None:\n        \"\"\"Fit the ForecasterDirectMultiVariate model using the provided training time series and optional exogenous variables. This method trains a separate regressor for each forecasting step by creating training matrices from the given series and exogenous data. It can store the last observed data window and in-sample residuals for performance evaluation.\n\nParameters\n----------\nseries : pandas DataFrame\n    Training time series. The DataFrame should contain the target variable specified during initialization and any other series included in the model.\nexog : pandas Series, pandas DataFrame, default `None`\n    Exogenous variables to include as predictors. Must match the length of `series` and share the same index.\nstore_last_window : bool, default `True`\n    If `True`, the last observed window of training data will be saved for predictions after training.\nstore_in_sample_residuals : bool, default `True`\n    If `True`, residuals (differences between actual and predicted values) during in-sample fitting will be stored within the forecaster for future analyses.\nsuppress_warnings : bool, default `False`\n    If `True`, warnings generated during fitting will be suppressed.\n\nReturns\n-------\nNone\n\nNotes\n-----\nThe method utilizes the `_create_train_X_y` function to generate appropriate training matrices, which incorporate lags, window features, and exogenous variables as configured during initialization. Parallel processing is employed to fit each regressor concurrently, optimizing the training process across multiple cores. The fitted regressors and their respective in-sample residuals are stored in internal attributes for later use.\"\"\"\n        '\\n        Training Forecaster.\\n\\n        Additional arguments to be passed to the `fit` method of the regressor \\n        can be added with the `fit_kwargs` argument when initializing the forecaster.\\n\\n        Parameters\\n        ----------\\n        series : pandas DataFrame\\n            Training time series.\\n        exog : pandas Series, pandas DataFrame, default `None`\\n            Exogenous variable/s included as predictor/s. Must have the same\\n            number of observations as `series` and their indexes must be aligned so\\n            that series[i] is regressed on exog[i].\\n        store_last_window : bool, default `True`\\n            Whether or not to store the last window (`last_window_`) of training data.\\n        store_in_sample_residuals : bool, default `True`\\n            If `True`, in-sample residuals will be stored in the forecaster object\\n            after fitting (`in_sample_residuals_` attribute).\\n        suppress_warnings : bool, default `False`\\n            If `True`, skforecast warnings will be suppressed during the training \\n            process. See skforecast.exceptions.warn_skforecast_categories for more\\n            information.\\n\\n        Returns\\n        -------\\n        None\\n        \\n        '\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n        self.lags_ = None\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.series_names_in_ = None\n        self.exog_in_ = False\n        self.exog_names_in_ = None\n        self.exog_type_in_ = None\n        self.exog_dtypes_in_ = None\n        self.X_train_series_names_in_ = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_ = None\n        self.X_train_direct_exog_names_out_ = None\n        self.X_train_features_names_out_ = None\n        self.in_sample_residuals_ = {step: None for step in range(1, self.steps + 1)}\n        self.is_fitted = False\n        self.fit_date = None\n        X_train, y_train, series_names_in_, X_train_series_names_in_, exog_names_in_, X_train_exog_names_out_, X_train_features_names_out_, exog_dtypes_in_ = self._create_train_X_y(series=series, exog=exog)\n\n        def fit_forecaster(regressor, X_train, y_train, step, store_in_sample_residuals):\n            \"\"\"\n            Auxiliary function to fit each of the forecaster's regressors in parallel.\n\n            Parameters\n            ----------\n            regressor : object\n                Regressor to be fitted.\n            X_train : pandas DataFrame\n                Dataframe created with the `_create_train_X_y` method, first return.\n            y_train : dict\n                Dict created with the `_create_train_X_y` method, second return.\n            step : int\n                Step of the forecaster to be fitted.\n            store_in_sample_residuals : bool\n                If `True`, in-sample residuals will be stored in the forecaster object\n                after fitting (`in_sample_residuals_` attribute).\n            \n            Returns\n            -------\n            Tuple with the step, fitted regressor and in-sample residuals.\n\n            \"\"\"\n            X_train_step, y_train_step = self.filter_train_X_y_for_step(step=step, X_train=X_train, y_train=y_train, remove_suffix=True)\n            sample_weight = self.create_sample_weights(X_train=X_train_step)\n            if sample_weight is not None:\n                regressor.fit(X=X_train_step, y=y_train_step, sample_weight=sample_weight, **self.fit_kwargs)\n            else:\n                regressor.fit(X=X_train_step, y=y_train_step, **self.fit_kwargs)\n            if store_in_sample_residuals:\n                residuals = (y_train_step - regressor.predict(X_train_step)).to_numpy()\n                if len(residuals) > 1000:\n                    rng = np.random.default_rng(seed=123)\n                    residuals = rng.choice(a=residuals, size=1000, replace=False)\n            else:\n                residuals = None\n            return (step, regressor, residuals)\n        results_fit = Parallel(n_jobs=self.n_jobs)((delayed(fit_forecaster)(regressor=copy(self.regressor), X_train=X_train, y_train=y_train, step=step, store_in_sample_residuals=store_in_sample_residuals) for step in range(1, self.steps + 1)))\n        self.regressors_ = {step: regressor for step, regressor, _ in results_fit}\n        if store_in_sample_residuals:\n            self.in_sample_residuals_ = {step: residuals for step, _, residuals in results_fit}\n        self.series_names_in_ = series_names_in_\n        self.X_train_series_names_in_ = X_train_series_names_in_\n        self.X_train_features_names_out_ = X_train_features_names_out_\n        self.is_fitted = True\n        self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.training_range_ = preprocess_y(y=series[self.level], return_values=False)[1][[0, -1]]\n        self.index_type_ = type(X_train.index)\n        if isinstance(X_train.index, pd.DatetimeIndex):\n            self.index_freq_ = X_train.index.freqstr\n        else:\n            self.index_freq_ = X_train.index.step\n        if exog is not None:\n            self.exog_in_ = True\n            self.exog_names_in_ = exog_names_in_\n            self.exog_type_in_ = type(exog)\n            self.exog_dtypes_in_ = exog_dtypes_in_\n            self.X_train_exog_names_out_ = X_train_exog_names_out_\n        if store_last_window:\n            self.last_window_ = series.iloc[-self.window_size:,][self.X_train_series_names_in_].copy()\n        set_skforecast_warnings(suppress_warnings, action='default')",
        "docstring": "Fit the ForecasterDirectMultiVariate model using the provided training time series and optional exogenous variables. This method trains a separate regressor for each forecasting step by creating training matrices from the given series and exogenous data. It can store the last observed data window and in-sample residuals for performance evaluation.\n\nParameters\n----------\nseries : pandas DataFrame\n    Training time series. The DataFrame should contain the target variable specified during initialization and any other series included in the model.\nexog : pandas Series, pandas DataFrame, default `None`\n    Exogenous variables to include as predictors. Must match the length of `series` and share the same index.\nstore_last_window : bool, default `True`\n    If `True`, the last observed window of training data will be saved for predictions after training.\nstore_in_sample_residuals : bool, default `True`\n    If `True`, residuals (differences between actual and predicted values) during in-sample fitting will be stored within the forecaster for future analyses.\nsuppress_warnings : bool, default `False`\n    If `True`, warnings generated during fitting will be suppressed.\n\nReturns\n-------\nNone\n\nNotes\n-----\nThe method utilizes the `_create_train_X_y` function to generate appropriate training matrices, which incorporate lags, window features, and exogenous variables as configured during initialization. Parallel processing is employed to fit each regressor concurrently, optimizing the training process across multiple cores. The fitted regressors and their respective in-sample residuals are stored in internal attributes for later use.",
        "signature": "def fit(self, series: pd.DataFrame, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, store_last_window: bool=True, store_in_sample_residuals: bool=True, suppress_warnings: bool=False) -> None:",
        "type": "Method",
        "class_signature": "class ForecasterDirectMultiVariate(ForecasterBase):"
      },
      "ForecasterDirectMultiVariate.predict_dist": {
        "code": "    def predict_dist(self, distribution: object, steps: Optional[Union[int, list]]=None, last_window: Optional[pd.DataFrame]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, suppress_warnings: bool=False, levels: Any=None) -> pd.DataFrame:\n        \"\"\"Fit a given probability distribution for each step of forecasts generated through a bootstrapping process. This method estimates parameters of the specified distribution based on bootstrapped samples of forecasts.\n\nParameters\n----------\ndistribution : object\n    A distribution object from `scipy.stats` that will be fitted to the bootstrapped samples.\nsteps : int, list, optional, default None\n    The number of forecast steps to predict. If an integer, predictions are made for that many steps. If a list, only the specified steps are predicted. If None, all defined steps in the forecaster will be predicted.\nlast_window : pandas DataFrame, optional, default None\n    Values used to create predictors necessary for predictions. If None, the forecaster uses the last observed window from the training data.\nexog : pandas Series, pandas DataFrame, optional, default None\n    Exogenous variables included as predictors.\nn_boot : int, default 250\n    The number of bootstrapping iterations used to estimate predictions.\nrandom_state : int, default 123\n    Seed for the random number generator to ensure reproducibility of results.\nuse_in_sample_residuals : bool, default True\n    If True, uses residuals from the training set as a proxy for prediction errors. If False, out-of-sample residuals are used, which must be set beforehand using `set_out_sample_residuals()`.\nsuppress_warnings : bool, default False\n    If True, suppresses warnings during the prediction process.\n\nReturns\n-------\npredictions : pandas DataFrame\n    A DataFrame of distribution parameters estimated for each step, indexed by the forecast index, with columns corresponding to the parameters of the fitted distribution.\n\nNotes\n-----\nThis method interacts with the bootstrapping functionality of ForecasterDirectMultiVariate. It is dependent on the bootstrapping process (`predict_bootstrapping`) to generate the predictions before fitting the specified distribution. Each parameter in the fitted distribution is returned for each forecast step, aiding in understanding the uncertainty of the predictions. The utility of the `distribution` parameter allows for flexible statistical modeling capabilities within the forecasting framework.\"\"\"\n        '\\n        Fit a given probability distribution for each step. After generating \\n        multiple forecasting predictions through a bootstrapping process, each \\n        step is fitted to the given distribution.\\n        \\n        Parameters\\n        ----------\\n        distribution : Object\\n            A distribution object from scipy.stats.\\n        steps : int, list, None, default `None`\\n            Predict n steps. The value of `steps` must be less than or equal to the \\n            value of steps defined when initializing the forecaster. Starts at 1.\\n        \\n            - If `int`: Only steps within the range of 1 to int are predicted.\\n            - If `list`: List of ints. Only the steps contained in the list \\n            are predicted.\\n            - If `None`: As many steps are predicted as were defined at \\n            initialization.\\n        last_window : pandas DataFrame, default `None`\\n            Series values used to create the predictors (lags) needed to \\n            predict `steps`.\\n            If `last_window = None`, the values stored in` self.last_window_` are\\n            used to calculate the initial predictors, and the predictions start\\n            right after training data.\\n        exog : pandas Series, pandas DataFrame, default `None`\\n            Exogenous variable/s included as predictor/s.\\n        n_boot : int, default `250`\\n            Number of bootstrapping iterations used to estimate predictions.\\n        random_state : int, default `123`\\n            Sets a seed to the random generator, so that boot predictions are always \\n            deterministic.\\n        use_in_sample_residuals : bool, default `True`\\n            If `True`, residuals from the training data are used as proxy of\\n            prediction error to create predictions. If `False`, out of sample \\n            residuals are used. In the latter case, the user should have\\n            calculated and stored the residuals within the forecaster (see\\n            `set_out_sample_residuals()`).\\n        suppress_warnings : bool, default `False`\\n            If `True`, skforecast warnings will be suppressed during the prediction \\n            process. See skforecast.exceptions.warn_skforecast_categories for more\\n            information.\\n        levels : Ignored\\n            Not used, present here for API consistency by convention.\\n\\n        Returns\\n        -------\\n        predictions : pandas DataFrame\\n            Distribution parameters estimated for each step.\\n\\n        '\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n        boot_samples = self.predict_bootstrapping(steps=steps, last_window=last_window, exog=exog, n_boot=n_boot, random_state=random_state, use_in_sample_residuals=use_in_sample_residuals)\n        param_names = [p for p in inspect.signature(distribution._pdf).parameters if not p == 'x'] + ['loc', 'scale']\n        param_values = np.apply_along_axis(lambda x: distribution.fit(x), axis=1, arr=boot_samples)\n        level_param_names = [f'{self.level}_{p}' for p in param_names]\n        predictions = pd.DataFrame(data=param_values, columns=level_param_names, index=boot_samples.index)\n        set_skforecast_warnings(suppress_warnings, action='default')\n        return predictions",
        "docstring": "Fit a given probability distribution for each step of forecasts generated through a bootstrapping process. This method estimates parameters of the specified distribution based on bootstrapped samples of forecasts.\n\nParameters\n----------\ndistribution : object\n    A distribution object from `scipy.stats` that will be fitted to the bootstrapped samples.\nsteps : int, list, optional, default None\n    The number of forecast steps to predict. If an integer, predictions are made for that many steps. If a list, only the specified steps are predicted. If None, all defined steps in the forecaster will be predicted.\nlast_window : pandas DataFrame, optional, default None\n    Values used to create predictors necessary for predictions. If None, the forecaster uses the last observed window from the training data.\nexog : pandas Series, pandas DataFrame, optional, default None\n    Exogenous variables included as predictors.\nn_boot : int, default 250\n    The number of bootstrapping iterations used to estimate predictions.\nrandom_state : int, default 123\n    Seed for the random number generator to ensure reproducibility of results.\nuse_in_sample_residuals : bool, default True\n    If True, uses residuals from the training set as a proxy for prediction errors. If False, out-of-sample residuals are used, which must be set beforehand using `set_out_sample_residuals()`.\nsuppress_warnings : bool, default False\n    If True, suppresses warnings during the prediction process.\n\nReturns\n-------\npredictions : pandas DataFrame\n    A DataFrame of distribution parameters estimated for each step, indexed by the forecast index, with columns corresponding to the parameters of the fitted distribution.\n\nNotes\n-----\nThis method interacts with the bootstrapping functionality of ForecasterDirectMultiVariate. It is dependent on the bootstrapping process (`predict_bootstrapping`) to generate the predictions before fitting the specified distribution. Each parameter in the fitted distribution is returned for each forecast step, aiding in understanding the uncertainty of the predictions. The utility of the `distribution` parameter allows for flexible statistical modeling capabilities within the forecasting framework.",
        "signature": "def predict_dist(self, distribution: object, steps: Optional[Union[int, list]]=None, last_window: Optional[pd.DataFrame]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, suppress_warnings: bool=False, levels: Any=None) -> pd.DataFrame:",
        "type": "Method",
        "class_signature": "class ForecasterDirectMultiVariate(ForecasterBase):"
      }
    }
  },
  "dependency_dict": {
    "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:__init__": {
      "skforecast/utils/utils.py": {
        "initialize_lags": {
          "code": "def initialize_lags(\n    forecaster_name: str,\n    lags: Any\n) -> Union[Optional[np.ndarray], Optional[list], Optional[int]]:\n    \"\"\"\n    Check lags argument input and generate the corresponding numpy ndarray.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    lags : Any\n        Lags used as predictors.\n\n    Returns\n    -------\n    lags : numpy ndarray, None\n        Lags used as predictors.\n    lags_names : list, None\n        Names of the lags used as predictors.\n    max_lag : int, None\n        Maximum value of the lags.\n    \n    \"\"\"\n\n    lags_names = None\n    max_lag = None\n    if lags is not None:\n        if isinstance(lags, int):\n            if lags < 1:\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n            lags = np.arange(1, lags + 1)\n\n        if isinstance(lags, (list, tuple, range)):\n            lags = np.array(lags)\n        \n        if isinstance(lags, np.ndarray):\n            if lags.size == 0:\n                return None, None, None\n            if lags.ndim != 1:\n                raise ValueError(\"`lags` must be a 1-dimensional array.\")\n            if not np.issubdtype(lags.dtype, np.integer):\n                raise TypeError(\"All values in `lags` must be integers.\")\n            if np.any(lags < 1):\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n        else:\n            if forecaster_name != 'ForecasterDirectMultiVariate':\n                raise TypeError(\n                    (f\"`lags` argument must be an int, 1d numpy ndarray, range, \"\n                     f\"tuple or list. Got {type(lags)}.\")\n                )\n            else:\n                raise TypeError(\n                    (f\"`lags` argument must be a dict, int, 1d numpy ndarray, range, \"\n                     f\"tuple or list. Got {type(lags)}.\")\n                )\n        \n        lags_names = [f'lag_{i}' for i in lags]\n        max_lag = max(lags)\n\n    return lags, lags_names, max_lag",
          "docstring": "Check lags argument input and generate the corresponding numpy ndarray.\n\nParameters\n----------\nforecaster_name : str\n    Forecaster name.\nlags : Any\n    Lags used as predictors.\n\nReturns\n-------\nlags : numpy ndarray, None\n    Lags used as predictors.\nlags_names : list, None\n    Names of the lags used as predictors.\nmax_lag : int, None\n    Maximum value of the lags.",
          "signature": "def initialize_lags(forecaster_name: str, lags: Any) -> Union[Optional[np.ndarray], Optional[list], Optional[int]]:",
          "type": "Function",
          "class_signature": null
        },
        "initialize_window_features": {
          "code": "def initialize_window_features(\n    window_features: Any\n) -> Union[Optional[list], Optional[list], Optional[int]]:\n    \"\"\"\n    Check window_features argument input and generate the corresponding list.\n\n    Parameters\n    ----------\n    window_features : Any\n        Classes used to create window features.\n\n    Returns\n    -------\n    window_features : list, None\n        List of classes used to create window features.\n    window_features_names : list, None\n        List with all the features names of the window features.\n    max_size_window_features : int, None\n        Maximum value of the `window_sizes` attribute of all classes.\n    \n    \"\"\"\n\n    needed_atts = ['window_sizes', 'features_names']\n    needed_methods = ['transform_batch', 'transform']\n\n    max_window_sizes = None\n    window_features_names = None\n    max_size_window_features = None\n    if window_features is not None:\n        if isinstance(window_features, list) and len(window_features) < 1:\n            raise ValueError(\n                \"Argument `window_features` must contain at least one element.\"\n            )\n        if not isinstance(window_features, list):\n            window_features = [window_features]\n\n        link_to_docs = (\n            \"\\nVisit the documentation for more information about how to create \"\n            \"custom window features:\\n\"\n            \"https://skforecast.org/latest/user_guides/window-features-and-custom-features.html#create-your-custom-window-features\"\n        )\n        \n        max_window_sizes = []\n        window_features_names = []\n        for wf in window_features:\n            wf_name = type(wf).__name__\n            atts_methods = set([a for a in dir(wf)])\n            if not set(needed_atts).issubset(atts_methods):\n                raise ValueError(\n                    f\"{wf_name} must have the attributes: {needed_atts}.\" + link_to_docs\n                )\n            if not set(needed_methods).issubset(atts_methods):\n                raise ValueError(\n                    f\"{wf_name} must have the methods: {needed_methods}.\" + link_to_docs\n                )\n            \n            window_sizes = wf.window_sizes\n            if not isinstance(window_sizes, (int, list)):\n                raise TypeError(\n                    f\"Attribute `window_sizes` of {wf_name} must be an int or a list \"\n                    f\"of ints. Got {type(window_sizes)}.\" + link_to_docs\n                )\n            \n            if isinstance(window_sizes, int):\n                if window_sizes < 1:\n                    raise ValueError(\n                        f\"If argument `window_sizes` is an integer, it must be equal to or \"\n                        f\"greater than 1. Got {window_sizes} from {wf_name}.\" + link_to_docs\n                    )\n                max_window_sizes.append(window_sizes)\n            else:\n                if not all(isinstance(ws, int) for ws in window_sizes) or not all(\n                    ws >= 1 for ws in window_sizes\n                ):                    \n                    raise ValueError(\n                        f\"If argument `window_sizes` is a list, all elements must be integers \"\n                        f\"equal to or greater than 1. Got {window_sizes} from {wf_name}.\" + link_to_docs\n                    )\n                max_window_sizes.append(max(window_sizes))\n\n            features_names = wf.features_names\n            if not isinstance(features_names, (str, list)):\n                raise TypeError(\n                    f\"Attribute `features_names` of {wf_name} must be a str or \"\n                    f\"a list of strings. Got {type(features_names)}.\" + link_to_docs\n                )\n            if isinstance(features_names, str):\n                window_features_names.append(features_names)\n            else:\n                if not all(isinstance(fn, str) for fn in features_names):\n                    raise TypeError(\n                        f\"If argument `features_names` is a list, all elements \"\n                        f\"must be strings. Got {features_names} from {wf_name}.\" + link_to_docs\n                    )\n                window_features_names.extend(features_names)\n\n        max_size_window_features = max(max_window_sizes)\n        if len(set(window_features_names)) != len(window_features_names):\n            raise ValueError(\n                f\"All window features names must be unique. Got {window_features_names}.\"\n            )\n\n    return window_features, window_features_names, max_size_window_features",
          "docstring": "Check window_features argument input and generate the corresponding list.\n\nParameters\n----------\nwindow_features : Any\n    Classes used to create window features.\n\nReturns\n-------\nwindow_features : list, None\n    List of classes used to create window features.\nwindow_features_names : list, None\n    List with all the features names of the window features.\nmax_size_window_features : int, None\n    Maximum value of the `window_sizes` attribute of all classes.",
          "signature": "def initialize_window_features(window_features: Any) -> Union[Optional[list], Optional[list], Optional[int]]:",
          "type": "Function",
          "class_signature": null
        },
        "initialize_weights": {
          "code": "def initialize_weights(\n    forecaster_name: str,\n    regressor: object,\n    weight_func: Union[Callable, dict],\n    series_weights: dict\n) -> Tuple[Union[Callable, dict], Union[str, dict], dict]:\n    \"\"\"\n    Check weights arguments, `weight_func` and `series_weights` for the different \n    forecasters. Create `source_code_weight_func`, source code of the custom \n    function(s) used to create weights.\n    \n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        Regressor of the forecaster.\n    weight_func : Callable, dict\n        Argument `weight_func` of the forecaster.\n    series_weights : dict\n        Argument `series_weights` of the forecaster.\n\n    Returns\n    -------\n    weight_func : Callable, dict\n        Argument `weight_func` of the forecaster.\n    source_code_weight_func : str, dict\n        Argument `source_code_weight_func` of the forecaster.\n    series_weights : dict\n        Argument `series_weights` of the forecaster.\n    \n    \"\"\"\n\n    source_code_weight_func = None\n\n    if weight_func is not None:\n\n        if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n            if not isinstance(weight_func, (Callable, dict)):\n                raise TypeError(\n                    (f\"Argument `weight_func` must be a Callable or a dict of \"\n                     f\"Callables. Got {type(weight_func)}.\")\n                )\n        elif not isinstance(weight_func, Callable):\n            raise TypeError(\n                f\"Argument `weight_func` must be a Callable. Got {type(weight_func)}.\"\n            )\n        \n        if isinstance(weight_func, dict):\n            source_code_weight_func = {}\n            for key in weight_func:\n                source_code_weight_func[key] = inspect.getsource(weight_func[key])\n        else:\n            source_code_weight_func = inspect.getsource(weight_func)\n\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(\n                (f\"Argument `weight_func` is ignored since regressor {regressor} \"\n                 f\"does not accept `sample_weight` in its `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n            weight_func = None\n            source_code_weight_func = None\n\n    if series_weights is not None:\n        if not isinstance(series_weights, dict):\n            raise TypeError(\n                (f\"Argument `series_weights` must be a dict of floats or ints.\"\n                 f\"Got {type(series_weights)}.\")\n            )\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(\n                (f\"Argument `series_weights` is ignored since regressor {regressor} \"\n                 f\"does not accept `sample_weight` in its `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n            series_weights = None\n\n    return weight_func, source_code_weight_func, series_weights",
          "docstring": "Check weights arguments, `weight_func` and `series_weights` for the different \nforecasters. Create `source_code_weight_func`, source code of the custom \nfunction(s) used to create weights.\n\nParameters\n----------\nforecaster_name : str\n    Forecaster name.\nregressor : regressor or pipeline compatible with the scikit-learn API\n    Regressor of the forecaster.\nweight_func : Callable, dict\n    Argument `weight_func` of the forecaster.\nseries_weights : dict\n    Argument `series_weights` of the forecaster.\n\nReturns\n-------\nweight_func : Callable, dict\n    Argument `weight_func` of the forecaster.\nsource_code_weight_func : str, dict\n    Argument `source_code_weight_func` of the forecaster.\nseries_weights : dict\n    Argument `series_weights` of the forecaster.",
          "signature": "def initialize_weights(forecaster_name: str, regressor: object, weight_func: Union[Callable, dict], series_weights: dict) -> Tuple[Union[Callable, dict], Union[str, dict], dict]:",
          "type": "Function",
          "class_signature": null
        },
        "check_select_fit_kwargs": {
          "code": "def check_select_fit_kwargs(\n    regressor: object,\n    fit_kwargs: Optional[dict] = None\n) -> dict:\n    \"\"\"\n    Check if `fit_kwargs` is a dict and select only the keys that are used by\n    the `fit` method of the regressor.\n\n    Parameters\n    ----------\n    regressor : object\n        Regressor object.\n    fit_kwargs : dict, default `None`\n        Dictionary with the arguments to pass to the `fit' method of the forecaster.\n\n    Returns\n    -------\n    fit_kwargs : dict\n        Dictionary with the arguments to be passed to the `fit` method of the \n        regressor after removing the unused keys.\n    \n    \"\"\"\n\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    else:\n        if not isinstance(fit_kwargs, dict):\n            raise TypeError(\n                f\"Argument `fit_kwargs` must be a dict. Got {type(fit_kwargs)}.\"\n            )\n\n        # Non used keys\n        non_used_keys = [k for k in fit_kwargs.keys()\n                         if k not in inspect.signature(regressor.fit).parameters]\n        if non_used_keys:\n            warnings.warn(\n                (f\"Argument/s {non_used_keys} ignored since they are not used by the \"\n                 f\"regressor's `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n\n        if 'sample_weight' in fit_kwargs.keys():\n            warnings.warn(\n                (\"The `sample_weight` argument is ignored. Use `weight_func` to pass \"\n                 \"a function that defines the individual weights for each sample \"\n                 \"based on its index.\"),\n                 IgnoredArgumentWarning\n            )\n            del fit_kwargs['sample_weight']\n\n        # Select only the keyword arguments allowed by the regressor's `fit` method.\n        fit_kwargs = {k: v for k, v in fit_kwargs.items()\n                      if k in inspect.signature(regressor.fit).parameters}\n\n    return fit_kwargs",
          "docstring": "Check if `fit_kwargs` is a dict and select only the keys that are used by\nthe `fit` method of the regressor.\n\nParameters\n----------\nregressor : object\n    Regressor object.\nfit_kwargs : dict, default `None`\n    Dictionary with the arguments to pass to the `fit' method of the forecaster.\n\nReturns\n-------\nfit_kwargs : dict\n    Dictionary with the arguments to be passed to the `fit` method of the \n    regressor after removing the unused keys.",
          "signature": "def check_select_fit_kwargs(regressor: object, fit_kwargs: Optional[dict]=None) -> dict:",
          "type": "Function",
          "class_signature": null
        },
        "select_n_jobs_fit_forecaster": {
          "code": "def select_n_jobs_fit_forecaster(\n    forecaster_name: str,\n    regressor: object,\n) -> int:\n    \"\"\"\n    Select the optimal number of jobs to use in the fitting process. This\n    selection is based on heuristics and is not guaranteed to be optimal. \n    \n    The number of jobs is chosen as follows:\n    \n    - If forecaster_name is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\n    and regressor_name is a linear regressor then `n_jobs = 1`, \n    otherwise `n_jobs = cpu_count() - 1`.\n    - If regressor is a `LGBMRegressor(n_jobs=1)`, then `n_jobs = cpu_count() - 1`.\n    - If regressor is a `LGBMRegressor` with internal n_jobs != 1, then `n_jobs = 1`.\n    This is because `lightgbm` is highly optimized for gradient boosting and\n    parallelizes operations at a very fine-grained level, making additional\n    parallelization unnecessary and potentially harmful due to resource contention.\n    \n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n\n    Returns\n    -------\n    n_jobs : int\n        The number of jobs to run in parallel.\n    \n    \"\"\"\n\n    if isinstance(regressor, Pipeline):\n        regressor = regressor[-1]\n        regressor_name = type(regressor).__name__\n    else:\n        regressor_name = type(regressor).__name__\n\n    linear_regressors = [\n        regressor_name\n        for regressor_name in dir(sklearn.linear_model)\n        if not regressor_name.startswith('_')\n    ]\n\n    if forecaster_name in ['ForecasterDirect', \n                           'ForecasterDirectMultiVariate']:\n        if regressor_name in linear_regressors:\n            n_jobs = 1\n        elif regressor_name == 'LGBMRegressor':\n            n_jobs = joblib.cpu_count() - 1 if regressor.n_jobs == 1 else 1\n        else:\n            n_jobs = joblib.cpu_count() - 1\n    else:\n        n_jobs = 1\n\n    return n_jobs",
          "docstring": "Select the optimal number of jobs to use in the fitting process. This\nselection is based on heuristics and is not guaranteed to be optimal. \n\nThe number of jobs is chosen as follows:\n\n- If forecaster_name is 'ForecasterDirect' or 'ForecasterDirectMultiVariate'\nand regressor_name is a linear regressor then `n_jobs = 1`, \notherwise `n_jobs = cpu_count() - 1`.\n- If regressor is a `LGBMRegressor(n_jobs=1)`, then `n_jobs = cpu_count() - 1`.\n- If regressor is a `LGBMRegressor` with internal n_jobs != 1, then `n_jobs = 1`.\nThis is because `lightgbm` is highly optimized for gradient boosting and\nparallelizes operations at a very fine-grained level, making additional\nparallelization unnecessary and potentially harmful due to resource contention.\n\nParameters\n----------\nforecaster_name : str\n    Forecaster name.\nregressor : regressor or pipeline compatible with the scikit-learn API\n    An instance of a regressor or pipeline compatible with the scikit-learn API.\n\nReturns\n-------\nn_jobs : int\n    The number of jobs to run in parallel.",
          "signature": "def select_n_jobs_fit_forecaster(forecaster_name: str, regressor: object) -> int:",
          "type": "Function",
          "class_signature": null
        }
      }
    },
    "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:fit": {
      "skforecast/utils/utils.py": {
        "preprocess_y": {
          "code": "def preprocess_y(\n    y: Union[pd.Series, pd.DataFrame],\n    return_values: bool = True\n) -> Tuple[Union[None, np.ndarray], pd.Index]:\n    \"\"\"\n    Return values and index of series separately. Index is overwritten \n    according to the next rules:\n    \n    - If index is of type `DatetimeIndex` and has frequency, nothing is \n    changed.\n    - If index is of type `RangeIndex`, nothing is changed.\n    - If index is of type `DatetimeIndex` but has no frequency, a \n    `RangeIndex` is created.\n    - If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\n    \n    Parameters\n    ----------\n    y : pandas Series, pandas DataFrame\n        Time series.\n    return_values : bool, default `True`\n        If `True` return the values of `y` as numpy ndarray. This option is \n        intended to avoid copying data when it is not necessary.\n\n    Returns\n    -------\n    y_values : None, numpy ndarray\n        Numpy array with values of `y`.\n    y_index : pandas Index\n        Index of `y` modified according to the rules.\n    \n    \"\"\"\n    \n    if isinstance(y.index, pd.DatetimeIndex) and y.index.freq is not None:\n        y_index = y.index\n    elif isinstance(y.index, pd.RangeIndex):\n        y_index = y.index\n    elif isinstance(y.index, pd.DatetimeIndex) and y.index.freq is None:\n        warnings.warn(\n            (\"Series has DatetimeIndex index but no frequency. \"\n             \"Index is overwritten with a RangeIndex of step 1.\")\n        )\n        y_index = pd.RangeIndex(\n                      start = 0,\n                      stop  = len(y),\n                      step  = 1\n                  )\n    else:\n        warnings.warn(\n            (\"Series has no DatetimeIndex nor RangeIndex index. \"\n             \"Index is overwritten with a RangeIndex.\")\n        )\n        y_index = pd.RangeIndex(\n                      start = 0,\n                      stop  = len(y),\n                      step  = 1\n                  )\n\n    y_values = y.to_numpy(copy=True).ravel() if return_values else None\n\n    return y_values, y_index",
          "docstring": "Return values and index of series separately. Index is overwritten \naccording to the next rules:\n\n- If index is of type `DatetimeIndex` and has frequency, nothing is \nchanged.\n- If index is of type `RangeIndex`, nothing is changed.\n- If index is of type `DatetimeIndex` but has no frequency, a \n`RangeIndex` is created.\n- If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\n\nParameters\n----------\ny : pandas Series, pandas DataFrame\n    Time series.\nreturn_values : bool, default `True`\n    If `True` return the values of `y` as numpy ndarray. This option is \n    intended to avoid copying data when it is not necessary.\n\nReturns\n-------\ny_values : None, numpy ndarray\n    Numpy array with values of `y`.\ny_index : pandas Index\n    Index of `y` modified according to the rules.",
          "signature": "def preprocess_y(y: Union[pd.Series, pd.DataFrame], return_values: bool=True) -> Tuple[Union[None, np.ndarray], pd.Index]:",
          "type": "Function",
          "class_signature": null
        },
        "set_skforecast_warnings": {
          "code": "def set_skforecast_warnings(\n    suppress_warnings: bool,\n    action: str = 'default'\n) -> None:\n    \"\"\"\n    Set skforecast warnings action.\n\n    Parameters\n    ----------\n    suppress_warnings : bool\n        If `True`, skforecast warnings will be suppressed. If `False`, skforecast\n        warnings will be shown as default. See \n        skforecast.exceptions.warn_skforecast_categories for more information.\n    action : str, default `'default'`\n        Action to be taken when a warning is raised. See the warnings module\n        for more information.\n\n    Returns\n    -------\n    None\n    \n    \"\"\"\n\n    if suppress_warnings:\n        for category in warn_skforecast_categories:\n            warnings.filterwarnings(action, category=category)",
          "docstring": "Set skforecast warnings action.\n\nParameters\n----------\nsuppress_warnings : bool\n    If `True`, skforecast warnings will be suppressed. If `False`, skforecast\n    warnings will be shown as default. See \n    skforecast.exceptions.warn_skforecast_categories for more information.\naction : str, default `'default'`\n    Action to be taken when a warning is raised. See the warnings module\n    for more information.\n\nReturns\n-------\nNone",
          "signature": "def set_skforecast_warnings(suppress_warnings: bool, action: str='default') -> None:",
          "type": "Function",
          "class_signature": null
        }
      },
      "skforecast/direct/_forecaster_direct_multivariate.py": {
        "ForecasterDirectMultiVariate._create_train_X_y": {
          "code": "    def _create_train_X_y(self, series: pd.DataFrame, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, dict, list, list, list, list, list, dict]:\n        \"\"\"\n        Create training matrices from multiple time series and exogenous\n        variables. The resulting matrices contain the target variable and predictors\n        needed to train all the regressors (one per step).\n        \n        Parameters\n        ----------\n        series : pandas DataFrame\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `series` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors) for each step. Note that the index \n            corresponds to that of the last step. It is updated for the corresponding \n            step in the filter_train_X_y_for_step method.\n        y_train : dict\n            Values of the time series related to each row of `X_train` for each \n            step in the form {step: y_step_[i]}.\n        series_names_in_ : list\n            Names of the series used during training.\n        X_train_series_names_in_ : list\n            Names of the series added to `X_train` when creating the training\n            matrices with `_create_train_X_y` method. It is a subset of \n            `series_names_in_`.\n        exog_names_in_ : list\n            Names of the exogenous variables included in the training matrices.\n        X_train_exog_names_out_ : list\n            Names of the exogenous variables included in the matrix `X_train` created\n            internally for training. It can be different from `exog_names_in_` if\n            some exogenous variables are transformed during the training process.\n        X_train_features_names_out_ : list\n            Names of the columns of the matrix created internally for training.\n        exog_dtypes_in_ : dict\n            Type of each exogenous variable/s used in training. If `transformer_exog` \n            is used, the dtypes are calculated before the transformation.\n        \n        \"\"\"\n        if not isinstance(series, pd.DataFrame):\n            raise TypeError(f'`series` must be a pandas DataFrame. Got {type(series)}.')\n        if len(series) < self.window_size + self.steps:\n            raise ValueError(f'Minimum length of `series` for training this forecaster is {self.window_size + self.steps}. Reduce the number of predicted steps, {self.steps}, or the maximum window_size, {self.window_size}, if no more data is available.\\n    Length `series`: {len(series)}.\\n    Max step : {self.steps}.\\n    Max window size: {self.window_size}.\\n    Lags window size: {self.max_lag}.\\n    Window features window size: {self.max_size_window_features}.')\n        series_names_in_ = list(series.columns)\n        if self.level not in series_names_in_:\n            raise ValueError(f'One of the `series` columns must be named as the `level` of the forecaster.\\n  Forecaster `level` : {self.level}.\\n  `series` columns   : {series_names_in_}.')\n        data_to_return_dict, X_train_series_names_in_ = self._create_data_to_return_dict(series_names_in_=series_names_in_)\n        series_to_create_autoreg_features_and_y = [col for col in series_names_in_ if col in X_train_series_names_in_ + [self.level]]\n        fit_transformer = False\n        if not self.is_fitted:\n            fit_transformer = True\n            self.transformer_series_ = initialize_transformer_series(forecaster_name=type(self).__name__, series_names_in_=series_to_create_autoreg_features_and_y, transformer_series=self.transformer_series)\n        if self.differentiation is None:\n            self.differentiator_ = {serie: None for serie in series_to_create_autoreg_features_and_y}\n        elif not self.is_fitted:\n            self.differentiator_ = {serie: copy(self.differentiator) for serie in series_to_create_autoreg_features_and_y}\n        exog_names_in_ = None\n        exog_dtypes_in_ = None\n        categorical_features = False\n        if exog is not None:\n            check_exog(exog=exog, allow_nan=True)\n            exog = input_to_frame(data=exog, input_name='exog')\n            series_index_no_ws = series.index[self.window_size:]\n            len_series = len(series)\n            len_series_no_ws = len_series - self.window_size\n            len_exog = len(exog)\n            if not len_exog == len_series and (not len_exog == len_series_no_ws):\n                raise ValueError(f'Length of `exog` must be equal to the length of `series` (if index is fully aligned) or length of `seriesy` - `window_size` (if `exog` starts after the first `window_size` values).\\n    `exog`                   : ({exog.index[0]} -- {exog.index[-1]})  (n={len_exog})\\n    `series`                 : ({series.index[0]} -- {series.index[-1]})  (n={len_series})\\n    `series` - `window_size` : ({series_index_no_ws[0]} -- {series_index_no_ws[-1]})  (n={len_series_no_ws})')\n            exog_names_in_ = exog.columns.to_list()\n            if len(set(exog_names_in_) - set(series_names_in_)) != len(exog_names_in_):\n                raise ValueError(f'`exog` cannot contain a column named the same as one of the series (column names of series).\\n  `series` columns : {series_names_in_}.\\n  `exog`   columns : {exog_names_in_}.')\n            self.exog_in_ = True\n            exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n            exog = transform_dataframe(df=exog, transformer=self.transformer_exog, fit=fit_transformer, inverse_transform=False)\n            check_exog_dtypes(exog, call_check_exog=True)\n            categorical_features = exog.select_dtypes(include=np.number).shape[1] != exog.shape[1]\n            if len_exog == len_series:\n                if not (exog.index == series.index).all():\n                    raise ValueError('When `exog` has the same length as `series`, the index of `exog` must be aligned with the index of `series` to ensure the correct alignment of values.')\n                exog = exog.iloc[self.window_size:,]\n            elif not (exog.index == series_index_no_ws).all():\n                raise ValueError(\"When `exog` doesn't contain the first `window_size` observations, the index of `exog` must be aligned with the index of `series` minus the first `window_size` observations to ensure the correct alignment of values.\")\n        X_train_autoreg = []\n        X_train_window_features_names_out_ = [] if self.window_features is not None else None\n        X_train_features_names_out_ = []\n        for col in series_to_create_autoreg_features_and_y:\n            y = series[col]\n            check_y(y=y, series_id=f\"Column '{col}'\")\n            y = transform_series(series=y, transformer=self.transformer_series_[col], fit=fit_transformer, inverse_transform=False)\n            y_values, y_index = preprocess_y(y=y)\n            if self.differentiation is not None:\n                if not self.is_fitted:\n                    y_values = self.differentiator_[col].fit_transform(y_values)\n                else:\n                    differentiator = copy(self.differentiator_[col])\n                    y_values = differentiator.fit_transform(y_values)\n            X_train_autoreg_col = []\n            train_index = y_index[self.window_size + (self.steps - 1):]\n            X_train_lags, y_train_values = self._create_lags(y=y_values, lags=self.lags_[col], data_to_return=data_to_return_dict.get(col, None))\n            if X_train_lags is not None:\n                X_train_autoreg_col.append(X_train_lags)\n                X_train_features_names_out_.extend(self.lags_names[col])\n            if col == self.level:\n                y_train = y_train_values\n            if self.window_features is not None:\n                n_diff = 0 if self.differentiation is None else self.differentiation\n                end_wf = None if self.steps == 1 else -(self.steps - 1)\n                y_window_features = pd.Series(y_values[n_diff:end_wf], index=y_index[n_diff:end_wf], name=col)\n                X_train_window_features, X_train_wf_names_out_ = self._create_window_features(y=y_window_features, X_as_pandas=False, train_index=train_index)\n                X_train_autoreg_col.extend(X_train_window_features)\n                X_train_window_features_names_out_.extend(X_train_wf_names_out_)\n                X_train_features_names_out_.extend(X_train_wf_names_out_)\n            if X_train_autoreg_col:\n                if len(X_train_autoreg_col) == 1:\n                    X_train_autoreg_col = X_train_autoreg_col[0]\n                else:\n                    X_train_autoreg_col = np.concatenate(X_train_autoreg_col, axis=1)\n                X_train_autoreg.append(X_train_autoreg_col)\n        X_train = []\n        len_train_index = len(train_index)\n        if categorical_features:\n            if len(X_train_autoreg) == 1:\n                X_train_autoreg = X_train_autoreg[0]\n            else:\n                X_train_autoreg = np.concatenate(X_train_autoreg, axis=1)\n            X_train_autoreg = pd.DataFrame(data=X_train_autoreg, columns=X_train_features_names_out_, index=train_index)\n            X_train.append(X_train_autoreg)\n        else:\n            X_train.extend(X_train_autoreg)\n        self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n        X_train_exog_names_out_ = None\n        if exog is not None:\n            X_train_exog_names_out_ = exog.columns.to_list()\n            if categorical_features:\n                exog_direct, X_train_direct_exog_names_out_ = exog_to_direct(exog=exog, steps=self.steps)\n                exog_direct.index = train_index\n            else:\n                exog_direct, X_train_direct_exog_names_out_ = exog_to_direct_numpy(exog=exog, steps=self.steps)\n            self.X_train_direct_exog_names_out_ = X_train_direct_exog_names_out_\n            X_train_features_names_out_.extend(self.X_train_direct_exog_names_out_)\n            X_train.append(exog_direct)\n        if len(X_train) == 1:\n            X_train = X_train[0]\n        elif categorical_features:\n            X_train = pd.concat(X_train, axis=1)\n        else:\n            X_train = np.concatenate(X_train, axis=1)\n        if categorical_features:\n            X_train.index = train_index\n        else:\n            X_train = pd.DataFrame(data=X_train, index=train_index, columns=X_train_features_names_out_)\n        y_train = {step: pd.Series(data=y_train[:, step - 1], index=y_index[self.window_size + step - 1:][:len_train_index], name=f'{self.level}_step_{step}') for step in range(1, self.steps + 1)}\n        return (X_train, y_train, series_names_in_, X_train_series_names_in_, exog_names_in_, X_train_exog_names_out_, X_train_features_names_out_, exog_dtypes_in_)",
          "docstring": "Create training matrices from multiple time series and exogenous\nvariables. The resulting matrices contain the target variable and predictors\nneeded to train all the regressors (one per step).\n\nParameters\n----------\nseries : pandas DataFrame\n    Training time series.\nexog : pandas Series, pandas DataFrame, default `None`\n    Exogenous variable/s included as predictor/s. Must have the same\n    number of observations as `series` and their indexes must be aligned.\n\nReturns\n-------\nX_train : pandas DataFrame\n    Training values (predictors) for each step. Note that the index \n    corresponds to that of the last step. It is updated for the corresponding \n    step in the filter_train_X_y_for_step method.\ny_train : dict\n    Values of the time series related to each row of `X_train` for each \n    step in the form {step: y_step_[i]}.\nseries_names_in_ : list\n    Names of the series used during training.\nX_train_series_names_in_ : list\n    Names of the series added to `X_train` when creating the training\n    matrices with `_create_train_X_y` method. It is a subset of \n    `series_names_in_`.\nexog_names_in_ : list\n    Names of the exogenous variables included in the training matrices.\nX_train_exog_names_out_ : list\n    Names of the exogenous variables included in the matrix `X_train` created\n    internally for training. It can be different from `exog_names_in_` if\n    some exogenous variables are transformed during the training process.\nX_train_features_names_out_ : list\n    Names of the columns of the matrix created internally for training.\nexog_dtypes_in_ : dict\n    Type of each exogenous variable/s used in training. If `transformer_exog` \n    is used, the dtypes are calculated before the transformation.",
          "signature": "def _create_train_X_y(self, series: pd.DataFrame, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, dict, list, list, list, list, list, dict]:",
          "type": "Method",
          "class_signature": "class ForecasterDirectMultiVariate(ForecasterBase):"
        }
      }
    },
    "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:predict_dist": {
      "skforecast/utils/utils.py": {
        "set_skforecast_warnings": {
          "code": "def set_skforecast_warnings(\n    suppress_warnings: bool,\n    action: str = 'default'\n) -> None:\n    \"\"\"\n    Set skforecast warnings action.\n\n    Parameters\n    ----------\n    suppress_warnings : bool\n        If `True`, skforecast warnings will be suppressed. If `False`, skforecast\n        warnings will be shown as default. See \n        skforecast.exceptions.warn_skforecast_categories for more information.\n    action : str, default `'default'`\n        Action to be taken when a warning is raised. See the warnings module\n        for more information.\n\n    Returns\n    -------\n    None\n    \n    \"\"\"\n\n    if suppress_warnings:\n        for category in warn_skforecast_categories:\n            warnings.filterwarnings(action, category=category)",
          "docstring": "Set skforecast warnings action.\n\nParameters\n----------\nsuppress_warnings : bool\n    If `True`, skforecast warnings will be suppressed. If `False`, skforecast\n    warnings will be shown as default. See \n    skforecast.exceptions.warn_skforecast_categories for more information.\naction : str, default `'default'`\n    Action to be taken when a warning is raised. See the warnings module\n    for more information.\n\nReturns\n-------\nNone",
          "signature": "def set_skforecast_warnings(suppress_warnings: bool, action: str='default') -> None:",
          "type": "Function",
          "class_signature": null
        }
      },
      "skforecast/direct/_forecaster_direct_multivariate.py": {
        "ForecasterDirectMultiVariate.predict_bootstrapping": {
          "code": "    def predict_bootstrapping(self, steps: Optional[Union[int, list]]=None, last_window: Optional[pd.DataFrame]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, suppress_warnings: bool=False, levels: Any=None) -> pd.DataFrame:\n        \"\"\"\n        Generate multiple forecasting predictions using a bootstrapping process. \n        By sampling from a collection of past observed errors (the residuals),\n        each iteration of bootstrapping generates a different set of predictions. \n        See the Notes section for more information. \n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.     \n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.               \n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the prediction \n            process. See skforecast.exceptions.warn_skforecast_categories for more\n            information.\n        levels : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        boot_predictions : pandas DataFrame\n            Predictions generated by bootstrapping.\n            Shape: (steps, n_boot)\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n        Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n        \"\"\"\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n        if self.is_fitted:\n            steps = prepare_steps_direct(steps=steps, max_step=self.steps)\n            if use_in_sample_residuals:\n                if not set(steps).issubset(set(self.in_sample_residuals_.keys())):\n                    raise ValueError(f'Not `forecaster.in_sample_residuals_` for steps: {set(steps) - set(self.in_sample_residuals_.keys())}.')\n                residuals = self.in_sample_residuals_\n            else:\n                if self.out_sample_residuals_ is None:\n                    raise ValueError('`forecaster.out_sample_residuals_` is `None`. Use `use_in_sample_residuals=True` or the `set_out_sample_residuals()` method before predicting.')\n                elif not set(steps).issubset(set(self.out_sample_residuals_.keys())):\n                    raise ValueError(f'Not `forecaster.out_sample_residuals_` for steps: {set(steps) - set(self.out_sample_residuals_.keys())}. Use method `set_out_sample_residuals()`.')\n                residuals = self.out_sample_residuals_\n            check_residuals = 'forecaster.in_sample_residuals_' if use_in_sample_residuals else 'forecaster.out_sample_residuals_'\n            for step in steps:\n                if residuals[step] is None:\n                    raise ValueError(f'forecaster residuals for step {step} are `None`. Check {check_residuals}.')\n                elif any((element is None for element in residuals[step])) or np.any(np.isnan(residuals[step])):\n                    raise ValueError(f'forecaster residuals for step {step} contains `None` or `NaNs` values. Check {check_residuals}.')\n        Xs, _, steps, prediction_index = self._create_predict_inputs(steps=steps, last_window=last_window, exog=exog)\n        regressors = [self.regressors_[step] for step in steps]\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', message='X does not have valid feature names', category=UserWarning)\n            predictions = np.array([regressor.predict(X).ravel()[0] for regressor, X in zip(regressors, Xs)])\n        boot_predictions = np.tile(predictions, (n_boot, 1)).T\n        boot_columns = [f'pred_boot_{i}' for i in range(n_boot)]\n        rng = np.random.default_rng(seed=random_state)\n        for i, step in enumerate(steps):\n            sampled_residuals = residuals[step][rng.integers(low=0, high=len(residuals[step]), size=n_boot)]\n            boot_predictions[i, :] = boot_predictions[i, :] + sampled_residuals\n        if self.differentiation is not None:\n            boot_predictions = self.differentiator_[self.level].inverse_transform_next_window(boot_predictions)\n        if self.transformer_series_[self.level]:\n            boot_predictions = np.apply_along_axis(func1d=transform_numpy, axis=0, arr=boot_predictions, transformer=self.transformer_series_[self.level], fit=False, inverse_transform=True)\n        boot_predictions = pd.DataFrame(data=boot_predictions, index=prediction_index, columns=boot_columns)\n        set_skforecast_warnings(suppress_warnings, action='default')\n        return boot_predictions",
          "docstring": "Generate multiple forecasting predictions using a bootstrapping process. \nBy sampling from a collection of past observed errors (the residuals),\neach iteration of bootstrapping generates a different set of predictions. \nSee the Notes section for more information. \n\nParameters\n----------\nsteps : int, list, None, default `None`\n    Predict n steps. The value of `steps` must be less than or equal to the \n    value of steps defined when initializing the forecaster. Starts at 1.\n\n    - If `int`: Only steps within the range of 1 to int are predicted.\n    - If `list`: List of ints. Only the steps contained in the list \n    are predicted.\n    - If `None`: As many steps are predicted as were defined at \n    initialization.\nlast_window : pandas DataFrame, default `None`\n    Series values used to create the predictors (lags) needed to \n    predict `steps`.\n    If `last_window = None`, the values stored in` self.last_window_` are\n    used to calculate the initial predictors, and the predictions start\n    right after training data.\nexog : pandas Series, pandas DataFrame, default `None`\n    Exogenous variable/s included as predictor/s.     \nn_boot : int, default `250`\n    Number of bootstrapping iterations used to estimate predictions.\nrandom_state : int, default `123`\n    Sets a seed to the random generator, so that boot predictions are always \n    deterministic.               \nuse_in_sample_residuals : bool, default `True`\n    If `True`, residuals from the training data are used as proxy of\n    prediction error to create predictions. If `False`, out of sample \n    residuals are used. In the latter case, the user should have\n    calculated and stored the residuals within the forecaster (see\n    `set_out_sample_residuals()`).\nsuppress_warnings : bool, default `False`\n    If `True`, skforecast warnings will be suppressed during the prediction \n    process. See skforecast.exceptions.warn_skforecast_categories for more\n    information.\nlevels : Ignored\n    Not used, present here for API consistency by convention.\n\nReturns\n-------\nboot_predictions : pandas DataFrame\n    Predictions generated by bootstrapping.\n    Shape: (steps, n_boot)\n\nNotes\n-----\nMore information about prediction intervals in forecasting:\nhttps://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\nForecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.",
          "signature": "def predict_bootstrapping(self, steps: Optional[Union[int, list]]=None, last_window: Optional[pd.DataFrame]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, suppress_warnings: bool=False, levels: Any=None) -> pd.DataFrame:",
          "type": "Method",
          "class_signature": "class ForecasterDirectMultiVariate(ForecasterBase):"
        }
      }
    }
  },
  "PRD": "# PROJECT NAME: skforecast-test_predict_dist\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 skforecast/\n    \u2514\u2500\u2500 direct/\n        \u2514\u2500\u2500 _forecaster_direct_multivariate.py\n            \u251c\u2500\u2500 ForecasterDirectMultiVariate.__init__\n            \u251c\u2500\u2500 ForecasterDirectMultiVariate.fit\n            \u2514\u2500\u2500 ForecasterDirectMultiVariate.predict_dist\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThe module provides functionality for testing and validating the predictive capabilities of a direct multivariate forecasting framework, specifically focusing on probabilistic predictions. By leveraging a configurable forecaster with regression models such as Linear Regression, the module supports making multi-step predictions while integrating external features (exogenous variables) that can undergo customizable transformations, including scaling and encoding. It also ensures flexibility in generating statistical distributions for forecasts (e.g., normal distributions) with both in-sample and out-of-sample residuals, enabling robust uncertainty estimation. This solves the problem of assessing the reliability and precision of forecasts in multivariate time series scenarios, which is critical for decision-making and model refinement.\n\n## FILE 1: skforecast/direct/_forecaster_direct_multivariate.py\n\n- CLASS METHOD: ForecasterDirectMultiVariate.predict_dist\n  - CLASS SIGNATURE: class ForecasterDirectMultiVariate(ForecasterBase):\n  - SIGNATURE: def predict_dist(self, distribution: object, steps: Optional[Union[int, list]]=None, last_window: Optional[pd.DataFrame]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, suppress_warnings: bool=False, levels: Any=None) -> pd.DataFrame:\n  - DOCSTRING: \n```python\n\"\"\"\nFit a given probability distribution for each step of forecasts generated through a bootstrapping process. This method estimates parameters of the specified distribution based on bootstrapped samples of forecasts.\n\nParameters\n----------\ndistribution : object\n    A distribution object from `scipy.stats` that will be fitted to the bootstrapped samples.\nsteps : int, list, optional, default None\n    The number of forecast steps to predict. If an integer, predictions are made for that many steps. If a list, only the specified steps are predicted. If None, all defined steps in the forecaster will be predicted.\nlast_window : pandas DataFrame, optional, default None\n    Values used to create predictors necessary for predictions. If None, the forecaster uses the last observed window from the training data.\nexog : pandas Series, pandas DataFrame, optional, default None\n    Exogenous variables included as predictors.\nn_boot : int, default 250\n    The number of bootstrapping iterations used to estimate predictions.\nrandom_state : int, default 123\n    Seed for the random number generator to ensure reproducibility of results.\nuse_in_sample_residuals : bool, default True\n    If True, uses residuals from the training set as a proxy for prediction errors. If False, out-of-sample residuals are used, which must be set beforehand using `set_out_sample_residuals()`.\nsuppress_warnings : bool, default False\n    If True, suppresses warnings during the prediction process.\n\nReturns\n-------\npredictions : pandas DataFrame\n    A DataFrame of distribution parameters estimated for each step, indexed by the forecast index, with columns corresponding to the parameters of the fitted distribution.\n\nNotes\n-----\nThis method interacts with the bootstrapping functionality of ForecasterDirectMultiVariate. It is dependent on the bootstrapping process (`predict_bootstrapping`) to generate the predictions before fitting the specified distribution. Each parameter in the fitted distribution is returned for each forecast step, aiding in understanding the uncertainty of the predictions. The utility of the `distribution` parameter allows for flexible statistical modeling capabilities within the forecasting framework.\n\"\"\"\n```\n\n- CLASS METHOD: ForecasterDirectMultiVariate.fit\n  - CLASS SIGNATURE: class ForecasterDirectMultiVariate(ForecasterBase):\n  - SIGNATURE: def fit(self, series: pd.DataFrame, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, store_last_window: bool=True, store_in_sample_residuals: bool=True, suppress_warnings: bool=False) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nFit the ForecasterDirectMultiVariate model using the provided training time series and optional exogenous variables. This method trains a separate regressor for each forecasting step by creating training matrices from the given series and exogenous data. It can store the last observed data window and in-sample residuals for performance evaluation.\n\nParameters\n----------\nseries : pandas DataFrame\n    Training time series. The DataFrame should contain the target variable specified during initialization and any other series included in the model.\nexog : pandas Series, pandas DataFrame, default `None`\n    Exogenous variables to include as predictors. Must match the length of `series` and share the same index.\nstore_last_window : bool, default `True`\n    If `True`, the last observed window of training data will be saved for predictions after training.\nstore_in_sample_residuals : bool, default `True`\n    If `True`, residuals (differences between actual and predicted values) during in-sample fitting will be stored within the forecaster for future analyses.\nsuppress_warnings : bool, default `False`\n    If `True`, warnings generated during fitting will be suppressed.\n\nReturns\n-------\nNone\n\nNotes\n-----\nThe method utilizes the `_create_train_X_y` function to generate appropriate training matrices, which incorporate lags, window features, and exogenous variables as configured during initialization. Parallel processing is employed to fit each regressor concurrently, optimizing the training process across multiple cores. The fitted regressors and their respective in-sample residuals are stored in internal attributes for later use.\n\"\"\"\n```\n\n- CLASS METHOD: ForecasterDirectMultiVariate.__init__\n  - CLASS SIGNATURE: class ForecasterDirectMultiVariate(ForecasterBase):\n  - SIGNATURE: def __init__(self, regressor: object, level: str, steps: int, lags: Optional[Union[int, list, np.ndarray, range, dict]]=None, window_features: Optional[Union[object, list]]=None, transformer_series: Optional[Union[object, dict]]=StandardScaler(), transformer_exog: Optional[object]=None, weight_func: Optional[Callable]=None, differentiation: Optional[int]=None, fit_kwargs: Optional[dict]=None, n_jobs: Union[int, str]='auto', forecaster_id: Optional[Union[str, int]]=None) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nInitializes the `ForecasterDirectMultiVariate` class, which enables multivariate direct multi-step forecasting using a scikit-learn compatible regressor. Each forecast step is modeled separately, allowing for flexible predictions with a shared configuration.\n\nParameters\n----------\nregressor : object\n    A scikit-learn compatible regressor or pipeline used for forecasting.\nlevel : str\n    The name of the time series to be predicted.\nsteps : int\n    Number of future steps to predict (must be >= 1).\nlags : Optional[Union[int, list, np.ndarray, range, dict]], default=None\n    Lags used as predictors. Can be an integer specifying the maximum lag, a list of specific lags, a dictionary assigning lags to specific series, or None to omit lags.\nwindow_features : Optional[Union[object, list]], default=None\n    Instances or lists of instances used to create additional window features to include as predictors.\ntransformer_series : Optional[Union[object, dict]], default=StandardScaler()\n    A transformer for preprocessing the series, applied before training. A separate transformer can be provided for each series if a dictionary is given.\ntransformer_exog : Optional[object], default=None\n    A transformer for preprocessing exogenous variables, applied before training.\nweight_func : Optional[Callable], default=None\n    A function that generates individual sample weights based on the input index.\ndifferentiation: Optional[int], default=None\n    The order of differencing to apply to the time series for making the data stationary.\nfit_kwargs : Optional[dict], default=None\n    Additional arguments to be passed to the `fit` method of the regressor.\nn_jobs : Union[int, str], default='auto'\n    Number of jobs for parallel execution during fitting. Can be an integer or 'auto'.\nforecaster_id : Optional[Union[str, int]], default=None\n    An identifier for the forecaster instance.\n\nAttributes\n----------\nregressor : object\n    A clone of the provided regressor, which will be used for training.\nregressors_ : dict\n    A dictionary of cloned regressors, one for each forecasting step.\nlags : numpy ndarray, dict\n    The lags used as predictors.\nmax_lag : int\n    Maximum lag from the provided lags.\nwindow_size : int\n    The size of the window used for creating predictors, determined by the maximum lag and window features.\nis_fitted : bool\n    Indicates whether the forecaster has been fitted.\ncreation_date : str\n    The timestamp of when the instance was created.\nfit_kwargs : dict\n    Stores additional arguments for fitting, validated through `check_select_fit_kwargs`.\n\nNotes\n-----\nThe class expects the input data to be in a pd.DataFrame format and manages preprocessing automatically. The lags and window features must be set appropriately to create training data for the respective models.\n\"\"\"\n```\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "skforecast/direct/_forecaster_direct_multivariate.py": "from typing import Union, Tuple, Optional, Callable, Any\nimport warnings\nimport sys\nimport numpy as np\nimport pandas as pd\nimport inspect\nfrom copy import copy\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import clone\nfrom joblib import Parallel, delayed, cpu_count\nfrom sklearn.preprocessing import StandardScaler\nfrom itertools import chain\nimport skforecast\nfrom ..base import ForecasterBase\nfrom ..exceptions import DataTransformationWarning\nfrom ..utils import initialize_lags, initialize_window_features, initialize_weights, initialize_transformer_series, check_select_fit_kwargs, check_y, check_exog, prepare_steps_direct, get_exog_dtypes, check_exog_dtypes, check_predict_input, check_interval, preprocess_y, preprocess_last_window, input_to_frame, exog_to_direct, exog_to_direct_numpy, expand_index, transform_numpy, transform_series, transform_dataframe, select_n_jobs_fit_forecaster, set_skforecast_warnings\nfrom ..preprocessing import TimeSeriesDifferentiator\nfrom ..model_selection._utils import _extract_data_folds_multiseries\n\nclass ForecasterDirectMultiVariate(ForecasterBase):\n    \"\"\"\n    This class turns any regressor compatible with the scikit-learn API into a\n    autoregressive multivariate direct multi-step forecaster. A separate model \n    is created for each forecast time step. See documentation for more details.\n\n    Parameters\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n    level : str\n        Name of the time series to be predicted.\n    steps : int\n        Maximum number of future steps the forecaster will predict when using\n        method `predict()`. Since a different model is created for each step,\n        this value must be defined before training.\n    lags : int, list, numpy ndarray, range, dict, default `None`\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n\n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n        - `dict`: create different lags for each series. {'series_column_name': lags}.\n        - `None`: no lags are included as predictors. \n    window_features : object, list, default `None`\n        Instance or list of instances used to create window features. Window features\n        are created from the original time series and are included as predictors.\n    transformer_series : transformer (preprocessor), dict, default `sklearn.preprocessing.StandardScaler`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and \n        inverse_transform. Transformation is applied to each `series` before training \n        the forecaster. ColumnTransformers are not allowed since they do not have \n        inverse_transform method.\n\n        - If single transformer: it is cloned and applied to all series. \n        - If `dict` of transformers: a different transformer can be used for each series.\n    transformer_exog : transformer, default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable, default `None`\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\n        method. The resulting `sample_weight` cannot have negative values.\n    fit_kwargs : dict, default `None`\n        Additional arguments to be passed to the `fit` method of the regressor.\n    n_jobs : int, 'auto', default `'auto'`\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the function\n        skforecast.utils.select_n_jobs_fit_forecaster.\n    forecaster_id : str, int, default `None`\n        Name used as an identifier of the forecaster.\n\n    Attributes\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n        An instance of this regressor is trained for each step. All of them \n        are stored in `self.regressors_`.\n    regressors_ : dict\n        Dictionary with regressors trained for each step. They are initialized \n        as a copy of `regressor`.\n    steps : int\n        Number of future steps the forecaster will predict when using method\n        `predict()`. Since a different model is created for each step, this value\n        should be defined before training.\n    lags : numpy ndarray, dict\n        Lags used as predictors.\n    lags_ : dict\n        Dictionary with the lags of each series. Created from `lags` when \n        creating the training matrices and used internally to avoid overwriting.\n    lags_names : dict\n        Names of the lags of each series.\n    max_lag : int\n        Maximum lag included in `lags`.\n    window_features : list\n        Class or list of classes used to create window features.\n    window_features_names : list\n        Names of the window features to be included in the `X_train` matrix.\n    window_features_class_names : list\n        Names of the classes used to create the window features.\n    max_size_window_features : int\n        Maximum window size required by the window features.\n    window_size : int\n        The window size needed to create the predictors. It is calculated as the \n        maximum value between `max_lag` and `max_size_window_features`. If \n        differentiation is used, `window_size` is increased by n units equal to \n        the order of differentiation so that predictors can be generated correctly.\n    transformer_series : transformer (preprocessor), dict, default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and \n        inverse_transform. Transformation is applied to each `series` before training \n        the forecaster. ColumnTransformers are not allowed since they do not have \n        inverse_transform method.\n\n        - If single transformer: it is cloned and applied to all series. \n        - If `dict` of transformers: a different transformer can be used for each series.\n    transformer_series_ : dict\n        Dictionary with the transformer for each series. It is created cloning the \n        objects in `transformer_series` and is used internally to avoid overwriting.\n    transformer_exog : transformer\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its\n        `fit` method. The resulting `sample_weight` cannot have negative values.\n    source_code_weight_func : str\n        Source code of the custom function used to create weights.\n    differentiation : int\n        Order of differencing applied to the time series before training the \n        forecaster.\n    differentiator : TimeSeriesDifferentiator\n        Skforecast object used to differentiate the time series.\n    differentiator_ : dict\n        Dictionary with the `differentiator` for each series. It is created cloning the\n        objects in `differentiator` and is used internally to avoid overwriting.\n    last_window_ : pandas DataFrame\n        This window represents the most recent data observed by the predictor\n        during its training phase. It contains the values needed to predict the\n        next step immediately after the training data. These values are stored\n        in the original scale of the time series before undergoing any transformations\n        or differentiation. When `differentiation` parameter is specified, the\n        dimensions of the `last_window_` are expanded as many values as the order\n        of differentiation. For example, if `lags` = 7 and `differentiation` = 1,\n        `last_window_` will have 8 values.\n    index_type_ : type\n        Type of index of the input used in training.\n    index_freq_ : str\n        Frequency of Index of the input used in training.\n    training_range_: pandas Index\n        First and last values of index of the data used during training.\n    exog_in_ : bool\n        If the forecaster has been trained using exogenous variable/s.\n    exog_type_in_ : type\n        Type of exogenous variable/s used in training.\n    exog_dtypes_in_ : dict\n        Type of each exogenous variable/s used in training. If `transformer_exog` \n        is used, the dtypes are calculated after the transformation.\n    exog_names_in_ : list\n        Names of the exogenous variables used during training.\n    series_names_in_ : list\n        Names of the series used during training.\n    X_train_series_names_in_ : list\n        Names of the series added to `X_train` when creating the training\n        matrices with `_create_train_X_y` method. It is a subset of \n        `series_names_in_`.\n    X_train_window_features_names_out_ : list\n        Names of the window features included in the matrix `X_train` created\n        internally for training.\n    X_train_exog_names_out_ : list\n        Names of the exogenous variables included in the matrix `X_train` created\n        internally for training. It can be different from `exog_names_in_` if\n        some exogenous variables are transformed during the training process.\n    X_train_direct_exog_names_out_ : list\n        Same as `X_train_exog_names_out_` but using the direct format. The same \n        exogenous variable is repeated for each step.\n    X_train_features_names_out_ : list\n        Names of columns of the matrix created internally for training.\n    fit_kwargs : dict\n        Additional arguments to be passed to the `fit` method of the regressor.\n    in_sample_residuals_ : dict\n        Residuals of the models when predicting training data. Only stored up to\n        1000 values per model in the form `{step: residuals}`. If `transformer_series` \n        is not `None`, residuals are stored in the transformed scale.\n    out_sample_residuals_ : dict\n        Residuals of the models when predicting non training data. Only stored\n        up to 1000 values per model in the form `{step: residuals}`. If `transformer_series` \n        is not `None`, residuals are assumed to be in the transformed scale. Use \n        `set_out_sample_residuals()` method to set values.\n    creation_date : str\n        Date of creation.\n    is_fitted : bool\n        Tag to identify if the regressor has been fitted (trained).\n    fit_date : str\n        Date of last fit.\n    skforecast_version : str\n        Version of skforecast library used to create the forecaster.\n    python_version : str\n        Version of python used to create the forecaster.\n    n_jobs : int, 'auto'\n        The number of jobs to run in parallel. If `-1`, then the number of jobs is \n        set to the number of cores. If 'auto', `n_jobs` is set using the fuction\n        skforecast.utils.select_n_jobs_fit_forecaster.\n    forecaster_id : str, int\n        Name used as an identifier of the forecaster.\n    dropna_from_series : Ignored\n        Not used, present here for API consistency by convention.\n    encoding : Ignored\n        Not used, present here for API consistency by convention.\n\n    Notes\n    -----\n    A separate model is created for each forecasting time step. It is important to\n    note that all models share the same parameter and hyperparameter configuration.\n    \n    \"\"\"\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Information displayed when a ForecasterDirectMultiVariate object is printed.\n        \"\"\"\n        params, _, series_names_in_, exog_names_in_, transformer_series = [self._format_text_repr(value) for value in self._preprocess_repr(regressor=self.regressor, series_names_in_=self.series_names_in_, exog_names_in_=self.exog_names_in_, transformer_series=self.transformer_series)]\n        info = f'{'=' * len(type(self).__name__)} \\n{type(self).__name__} \\n{'=' * len(type(self).__name__)} \\nRegressor: {type(self.regressor).__name__} \\nTarget series (level): {self.level} \\nLags: {self.lags} \\nWindow features: {self.window_features_names} \\nWindow size: {self.window_size} \\nMaximum steps to predict: {self.steps} \\nMultivariate series: {series_names_in_} \\nExogenous included: {self.exog_in_} \\nExogenous names: {exog_names_in_} \\nTransformer for series: {transformer_series} \\nTransformer for exog: {self.transformer_exog} \\nWeight function included: {(True if self.weight_func is not None else False)} \\nDifferentiation order: {self.differentiation} \\nTraining range: {(self.training_range_.to_list() if self.is_fitted else None)} \\nTraining index type: {(str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None)} \\nTraining index frequency: {(self.index_freq_ if self.is_fitted else None)} \\nRegressor parameters: {params} \\nfit_kwargs: {self.fit_kwargs} \\nCreation date: {self.creation_date} \\nLast fit date: {self.fit_date} \\nSkforecast version: {self.skforecast_version} \\nPython version: {self.python_version} \\nForecaster id: {self.forecaster_id} \\n'\n        return info\n\n    def _repr_html_(self):\n        \"\"\"\n        HTML representation of the object.\n        The \"General Information\" section is expanded by default.\n        \"\"\"\n        params, _, series_names_in_, exog_names_in_, transformer_series = self._preprocess_repr(regressor=self.regressor, series_names_in_=self.series_names_in_, exog_names_in_=self.exog_names_in_, transformer_series=self.transformer_series)\n        style, unique_id = self._get_style_repr_html(self.is_fitted)\n        content = f'\\n        <div class=\"container-{unique_id}\">\\n            <h2>{type(self).__name__}</h2>\\n            <details open>\\n                <summary>General Information</summary>\\n                <ul>\\n                    <li><strong>Regressor:</strong> {type(self.regressor).__name__}</li>\\n                    <li><strong>Target series (level):</strong> {self.level}</li>\\n                    <li><strong>Lags:</strong> {self.lags}</li>\\n                    <li><strong>Window features:</strong> {self.window_features_names}</li>\\n                    <li><strong>Window size:</strong> {self.window_size}</li>\\n                    <li><strong>Maximum steps to predict:</strong> {self.steps}</li>\\n                    <li><strong>Exogenous included:</strong> {self.exog_in_}</li>\\n                    <li><strong>Weight function included:</strong> {self.weight_func is not None}</li>\\n                    <li><strong>Differentiation order:</strong> {self.differentiation}</li>\\n                    <li><strong>Creation date:</strong> {self.creation_date}</li>\\n                    <li><strong>Last fit date:</strong> {self.fit_date}</li>\\n                    <li><strong>Skforecast version:</strong> {self.skforecast_version}</li>\\n                    <li><strong>Python version:</strong> {self.python_version}</li>\\n                    <li><strong>Forecaster id:</strong> {self.forecaster_id}</li>\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Exogenous Variables</summary>\\n                <ul>\\n                    {exog_names_in_}\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Data Transformations</summary>\\n                <ul>\\n                    <li><strong>Transformer for series:</strong> {transformer_series}</li>\\n                    <li><strong>Transformer for exog:</strong> {self.transformer_exog}</li>\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Training Information</summary>\\n                <ul>\\n                    <li><strong>Target series (level):</strong> {self.level}</li>\\n                    <li><strong>Multivariate series:</strong> {series_names_in_}</li>\\n                    <li><strong>Training range:</strong> {(self.training_range_.to_list() if self.is_fitted else 'Not fitted')}</li>\\n                    <li><strong>Training index type:</strong> {(str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted')}</li>\\n                    <li><strong>Training index frequency:</strong> {(self.index_freq_ if self.is_fitted else 'Not fitted')}</li>\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Regressor Parameters</summary>\\n                <ul>\\n                    {params}\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Fit Kwargs</summary>\\n                <ul>\\n                    {self.fit_kwargs}\\n                </ul>\\n            </details>\\n            <p>\\n                <a href=\"https://skforecast.org/{skforecast.__version__}/api/forecasterdirectmultivariate.html\">&#128712 <strong>API Reference</strong></a>\\n                &nbsp;&nbsp;\\n                <a href=\"https://skforecast.org/{skforecast.__version__}/user_guides/dependent-multi-series-multivariate-forecasting.html\">&#128462 <strong>User Guide</strong></a>\\n            </p>\\n        </div>\\n        '\n        return style + content\n\n    def _create_data_to_return_dict(self, series_names_in_: list) -> Tuple[dict, list]:\n        \"\"\"\n        Create `data_to_return_dict` based on series names and lags configuration.\n        The dictionary contains the information to decide what data to return in \n        the `_create_lags` method.\n        \n        Parameters\n        ----------\n        series_names_in_ : list\n            Names of the series used during training.\n\n        Returns\n        -------\n        data_to_return_dict : dict\n            Dictionary with the information to decide what data to return in the\n            `_create_lags` method.\n        X_train_series_names_in_ : list\n            Names of the series added to `X_train` when creating the training\n            matrices with `_create_train_X_y` method. It is a subset of \n            `series_names_in_`.\n        \n        \"\"\"\n        if isinstance(self.lags, dict):\n            lags_keys = list(self.lags.keys())\n            if set(lags_keys) != set(series_names_in_):\n                raise ValueError(f\"When `lags` parameter is a `dict`, its keys must be the same as `series` column names. If don't want to include lags, add '{{column: None}}' to the lags dict.\\n  Lags keys        : {lags_keys}.\\n  `series` columns : {series_names_in_}.\")\n            self.lags_ = copy(self.lags)\n        else:\n            self.lags_ = {serie: self.lags for serie in series_names_in_}\n            if self.lags is not None:\n                lags_names = [f'lag_{i}' for i in self.lags]\n                self.lags_names = {serie: [f'{serie}_{lag}' for lag in lags_names] for serie in series_names_in_}\n            else:\n                self.lags_names = {serie: None for serie in series_names_in_}\n        X_train_series_names_in_ = series_names_in_\n        if self.lags is None:\n            data_to_return_dict = {self.level: 'y'}\n        else:\n            data_to_return_dict = {col: 'both' if col == self.level else 'X' for col in series_names_in_ if col == self.level or self.lags_.get(col) is not None}\n            if self.lags_.get(self.level) is None:\n                data_to_return_dict[self.level] = 'y'\n            if self.window_features is None:\n                X_train_series_names_in_ = [col for col in data_to_return_dict.keys() if data_to_return_dict[col] in ['X', 'both']]\n        return (data_to_return_dict, X_train_series_names_in_)\n\n    def _create_lags(self, y: np.ndarray, lags: np.ndarray, data_to_return: Optional[str]='both') -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n        \"\"\"\n        Create the lagged values and their target variable from a time series.\n        \n        Note that the returned matrix `X_data` contains the lag 1 in the first \n        column, the lag 2 in the in the second column and so on.\n        \n        Parameters\n        ----------\n        y : numpy ndarray\n            Training time series values.\n        lags : numpy ndarray\n            lags to create.\n        data_to_return : str, default 'both'\n            Specifies which data to return. Options are 'X', 'y', 'both' or None.\n\n        Returns\n        -------\n        X_data : numpy ndarray, None\n            Lagged values (predictors).\n        y_data : numpy ndarray, None\n            Values of the time series related to each row of `X_data`.\n        \n        \"\"\"\n        X_data = None\n        y_data = None\n        if data_to_return is not None:\n            n_rows = len(y) - self.window_size - (self.steps - 1)\n            if data_to_return != 'y':\n                X_data = np.full(shape=(n_rows, len(lags)), fill_value=np.nan, order='F', dtype=float)\n                for i, lag in enumerate(lags):\n                    X_data[:, i] = y[self.window_size - lag:-(lag + self.steps - 1)]\n            if data_to_return != 'X':\n                y_data = np.full(shape=(n_rows, self.steps), fill_value=np.nan, order='F', dtype=float)\n                for step in range(self.steps):\n                    y_data[:, step] = y[self.window_size + step:self.window_size + step + n_rows]\n        return (X_data, y_data)\n\n    def _create_window_features(self, y: pd.Series, train_index: pd.Index, X_as_pandas: bool=False) -> Tuple[list, list]:\n        \"\"\"\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        train_index : pandas Index\n            Index of the training data. It is used to create the pandas DataFrame\n            `X_train_window_features` when `X_as_pandas` is `True`.\n        X_as_pandas : bool, default `False`\n            If `True`, the returned matrix `X_train_window_features` is a \n            pandas DataFrame.\n\n        Returns\n        -------\n        X_train_window_features : list\n            List of numpy ndarrays or pandas DataFrames with the window features.\n        X_train_window_features_names_out_ : list\n            Names of the window features.\n        \n        \"\"\"\n        len_train_index = len(train_index)\n        X_train_window_features = []\n        X_train_window_features_names_out_ = []\n        for wf in self.window_features:\n            X_train_wf = wf.transform_batch(y)\n            if not isinstance(X_train_wf, pd.DataFrame):\n                raise TypeError(f'The method `transform_batch` of {type(wf).__name__} must return a pandas DataFrame.')\n            X_train_wf = X_train_wf.iloc[-len_train_index:]\n            if not len(X_train_wf) == len_train_index:\n                raise ValueError(f'The method `transform_batch` of {type(wf).__name__} must return a DataFrame with the same number of rows as the input time series - (`window_size` + (`steps` - 1)): {len_train_index}.')\n            X_train_wf.index = train_index\n            X_train_wf.columns = [f'{y.name}_{col}' for col in X_train_wf.columns]\n            X_train_window_features_names_out_.extend(X_train_wf.columns)\n            if not X_as_pandas:\n                X_train_wf = X_train_wf.to_numpy()\n            X_train_window_features.append(X_train_wf)\n        return (X_train_window_features, X_train_window_features_names_out_)\n\n    def _create_train_X_y(self, series: pd.DataFrame, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, dict, list, list, list, list, list, dict]:\n        \"\"\"\n        Create training matrices from multiple time series and exogenous\n        variables. The resulting matrices contain the target variable and predictors\n        needed to train all the regressors (one per step).\n        \n        Parameters\n        ----------\n        series : pandas DataFrame\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `series` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors) for each step. Note that the index \n            corresponds to that of the last step. It is updated for the corresponding \n            step in the filter_train_X_y_for_step method.\n        y_train : dict\n            Values of the time series related to each row of `X_train` for each \n            step in the form {step: y_step_[i]}.\n        series_names_in_ : list\n            Names of the series used during training.\n        X_train_series_names_in_ : list\n            Names of the series added to `X_train` when creating the training\n            matrices with `_create_train_X_y` method. It is a subset of \n            `series_names_in_`.\n        exog_names_in_ : list\n            Names of the exogenous variables included in the training matrices.\n        X_train_exog_names_out_ : list\n            Names of the exogenous variables included in the matrix `X_train` created\n            internally for training. It can be different from `exog_names_in_` if\n            some exogenous variables are transformed during the training process.\n        X_train_features_names_out_ : list\n            Names of the columns of the matrix created internally for training.\n        exog_dtypes_in_ : dict\n            Type of each exogenous variable/s used in training. If `transformer_exog` \n            is used, the dtypes are calculated before the transformation.\n        \n        \"\"\"\n        if not isinstance(series, pd.DataFrame):\n            raise TypeError(f'`series` must be a pandas DataFrame. Got {type(series)}.')\n        if len(series) < self.window_size + self.steps:\n            raise ValueError(f'Minimum length of `series` for training this forecaster is {self.window_size + self.steps}. Reduce the number of predicted steps, {self.steps}, or the maximum window_size, {self.window_size}, if no more data is available.\\n    Length `series`: {len(series)}.\\n    Max step : {self.steps}.\\n    Max window size: {self.window_size}.\\n    Lags window size: {self.max_lag}.\\n    Window features window size: {self.max_size_window_features}.')\n        series_names_in_ = list(series.columns)\n        if self.level not in series_names_in_:\n            raise ValueError(f'One of the `series` columns must be named as the `level` of the forecaster.\\n  Forecaster `level` : {self.level}.\\n  `series` columns   : {series_names_in_}.')\n        data_to_return_dict, X_train_series_names_in_ = self._create_data_to_return_dict(series_names_in_=series_names_in_)\n        series_to_create_autoreg_features_and_y = [col for col in series_names_in_ if col in X_train_series_names_in_ + [self.level]]\n        fit_transformer = False\n        if not self.is_fitted:\n            fit_transformer = True\n            self.transformer_series_ = initialize_transformer_series(forecaster_name=type(self).__name__, series_names_in_=series_to_create_autoreg_features_and_y, transformer_series=self.transformer_series)\n        if self.differentiation is None:\n            self.differentiator_ = {serie: None for serie in series_to_create_autoreg_features_and_y}\n        elif not self.is_fitted:\n            self.differentiator_ = {serie: copy(self.differentiator) for serie in series_to_create_autoreg_features_and_y}\n        exog_names_in_ = None\n        exog_dtypes_in_ = None\n        categorical_features = False\n        if exog is not None:\n            check_exog(exog=exog, allow_nan=True)\n            exog = input_to_frame(data=exog, input_name='exog')\n            series_index_no_ws = series.index[self.window_size:]\n            len_series = len(series)\n            len_series_no_ws = len_series - self.window_size\n            len_exog = len(exog)\n            if not len_exog == len_series and (not len_exog == len_series_no_ws):\n                raise ValueError(f'Length of `exog` must be equal to the length of `series` (if index is fully aligned) or length of `seriesy` - `window_size` (if `exog` starts after the first `window_size` values).\\n    `exog`                   : ({exog.index[0]} -- {exog.index[-1]})  (n={len_exog})\\n    `series`                 : ({series.index[0]} -- {series.index[-1]})  (n={len_series})\\n    `series` - `window_size` : ({series_index_no_ws[0]} -- {series_index_no_ws[-1]})  (n={len_series_no_ws})')\n            exog_names_in_ = exog.columns.to_list()\n            if len(set(exog_names_in_) - set(series_names_in_)) != len(exog_names_in_):\n                raise ValueError(f'`exog` cannot contain a column named the same as one of the series (column names of series).\\n  `series` columns : {series_names_in_}.\\n  `exog`   columns : {exog_names_in_}.')\n            self.exog_in_ = True\n            exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n            exog = transform_dataframe(df=exog, transformer=self.transformer_exog, fit=fit_transformer, inverse_transform=False)\n            check_exog_dtypes(exog, call_check_exog=True)\n            categorical_features = exog.select_dtypes(include=np.number).shape[1] != exog.shape[1]\n            if len_exog == len_series:\n                if not (exog.index == series.index).all():\n                    raise ValueError('When `exog` has the same length as `series`, the index of `exog` must be aligned with the index of `series` to ensure the correct alignment of values.')\n                exog = exog.iloc[self.window_size:,]\n            elif not (exog.index == series_index_no_ws).all():\n                raise ValueError(\"When `exog` doesn't contain the first `window_size` observations, the index of `exog` must be aligned with the index of `series` minus the first `window_size` observations to ensure the correct alignment of values.\")\n        X_train_autoreg = []\n        X_train_window_features_names_out_ = [] if self.window_features is not None else None\n        X_train_features_names_out_ = []\n        for col in series_to_create_autoreg_features_and_y:\n            y = series[col]\n            check_y(y=y, series_id=f\"Column '{col}'\")\n            y = transform_series(series=y, transformer=self.transformer_series_[col], fit=fit_transformer, inverse_transform=False)\n            y_values, y_index = preprocess_y(y=y)\n            if self.differentiation is not None:\n                if not self.is_fitted:\n                    y_values = self.differentiator_[col].fit_transform(y_values)\n                else:\n                    differentiator = copy(self.differentiator_[col])\n                    y_values = differentiator.fit_transform(y_values)\n            X_train_autoreg_col = []\n            train_index = y_index[self.window_size + (self.steps - 1):]\n            X_train_lags, y_train_values = self._create_lags(y=y_values, lags=self.lags_[col], data_to_return=data_to_return_dict.get(col, None))\n            if X_train_lags is not None:\n                X_train_autoreg_col.append(X_train_lags)\n                X_train_features_names_out_.extend(self.lags_names[col])\n            if col == self.level:\n                y_train = y_train_values\n            if self.window_features is not None:\n                n_diff = 0 if self.differentiation is None else self.differentiation\n                end_wf = None if self.steps == 1 else -(self.steps - 1)\n                y_window_features = pd.Series(y_values[n_diff:end_wf], index=y_index[n_diff:end_wf], name=col)\n                X_train_window_features, X_train_wf_names_out_ = self._create_window_features(y=y_window_features, X_as_pandas=False, train_index=train_index)\n                X_train_autoreg_col.extend(X_train_window_features)\n                X_train_window_features_names_out_.extend(X_train_wf_names_out_)\n                X_train_features_names_out_.extend(X_train_wf_names_out_)\n            if X_train_autoreg_col:\n                if len(X_train_autoreg_col) == 1:\n                    X_train_autoreg_col = X_train_autoreg_col[0]\n                else:\n                    X_train_autoreg_col = np.concatenate(X_train_autoreg_col, axis=1)\n                X_train_autoreg.append(X_train_autoreg_col)\n        X_train = []\n        len_train_index = len(train_index)\n        if categorical_features:\n            if len(X_train_autoreg) == 1:\n                X_train_autoreg = X_train_autoreg[0]\n            else:\n                X_train_autoreg = np.concatenate(X_train_autoreg, axis=1)\n            X_train_autoreg = pd.DataFrame(data=X_train_autoreg, columns=X_train_features_names_out_, index=train_index)\n            X_train.append(X_train_autoreg)\n        else:\n            X_train.extend(X_train_autoreg)\n        self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n        X_train_exog_names_out_ = None\n        if exog is not None:\n            X_train_exog_names_out_ = exog.columns.to_list()\n            if categorical_features:\n                exog_direct, X_train_direct_exog_names_out_ = exog_to_direct(exog=exog, steps=self.steps)\n                exog_direct.index = train_index\n            else:\n                exog_direct, X_train_direct_exog_names_out_ = exog_to_direct_numpy(exog=exog, steps=self.steps)\n            self.X_train_direct_exog_names_out_ = X_train_direct_exog_names_out_\n            X_train_features_names_out_.extend(self.X_train_direct_exog_names_out_)\n            X_train.append(exog_direct)\n        if len(X_train) == 1:\n            X_train = X_train[0]\n        elif categorical_features:\n            X_train = pd.concat(X_train, axis=1)\n        else:\n            X_train = np.concatenate(X_train, axis=1)\n        if categorical_features:\n            X_train.index = train_index\n        else:\n            X_train = pd.DataFrame(data=X_train, index=train_index, columns=X_train_features_names_out_)\n        y_train = {step: pd.Series(data=y_train[:, step - 1], index=y_index[self.window_size + step - 1:][:len_train_index], name=f'{self.level}_step_{step}') for step in range(1, self.steps + 1)}\n        return (X_train, y_train, series_names_in_, X_train_series_names_in_, exog_names_in_, X_train_exog_names_out_, X_train_features_names_out_, exog_dtypes_in_)\n\n    def create_train_X_y(self, series: pd.DataFrame, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, suppress_warnings: bool=False) -> Tuple[pd.DataFrame, dict]:\n        \"\"\"\n        Create training matrices from multiple time series and exogenous\n        variables. The resulting matrices contain the target variable and predictors\n        needed to train all the regressors (one per step).\n        \n        Parameters\n        ----------\n        series : pandas DataFrame\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `series` and their indexes must be aligned.\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the creation\n            of the training matrices. See skforecast.exceptions.warn_skforecast_categories \n            for more information.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors) for each step. Note that the index \n            corresponds to that of the last step. It is updated for the corresponding \n            step in the filter_train_X_y_for_step method.\n        y_train : dict\n            Values of the time series related to each row of `X_train` for each \n            step in the form {step: y_step_[i]}.\n        \n        \"\"\"\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n        output = self._create_train_X_y(series=series, exog=exog)\n        X_train = output[0]\n        y_train = output[1]\n        set_skforecast_warnings(suppress_warnings, action='default')\n        return (X_train, y_train)\n\n    def filter_train_X_y_for_step(self, step: int, X_train: pd.DataFrame, y_train: dict, remove_suffix: bool=False) -> Tuple[pd.DataFrame, pd.Series]:\n        \"\"\"\n        Select the columns needed to train a forecaster for a specific step.  \n        The input matrices should be created using `_create_train_X_y` method. \n        This method updates the index of `X_train` to the corresponding one \n        according to `y_train`. If `remove_suffix=True` the suffix \"_step_i\" \n        will be removed from the column names. \n\n        Parameters\n        ----------\n        step : int\n            step for which columns must be selected selected. Starts at 1.\n        X_train : pandas DataFrame\n            Dataframe created with the `_create_train_X_y` method, first return.\n        y_train : dict\n            Dict created with the `_create_train_X_y` method, second return.\n        remove_suffix : bool, default `False`\n            If True, suffix \"_step_i\" is removed from the column names.\n\n        Returns\n        -------\n        X_train_step : pandas DataFrame\n            Training values (predictors) for the selected step.\n        y_train_step : pandas Series\n            Values of the time series related to each row of `X_train`.\n\n        \"\"\"\n        if step < 1 or step > self.steps:\n            raise ValueError(f'Invalid value `step`. For this forecaster, minimum value is 1 and the maximum step is {self.steps}.')\n        y_train_step = y_train[step]\n        if not self.exog_in_:\n            X_train_step = X_train\n        else:\n            n_lags = len(list(chain(*[v for v in self.lags_.values() if v is not None])))\n            n_window_features = len(self.X_train_window_features_names_out_) if self.window_features is not None else 0\n            idx_columns_autoreg = np.arange(n_lags + n_window_features)\n            n_exog = len(self.X_train_direct_exog_names_out_) / self.steps\n            idx_columns_exog = np.arange((step - 1) * n_exog, step * n_exog) + idx_columns_autoreg[-1] + 1\n            idx_columns = np.concatenate((idx_columns_autoreg, idx_columns_exog))\n            X_train_step = X_train.iloc[:, idx_columns]\n        X_train_step.index = y_train_step.index\n        if remove_suffix:\n            X_train_step.columns = [col_name.replace(f'_step_{step}', '') for col_name in X_train_step.columns]\n            y_train_step.name = y_train_step.name.replace(f'_step_{step}', '')\n        return (X_train_step, y_train_step)\n\n    def _train_test_split_one_step_ahead(self, series: pd.DataFrame, initial_train_size: int, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, dict, pd.DataFrame, dict, pd.Series, pd.Series]:\n        \"\"\"\n        Create matrices needed to train and test the forecaster for one-step-ahead\n        predictions.\n        \n        Parameters\n        ----------\n        series : pandas DataFrame\n            Training time series.\n        initial_train_size : int\n            Initial size of the training set. It is the number of observations used\n            to train the forecaster before making the first prediction.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `series` and their indexes must be aligned.\n        \n        Returns\n        -------\n        X_train : pandas DataFrame\n            Predictor values used to train the model.\n        y_train : dict\n            Values of the time series related to each row of `X_train` for each \n            step in the form {step: y_step_[i]}.\n        X_test : pandas DataFrame\n            Predictor values used to test the model.\n        y_test : dict\n            Values of the time series related to each row of `X_test` for each \n            step in the form {step: y_step_[i]}.\n        X_train_encoding : pandas Series\n            Series identifiers for each row of `X_train`.\n        X_test_encoding : pandas Series\n            Series identifiers for each row of `X_test`.\n        \n        \"\"\"\n        span_index = series.index\n        fold = [[0, initial_train_size], [initial_train_size - self.window_size, initial_train_size], [initial_train_size - self.window_size, len(span_index)], [0, 0], True]\n        data_fold = _extract_data_folds_multiseries(series=series, folds=[fold], span_index=span_index, window_size=self.window_size, exog=exog, dropna_last_window=self.dropna_from_series, externally_fitted=False)\n        series_train, _, levels_last_window, exog_train, exog_test, _ = next(data_fold)\n        start_test_idx = initial_train_size - self.window_size\n        series_test = series.iloc[start_test_idx:, :]\n        series_test = series_test.loc[:, levels_last_window]\n        series_test = series_test.dropna(axis=1, how='all')\n        _is_fitted = self.is_fitted\n        _series_names_in_ = self.series_names_in_\n        _exog_names_in_ = self.exog_names_in_\n        self.is_fitted = False\n        X_train, y_train, series_names_in_, _, exog_names_in_, *_ = self._create_train_X_y(series=series_train, exog=exog_train)\n        self.series_names_in_ = series_names_in_\n        if exog is not None:\n            self.exog_names_in_ = exog_names_in_\n        self.is_fitted = True\n        X_test, y_test, *_ = self._create_train_X_y(series=series_test, exog=exog_test)\n        self.is_fitted = _is_fitted\n        self.series_names_in_ = _series_names_in_\n        self.exog_names_in_ = _exog_names_in_\n        X_train_encoding = pd.Series(self.level, index=X_train.index)\n        X_test_encoding = pd.Series(self.level, index=X_test.index)\n        return (X_train, y_train, X_test, y_test, X_train_encoding, X_test_encoding)\n\n    def create_sample_weights(self, X_train: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Crate weights for each observation according to the forecaster's attribute\n        `weight_func`. \n\n        Parameters\n        ----------\n        X_train : pandas DataFrame\n            Dataframe created with `_create_train_X_y` and filter_train_X_y_for_step`\n            methods, first return.\n\n        Returns\n        -------\n        sample_weight : numpy ndarray\n            Weights to use in `fit` method.\n        \n        \"\"\"\n        sample_weight = None\n        if self.weight_func is not None:\n            sample_weight = self.weight_func(X_train.index)\n        if sample_weight is not None:\n            if np.isnan(sample_weight).any():\n                raise ValueError('The resulting `sample_weight` cannot have NaN values.')\n            if np.any(sample_weight < 0):\n                raise ValueError('The resulting `sample_weight` cannot have negative values.')\n            if np.sum(sample_weight) == 0:\n                raise ValueError('The resulting `sample_weight` cannot be normalized because the sum of the weights is zero.')\n        return sample_weight\n\n    def _create_predict_inputs(self, steps: Optional[Union[int, list]]=None, last_window: Optional[pd.DataFrame]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, check_inputs: bool=True) -> Tuple[list, list, list, pd.Index]:\n        \"\"\"\n        Create the inputs needed for the prediction process.\n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n\n        Returns\n        -------\n        Xs : list\n            List of numpy arrays with the predictors for each step.\n        Xs_col_names : list\n            Names of the columns of the matrix created internally for prediction.\n        steps : list\n            Steps to predict.\n        prediction_index : pandas Index\n            Index of the predictions.\n        \n        \"\"\"\n        steps = prepare_steps_direct(steps=steps, max_step=self.steps)\n        if last_window is None:\n            last_window = self.last_window_\n        if check_inputs:\n            check_predict_input(forecaster_name=type(self).__name__, steps=steps, is_fitted=self.is_fitted, exog_in_=self.exog_in_, index_type_=self.index_type_, index_freq_=self.index_freq_, window_size=self.window_size, last_window=last_window, exog=exog, exog_type_in_=self.exog_type_in_, exog_names_in_=self.exog_names_in_, interval=None, max_steps=self.steps, series_names_in_=self.X_train_series_names_in_)\n        last_window = last_window.iloc[-self.window_size:, last_window.columns.get_indexer(self.X_train_series_names_in_)].copy()\n        X_autoreg = []\n        Xs_col_names = []\n        for serie in self.X_train_series_names_in_:\n            last_window_serie = transform_numpy(array=last_window[serie].to_numpy(), transformer=self.transformer_series_[serie], fit=False, inverse_transform=False)\n            if self.differentiation is not None:\n                last_window_serie = self.differentiator_[serie].fit_transform(last_window_serie)\n            if self.lags is not None:\n                X_lags = last_window_serie[-self.lags_[serie]]\n                X_autoreg.append(X_lags)\n                Xs_col_names.extend(self.lags_names[serie])\n            if self.window_features is not None:\n                n_diff = 0 if self.differentiation is None else self.differentiation\n                X_window_features = np.concatenate([wf.transform(last_window_serie[n_diff:]) for wf in self.window_features])\n                X_autoreg.append(X_window_features)\n                Xs_col_names.extend([f'{serie}_{wf}' for wf in self.window_features_names])\n        X_autoreg = np.concatenate(X_autoreg).reshape(1, -1)\n        _, last_window_index = preprocess_last_window(last_window=last_window, return_values=False)\n        if exog is not None:\n            exog = input_to_frame(data=exog, input_name='exog')\n            exog = exog.loc[:, self.exog_names_in_]\n            exog = transform_dataframe(df=exog, transformer=self.transformer_exog, fit=False, inverse_transform=False)\n            check_exog_dtypes(exog=exog)\n            exog_values, _ = exog_to_direct_numpy(exog=exog.to_numpy()[:max(steps)], steps=max(steps))\n            exog_values = exog_values[0]\n            n_exog = exog.shape[1]\n            Xs = [np.concatenate([X_autoreg, exog_values[(step - 1) * n_exog:step * n_exog].reshape(1, -1)], axis=1) for step in steps]\n            Xs_col_names = Xs_col_names + exog.columns.to_list()\n        else:\n            Xs = [X_autoreg] * len(steps)\n        prediction_index = expand_index(index=last_window_index, steps=max(steps))[np.array(steps) - 1]\n        if isinstance(last_window_index, pd.DatetimeIndex) and np.array_equal(steps, np.arange(min(steps), max(steps) + 1)):\n            prediction_index.freq = last_window_index.freq\n        return (Xs, Xs_col_names, steps, prediction_index)\n\n    def create_predict_X(self, steps: Optional[Union[int, list]]=None, last_window: Optional[pd.DataFrame]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, suppress_warnings: bool=False) -> pd.DataFrame:\n        \"\"\"\n        Create the predictors needed to predict `steps` ahead.\n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the prediction \n            process. See skforecast.exceptions.warn_skforecast_categories for more\n            information.\n\n        Returns\n        -------\n        X_predict : pandas DataFrame\n            Pandas DataFrame with the predictors for each step. The index \n            is the same as the prediction index.\n        \n        \"\"\"\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n        Xs, Xs_col_names, steps, prediction_index = self._create_predict_inputs(steps=steps, last_window=last_window, exog=exog)\n        X_predict = pd.DataFrame(data=np.concatenate(Xs, axis=0), columns=Xs_col_names, index=prediction_index)\n        if self.transformer_series is not None or self.differentiation is not None:\n            warnings.warn('The output matrix is in the transformed scale due to the inclusion of transformations or differentiation in the Forecaster. As a result, any predictions generated using this matrix will also be in the transformed scale. Please refer to the documentation for more details: https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html', DataTransformationWarning)\n        set_skforecast_warnings(suppress_warnings, action='default')\n        return X_predict\n\n    def predict(self, steps: Optional[Union[int, list]]=None, last_window: Optional[pd.DataFrame]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, suppress_warnings: bool=False, check_inputs: bool=True, levels: Any=None) -> pd.DataFrame:\n        \"\"\"\n        Predict n steps ahead\n\n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the prediction \n            process. See skforecast.exceptions.warn_skforecast_categories for more\n            information.\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n        levels : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Predicted values.\n\n        \"\"\"\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n        Xs, _, steps, prediction_index = self._create_predict_inputs(steps=steps, last_window=last_window, exog=exog, check_inputs=check_inputs)\n        regressors = [self.regressors_[step] for step in steps]\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', message='X does not have valid feature names', category=UserWarning)\n            predictions = np.array([regressor.predict(X).ravel()[0] for regressor, X in zip(regressors, Xs)])\n        if self.differentiation is not None:\n            predictions = self.differentiator_[self.level].inverse_transform_next_window(predictions)\n        predictions = transform_numpy(array=predictions, transformer=self.transformer_series_[self.level], fit=False, inverse_transform=True)\n        predictions = pd.DataFrame(data=predictions, columns=[self.level], index=prediction_index)\n        set_skforecast_warnings(suppress_warnings, action='default')\n        return predictions\n\n    def predict_bootstrapping(self, steps: Optional[Union[int, list]]=None, last_window: Optional[pd.DataFrame]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, suppress_warnings: bool=False, levels: Any=None) -> pd.DataFrame:\n        \"\"\"\n        Generate multiple forecasting predictions using a bootstrapping process. \n        By sampling from a collection of past observed errors (the residuals),\n        each iteration of bootstrapping generates a different set of predictions. \n        See the Notes section for more information. \n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.     \n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.               \n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the prediction \n            process. See skforecast.exceptions.warn_skforecast_categories for more\n            information.\n        levels : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        boot_predictions : pandas DataFrame\n            Predictions generated by bootstrapping.\n            Shape: (steps, n_boot)\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n        Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n        \"\"\"\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n        if self.is_fitted:\n            steps = prepare_steps_direct(steps=steps, max_step=self.steps)\n            if use_in_sample_residuals:\n                if not set(steps).issubset(set(self.in_sample_residuals_.keys())):\n                    raise ValueError(f'Not `forecaster.in_sample_residuals_` for steps: {set(steps) - set(self.in_sample_residuals_.keys())}.')\n                residuals = self.in_sample_residuals_\n            else:\n                if self.out_sample_residuals_ is None:\n                    raise ValueError('`forecaster.out_sample_residuals_` is `None`. Use `use_in_sample_residuals=True` or the `set_out_sample_residuals()` method before predicting.')\n                elif not set(steps).issubset(set(self.out_sample_residuals_.keys())):\n                    raise ValueError(f'Not `forecaster.out_sample_residuals_` for steps: {set(steps) - set(self.out_sample_residuals_.keys())}. Use method `set_out_sample_residuals()`.')\n                residuals = self.out_sample_residuals_\n            check_residuals = 'forecaster.in_sample_residuals_' if use_in_sample_residuals else 'forecaster.out_sample_residuals_'\n            for step in steps:\n                if residuals[step] is None:\n                    raise ValueError(f'forecaster residuals for step {step} are `None`. Check {check_residuals}.')\n                elif any((element is None for element in residuals[step])) or np.any(np.isnan(residuals[step])):\n                    raise ValueError(f'forecaster residuals for step {step} contains `None` or `NaNs` values. Check {check_residuals}.')\n        Xs, _, steps, prediction_index = self._create_predict_inputs(steps=steps, last_window=last_window, exog=exog)\n        regressors = [self.regressors_[step] for step in steps]\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', message='X does not have valid feature names', category=UserWarning)\n            predictions = np.array([regressor.predict(X).ravel()[0] for regressor, X in zip(regressors, Xs)])\n        boot_predictions = np.tile(predictions, (n_boot, 1)).T\n        boot_columns = [f'pred_boot_{i}' for i in range(n_boot)]\n        rng = np.random.default_rng(seed=random_state)\n        for i, step in enumerate(steps):\n            sampled_residuals = residuals[step][rng.integers(low=0, high=len(residuals[step]), size=n_boot)]\n            boot_predictions[i, :] = boot_predictions[i, :] + sampled_residuals\n        if self.differentiation is not None:\n            boot_predictions = self.differentiator_[self.level].inverse_transform_next_window(boot_predictions)\n        if self.transformer_series_[self.level]:\n            boot_predictions = np.apply_along_axis(func1d=transform_numpy, axis=0, arr=boot_predictions, transformer=self.transformer_series_[self.level], fit=False, inverse_transform=True)\n        boot_predictions = pd.DataFrame(data=boot_predictions, index=prediction_index, columns=boot_columns)\n        set_skforecast_warnings(suppress_warnings, action='default')\n        return boot_predictions\n\n    def predict_interval(self, steps: Optional[Union[int, list]]=None, last_window: Optional[pd.DataFrame]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, interval: list=[5, 95], n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, suppress_warnings: bool=False, levels: Any=None) -> pd.DataFrame:\n        \"\"\"\n        Bootstrapping based predicted intervals.\n        Both predictions and intervals are returned.\n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        interval : list, default `[5, 95]`\n            Confidence of the prediction interval estimated. Sequence of \n            percentiles to compute, which must be between 0 and 100 inclusive. \n            For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the prediction \n            process. See skforecast.exceptions.warn_skforecast_categories for more\n            information.\n        levels : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Values predicted by the forecaster and their estimated interval.\n\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n        check_interval(interval=interval)\n        boot_predictions = self.predict_bootstrapping(steps=steps, last_window=last_window, exog=exog, n_boot=n_boot, random_state=random_state, use_in_sample_residuals=use_in_sample_residuals)\n        predictions = self.predict(steps=steps, last_window=last_window, exog=exog, check_inputs=False)\n        interval = np.array(interval) / 100\n        predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n        predictions_interval.columns = [f'{self.level}_lower_bound', f'{self.level}_upper_bound']\n        predictions = pd.concat((predictions, predictions_interval), axis=1)\n        set_skforecast_warnings(suppress_warnings, action='default')\n        return predictions\n\n    def predict_quantiles(self, steps: Optional[Union[int, list]]=None, last_window: Optional[pd.DataFrame]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, quantiles: list=[0.05, 0.5, 0.95], n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, suppress_warnings: bool=False, levels: Any=None) -> pd.DataFrame:\n        \"\"\"\n        Bootstrapping based predicted quantiles.\n        \n        Parameters\n        ----------\n        steps : int, list, None, default `None`\n            Predict n steps. The value of `steps` must be less than or equal to the \n            value of steps defined when initializing the forecaster. Starts at 1.\n        \n            - If `int`: Only steps within the range of 1 to int are predicted.\n            - If `list`: List of ints. Only the steps contained in the list \n            are predicted.\n            - If `None`: As many steps are predicted as were defined at \n            initialization.\n        last_window : pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed to \n            predict `steps`.\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        quantiles : list, default `[0.05, 0.5, 0.95]`\n            Sequence of quantiles to compute, which must be between 0 and 1 \n            inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n            `quantiles = [0.05, 0.5, 0.95]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate quantiles.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot quantiles are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create quantiles. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        suppress_warnings : bool, default `False`\n            If `True`, skforecast warnings will be suppressed during the prediction \n            process. See skforecast.exceptions.warn_skforecast_categories for more\n            information.\n        levels : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Quantiles predicted by the forecaster.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n        set_skforecast_warnings(suppress_warnings, action='ignore')\n        check_interval(quantiles=quantiles)\n        boot_predictions = self.predict_bootstrapping(steps=steps, last_window=last_window, exog=exog, n_boot=n_boot, random_state=random_state, use_in_sample_residuals=use_in_sample_residuals)\n        predictions = boot_predictions.quantile(q=quantiles, axis=1).transpose()\n        predictions.columns = [f'{self.level}_q_{q}' for q in quantiles]\n        set_skforecast_warnings(suppress_warnings, action='default')\n        return predictions\n\n    def set_params(self, params: dict) -> None:\n        \"\"\"\n        Set new values to the parameters of the scikit learn model stored in the\n        forecaster. It is important to note that all models share the same \n        configuration of parameters and hyperparameters.\n        \n        Parameters\n        ----------\n        params : dict\n            Parameters values.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        self.regressor = clone(self.regressor)\n        self.regressor.set_params(**params)\n        self.regressors_ = {step: clone(self.regressor) for step in range(1, self.steps + 1)}\n\n    def set_fit_kwargs(self, fit_kwargs: dict) -> None:\n        \"\"\"\n        Set new values for the additional keyword arguments passed to the `fit` \n        method of the regressor.\n        \n        Parameters\n        ----------\n        fit_kwargs : dict\n            Dict of the form {\"argument\": new_value}.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n\n    def set_lags(self, lags: Optional[Union[int, list, np.ndarray, range, dict]]=None) -> None:\n        \"\"\"\n        Set new value to the attribute `lags`. Attributes `lags_names`, \n        `max_lag` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        lags : int, list, numpy ndarray, range, dict, default `None`\n            Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n\n            - `int`: include lags from 1 to `lags` (included).\n            - `list`, `1d numpy ndarray` or `range`: include only lags present in \n            `lags`, all elements must be int.\n            - `dict`: create different lags for each series. {'series_column_name': lags}.\n            - `None`: no lags are included as predictors. \n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        if self.window_features is None and lags is None:\n            raise ValueError('At least one of the arguments `lags` or `window_features` must be different from None. This is required to create the predictors used in training the forecaster.')\n        if isinstance(lags, dict):\n            self.lags = {}\n            self.lags_names = {}\n            list_max_lags = []\n            for key in lags:\n                if lags[key] is None:\n                    self.lags[key] = None\n                    self.lags_names[key] = None\n                else:\n                    self.lags[key], lags_names, max_lag = initialize_lags(forecaster_name=type(self).__name__, lags=lags[key])\n                    self.lags_names[key] = [f'{key}_{lag}' for lag in lags_names] if lags_names is not None else None\n                    if max_lag is not None:\n                        list_max_lags.append(max_lag)\n            self.max_lag = max(list_max_lags) if len(list_max_lags) != 0 else None\n        else:\n            self.lags, self.lags_names, self.max_lag = initialize_lags(forecaster_name=type(self).__name__, lags=lags)\n        if self.window_features is None and (lags is None or self.max_lag is None):\n            raise ValueError('At least one of the arguments `lags` or `window_features` must be different from None. This is required to create the predictors used in training the forecaster.')\n        self.window_size = max([ws for ws in [self.max_lag, self.max_size_window_features] if ws is not None])\n        if self.differentiation is not None:\n            self.window_size += self.differentiation\n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_window_features(self, window_features: Optional[Union[object, list]]=None) -> None:\n        \"\"\"\n        Set new value to the attribute `window_features`. Attributes \n        `max_size_window_features`, `window_features_names`, \n        `window_features_class_names` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        window_features : object, list, default `None`\n            Instance or list of instances used to create window features. Window features\n            are created from the original time series and are included as predictors.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        if window_features is None and self.max_lag is None:\n            raise ValueError('At least one of the arguments `lags` or `window_features` must be different from None. This is required to create the predictors used in training the forecaster.')\n        self.window_features, self.window_features_names, self.max_size_window_features = initialize_window_features(window_features)\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [type(wf).__name__ for wf in self.window_features]\n        self.window_size = max([ws for ws in [self.max_lag, self.max_size_window_features] if ws is not None])\n        if self.differentiation is not None:\n            self.window_size += self.differentiation\n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_out_sample_residuals(self, y_true: dict, y_pred: dict, append: bool=False, random_state: int=123) -> None:\n        \"\"\"\n        Set new values to the attribute `out_sample_residuals_`. Out of sample\n        residuals are meant to be calculated using observations that did not\n        participate in the training process. `y_true` and `y_pred` are expected\n        to be in the original scale of the time series. Residuals are calculated\n        as `y_true` - `y_pred`, after applying the necessary transformations and\n        differentiations if the forecaster includes them (`self.transformer_series`\n        and `self.differentiation`).\n\n        A total of 10_000 residuals are stored in the attribute `out_sample_residuals_`.\n        If the number of residuals is greater than 10_000, a random sample of\n        10_000 residuals is stored.\n        \n        Parameters\n        ----------\n        y_true : dict\n            Dictionary of numpy ndarrays or pandas Series with the true values of\n            the time series for each model in the form {step: y_true}.\n        y_pred : dict\n            Dictionary of numpy ndarrays or pandas Series with the predicted values\n            of the time series for each model in the form {step: y_pred}.\n        append : bool, default `False`\n            If `True`, new residuals are added to the once already stored in the\n            attribute `out_sample_residuals_`. If after appending the new residuals,\n            the limit of 10_000 samples is exceeded, a random sample of 10_000 is\n            kept.\n        random_state : int, default `123`\n            Sets a seed to the random sampling for reproducible output.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n        if not self.is_fitted:\n            raise NotFittedError('This forecaster is not fitted yet. Call `fit` with appropriate arguments before using `set_out_sample_residuals()`.')\n        if not isinstance(y_true, dict):\n            raise TypeError(f'`y_true` must be a dictionary of numpy ndarrays or pandas Series. Got {type(y_true)}.')\n        if not isinstance(y_pred, dict):\n            raise TypeError(f'`y_pred` must be a dictionary of numpy ndarrays or pandas Series. Got {type(y_pred)}.')\n        if not set(y_true.keys()) == set(y_pred.keys()):\n            raise ValueError(f'`y_true` and `y_pred` must have the same keys. Got {set(y_true.keys())} and {set(y_pred.keys())}.')\n        for k in y_true.keys():\n            if not isinstance(y_true[k], (np.ndarray, pd.Series)):\n                raise TypeError(f'Values of `y_true` must be numpy ndarrays or pandas Series. Got {type(y_true[k])} for step {k}.')\n            if not isinstance(y_pred[k], (np.ndarray, pd.Series)):\n                raise TypeError(f'Values of `y_pred` must be numpy ndarrays or pandas Series. Got {type(y_pred[k])} for step {k}.')\n            if len(y_true[k]) != len(y_pred[k]):\n                raise ValueError(f'`y_true` and `y_pred` must have the same length. Got {len(y_true[k])} and {len(y_pred[k])} for step {k}.')\n            if isinstance(y_true[k], pd.Series) and isinstance(y_pred[k], pd.Series):\n                if not y_true[k].index.equals(y_pred[k].index):\n                    raise ValueError(f'When containing pandas Series, elements in `y_true` and `y_pred` must have the same index. Error in step {k}.')\n        if self.out_sample_residuals_ is None:\n            self.out_sample_residuals_ = {step: None for step in range(1, self.steps + 1)}\n        steps_to_update = set(range(1, self.steps + 1)).intersection(set(y_pred.keys()))\n        if not steps_to_update:\n            raise ValueError('Provided keys in `y_pred` and `y_true` do not match any step. Residuals cannot be updated.')\n        residuals = {}\n        rng = np.random.default_rng(seed=random_state)\n        y_true = y_true.copy()\n        y_pred = y_pred.copy()\n        if self.differentiation is not None:\n            differentiator = copy(self.differentiator)\n            differentiator.set_params(window_size=None)\n        for k in steps_to_update:\n            if isinstance(y_true[k], pd.Series):\n                y_true[k] = y_true[k].to_numpy()\n            if isinstance(y_pred[k], pd.Series):\n                y_pred[k] = y_pred[k].to_numpy()\n            if self.transformer_series:\n                y_true[k] = transform_numpy(array=y_true[k], transformer=self.transformer_series_[self.level], fit=False, inverse_transform=False)\n                y_pred[k] = transform_numpy(array=y_pred[k], transformer=self.transformer_series_[self.level], fit=False, inverse_transform=False)\n            if self.differentiation is not None:\n                y_true[k] = differentiator.fit_transform(y_true[k])[self.differentiation:]\n                y_pred[k] = differentiator.fit_transform(y_pred[k])[self.differentiation:]\n            residuals[k] = y_true[k] - y_pred[k]\n        for key, value in residuals.items():\n            if append and self.out_sample_residuals_[key] is not None:\n                value = np.concatenate((self.out_sample_residuals_[key], value))\n            if len(value) > 10000:\n                value = rng.choice(value, size=10000, replace=False)\n            self.out_sample_residuals_[key] = value\n\n    def get_feature_importances(self, step: int, sort_importance: bool=True) -> pd.DataFrame:\n        \"\"\"\n        Return feature importance of the model stored in the forecaster for a\n        specific step. Since a separate model is created for each forecast time\n        step, it is necessary to select the model from which retrieve information.\n        Only valid when regressor stores internally the feature importances in\n        the attribute `feature_importances_` or `coef_`. Otherwise, it returns  \n        `None`.\n\n        Parameters\n        ----------\n        step : int\n            Model from which retrieve information (a separate model is created \n            for each forecast time step). First step is 1.\n        sort_importance: bool, default `True`\n            If `True`, sorts the feature importances in descending order.\n\n        Returns\n        -------\n        feature_importances : pandas DataFrame\n            Feature importances associated with each predictor.\n        \n        \"\"\"\n        if not isinstance(step, int):\n            raise TypeError(f'`step` must be an integer. Got {type(step)}.')\n        if not self.is_fitted:\n            raise NotFittedError('This forecaster is not fitted yet. Call `fit` with appropriate arguments before using `get_feature_importances()`.')\n        if step < 1 or step > self.steps:\n            raise ValueError(f'The step must have a value from 1 to the maximum number of steps ({self.steps}). Got {step}.')\n        if isinstance(self.regressor, Pipeline):\n            estimator = self.regressors_[step][-1]\n        else:\n            estimator = self.regressors_[step]\n        n_lags = len(list(chain(*[v for v in self.lags_.values() if v is not None])))\n        n_window_features = len(self.X_train_window_features_names_out_) if self.window_features is not None else 0\n        idx_columns_autoreg = np.arange(n_lags + n_window_features)\n        if self.exog_in_:\n            idx_columns_exog = np.flatnonzero([name.endswith(f'step_{step}') for name in self.X_train_features_names_out_])\n        else:\n            idx_columns_exog = np.array([], dtype=int)\n        idx_columns = np.concatenate((idx_columns_autoreg, idx_columns_exog))\n        idx_columns = [int(x) for x in idx_columns]\n        feature_names = [self.X_train_features_names_out_[i].replace(f'_step_{step}', '') for i in idx_columns]\n        if hasattr(estimator, 'feature_importances_'):\n            feature_importances = estimator.feature_importances_\n        elif hasattr(estimator, 'coef_'):\n            feature_importances = estimator.coef_\n        else:\n            warnings.warn(f'Impossible to access feature importances for regressor of type {type(estimator)}. This method is only valid when the regressor stores internally the feature importances in the attribute `feature_importances_` or `coef_`.')\n            feature_importances = None\n        if feature_importances is not None:\n            feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importances})\n            if sort_importance:\n                feature_importances = feature_importances.sort_values(by='importance', ascending=False)\n        return feature_importances"
  },
  "call_tree": {
    "skforecast/direct/tests/tests_forecaster_direct_multivariate/test_predict_dist.py:test_predict_dist_output_when_forecaster_is_LinearRegression_steps_is_2_in_sample_residuals_True_exog_and_transformer": {
      "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:__init__": {
        "skforecast/utils/utils.py:initialize_lags": {},
        "skforecast/utils/utils.py:initialize_window_features": {},
        "skforecast/utils/utils.py:initialize_weights": {},
        "skforecast/utils/utils.py:check_select_fit_kwargs": {},
        "skforecast/utils/utils.py:select_n_jobs_fit_forecaster": {}
      },
      "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:fit": {
        "skforecast/utils/utils.py:set_skforecast_warnings": {},
        "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:_create_train_X_y": {
          "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:_create_data_to_return_dict": {},
          "skforecast/utils/utils.py:initialize_transformer_series": {},
          "skforecast/utils/utils.py:check_exog": {},
          "skforecast/utils/utils.py:input_to_frame": {},
          "skforecast/utils/utils.py:get_exog_dtypes": {},
          "skforecast/utils/utils.py:transform_dataframe": {},
          "skforecast/utils/utils.py:check_exog_dtypes": {
            "skforecast/utils/utils.py:check_exog": {}
          },
          "skforecast/utils/utils.py:check_y": {},
          "skforecast/utils/utils.py:transform_series": {},
          "skforecast/utils/utils.py:preprocess_y": {},
          "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:_create_lags": {},
          "skforecast/utils/utils.py:exog_to_direct_numpy": {}
        },
        "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:fit_forecaster": {
          "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:filter_train_X_y_for_step": {},
          "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:create_sample_weights": {}
        },
        "skforecast/utils/utils.py:preprocess_y": {}
      },
      "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:predict_dist": {
        "skforecast/utils/utils.py:set_skforecast_warnings": {},
        "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:predict_bootstrapping": {
          "skforecast/utils/utils.py:set_skforecast_warnings": {},
          "skforecast/utils/utils.py:prepare_steps_direct": {},
          "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:_create_predict_inputs": {
            "skforecast/utils/utils.py:prepare_steps_direct": {},
            "skforecast/utils/utils.py:check_predict_input": {
              "skforecast/utils/utils.py:preprocess_last_window": {},
              "skforecast/utils/utils.py:preprocess_exog": {},
              "skforecast/utils/utils.py:expand_index": {}
            },
            "skforecast/utils/utils.py:transform_numpy": {},
            "skforecast/utils/utils.py:preprocess_last_window": {},
            "skforecast/utils/utils.py:input_to_frame": {},
            "skforecast/utils/utils.py:transform_dataframe": {},
            "skforecast/utils/utils.py:check_exog_dtypes": {
              "skforecast/utils/utils.py:check_exog": {}
            },
            "skforecast/utils/utils.py:exog_to_direct_numpy": {},
            "skforecast/utils/utils.py:expand_index": {}
          },
          "skforecast/utils/utils.py:transform_numpy": {}
        }
      }
    },
    "skforecast/direct/tests/tests_forecaster_direct_multivariate/test_predict_dist.py:test_predict_dist_output_when_forecaster_is_LinearRegression_steps_is_2_in_sample_residuals_False_exog_and_transformer": {
      "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:__init__": {
        "skforecast/utils/utils.py:initialize_lags": {},
        "skforecast/utils/utils.py:initialize_window_features": {},
        "skforecast/utils/utils.py:initialize_weights": {},
        "skforecast/utils/utils.py:check_select_fit_kwargs": {},
        "skforecast/utils/utils.py:select_n_jobs_fit_forecaster": {}
      },
      "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:fit": {
        "skforecast/utils/utils.py:set_skforecast_warnings": {},
        "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:_create_train_X_y": {
          "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:_create_data_to_return_dict": {},
          "skforecast/utils/utils.py:initialize_transformer_series": {},
          "skforecast/utils/utils.py:check_exog": {},
          "skforecast/utils/utils.py:input_to_frame": {},
          "skforecast/utils/utils.py:get_exog_dtypes": {},
          "skforecast/utils/utils.py:transform_dataframe": {},
          "skforecast/utils/utils.py:check_exog_dtypes": {
            "skforecast/utils/utils.py:check_exog": {}
          },
          "skforecast/utils/utils.py:check_y": {},
          "skforecast/utils/utils.py:transform_series": {},
          "skforecast/utils/utils.py:preprocess_y": {},
          "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:_create_lags": {},
          "skforecast/utils/utils.py:exog_to_direct_numpy": {}
        },
        "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:fit_forecaster": {
          "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:filter_train_X_y_for_step": {},
          "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:create_sample_weights": {}
        },
        "skforecast/utils/utils.py:preprocess_y": {}
      },
      "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:predict_dist": {
        "skforecast/utils/utils.py:set_skforecast_warnings": {},
        "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:predict_bootstrapping": {
          "skforecast/utils/utils.py:set_skforecast_warnings": {},
          "skforecast/utils/utils.py:prepare_steps_direct": {},
          "skforecast/direct/_forecaster_direct_multivariate.py:ForecasterDirectMultiVariate:_create_predict_inputs": {
            "skforecast/utils/utils.py:prepare_steps_direct": {},
            "skforecast/utils/utils.py:check_predict_input": {
              "skforecast/utils/utils.py:preprocess_last_window": {},
              "skforecast/utils/utils.py:preprocess_exog": {},
              "skforecast/utils/utils.py:expand_index": {}
            },
            "skforecast/utils/utils.py:transform_numpy": {},
            "skforecast/utils/utils.py:preprocess_last_window": {},
            "skforecast/utils/utils.py:input_to_frame": {},
            "skforecast/utils/utils.py:transform_dataframe": {},
            "skforecast/utils/utils.py:check_exog_dtypes": {
              "skforecast/utils/utils.py:check_exog": {}
            },
            "skforecast/utils/utils.py:exog_to_direct_numpy": {},
            "skforecast/utils/utils.py:expand_index": {}
          },
          "skforecast/utils/utils.py:transform_numpy": {}
        }
      }
    }
  }
}