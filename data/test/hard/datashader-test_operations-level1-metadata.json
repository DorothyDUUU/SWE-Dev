{
  "dir_path": "/app/datashader",
  "package_name": "datashader",
  "sample_name": "datashader-test_operations",
  "src_dir": "datashader/",
  "test_dir": "datashader/tests/",
  "test_file": "datashader/datashape/tests/test_operations.py",
  "test_code": "from datashader import datashape\nimport pytest\n\n\ndef test_scalar_subarray():\n    assert datashape.int32.subarray(0) == datashape.int32\n    with pytest.raises(IndexError):\n        datashape.int32.subarray(1)\n    assert datashape.string.subarray(0) == datashape.string\n    with pytest.raises(IndexError):\n        datashape.string.subarray(1)\n\n\ndef test_array_subarray():\n    assert (datashape.dshape('3 * int32').subarray(0) ==\n            datashape.dshape('3 * int32'))\n    assert (datashape.dshape('3 * int32').subarray(1) ==\n            datashape.DataShape(datashape.int32))\n    assert (str(datashape.dshape('3 * var * M * int32').subarray(2)) ==\n            str(datashape.dshape('M * int32')))\n    assert (str(datashape.dshape('3 * var * M * float64').subarray(3)) ==\n            str(datashape.float64))\n\n\ndef test_dshape_compare():\n    assert datashape.int32 != datashape.dshape('1 * int32')\n",
  "GT_file_code": {
    "datashader/datashape/coretypes.py": "\"\"\"\nThis defines the DataShape type system, with unified\nshape and data type.\n\"\"\"\n\nimport ctypes\nimport operator\n\nfrom collections import OrderedDict\nfrom math import ceil\n\nfrom datashader import datashape\n\nimport numpy as np\n\nfrom .internal_utils import IndexCallable, isidentifier\n\n\n# Classes of unit types.\nDIMENSION = 1\nMEASURE = 2\n\n\nclass Type(type):\n    _registry = {}\n\n    def __new__(meta, name, bases, dct):\n        cls = super(Type, meta).__new__(meta, name, bases, dct)  # noqa: UP008\n        # Don't register abstract classes\n        if not dct.get('abstract'):\n            Type._registry[name] = cls\n        return cls\n\n    @classmethod\n    def register(cls, name, type):\n        # Don't clobber existing types.\n        if name in cls._registry:\n            raise TypeError('There is another type registered with name %s'\n                            % name)\n\n        cls._registry[name] = type\n\n    @classmethod\n    def lookup_type(cls, name):\n        return cls._registry[name]\n\n\nclass Mono(metaclass=Type):\n\n    \"\"\"\n    Monotype are unqualified 0 parameters.\n\n    Each type must be reconstructable using its parameters:\n\n        type(datashape_type)(*type.parameters)\n    \"\"\"\n\n    composite = False\n\n    def __init__(self, *params):\n        self._parameters = params\n\n    @property\n    def _slotted(self):\n        return hasattr(self, '__slots__')\n\n    @property\n    def parameters(self):\n        if self._slotted:\n            return tuple(getattr(self, slot) for slot in self.__slots__)\n        else:\n            return self._parameters\n\n    def info(self):\n        return type(self), self.parameters\n\n    def __eq__(self, other):\n        return (isinstance(other, Mono) and\n                self.shape == other.shape and\n                self.measure.info() == other.measure.info())\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __hash__(self):\n        try:\n            h = self._hash\n        except AttributeError:\n            h = self._hash = hash(self.shape) ^ hash(self.measure.info())\n        return h\n\n    @property\n    def shape(self):\n        return ()\n\n    def __len__(self):\n        return 1\n\n    def __getitem__(self, key):\n        return [self][key]\n\n    def __repr__(self):\n        return '%s(%s)' % (\n            type(self).__name__,\n            ', '.join(\n                (\n                    '%s=%r' % (slot, getattr(self, slot))\n                    for slot in self.__slots__\n                ) if self._slotted else\n                map(repr, self.parameters),\n            ),\n        )\n\n    # Monotypes are their own measure\n    @property\n    def measure(self):\n        return self\n\n    def subarray(self, leading):\n        \"\"\"Returns a data shape object of the subarray with 'leading'\n        dimensions removed. In the case of a measure such as CType,\n        'leading' must be 0, and self is returned.\n        \"\"\"\n        if leading >= 1:\n            raise IndexError(('Not enough dimensions in data shape '\n                              'to remove %d leading dimensions.') % leading)\n        else:\n            return self\n\n    def __mul__(self, other):\n        if isinstance(other, str):\n            from datashader import datashape\n            return datashape.dshape(other).__rmul__(self)\n        if isinstance(other, int):\n            other = Fixed(other)\n        if isinstance(other, DataShape):\n            return other.__rmul__(self)\n\n        return DataShape(self, other)\n\n    def __rmul__(self, other):\n        if isinstance(other, str):\n            from datashader import datashape\n            return self * datashape.dshape(other)\n        if isinstance(other, int):\n            other = Fixed(other)\n\n        return DataShape(other, self)\n\n    def __getstate__(self):\n        return self.parameters\n\n    def __setstate__(self, state):\n        if self._slotted:\n            for slot, val in zip(self.__slots__, state):\n                setattr(self, slot, val)\n        else:\n            self._parameters = state\n\n    def to_numpy_dtype(self):\n        raise TypeError('DataShape %s is not NumPy-compatible' % self)\n\n\nclass Unit(Mono):\n\n    \"\"\"\n    Unit type that does not need to be reconstructed.\n    \"\"\"\n\n    def __str__(self):\n        return type(self).__name__.lower()\n\n\nclass Ellipsis(Mono):\n\n    \"\"\"Ellipsis (...). Used to indicate a variable number of dimensions.\n\n    E.g.:\n\n        ... * float32    # float32 array w/ any number of dimensions\n        A... * float32   # float32 array w/ any number of dimensions,\n                        # associated with type variable A\n    \"\"\"\n    __slots__ = 'typevar',\n\n    def __init__(self, typevar=None):\n        self.typevar = typevar\n\n    def __str__(self):\n        return str(self.typevar) + '...' if self.typevar else '...'\n\n    def __repr__(self):\n        return '%s(%r)' % (type(self).__name__, str(self))\n\n\nclass Null(Unit):\n\n    \"\"\"The null datashape.\"\"\"\n    pass\n\n\nclass Date(Unit):\n\n    \"\"\" Date type \"\"\"\n    cls = MEASURE\n    __slots__ = ()\n\n    def to_numpy_dtype(self):\n        return np.dtype('datetime64[D]')\n\n\nclass Time(Unit):\n\n    \"\"\" Time type \"\"\"\n    cls = MEASURE\n    __slots__ = 'tz',\n\n    def __init__(self, tz=None):\n        if tz is not None and not isinstance(tz, str):\n            raise TypeError('tz parameter to time datashape must be a string')\n        # TODO validate against Olson tz database\n        self.tz = tz\n\n    def __str__(self):\n        basename = super().__str__()\n        if self.tz is None:\n            return basename\n        else:\n            return '%s[tz=%r]' % (basename, str(self.tz))\n\n\nclass DateTime(Unit):\n\n    \"\"\" DateTime type \"\"\"\n    cls = MEASURE\n    __slots__ = 'tz',\n\n    def __init__(self, tz=None):\n        if tz is not None and not isinstance(tz, str):\n            raise TypeError('tz parameter to datetime datashape must be a '\n                            'string')\n        # TODO validate against Olson tz database\n        self.tz = tz\n\n    def __str__(self):\n        basename = super().__str__()\n        if self.tz is None:\n            return basename\n        else:\n            return '%s[tz=%r]' % (basename, str(self.tz))\n\n    def to_numpy_dtype(self):\n        return np.dtype('datetime64[us]')\n\n\n_units = ('ns', 'us', 'ms', 's', 'm', 'h', 'D', 'W', 'M', 'Y')\n\n\n_unit_aliases = {\n    'year': 'Y',\n    'week': 'W',\n    'day': 'D',\n    'date': 'D',\n    'hour': 'h',\n    'second': 's',\n    'millisecond': 'ms',\n    'microsecond': 'us',\n    'nanosecond': 'ns'\n}\n\n\ndef normalize_time_unit(s):\n    \"\"\" Normalize time input to one of 'year', 'second', 'millisecond', etc..\n    Example\n    -------\n    >>> normalize_time_unit('milliseconds')\n    'ms'\n    >>> normalize_time_unit('ms')\n    'ms'\n    >>> normalize_time_unit('nanoseconds')\n    'ns'\n    >>> normalize_time_unit('nanosecond')\n    'ns'\n    \"\"\"\n    s = s.strip()\n    if s in _units:\n        return s\n    if s in _unit_aliases:\n        return _unit_aliases[s]\n    if s[-1] == 's' and len(s) > 2:\n        return normalize_time_unit(s.rstrip('s'))\n\n    raise ValueError(\"Do not understand time unit %s\" % s)\n\n\nclass TimeDelta(Unit):\n    cls = MEASURE\n    __slots__ = 'unit',\n\n    def __init__(self, unit='us'):\n        self.unit = normalize_time_unit(str(unit))\n\n    def __str__(self):\n        return 'timedelta[unit=%r]' % self.unit\n\n    def to_numpy_dtype(self):\n        return np.dtype('timedelta64[%s]' % self.unit)\n\n\nclass Units(Unit):\n    \"\"\" Units type for values with physical units \"\"\"\n    cls = MEASURE\n    __slots__ = 'unit', 'tp'\n\n    def __init__(self, unit, tp=None):\n        if not isinstance(unit, str):\n            raise TypeError('unit parameter to units datashape must be a '\n                            'string')\n        if tp is None:\n            tp = DataShape(float64)\n        elif not isinstance(tp, DataShape):\n            raise TypeError('tp parameter to units datashape must be a '\n                            'datashape type')\n        self.unit = unit\n        self.tp = tp\n\n    def __str__(self):\n        if self.tp == DataShape(float64):\n            return 'units[%r]' % (self.unit)\n        else:\n            return 'units[%r, %s]' % (self.unit, self.tp)\n\n\nclass Bytes(Unit):\n\n    \"\"\" Bytes type \"\"\"\n    cls = MEASURE\n    __slots__ = ()\n\n\n_canonical_string_encodings = {\n    'A': 'A',\n    'ascii': 'A',\n    'U8': 'U8',\n    'utf-8': 'U8',\n    'utf_8': 'U8',\n    'utf8': 'U8',\n    'U16': 'U16',\n    'utf-16': 'U16',\n    'utf_16': 'U16',\n    'utf16': 'U16',\n    'U32': 'U32',\n    'utf-32': 'U32',\n    'utf_32': 'U32',\n    'utf32': 'U32',\n}\n\n\nclass String(Unit):\n\n    \"\"\" String container\n\n    >>> String()\n    ctype(\"string\")\n    >>> String(10, 'ascii')\n    ctype(\"string[10, 'A']\")\n    \"\"\"\n    cls = MEASURE\n    __slots__ = 'fixlen', 'encoding'\n\n    def __init__(self, *args):\n        if len(args) == 0:\n            fixlen, encoding = None, None\n        if len(args) == 1:\n            if isinstance(args[0], str):\n                fixlen, encoding = None, args[0]\n            if isinstance(args[0], int):\n                fixlen, encoding = args[0], None\n        elif len(args) == 2:\n            fixlen, encoding = args\n\n        encoding = encoding or 'U8'\n        if isinstance(encoding, str):\n            encoding = str(encoding)\n        try:\n            encoding = _canonical_string_encodings[encoding]\n        except KeyError:\n            raise ValueError('Unsupported string encoding %s' %\n                             repr(encoding))\n\n        self.encoding = encoding\n        self.fixlen = fixlen\n\n        # Put it in a canonical form\n\n    def __str__(self):\n        if self.fixlen is None and self.encoding == 'U8':\n            return 'string'\n        elif self.fixlen is not None and self.encoding == 'U8':\n            return 'string[%i]' % self.fixlen\n        elif self.fixlen is None and self.encoding != 'U8':\n            return 'string[%s]' % repr(self.encoding).strip('u')\n        else:\n            return 'string[%i, %s]' % (self.fixlen,\n                                       repr(self.encoding).strip('u'))\n\n    def __repr__(self):\n        s = str(self)\n        return 'ctype(\"%s\")' % s.encode('unicode_escape').decode('ascii')\n\n    def to_numpy_dtype(self):\n        \"\"\"\n        >>> String().to_numpy_dtype()\n        dtype('O')\n        >>> String(30).to_numpy_dtype()\n        dtype('<U30')\n        >>> String(30, 'A').to_numpy_dtype()\n        dtype('S30')\n        \"\"\"\n        if self.fixlen:\n            if self.encoding == 'A':\n                return np.dtype('S%d' % self.fixlen)\n            else:\n                return np.dtype('U%d' % self.fixlen)\n\n        # Create a dtype with metadata indicating it's\n        # a string in the same style as the h5py special_dtype\n        return np.dtype('O', metadata={'vlen': str})\n\n\nclass Decimal(Unit):\n\n    \"\"\"Decimal type corresponding to SQL Decimal/Numeric types.\n\n    The first parameter passed specifies the number of digits of precision that\n    the Decimal contains. If an additional parameter is given, it represents\n    the scale, or number of digits of precision that are after the decimal\n    point.\n\n    The Decimal type makes no requirement of how it is to be stored in memory,\n    therefore, the number of bytes needed to store a Decimal for a given\n    precision will vary based on the platform where it is used.\n\n    Examples\n    --------\n    >>> Decimal(18)\n    Decimal(precision=18, scale=0)\n    >>> Decimal(7, 4)\n    Decimal(precision=7, scale=4)\n    >>> Decimal(precision=11, scale=2)\n    Decimal(precision=11, scale=2)\n    \"\"\"\n\n    cls = MEASURE\n    __slots__ = 'precision', 'scale'\n\n    def __init__(self, precision, scale=0):\n        self.precision = precision\n        self.scale = scale\n\n    def __str__(self):\n        return 'decimal[precision={precision}, scale={scale}]'.format(\n            precision=self.precision, scale=self.scale\n        )\n\n    def to_numpy_dtype(self):\n        \"\"\"Convert a decimal datashape to a NumPy dtype.\n\n        Note that floating-point (scale > 0) precision will be lost converting\n        to NumPy floats.\n\n        Examples\n        --------\n        >>> Decimal(18).to_numpy_dtype()\n        dtype('int64')\n        >>> Decimal(7,4).to_numpy_dtype()\n        dtype('float64')\n        \"\"\"\n\n        if self.scale == 0:\n            if self.precision <= 2:\n                return np.dtype(np.int8)\n            elif self.precision <= 4:\n                return np.dtype(np.int16)\n            elif self.precision <= 9:\n                return np.dtype(np.int32)\n            elif self.precision <= 18:\n                return np.dtype(np.int64)\n            else:\n                raise TypeError(\n                    'Integer Decimal precision > 18 is not NumPy-compatible')\n        else:\n            return np.dtype(np.float64)\n\n\nclass DataShape(Mono):\n\n    \"\"\"\n    Composite container for datashape elements.\n\n    Elements of a datashape like ``Fixed(3)``, ``Var()`` or ``int32`` are on,\n    on their own, valid datashapes.  These elements are collected together into\n    a composite ``DataShape`` to be complete.\n\n    This class is not intended to be used directly.  Instead, use the utility\n    ``dshape`` function to create datashapes from strings or datashape\n    elements.\n\n    Examples\n    --------\n\n    >>> from datashader.datashape import Fixed, int32, DataShape, dshape\n\n    >>> DataShape(Fixed(5), int32)  # Rare to DataShape directly\n    dshape(\"5 * int32\")\n\n    >>> dshape('5 * int32')         # Instead use the dshape function\n    dshape(\"5 * int32\")\n\n    >>> dshape([Fixed(5), int32])   # It can even do construction from elements\n    dshape(\"5 * int32\")\n\n    See Also\n    --------\n    datashape.dshape\n    \"\"\"\n    composite = False\n\n    def __init__(self, *parameters, **kwds):\n        if len(parameters) == 1 and isinstance(parameters[0], str):\n            raise TypeError(\"DataShape constructor for internal use.\\n\"\n                            \"Use dshape function to convert strings into \"\n                            \"datashapes.\\nTry:\\n\\tdshape('%s')\"\n                            % parameters[0])\n        if len(parameters) > 0:\n            self._parameters = tuple(map(_launder, parameters))\n            if getattr(self._parameters[-1], 'cls', MEASURE) != MEASURE:\n                raise TypeError(('Only a measure can appear on the'\n                                 ' last position of a datashape, not %s') %\n                                repr(self._parameters[-1]))\n            for dim in self._parameters[:-1]:\n                if getattr(dim, 'cls', DIMENSION) != DIMENSION:\n                    raise TypeError(('Only dimensions can appear before the'\n                                     ' last position of a datashape, not %s') %\n                                    repr(dim))\n        else:\n            raise ValueError('the data shape should be constructed from 2 or'\n                             ' more parameters, only got %s' % len(parameters))\n        self.composite = True\n        self.name = kwds.get('name')\n\n        if self.name:\n            type(type(self))._registry[self.name] = self\n\n    def __len__(self):\n        return len(self.parameters)\n\n    def __getitem__(self, index):\n        return self.parameters[index]\n\n    def __str__(self):\n        return self.name or ' * '.join(map(str, self.parameters))\n\n    def __repr__(self):\n        s = pprint(self)\n        if '\\n' in s:\n            return 'dshape(\"\"\"%s\"\"\")' % s\n        else:\n            return 'dshape(\"%s\")' % s\n\n    @property\n    def shape(self):\n        return self.parameters[:-1]\n\n    @property\n    def measure(self):\n        return self.parameters[-1]\n\n    def subarray(self, leading):\n        \"\"\"Returns a data shape object of the subarray with 'leading'\n        dimensions removed.\n\n        >>> from datashader.datashape import dshape\n        >>> dshape('1 * 2 * 3 * int32').subarray(1)\n        dshape(\"2 * 3 * int32\")\n        >>> dshape('1 * 2 * 3 * int32').subarray(2)\n        dshape(\"3 * int32\")\n        \"\"\"\n        if leading >= len(self.parameters):\n            raise IndexError('Not enough dimensions in data shape '\n                             'to remove %d leading dimensions.' % leading)\n        elif leading in [len(self.parameters) - 1, -1]:\n            return DataShape(self.parameters[-1])\n        else:\n            return DataShape(*self.parameters[leading:])\n\n    def __rmul__(self, other):\n        if isinstance(other, int):\n            other = Fixed(other)\n        return DataShape(other, *self)\n\n    @property\n    def subshape(self):\n        return IndexCallable(self._subshape)\n\n    def _subshape(self, index):\n        \"\"\" The DataShape of an indexed subarray\n\n        >>> from datashader.datashape import dshape\n\n        >>> ds = dshape('var * {name: string, amount: int32}')\n        >>> print(ds.subshape[0])\n        {name: string, amount: int32}\n\n        >>> print(ds.subshape[0:3])\n        3 * {name: string, amount: int32}\n\n        >>> print(ds.subshape[0:7:2, 'amount'])\n        4 * int32\n\n        >>> print(ds.subshape[[1, 10, 15]])\n        3 * {name: string, amount: int32}\n\n        >>> ds = dshape('{x: int, y: int}')\n        >>> print(ds.subshape['x'])\n        int32\n\n        >>> ds = dshape('10 * var * 10 * int32')\n        >>> print(ds.subshape[0:5, 0:3, 5])\n        5 * 3 * int32\n\n        >>> ds = dshape('var * {name: string, amount: int32, id: int32}')\n        >>> print(ds.subshape[:, [0, 2]])\n        var * {name: string, id: int32}\n\n        >>> ds = dshape('var * {name: string, amount: int32, id: int32}')\n        >>> print(ds.subshape[:, ['name', 'id']])\n        var * {name: string, id: int32}\n\n        >>> print(ds.subshape[0, 1:])\n        {amount: int32, id: int32}\n        \"\"\"\n        from .predicates import isdimension\n        if isinstance(index, int) and isdimension(self[0]):\n            return self.subarray(1)\n        if isinstance(self[0], Record) and isinstance(index, str):\n            return self[0][index]\n        if isinstance(self[0], Record) and isinstance(index, int):\n            return self[0].parameters[0][index][1]\n        if isinstance(self[0], Record) and isinstance(index, list):\n            rec = self[0]\n            # Translate strings to corresponding integers\n            index = [self[0].names.index(i) if isinstance(i, str) else i\n                     for i in index]\n            return DataShape(Record([rec.parameters[0][i] for i in index]))\n        if isinstance(self[0], Record) and isinstance(index, slice):\n            rec = self[0]\n            return DataShape(Record(rec.parameters[0][index]))\n        if isinstance(index, list) and isdimension(self[0]):\n            return len(index) * self.subarray(1)\n        if isinstance(index, slice):\n            if isinstance(self[0], Fixed):\n                n = int(self[0])\n                start = index.start or 0\n                stop = index.stop or n\n                if start < 0:\n                    start = n + start\n                if stop < 0:\n                    stop = n + stop\n                count = stop - start\n            else:\n                start = index.start or 0\n                stop = index.stop\n                if not stop:\n                    count = -start if start < 0 else var\n                if (stop is not None and start is not None and stop >= 0 and\n                        start >= 0):\n                    count = stop - start\n                else:\n                    count = var\n\n            if count != var and index.step is not None:\n                count = int(ceil(count / index.step))\n\n            return count * self.subarray(1)\n        if isinstance(index, tuple):\n            if not index:\n                return self\n            elif index[0] is None:\n                return 1 * self._subshape(index[1:])\n            elif len(index) == 1:\n                return self._subshape(index[0])\n            else:\n                ds = self.subarray(1)._subshape(index[1:])\n                return (self[0] * ds)._subshape(index[0])\n        raise TypeError('invalid index value %s of type %r' %\n                        (index, type(index).__name__))\n\n    def __setstate__(self, state):\n        self._parameters = state\n        self.composite = True\n        self.name = None\n\n\nnumpy_provides_missing = frozenset((Date, DateTime, TimeDelta))\n\n\nclass Option(Mono):\n\n    \"\"\"\n    Measure types which may or may not hold data. Makes no\n    indication of how this is implemented in memory.\n    \"\"\"\n    __slots__ = 'ty',\n\n    def __init__(self, ds):\n        self.ty = _launder(ds)\n\n    @property\n    def shape(self):\n        return self.ty.shape\n\n    @property\n    def itemsize(self):\n        return self.ty.itemsize\n\n    def __str__(self):\n        return '?%s' % self.ty\n\n    def to_numpy_dtype(self):\n        if type(self.ty) in numpy_provides_missing:\n            return self.ty.to_numpy_dtype()\n        raise TypeError('DataShape measure %s is not NumPy-compatible' % self)\n\n\nclass CType(Unit):\n\n    \"\"\"\n    Symbol for a sized type mapping uniquely to a native type.\n    \"\"\"\n    cls = MEASURE\n    __slots__ = 'name', '_itemsize', '_alignment'\n\n    def __init__(self, name, itemsize, alignment):\n        self.name = name\n        self._itemsize = itemsize\n        self._alignment = alignment\n        Type.register(name, self)\n\n    @classmethod\n    def from_numpy_dtype(self, dt):\n        \"\"\"\n        From Numpy dtype.\n\n        >>> from datashader.datashape import CType\n        >>> from numpy import dtype\n        >>> CType.from_numpy_dtype(dtype('int32'))\n        ctype(\"int32\")\n        >>> CType.from_numpy_dtype(dtype('i8'))\n        ctype(\"int64\")\n        >>> CType.from_numpy_dtype(dtype('M8'))\n        DateTime(tz=None)\n        >>> CType.from_numpy_dtype(dtype('U30'))  # doctest: +SKIP\n        ctype(\"string[30, 'U32']\")\n        \"\"\"\n        try:\n            return Type.lookup_type(dt.name)\n        except KeyError:\n            pass\n        if np.issubdtype(dt, np.datetime64):\n            unit, _ = np.datetime_data(dt)\n            defaults = {'D': date_, 'Y': date_, 'M': date_, 'W': date_}\n            return defaults.get(unit, datetime_)\n        elif np.issubdtype(dt, np.timedelta64):\n            unit, _ = np.datetime_data(dt)\n            return TimeDelta(unit=unit)\n        elif np.__version__[0] < \"2\" and np.issubdtype(dt, np.unicode_):  # noqa: NPY201\n            return String(dt.itemsize // 4, 'U32')\n        elif np.issubdtype(dt, np.str_) or np.issubdtype(dt, np.bytes_):\n            return String(dt.itemsize, 'ascii')\n        raise NotImplementedError(\"NumPy datatype %s not supported\" % dt)\n\n    @property\n    def itemsize(self):\n        \"\"\"The size of one element of this type.\"\"\"\n        return self._itemsize\n\n    @property\n    def alignment(self):\n        \"\"\"The alignment of one element of this type.\"\"\"\n        return self._alignment\n\n    def to_numpy_dtype(self):\n        \"\"\"\n        To Numpy dtype.\n        \"\"\"\n        # TODO: Fixup the complex type to how numpy does it\n        name = self.name\n        return np.dtype({\n            'complex[float32]': 'complex64',\n            'complex[float64]': 'complex128'\n        }.get(name, name))\n\n    def __str__(self):\n        return self.name\n\n    def __repr__(self):\n        s = str(self)\n        return 'ctype(\"%s\")' % s.encode('unicode_escape').decode('ascii')\n\n\nclass Fixed(Unit):\n\n    \"\"\"\n    Fixed dimension.\n    \"\"\"\n    cls = DIMENSION\n    __slots__ = 'val',\n\n    def __init__(self, i):\n        # Use operator.index, so Python integers, numpy int scalars, etc work\n        i = operator.index(i)\n\n        if i < 0:\n            raise ValueError('Fixed dimensions must be positive')\n\n        self.val = i\n\n    def __index__(self):\n        return self.val\n\n    def __int__(self):\n        return self.val\n\n    def __eq__(self, other):\n        return (type(other) is Fixed and self.val == other.val or\n                isinstance(other, int) and self.val == other)\n\n    __hash__ = Mono.__hash__\n\n    def __str__(self):\n        return str(self.val)\n\n\nclass Var(Unit):\n\n    \"\"\" Variable dimension \"\"\"\n    cls = DIMENSION\n    __slots__ = ()\n\n\nclass TypeVar(Unit):\n\n    \"\"\"\n    A free variable in the signature. Not user facing.\n    \"\"\"\n    # cls could be MEASURE or DIMENSION, depending on context\n    __slots__ = 'symbol',\n\n    def __init__(self, symbol):\n        if not symbol[0].isupper():\n            raise ValueError(('TypeVar symbol %r does not '\n                              'begin with a capital') % symbol)\n        self.symbol = symbol\n\n    def __str__(self):\n        return str(self.symbol)\n\n\nclass Function(Mono):\n    \"\"\"Function signature type\n    \"\"\"\n    @property\n    def restype(self):\n        return self.parameters[-1]\n\n    @property\n    def argtypes(self):\n        return self.parameters[:-1]\n\n    def __str__(self):\n        return '(%s) -> %s' % (\n            ', '.join(map(str, self.argtypes)), self.restype\n        )\n\n\nclass Map(Mono):\n    __slots__ = 'key', 'value'\n\n    def __init__(self, key, value):\n        self.key = _launder(key)\n        self.value = _launder(value)\n\n    def __str__(self):\n        return '%s[%s, %s]' % (type(self).__name__.lower(),\n                               self.key,\n                               self.value)\n\n    def to_numpy_dtype(self):\n        return to_numpy_dtype(self)\n\n\ndef _launder(x):\n    \"\"\" Clean up types prior to insertion into DataShape\n\n    >>> from datashader.datashape import dshape\n    >>> _launder(5)         # convert ints to Fixed\n    Fixed(val=5)\n    >>> _launder('int32')   # parse strings\n    ctype(\"int32\")\n    >>> _launder(dshape('int32'))\n    ctype(\"int32\")\n    >>> _launder(Fixed(5))  # No-op on valid parameters\n    Fixed(val=5)\n    \"\"\"\n    if isinstance(x, int):\n        x = Fixed(x)\n    if isinstance(x, str):\n        x = datashape.dshape(x)\n    if isinstance(x, DataShape) and len(x) == 1:\n        return x[0]\n    if isinstance(x, Mono):\n        return x\n    return x\n\n\nclass CollectionPrinter:\n\n    def __repr__(self):\n        s = str(self)\n        strs = ('\"\"\"%s\"\"\"' if '\\n' in s else '\"%s\"') % s\n        return 'dshape(%s)' % strs\n\n\nclass RecordMeta(Type):\n    @staticmethod\n    def _unpack_slice(s, idx):\n        if not isinstance(s, slice):\n            raise TypeError(\n                'invalid field specification at position %d.\\n'\n                'fields must be formatted like: {name}:{type}' % idx,\n            )\n\n        name, type_ = packed = s.start, s.stop\n        if name is None:\n            raise TypeError('missing field name at position %d' % idx)\n        if not isinstance(name, str):\n            raise TypeError(\n                \"field name at position %d ('%s') was not a string\" % (\n                    idx, name,\n                ),\n            )\n        if type_ is None and s.step is None:\n            raise TypeError(\n                \"missing type for field '%s' at position %d\" % (name, idx))\n        if s.step is not None:\n            raise TypeError(\n                \"unexpected slice step for field '%s' at position %d.\\n\"\n                \"hint: you might have a second ':'\" % (name, idx),\n            )\n\n        return packed\n\n    def __getitem__(self, types):\n        if not isinstance(types, tuple):\n            types = types,\n\n        return self(list(map(self._unpack_slice, types, range(len(types)))))\n\n\nclass Record(CollectionPrinter, Mono, metaclass=RecordMeta):\n    \"\"\"\n    A composite data structure of ordered fields mapped to types.\n\n    Properties\n    ----------\n\n    fields: tuple of (name, type) pairs\n        The only stored data, also the input to ``__init__``\n    dict: dict\n        A dictionary view of ``fields``\n    names: list of strings\n        A list of the names\n    types: list of datashapes\n        A list of the datashapes\n\n    Example\n    -------\n\n    >>> Record([['id', 'int'], ['name', 'string'], ['amount', 'real']])\n    dshape(\"{id: int32, name: string, amount: float64}\")\n    \"\"\"\n    cls = MEASURE\n\n    def __init__(self, fields):\n        \"\"\"\n        Parameters\n        ----------\n        fields : list/OrderedDict of (name, type) entries\n            The fields which make up the record.\n        \"\"\"\n        if isinstance(fields, OrderedDict):\n            fields = fields.items()\n        fields = list(fields)\n        names = [\n            str(name) if not isinstance(name, str) else name\n            for name, _ in fields\n        ]\n        types = [_launder(v) for _, v in fields]\n\n        if len(set(names)) != len(names):\n            for name in set(names):\n                names.remove(name)\n            raise ValueError(\"duplicate field names found: %s\" % names)\n\n        self._parameters = tuple(zip(names, types)),\n\n    @property\n    def fields(self):\n        return self._parameters[0]\n\n    @property\n    def dict(self):\n        return dict(self.fields)\n\n    @property\n    def names(self):\n        return [n for n, t in self.fields]\n\n    @property\n    def types(self):\n        return [t for n, t in self.fields]\n\n    def to_numpy_dtype(self):\n        \"\"\"\n        To Numpy record dtype.\n        \"\"\"\n        return np.dtype([(str(name), to_numpy_dtype(typ))\n                         for name, typ in self.fields])\n\n    def __getitem__(self, key):\n        return self.dict[key]\n\n    def __str__(self):\n        return pprint(self)\n\n\nR = Record  # Alias for record literals\n\n\ndef _format_categories(cats, n=10):\n    return '[%s%s]' % (\n        ', '.join(map(repr, cats[:n])),\n        ', ...' if len(cats) > n else ''\n    )\n\n\nclass Categorical(Mono):\n    \"\"\"Unordered categorical type.\n    \"\"\"\n\n    __slots__ = 'categories', 'type', 'ordered'\n    cls = MEASURE\n\n    def __init__(self, categories, type=None, ordered=False):\n        self.categories = tuple(categories)\n        self.type = (type or datashape.discover(self.categories)).measure\n        self.ordered = ordered\n\n    def __str__(self):\n        return '%s[%s, type=%s, ordered=%s]' % (\n            type(self).__name__.lower(),\n            _format_categories(self.categories),\n            self.type,\n            self.ordered\n        )\n\n    def __repr__(self):\n        return '%s(categories=%s, type=%r, ordered=%s)' % (\n            type(self).__name__,\n            _format_categories(self.categories),\n            self.type,\n            self.ordered\n        )\n\n\nclass Tuple(CollectionPrinter, Mono):\n\n    \"\"\"\n    A product type.\n    \"\"\"\n    __slots__ = 'dshapes',\n    cls = MEASURE\n\n    def __init__(self, dshapes):\n        \"\"\"\n        Parameters\n        ----------\n        dshapes : list of dshapes\n            The datashapes which make up the tuple.\n        \"\"\"\n        dshapes = [DataShape(ds) if not isinstance(ds, DataShape) else ds\n                   for ds in dshapes]\n        self.dshapes = tuple(dshapes)\n\n    def __str__(self):\n        return '(%s)' % ', '.join(map(str, self.dshapes))\n\n    def to_numpy_dtype(self):\n        \"\"\"\n        To Numpy record dtype.\n        \"\"\"\n        return np.dtype([('f%d' % i, to_numpy_dtype(typ))\n                         for i, typ in enumerate(self.parameters[0])])\n\n\nclass JSON(Mono):\n\n    \"\"\" JSON measure \"\"\"\n    cls = MEASURE\n    __slots__ = ()\n\n    def __str__(self):\n        return 'json'\n\n\nbool_ = CType('bool', 1, 1)\nchar = CType('char', 1, 1)\n\nint8 = CType('int8', 1, 1)\nint16 = CType('int16', 2, ctypes.alignment(ctypes.c_int16))\nint32 = CType('int32', 4, ctypes.alignment(ctypes.c_int32))\nint64 = CType('int64', 8, ctypes.alignment(ctypes.c_int64))\n\n# int is an alias for int32\nint_ = int32\nType.register('int', int_)\n\nuint8 = CType('uint8', 1, 1)\nuint16 = CType('uint16', 2, ctypes.alignment(ctypes.c_uint16))\nuint32 = CType('uint32', 4, ctypes.alignment(ctypes.c_uint32))\nuint64 = CType('uint64', 8, ctypes.alignment(ctypes.c_uint64))\n\nfloat16 = CType('float16', 2, ctypes.alignment(ctypes.c_uint16))\nfloat32 = CType('float32', 4, ctypes.alignment(ctypes.c_float))\nfloat64 = CType('float64', 8, ctypes.alignment(ctypes.c_double))\n# float128 = CType('float128', 16)\n\n# real is an alias for float64\nreal = float64\nType.register('real', real)\n\ncomplex_float32 = CType('complex[float32]', 8,\n                        ctypes.alignment(ctypes.c_float))\ncomplex_float64 = CType('complex[float64]', 16,\n                        ctypes.alignment(ctypes.c_double))\nType.register('complex64', complex_float32)\ncomplex64 = complex_float32\n\nType.register('complex128', complex_float64)\ncomplex128 = complex_float64\n# complex256 = CType('complex256', 32)\n\n# complex is an alias for complex[float64]\ncomplex_ = complex_float64\n\ndate_ = Date()\ntime_ = Time()\ndatetime_ = DateTime()\ntimedelta_ = TimeDelta()\nType.register('date', date_)\nType.register('time', time_)\nType.register('datetime', datetime_)\nType.register('timedelta', timedelta_)\n\nnull = Null()\nType.register('null', null)\n\nc_byte = int8\nc_short = int16\nc_int = int32\nc_longlong = int64\n\nc_ubyte = uint8\nc_ushort = uint16\nc_ulonglong = uint64\n\nif ctypes.sizeof(ctypes.c_long) == 4:\n    c_long = int32\n    c_ulong = uint32\nelse:\n    c_long = int64\n    c_ulong = uint64\n\nif ctypes.sizeof(ctypes.c_void_p) == 4:\n    intptr = c_ssize_t = int32\n    uintptr = c_size_t = uint32\nelse:\n    intptr = c_ssize_t = int64\n    uintptr = c_size_t = uint64\nType.register('intptr', intptr)\nType.register('uintptr', uintptr)\n\nc_half = float16\nc_float = float32\nc_double = float64\n\n# TODO: Deal with the longdouble == one of float64/float80/float96/float128\n# situation\n\n# c_longdouble = float128\n\nhalf = float16\nsingle = float32\ndouble = float64\n\nvoid = CType('void', 0, 1)\nobject_ = pyobj = CType('object',\n                        ctypes.sizeof(ctypes.py_object),\n                        ctypes.alignment(ctypes.py_object))\n\nna = Null\nNullRecord = Record(())\nbytes_ = Bytes()\n\nstring = String()\njson = JSON()\n\nType.register('float', c_float)\nType.register('double', c_double)\n\nType.register('bytes', bytes_)\n\nType.register('string', String())\n\nvar = Var()\n\n\ndef to_numpy_dtype(ds):\n    \"\"\" Throw away the shape information and just return the\n    measure as NumPy dtype instance.\"\"\"\n    if isinstance(ds.measure, datashape.coretypes.Map):\n        ds = ds.measure.key\n    return to_numpy(ds.measure)[1]\n\n\ndef to_numpy(ds):\n    \"\"\"\n    Downcast a datashape object into a Numpy (shape, dtype) tuple if\n    possible.\n\n    >>> from datashader.datashape import dshape, to_numpy\n    >>> to_numpy(dshape('5 * 5 * int32'))\n    ((5, 5), dtype('int32'))\n    >>> to_numpy(dshape('10 * string[30]'))\n    ((10,), dtype('<U30'))\n    >>> to_numpy(dshape('N * int32'))\n    ((-1,), dtype('int32'))\n    \"\"\"\n    shape = []\n    if isinstance(ds, DataShape):\n        # The datashape dimensions\n        for dim in ds[:-1]:\n            if isinstance(dim, Fixed):\n                shape.append(int(dim))\n            elif isinstance(dim, TypeVar):\n                shape.append(-1)\n            else:\n                raise TypeError('DataShape dimension %s is not '\n                                'NumPy-compatible' % dim)\n\n        # The datashape measure\n        msr = ds[-1]\n    else:\n        msr = ds\n\n    return tuple(shape), msr.to_numpy_dtype()\n\n\ndef from_numpy(shape, dt):\n    \"\"\"\n    Upcast a (shape, dtype) tuple if possible.\n\n    >>> from datashader.datashape import from_numpy\n    >>> from numpy import dtype\n    >>> from_numpy((5, 5), dtype('int32'))\n    dshape(\"5 * 5 * int32\")\n\n    >>> from_numpy((10,), dtype('S10'))\n    dshape(\"10 * string[10, 'A']\")\n    \"\"\"\n    dtype = np.dtype(dt)\n\n    if dtype.kind == 'S':\n        measure = String(dtype.itemsize, 'A')\n    elif dtype.kind == 'U':\n        measure = String(dtype.itemsize // 4, 'U32')\n    elif dtype.fields:\n        fields = [(name, dtype.fields[name]) for name in dtype.names]\n        rec = [(name, from_numpy(t.shape, t.base))  # recurse into nested dtype\n               for name, (t, _) in fields]  # _ is the byte offset: ignore it\n        measure = Record(rec)\n    else:\n        measure = CType.from_numpy_dtype(dtype)\n\n    if not shape:\n        return measure\n    return DataShape(*tuple(map(Fixed, shape)) + (measure,))\n\n\ndef print_unicode_string(s):\n    try:\n        return s.decode('unicode_escape').encode('ascii')\n    except AttributeError:\n        return s\n\n\ndef pprint(ds, width=80):\n    ''' Pretty print a datashape\n\n    >>> from datashader.datashape import dshape, pprint\n    >>> print(pprint(dshape('5 * 3 * int32')))\n    5 * 3 * int32\n\n    >>> ds = dshape(\"\"\"\n    ... 5000000000 * {\n    ...     a: (int, float32, real, string, datetime),\n    ...     b: {c: 5 * int, d: var * 100 * float32}\n    ... }\"\"\")\n    >>> print(pprint(ds))\n    5000000000 * {\n      a: (int32, float32, float64, string, datetime),\n      b: {c: 5 * int32, d: var * 100 * float32}\n      }\n\n    Record measures print like full datashapes\n    >>> print(pprint(ds.measure, width=30))\n    {\n      a: (\n        int32,\n        float32,\n        float64,\n        string,\n        datetime\n        ),\n      b: {\n        c: 5 * int32,\n        d: var * 100 * float32\n        }\n      }\n\n    Control width of the result\n    >>> print(pprint(ds, width=30))\n    5000000000 * {\n      a: (\n        int32,\n        float32,\n        float64,\n        string,\n        datetime\n        ),\n      b: {\n        c: 5 * int32,\n        d: var * 100 * float32\n        }\n      }\n    >>>\n    '''\n    result = ''\n\n    if isinstance(ds, DataShape):\n        if ds.shape:\n            result += ' * '.join(map(str, ds.shape))\n            result += ' * '\n        ds = ds[-1]\n\n    if isinstance(ds, Record):\n        pairs = ['%s: %s' % (name if isidentifier(name) else\n                             repr(print_unicode_string(name)),\n                             pprint(typ, width - len(result) - len(name)))\n                 for name, typ in zip(ds.names, ds.types)]\n        short = '{%s}' % ', '.join(pairs)\n\n        if len(result + short) < width:\n            return result + short\n        else:\n            long = '{\\n%s\\n}' % ',\\n'.join(pairs)\n            return result + long.replace('\\n', '\\n  ')\n\n    elif isinstance(ds, Tuple):\n        typs = [pprint(typ, width-len(result))\n                for typ in ds.dshapes]\n        short = '(%s)' % ', '.join(typs)\n        if len(result + short) < width:\n            return result + short\n        else:\n            long = '(\\n%s\\n)' % ',\\n'.join(typs)\n            return result + long.replace('\\n', '\\n  ')\n    else:\n        result += str(ds)\n    return result\n",
    "datashader/datatypes.py": "from __future__ import annotations\n\nimport re\n\nfrom functools import total_ordering\nfrom packaging.version import Version\n\nimport numpy as np\nimport pandas as pd\n\nfrom numba import jit\nfrom pandas.api.extensions import (\n    ExtensionDtype, ExtensionArray, register_extension_dtype)\nfrom numbers import Integral\n\nfrom pandas.api.types import pandas_dtype, is_extension_array_dtype\n\n\ntry:\n    # See if we can register extension type with dask >= 1.1.0\n    from dask.dataframe.extensions import make_array_nonempty\nexcept ImportError:\n    make_array_nonempty = None\n\n\ndef _validate_ragged_properties(start_indices, flat_array):\n    \"\"\"\n    Validate that start_indices are flat_array arrays that may be used to\n    represent a valid RaggedArray.\n\n    Parameters\n    ----------\n    flat_array: numpy array containing concatenation\n                of all nested arrays to be represented\n                by this ragged array\n    start_indices: unsigned integer numpy array the same\n                   length as the ragged array where values\n                   represent the index into flat_array where\n                   the corresponding ragged array element\n                   begins\n    Raises\n    ------\n    ValueError:\n        if input arguments are invalid or incompatible properties\n    \"\"\"\n\n    # Validate start_indices\n    if (not isinstance(start_indices, np.ndarray) or\n            start_indices.dtype.kind != 'u' or\n            start_indices.ndim != 1):\n        raise ValueError(\"\"\"\nThe start_indices property of a RaggedArray must be a 1D numpy array of\nunsigned integers (start_indices.dtype.kind == 'u')\n    Received value of type {typ}: {v}\"\"\".format(\n            typ=type(start_indices), v=repr(start_indices)))\n\n    # Validate flat_array\n    if (not isinstance(flat_array, np.ndarray) or\n            flat_array.ndim != 1):\n        raise ValueError(\"\"\"\nThe flat_array property of a RaggedArray must be a 1D numpy array\n    Received value of type {typ}: {v}\"\"\".format(\n            typ=type(flat_array), v=repr(flat_array)))\n\n    # Validate start_indices values\n    # We don't need to check start_indices < 0 because we already know that it\n    # has an unsigned integer datatype\n    #\n    # Note that start_indices[i] == len(flat_array) is valid as it represents\n    # and empty array element at the end of the ragged array.\n    invalid_inds = start_indices > len(flat_array)\n\n    if invalid_inds.any():\n        some_invalid_vals = start_indices[invalid_inds[:10]]\n\n        raise ValueError(\"\"\"\nElements of start_indices must be less than the length of flat_array ({m})\n    Invalid values include: {vals}\"\"\".format(\n            m=len(flat_array), vals=repr(some_invalid_vals)))\n\n\n# Internal ragged element array wrapper that provides\n# equality, ordering, and hashing.\n@total_ordering\nclass _RaggedElement:\n\n    @staticmethod\n    def ragged_or_nan(a):\n        if np.isscalar(a) and np.isnan(a):\n            return a\n        else:\n            return _RaggedElement(a)\n\n    @staticmethod\n    def array_or_nan(a):\n        if np.isscalar(a) and np.isnan(a):\n            return a\n        else:\n            return a.array\n\n    def __init__(self, array):\n        self.array = array\n\n    def __hash__(self):\n        return hash(self.array.tobytes())\n\n    def __eq__(self, other):\n        if not isinstance(other, _RaggedElement):\n            return False\n        return np.array_equal(self.array, other.array)\n\n    def __lt__(self, other):\n        if not isinstance(other, _RaggedElement):\n            return NotImplemented\n        return _lexograph_lt(self.array, other.array)\n\n    def __repr__(self):\n        array_repr = repr(self.array)\n        return array_repr.replace('array', 'ragged_element')\n\n\n@register_extension_dtype\nclass RaggedDtype(ExtensionDtype):\n    \"\"\"\n    Pandas ExtensionDtype to represent a ragged array datatype\n\n    Methods not otherwise documented here are inherited from ExtensionDtype;\n    please see the corresponding method on that class for the docstring\n    \"\"\"\n    type = np.ndarray\n    base = np.dtype('O')\n    _subtype_re = re.compile(r\"^ragged\\[(?P<subtype>\\w+)\\]$\")\n    _metadata = ('_dtype',)\n\n    @property\n    def name(self):\n        return 'Ragged[{subtype}]'.format(subtype=self.subtype)\n\n    def __repr__(self):\n        return self.name\n\n    @classmethod\n    def construct_array_type(cls):\n        return RaggedArray\n\n    @classmethod\n    def construct_from_string(cls, string):\n        if not isinstance(string, str):\n            raise TypeError(\"'construct_from_string' expects a string, got %s\" % type(string))\n\n        # lowercase string\n        string = string.lower()\n\n        msg = \"Cannot construct a 'RaggedDtype' from '{}'\"\n        if string.startswith('ragged'):\n            # Extract subtype\n            try:\n                subtype_string = cls._parse_subtype(string)\n                return RaggedDtype(dtype=subtype_string)\n            except Exception:\n                raise TypeError(msg.format(string))\n        else:\n            raise TypeError(msg.format(string))\n\n    def __init__(self, dtype=np.float64):\n        if isinstance(dtype, RaggedDtype):\n            self._dtype = dtype.subtype\n        else:\n            self._dtype = np.dtype(dtype)\n\n    @property\n    def subtype(self):\n        return self._dtype\n\n    @classmethod\n    def _parse_subtype(cls, dtype_string):\n        \"\"\"\n        Parse a datatype string to get the subtype\n\n        Parameters\n        ----------\n        dtype_string: str\n            A string like Ragged[subtype]\n\n        Returns\n        -------\n        subtype: str\n\n        Raises\n        ------\n        ValueError\n            When the subtype cannot be extracted\n        \"\"\"\n        # Be case insensitive\n        dtype_string = dtype_string.lower()\n\n        match = cls._subtype_re.match(dtype_string)\n        if match:\n            subtype_string = match.groupdict()['subtype']\n        elif dtype_string == 'ragged':\n            subtype_string = 'float64'\n        else:\n            raise ValueError(\"Cannot parse {dtype_string}\".format(\n                dtype_string=dtype_string))\n        return subtype_string\n\n\ndef missing(v):\n    return v is None or (np.isscalar(v) and np.isnan(v))\n\n\nclass RaggedArray(ExtensionArray):\n    \"\"\"\n    Pandas ExtensionArray to represent ragged arrays\n\n    Methods not otherwise documented here are inherited from ExtensionArray;\n    please see the corresponding method on that class for the docstring\n    \"\"\"\n    def __init__(self, data, dtype=None, copy=False):\n        \"\"\"\n        Construct a RaggedArray\n\n        Parameters\n        ----------\n        data: list or array or dict or RaggedArray\n            * list or 1D-array: A List or 1D array of lists or 1D arrays that\n                                should be represented by the RaggedArray\n\n            * dict: A dict containing 'start_indices' and 'flat_array' keys\n                    with numpy array values where:\n                    - flat_array:  numpy array containing concatenation\n                                   of all nested arrays to be represented\n                                   by this ragged array\n                    - start_indices: unsigned integer numpy array the same\n                                     length as the ragged array where values\n                                     represent the index into flat_array where\n                                     the corresponding ragged array element\n                                     begins\n            * RaggedArray: A RaggedArray instance to copy\n\n        dtype: RaggedDtype or np.dtype or str or None (default None)\n            Datatype to use to store underlying values from data.\n            If none (the default) then dtype will be determined using the\n            numpy.result_type function.\n        copy : bool (default False)\n            Whether to deep copy the input arrays. Only relevant when `data`\n            has type `dict` or `RaggedArray`. When data is a `list` or\n            `array`, input arrays are always copied.\n        \"\"\"\n        if (isinstance(data, dict) and\n                all(k in data for k in\n                    ['start_indices', 'flat_array'])):\n\n            _validate_ragged_properties(\n                start_indices=data['start_indices'],\n                flat_array=data['flat_array'])\n\n            self._start_indices = data['start_indices']\n            self._flat_array = data['flat_array']\n            dtype = self._flat_array.dtype\n\n            if copy:\n                self._start_indices = self._start_indices.copy()\n                self._flat_array = self._flat_array.copy()\n\n        elif isinstance(data, RaggedArray):\n            self._flat_array = data.flat_array\n            self._start_indices = data.start_indices\n            dtype = self._flat_array.dtype\n\n            if copy:\n                self._start_indices = self._start_indices.copy()\n                self._flat_array = self._flat_array.copy()\n        else:\n            # Compute lengths\n            index_len = len(data)\n            buffer_len = sum(len(datum)\n                             if not missing(datum)\n                             else 0 for datum in data)\n\n            # Compute necessary precision of start_indices array\n            for nbits in [8, 16, 32, 64]:\n                start_indices_dtype = 'uint' + str(nbits)\n                max_supported = np.iinfo(start_indices_dtype).max\n                if buffer_len <= max_supported:\n                    break\n\n            # infer dtype if not provided\n            if dtype is None:\n                non_missing = [np.atleast_1d(v)\n                               for v in data if not missing(v)]\n                if non_missing:\n                    dtype = np.result_type(*non_missing)\n                else:\n                    dtype = 'float64'\n            elif isinstance(dtype, RaggedDtype):\n                dtype = dtype.subtype\n\n            # Initialize representation arrays\n            self._start_indices = np.zeros(index_len, dtype=start_indices_dtype)\n            self._flat_array = np.zeros(buffer_len, dtype=dtype)\n\n            # Populate arrays\n            next_start_ind = 0\n            for i, array_el in enumerate(data):\n                # Compute element length\n                n = len(array_el) if not missing(array_el) else 0\n\n                # Update start indices\n                self._start_indices[i] = next_start_ind\n\n                # Do not assign when slice is empty avoiding possible\n                # nan assignment to integer array\n                if not n:\n                    continue\n\n                # Update flat array\n                self._flat_array[next_start_ind:next_start_ind+n] = array_el\n\n                # increment next start index\n                next_start_ind += n\n\n        self._dtype = RaggedDtype(dtype=dtype)\n\n    def __eq__(self, other):\n        if isinstance(other, RaggedArray):\n            if len(other) != len(self):\n                raise ValueError(\"\"\"\nCannot check equality of RaggedArray values of unequal length\n    len(ra1) == {len_ra1}\n    len(ra2) == {len_ra2}\"\"\".format(\n                    len_ra1=len(self),\n                    len_ra2=len(other)))\n\n            result = _eq_ragged_ragged(\n                self.start_indices, self.flat_array,\n                other.start_indices, other.flat_array)\n        else:\n            # Convert other to numpy array\n            if not isinstance(other, np.ndarray):\n                other_array = np.asarray(other)\n            else:\n                other_array = other\n\n            if other_array.ndim == 1 and other_array.dtype.kind != 'O':\n\n                # Treat as ragged scalar\n                result = _eq_ragged_scalar(\n                    self.start_indices, self.flat_array, other_array)\n            elif (other_array.ndim == 1 and\n                  other_array.dtype.kind == 'O' and\n                  len(other_array) == len(self)):\n\n                # Treat as vector\n                result = _eq_ragged_ndarray1d(\n                    self.start_indices, self.flat_array, other_array)\n            elif (other_array.ndim == 2 and\n                  other_array.dtype.kind != 'O' and\n                  other_array.shape[0] == len(self)):\n\n                # Treat rows as ragged elements\n                result = _eq_ragged_ndarray2d(\n                    self.start_indices, self.flat_array, other_array)\n            else:\n                raise ValueError(\"\"\"\nCannot check equality of RaggedArray of length {ra_len} with:\n    {other}\"\"\".format(ra_len=len(self), other=repr(other)))\n\n        return result\n\n    def __ne__(self, other):\n        return np.logical_not(self == other)\n\n    @property\n    def flat_array(self):\n        \"\"\"\n        numpy array containing concatenation of all nested arrays\n\n        Returns\n        -------\n        np.ndarray\n        \"\"\"\n        return self._flat_array\n\n    @property\n    def start_indices(self):\n        \"\"\"\n        unsigned integer numpy array the same length as the ragged array where\n        values represent the index into flat_array where the corresponding\n        ragged array element begins\n\n        Returns\n        -------\n        np.ndarray\n        \"\"\"\n        return self._start_indices\n\n    def __len__(self):\n        return len(self._start_indices)\n\n    def __getitem__(self, item):\n        err_msg = (\"Only integers, slices and integer or boolean\"\n                   \"arrays are valid indices.\")\n        if isinstance(item, Integral):\n            if item < -len(self) or item >= len(self):\n                raise IndexError(\"{item} is out of bounds\".format(item=item))\n            else:\n                # Convert negative item index\n                if item < 0:\n                    item += len(self)\n\n                slice_start = self.start_indices[item]\n                slice_end = (self.start_indices[item+1]\n                             if item + 1 <= len(self) - 1\n                             else len(self.flat_array))\n\n                return (self.flat_array[slice_start:slice_end]\n                        if slice_end!=slice_start\n                        else np.nan)\n\n        elif type(item) is slice:\n            data = []\n            selected_indices = np.arange(len(self))[item]\n\n            for selected_index in selected_indices:\n                data.append(self[selected_index])\n\n            return RaggedArray(data, dtype=self.flat_array.dtype)\n\n        elif isinstance(item, (np.ndarray, ExtensionArray, list, tuple)):\n            if isinstance(item, (np.ndarray, ExtensionArray)):\n                # Leave numpy and pandas arrays alone\n                kind = item.dtype.kind\n            else:\n                # Convert others to pandas arrays\n                item = pd.array(item)\n                kind = item.dtype.kind\n\n            if len(item) == 0:\n                return self.take([], allow_fill=False)\n            elif kind == 'b':\n                # Check mask length is compatible\n                if len(item) != len(self):\n                    raise IndexError(\n                        \"Boolean index has wrong length: {} instead of {}\"\n                        .format(len(item), len(self))\n                    )\n\n                # check for NA values\n                isna = pd.isna(item)\n                if isna.any():\n                    if Version(pd.__version__) > Version('1.0.1'):\n                        item[isna] = False\n                    else:\n                        raise ValueError(\n                            \"Cannot mask with a boolean indexer containing NA values\"\n                        )\n\n                data = []\n\n                for i, m in enumerate(item):\n                    if m:\n                        data.append(self[i])\n\n                return RaggedArray(data, dtype=self.flat_array.dtype)\n            elif kind in ('i', 'u'):\n                if any(pd.isna(item)):\n                    raise ValueError(\n                        \"Cannot index with an integer indexer containing NA values\"\n                    )\n                return self.take(item, allow_fill=False)\n            else:\n                raise IndexError(err_msg)\n        else:\n            raise IndexError(err_msg)\n\n    @classmethod\n    def _from_sequence(cls, scalars, dtype=None, copy=False):\n        return RaggedArray(scalars, dtype=dtype)\n\n    @classmethod\n    def _from_factorized(cls, values, original):\n        return RaggedArray(\n            [_RaggedElement.array_or_nan(v) for v in values],\n            dtype=original.flat_array.dtype)\n\n    def _as_ragged_element_array(self):\n        return np.array([_RaggedElement.ragged_or_nan(self[i])\n                         for i in range(len(self))])\n\n    def _values_for_factorize(self):\n        return self._as_ragged_element_array(), np.nan\n\n    def _values_for_argsort(self):\n        return self._as_ragged_element_array()\n\n    def unique(self):\n        from pandas import unique\n\n        uniques = unique(self._as_ragged_element_array())\n        return self._from_sequence(\n            [_RaggedElement.array_or_nan(v) for v in uniques],\n            dtype=self.dtype)\n\n    def fillna(self, value=None, method=None, limit=None):\n        # Override in RaggedArray to handle ndarray fill values\n        from pandas.util._validators import validate_fillna_kwargs\n        from pandas.core.missing import get_fill_func\n\n        value, method = validate_fillna_kwargs(value, method)\n\n        mask = self.isna()\n\n        if isinstance(value, RaggedArray):\n            if len(value) != len(self):\n                raise ValueError(\"Length of 'value' does not match. Got ({}) \"\n                                 \" expected {}\".format(len(value), len(self)))\n            value = value[mask]\n\n        if mask.any():\n            if method is not None:\n                func = get_fill_func(method)\n                new_values = func(self.astype(object), limit=limit,\n                                  mask=mask)\n                new_values = self._from_sequence(new_values, dtype=self.dtype)\n            else:\n                # fill with value\n                new_values = list(self)\n                mask_indices, = np.where(mask)\n                for ind in mask_indices:\n                    new_values[ind] = value\n\n                new_values = self._from_sequence(new_values, dtype=self.dtype)\n        else:\n            new_values = self.copy()\n        return new_values\n\n    def shift(self, periods=1, fill_value=None):\n        # Override in RaggedArray to handle ndarray fill values\n\n        # Note: this implementation assumes that `self.dtype.na_value` can be\n        # stored in an instance of your ExtensionArray with `self.dtype`.\n        if not len(self) or periods == 0:\n            return self.copy()\n\n        if fill_value is None:\n            fill_value = np.nan\n\n        empty = self._from_sequence(\n            [fill_value] * min(abs(periods), len(self)),\n            dtype=self.dtype\n        )\n        if periods > 0:\n            a = empty\n            b = self[:-periods]\n        else:\n            a = self[abs(periods):]\n            b = empty\n        return self._concat_same_type([a, b])\n\n    def searchsorted(self, value, side=\"left\", sorter=None):\n        arr = self._as_ragged_element_array()\n        if isinstance(value, RaggedArray):\n            search_value = value._as_ragged_element_array()\n        else:\n            search_value = _RaggedElement(value)\n        return arr.searchsorted(search_value, side=side, sorter=sorter)\n\n    def isna(self):\n        stop_indices = np.hstack([self.start_indices[1:],\n                                  [len(self.flat_array)]])\n\n        element_lengths = stop_indices - self.start_indices\n        return element_lengths == 0\n\n    def take(self, indices, allow_fill=False, fill_value=None):\n        if allow_fill:\n            invalid_inds = [i for i in indices if i < -1]\n            if invalid_inds:\n                raise ValueError(\"\"\"\nInvalid indices for take with allow_fill True: {inds}\"\"\".format(\n                    inds=invalid_inds[:9]))\n            sequence = [self[i] if i >= 0 else fill_value\n                        for i in indices]\n        else:\n            if len(self) == 0 and len(indices) > 0:\n                raise IndexError(\n                    \"cannot do a non-empty take from an empty axis|out of bounds\"\n                )\n\n            sequence = [self[i] for i in indices]\n\n        return RaggedArray(sequence, dtype=self.flat_array.dtype)\n\n    def copy(self, deep=False):\n        data = dict(\n            flat_array=self.flat_array,\n            start_indices=self.start_indices)\n\n        return RaggedArray(data, copy=deep)\n\n    @classmethod\n    def _concat_same_type(cls, to_concat):\n        # concat flat_arrays\n        flat_array = np.hstack([ra.flat_array for ra in to_concat])\n\n        # offset and concat start_indices\n        offsets = np.hstack([\n            [0], np.cumsum([len(ra.flat_array) for ra in to_concat[:-1]])\n        ]).astype('uint64')\n\n        start_indices = np.hstack([ra.start_indices + offset\n                                   for offset, ra in zip(offsets, to_concat)])\n\n        return RaggedArray(dict(\n            flat_array=flat_array, start_indices=start_indices),\n            copy=False)\n\n    @property\n    def dtype(self):\n        return self._dtype\n\n    @property\n    def nbytes(self):\n        return (self._flat_array.nbytes +\n                self._start_indices.nbytes)\n\n    def astype(self, dtype, copy=True):\n        dtype = pandas_dtype(dtype)\n        if isinstance(dtype, RaggedDtype):\n            if copy:\n                return self.copy()\n            return self\n\n        elif is_extension_array_dtype(dtype):\n            return dtype.construct_array_type()._from_sequence(\n                np.asarray(self))\n\n        return np.array([v for v in self], dtype=dtype)\n\n    def tolist(self):\n        # Based on pandas ExtensionArray.tolist\n        if self.ndim > 1:\n            return [item.tolist() for item in self]\n        else:\n            return list(self)\n\n    def __array__(self, dtype=None, copy=True):\n        dtype = np.dtype(object) if dtype is None else np.dtype(dtype)\n        if copy:\n            return np.array(self.tolist(), dtype=dtype)\n        else:\n            return np.array(self, dtype=dtype)\n\n    def duplicated(self, *args, **kwargs):\n        msg = \"duplicated is not implemented for RaggedArray\"\n        raise NotImplementedError(msg)\n\n\n@jit(nopython=True, nogil=True)\ndef _eq_ragged_ragged(start_indices1,\n                      flat_array1,\n                      start_indices2,\n                      flat_array2):\n    \"\"\"\n    Compare elements of two ragged arrays of the same length\n\n    Parameters\n    ----------\n    start_indices1: ndarray\n        start indices of a RaggedArray 1\n    flat_array1: ndarray\n        flat_array property of a RaggedArray 1\n    start_indices2: ndarray\n        start indices of a RaggedArray 2\n    flat_array2: ndarray\n        flat_array property of a RaggedArray 2\n\n    Returns\n    -------\n    mask: ndarray\n        1D bool array of same length as inputs with elements True when\n        corresponding elements are equal, False otherwise\n    \"\"\"\n    n = len(start_indices1)\n    m1 = len(flat_array1)\n    m2 = len(flat_array2)\n\n    result = np.zeros(n, dtype=np.bool_)\n\n    for i in range(n):\n        # Extract inds for ra1\n        start_index1 = start_indices1[i]\n        stop_index1 = start_indices1[i + 1] if i < n - 1 else m1\n        len_1 = stop_index1 - start_index1\n\n        # Extract inds for ra2\n        start_index2 = start_indices2[i]\n        stop_index2 = start_indices2[i + 1] if i < n - 1 else m2\n        len_2 = stop_index2 - start_index2\n\n        if len_1 != len_2:\n            el_equal = False\n        else:\n            el_equal = True\n            for flat_index1, flat_index2 in \\\n                    zip(range(start_index1, stop_index1),\n                        range(start_index2, stop_index2)):\n                el_1 = flat_array1[flat_index1]\n                el_2 = flat_array2[flat_index2]\n                el_equal &= el_1 == el_2\n\n        result[i] = el_equal\n\n    return result\n\n\n@jit(nopython=True, nogil=True)\ndef _eq_ragged_scalar(start_indices, flat_array, val):\n    \"\"\"\n    Compare elements of a RaggedArray with a scalar array\n\n    Parameters\n    ----------\n    start_indices: ndarray\n        start indices of a RaggedArray\n    flat_array: ndarray\n        flat_array property of a RaggedArray\n    val: ndarray\n\n    Returns\n    -------\n    mask: ndarray\n        1D bool array of same length as inputs with elements True when\n        ragged element equals scalar val, False otherwise.\n    \"\"\"\n    n = len(start_indices)\n    m = len(flat_array)\n    cols = len(val)\n    result = np.zeros(n, dtype=np.bool_)\n    for i in range(n):\n        start_index = start_indices[i]\n        stop_index = start_indices[i+1] if i < n - 1 else m\n\n        if stop_index - start_index != cols:\n            el_equal = False\n        else:\n            el_equal = True\n            for val_index, flat_index in \\\n                    enumerate(range(start_index, stop_index)):\n                el_equal &= flat_array[flat_index] == val[val_index]\n        result[i] = el_equal\n\n    return result\n\n\ndef _eq_ragged_ndarray1d(start_indices, flat_array, a):\n    \"\"\"\n    Compare a RaggedArray with a 1D numpy object array of the same length\n\n    Parameters\n    ----------\n    start_indices: ndarray\n        start indices of a RaggedArray\n    flat_array: ndarray\n        flat_array property of a RaggedArray\n    a: ndarray\n        1D numpy array of same length as ra\n\n    Returns\n    -------\n    mask: ndarray\n        1D bool array of same length as input with elements True when\n        corresponding elements are equal, False otherwise\n\n    Notes\n    -----\n    This function is not numba accelerated because it, by design, inputs\n    a numpy object array\n    \"\"\"\n\n    n = len(start_indices)\n    m = len(flat_array)\n    result = np.zeros(n, dtype=np.bool_)\n    for i in range(n):\n        start_index = start_indices[i]\n        stop_index = start_indices[i + 1] if i < n - 1 else m\n        a_val = a[i]\n        if (a_val is None or\n                (np.isscalar(a_val) and np.isnan(a_val)) or\n                len(a_val) == 0):\n            result[i] = start_index == stop_index\n        else:\n            result[i] = np.array_equal(flat_array[start_index:stop_index],\n                                       a_val)\n\n    return result\n\n\n@jit(nopython=True, nogil=True)\ndef _eq_ragged_ndarray2d(start_indices, flat_array, a):\n    \"\"\"\n    Compare a RaggedArray with rows of a 2D numpy object array\n\n    Parameters\n    ----------\n    start_indices: ndarray\n        start indices of a RaggedArray\n    flat_array: ndarray\n        flat_array property of a RaggedArray\n    a: ndarray\n        A 2D numpy array where the length of the first dimension matches the\n        length of the RaggedArray\n\n    Returns\n    -------\n    mask: ndarray\n        1D bool array of same length as input RaggedArray with elements True\n        when corresponding elements of ra equal corresponding row of `a`\n    \"\"\"\n    n = len(start_indices)\n    m = len(flat_array)\n    cols = a.shape[1]\n\n    # np.bool is an alias for Python's built-in bool type, np.bool_ is the\n    # numpy type that numba recognizes\n    result = np.zeros(n, dtype=np.bool_)\n    for row in range(n):\n        start_index = start_indices[row]\n        stop_index = start_indices[row + 1] if row < n - 1 else m\n\n        # Check equality\n        if stop_index - start_index != cols:\n            el_equal = False\n        else:\n            el_equal = True\n            for col, flat_index in enumerate(range(start_index, stop_index)):\n                el_equal &= flat_array[flat_index] == a[row, col]\n        result[row] = el_equal\n    return result\n\n\n@jit(nopython=True, nogil=True)\ndef _lexograph_lt(a1, a2):\n    \"\"\"\n    Compare two 1D numpy arrays lexographically\n    Parameters\n    ----------\n    a1: ndarray\n        1D numpy array\n    a2: ndarray\n        1D numpy array\n\n    Returns\n    -------\n    comparison:\n        True if a1 < a2, False otherwise\n    \"\"\"\n    for e1, e2 in zip(a1, a2):\n        if e1 < e2:\n            return True\n        elif e1 > e2:\n            return False\n    return len(a1) < len(a2)\n\n\ndef ragged_array_non_empty(dtype):\n    return RaggedArray([[1], [1, 2]], dtype=dtype)\n\n\nif make_array_nonempty:\n    make_array_nonempty.register(RaggedDtype)(ragged_array_non_empty)\n",
    "datashader/datashape/util/__init__.py": "\nfrom itertools import chain\nimport operator\n\nfrom .. import parser\nfrom .. import type_symbol_table\nfrom ..validation import validate\nfrom .. import coretypes\n\n\n__all__ = 'dshape', 'dshapes', 'has_var_dim', 'has_ellipsis', 'cat_dshapes'\n\nsubclasses = operator.methodcaller('__subclasses__')\n\n#------------------------------------------------------------------------\n# Utility Functions for DataShapes\n#------------------------------------------------------------------------\n\ndef dshapes(*args):\n    \"\"\"\n    Parse a bunch of datashapes all at once.\n\n    >>> a, b = dshapes('3 * int32', '2 * var * float64')\n    \"\"\"\n    return [dshape(arg) for arg in args]\n\n\ndef dshape(o):\n    \"\"\"\n    Parse a datashape. For a thorough description see\n    https://datashape.readthedocs.io/en/latest/\n\n    >>> ds = dshape('2 * int32')\n    >>> ds[1]\n    ctype(\"int32\")\n    \"\"\"\n    if isinstance(o, coretypes.DataShape):\n        return o\n    if isinstance(o, str):\n        ds = parser.parse(o, type_symbol_table.sym)\n    elif isinstance(o, (coretypes.CType, coretypes.String,\n                        coretypes.Record, coretypes.JSON,\n                        coretypes.Date, coretypes.Time, coretypes.DateTime,\n                        coretypes.Unit)):\n        ds = coretypes.DataShape(o)\n    elif isinstance(o, coretypes.Mono):\n        ds = o\n    elif isinstance(o, (list, tuple)):\n        ds = coretypes.DataShape(*o)\n    else:\n        raise TypeError('Cannot create dshape from object of type %s' % type(o))\n    validate(ds)\n    return ds\n\n\ndef cat_dshapes(dslist):\n    \"\"\"\n    Concatenates a list of dshapes together along\n    the first axis. Raises an error if there is\n    a mismatch along another axis or the measures\n    are different.\n\n    Requires that the leading dimension be a known\n    size for all data shapes.\n    TODO: Relax this restriction to support\n          streaming dimensions.\n\n    >>> cat_dshapes(dshapes('10 * int32', '5 * int32'))\n    dshape(\"15 * int32\")\n    \"\"\"\n    if len(dslist) == 0:\n        raise ValueError('Cannot concatenate an empty list of dshapes')\n    elif len(dslist) == 1:\n        return dslist[0]\n\n    outer_dim_size = operator.index(dslist[0][0])\n    inner_ds = dslist[0][1:]\n    for ds in dslist[1:]:\n        outer_dim_size += operator.index(ds[0])\n        if ds[1:] != inner_ds:\n            raise ValueError(('The datashapes to concatenate much'\n                              ' all match after'\n                              ' the first dimension (%s vs %s)') %\n                              (inner_ds, ds[1:]))\n    return coretypes.DataShape(*[coretypes.Fixed(outer_dim_size)] + list(inner_ds))\n\n\ndef collect(pred, expr):\n    \"\"\" Collect terms in expression that match predicate\n\n    >>> from datashader.datashape import Unit, dshape\n    >>> predicate = lambda term: isinstance(term, Unit)\n    >>> dshape = dshape('var * {value: int64, loc: 2 * int32}')\n    >>> sorted(set(collect(predicate, dshape)), key=str)\n    [Fixed(val=2), ctype(\"int32\"), ctype(\"int64\"), Var()]\n    >>> from datashader.datashape import var, int64\n    >>> sorted(set(collect(predicate, [var, int64])), key=str)\n    [ctype(\"int64\"), Var()]\n    \"\"\"\n    if pred(expr):\n        return [expr]\n    if isinstance(expr, coretypes.Record):\n        return chain.from_iterable(collect(pred, typ) for typ in expr.types)\n    if isinstance(expr, coretypes.Mono):\n        return chain.from_iterable(collect(pred, typ) for typ in expr.parameters)\n    if isinstance(expr, (list, tuple)):\n        return chain.from_iterable(collect(pred, item) for item in expr)\n\n\ndef has_var_dim(ds):\n    \"\"\"Returns True if datashape has a variable dimension\n\n    Note currently treats variable length string as scalars.\n\n    >>> has_var_dim(dshape('2 * int32'))\n    False\n    >>> has_var_dim(dshape('var * 2 * int32'))\n    True\n    \"\"\"\n    return has((coretypes.Ellipsis, coretypes.Var), ds)\n\n\ndef has(typ, ds):\n    if isinstance(ds, typ):\n        return True\n    if isinstance(ds, coretypes.Record):\n        return any(has(typ, t) for t in ds.types)\n    if isinstance(ds, coretypes.Mono):\n        return any(has(typ, p) for p in ds.parameters)\n    if isinstance(ds, (list, tuple)):\n        return any(has(typ, item) for item in ds)\n    return False\n\n\ndef has_ellipsis(ds):\n    \"\"\"Returns True if the datashape has an ellipsis\n\n    >>> has_ellipsis(dshape('2 * int'))\n    False\n    >>> has_ellipsis(dshape('... * int'))\n    True\n    \"\"\"\n    return has(coretypes.Ellipsis, ds)\n"
  },
  "GT_src_dict": {
    "datashader/datashape/coretypes.py": {
      "Mono.__eq__": {
        "code": "    def __eq__(self, other):\n        \"\"\"Checks equality between two Mono instances.\n\nParameters\n----------\nother : object\n    The object to compare with the current Mono instance.\n\nReturns\n-------\nbool\n    Returns True if the other object is an instance of Mono and has the same shape and measure information as the current instance, otherwise returns False.\n\nNotes\n-----\nThe equality check relies on the `shape` property, which represents the dimensions of the Mono instance, and the `measure.info()` method, which provides the info of the measure associated with the Mono instances being compared. Both attributes are crucial in determining if the Moons are equivalent in structure and type.\"\"\"\n        return isinstance(other, Mono) and self.shape == other.shape and (self.measure.info() == other.measure.info())",
        "docstring": "Checks equality between two Mono instances.\n\nParameters\n----------\nother : object\n    The object to compare with the current Mono instance.\n\nReturns\n-------\nbool\n    Returns True if the other object is an instance of Mono and has the same shape and measure information as the current instance, otherwise returns False.\n\nNotes\n-----\nThe equality check relies on the `shape` property, which represents the dimensions of the Mono instance, and the `measure.info()` method, which provides the info of the measure associated with the Mono instances being compared. Both attributes are crucial in determining if the Moons are equivalent in structure and type.",
        "signature": "def __eq__(self, other):",
        "type": "Method",
        "class_signature": "class Mono(metaclass=Type):"
      },
      "Mono.__ne__": {
        "code": "    def __ne__(self, other):\n        \"\"\"Determines if two Mono instances are not equal.\n\nParameters\n----------\nother : Mono\n    The instance to compare against the current Mono instance.\n\nReturns\n-------\nbool\n    Returns `True` if the two instances are not equal; otherwise, returns `False`.\n\nThe method uses the `__eq__` method to check for equality, effectively implementing the logical negation of the equality comparison.\"\"\"\n        return not self.__eq__(other)",
        "docstring": "Determines if two Mono instances are not equal.\n\nParameters\n----------\nother : Mono\n    The instance to compare against the current Mono instance.\n\nReturns\n-------\nbool\n    Returns `True` if the two instances are not equal; otherwise, returns `False`.\n\nThe method uses the `__eq__` method to check for equality, effectively implementing the logical negation of the equality comparison.",
        "signature": "def __ne__(self, other):",
        "type": "Method",
        "class_signature": "class Mono(metaclass=Type):"
      },
      "Mono.subarray": {
        "code": "    def subarray(self, leading):\n        \"\"\"Returns a data shape object representing the subarray formed by removing a specified number of leading dimensions from the current shape. \n\nParameters\n----------\nleading : int\n    The number of leading dimensions to remove from the data shape. It must not exceed the current number of dimensions.\n\nReturns\n-------\nMono\n    A new instance of Mono representing the subarray after removing the specified number of leading dimensions. If `leading` is greater than or equal to 1, an IndexError is raised, indicating an insufficient number of dimensions to remove.\n\nNotes\n-----\nThis method is particularly relevant for instances of `CType`, where removing dimensions is restricted to zero. The method handles cases where there are insufficient dimensions by raising an IndexError with a descriptive message.\"\"\"\n        \"Returns a data shape object of the subarray with 'leading'\\n        dimensions removed. In the case of a measure such as CType,\\n        'leading' must be 0, and self is returned.\\n        \"\n        if leading >= 1:\n            raise IndexError('Not enough dimensions in data shape to remove %d leading dimensions.' % leading)\n        else:\n            return self",
        "docstring": "Returns a data shape object representing the subarray formed by removing a specified number of leading dimensions from the current shape. \n\nParameters\n----------\nleading : int\n    The number of leading dimensions to remove from the data shape. It must not exceed the current number of dimensions.\n\nReturns\n-------\nMono\n    A new instance of Mono representing the subarray after removing the specified number of leading dimensions. If `leading` is greater than or equal to 1, an IndexError is raised, indicating an insufficient number of dimensions to remove.\n\nNotes\n-----\nThis method is particularly relevant for instances of `CType`, where removing dimensions is restricted to zero. The method handles cases where there are insufficient dimensions by raising an IndexError with a descriptive message.",
        "signature": "def subarray(self, leading):",
        "type": "Method",
        "class_signature": "class Mono(metaclass=Type):"
      },
      "DataShape.__init__": {
        "code": "    def __init__(self, *parameters, **kwds):\n        \"\"\"Initialization method for the DataShape class, which represents a composite container for datashape elements in the unified shape and data type system.\n\nParameters\n----------\n*parameters : variable length argument list\n    Represents the datashape elements; these can be dimensions (of type Fixed or Var) and a measure (of type Measure). The last element must be a measure, while all preceding elements must be dimensions.\n**kwds : keyword arguments\n    Optional parameters including 'name', which can be used to assign a name to the DataShape and register it in the class's type registry.\n\nRaises\n------\nTypeError : If the constructor is called with a single string parameter or if the last parameter is not a measure, or if any preceding parameter is not a dimension.\nValueError : If no parameters are provided.\n\nAttributes\n----------\nself._parameters : tuple\n    A tuple of datashape elements created from the parameters after potential normalization via the _launder function.\n\nConstants Used\n--------------\nMEASURE : A constant representing the type of measures, defined for validation of the last parameter. It ensures that only a measure can occupy the last position in a datashape.\n\nThis method ensures that the DataShape is correctly configured with the right combination of dimensions and a measure, facilitating the representation of complex data structures.\"\"\"\n        if len(parameters) == 1 and isinstance(parameters[0], str):\n            raise TypeError(\"DataShape constructor for internal use.\\nUse dshape function to convert strings into datashapes.\\nTry:\\n\\tdshape('%s')\" % parameters[0])\n        if len(parameters) > 0:\n            self._parameters = tuple(map(_launder, parameters))\n            if getattr(self._parameters[-1], 'cls', MEASURE) != MEASURE:\n                raise TypeError('Only a measure can appear on the last position of a datashape, not %s' % repr(self._parameters[-1]))\n            for dim in self._parameters[:-1]:\n                if getattr(dim, 'cls', DIMENSION) != DIMENSION:\n                    raise TypeError('Only dimensions can appear before the last position of a datashape, not %s' % repr(dim))\n        else:\n            raise ValueError('the data shape should be constructed from 2 or more parameters, only got %s' % len(parameters))\n        self.composite = True\n        self.name = kwds.get('name')\n        if self.name:\n            type(type(self))._registry[self.name] = self",
        "docstring": "Initialization method for the DataShape class, which represents a composite container for datashape elements in the unified shape and data type system.\n\nParameters\n----------\n*parameters : variable length argument list\n    Represents the datashape elements; these can be dimensions (of type Fixed or Var) and a measure (of type Measure). The last element must be a measure, while all preceding elements must be dimensions.\n**kwds : keyword arguments\n    Optional parameters including 'name', which can be used to assign a name to the DataShape and register it in the class's type registry.\n\nRaises\n------\nTypeError : If the constructor is called with a single string parameter or if the last parameter is not a measure, or if any preceding parameter is not a dimension.\nValueError : If no parameters are provided.\n\nAttributes\n----------\nself._parameters : tuple\n    A tuple of datashape elements created from the parameters after potential normalization via the _launder function.\n\nConstants Used\n--------------\nMEASURE : A constant representing the type of measures, defined for validation of the last parameter. It ensures that only a measure can occupy the last position in a datashape.\n\nThis method ensures that the DataShape is correctly configured with the right combination of dimensions and a measure, facilitating the representation of complex data structures.",
        "signature": "def __init__(self, *parameters, **kwds):",
        "type": "Method",
        "class_signature": "class DataShape(Mono):"
      },
      "DataShape.__str__": {
        "code": "    def __str__(self):\n        \"\"\"Return a string representation of the DataShape instance.\n\nThis method generates a human-readable representation of the DataShape object,\neither by returning its registered name (if applicable) or by concatenating\nthe string representations of its parameters with a ' * ' separator. The parameters \ntypically include dimensions and a measure type, showcasing the structure of the data shape.\n\nReturns\n-------\nstr\n    A formatted string that represents the DataShape, either as its name or as a \n    combination of its components.\n\nAttributes\n----------\nself.name : str or None\n    The registered name of the DataShape, which is set during its initialization. \n    If a name is provided, it will be used for representation; otherwise, the parameters will be joined.\nself.parameters : tuple\n    A collection of parameters that define the DataShape. Each parameter is converted to a string\n    using its own __str__ method.\n\nThis method is primarily used for debugging and visualization purposes, allowing users to\nunderstand the structure of data shapes at a glance.\"\"\"\n        return self.name or ' * '.join(map(str, self.parameters))",
        "docstring": "Return a string representation of the DataShape instance.\n\nThis method generates a human-readable representation of the DataShape object,\neither by returning its registered name (if applicable) or by concatenating\nthe string representations of its parameters with a ' * ' separator. The parameters \ntypically include dimensions and a measure type, showcasing the structure of the data shape.\n\nReturns\n-------\nstr\n    A formatted string that represents the DataShape, either as its name or as a \n    combination of its components.\n\nAttributes\n----------\nself.name : str or None\n    The registered name of the DataShape, which is set during its initialization. \n    If a name is provided, it will be used for representation; otherwise, the parameters will be joined.\nself.parameters : tuple\n    A collection of parameters that define the DataShape. Each parameter is converted to a string\n    using its own __str__ method.\n\nThis method is primarily used for debugging and visualization purposes, allowing users to\nunderstand the structure of data shapes at a glance.",
        "signature": "def __str__(self):",
        "type": "Method",
        "class_signature": "class DataShape(Mono):"
      },
      "DataShape.subarray": {
        "code": "    def subarray(self, leading):\n        \"\"\"Returns a new DataShape object representing a subarray with a specified number of leading dimensions removed.\n\nParameters\n----------\nleading : int\n    The number of leading dimensions to remove from the DataShape.\n\nReturns\n-------\nDataShape\n    A new DataShape instance reflecting the shape after the specified dimensions have been eliminated.\n\nRaises\n------\nIndexError\n    If the leading parameter exceeds the number of dimensions available in the DataShape.\n\nNotes\n-----\nThis method utilizes the `parameters` attribute, which is a tuple storing the components of the DataShape. The method checks the length of this tuple to ensure valid subarray slicing before proceeding to construct a new DataShape from the remaining parameters. If the `leading` value corresponds to the shape's last index or is negative, it returns a new DataShape containing only the measure.\n\nExamples\n--------\n>>> from datashader.datashape import dshape\n>>> dshape('1 * 2 * 3 * int32').subarray(1)\ndshape(\"2 * 3 * int32\")\n>>> dshape('1 * 2 * 3 * int32').subarray(2)\ndshape(\"3 * int32\")\"\"\"\n        'Returns a data shape object of the subarray with \\'leading\\'\\n        dimensions removed.\\n\\n        >>> from datashader.datashape import dshape\\n        >>> dshape(\\'1 * 2 * 3 * int32\\').subarray(1)\\n        dshape(\"2 * 3 * int32\")\\n        >>> dshape(\\'1 * 2 * 3 * int32\\').subarray(2)\\n        dshape(\"3 * int32\")\\n        '\n        if leading >= len(self.parameters):\n            raise IndexError('Not enough dimensions in data shape to remove %d leading dimensions.' % leading)\n        elif leading in [len(self.parameters) - 1, -1]:\n            return DataShape(self.parameters[-1])\n        else:\n            return DataShape(*self.parameters[leading:])",
        "docstring": "Returns a new DataShape object representing a subarray with a specified number of leading dimensions removed.\n\nParameters\n----------\nleading : int\n    The number of leading dimensions to remove from the DataShape.\n\nReturns\n-------\nDataShape\n    A new DataShape instance reflecting the shape after the specified dimensions have been eliminated.\n\nRaises\n------\nIndexError\n    If the leading parameter exceeds the number of dimensions available in the DataShape.\n\nNotes\n-----\nThis method utilizes the `parameters` attribute, which is a tuple storing the components of the DataShape. The method checks the length of this tuple to ensure valid subarray slicing before proceeding to construct a new DataShape from the remaining parameters. If the `leading` value corresponds to the shape's last index or is negative, it returns a new DataShape containing only the measure.\n\nExamples\n--------\n>>> from datashader.datashape import dshape\n>>> dshape('1 * 2 * 3 * int32').subarray(1)\ndshape(\"2 * 3 * int32\")\n>>> dshape('1 * 2 * 3 * int32').subarray(2)\ndshape(\"3 * int32\")",
        "signature": "def subarray(self, leading):",
        "type": "Method",
        "class_signature": "class DataShape(Mono):"
      },
      "CType.__str__": {
        "code": "    def __str__(self):\n        \"\"\"Returns the string representation of the CType instance, which is the name of the data type. This method does not take any parameters. The output is a string that corresponds to the name associated with the CType object, allowing for easy identification of the data type defined by this instance. The class attribute `name` is initialized during the instantiation of the CType class and represents the type's designation (e.g., 'int32', 'float64'). This string output serves as a simplified way to reference data types defined within the context of data shape and type management in the code.\"\"\"\n        return self.name",
        "docstring": "Returns the string representation of the CType instance, which is the name of the data type. This method does not take any parameters. The output is a string that corresponds to the name associated with the CType object, allowing for easy identification of the data type defined by this instance. The class attribute `name` is initialized during the instantiation of the CType class and represents the type's designation (e.g., 'int32', 'float64'). This string output serves as a simplified way to reference data types defined within the context of data shape and type management in the code.",
        "signature": "def __str__(self):",
        "type": "Method",
        "class_signature": "class CType(Unit):"
      }
    },
    "datashader/datatypes.py": {},
    "datashader/datashape/util/__init__.py": {
      "dshape": {
        "code": "def dshape(o):\n    \"\"\"Parse a data shape (dshape) representation from various input types including strings, coretypes, and data structure representations.\n\nParameters:\n- o (Union[str, coretypes.DataShape, coretypes.CType, coretypes.String, coretypes.Record, coretypes.JSON, coretypes.Date, coretypes.Time, coretypes.DateTime, coretypes.Unit, coretypes.Mono, list, tuple]): The input object that defines the data shape. It can be a string representation of a dshape, an instance of various coretype classes, or a list/tuple of dimensions and types.\n\nReturns:\n- coretypes.DataShape: A validated DataShape object representing the parsed data structure.\n\nRaises:\n- TypeError: If the input object cannot be converted into a dshape.\n- ValidationError: If the resulting DataShape does not meet the validation criteria defined in the validate function.\n\nDependencies:\n- This function relies on the `parser` module to convert string representations into dshape objects, and the `validate` function to ensure the constructed dshape is valid. Various core types are sourced from the `coretypes` module, which includes classes for different data types.\"\"\"\n    '\\n    Parse a datashape. For a thorough description see\\n    https://datashape.readthedocs.io/en/latest/\\n\\n    >>> ds = dshape(\\'2 * int32\\')\\n    >>> ds[1]\\n    ctype(\"int32\")\\n    '\n    if isinstance(o, coretypes.DataShape):\n        return o\n    if isinstance(o, str):\n        ds = parser.parse(o, type_symbol_table.sym)\n    elif isinstance(o, (coretypes.CType, coretypes.String, coretypes.Record, coretypes.JSON, coretypes.Date, coretypes.Time, coretypes.DateTime, coretypes.Unit)):\n        ds = coretypes.DataShape(o)\n    elif isinstance(o, coretypes.Mono):\n        ds = o\n    elif isinstance(o, (list, tuple)):\n        ds = coretypes.DataShape(*o)\n    else:\n        raise TypeError('Cannot create dshape from object of type %s' % type(o))\n    validate(ds)\n    return ds",
        "docstring": "Parse a data shape (dshape) representation from various input types including strings, coretypes, and data structure representations.\n\nParameters:\n- o (Union[str, coretypes.DataShape, coretypes.CType, coretypes.String, coretypes.Record, coretypes.JSON, coretypes.Date, coretypes.Time, coretypes.DateTime, coretypes.Unit, coretypes.Mono, list, tuple]): The input object that defines the data shape. It can be a string representation of a dshape, an instance of various coretype classes, or a list/tuple of dimensions and types.\n\nReturns:\n- coretypes.DataShape: A validated DataShape object representing the parsed data structure.\n\nRaises:\n- TypeError: If the input object cannot be converted into a dshape.\n- ValidationError: If the resulting DataShape does not meet the validation criteria defined in the validate function.\n\nDependencies:\n- This function relies on the `parser` module to convert string representations into dshape objects, and the `validate` function to ensure the constructed dshape is valid. Various core types are sourced from the `coretypes` module, which includes classes for different data types.",
        "signature": "def dshape(o):",
        "type": "Function",
        "class_signature": null
      }
    }
  },
  "dependency_dict": {
    "datashader/datashape/coretypes.py:Mono:__eq__": {
      "datashader/datashape/coretypes.py": {
        "Mono.info": {
          "code": "    def info(self):\n        return (type(self), self.parameters)",
          "docstring": "",
          "signature": "def info(self):",
          "type": "Method",
          "class_signature": "class Mono(metaclass=Type):"
        },
        "Mono.shape": {
          "code": "    def shape(self):\n        return ()",
          "docstring": "",
          "signature": "def shape(self):",
          "type": "Method",
          "class_signature": "class Mono(metaclass=Type):"
        },
        "Mono.measure": {
          "code": "    def measure(self):\n        return self",
          "docstring": "",
          "signature": "def measure(self):",
          "type": "Method",
          "class_signature": "class Mono(metaclass=Type):"
        },
        "DataShape.shape": {
          "code": "    def shape(self):\n        return self.parameters[:-1]",
          "docstring": "",
          "signature": "def shape(self):",
          "type": "Method",
          "class_signature": "class DataShape(Mono):"
        },
        "DataShape.measure": {
          "code": "    def measure(self):\n        return self.parameters[-1]",
          "docstring": "",
          "signature": "def measure(self):",
          "type": "Method",
          "class_signature": "class DataShape(Mono):"
        },
        "Fixed.__eq__": {
          "code": "    def __eq__(self, other):\n        return type(other) is Fixed and self.val == other.val or (isinstance(other, int) and self.val == other)",
          "docstring": "",
          "signature": "def __eq__(self, other):",
          "type": "Method",
          "class_signature": "class Fixed(Unit):"
        }
      }
    },
    "datashader/datashape/util/__init__.py:dshape": {
      "datashader/datashape/validation.py": {
        "validate": {
          "code": "def validate(ds):\n    \"\"\"\n    Validate a datashape to see whether it is well-formed.\n\n    Parameters\n    ----------\n    ds : DataShape\n\n    Examples\n    --------\n    >>> from datashader.datashape import dshape\n    >>> dshape('10 * int32')\n    dshape(\"10 * int32\")\n    >>> dshape('... * int32')\n    dshape(\"... * int32\")\n    >>> dshape('... * ... * int32') # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    TypeError: Can only use a single wildcard\n    >>> dshape('T * ... * X * ... * X') # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    TypeError: Can only use a single wildcard\n    >>> dshape('T * ...') # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    DataShapeSyntaxError: Expected a dtype\n    \"\"\"\n    traverse(_validate, ds)",
          "docstring": "Validate a datashape to see whether it is well-formed.\n\nParameters\n----------\nds : DataShape\n\nExamples\n--------\n>>> from datashader.datashape import dshape\n>>> dshape('10 * int32')\ndshape(\"10 * int32\")\n>>> dshape('... * int32')\ndshape(\"... * int32\")\n>>> dshape('... * ... * int32') # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\n    ...\nTypeError: Can only use a single wildcard\n>>> dshape('T * ... * X * ... * X') # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\n    ...\nTypeError: Can only use a single wildcard\n>>> dshape('T * ...') # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\n    ...\nDataShapeSyntaxError: Expected a dtype",
          "signature": "def validate(ds):",
          "type": "Function",
          "class_signature": null
        }
      },
      "datashader/datashape/parser.py": {
        "parse": {
          "code": "def parse(ds_str, sym):\n    \"\"\"Parses a single datashape from a string.\n\n    Parameters\n    ----------\n    ds_str : string\n        The datashape string to parse.\n    sym : TypeSymbolTable\n        The symbol tables of dimensions, dtypes, and type constructors for each.\n\n    \"\"\"\n    dsp = DataShapeParser(ds_str, sym)\n    ds = dsp.parse_datashape()\n    # If no datashape could be found\n    if ds is None:\n        dsp.raise_error('Invalid datashape')\n\n    # Make sure there's no garbage at the end\n    if dsp.pos != dsp.end_pos:\n        dsp.raise_error('Unexpected token in datashape')\n    return ds",
          "docstring": "Parses a single datashape from a string.\n\nParameters\n----------\nds_str : string\n    The datashape string to parse.\nsym : TypeSymbolTable\n    The symbol tables of dimensions, dtypes, and type constructors for each.",
          "signature": "def parse(ds_str, sym):",
          "type": "Function",
          "class_signature": null
        }
      }
    },
    "datashader/datashape/coretypes.py:DataShape:subarray": {
      "datashader/datashape/coretypes.py": {
        "Mono.parameters": {
          "code": "    def parameters(self):\n        if self._slotted:\n            return tuple((getattr(self, slot) for slot in self.__slots__))\n        else:\n            return self._parameters",
          "docstring": "",
          "signature": "def parameters(self):",
          "type": "Method",
          "class_signature": "class Mono(metaclass=Type):"
        }
      }
    },
    "datashader/datashape/coretypes.py:DataShape:__init__": {
      "datashader/datashape/coretypes.py": {
        "_launder": {
          "code": "def _launder(x):\n    \"\"\" Clean up types prior to insertion into DataShape\n\n    >>> from datashader.datashape import dshape\n    >>> _launder(5)         # convert ints to Fixed\n    Fixed(val=5)\n    >>> _launder('int32')   # parse strings\n    ctype(\"int32\")\n    >>> _launder(dshape('int32'))\n    ctype(\"int32\")\n    >>> _launder(Fixed(5))  # No-op on valid parameters\n    Fixed(val=5)\n    \"\"\"\n    if isinstance(x, int):\n        x = Fixed(x)\n    if isinstance(x, str):\n        x = datashape.dshape(x)\n    if isinstance(x, DataShape) and len(x) == 1:\n        return x[0]\n    if isinstance(x, Mono):\n        return x\n    return x",
          "docstring": "Clean up types prior to insertion into DataShape\n\n>>> from datashader.datashape import dshape\n>>> _launder(5)         # convert ints to Fixed\nFixed(val=5)\n>>> _launder('int32')   # parse strings\nctype(\"int32\")\n>>> _launder(dshape('int32'))\nctype(\"int32\")\n>>> _launder(Fixed(5))  # No-op on valid parameters\nFixed(val=5)",
          "signature": "def _launder(x):",
          "type": "Function",
          "class_signature": null
        }
      }
    },
    "datashader/datashape/coretypes.py:DataShape:__str__": {
      "datashader/datashape/coretypes.py": {
        "Mono.parameters": {
          "code": "    def parameters(self):\n        if self._slotted:\n            return tuple((getattr(self, slot) for slot in self.__slots__))\n        else:\n            return self._parameters",
          "docstring": "",
          "signature": "def parameters(self):",
          "type": "Method",
          "class_signature": "class Mono(metaclass=Type):"
        },
        "TypeVar.__str__": {
          "code": "    def __str__(self):\n        return str(self.symbol)",
          "docstring": "",
          "signature": "def __str__(self):",
          "type": "Method",
          "class_signature": "class TypeVar(Unit):"
        }
      }
    },
    "datashader/datashape/coretypes.py:Mono:__ne__": {}
  },
  "call_tree": {
    "datashader/datashape/tests/test_operations.py:test_scalar_subarray": {
      "datashader/datashape/coretypes.py:Mono:subarray": {},
      "datashader/datashape/coretypes.py:Mono:__eq__": {
        "datashader/datashape/coretypes.py:Mono:Mono": {},
        "datashader/datashape/coretypes.py:Mono:shape": {},
        "datashader/datashape/coretypes.py:Mono:measure": {},
        "datashader/datashape/coretypes.py:Mono:info": {
          "datashader/datashape/coretypes.py:Mono:parameters": {
            "datashader/datashape/coretypes.py:Mono:_slotted": {}
          }
        }
      }
    },
    "datashader/datashape/tests/test_operations.py:test_array_subarray": {
      "datashader/datashape/util/__init__.py:dshape": {
        "datashader/datashape/parser.py:parse": {
          "datashader/datashape/parser.py:DataShapeParser:__init__": {
            "datashader/datashape/parser.py:DataShapeParser:advance_tok": {
              "datashader/datashape/lexer.py:lex": {}
            }
          },
          "datashader/datashape/parser.py:DataShapeParser:parse_datashape": {
            "datashader/datashape/parser.py:DataShapeParser:tok": {},
            "datashader/datashape/parser.py:DataShapeParser:parse_datashape_nooption": {
              "datashader/datashape/parser.py:DataShapeParser:parse_dim": {
                "datashader/datashape/parser.py:DataShapeParser:tok": {},
                "datashader/datashape/parser.py:DataShapeParser:advance_tok": {
                  "datashader/datashape/lexer.py:lex": {}
                },
                "datashader/datashape/parser.py:DataShapeParser:syntactic_sugar": {},
                "datashader/datashape/coretypes.py:Fixed:__init__": {},
                "datashader/datashape/type_symbol_table.py:_typevar_dim": {
                  "datashader/datashape/coretypes.py:TypeVar:__init__": {}
                }
              },
              "datashader/datashape/parser.py:DataShapeParser:tok": {},
              "datashader/datashape/parser.py:DataShapeParser:advance_tok": {
                "datashader/datashape/lexer.py:lex": {}
              },
              "datashader/datashape/parser.py:DataShapeParser:parse_datashape": {
                "[ignored_or_cut_off]": "..."
              },
              "datashader/datashape/coretypes.py:Mono:parameters": {
                "datashader/datashape/coretypes.py:Mono:_slotted": {}
              },
              "datashader/datashape/coretypes.py:DataShape:__init__": {
                "datashader/datashape/coretypes.py:_launder": {}
              }
            }
          }
        },
        "datashader/datashape/validation.py:validate": {
          "datashader/datashape/coretypes.py:Var:Var": {},
          "datashader/datashape/validation.py:traverse": {
            "datashader/datashape/coretypes.py:Mono:parameters": {
              "datashader/datashape/coretypes.py:Mono:_slotted": {}
            },
            "datashader/datashape/validation.py:traverse": {
              "[ignored_or_cut_off]": "..."
            },
            "datashader/datashape/validation.py:_validate": {
              "datashader/datashape/coretypes.py:Mono:parameters": {
                "datashader/datashape/coretypes.py:Mono:_slotted": {}
              }
            }
          }
        }
      },
      "datashader/datashape/coretypes.py:DataShape:subarray": {
        "datashader/datashape/coretypes.py:Mono:parameters": {
          "datashader/datashape/coretypes.py:Mono:_slotted": {}
        },
        "datashader/datashape/coretypes.py:DataShape:__init__": {
          "datashader/datashape/coretypes.py:_launder": {}
        }
      },
      "datashader/datashape/coretypes.py:Mono:__eq__": {
        "datashader/datashape/coretypes.py:DataShape:shape": {
          "datashader/datashape/coretypes.py:Mono:parameters": {
            "datashader/datashape/coretypes.py:Mono:_slotted": {}
          }
        },
        "datashader/datashape/coretypes.py:Fixed:__eq__": {},
        "datashader/datashape/coretypes.py:DataShape:measure": {
          "datashader/datashape/coretypes.py:Mono:parameters": {
            "datashader/datashape/coretypes.py:Mono:_slotted": {}
          }
        },
        "datashader/datashape/coretypes.py:Mono:info": {
          "datashader/datashape/coretypes.py:Mono:parameters": {
            "datashader/datashape/coretypes.py:Mono:_slotted": {}
          }
        }
      },
      "datashader/datashape/coretypes.py:DataShape:__init__": {
        "datashader/datashape/coretypes.py:_launder": {}
      },
      "datashader/datashape/coretypes.py:DataShape:__str__": {
        "datashader/datashape/coretypes.py:Mono:parameters": {
          "datashader/datashape/coretypes.py:Mono:_slotted": {}
        },
        "datashader/datashape/coretypes.py:TypeVar:__str__": {},
        "datashader/datashape/coretypes.py:CType:__str__": {}
      },
      "datashader/datashape/coretypes.py:CType:__str__": {}
    },
    "datashader/datashape/tests/test_operations.py:test_dshape_compare": {
      "datashader/datashape/util/__init__.py:dshape": {
        "datashader/datashape/parser.py:parse": {
          "datashader/datashape/parser.py:DataShapeParser:__init__": {
            "datashader/datashape/parser.py:DataShapeParser:advance_tok": {
              "datashader/datashape/lexer.py:lex": {}
            }
          },
          "datashader/datashape/parser.py:DataShapeParser:parse_datashape": {
            "datashader/datashape/parser.py:DataShapeParser:tok": {},
            "datashader/datashape/parser.py:DataShapeParser:parse_datashape_nooption": {
              "datashader/datashape/parser.py:DataShapeParser:parse_dim": {
                "datashader/datashape/parser.py:DataShapeParser:tok": {},
                "datashader/datashape/parser.py:DataShapeParser:advance_tok": {
                  "datashader/datashape/lexer.py:lex": {}
                },
                "datashader/datashape/parser.py:DataShapeParser:syntactic_sugar": {},
                "datashader/datashape/coretypes.py:Fixed:__init__": {}
              },
              "datashader/datashape/parser.py:DataShapeParser:tok": {},
              "datashader/datashape/parser.py:DataShapeParser:advance_tok": {
                "datashader/datashape/lexer.py:lex": {}
              },
              "datashader/datashape/parser.py:DataShapeParser:parse_datashape": {
                "[ignored_or_cut_off]": "..."
              },
              "datashader/datashape/coretypes.py:Mono:parameters": {
                "datashader/datashape/coretypes.py:Mono:_slotted": {}
              },
              "datashader/datashape/coretypes.py:DataShape:__init__": {
                "datashader/datashape/coretypes.py:_launder": {}
              }
            }
          }
        },
        "datashader/datashape/validation.py:validate": {
          "datashader/datashape/validation.py:traverse": {
            "datashader/datashape/coretypes.py:Mono:parameters": {
              "datashader/datashape/coretypes.py:Mono:_slotted": {}
            },
            "datashader/datashape/validation.py:traverse": {
              "[ignored_or_cut_off]": "..."
            },
            "datashader/datashape/validation.py:_validate": {
              "datashader/datashape/coretypes.py:Mono:parameters": {
                "datashader/datashape/coretypes.py:Mono:_slotted": {}
              }
            }
          }
        }
      },
      "datashader/datashape/coretypes.py:Mono:__ne__": {
        "datashader/datashape/coretypes.py:Mono:__eq__": {
          "datashader/datashape/coretypes.py:Mono:shape": {},
          "datashader/datashape/coretypes.py:DataShape:shape": {
            "datashader/datashape/coretypes.py:Mono:parameters": {
              "datashader/datashape/coretypes.py:Mono:_slotted": {}
            }
          }
        }
      }
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_operations/datashader-test_operations/datashader/tests/test_pandas.py:test_line_manual_range": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_operations/datashader-test_operations/datashader/tests/test_pandas.py:test_line_autorange": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_operations/datashader-test_operations/datashader/tests/test_pandas.py:test_area_to_zero_fixedrange": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_operations/datashader-test_operations/datashader/tests/test_pandas.py:test_area_to_zero_autorange": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_operations/datashader-test_operations/datashader/tests/test_pandas.py:test_area_to_zero_autorange_gap": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_operations/datashader-test_operations/datashader/tests/test_pandas.py:test_area_to_line_autorange": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_operations/datashader-test_operations/datashader/datashape/tests/test_coretypes.py:test_record_parse_optional": {
      "datashader/datashape/coretypes.py:Option:Option": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_operations/datashader-test_operations/modified_testcases/test_pandas.py:test_line_manual_range": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_operations/datashader-test_operations/modified_testcases/test_pandas.py:test_line_autorange": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_operations/datashader-test_operations/modified_testcases/test_pandas.py:test_area_to_zero_fixedrange": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_operations/datashader-test_operations/modified_testcases/test_pandas.py:test_area_to_zero_autorange": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_operations/datashader-test_operations/modified_testcases/test_pandas.py:test_area_to_zero_autorange_gap": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_operations/datashader-test_operations/modified_testcases/test_pandas.py:test_area_to_line_autorange": {
      "datashader/datatypes.py:RaggedDtype": {}
    }
  },
  "PRD": "# PROJECT NAME: datashader-test_operations\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 datashader/\n    \u251c\u2500\u2500 datashape/\n    \u2502   \u251c\u2500\u2500 coretypes.py\n    \u2502   \u2502   \u251c\u2500\u2500 CType.__str__\n    \u2502   \u2502   \u251c\u2500\u2500 DataShape.__init__\n    \u2502   \u2502   \u251c\u2500\u2500 DataShape.__str__\n    \u2502   \u2502   \u251c\u2500\u2500 DataShape.subarray\n    \u2502   \u2502   \u251c\u2500\u2500 Mono.__eq__\n    \u2502   \u2502   \u251c\u2500\u2500 Mono.__ne__\n    \u2502   \u2502   \u251c\u2500\u2500 Mono.subarray\n    \u2502   \u2502   \u2514\u2500\u2500 Option.Option\n    \u2502   \u2514\u2500\u2500 util/\n    \u2502       \u2514\u2500\u2500 __init__.py\n    \u2502           \u2514\u2500\u2500 dshape\n    \u2514\u2500\u2500 datatypes.py\n        \u2514\u2500\u2500 RaggedDtype\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThis module is designed to validate and test the behavior of the `datashape` library, specifically its functionality for handling and manipulating data shapes and subarrays. It ensures correct behavior when accessing subarray representations of scalar types and multi-dimensional arrays while enforcing boundaries to prevent invalid subarray indexing. The module also evaluates and confirms proper comparisons between data shape objects. By providing rigorous validation, it helps developers ensure the reliability and correctness of `datashape` operations, reducing errors when working with complex data structures in applications involving data modeling or analysis.\n\n## FILE 1: datashader/datashape/coretypes.py\n\n- CLASS METHOD: DataShape.__str__\n  - CLASS SIGNATURE: class DataShape(Mono):\n  - SIGNATURE: def __str__(self):\n  - DOCSTRING: \n```python\n\"\"\"\nReturn a string representation of the DataShape instance.\n\nThis method generates a human-readable representation of the DataShape object,\neither by returning its registered name (if applicable) or by concatenating\nthe string representations of its parameters with a ' * ' separator. The parameters \ntypically include dimensions and a measure type, showcasing the structure of the data shape.\n\nReturns\n-------\nstr\n    A formatted string that represents the DataShape, either as its name or as a \n    combination of its components.\n\nAttributes\n----------\nself.name : str or None\n    The registered name of the DataShape, which is set during its initialization. \n    If a name is provided, it will be used for representation; otherwise, the parameters will be joined.\nself.parameters : tuple\n    A collection of parameters that define the DataShape. Each parameter is converted to a string\n    using its own __str__ method.\n\nThis method is primarily used for debugging and visualization purposes, allowing users to\nunderstand the structure of data shapes at a glance.\n\"\"\"\n```\n\n- CLASS METHOD: Mono.__eq__\n  - CLASS SIGNATURE: class Mono(metaclass=Type):\n  - SIGNATURE: def __eq__(self, other):\n  - DOCSTRING: \n```python\n\"\"\"\nChecks equality between two Mono instances.\n\nParameters\n----------\nother : object\n    The object to compare with the current Mono instance.\n\nReturns\n-------\nbool\n    Returns True if the other object is an instance of Mono and has the same shape and measure information as the current instance, otherwise returns False.\n\nNotes\n-----\nThe equality check relies on the `shape` property, which represents the dimensions of the Mono instance, and the `measure.info()` method, which provides the info of the measure associated with the Mono instances being compared. Both attributes are crucial in determining if the Moons are equivalent in structure and type.\n\"\"\"\n```\n\n- CLASS METHOD: CType.__str__\n  - CLASS SIGNATURE: class CType(Unit):\n  - SIGNATURE: def __str__(self):\n  - DOCSTRING: \n```python\n\"\"\"\nReturns the string representation of the CType instance, which is the name of the data type. This method does not take any parameters. The output is a string that corresponds to the name associated with the CType object, allowing for easy identification of the data type defined by this instance. The class attribute `name` is initialized during the instantiation of the CType class and represents the type's designation (e.g., 'int32', 'float64'). This string output serves as a simplified way to reference data types defined within the context of data shape and type management in the code.\n\"\"\"\n```\n\n- CLASS METHOD: Mono.__ne__\n  - CLASS SIGNATURE: class Mono(metaclass=Type):\n  - SIGNATURE: def __ne__(self, other):\n  - DOCSTRING: \n```python\n\"\"\"\nDetermines if two Mono instances are not equal.\n\nParameters\n----------\nother : Mono\n    The instance to compare against the current Mono instance.\n\nReturns\n-------\nbool\n    Returns `True` if the two instances are not equal; otherwise, returns `False`.\n\nThe method uses the `__eq__` method to check for equality, effectively implementing the logical negation of the equality comparison.\n\"\"\"\n```\n\n- CLASS METHOD: DataShape.subarray\n  - CLASS SIGNATURE: class DataShape(Mono):\n  - SIGNATURE: def subarray(self, leading):\n  - DOCSTRING: \n```python\n\"\"\"\nReturns a new DataShape object representing a subarray with a specified number of leading dimensions removed.\n\nParameters\n----------\nleading : int\n    The number of leading dimensions to remove from the DataShape.\n\nReturns\n-------\nDataShape\n    A new DataShape instance reflecting the shape after the specified dimensions have been eliminated.\n\nRaises\n------\nIndexError\n    If the leading parameter exceeds the number of dimensions available in the DataShape.\n\nNotes\n-----\nThis method utilizes the `parameters` attribute, which is a tuple storing the components of the DataShape. The method checks the length of this tuple to ensure valid subarray slicing before proceeding to construct a new DataShape from the remaining parameters. If the `leading` value corresponds to the shape's last index or is negative, it returns a new DataShape containing only the measure.\n\nExamples\n--------\n>>> from datashader.datashape import dshape\n>>> dshape('1 * 2 * 3 * int32').subarray(1)\ndshape(\"2 * 3 * int32\")\n>>> dshape('1 * 2 * 3 * int32').subarray(2)\ndshape(\"3 * int32\")\n\"\"\"\n```\n\n- CLASS METHOD: Mono.subarray\n  - CLASS SIGNATURE: class Mono(metaclass=Type):\n  - SIGNATURE: def subarray(self, leading):\n  - DOCSTRING: \n```python\n\"\"\"\nReturns a data shape object representing the subarray formed by removing a specified number of leading dimensions from the current shape. \n\nParameters\n----------\nleading : int\n    The number of leading dimensions to remove from the data shape. It must not exceed the current number of dimensions.\n\nReturns\n-------\nMono\n    A new instance of Mono representing the subarray after removing the specified number of leading dimensions. If `leading` is greater than or equal to 1, an IndexError is raised, indicating an insufficient number of dimensions to remove.\n\nNotes\n-----\nThis method is particularly relevant for instances of `CType`, where removing dimensions is restricted to zero. The method handles cases where there are insufficient dimensions by raising an IndexError with a descriptive message.\n\"\"\"\n```\n\n- CLASS METHOD: DataShape.__init__\n  - CLASS SIGNATURE: class DataShape(Mono):\n  - SIGNATURE: def __init__(self, *parameters, **kwds):\n  - DOCSTRING: \n```python\n\"\"\"\nInitialization method for the DataShape class, which represents a composite container for datashape elements in the unified shape and data type system.\n\nParameters\n----------\n*parameters : variable length argument list\n    Represents the datashape elements; these can be dimensions (of type Fixed or Var) and a measure (of type Measure). The last element must be a measure, while all preceding elements must be dimensions.\n**kwds : keyword arguments\n    Optional parameters including 'name', which can be used to assign a name to the DataShape and register it in the class's type registry.\n\nRaises\n------\nTypeError : If the constructor is called with a single string parameter or if the last parameter is not a measure, or if any preceding parameter is not a dimension.\nValueError : If no parameters are provided.\n\nAttributes\n----------\nself._parameters : tuple\n    A tuple of datashape elements created from the parameters after potential normalization via the _launder function.\n\nConstants Used\n--------------\nMEASURE : A constant representing the type of measures, defined for validation of the last parameter. It ensures that only a measure can occupy the last position in a datashape.\n\nThis method ensures that the DataShape is correctly configured with the right combination of dimensions and a measure, facilitating the representation of complex data structures.\n\"\"\"\n```\n\n## FILE 2: datashader/datatypes.py\n\n## FILE 3: datashader/datashape/util/__init__.py\n\n- FUNCTION NAME: dshape\n  - SIGNATURE: def dshape(o):\n  - DOCSTRING: \n```python\n\"\"\"\nParse a data shape (dshape) representation from various input types including strings, coretypes, and data structure representations.\n\nParameters:\n- o (Union[str, coretypes.DataShape, coretypes.CType, coretypes.String, coretypes.Record, coretypes.JSON, coretypes.Date, coretypes.Time, coretypes.DateTime, coretypes.Unit, coretypes.Mono, list, tuple]): The input object that defines the data shape. It can be a string representation of a dshape, an instance of various coretype classes, or a list/tuple of dimensions and types.\n\nReturns:\n- coretypes.DataShape: A validated DataShape object representing the parsed data structure.\n\nRaises:\n- TypeError: If the input object cannot be converted into a dshape.\n- ValidationError: If the resulting DataShape does not meet the validation criteria defined in the validate function.\n\nDependencies:\n- This function relies on the `parser` module to convert string representations into dshape objects, and the `validate` function to ensure the constructed dshape is valid. Various core types are sourced from the `coretypes` module, which includes classes for different data types.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - datashader/datashape/validation.py:validate\n    - datashader/datashape/parser.py:parse\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "datashader/datashape/coretypes.py": "\"\"\"\nThis defines the DataShape type system, with unified\nshape and data type.\n\"\"\"\nimport ctypes\nimport operator\nfrom collections import OrderedDict\nfrom math import ceil\nfrom datashader import datashape\nimport numpy as np\nfrom .internal_utils import IndexCallable, isidentifier\nDIMENSION = 1\nMEASURE = 2\n\nclass Type(type):\n    _registry = {}\n\n    def __new__(meta, name, bases, dct):\n        cls = super(Type, meta).__new__(meta, name, bases, dct)\n        if not dct.get('abstract'):\n            Type._registry[name] = cls\n        return cls\n\n    @classmethod\n    def register(cls, name, type):\n        if name in cls._registry:\n            raise TypeError('There is another type registered with name %s' % name)\n        cls._registry[name] = type\n\n    @classmethod\n    def lookup_type(cls, name):\n        return cls._registry[name]\n\nclass Mono(metaclass=Type):\n    \"\"\"\n    Monotype are unqualified 0 parameters.\n\n    Each type must be reconstructable using its parameters:\n\n        type(datashape_type)(*type.parameters)\n    \"\"\"\n    composite = False\n\n    def __init__(self, *params):\n        self._parameters = params\n\n    @property\n    def _slotted(self):\n        return hasattr(self, '__slots__')\n\n    @property\n    def parameters(self):\n        if self._slotted:\n            return tuple((getattr(self, slot) for slot in self.__slots__))\n        else:\n            return self._parameters\n\n    def info(self):\n        return (type(self), self.parameters)\n\n    def __hash__(self):\n        try:\n            h = self._hash\n        except AttributeError:\n            h = self._hash = hash(self.shape) ^ hash(self.measure.info())\n        return h\n\n    @property\n    def shape(self):\n        return ()\n\n    def __len__(self):\n        return 1\n\n    def __getitem__(self, key):\n        return [self][key]\n\n    def __repr__(self):\n        return '%s(%s)' % (type(self).__name__, ', '.join(('%s=%r' % (slot, getattr(self, slot)) for slot in self.__slots__) if self._slotted else map(repr, self.parameters)))\n\n    @property\n    def measure(self):\n        return self\n\n    def __mul__(self, other):\n        if isinstance(other, str):\n            from datashader import datashape\n            return datashape.dshape(other).__rmul__(self)\n        if isinstance(other, int):\n            other = Fixed(other)\n        if isinstance(other, DataShape):\n            return other.__rmul__(self)\n        return DataShape(self, other)\n\n    def __rmul__(self, other):\n        if isinstance(other, str):\n            from datashader import datashape\n            return self * datashape.dshape(other)\n        if isinstance(other, int):\n            other = Fixed(other)\n        return DataShape(other, self)\n\n    def __getstate__(self):\n        return self.parameters\n\n    def __setstate__(self, state):\n        if self._slotted:\n            for slot, val in zip(self.__slots__, state):\n                setattr(self, slot, val)\n        else:\n            self._parameters = state\n\n    def to_numpy_dtype(self):\n        raise TypeError('DataShape %s is not NumPy-compatible' % self)\n\nclass Unit(Mono):\n    \"\"\"\n    Unit type that does not need to be reconstructed.\n    \"\"\"\n\n    def __str__(self):\n        return type(self).__name__.lower()\n\nclass Ellipsis(Mono):\n    \"\"\"Ellipsis (...). Used to indicate a variable number of dimensions.\n\n    E.g.:\n\n        ... * float32    # float32 array w/ any number of dimensions\n        A... * float32   # float32 array w/ any number of dimensions,\n                        # associated with type variable A\n    \"\"\"\n    __slots__ = ('typevar',)\n\n    def __init__(self, typevar=None):\n        self.typevar = typevar\n\n    def __str__(self):\n        return str(self.typevar) + '...' if self.typevar else '...'\n\n    def __repr__(self):\n        return '%s(%r)' % (type(self).__name__, str(self))\n\nclass Null(Unit):\n    \"\"\"The null datashape.\"\"\"\n    pass\n\nclass Date(Unit):\n    \"\"\" Date type \"\"\"\n    cls = MEASURE\n    __slots__ = ()\n\n    def to_numpy_dtype(self):\n        return np.dtype('datetime64[D]')\n\nclass Time(Unit):\n    \"\"\" Time type \"\"\"\n    cls = MEASURE\n    __slots__ = ('tz',)\n\n    def __init__(self, tz=None):\n        if tz is not None and (not isinstance(tz, str)):\n            raise TypeError('tz parameter to time datashape must be a string')\n        self.tz = tz\n\n    def __str__(self):\n        basename = super().__str__()\n        if self.tz is None:\n            return basename\n        else:\n            return '%s[tz=%r]' % (basename, str(self.tz))\n\nclass DateTime(Unit):\n    \"\"\" DateTime type \"\"\"\n    cls = MEASURE\n    __slots__ = ('tz',)\n\n    def __init__(self, tz=None):\n        if tz is not None and (not isinstance(tz, str)):\n            raise TypeError('tz parameter to datetime datashape must be a string')\n        self.tz = tz\n\n    def __str__(self):\n        basename = super().__str__()\n        if self.tz is None:\n            return basename\n        else:\n            return '%s[tz=%r]' % (basename, str(self.tz))\n\n    def to_numpy_dtype(self):\n        return np.dtype('datetime64[us]')\n_units = ('ns', 'us', 'ms', 's', 'm', 'h', 'D', 'W', 'M', 'Y')\n_unit_aliases = {'year': 'Y', 'week': 'W', 'day': 'D', 'date': 'D', 'hour': 'h', 'second': 's', 'millisecond': 'ms', 'microsecond': 'us', 'nanosecond': 'ns'}\n\ndef normalize_time_unit(s):\n    \"\"\" Normalize time input to one of 'year', 'second', 'millisecond', etc..\n    Example\n    -------\n    >>> normalize_time_unit('milliseconds')\n    'ms'\n    >>> normalize_time_unit('ms')\n    'ms'\n    >>> normalize_time_unit('nanoseconds')\n    'ns'\n    >>> normalize_time_unit('nanosecond')\n    'ns'\n    \"\"\"\n    s = s.strip()\n    if s in _units:\n        return s\n    if s in _unit_aliases:\n        return _unit_aliases[s]\n    if s[-1] == 's' and len(s) > 2:\n        return normalize_time_unit(s.rstrip('s'))\n    raise ValueError('Do not understand time unit %s' % s)\n\nclass TimeDelta(Unit):\n    cls = MEASURE\n    __slots__ = ('unit',)\n\n    def __init__(self, unit='us'):\n        self.unit = normalize_time_unit(str(unit))\n\n    def __str__(self):\n        return 'timedelta[unit=%r]' % self.unit\n\n    def to_numpy_dtype(self):\n        return np.dtype('timedelta64[%s]' % self.unit)\n\nclass Units(Unit):\n    \"\"\" Units type for values with physical units \"\"\"\n    cls = MEASURE\n    __slots__ = ('unit', 'tp')\n\n    def __init__(self, unit, tp=None):\n        if not isinstance(unit, str):\n            raise TypeError('unit parameter to units datashape must be a string')\n        if tp is None:\n            tp = DataShape(float64)\n        elif not isinstance(tp, DataShape):\n            raise TypeError('tp parameter to units datashape must be a datashape type')\n        self.unit = unit\n        self.tp = tp\n\n    def __str__(self):\n        if self.tp == DataShape(float64):\n            return 'units[%r]' % self.unit\n        else:\n            return 'units[%r, %s]' % (self.unit, self.tp)\n\nclass Bytes(Unit):\n    \"\"\" Bytes type \"\"\"\n    cls = MEASURE\n    __slots__ = ()\n_canonical_string_encodings = {'A': 'A', 'ascii': 'A', 'U8': 'U8', 'utf-8': 'U8', 'utf_8': 'U8', 'utf8': 'U8', 'U16': 'U16', 'utf-16': 'U16', 'utf_16': 'U16', 'utf16': 'U16', 'U32': 'U32', 'utf-32': 'U32', 'utf_32': 'U32', 'utf32': 'U32'}\n\nclass String(Unit):\n    \"\"\" String container\n\n    >>> String()\n    ctype(\"string\")\n    >>> String(10, 'ascii')\n    ctype(\"string[10, 'A']\")\n    \"\"\"\n    cls = MEASURE\n    __slots__ = ('fixlen', 'encoding')\n\n    def __init__(self, *args):\n        if len(args) == 0:\n            fixlen, encoding = (None, None)\n        if len(args) == 1:\n            if isinstance(args[0], str):\n                fixlen, encoding = (None, args[0])\n            if isinstance(args[0], int):\n                fixlen, encoding = (args[0], None)\n        elif len(args) == 2:\n            fixlen, encoding = args\n        encoding = encoding or 'U8'\n        if isinstance(encoding, str):\n            encoding = str(encoding)\n        try:\n            encoding = _canonical_string_encodings[encoding]\n        except KeyError:\n            raise ValueError('Unsupported string encoding %s' % repr(encoding))\n        self.encoding = encoding\n        self.fixlen = fixlen\n\n    def __str__(self):\n        if self.fixlen is None and self.encoding == 'U8':\n            return 'string'\n        elif self.fixlen is not None and self.encoding == 'U8':\n            return 'string[%i]' % self.fixlen\n        elif self.fixlen is None and self.encoding != 'U8':\n            return 'string[%s]' % repr(self.encoding).strip('u')\n        else:\n            return 'string[%i, %s]' % (self.fixlen, repr(self.encoding).strip('u'))\n\n    def __repr__(self):\n        s = str(self)\n        return 'ctype(\"%s\")' % s.encode('unicode_escape').decode('ascii')\n\n    def to_numpy_dtype(self):\n        \"\"\"\n        >>> String().to_numpy_dtype()\n        dtype('O')\n        >>> String(30).to_numpy_dtype()\n        dtype('<U30')\n        >>> String(30, 'A').to_numpy_dtype()\n        dtype('S30')\n        \"\"\"\n        if self.fixlen:\n            if self.encoding == 'A':\n                return np.dtype('S%d' % self.fixlen)\n            else:\n                return np.dtype('U%d' % self.fixlen)\n        return np.dtype('O', metadata={'vlen': str})\n\nclass Decimal(Unit):\n    \"\"\"Decimal type corresponding to SQL Decimal/Numeric types.\n\n    The first parameter passed specifies the number of digits of precision that\n    the Decimal contains. If an additional parameter is given, it represents\n    the scale, or number of digits of precision that are after the decimal\n    point.\n\n    The Decimal type makes no requirement of how it is to be stored in memory,\n    therefore, the number of bytes needed to store a Decimal for a given\n    precision will vary based on the platform where it is used.\n\n    Examples\n    --------\n    >>> Decimal(18)\n    Decimal(precision=18, scale=0)\n    >>> Decimal(7, 4)\n    Decimal(precision=7, scale=4)\n    >>> Decimal(precision=11, scale=2)\n    Decimal(precision=11, scale=2)\n    \"\"\"\n    cls = MEASURE\n    __slots__ = ('precision', 'scale')\n\n    def __init__(self, precision, scale=0):\n        self.precision = precision\n        self.scale = scale\n\n    def __str__(self):\n        return 'decimal[precision={precision}, scale={scale}]'.format(precision=self.precision, scale=self.scale)\n\n    def to_numpy_dtype(self):\n        \"\"\"Convert a decimal datashape to a NumPy dtype.\n\n        Note that floating-point (scale > 0) precision will be lost converting\n        to NumPy floats.\n\n        Examples\n        --------\n        >>> Decimal(18).to_numpy_dtype()\n        dtype('int64')\n        >>> Decimal(7,4).to_numpy_dtype()\n        dtype('float64')\n        \"\"\"\n        if self.scale == 0:\n            if self.precision <= 2:\n                return np.dtype(np.int8)\n            elif self.precision <= 4:\n                return np.dtype(np.int16)\n            elif self.precision <= 9:\n                return np.dtype(np.int32)\n            elif self.precision <= 18:\n                return np.dtype(np.int64)\n            else:\n                raise TypeError('Integer Decimal precision > 18 is not NumPy-compatible')\n        else:\n            return np.dtype(np.float64)\n\nclass DataShape(Mono):\n    \"\"\"\n    Composite container for datashape elements.\n\n    Elements of a datashape like ``Fixed(3)``, ``Var()`` or ``int32`` are on,\n    on their own, valid datashapes.  These elements are collected together into\n    a composite ``DataShape`` to be complete.\n\n    This class is not intended to be used directly.  Instead, use the utility\n    ``dshape`` function to create datashapes from strings or datashape\n    elements.\n\n    Examples\n    --------\n\n    >>> from datashader.datashape import Fixed, int32, DataShape, dshape\n\n    >>> DataShape(Fixed(5), int32)  # Rare to DataShape directly\n    dshape(\"5 * int32\")\n\n    >>> dshape('5 * int32')         # Instead use the dshape function\n    dshape(\"5 * int32\")\n\n    >>> dshape([Fixed(5), int32])   # It can even do construction from elements\n    dshape(\"5 * int32\")\n\n    See Also\n    --------\n    datashape.dshape\n    \"\"\"\n    composite = False\n\n    def __len__(self):\n        return len(self.parameters)\n\n    def __getitem__(self, index):\n        return self.parameters[index]\n\n    def __repr__(self):\n        s = pprint(self)\n        if '\\n' in s:\n            return 'dshape(\"\"\"%s\"\"\")' % s\n        else:\n            return 'dshape(\"%s\")' % s\n\n    @property\n    def shape(self):\n        return self.parameters[:-1]\n\n    @property\n    def measure(self):\n        return self.parameters[-1]\n\n    def __rmul__(self, other):\n        if isinstance(other, int):\n            other = Fixed(other)\n        return DataShape(other, *self)\n\n    @property\n    def subshape(self):\n        return IndexCallable(self._subshape)\n\n    def _subshape(self, index):\n        \"\"\" The DataShape of an indexed subarray\n\n        >>> from datashader.datashape import dshape\n\n        >>> ds = dshape('var * {name: string, amount: int32}')\n        >>> print(ds.subshape[0])\n        {name: string, amount: int32}\n\n        >>> print(ds.subshape[0:3])\n        3 * {name: string, amount: int32}\n\n        >>> print(ds.subshape[0:7:2, 'amount'])\n        4 * int32\n\n        >>> print(ds.subshape[[1, 10, 15]])\n        3 * {name: string, amount: int32}\n\n        >>> ds = dshape('{x: int, y: int}')\n        >>> print(ds.subshape['x'])\n        int32\n\n        >>> ds = dshape('10 * var * 10 * int32')\n        >>> print(ds.subshape[0:5, 0:3, 5])\n        5 * 3 * int32\n\n        >>> ds = dshape('var * {name: string, amount: int32, id: int32}')\n        >>> print(ds.subshape[:, [0, 2]])\n        var * {name: string, id: int32}\n\n        >>> ds = dshape('var * {name: string, amount: int32, id: int32}')\n        >>> print(ds.subshape[:, ['name', 'id']])\n        var * {name: string, id: int32}\n\n        >>> print(ds.subshape[0, 1:])\n        {amount: int32, id: int32}\n        \"\"\"\n        from .predicates import isdimension\n        if isinstance(index, int) and isdimension(self[0]):\n            return self.subarray(1)\n        if isinstance(self[0], Record) and isinstance(index, str):\n            return self[0][index]\n        if isinstance(self[0], Record) and isinstance(index, int):\n            return self[0].parameters[0][index][1]\n        if isinstance(self[0], Record) and isinstance(index, list):\n            rec = self[0]\n            index = [self[0].names.index(i) if isinstance(i, str) else i for i in index]\n            return DataShape(Record([rec.parameters[0][i] for i in index]))\n        if isinstance(self[0], Record) and isinstance(index, slice):\n            rec = self[0]\n            return DataShape(Record(rec.parameters[0][index]))\n        if isinstance(index, list) and isdimension(self[0]):\n            return len(index) * self.subarray(1)\n        if isinstance(index, slice):\n            if isinstance(self[0], Fixed):\n                n = int(self[0])\n                start = index.start or 0\n                stop = index.stop or n\n                if start < 0:\n                    start = n + start\n                if stop < 0:\n                    stop = n + stop\n                count = stop - start\n            else:\n                start = index.start or 0\n                stop = index.stop\n                if not stop:\n                    count = -start if start < 0 else var\n                if stop is not None and start is not None and (stop >= 0) and (start >= 0):\n                    count = stop - start\n                else:\n                    count = var\n            if count != var and index.step is not None:\n                count = int(ceil(count / index.step))\n            return count * self.subarray(1)\n        if isinstance(index, tuple):\n            if not index:\n                return self\n            elif index[0] is None:\n                return 1 * self._subshape(index[1:])\n            elif len(index) == 1:\n                return self._subshape(index[0])\n            else:\n                ds = self.subarray(1)._subshape(index[1:])\n                return (self[0] * ds)._subshape(index[0])\n        raise TypeError('invalid index value %s of type %r' % (index, type(index).__name__))\n\n    def __setstate__(self, state):\n        self._parameters = state\n        self.composite = True\n        self.name = None\nnumpy_provides_missing = frozenset((Date, DateTime, TimeDelta))\n\nclass Option(Mono):\n    \"\"\"\n    Measure types which may or may not hold data. Makes no\n    indication of how this is implemented in memory.\n    \"\"\"\n    __slots__ = ('ty',)\n\n    def __init__(self, ds):\n        self.ty = _launder(ds)\n\n    @property\n    def shape(self):\n        return self.ty.shape\n\n    @property\n    def itemsize(self):\n        return self.ty.itemsize\n\n    def __str__(self):\n        return '?%s' % self.ty\n\n    def to_numpy_dtype(self):\n        if type(self.ty) in numpy_provides_missing:\n            return self.ty.to_numpy_dtype()\n        raise TypeError('DataShape measure %s is not NumPy-compatible' % self)\n\nclass CType(Unit):\n    \"\"\"\n    Symbol for a sized type mapping uniquely to a native type.\n    \"\"\"\n    cls = MEASURE\n    __slots__ = ('name', '_itemsize', '_alignment')\n\n    def __init__(self, name, itemsize, alignment):\n        self.name = name\n        self._itemsize = itemsize\n        self._alignment = alignment\n        Type.register(name, self)\n\n    @classmethod\n    def from_numpy_dtype(self, dt):\n        \"\"\"\n        From Numpy dtype.\n\n        >>> from datashader.datashape import CType\n        >>> from numpy import dtype\n        >>> CType.from_numpy_dtype(dtype('int32'))\n        ctype(\"int32\")\n        >>> CType.from_numpy_dtype(dtype('i8'))\n        ctype(\"int64\")\n        >>> CType.from_numpy_dtype(dtype('M8'))\n        DateTime(tz=None)\n        >>> CType.from_numpy_dtype(dtype('U30'))  # doctest: +SKIP\n        ctype(\"string[30, 'U32']\")\n        \"\"\"\n        try:\n            return Type.lookup_type(dt.name)\n        except KeyError:\n            pass\n        if np.issubdtype(dt, np.datetime64):\n            unit, _ = np.datetime_data(dt)\n            defaults = {'D': date_, 'Y': date_, 'M': date_, 'W': date_}\n            return defaults.get(unit, datetime_)\n        elif np.issubdtype(dt, np.timedelta64):\n            unit, _ = np.datetime_data(dt)\n            return TimeDelta(unit=unit)\n        elif np.__version__[0] < '2' and np.issubdtype(dt, np.unicode_):\n            return String(dt.itemsize // 4, 'U32')\n        elif np.issubdtype(dt, np.str_) or np.issubdtype(dt, np.bytes_):\n            return String(dt.itemsize, 'ascii')\n        raise NotImplementedError('NumPy datatype %s not supported' % dt)\n\n    @property\n    def itemsize(self):\n        \"\"\"The size of one element of this type.\"\"\"\n        return self._itemsize\n\n    @property\n    def alignment(self):\n        \"\"\"The alignment of one element of this type.\"\"\"\n        return self._alignment\n\n    def to_numpy_dtype(self):\n        \"\"\"\n        To Numpy dtype.\n        \"\"\"\n        name = self.name\n        return np.dtype({'complex[float32]': 'complex64', 'complex[float64]': 'complex128'}.get(name, name))\n\n    def __repr__(self):\n        s = str(self)\n        return 'ctype(\"%s\")' % s.encode('unicode_escape').decode('ascii')\n\nclass Fixed(Unit):\n    \"\"\"\n    Fixed dimension.\n    \"\"\"\n    cls = DIMENSION\n    __slots__ = ('val',)\n\n    def __init__(self, i):\n        i = operator.index(i)\n        if i < 0:\n            raise ValueError('Fixed dimensions must be positive')\n        self.val = i\n\n    def __index__(self):\n        return self.val\n\n    def __int__(self):\n        return self.val\n\n    def __eq__(self, other):\n        return type(other) is Fixed and self.val == other.val or (isinstance(other, int) and self.val == other)\n    __hash__ = Mono.__hash__\n\n    def __str__(self):\n        return str(self.val)\n\nclass Var(Unit):\n    \"\"\" Variable dimension \"\"\"\n    cls = DIMENSION\n    __slots__ = ()\n\nclass TypeVar(Unit):\n    \"\"\"\n    A free variable in the signature. Not user facing.\n    \"\"\"\n    __slots__ = ('symbol',)\n\n    def __init__(self, symbol):\n        if not symbol[0].isupper():\n            raise ValueError('TypeVar symbol %r does not begin with a capital' % symbol)\n        self.symbol = symbol\n\n    def __str__(self):\n        return str(self.symbol)\n\nclass Function(Mono):\n    \"\"\"Function signature type\n    \"\"\"\n\n    @property\n    def restype(self):\n        return self.parameters[-1]\n\n    @property\n    def argtypes(self):\n        return self.parameters[:-1]\n\n    def __str__(self):\n        return '(%s) -> %s' % (', '.join(map(str, self.argtypes)), self.restype)\n\nclass Map(Mono):\n    __slots__ = ('key', 'value')\n\n    def __init__(self, key, value):\n        self.key = _launder(key)\n        self.value = _launder(value)\n\n    def __str__(self):\n        return '%s[%s, %s]' % (type(self).__name__.lower(), self.key, self.value)\n\n    def to_numpy_dtype(self):\n        return to_numpy_dtype(self)\n\ndef _launder(x):\n    \"\"\" Clean up types prior to insertion into DataShape\n\n    >>> from datashader.datashape import dshape\n    >>> _launder(5)         # convert ints to Fixed\n    Fixed(val=5)\n    >>> _launder('int32')   # parse strings\n    ctype(\"int32\")\n    >>> _launder(dshape('int32'))\n    ctype(\"int32\")\n    >>> _launder(Fixed(5))  # No-op on valid parameters\n    Fixed(val=5)\n    \"\"\"\n    if isinstance(x, int):\n        x = Fixed(x)\n    if isinstance(x, str):\n        x = datashape.dshape(x)\n    if isinstance(x, DataShape) and len(x) == 1:\n        return x[0]\n    if isinstance(x, Mono):\n        return x\n    return x\n\nclass CollectionPrinter:\n\n    def __repr__(self):\n        s = str(self)\n        strs = ('\"\"\"%s\"\"\"' if '\\n' in s else '\"%s\"') % s\n        return 'dshape(%s)' % strs\n\nclass RecordMeta(Type):\n\n    @staticmethod\n    def _unpack_slice(s, idx):\n        if not isinstance(s, slice):\n            raise TypeError('invalid field specification at position %d.\\nfields must be formatted like: {name}:{type}' % idx)\n        name, type_ = packed = (s.start, s.stop)\n        if name is None:\n            raise TypeError('missing field name at position %d' % idx)\n        if not isinstance(name, str):\n            raise TypeError(\"field name at position %d ('%s') was not a string\" % (idx, name))\n        if type_ is None and s.step is None:\n            raise TypeError(\"missing type for field '%s' at position %d\" % (name, idx))\n        if s.step is not None:\n            raise TypeError(\"unexpected slice step for field '%s' at position %d.\\nhint: you might have a second ':'\" % (name, idx))\n        return packed\n\n    def __getitem__(self, types):\n        if not isinstance(types, tuple):\n            types = (types,)\n        return self(list(map(self._unpack_slice, types, range(len(types)))))\n\nclass Record(CollectionPrinter, Mono, metaclass=RecordMeta):\n    \"\"\"\n    A composite data structure of ordered fields mapped to types.\n\n    Properties\n    ----------\n\n    fields: tuple of (name, type) pairs\n        The only stored data, also the input to ``__init__``\n    dict: dict\n        A dictionary view of ``fields``\n    names: list of strings\n        A list of the names\n    types: list of datashapes\n        A list of the datashapes\n\n    Example\n    -------\n\n    >>> Record([['id', 'int'], ['name', 'string'], ['amount', 'real']])\n    dshape(\"{id: int32, name: string, amount: float64}\")\n    \"\"\"\n    cls = MEASURE\n\n    def __init__(self, fields):\n        \"\"\"\n        Parameters\n        ----------\n        fields : list/OrderedDict of (name, type) entries\n            The fields which make up the record.\n        \"\"\"\n        if isinstance(fields, OrderedDict):\n            fields = fields.items()\n        fields = list(fields)\n        names = [str(name) if not isinstance(name, str) else name for name, _ in fields]\n        types = [_launder(v) for _, v in fields]\n        if len(set(names)) != len(names):\n            for name in set(names):\n                names.remove(name)\n            raise ValueError('duplicate field names found: %s' % names)\n        self._parameters = (tuple(zip(names, types)),)\n\n    @property\n    def fields(self):\n        return self._parameters[0]\n\n    @property\n    def dict(self):\n        return dict(self.fields)\n\n    @property\n    def names(self):\n        return [n for n, t in self.fields]\n\n    @property\n    def types(self):\n        return [t for n, t in self.fields]\n\n    def to_numpy_dtype(self):\n        \"\"\"\n        To Numpy record dtype.\n        \"\"\"\n        return np.dtype([(str(name), to_numpy_dtype(typ)) for name, typ in self.fields])\n\n    def __getitem__(self, key):\n        return self.dict[key]\n\n    def __str__(self):\n        return pprint(self)\nR = Record\n\ndef _format_categories(cats, n=10):\n    return '[%s%s]' % (', '.join(map(repr, cats[:n])), ', ...' if len(cats) > n else '')\n\nclass Categorical(Mono):\n    \"\"\"Unordered categorical type.\n    \"\"\"\n    __slots__ = ('categories', 'type', 'ordered')\n    cls = MEASURE\n\n    def __init__(self, categories, type=None, ordered=False):\n        self.categories = tuple(categories)\n        self.type = (type or datashape.discover(self.categories)).measure\n        self.ordered = ordered\n\n    def __str__(self):\n        return '%s[%s, type=%s, ordered=%s]' % (type(self).__name__.lower(), _format_categories(self.categories), self.type, self.ordered)\n\n    def __repr__(self):\n        return '%s(categories=%s, type=%r, ordered=%s)' % (type(self).__name__, _format_categories(self.categories), self.type, self.ordered)\n\nclass Tuple(CollectionPrinter, Mono):\n    \"\"\"\n    A product type.\n    \"\"\"\n    __slots__ = ('dshapes',)\n    cls = MEASURE\n\n    def __init__(self, dshapes):\n        \"\"\"\n        Parameters\n        ----------\n        dshapes : list of dshapes\n            The datashapes which make up the tuple.\n        \"\"\"\n        dshapes = [DataShape(ds) if not isinstance(ds, DataShape) else ds for ds in dshapes]\n        self.dshapes = tuple(dshapes)\n\n    def __str__(self):\n        return '(%s)' % ', '.join(map(str, self.dshapes))\n\n    def to_numpy_dtype(self):\n        \"\"\"\n        To Numpy record dtype.\n        \"\"\"\n        return np.dtype([('f%d' % i, to_numpy_dtype(typ)) for i, typ in enumerate(self.parameters[0])])\n\nclass JSON(Mono):\n    \"\"\" JSON measure \"\"\"\n    cls = MEASURE\n    __slots__ = ()\n\n    def __str__(self):\n        return 'json'\nbool_ = CType('bool', 1, 1)\nchar = CType('char', 1, 1)\nint8 = CType('int8', 1, 1)\nint16 = CType('int16', 2, ctypes.alignment(ctypes.c_int16))\nint32 = CType('int32', 4, ctypes.alignment(ctypes.c_int32))\nint64 = CType('int64', 8, ctypes.alignment(ctypes.c_int64))\nint_ = int32\nType.register('int', int_)\nuint8 = CType('uint8', 1, 1)\nuint16 = CType('uint16', 2, ctypes.alignment(ctypes.c_uint16))\nuint32 = CType('uint32', 4, ctypes.alignment(ctypes.c_uint32))\nuint64 = CType('uint64', 8, ctypes.alignment(ctypes.c_uint64))\nfloat16 = CType('float16', 2, ctypes.alignment(ctypes.c_uint16))\nfloat32 = CType('float32', 4, ctypes.alignment(ctypes.c_float))\nfloat64 = CType('float64', 8, ctypes.alignment(ctypes.c_double))\nreal = float64\nType.register('real', real)\ncomplex_float32 = CType('complex[float32]', 8, ctypes.alignment(ctypes.c_float))\ncomplex_float64 = CType('complex[float64]', 16, ctypes.alignment(ctypes.c_double))\nType.register('complex64', complex_float32)\ncomplex64 = complex_float32\nType.register('complex128', complex_float64)\ncomplex128 = complex_float64\ncomplex_ = complex_float64\ndate_ = Date()\ntime_ = Time()\ndatetime_ = DateTime()\ntimedelta_ = TimeDelta()\nType.register('date', date_)\nType.register('time', time_)\nType.register('datetime', datetime_)\nType.register('timedelta', timedelta_)\nnull = Null()\nType.register('null', null)\nc_byte = int8\nc_short = int16\nc_int = int32\nc_longlong = int64\nc_ubyte = uint8\nc_ushort = uint16\nc_ulonglong = uint64\nif ctypes.sizeof(ctypes.c_long) == 4:\n    c_long = int32\n    c_ulong = uint32\nelse:\n    c_long = int64\n    c_ulong = uint64\nif ctypes.sizeof(ctypes.c_void_p) == 4:\n    intptr = c_ssize_t = int32\n    uintptr = c_size_t = uint32\nelse:\n    intptr = c_ssize_t = int64\n    uintptr = c_size_t = uint64\nType.register('intptr', intptr)\nType.register('uintptr', uintptr)\nc_half = float16\nc_float = float32\nc_double = float64\nhalf = float16\nsingle = float32\ndouble = float64\nvoid = CType('void', 0, 1)\nobject_ = pyobj = CType('object', ctypes.sizeof(ctypes.py_object), ctypes.alignment(ctypes.py_object))\nna = Null\nNullRecord = Record(())\nbytes_ = Bytes()\nstring = String()\njson = JSON()\nType.register('float', c_float)\nType.register('double', c_double)\nType.register('bytes', bytes_)\nType.register('string', String())\nvar = Var()\n\ndef to_numpy_dtype(ds):\n    \"\"\" Throw away the shape information and just return the\n    measure as NumPy dtype instance.\"\"\"\n    if isinstance(ds.measure, datashape.coretypes.Map):\n        ds = ds.measure.key\n    return to_numpy(ds.measure)[1]\n\ndef to_numpy(ds):\n    \"\"\"\n    Downcast a datashape object into a Numpy (shape, dtype) tuple if\n    possible.\n\n    >>> from datashader.datashape import dshape, to_numpy\n    >>> to_numpy(dshape('5 * 5 * int32'))\n    ((5, 5), dtype('int32'))\n    >>> to_numpy(dshape('10 * string[30]'))\n    ((10,), dtype('<U30'))\n    >>> to_numpy(dshape('N * int32'))\n    ((-1,), dtype('int32'))\n    \"\"\"\n    shape = []\n    if isinstance(ds, DataShape):\n        for dim in ds[:-1]:\n            if isinstance(dim, Fixed):\n                shape.append(int(dim))\n            elif isinstance(dim, TypeVar):\n                shape.append(-1)\n            else:\n                raise TypeError('DataShape dimension %s is not NumPy-compatible' % dim)\n        msr = ds[-1]\n    else:\n        msr = ds\n    return (tuple(shape), msr.to_numpy_dtype())\n\ndef from_numpy(shape, dt):\n    \"\"\"\n    Upcast a (shape, dtype) tuple if possible.\n\n    >>> from datashader.datashape import from_numpy\n    >>> from numpy import dtype\n    >>> from_numpy((5, 5), dtype('int32'))\n    dshape(\"5 * 5 * int32\")\n\n    >>> from_numpy((10,), dtype('S10'))\n    dshape(\"10 * string[10, 'A']\")\n    \"\"\"\n    dtype = np.dtype(dt)\n    if dtype.kind == 'S':\n        measure = String(dtype.itemsize, 'A')\n    elif dtype.kind == 'U':\n        measure = String(dtype.itemsize // 4, 'U32')\n    elif dtype.fields:\n        fields = [(name, dtype.fields[name]) for name in dtype.names]\n        rec = [(name, from_numpy(t.shape, t.base)) for name, (t, _) in fields]\n        measure = Record(rec)\n    else:\n        measure = CType.from_numpy_dtype(dtype)\n    if not shape:\n        return measure\n    return DataShape(*tuple(map(Fixed, shape)) + (measure,))\n\ndef print_unicode_string(s):\n    try:\n        return s.decode('unicode_escape').encode('ascii')\n    except AttributeError:\n        return s\n\ndef pprint(ds, width=80):\n    ''' Pretty print a datashape\n\n    >>> from datashader.datashape import dshape, pprint\n    >>> print(pprint(dshape('5 * 3 * int32')))\n    5 * 3 * int32\n\n    >>> ds = dshape(\"\"\"\n    ... 5000000000 * {\n    ...     a: (int, float32, real, string, datetime),\n    ...     b: {c: 5 * int, d: var * 100 * float32}\n    ... }\"\"\")\n    >>> print(pprint(ds))\n    5000000000 * {\n      a: (int32, float32, float64, string, datetime),\n      b: {c: 5 * int32, d: var * 100 * float32}\n      }\n\n    Record measures print like full datashapes\n    >>> print(pprint(ds.measure, width=30))\n    {\n      a: (\n        int32,\n        float32,\n        float64,\n        string,\n        datetime\n        ),\n      b: {\n        c: 5 * int32,\n        d: var * 100 * float32\n        }\n      }\n\n    Control width of the result\n    >>> print(pprint(ds, width=30))\n    5000000000 * {\n      a: (\n        int32,\n        float32,\n        float64,\n        string,\n        datetime\n        ),\n      b: {\n        c: 5 * int32,\n        d: var * 100 * float32\n        }\n      }\n    >>>\n    '''\n    result = ''\n    if isinstance(ds, DataShape):\n        if ds.shape:\n            result += ' * '.join(map(str, ds.shape))\n            result += ' * '\n        ds = ds[-1]\n    if isinstance(ds, Record):\n        pairs = ['%s: %s' % (name if isidentifier(name) else repr(print_unicode_string(name)), pprint(typ, width - len(result) - len(name))) for name, typ in zip(ds.names, ds.types)]\n        short = '{%s}' % ', '.join(pairs)\n        if len(result + short) < width:\n            return result + short\n        else:\n            long = '{\\n%s\\n}' % ',\\n'.join(pairs)\n            return result + long.replace('\\n', '\\n  ')\n    elif isinstance(ds, Tuple):\n        typs = [pprint(typ, width - len(result)) for typ in ds.dshapes]\n        short = '(%s)' % ', '.join(typs)\n        if len(result + short) < width:\n            return result + short\n        else:\n            long = '(\\n%s\\n)' % ',\\n'.join(typs)\n            return result + long.replace('\\n', '\\n  ')\n    else:\n        result += str(ds)\n    return result",
    "datashader/datatypes.py": "from __future__ import annotations\nimport re\nfrom functools import total_ordering\nfrom packaging.version import Version\nimport numpy as np\nimport pandas as pd\nfrom numba import jit\nfrom pandas.api.extensions import ExtensionDtype, ExtensionArray, register_extension_dtype\nfrom numbers import Integral\nfrom pandas.api.types import pandas_dtype, is_extension_array_dtype\ntry:\n    from dask.dataframe.extensions import make_array_nonempty\nexcept ImportError:\n    make_array_nonempty = None\n\ndef _validate_ragged_properties(start_indices, flat_array):\n    \"\"\"\n    Validate that start_indices are flat_array arrays that may be used to\n    represent a valid RaggedArray.\n\n    Parameters\n    ----------\n    flat_array: numpy array containing concatenation\n                of all nested arrays to be represented\n                by this ragged array\n    start_indices: unsigned integer numpy array the same\n                   length as the ragged array where values\n                   represent the index into flat_array where\n                   the corresponding ragged array element\n                   begins\n    Raises\n    ------\n    ValueError:\n        if input arguments are invalid or incompatible properties\n    \"\"\"\n    if not isinstance(start_indices, np.ndarray) or start_indices.dtype.kind != 'u' or start_indices.ndim != 1:\n        raise ValueError(\"\\nThe start_indices property of a RaggedArray must be a 1D numpy array of\\nunsigned integers (start_indices.dtype.kind == 'u')\\n    Received value of type {typ}: {v}\".format(typ=type(start_indices), v=repr(start_indices)))\n    if not isinstance(flat_array, np.ndarray) or flat_array.ndim != 1:\n        raise ValueError('\\nThe flat_array property of a RaggedArray must be a 1D numpy array\\n    Received value of type {typ}: {v}'.format(typ=type(flat_array), v=repr(flat_array)))\n    invalid_inds = start_indices > len(flat_array)\n    if invalid_inds.any():\n        some_invalid_vals = start_indices[invalid_inds[:10]]\n        raise ValueError('\\nElements of start_indices must be less than the length of flat_array ({m})\\n    Invalid values include: {vals}'.format(m=len(flat_array), vals=repr(some_invalid_vals)))\n\n@total_ordering\nclass _RaggedElement:\n\n    @staticmethod\n    def ragged_or_nan(a):\n        if np.isscalar(a) and np.isnan(a):\n            return a\n        else:\n            return _RaggedElement(a)\n\n    @staticmethod\n    def array_or_nan(a):\n        if np.isscalar(a) and np.isnan(a):\n            return a\n        else:\n            return a.array\n\n    def __init__(self, array):\n        self.array = array\n\n    def __hash__(self):\n        return hash(self.array.tobytes())\n\n    def __eq__(self, other):\n        if not isinstance(other, _RaggedElement):\n            return False\n        return np.array_equal(self.array, other.array)\n\n    def __lt__(self, other):\n        if not isinstance(other, _RaggedElement):\n            return NotImplemented\n        return _lexograph_lt(self.array, other.array)\n\n    def __repr__(self):\n        array_repr = repr(self.array)\n        return array_repr.replace('array', 'ragged_element')\n\n@register_extension_dtype\nclass RaggedDtype(ExtensionDtype):\n    \"\"\"\n    Pandas ExtensionDtype to represent a ragged array datatype\n\n    Methods not otherwise documented here are inherited from ExtensionDtype;\n    please see the corresponding method on that class for the docstring\n    \"\"\"\n    type = np.ndarray\n    base = np.dtype('O')\n    _subtype_re = re.compile('^ragged\\\\[(?P<subtype>\\\\w+)\\\\]$')\n    _metadata = ('_dtype',)\n\n    @property\n    def name(self):\n        return 'Ragged[{subtype}]'.format(subtype=self.subtype)\n\n    def __repr__(self):\n        return self.name\n\n    @classmethod\n    def construct_array_type(cls):\n        return RaggedArray\n\n    @classmethod\n    def construct_from_string(cls, string):\n        if not isinstance(string, str):\n            raise TypeError(\"'construct_from_string' expects a string, got %s\" % type(string))\n        string = string.lower()\n        msg = \"Cannot construct a 'RaggedDtype' from '{}'\"\n        if string.startswith('ragged'):\n            try:\n                subtype_string = cls._parse_subtype(string)\n                return RaggedDtype(dtype=subtype_string)\n            except Exception:\n                raise TypeError(msg.format(string))\n        else:\n            raise TypeError(msg.format(string))\n\n    def __init__(self, dtype=np.float64):\n        if isinstance(dtype, RaggedDtype):\n            self._dtype = dtype.subtype\n        else:\n            self._dtype = np.dtype(dtype)\n\n    @property\n    def subtype(self):\n        return self._dtype\n\n    @classmethod\n    def _parse_subtype(cls, dtype_string):\n        \"\"\"\n        Parse a datatype string to get the subtype\n\n        Parameters\n        ----------\n        dtype_string: str\n            A string like Ragged[subtype]\n\n        Returns\n        -------\n        subtype: str\n\n        Raises\n        ------\n        ValueError\n            When the subtype cannot be extracted\n        \"\"\"\n        dtype_string = dtype_string.lower()\n        match = cls._subtype_re.match(dtype_string)\n        if match:\n            subtype_string = match.groupdict()['subtype']\n        elif dtype_string == 'ragged':\n            subtype_string = 'float64'\n        else:\n            raise ValueError('Cannot parse {dtype_string}'.format(dtype_string=dtype_string))\n        return subtype_string\n\ndef missing(v):\n    return v is None or (np.isscalar(v) and np.isnan(v))\n\nclass RaggedArray(ExtensionArray):\n    \"\"\"\n    Pandas ExtensionArray to represent ragged arrays\n\n    Methods not otherwise documented here are inherited from ExtensionArray;\n    please see the corresponding method on that class for the docstring\n    \"\"\"\n\n    def __init__(self, data, dtype=None, copy=False):\n        \"\"\"\n        Construct a RaggedArray\n\n        Parameters\n        ----------\n        data: list or array or dict or RaggedArray\n            * list or 1D-array: A List or 1D array of lists or 1D arrays that\n                                should be represented by the RaggedArray\n\n            * dict: A dict containing 'start_indices' and 'flat_array' keys\n                    with numpy array values where:\n                    - flat_array:  numpy array containing concatenation\n                                   of all nested arrays to be represented\n                                   by this ragged array\n                    - start_indices: unsigned integer numpy array the same\n                                     length as the ragged array where values\n                                     represent the index into flat_array where\n                                     the corresponding ragged array element\n                                     begins\n            * RaggedArray: A RaggedArray instance to copy\n\n        dtype: RaggedDtype or np.dtype or str or None (default None)\n            Datatype to use to store underlying values from data.\n            If none (the default) then dtype will be determined using the\n            numpy.result_type function.\n        copy : bool (default False)\n            Whether to deep copy the input arrays. Only relevant when `data`\n            has type `dict` or `RaggedArray`. When data is a `list` or\n            `array`, input arrays are always copied.\n        \"\"\"\n        if isinstance(data, dict) and all((k in data for k in ['start_indices', 'flat_array'])):\n            _validate_ragged_properties(start_indices=data['start_indices'], flat_array=data['flat_array'])\n            self._start_indices = data['start_indices']\n            self._flat_array = data['flat_array']\n            dtype = self._flat_array.dtype\n            if copy:\n                self._start_indices = self._start_indices.copy()\n                self._flat_array = self._flat_array.copy()\n        elif isinstance(data, RaggedArray):\n            self._flat_array = data.flat_array\n            self._start_indices = data.start_indices\n            dtype = self._flat_array.dtype\n            if copy:\n                self._start_indices = self._start_indices.copy()\n                self._flat_array = self._flat_array.copy()\n        else:\n            index_len = len(data)\n            buffer_len = sum((len(datum) if not missing(datum) else 0 for datum in data))\n            for nbits in [8, 16, 32, 64]:\n                start_indices_dtype = 'uint' + str(nbits)\n                max_supported = np.iinfo(start_indices_dtype).max\n                if buffer_len <= max_supported:\n                    break\n            if dtype is None:\n                non_missing = [np.atleast_1d(v) for v in data if not missing(v)]\n                if non_missing:\n                    dtype = np.result_type(*non_missing)\n                else:\n                    dtype = 'float64'\n            elif isinstance(dtype, RaggedDtype):\n                dtype = dtype.subtype\n            self._start_indices = np.zeros(index_len, dtype=start_indices_dtype)\n            self._flat_array = np.zeros(buffer_len, dtype=dtype)\n            next_start_ind = 0\n            for i, array_el in enumerate(data):\n                n = len(array_el) if not missing(array_el) else 0\n                self._start_indices[i] = next_start_ind\n                if not n:\n                    continue\n                self._flat_array[next_start_ind:next_start_ind + n] = array_el\n                next_start_ind += n\n        self._dtype = RaggedDtype(dtype=dtype)\n\n    def __eq__(self, other):\n        if isinstance(other, RaggedArray):\n            if len(other) != len(self):\n                raise ValueError('\\nCannot check equality of RaggedArray values of unequal length\\n    len(ra1) == {len_ra1}\\n    len(ra2) == {len_ra2}'.format(len_ra1=len(self), len_ra2=len(other)))\n            result = _eq_ragged_ragged(self.start_indices, self.flat_array, other.start_indices, other.flat_array)\n        else:\n            if not isinstance(other, np.ndarray):\n                other_array = np.asarray(other)\n            else:\n                other_array = other\n            if other_array.ndim == 1 and other_array.dtype.kind != 'O':\n                result = _eq_ragged_scalar(self.start_indices, self.flat_array, other_array)\n            elif other_array.ndim == 1 and other_array.dtype.kind == 'O' and (len(other_array) == len(self)):\n                result = _eq_ragged_ndarray1d(self.start_indices, self.flat_array, other_array)\n            elif other_array.ndim == 2 and other_array.dtype.kind != 'O' and (other_array.shape[0] == len(self)):\n                result = _eq_ragged_ndarray2d(self.start_indices, self.flat_array, other_array)\n            else:\n                raise ValueError('\\nCannot check equality of RaggedArray of length {ra_len} with:\\n    {other}'.format(ra_len=len(self), other=repr(other)))\n        return result\n\n    def __ne__(self, other):\n        return np.logical_not(self == other)\n\n    @property\n    def flat_array(self):\n        \"\"\"\n        numpy array containing concatenation of all nested arrays\n\n        Returns\n        -------\n        np.ndarray\n        \"\"\"\n        return self._flat_array\n\n    @property\n    def start_indices(self):\n        \"\"\"\n        unsigned integer numpy array the same length as the ragged array where\n        values represent the index into flat_array where the corresponding\n        ragged array element begins\n\n        Returns\n        -------\n        np.ndarray\n        \"\"\"\n        return self._start_indices\n\n    def __len__(self):\n        return len(self._start_indices)\n\n    def __getitem__(self, item):\n        err_msg = 'Only integers, slices and integer or booleanarrays are valid indices.'\n        if isinstance(item, Integral):\n            if item < -len(self) or item >= len(self):\n                raise IndexError('{item} is out of bounds'.format(item=item))\n            else:\n                if item < 0:\n                    item += len(self)\n                slice_start = self.start_indices[item]\n                slice_end = self.start_indices[item + 1] if item + 1 <= len(self) - 1 else len(self.flat_array)\n                return self.flat_array[slice_start:slice_end] if slice_end != slice_start else np.nan\n        elif type(item) is slice:\n            data = []\n            selected_indices = np.arange(len(self))[item]\n            for selected_index in selected_indices:\n                data.append(self[selected_index])\n            return RaggedArray(data, dtype=self.flat_array.dtype)\n        elif isinstance(item, (np.ndarray, ExtensionArray, list, tuple)):\n            if isinstance(item, (np.ndarray, ExtensionArray)):\n                kind = item.dtype.kind\n            else:\n                item = pd.array(item)\n                kind = item.dtype.kind\n            if len(item) == 0:\n                return self.take([], allow_fill=False)\n            elif kind == 'b':\n                if len(item) != len(self):\n                    raise IndexError('Boolean index has wrong length: {} instead of {}'.format(len(item), len(self)))\n                isna = pd.isna(item)\n                if isna.any():\n                    if Version(pd.__version__) > Version('1.0.1'):\n                        item[isna] = False\n                    else:\n                        raise ValueError('Cannot mask with a boolean indexer containing NA values')\n                data = []\n                for i, m in enumerate(item):\n                    if m:\n                        data.append(self[i])\n                return RaggedArray(data, dtype=self.flat_array.dtype)\n            elif kind in ('i', 'u'):\n                if any(pd.isna(item)):\n                    raise ValueError('Cannot index with an integer indexer containing NA values')\n                return self.take(item, allow_fill=False)\n            else:\n                raise IndexError(err_msg)\n        else:\n            raise IndexError(err_msg)\n\n    @classmethod\n    def _from_sequence(cls, scalars, dtype=None, copy=False):\n        return RaggedArray(scalars, dtype=dtype)\n\n    @classmethod\n    def _from_factorized(cls, values, original):\n        return RaggedArray([_RaggedElement.array_or_nan(v) for v in values], dtype=original.flat_array.dtype)\n\n    def _as_ragged_element_array(self):\n        return np.array([_RaggedElement.ragged_or_nan(self[i]) for i in range(len(self))])\n\n    def _values_for_factorize(self):\n        return (self._as_ragged_element_array(), np.nan)\n\n    def _values_for_argsort(self):\n        return self._as_ragged_element_array()\n\n    def unique(self):\n        from pandas import unique\n        uniques = unique(self._as_ragged_element_array())\n        return self._from_sequence([_RaggedElement.array_or_nan(v) for v in uniques], dtype=self.dtype)\n\n    def fillna(self, value=None, method=None, limit=None):\n        from pandas.util._validators import validate_fillna_kwargs\n        from pandas.core.missing import get_fill_func\n        value, method = validate_fillna_kwargs(value, method)\n        mask = self.isna()\n        if isinstance(value, RaggedArray):\n            if len(value) != len(self):\n                raise ValueError(\"Length of 'value' does not match. Got ({})  expected {}\".format(len(value), len(self)))\n            value = value[mask]\n        if mask.any():\n            if method is not None:\n                func = get_fill_func(method)\n                new_values = func(self.astype(object), limit=limit, mask=mask)\n                new_values = self._from_sequence(new_values, dtype=self.dtype)\n            else:\n                new_values = list(self)\n                mask_indices, = np.where(mask)\n                for ind in mask_indices:\n                    new_values[ind] = value\n                new_values = self._from_sequence(new_values, dtype=self.dtype)\n        else:\n            new_values = self.copy()\n        return new_values\n\n    def shift(self, periods=1, fill_value=None):\n        if not len(self) or periods == 0:\n            return self.copy()\n        if fill_value is None:\n            fill_value = np.nan\n        empty = self._from_sequence([fill_value] * min(abs(periods), len(self)), dtype=self.dtype)\n        if periods > 0:\n            a = empty\n            b = self[:-periods]\n        else:\n            a = self[abs(periods):]\n            b = empty\n        return self._concat_same_type([a, b])\n\n    def searchsorted(self, value, side='left', sorter=None):\n        arr = self._as_ragged_element_array()\n        if isinstance(value, RaggedArray):\n            search_value = value._as_ragged_element_array()\n        else:\n            search_value = _RaggedElement(value)\n        return arr.searchsorted(search_value, side=side, sorter=sorter)\n\n    def isna(self):\n        stop_indices = np.hstack([self.start_indices[1:], [len(self.flat_array)]])\n        element_lengths = stop_indices - self.start_indices\n        return element_lengths == 0\n\n    def take(self, indices, allow_fill=False, fill_value=None):\n        if allow_fill:\n            invalid_inds = [i for i in indices if i < -1]\n            if invalid_inds:\n                raise ValueError('\\nInvalid indices for take with allow_fill True: {inds}'.format(inds=invalid_inds[:9]))\n            sequence = [self[i] if i >= 0 else fill_value for i in indices]\n        else:\n            if len(self) == 0 and len(indices) > 0:\n                raise IndexError('cannot do a non-empty take from an empty axis|out of bounds')\n            sequence = [self[i] for i in indices]\n        return RaggedArray(sequence, dtype=self.flat_array.dtype)\n\n    def copy(self, deep=False):\n        data = dict(flat_array=self.flat_array, start_indices=self.start_indices)\n        return RaggedArray(data, copy=deep)\n\n    @classmethod\n    def _concat_same_type(cls, to_concat):\n        flat_array = np.hstack([ra.flat_array for ra in to_concat])\n        offsets = np.hstack([[0], np.cumsum([len(ra.flat_array) for ra in to_concat[:-1]])]).astype('uint64')\n        start_indices = np.hstack([ra.start_indices + offset for offset, ra in zip(offsets, to_concat)])\n        return RaggedArray(dict(flat_array=flat_array, start_indices=start_indices), copy=False)\n\n    @property\n    def dtype(self):\n        return self._dtype\n\n    @property\n    def nbytes(self):\n        return self._flat_array.nbytes + self._start_indices.nbytes\n\n    def astype(self, dtype, copy=True):\n        dtype = pandas_dtype(dtype)\n        if isinstance(dtype, RaggedDtype):\n            if copy:\n                return self.copy()\n            return self\n        elif is_extension_array_dtype(dtype):\n            return dtype.construct_array_type()._from_sequence(np.asarray(self))\n        return np.array([v for v in self], dtype=dtype)\n\n    def tolist(self):\n        if self.ndim > 1:\n            return [item.tolist() for item in self]\n        else:\n            return list(self)\n\n    def __array__(self, dtype=None, copy=True):\n        dtype = np.dtype(object) if dtype is None else np.dtype(dtype)\n        if copy:\n            return np.array(self.tolist(), dtype=dtype)\n        else:\n            return np.array(self, dtype=dtype)\n\n    def duplicated(self, *args, **kwargs):\n        msg = 'duplicated is not implemented for RaggedArray'\n        raise NotImplementedError(msg)\n\n@jit(nopython=True, nogil=True)\ndef _eq_ragged_ragged(start_indices1, flat_array1, start_indices2, flat_array2):\n    \"\"\"\n    Compare elements of two ragged arrays of the same length\n\n    Parameters\n    ----------\n    start_indices1: ndarray\n        start indices of a RaggedArray 1\n    flat_array1: ndarray\n        flat_array property of a RaggedArray 1\n    start_indices2: ndarray\n        start indices of a RaggedArray 2\n    flat_array2: ndarray\n        flat_array property of a RaggedArray 2\n\n    Returns\n    -------\n    mask: ndarray\n        1D bool array of same length as inputs with elements True when\n        corresponding elements are equal, False otherwise\n    \"\"\"\n    n = len(start_indices1)\n    m1 = len(flat_array1)\n    m2 = len(flat_array2)\n    result = np.zeros(n, dtype=np.bool_)\n    for i in range(n):\n        start_index1 = start_indices1[i]\n        stop_index1 = start_indices1[i + 1] if i < n - 1 else m1\n        len_1 = stop_index1 - start_index1\n        start_index2 = start_indices2[i]\n        stop_index2 = start_indices2[i + 1] if i < n - 1 else m2\n        len_2 = stop_index2 - start_index2\n        if len_1 != len_2:\n            el_equal = False\n        else:\n            el_equal = True\n            for flat_index1, flat_index2 in zip(range(start_index1, stop_index1), range(start_index2, stop_index2)):\n                el_1 = flat_array1[flat_index1]\n                el_2 = flat_array2[flat_index2]\n                el_equal &= el_1 == el_2\n        result[i] = el_equal\n    return result\n\n@jit(nopython=True, nogil=True)\ndef _eq_ragged_scalar(start_indices, flat_array, val):\n    \"\"\"\n    Compare elements of a RaggedArray with a scalar array\n\n    Parameters\n    ----------\n    start_indices: ndarray\n        start indices of a RaggedArray\n    flat_array: ndarray\n        flat_array property of a RaggedArray\n    val: ndarray\n\n    Returns\n    -------\n    mask: ndarray\n        1D bool array of same length as inputs with elements True when\n        ragged element equals scalar val, False otherwise.\n    \"\"\"\n    n = len(start_indices)\n    m = len(flat_array)\n    cols = len(val)\n    result = np.zeros(n, dtype=np.bool_)\n    for i in range(n):\n        start_index = start_indices[i]\n        stop_index = start_indices[i + 1] if i < n - 1 else m\n        if stop_index - start_index != cols:\n            el_equal = False\n        else:\n            el_equal = True\n            for val_index, flat_index in enumerate(range(start_index, stop_index)):\n                el_equal &= flat_array[flat_index] == val[val_index]\n        result[i] = el_equal\n    return result\n\ndef _eq_ragged_ndarray1d(start_indices, flat_array, a):\n    \"\"\"\n    Compare a RaggedArray with a 1D numpy object array of the same length\n\n    Parameters\n    ----------\n    start_indices: ndarray\n        start indices of a RaggedArray\n    flat_array: ndarray\n        flat_array property of a RaggedArray\n    a: ndarray\n        1D numpy array of same length as ra\n\n    Returns\n    -------\n    mask: ndarray\n        1D bool array of same length as input with elements True when\n        corresponding elements are equal, False otherwise\n\n    Notes\n    -----\n    This function is not numba accelerated because it, by design, inputs\n    a numpy object array\n    \"\"\"\n    n = len(start_indices)\n    m = len(flat_array)\n    result = np.zeros(n, dtype=np.bool_)\n    for i in range(n):\n        start_index = start_indices[i]\n        stop_index = start_indices[i + 1] if i < n - 1 else m\n        a_val = a[i]\n        if a_val is None or (np.isscalar(a_val) and np.isnan(a_val)) or len(a_val) == 0:\n            result[i] = start_index == stop_index\n        else:\n            result[i] = np.array_equal(flat_array[start_index:stop_index], a_val)\n    return result\n\n@jit(nopython=True, nogil=True)\ndef _eq_ragged_ndarray2d(start_indices, flat_array, a):\n    \"\"\"\n    Compare a RaggedArray with rows of a 2D numpy object array\n\n    Parameters\n    ----------\n    start_indices: ndarray\n        start indices of a RaggedArray\n    flat_array: ndarray\n        flat_array property of a RaggedArray\n    a: ndarray\n        A 2D numpy array where the length of the first dimension matches the\n        length of the RaggedArray\n\n    Returns\n    -------\n    mask: ndarray\n        1D bool array of same length as input RaggedArray with elements True\n        when corresponding elements of ra equal corresponding row of `a`\n    \"\"\"\n    n = len(start_indices)\n    m = len(flat_array)\n    cols = a.shape[1]\n    result = np.zeros(n, dtype=np.bool_)\n    for row in range(n):\n        start_index = start_indices[row]\n        stop_index = start_indices[row + 1] if row < n - 1 else m\n        if stop_index - start_index != cols:\n            el_equal = False\n        else:\n            el_equal = True\n            for col, flat_index in enumerate(range(start_index, stop_index)):\n                el_equal &= flat_array[flat_index] == a[row, col]\n        result[row] = el_equal\n    return result\n\n@jit(nopython=True, nogil=True)\ndef _lexograph_lt(a1, a2):\n    \"\"\"\n    Compare two 1D numpy arrays lexographically\n    Parameters\n    ----------\n    a1: ndarray\n        1D numpy array\n    a2: ndarray\n        1D numpy array\n\n    Returns\n    -------\n    comparison:\n        True if a1 < a2, False otherwise\n    \"\"\"\n    for e1, e2 in zip(a1, a2):\n        if e1 < e2:\n            return True\n        elif e1 > e2:\n            return False\n    return len(a1) < len(a2)\n\ndef ragged_array_non_empty(dtype):\n    return RaggedArray([[1], [1, 2]], dtype=dtype)\nif make_array_nonempty:\n    make_array_nonempty.register(RaggedDtype)(ragged_array_non_empty)",
    "datashader/datashape/util/__init__.py": "from itertools import chain\nimport operator\nfrom .. import parser\nfrom .. import type_symbol_table\nfrom ..validation import validate\nfrom .. import coretypes\n__all__ = ('dshape', 'dshapes', 'has_var_dim', 'has_ellipsis', 'cat_dshapes')\nsubclasses = operator.methodcaller('__subclasses__')\n\ndef dshapes(*args):\n    \"\"\"\n    Parse a bunch of datashapes all at once.\n\n    >>> a, b = dshapes('3 * int32', '2 * var * float64')\n    \"\"\"\n    return [dshape(arg) for arg in args]\n\ndef cat_dshapes(dslist):\n    \"\"\"\n    Concatenates a list of dshapes together along\n    the first axis. Raises an error if there is\n    a mismatch along another axis or the measures\n    are different.\n\n    Requires that the leading dimension be a known\n    size for all data shapes.\n    TODO: Relax this restriction to support\n          streaming dimensions.\n\n    >>> cat_dshapes(dshapes('10 * int32', '5 * int32'))\n    dshape(\"15 * int32\")\n    \"\"\"\n    if len(dslist) == 0:\n        raise ValueError('Cannot concatenate an empty list of dshapes')\n    elif len(dslist) == 1:\n        return dslist[0]\n    outer_dim_size = operator.index(dslist[0][0])\n    inner_ds = dslist[0][1:]\n    for ds in dslist[1:]:\n        outer_dim_size += operator.index(ds[0])\n        if ds[1:] != inner_ds:\n            raise ValueError('The datashapes to concatenate much all match after the first dimension (%s vs %s)' % (inner_ds, ds[1:]))\n    return coretypes.DataShape(*[coretypes.Fixed(outer_dim_size)] + list(inner_ds))\n\ndef collect(pred, expr):\n    \"\"\" Collect terms in expression that match predicate\n\n    >>> from datashader.datashape import Unit, dshape\n    >>> predicate = lambda term: isinstance(term, Unit)\n    >>> dshape = dshape('var * {value: int64, loc: 2 * int32}')\n    >>> sorted(set(collect(predicate, dshape)), key=str)\n    [Fixed(val=2), ctype(\"int32\"), ctype(\"int64\"), Var()]\n    >>> from datashader.datashape import var, int64\n    >>> sorted(set(collect(predicate, [var, int64])), key=str)\n    [ctype(\"int64\"), Var()]\n    \"\"\"\n    if pred(expr):\n        return [expr]\n    if isinstance(expr, coretypes.Record):\n        return chain.from_iterable((collect(pred, typ) for typ in expr.types))\n    if isinstance(expr, coretypes.Mono):\n        return chain.from_iterable((collect(pred, typ) for typ in expr.parameters))\n    if isinstance(expr, (list, tuple)):\n        return chain.from_iterable((collect(pred, item) for item in expr))\n\ndef has_var_dim(ds):\n    \"\"\"Returns True if datashape has a variable dimension\n\n    Note currently treats variable length string as scalars.\n\n    >>> has_var_dim(dshape('2 * int32'))\n    False\n    >>> has_var_dim(dshape('var * 2 * int32'))\n    True\n    \"\"\"\n    return has((coretypes.Ellipsis, coretypes.Var), ds)\n\ndef has(typ, ds):\n    if isinstance(ds, typ):\n        return True\n    if isinstance(ds, coretypes.Record):\n        return any((has(typ, t) for t in ds.types))\n    if isinstance(ds, coretypes.Mono):\n        return any((has(typ, p) for p in ds.parameters))\n    if isinstance(ds, (list, tuple)):\n        return any((has(typ, item) for item in ds))\n    return False\n\ndef has_ellipsis(ds):\n    \"\"\"Returns True if the datashape has an ellipsis\n\n    >>> has_ellipsis(dshape('2 * int'))\n    False\n    >>> has_ellipsis(dshape('... * int'))\n    True\n    \"\"\"\n    return has(coretypes.Ellipsis, ds)"
  }
}