{
  "dir_path": "/app/motmetrics",
  "package_name": "motmetrics",
  "sample_name": "motmetrics-test_utils",
  "src_dir": "motmetrics/",
  "test_dir": "motmetrics/tests/",
  "test_file": "modified_testcases/test_utils.py",
  "test_code": "# py-motmetrics - Metrics for multiple object tracker (MOT) benchmarking.\n# https://github.com/cheind/py-motmetrics/\n#\n# MIT License\n# Copyright (c) 2017-2020 Christoph Heindl, Jack Valmadre and others.\n# See LICENSE file for terms.\n\n\"\"\"Tests accumulation of events using utility functions.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\n\nimport numpy as np\nimport pandas as pd\n\nimport motmetrics as mm\n\n\ndef test_annotations_xor_predictions_present():\n    \"\"\"Tests frames that contain only annotations or predictions.\"\"\"\n    _ = None\n    anno_tracks = {\n        1: [0, 2, 4, 6, _, _, _],\n        2: [_, _, 0, 2, 4, _, _],\n    }\n    pred_tracks = {\n        1: [_, _, 3, 5, 7, 7, 7],\n    }\n    anno = _tracks_to_dataframe(anno_tracks)\n    pred = _tracks_to_dataframe(pred_tracks)\n    acc = mm.utils.compare_to_groundtruth(anno, pred, 'euclidean', distfields=['Position'], distth=2)\n    mh = mm.metrics.create()\n    metrics = mh.compute(acc, return_dataframe=False, metrics=[\n        'num_objects', 'num_predictions', 'num_unique_objects',\n    ])\n    np.testing.assert_equal(metrics['num_objects'], 7)\n    np.testing.assert_equal(metrics['num_predictions'], 5)\n    np.testing.assert_equal(metrics['num_unique_objects'], 2)\n\n\ndef _tracks_to_dataframe(tracks):\n    rows = []\n    for track_id, track in tracks.items():\n        for frame_id, position in zip(itertools.count(1), track):\n            if position is None:\n                continue\n            rows.append({\n                'FrameId': frame_id,\n                'Id': track_id,\n                'Position': position,\n            })\n    return pd.DataFrame(rows).set_index(['FrameId', 'Id'])\n",
  "GT_file_code": {
    "motmetrics/metrics.py": "# py-motmetrics - Metrics for multiple object tracker (MOT) benchmarking.\n# https://github.com/cheind/py-motmetrics/\n#\n# MIT License\n# Copyright (c) 2017-2020 Christoph Heindl, Jack Valmadre and others.\n# See LICENSE file for terms.\n\n\"\"\"Obtain metrics from event logs.\"\"\"\n\n# pylint: disable=redefined-outer-name\n\nfrom __future__ import absolute_import, division, print_function\n\nimport inspect\nimport logging\nimport time\nfrom collections import OrderedDict\n\nimport numpy as np\nimport pandas as pd\n\nfrom motmetrics import math_util\nfrom motmetrics.lap import linear_sum_assignment\nfrom motmetrics.mot import MOTAccumulator\n\ntry:\n    _getargspec = inspect.getfullargspec\nexcept AttributeError:\n    _getargspec = inspect.getargspec\n\n\nclass MetricsHost:\n    \"\"\"Keeps track of metrics and intra metric dependencies.\"\"\"\n\n    def __init__(self):\n        self.metrics = OrderedDict()\n\n    def register(\n        self,\n        fnc,\n        deps=\"auto\",\n        name=None,\n        helpstr=None,\n        formatter=None,\n        fnc_m=None,\n        deps_m=\"auto\",\n    ):\n        \"\"\"Register a new metric.\n\n        Params\n        ------\n        fnc : Function\n            Function that computes the metric to be registered. The number of arguments\n            is 1 + N, where N is the number of dependencies of the metric to be registered.\n            The order of the argument passed is `df, result_dep1, result_dep2, ...`.\n\n        Kwargs\n        ------\n        deps : string, list of strings or None, optional\n            The dependencies of this metric. Each dependency is evaluated and the result\n            is passed as argument to `fnc` as described above. If None is specified, the\n            function does not have any dependencies. If a list of strings is given, dependencies\n            for these metric strings are registered. If 'auto' is passed, the dependencies\n            are deduced from argument inspection of the method. For this to work the argument\n            names have to be equal to the intended dependencies.\n        name : string or None, optional\n            Name identifier of this metric. If None is passed the name is deduced from\n            function inspection.\n        helpstr : string or None, optional\n            A description of what the metric computes. If no help message is given it\n            is deduced from the docstring of the function.\n        formatter: Format object, optional\n            An optional default formatter when rendering metric results as string. I.e to\n            render the result `0.35` as `35%` one would pass `{:.2%}.format`\n        fnc_m : Function or None, optional\n            Function that merges metric results. The number of arguments\n            is 1 + N, where N is the number of dependencies of the metric to be registered.\n            The order of the argument passed is `df, result_dep1, result_dep2, ...`.\n        \"\"\"\n\n        assert fnc is not None, \"No function given for metric {}\".format(name)\n\n        if deps is None:\n            deps = []\n        elif deps == \"auto\":\n            if _getargspec(fnc).defaults is not None:\n                k = -len(_getargspec(fnc).defaults)\n            else:\n                k = len(_getargspec(fnc).args)\n            deps = _getargspec(fnc).args[1:k]  # assumes dataframe as first argument\n\n        if name is None:\n            name = (\n                fnc.__name__\n            )  # Relies on meaningful function names, i.e don't use for lambdas\n\n        if helpstr is None:\n            helpstr = inspect.getdoc(fnc) if inspect.getdoc(fnc) else \"No description.\"\n            helpstr = \" \".join(helpstr.split())\n        if fnc_m is None and name + \"_m\" in globals():\n            fnc_m = globals()[name + \"_m\"]\n        if fnc_m is not None:\n            if deps_m is None:\n                deps_m = []\n            elif deps_m == \"auto\":\n                if _getargspec(fnc_m).defaults is not None:\n                    k = -len(_getargspec(fnc_m).defaults)\n                else:\n                    k = len(_getargspec(fnc_m).args)\n                deps_m = _getargspec(fnc_m).args[\n                    1:k\n                ]  # assumes dataframe as first argument\n        else:\n            deps_m = None\n\n        self.metrics[name] = {\n            \"name\": name,\n            \"fnc\": fnc,\n            \"fnc_m\": fnc_m,\n            \"deps\": deps,\n            \"deps_m\": deps_m,\n            \"help\": helpstr,\n            \"formatter\": formatter,\n        }\n\n    @property\n    def names(self):\n        \"\"\"Returns the name identifiers of all registered metrics.\"\"\"\n        return [v[\"name\"] for v in self.metrics.values()]\n\n    @property\n    def formatters(self):\n        \"\"\"Returns the formatters for all metrics that have associated formatters.\"\"\"\n        return {\n            k: v[\"formatter\"]\n            for k, v in self.metrics.items()\n            if v[\"formatter\"] is not None\n        }\n\n    def list_metrics(self, include_deps=False):\n        \"\"\"Returns a dataframe containing names, descriptions and optionally dependencies for each metric.\"\"\"\n        cols = [\"Name\", \"Description\", \"Dependencies\"]\n        if include_deps:\n            data = [(m[\"name\"], m[\"help\"], m[\"deps\"]) for m in self.metrics.values()]\n        else:\n            data = [(m[\"name\"], m[\"help\"]) for m in self.metrics.values()]\n            cols = cols[:-1]\n\n        return pd.DataFrame(data, columns=cols)\n\n    def list_metrics_markdown(self, include_deps=False):\n        \"\"\"Returns a markdown ready version of `list_metrics`.\"\"\"\n        df = self.list_metrics(include_deps=include_deps)\n        fmt = [\":---\" for i in range(len(df.columns))]\n        df_fmt = pd.DataFrame([fmt], columns=df.columns)\n        df_formatted = pd.concat([df_fmt, df])\n        return df_formatted.to_csv(sep=\"|\", index=False)\n\n    def compute(\n        self,\n        df,\n        ana=None,\n        metrics=None,\n        return_dataframe=True,\n        return_cached=False,\n        name=None,\n    ):\n        \"\"\"Compute metrics on the dataframe / accumulator.\n\n        Params\n        ------\n        df : MOTAccumulator or pandas.DataFrame\n            The dataframe to compute the metrics on\n\n        Kwargs\n        ------\n        ana: dict or None, optional\n            To cache results for fast computation.\n        metrics : string, list of string or None, optional\n            The identifiers of the metrics to be computed. This method will only\n            compute the minimal set of necessary metrics to fullfill the request.\n            If None is passed all registered metrics are computed.\n        return_dataframe : bool, optional\n            Return the result as pandas.DataFrame (default) or dict.\n        return_cached : bool, optional\n           If true all intermediate metrics required to compute the desired metrics are returned as well.\n        name : string, optional\n            When returning a pandas.DataFrame this is the index of the row containing\n            the computed metric values.\n        \"\"\"\n\n        if isinstance(df, MOTAccumulator):\n            df = df.events\n\n        if metrics is None:\n            metrics = motchallenge_metrics\n        elif isinstance(metrics, str):\n            metrics = [metrics]\n\n        df_map = events_to_df_map(df)\n\n        cache = {}\n        options = {\"ana\": ana}\n        for mname in metrics:\n            cache[mname] = self._compute(\n                df_map, mname, cache, options, parent=\"summarize\"\n            )\n\n        if name is None:\n            name = 0\n\n        if return_cached:\n            data = cache\n        else:\n            data = OrderedDict([(k, cache[k]) for k in metrics])\n\n        ret = pd.DataFrame(data, index=[name]) if return_dataframe else data\n        return ret\n\n    def compute_overall(\n        self,\n        partials,\n        metrics=None,\n        return_dataframe=True,\n        return_cached=False,\n        name=None,\n    ):\n        \"\"\"Compute overall metrics based on multiple results.\n\n        Params\n        ------\n        partials : list of metric results to combine overall\n\n        Kwargs\n        ------\n        metrics : string, list of string or None, optional\n            The identifiers of the metrics to be computed. This method will only\n            compute the minimal set of necessary metrics to fullfill the request.\n            If None is passed all registered metrics are computed.\n        return_dataframe : bool, optional\n            Return the result as pandas.DataFrame (default) or dict.\n        return_cached : bool, optional\n           If true all intermediate metrics required to compute the desired metrics are returned as well.\n        name : string, optional\n            When returning a pandas.DataFrame this is the index of the row containing\n            the computed metric values.\n\n        Returns\n        -------\n        df : pandas.DataFrame\n            A datafrom containing the metrics in columns and names in rows.\n        \"\"\"\n        if metrics is None:\n            metrics = motchallenge_metrics\n        elif isinstance(metrics, str):\n            metrics = [metrics]\n        cache = {}\n\n        for mname in metrics:\n            cache[mname] = self._compute_overall(\n                partials, mname, cache, parent=\"summarize\"\n            )\n\n        if name is None:\n            name = 0\n        if return_cached:\n            data = cache\n        else:\n            data = OrderedDict([(k, cache[k]) for k in metrics])\n        return pd.DataFrame(data, index=[name]) if return_dataframe else data\n\n    def compute_many(\n        self, dfs, anas=None, metrics=None, names=None, generate_overall=False\n    ):\n        \"\"\"Compute metrics on multiple dataframe / accumulators.\n\n        Params\n        ------\n        dfs : list of MOTAccumulator or list of pandas.DataFrame\n            The data to compute metrics on.\n\n        Kwargs\n        ------\n        anas: dict or None, optional\n            To cache results for fast computation.\n        metrics : string, list of string or None, optional\n            The identifiers of the metrics to be computed. This method will only\n            compute the minimal set of necessary metrics to fullfill the request.\n            If None is passed all registered metrics are computed.\n        names : list of string, optional\n            The names of individual rows in the resulting dataframe.\n        generate_overall : boolean, optional\n            If true resulting dataframe will contain a summary row that is computed\n            using the same metrics over an accumulator that is the concatentation of\n            all input containers. In creating this temporary accumulator, care is taken\n            to offset frame indices avoid object id collisions.\n\n        Returns\n        -------\n        df : pandas.DataFrame\n            A datafrom containing the metrics in columns and names in rows.\n        \"\"\"\n        if metrics is None:\n            metrics = motchallenge_metrics\n        elif isinstance(metrics, str):\n            metrics = [metrics]\n\n        assert names is None or len(names) == len(dfs)\n        st = time.time()\n        if names is None:\n            names = list(range(len(dfs)))\n        if anas is None:\n            anas = [None] * len(dfs)\n        partials = [\n            self.compute(\n                acc,\n                ana=analysis,\n                metrics=metrics,\n                name=name,\n                return_cached=True,\n                return_dataframe=False,\n            )\n            for acc, analysis, name in zip(dfs, anas, names)\n        ]\n        logging.info(\"partials: %.3f seconds.\", time.time() - st)\n        details = partials\n        partials = [\n            pd.DataFrame(OrderedDict([(k, i[k]) for k in metrics]), index=[name])\n            for i, name in zip(partials, names)\n        ]\n        if generate_overall:\n            names = \"OVERALL\"\n            # merged, infomap = MOTAccumulator.merge_event_dataframes(dfs, return_mappings = True)\n            # dfs = merged\n            # anas = MOTAccumulator.merge_analysis(anas, infomap)\n            # partials.append(self.compute(dfs, ana=anas, metrics=metrics, name=names)[0])\n            partials.append(self.compute_overall(details, metrics=metrics, name=names))\n        logging.info(\"mergeOverall: %.3f seconds.\", time.time() - st)\n        return pd.concat(partials)\n\n    def _compute(self, df_map, name, cache, options, parent=None):\n        \"\"\"Compute metric and resolve dependencies.\"\"\"\n        assert name in self.metrics, \"Cannot find metric {} required by {}.\".format(\n            name, parent\n        )\n        already = cache.get(name, None)\n        if already is not None:\n            return already\n        minfo = self.metrics[name]\n        vals = []\n        for depname in minfo[\"deps\"]:\n            v = cache.get(depname, None)\n            if v is None:\n                v = cache[depname] = self._compute(\n                    df_map, depname, cache, options, parent=name\n                )\n            vals.append(v)\n        if _getargspec(minfo[\"fnc\"]).defaults is None:\n            return minfo[\"fnc\"](df_map, *vals)\n        else:\n            return minfo[\"fnc\"](df_map, *vals, **options)\n\n    def _compute_overall(self, partials, name, cache, parent=None):\n        assert name in self.metrics, \"Cannot find metric {} required by {}.\".format(\n            name, parent\n        )\n        already = cache.get(name, None)\n        if already is not None:\n            return already\n        minfo = self.metrics[name]\n        vals = []\n        for depname in minfo[\"deps_m\"]:\n            v = cache.get(depname, None)\n            if v is None:\n                v = cache[depname] = self._compute_overall(\n                    partials, depname, cache, parent=name\n                )\n            vals.append(v)\n        assert minfo[\"fnc_m\"] is not None, \"merge function for metric %s is None\" % name\n        return minfo[\"fnc_m\"](partials, *vals)\n\n\nsimple_add_func = []\n\n\ndef num_frames(df):\n    \"\"\"Total number of frames.\"\"\"\n    return df.full.index.get_level_values(0).unique().shape[0]\n\n\nsimple_add_func.append(num_frames)\n\n\ndef obj_frequencies(df):\n    \"\"\"Total number of occurrences of individual objects over all frames.\"\"\"\n    return df.noraw.OId.value_counts()\n\n\ndef pred_frequencies(df):\n    \"\"\"Total number of occurrences of individual predictions over all frames.\"\"\"\n    return df.noraw.HId.value_counts()\n\n\ndef num_unique_objects(df, obj_frequencies):\n    \"\"\"Total number of unique object ids encountered.\"\"\"\n    del df  # unused\n    return len(obj_frequencies)\n\n\nsimple_add_func.append(num_unique_objects)\n\n\ndef num_matches(df):\n    \"\"\"Total number matches.\"\"\"\n    return df.noraw.Type.isin([\"MATCH\"]).sum()\n\n\nsimple_add_func.append(num_matches)\n\n\ndef num_switches(df):\n    \"\"\"Total number of track switches.\"\"\"\n    return df.noraw.Type.isin([\"SWITCH\"]).sum()\n\n\nsimple_add_func.append(num_switches)\n\n\ndef num_transfer(df):\n    \"\"\"Total number of track transfer.\"\"\"\n    return df.extra.Type.isin([\"TRANSFER\"]).sum()\n\n\nsimple_add_func.append(num_transfer)\n\n\ndef num_ascend(df):\n    \"\"\"Total number of track ascend.\"\"\"\n    return df.extra.Type.isin([\"ASCEND\"]).sum()\n\n\nsimple_add_func.append(num_ascend)\n\n\ndef num_migrate(df):\n    \"\"\"Total number of track migrate.\"\"\"\n    return df.extra.Type.isin([\"MIGRATE\"]).sum()\n\n\nsimple_add_func.append(num_migrate)\n\n\ndef num_false_positives(df):\n    \"\"\"Total number of false positives (false-alarms).\"\"\"\n    return df.noraw.Type.isin([\"FP\"]).sum()\n\n\nsimple_add_func.append(num_false_positives)\n\n\ndef num_misses(df):\n    \"\"\"Total number of misses.\"\"\"\n    return df.noraw.Type.isin([\"MISS\"]).sum()\n\n\nsimple_add_func.append(num_misses)\n\n\ndef num_detections(df, num_matches, num_switches):\n    \"\"\"Total number of detected objects including matches and switches.\"\"\"\n    del df  # unused\n    return num_matches + num_switches\n\n\nsimple_add_func.append(num_detections)\n\n\ndef num_objects(df, obj_frequencies):\n    \"\"\"Total number of unique object appearances over all frames.\"\"\"\n    del df  # unused\n    return obj_frequencies.sum()\n\n\nsimple_add_func.append(num_objects)\n\n\ndef num_predictions(df, pred_frequencies):\n    \"\"\"Total number of unique prediction appearances over all frames.\"\"\"\n    del df  # unused\n    return pred_frequencies.sum()\n\n\nsimple_add_func.append(num_predictions)\n\n\ndef num_gt_ids(df):\n    \"\"\"Number of unique gt ids.\"\"\"\n    return df.full[\"OId\"].dropna().unique().shape[0]\n\n\nsimple_add_func.append(num_gt_ids)\n\n\ndef num_dt_ids(df):\n    \"\"\"Number of unique dt ids.\"\"\"\n    return df.full[\"HId\"].dropna().unique().shape[0]\n\n\nsimple_add_func.append(num_dt_ids)\n\n\ndef track_ratios(df, obj_frequencies):\n    \"\"\"Ratio of assigned to total appearance count per unique object id.\"\"\"\n    tracked = df.noraw[df.noraw.Type != \"MISS\"][\"OId\"].value_counts()\n    return tracked.div(obj_frequencies).fillna(0.0)\n\n\ndef mostly_tracked(df, track_ratios):\n    \"\"\"Number of objects tracked for at least 80 percent of lifespan.\"\"\"\n    del df  # unused\n    return track_ratios[track_ratios >= 0.8].count()\n\n\nsimple_add_func.append(mostly_tracked)\n\n\ndef partially_tracked(df, track_ratios):\n    \"\"\"Number of objects tracked between 20 and 80 percent of lifespan.\"\"\"\n    del df  # unused\n    return track_ratios[(track_ratios >= 0.2) & (track_ratios < 0.8)].count()\n\n\nsimple_add_func.append(partially_tracked)\n\n\ndef mostly_lost(df, track_ratios):\n    \"\"\"Number of objects tracked less than 20 percent of lifespan.\"\"\"\n    del df  # unused\n    return track_ratios[track_ratios < 0.2].count()\n\n\nsimple_add_func.append(mostly_lost)\n\n\ndef num_fragmentations(df, obj_frequencies):\n    \"\"\"Total number of switches from tracked to not tracked.\"\"\"\n    fra = 0\n    for o in obj_frequencies.index:\n        # Find first and last time object was not missed (track span). Then count\n        # the number switches from NOT MISS to MISS state.\n        dfo = df.noraw[df.noraw.OId == o]\n        notmiss = dfo[dfo.Type != \"MISS\"]\n        if len(notmiss) == 0:\n            continue\n        first = notmiss.index[0]\n        last = notmiss.index[-1]\n        diffs = dfo.loc[first:last].Type.apply(lambda x: 1 if x == \"MISS\" else 0).diff()\n        fra += diffs[diffs == 1].count()\n    return fra\n\n\nsimple_add_func.append(num_fragmentations)\n\n\ndef motp(df, num_detections):\n    \"\"\"Multiple object tracker precision.\"\"\"\n    return math_util.quiet_divide(df.noraw[\"D\"].sum(), num_detections)\n\n\ndef motp_m(partials, num_detections):\n    res = 0\n    for v in partials:\n        res += v[\"motp\"] * v[\"num_detections\"]\n    return math_util.quiet_divide(res, num_detections)\n\n\ndef mota(df, num_misses, num_switches, num_false_positives, num_objects):\n    \"\"\"Multiple object tracker accuracy.\"\"\"\n    del df  # unused\n    return 1.0 - math_util.quiet_divide(\n        num_misses + num_switches + num_false_positives, num_objects\n    )\n\n\ndef mota_m(partials, num_misses, num_switches, num_false_positives, num_objects):\n    del partials  # unused\n    return 1.0 - math_util.quiet_divide(\n        num_misses + num_switches + num_false_positives, num_objects\n    )\n\n\ndef precision(df, num_detections, num_false_positives):\n    \"\"\"Number of detected objects over sum of detected and false positives.\"\"\"\n    del df  # unused\n    return math_util.quiet_divide(num_detections, num_false_positives + num_detections)\n\n\ndef precision_m(partials, num_detections, num_false_positives):\n    del partials  # unused\n    return math_util.quiet_divide(num_detections, num_false_positives + num_detections)\n\n\ndef recall(df, num_detections, num_objects):\n    \"\"\"Number of detections over number of objects.\"\"\"\n    del df  # unused\n    return math_util.quiet_divide(num_detections, num_objects)\n\n\ndef recall_m(partials, num_detections, num_objects):\n    del partials  # unused\n    return math_util.quiet_divide(num_detections, num_objects)\n\n\ndef deta_alpha(df, num_detections, num_objects, num_false_positives):\n    r\"\"\"DeTA under specific threshold $\\alpha$\n    Source: https://jonathonluiten.medium.com/how-to-evaluate-tracking-with-the-hota-metrics-754036d183e1\n    \"\"\"\n    del df  # unused\n    return math_util.quiet_divide(num_detections, max(1, num_objects + num_false_positives))\n\n\ndef deta_alpha_m(partials):\n    res = 0\n    for v in partials:\n        res += v[\"deta_alpha\"]\n    return math_util.quiet_divide(res, len(partials))\n\n\ndef assa_alpha(df, num_detections, num_gt_ids, num_dt_ids):\n    r\"\"\"AssA under specific threshold $\\alpha$\n    Source: https://github.com/JonathonLuiten/TrackEval/blob/12c8791b303e0a0b50f753af204249e622d0281a/trackeval/metrics/hota.py#L107-L108\n    \"\"\"\n    max_gt_ids = int(df.noraw.OId.max())\n    max_dt_ids = int(df.noraw.HId.max())\n\n    match_count_array = np.zeros((max_gt_ids, max_dt_ids))\n    gt_id_counts = np.zeros((max_gt_ids, 1))\n    tracker_id_counts = np.zeros((1, max_dt_ids))\n    for idx in range(len(df.noraw)):\n        oid, hid = df.noraw.iloc[idx, 1], df.noraw.iloc[idx, 2]\n        if df.noraw.iloc[idx, 0] in [\"SWITCH\", \"MATCH\"]:\n            match_count_array[int(oid) - 1, int(hid) - 1] += 1\n        if oid == oid:  # check non nan\n            gt_id_counts[int(oid) - 1] += 1\n        if hid == hid:\n            tracker_id_counts[0, int(hid) - 1] += 1\n\n    ass_a = match_count_array / np.maximum(1, gt_id_counts + tracker_id_counts - match_count_array)\n    return math_util.quiet_divide((ass_a * match_count_array).sum(), max(1, num_detections))\n\n\ndef assa_alpha_m(partials):\n    res = 0\n    for v in partials:\n        res += v[\"assa_alpha\"]\n    return math_util.quiet_divide(res, len(partials))\n\n\ndef hota_alpha(df, deta_alpha, assa_alpha):\n    r\"\"\"HOTA under specific threshold $\\alpha$\"\"\"\n    del df\n    return (deta_alpha * assa_alpha) ** 0.5\n\n\ndef hota_alpha_m(partials):\n    res = 0\n    for v in partials:\n        res += v[\"hota_alpha\"]\n    return math_util.quiet_divide(res, len(partials))\n\n\nclass DataFrameMap:  # pylint: disable=too-few-public-methods\n    def __init__(self, full, raw, noraw, extra):\n        self.full = full\n        self.raw = raw\n        self.noraw = noraw\n        self.extra = extra\n\n\ndef events_to_df_map(df):\n    raw = df[df.Type == \"RAW\"]\n    noraw = df[\n        (df.Type != \"RAW\")\n        & (df.Type != \"ASCEND\")\n        & (df.Type != \"TRANSFER\")\n        & (df.Type != \"MIGRATE\")\n    ]\n    extra = df[df.Type != \"RAW\"]\n    df_map = DataFrameMap(full=df, raw=raw, noraw=noraw, extra=extra)\n    return df_map\n\n\ndef extract_counts_from_df_map(df):\n    \"\"\"\n    Returns:\n        Tuple (ocs, hcs, tps).\n        ocs: Dict from object id to count.\n        hcs: Dict from hypothesis id to count.\n        tps: Dict from (object id, hypothesis id) to true-positive count.\n        The ids are arbitrary, they might NOT be consecutive integers from 0.\n    \"\"\"\n    oids = df.full[\"OId\"].dropna().unique()\n    hids = df.full[\"HId\"].dropna().unique()\n\n    flat = df.raw.reset_index()\n    # Exclude events that do not belong to either set.\n    flat = flat[flat[\"OId\"].isin(oids) | flat[\"HId\"].isin(hids)]\n    # Count number of frames where each (non-empty) OId and HId appears.\n    ocs = flat.set_index(\"OId\")[\"FrameId\"].groupby(\"OId\").nunique().to_dict()\n    hcs = flat.set_index(\"HId\")[\"FrameId\"].groupby(\"HId\").nunique().to_dict()\n    # Select three columns of interest and index by ('OId', 'HId').\n    dists = flat[[\"OId\", \"HId\", \"D\"]].set_index([\"OId\", \"HId\"]).dropna()\n    # Count events with non-empty distance for each pair.\n    tps = dists.groupby([\"OId\", \"HId\"])[\"D\"].count().to_dict()\n    return ocs, hcs, tps\n\n\ndef id_global_assignment(df, ana=None):\n    \"\"\"ID measures: Global min-cost assignment for ID measures.\"\"\"\n    # pylint: disable=too-many-locals\n    del ana  # unused\n    ocs, hcs, tps = extract_counts_from_df_map(df)\n    oids = sorted(ocs.keys())\n    hids = sorted(hcs.keys())\n    oids_idx = dict((o, i) for i, o in enumerate(oids))\n    hids_idx = dict((h, i) for i, h in enumerate(hids))\n    no = len(ocs)\n    nh = len(hcs)\n\n    fpmatrix = np.full((no + nh, no + nh), 0.0)\n    fnmatrix = np.full((no + nh, no + nh), 0.0)\n    fpmatrix[no:, :nh] = np.nan\n    fnmatrix[:no, nh:] = np.nan\n\n    for oid, oc in ocs.items():\n        r = oids_idx[oid]\n        fnmatrix[r, :nh] = oc\n        fnmatrix[r, nh + r] = oc\n\n    for hid, hc in hcs.items():\n        c = hids_idx[hid]\n        fpmatrix[:no, c] = hc\n        fpmatrix[c + no, c] = hc\n\n    for (oid, hid), ex in tps.items():\n        r = oids_idx[oid]\n        c = hids_idx[hid]\n        fpmatrix[r, c] -= ex\n        fnmatrix[r, c] -= ex\n\n    costs = fpmatrix + fnmatrix\n    rids, cids = linear_sum_assignment(costs)\n\n    return {\n        \"fpmatrix\": fpmatrix,\n        \"fnmatrix\": fnmatrix,\n        \"rids\": rids,\n        \"cids\": cids,\n        \"costs\": costs,\n        \"min_cost\": costs[rids, cids].sum(),\n    }\n\n\ndef idfp(df, id_global_assignment):\n    \"\"\"ID measures: Number of false positive matches after global min-cost matching.\"\"\"\n    del df  # unused\n    rids, cids = id_global_assignment[\"rids\"], id_global_assignment[\"cids\"]\n    return id_global_assignment[\"fpmatrix\"][rids, cids].sum()\n\n\nsimple_add_func.append(idfp)\n\n\ndef idfn(df, id_global_assignment):\n    \"\"\"ID measures: Number of false negatives matches after global min-cost matching.\"\"\"\n    del df  # unused\n    rids, cids = id_global_assignment[\"rids\"], id_global_assignment[\"cids\"]\n    return id_global_assignment[\"fnmatrix\"][rids, cids].sum()\n\n\nsimple_add_func.append(idfn)\n\n\ndef idtp(df, id_global_assignment, num_objects, idfn):\n    \"\"\"ID measures: Number of true positives matches after global min-cost matching.\"\"\"\n    del df, id_global_assignment  # unused\n    return num_objects - idfn\n\n\nsimple_add_func.append(idtp)\n\n\ndef idp(df, idtp, idfp):\n    \"\"\"ID measures: global min-cost precision.\"\"\"\n    del df  # unused\n    return math_util.quiet_divide(idtp, idtp + idfp)\n\n\ndef idp_m(partials, idtp, idfp):\n    del partials  # unused\n    return math_util.quiet_divide(idtp, idtp + idfp)\n\n\ndef idr(df, idtp, idfn):\n    \"\"\"ID measures: global min-cost recall.\"\"\"\n    del df  # unused\n    return math_util.quiet_divide(idtp, idtp + idfn)\n\n\ndef idr_m(partials, idtp, idfn):\n    del partials  # unused\n    return math_util.quiet_divide(idtp, idtp + idfn)\n\n\ndef idf1(df, idtp, num_objects, num_predictions):\n    \"\"\"ID measures: global min-cost F1 score.\"\"\"\n    del df  # unused\n    return math_util.quiet_divide(2 * idtp, num_objects + num_predictions)\n\n\ndef idf1_m(partials, idtp, num_objects, num_predictions):\n    del partials  # unused\n    return math_util.quiet_divide(2 * idtp, num_objects + num_predictions)\n\n\nfor one in simple_add_func:\n    name = one.__name__\n\n    def getSimpleAdd(nm):\n        def simpleAddHolder(partials):\n            res = 0\n            for v in partials:\n                res += v[nm]\n            return res\n\n        return simpleAddHolder\n\n    locals()[name + \"_m\"] = getSimpleAdd(name)\n\n\ndef create():\n    \"\"\"Creates a MetricsHost and populates it with default metrics.\"\"\"\n    m = MetricsHost()\n\n    m.register(num_frames, formatter=\"{:d}\".format)\n    m.register(obj_frequencies, formatter=\"{:d}\".format)\n    m.register(pred_frequencies, formatter=\"{:d}\".format)\n    m.register(num_matches, formatter=\"{:d}\".format)\n    m.register(num_switches, formatter=\"{:d}\".format)\n    m.register(num_transfer, formatter=\"{:d}\".format)\n    m.register(num_ascend, formatter=\"{:d}\".format)\n    m.register(num_migrate, formatter=\"{:d}\".format)\n    m.register(num_false_positives, formatter=\"{:d}\".format)\n    m.register(num_misses, formatter=\"{:d}\".format)\n    m.register(num_detections, formatter=\"{:d}\".format)\n    m.register(num_objects, formatter=\"{:d}\".format)\n    m.register(num_predictions, formatter=\"{:d}\".format)\n    m.register(num_gt_ids, formatter=\"{:d}\".format)\n    m.register(num_dt_ids, formatter=\"{:d}\".format)\n    m.register(num_unique_objects, formatter=\"{:d}\".format)\n    m.register(track_ratios)\n    m.register(mostly_tracked, formatter=\"{:d}\".format)\n    m.register(partially_tracked, formatter=\"{:d}\".format)\n    m.register(mostly_lost, formatter=\"{:d}\".format)\n    m.register(num_fragmentations)\n    m.register(motp, formatter=\"{:.3f}\".format)\n    m.register(mota, formatter=\"{:.1%}\".format)\n    m.register(precision, formatter=\"{:.1%}\".format)\n    m.register(recall, formatter=\"{:.1%}\".format)\n\n    m.register(id_global_assignment)\n    m.register(idfp)\n    m.register(idfn)\n    m.register(idtp)\n    m.register(idp, formatter=\"{:.1%}\".format)\n    m.register(idr, formatter=\"{:.1%}\".format)\n    m.register(idf1, formatter=\"{:.1%}\".format)\n\n    m.register(deta_alpha, formatter=\"{:.1%}\".format)\n    m.register(assa_alpha, formatter=\"{:.1%}\".format)\n    m.register(hota_alpha, formatter=\"{:.1%}\".format)\n\n    return m\n\n\nmotchallenge_metrics = [\n    \"idf1\",\n    \"idp\",\n    \"idr\",\n    \"recall\",\n    \"precision\",\n    \"num_unique_objects\",\n    \"mostly_tracked\",\n    \"partially_tracked\",\n    \"mostly_lost\",\n    \"num_false_positives\",\n    \"num_misses\",\n    \"num_switches\",\n    \"num_fragmentations\",\n    \"mota\",\n    \"motp\",\n    \"num_transfer\",\n    \"num_ascend\",\n    \"num_migrate\",\n]\n\"\"\"A list of all metrics from MOTChallenge.\"\"\"\n",
    "motmetrics/utils.py": "# py-motmetrics - Metrics for multiple object tracker (MOT) benchmarking.\n# https://github.com/cheind/py-motmetrics/\n#\n# MIT License\n# Copyright (c) 2017-2020 Christoph Heindl, Jack Valmadre and others.\n# See LICENSE file for terms.\n\n\"\"\"Functions for populating event accumulators.\"\"\"\n\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\n\nfrom motmetrics.distances import iou_matrix, norm2squared_matrix\nfrom motmetrics.mot import MOTAccumulator\nfrom motmetrics.preprocess import preprocessResult\n\n\ndef compute_global_aligment_score(\n    allframeids, fid_to_fgt, fid_to_fdt, num_gt_id, num_det_id, dist_func\n):\n    \"\"\"Taken from https://github.com/JonathonLuiten/TrackEval/blob/12c8791b303e0a0b50f753af204249e622d0281a/trackeval/metrics/hota.py\"\"\"\n    potential_matches_count = np.zeros((num_gt_id, num_det_id))\n    gt_id_count = np.zeros((num_gt_id, 1))\n    tracker_id_count = np.zeros((1, num_det_id))\n\n    for fid in allframeids:\n        oids = np.empty(0)\n        hids = np.empty(0)\n        if fid in fid_to_fgt:\n            fgt = fid_to_fgt[fid]\n            oids = fgt.index.get_level_values(\"Id\")\n        if fid in fid_to_fdt:\n            fdt = fid_to_fdt[fid]\n            hids = fdt.index.get_level_values(\"Id\")\n        if len(oids) > 0 and len(hids) > 0:\n            gt_ids = np.array(oids.values) - 1\n            dt_ids = np.array(hids.values) - 1\n            similarity = dist_func(fgt.values, fdt.values, return_dist=False)\n\n            sim_iou_denom = (\n                similarity.sum(0)[np.newaxis, :] + similarity.sum(1)[:, np.newaxis] - similarity\n            )\n            sim_iou = np.zeros_like(similarity)\n            sim_iou_mask = sim_iou_denom > 0 + np.finfo(\"float\").eps\n            sim_iou[sim_iou_mask] = similarity[sim_iou_mask] / sim_iou_denom[sim_iou_mask]\n            potential_matches_count[gt_ids[:, np.newaxis], dt_ids[np.newaxis, :]] += sim_iou\n\n            # Calculate the total number of dets for each gt_id and tracker_id.\n            gt_id_count[gt_ids] += 1\n            tracker_id_count[0, dt_ids] += 1\n    global_alignment_score = potential_matches_count / (\n        np.maximum(1, gt_id_count + tracker_id_count - potential_matches_count)\n    )\n    return global_alignment_score\n\n\ndef compare_to_groundtruth_reweighting(gt, dt, dist=\"iou\", distfields=None, distth=(0.5)):\n    \"\"\"Compare groundtruth and detector results with global alignment score.\n\n    This method assumes both results are given in terms of DataFrames with at least the following fields\n     - `FrameId` First level index used for matching ground-truth and test frames.\n     - `Id` Secondary level index marking available object / hypothesis ids\n\n    Depending on the distance to be used relevant distfields need to be specified.\n\n    Params\n    ------\n    gt : pd.DataFrame\n        Dataframe for ground-truth\n    test : pd.DataFrame\n        Dataframe for detector results\n\n    Kwargs\n    ------\n    dist : str, optional\n        String identifying distance to be used. Defaults to intersection over union ('iou'). Euclidean\n        distance ('euclidean') and squared euclidean distance ('seuc') are also supported.\n    distfields: array, optional\n        Fields relevant for extracting distance information. Defaults to ['X', 'Y', 'Width', 'Height']\n    distth: Union(float, array_like), optional\n        Maximum tolerable distance. Pairs exceeding this threshold are marked 'do-not-pair'.\n        If a list of thresholds is given, multiple accumulators are returned.\n    \"\"\"\n    # pylint: disable=too-many-locals\n    if distfields is None:\n        distfields = [\"X\", \"Y\", \"Width\", \"Height\"]\n\n    def compute_iou(a, b, return_dist):\n        return iou_matrix(a, b, max_iou=distth, return_dist=return_dist)\n\n    def compute_euc(a, b, *args, **kwargs):\n        return np.sqrt(norm2squared_matrix(a, b, max_d2=distth**2))\n\n    def compute_seuc(a, b, *args, **kwargs):\n        return norm2squared_matrix(a, b, max_d2=distth)\n\n    if dist.upper() == \"IOU\":\n        compute_dist = compute_iou\n    elif dist.upper() == \"EUC\":\n        compute_dist = compute_euc\n        import warnings\n\n        warnings.warn(\n            f\"'euc' flag changed its behavior. The euclidean distance is now used instead of the squared euclidean distance. Make sure the used threshold (distth={distth}) is not squared. Use 'euclidean' flag to avoid this warning.\"\n        )\n    elif dist.upper() == \"EUCLIDEAN\":\n        compute_dist = compute_euc\n    elif dist.upper() == \"SEUC\":\n        compute_dist = compute_seuc\n    else:\n        raise f'Unknown distance metric {dist}. Use \"IOU\", \"EUCLIDEAN\",  or \"SEUC\"'\n\n    return_single = False\n    if isinstance(distth, float):\n        distth = [distth]\n        return_single = True\n\n    acc_list = [MOTAccumulator() for _ in range(len(distth))]\n\n    num_gt_id = gt.index.get_level_values(\"Id\").max()\n    num_det_id = dt.index.get_level_values(\"Id\").max()\n\n    # We need to account for all frames reported either by ground truth or\n    # detector. In case a frame is missing in GT this will lead to FPs, in\n    # case a frame is missing in detector results this will lead to FNs.\n    allframeids = gt.index.union(dt.index).levels[0]\n\n    gt = gt[distfields]\n    dt = dt[distfields]\n    fid_to_fgt = dict(iter(gt.groupby(\"FrameId\")))\n    fid_to_fdt = dict(iter(dt.groupby(\"FrameId\")))\n\n    global_alignment_score = compute_global_aligment_score(\n        allframeids, fid_to_fgt, fid_to_fdt, num_gt_id, num_det_id, compute_dist\n    )\n\n    for fid in allframeids:\n        oids = np.empty(0)\n        hids = np.empty(0)\n        weighted_dists = np.empty((0, 0))\n        if fid in fid_to_fgt:\n            fgt = fid_to_fgt[fid]\n            oids = fgt.index.get_level_values(\"Id\")\n        if fid in fid_to_fdt:\n            fdt = fid_to_fdt[fid]\n            hids = fdt.index.get_level_values(\"Id\")\n        if len(oids) > 0 and len(hids) > 0:\n            gt_ids = np.array(oids.values) - 1\n            dt_ids = np.array(hids.values) - 1\n            dists = compute_dist(fgt.values, fdt.values, return_dist=False)\n            weighted_dists = (\n                dists * global_alignment_score[gt_ids[:, np.newaxis], dt_ids[np.newaxis, :]]\n            )\n        for acc, th in zip(acc_list, distth):\n            acc.update(oids, hids, 1 - weighted_dists, frameid=fid, similartiy_matrix=dists, th=th)\n    return acc_list[0] if return_single else acc_list\n\n\ndef compare_to_groundtruth(gt, dt, dist='iou', distfields=None, distth=0.5):\n    \"\"\"Compare groundtruth and detector results.\n\n    This method assumes both results are given in terms of DataFrames with at least the following fields\n     - `FrameId` First level index used for matching ground-truth and test frames.\n     - `Id` Secondary level index marking available object / hypothesis ids\n\n    Depending on the distance to be used relevant distfields need to be specified.\n\n    Params\n    ------\n    gt : pd.DataFrame\n        Dataframe for ground-truth\n    test : pd.DataFrame\n        Dataframe for detector results\n\n    Kwargs\n    ------\n    dist : str, optional\n        String identifying distance to be used. Defaults to intersection over union ('iou'). Euclidean\n        distance ('euclidean') and squared euclidean distance ('seuc') are also supported.\n    distfields: array, optional\n        Fields relevant for extracting distance information. Defaults to ['X', 'Y', 'Width', 'Height']\n    distth: float, optional\n        Maximum tolerable distance. Pairs exceeding this threshold are marked 'do-not-pair'.\n    \"\"\"\n    # pylint: disable=too-many-locals\n    if distfields is None:\n        distfields = ['X', 'Y', 'Width', 'Height']\n\n    def compute_iou(a, b):\n        return iou_matrix(a, b, max_iou=distth)\n\n    def compute_euc(a, b):\n        return np.sqrt(norm2squared_matrix(a, b, max_d2=distth**2))\n\n    def compute_seuc(a, b):\n        return norm2squared_matrix(a, b, max_d2=distth)\n\n    if dist.upper() == 'IOU':\n        compute_dist = compute_iou\n    elif dist.upper() == 'EUC':\n        compute_dist = compute_euc\n        import warnings\n        warnings.warn(f\"'euc' flag changed its behavior. The euclidean distance is now used instead of the squared euclidean distance. Make sure the used threshold (distth={distth}) is not squared. Use 'euclidean' flag to avoid this warning.\")\n    elif dist.upper() == 'EUCLIDEAN':\n        compute_dist = compute_euc\n    elif dist.upper() == 'SEUC':\n        compute_dist = compute_seuc\n    else:\n        raise f'Unknown distance metric {dist}. Use \"IOU\", \"EUCLIDEAN\",  or \"SEUC\"'\n\n    acc = MOTAccumulator()\n\n    # We need to account for all frames reported either by ground truth or\n    # detector. In case a frame is missing in GT this will lead to FPs, in\n    # case a frame is missing in detector results this will lead to FNs.\n    allframeids = gt.index.union(dt.index).levels[0]\n\n    gt = gt[distfields]\n    dt = dt[distfields]\n    fid_to_fgt = dict(iter(gt.groupby('FrameId')))\n    fid_to_fdt = dict(iter(dt.groupby('FrameId')))\n\n    for fid in allframeids:\n        oids = np.empty(0)\n        hids = np.empty(0)\n        dists = np.empty((0, 0))\n        if fid in fid_to_fgt:\n            fgt = fid_to_fgt[fid]\n            oids = fgt.index.get_level_values('Id')\n        if fid in fid_to_fdt:\n            fdt = fid_to_fdt[fid]\n            hids = fdt.index.get_level_values('Id')\n        if len(oids) > 0 and len(hids) > 0:\n            dists = compute_dist(fgt.values, fdt.values)\n        acc.update(oids, hids, dists, frameid=fid)\n\n    return acc\n\n\ndef CLEAR_MOT_M(gt, dt, inifile, dist='iou', distfields=None, distth=0.5, include_all=False, vflag=''):\n    \"\"\"Compare groundtruth and detector results.\n\n    This method assumes both results are given in terms of DataFrames with at least the following fields\n     - `FrameId` First level index used for matching ground-truth and test frames.\n     - `Id` Secondary level index marking available object / hypothesis ids\n\n    Depending on the distance to be used relevant distfields need to be specified.\n\n    Params\n    ------\n    gt : pd.DataFrame\n        Dataframe for ground-truth\n    test : pd.DataFrame\n        Dataframe for detector results\n\n    Kwargs\n    ------\n    dist : str, optional\n        String identifying distance to be used. Defaults to intersection over union.\n    distfields: array, optional\n        Fields relevant for extracting distance information. Defaults to ['X', 'Y', 'Width', 'Height']\n    distth: float, optional\n        Maximum tolerable distance. Pairs exceeding this threshold are marked 'do-not-pair'.\n    \"\"\"\n    # pylint: disable=too-many-locals\n    if distfields is None:\n        distfields = ['X', 'Y', 'Width', 'Height']\n\n    def compute_iou(a, b):\n        return iou_matrix(a, b, max_iou=distth)\n\n    def compute_euc(a, b):\n        return norm2squared_matrix(a, b, max_d2=distth)\n\n    compute_dist = compute_iou if dist.upper() == 'IOU' else compute_euc\n\n    acc = MOTAccumulator()\n    dt = preprocessResult(dt, gt, inifile)\n    if include_all:\n        gt = gt[gt['Confidence'] >= 0.99]\n    else:\n        gt = gt[(gt['Confidence'] >= 0.99) & (gt['ClassId'] == 1)]\n    # We need to account for all frames reported either by ground truth or\n    # detector. In case a frame is missing in GT this will lead to FPs, in\n    # case a frame is missing in detector results this will lead to FNs.\n    allframeids = gt.index.union(dt.index).levels[0]\n    analysis = {'hyp': {}, 'obj': {}}\n    for fid in allframeids:\n        oids = np.empty(0)\n        hids = np.empty(0)\n        dists = np.empty((0, 0))\n\n        if fid in gt.index:\n            fgt = gt.loc[fid]\n            oids = fgt.index.values\n            for oid in oids:\n                oid = int(oid)\n                if oid not in analysis['obj']:\n                    analysis['obj'][oid] = 0\n                analysis['obj'][oid] += 1\n\n        if fid in dt.index:\n            fdt = dt.loc[fid]\n            hids = fdt.index.values\n            for hid in hids:\n                hid = int(hid)\n                if hid not in analysis['hyp']:\n                    analysis['hyp'][hid] = 0\n                analysis['hyp'][hid] += 1\n\n        if oids.shape[0] > 0 and hids.shape[0] > 0:\n            dists = compute_dist(fgt[distfields].values, fdt[distfields].values)\n\n        acc.update(oids, hids, dists, frameid=fid, vf=vflag)\n\n    return acc, analysis\n"
  },
  "GT_src_dict": {
    "motmetrics/metrics.py": {
      "MetricsHost.compute": {
        "code": "    def compute(self, df, ana=None, metrics=None, return_dataframe=True, return_cached=False, name=None):\n        \"\"\"Compute metrics on a given dataframe or MOTAccumulator.\n\nThis method takes a dataframe or an MOTAccumulator containing event data, computes specified metrics (or all registered metrics if none are specified), and returns the results in the desired format (either as a pandas DataFrame or a dictionary). The method handles dependencies between metrics automatically, caching results for efficiency.\n\nParameters\n----------\ndf : MOTAccumulator or pandas.DataFrame\n    The dataframe or accumulator to compute the metrics on.\nana : dict or None, optional\n    A cache for fast computation of results.\nmetrics : string, list of string or None, optional\n    Identifiers for the metrics to compute. If None, all registered metrics are computed.\nreturn_dataframe : bool, optional\n    If True, returns the results as a pandas DataFrame (default); otherwise, returns a dictionary.\nreturn_cached : bool, optional\n    If True, all intermediate metrics required to compute the desired metrics are returned as well.\nname : string, optional\n    The index of the row containing the computed metric values when returning a DataFrame.\n\nReturns\n-------\npd.DataFrame or dict\n    If return_dataframe is True, returns a DataFrame with the computed metrics; otherwise, returns a dictionary with metric names as keys and computed values as values.\n\nNotes\n-----\nIf `df` is an instance of MOTAccumulator, its `events` attribute is used for metric computation. The `events_to_df_map` function is utilized to categorize events into different dataframes (full, raw, noraw, and extra) for metric calculations. The `motchallenge_metrics` constant lists all metrics registered for computation when no specific metrics are provided.\"\"\"\n        'Compute metrics on the dataframe / accumulator.\\n\\n        Params\\n        ------\\n        df : MOTAccumulator or pandas.DataFrame\\n            The dataframe to compute the metrics on\\n\\n        Kwargs\\n        ------\\n        ana: dict or None, optional\\n            To cache results for fast computation.\\n        metrics : string, list of string or None, optional\\n            The identifiers of the metrics to be computed. This method will only\\n            compute the minimal set of necessary metrics to fullfill the request.\\n            If None is passed all registered metrics are computed.\\n        return_dataframe : bool, optional\\n            Return the result as pandas.DataFrame (default) or dict.\\n        return_cached : bool, optional\\n           If true all intermediate metrics required to compute the desired metrics are returned as well.\\n        name : string, optional\\n            When returning a pandas.DataFrame this is the index of the row containing\\n            the computed metric values.\\n        '\n        if isinstance(df, MOTAccumulator):\n            df = df.events\n        if metrics is None:\n            metrics = motchallenge_metrics\n        elif isinstance(metrics, str):\n            metrics = [metrics]\n        df_map = events_to_df_map(df)\n        cache = {}\n        options = {'ana': ana}\n        for mname in metrics:\n            cache[mname] = self._compute(df_map, mname, cache, options, parent='summarize')\n        if name is None:\n            name = 0\n        if return_cached:\n            data = cache\n        else:\n            data = OrderedDict([(k, cache[k]) for k in metrics])\n        ret = pd.DataFrame(data, index=[name]) if return_dataframe else data\n        return ret",
        "docstring": "Compute metrics on a given dataframe or MOTAccumulator.\n\nThis method takes a dataframe or an MOTAccumulator containing event data, computes specified metrics (or all registered metrics if none are specified), and returns the results in the desired format (either as a pandas DataFrame or a dictionary). The method handles dependencies between metrics automatically, caching results for efficiency.\n\nParameters\n----------\ndf : MOTAccumulator or pandas.DataFrame\n    The dataframe or accumulator to compute the metrics on.\nana : dict or None, optional\n    A cache for fast computation of results.\nmetrics : string, list of string or None, optional\n    Identifiers for the metrics to compute. If None, all registered metrics are computed.\nreturn_dataframe : bool, optional\n    If True, returns the results as a pandas DataFrame (default); otherwise, returns a dictionary.\nreturn_cached : bool, optional\n    If True, all intermediate metrics required to compute the desired metrics are returned as well.\nname : string, optional\n    The index of the row containing the computed metric values when returning a DataFrame.\n\nReturns\n-------\npd.DataFrame or dict\n    If return_dataframe is True, returns a DataFrame with the computed metrics; otherwise, returns a dictionary with metric names as keys and computed values as values.\n\nNotes\n-----\nIf `df` is an instance of MOTAccumulator, its `events` attribute is used for metric computation. The `events_to_df_map` function is utilized to categorize events into different dataframes (full, raw, noraw, and extra) for metric calculations. The `motchallenge_metrics` constant lists all metrics registered for computation when no specific metrics are provided.",
        "signature": "def compute(self, df, ana=None, metrics=None, return_dataframe=True, return_cached=False, name=None):",
        "type": "Method",
        "class_signature": "class MetricsHost:"
      },
      "create": {
        "code": "def create():\n    \"\"\"Creates and initializes an instance of the `MetricsHost` class, populating it with a predefined set of metrics related to multiple object tracking performance. This function registers various metrics such as the number of frames, false positives, unique objects, and several accuracy metrics related to object tracking.\n\nParameters\n----------\nNone\n\nReturns\n-------\nMetricsHost\n    An instance of the `MetricsHost` class populated with default metrics used for evaluating tracking performance.\n\nDependencies\n------------\n- `MetricsHost`: A class that manages the registration and computation of metrics.\n- Several metric functions such as `num_frames`, `obj_frequencies`, `num_matches`, etc., which provide specific calculations for performance evaluation.\n- Formatting options for displaying the metrics, for example, using \"{:d}\".format for integer outputs and \"{:.1%}\".format for percentage outputs.\n\nConstants\n---------\n- `motchallenge_metrics`: A predefined list of metric identifiers that include various accuracy measures like `idf1`, `idp`, `idr`, etc. This list is used for specifying which metrics to compute during evaluation.\"\"\"\n    'Creates a MetricsHost and populates it with default metrics.'\n    m = MetricsHost()\n    m.register(num_frames, formatter='{:d}'.format)\n    m.register(obj_frequencies, formatter='{:d}'.format)\n    m.register(pred_frequencies, formatter='{:d}'.format)\n    m.register(num_matches, formatter='{:d}'.format)\n    m.register(num_switches, formatter='{:d}'.format)\n    m.register(num_transfer, formatter='{:d}'.format)\n    m.register(num_ascend, formatter='{:d}'.format)\n    m.register(num_migrate, formatter='{:d}'.format)\n    m.register(num_false_positives, formatter='{:d}'.format)\n    m.register(num_misses, formatter='{:d}'.format)\n    m.register(num_detections, formatter='{:d}'.format)\n    m.register(num_objects, formatter='{:d}'.format)\n    m.register(num_predictions, formatter='{:d}'.format)\n    m.register(num_gt_ids, formatter='{:d}'.format)\n    m.register(num_dt_ids, formatter='{:d}'.format)\n    m.register(num_unique_objects, formatter='{:d}'.format)\n    m.register(track_ratios)\n    m.register(mostly_tracked, formatter='{:d}'.format)\n    m.register(partially_tracked, formatter='{:d}'.format)\n    m.register(mostly_lost, formatter='{:d}'.format)\n    m.register(num_fragmentations)\n    m.register(motp, formatter='{:.3f}'.format)\n    m.register(mota, formatter='{:.1%}'.format)\n    m.register(precision, formatter='{:.1%}'.format)\n    m.register(recall, formatter='{:.1%}'.format)\n    m.register(id_global_assignment)\n    m.register(idfp)\n    m.register(idfn)\n    m.register(idtp)\n    m.register(idp, formatter='{:.1%}'.format)\n    m.register(idr, formatter='{:.1%}'.format)\n    m.register(idf1, formatter='{:.1%}'.format)\n    m.register(deta_alpha, formatter='{:.1%}'.format)\n    m.register(assa_alpha, formatter='{:.1%}'.format)\n    m.register(hota_alpha, formatter='{:.1%}'.format)\n    return m",
        "docstring": "Creates and initializes an instance of the `MetricsHost` class, populating it with a predefined set of metrics related to multiple object tracking performance. This function registers various metrics such as the number of frames, false positives, unique objects, and several accuracy metrics related to object tracking.\n\nParameters\n----------\nNone\n\nReturns\n-------\nMetricsHost\n    An instance of the `MetricsHost` class populated with default metrics used for evaluating tracking performance.\n\nDependencies\n------------\n- `MetricsHost`: A class that manages the registration and computation of metrics.\n- Several metric functions such as `num_frames`, `obj_frequencies`, `num_matches`, etc., which provide specific calculations for performance evaluation.\n- Formatting options for displaying the metrics, for example, using \"{:d}\".format for integer outputs and \"{:.1%}\".format for percentage outputs.\n\nConstants\n---------\n- `motchallenge_metrics`: A predefined list of metric identifiers that include various accuracy measures like `idf1`, `idp`, `idr`, etc. This list is used for specifying which metrics to compute during evaluation.",
        "signature": "def create():",
        "type": "Function",
        "class_signature": null
      }
    },
    "motmetrics/utils.py": {
      "compare_to_groundtruth": {
        "code": "def compare_to_groundtruth(gt, dt, dist='iou', distfields=None, distth=0.5):\n    \"\"\"Compare groundtruth and detector results using specified distance metrics.\n\nThis function evaluates the performance of a multi-object tracker by comparing its output (detector results) against groundtruth data. It uses various distance measures to compute the similarity between detected objects and groundtruth objects based on provided spatial fields (defaulting to 'X', 'Y', 'Width', and 'Height'). The function is particularly helpful in estimating tracking accuracy and identifying potential false positives or negatives.\n\nParameters\n----------\ngt : pd.DataFrame\n    The groundtruth data containing information about the actual tracked objects, which must include 'FrameId' and 'Id' as indices.\ndt : pd.DataFrame\n    The detection results from the tracker, following the same structure as the groundtruth DataFrame.\ndist : str, optional\n    The distance measure to use for comparisons. Acceptable values are 'iou' (Intersection over Union), 'euc' (Euclidean), and 'seuc' (Squared Euclidean). Defaults to 'iou'.\ndistfields : array, optional\n    Relevant spatial fields for calculating distance. Defaults to ['X', 'Y', 'Width', 'Height'].\ndistth : float, optional\n    The maximum tolerable distance for considering a detection valid, dictating the robustness of matches. Defaults to 0.5.\n\nReturns\n-------\nMOTAccumulator\n    An instance of MOTAccumulator containing the recorded performance metrics based on the computed distances.\n\nNotes\n-----\nThe function identifies the union of all frame IDs present in both groundtruth and detection data, ensuring that missing frames are appropriately accounted for as false positives or negatives. The computed distance matrices leverage the `iou_matrix` and `norm2squared_matrix` functions imported from the `motmetrics` library, highlighting its dependency on these distance calculation implementations.\"\"\"\n    \"Compare groundtruth and detector results.\\n\\n    This method assumes both results are given in terms of DataFrames with at least the following fields\\n     - `FrameId` First level index used for matching ground-truth and test frames.\\n     - `Id` Secondary level index marking available object / hypothesis ids\\n\\n    Depending on the distance to be used relevant distfields need to be specified.\\n\\n    Params\\n    ------\\n    gt : pd.DataFrame\\n        Dataframe for ground-truth\\n    test : pd.DataFrame\\n        Dataframe for detector results\\n\\n    Kwargs\\n    ------\\n    dist : str, optional\\n        String identifying distance to be used. Defaults to intersection over union ('iou'). Euclidean\\n        distance ('euclidean') and squared euclidean distance ('seuc') are also supported.\\n    distfields: array, optional\\n        Fields relevant for extracting distance information. Defaults to ['X', 'Y', 'Width', 'Height']\\n    distth: float, optional\\n        Maximum tolerable distance. Pairs exceeding this threshold are marked 'do-not-pair'.\\n    \"\n    if distfields is None:\n        distfields = ['X', 'Y', 'Width', 'Height']\n\n    def compute_iou(a, b):\n        return iou_matrix(a, b, max_iou=distth)\n\n    def compute_euc(a, b):\n        return np.sqrt(norm2squared_matrix(a, b, max_d2=distth ** 2))\n\n    def compute_seuc(a, b):\n        return norm2squared_matrix(a, b, max_d2=distth)\n    if dist.upper() == 'IOU':\n        compute_dist = compute_iou\n    elif dist.upper() == 'EUC':\n        compute_dist = compute_euc\n        import warnings\n        warnings.warn(f\"'euc' flag changed its behavior. The euclidean distance is now used instead of the squared euclidean distance. Make sure the used threshold (distth={distth}) is not squared. Use 'euclidean' flag to avoid this warning.\")\n    elif dist.upper() == 'EUCLIDEAN':\n        compute_dist = compute_euc\n    elif dist.upper() == 'SEUC':\n        compute_dist = compute_seuc\n    else:\n        raise f'Unknown distance metric {dist}. Use \"IOU\", \"EUCLIDEAN\",  or \"SEUC\"'\n    acc = MOTAccumulator()\n    allframeids = gt.index.union(dt.index).levels[0]\n    gt = gt[distfields]\n    dt = dt[distfields]\n    fid_to_fgt = dict(iter(gt.groupby('FrameId')))\n    fid_to_fdt = dict(iter(dt.groupby('FrameId')))\n    for fid in allframeids:\n        oids = np.empty(0)\n        hids = np.empty(0)\n        dists = np.empty((0, 0))\n        if fid in fid_to_fgt:\n            fgt = fid_to_fgt[fid]\n            oids = fgt.index.get_level_values('Id')\n        if fid in fid_to_fdt:\n            fdt = fid_to_fdt[fid]\n            hids = fdt.index.get_level_values('Id')\n        if len(oids) > 0 and len(hids) > 0:\n            dists = compute_dist(fgt.values, fdt.values)\n        acc.update(oids, hids, dists, frameid=fid)\n    return acc",
        "docstring": "Compare groundtruth and detector results using specified distance metrics.\n\nThis function evaluates the performance of a multi-object tracker by comparing its output (detector results) against groundtruth data. It uses various distance measures to compute the similarity between detected objects and groundtruth objects based on provided spatial fields (defaulting to 'X', 'Y', 'Width', and 'Height'). The function is particularly helpful in estimating tracking accuracy and identifying potential false positives or negatives.\n\nParameters\n----------\ngt : pd.DataFrame\n    The groundtruth data containing information about the actual tracked objects, which must include 'FrameId' and 'Id' as indices.\ndt : pd.DataFrame\n    The detection results from the tracker, following the same structure as the groundtruth DataFrame.\ndist : str, optional\n    The distance measure to use for comparisons. Acceptable values are 'iou' (Intersection over Union), 'euc' (Euclidean), and 'seuc' (Squared Euclidean). Defaults to 'iou'.\ndistfields : array, optional\n    Relevant spatial fields for calculating distance. Defaults to ['X', 'Y', 'Width', 'Height'].\ndistth : float, optional\n    The maximum tolerable distance for considering a detection valid, dictating the robustness of matches. Defaults to 0.5.\n\nReturns\n-------\nMOTAccumulator\n    An instance of MOTAccumulator containing the recorded performance metrics based on the computed distances.\n\nNotes\n-----\nThe function identifies the union of all frame IDs present in both groundtruth and detection data, ensuring that missing frames are appropriately accounted for as false positives or negatives. The computed distance matrices leverage the `iou_matrix` and `norm2squared_matrix` functions imported from the `motmetrics` library, highlighting its dependency on these distance calculation implementations.",
        "signature": "def compare_to_groundtruth(gt, dt, dist='iou', distfields=None, distth=0.5):",
        "type": "Function",
        "class_signature": null
      }
    }
  },
  "dependency_dict": {
    "motmetrics/utils.py:compare_to_groundtruth": {
      "motmetrics/mot.py": {
        "MOTAccumulator.__init__": {
          "code": "    def __init__(self, auto_id=False, max_switch_time=float('inf')):\n        \"\"\"Create a MOTAccumulator.\n\n        Params\n        ------\n        auto_id : bool, optional\n            Whether or not frame indices are auto-incremented or provided upon\n            updating. Defaults to false. Not specifying a frame-id when this value\n            is true results in an error. Specifying a frame-id when this value is\n            false also results in an error.\n\n        max_switch_time : scalar, optional\n            Allows specifying an upper bound on the timespan an unobserved but\n            tracked object is allowed to generate track switch events. Useful if groundtruth\n            objects leaving the field of view keep their ID when they reappear,\n            but your tracker is not capable of recognizing this (resulting in\n            track switch events). The default is that there is no upper bound\n            on the timespan. In units of frame timestamps. When using auto_id\n            in units of count.\n        \"\"\"\n\n        # Parameters of the accumulator.\n        self.auto_id = auto_id\n        self.max_switch_time = max_switch_time\n\n        # Accumulator state.\n        self._events = None\n        self._indices = None\n        self.m = None\n        self.res_m = None\n        self.last_occurrence = None\n        self.last_match = None\n        self.hypHistory = None\n        self.dirty_events = None\n        self.cached_events_df = None\n        self.last_update_frameid = None\n\n        self.reset()",
          "docstring": "Create a MOTAccumulator.\n\nParams\n------\nauto_id : bool, optional\n    Whether or not frame indices are auto-incremented or provided upon\n    updating. Defaults to false. Not specifying a frame-id when this value\n    is true results in an error. Specifying a frame-id when this value is\n    false also results in an error.\n\nmax_switch_time : scalar, optional\n    Allows specifying an upper bound on the timespan an unobserved but\n    tracked object is allowed to generate track switch events. Useful if groundtruth\n    objects leaving the field of view keep their ID when they reappear,\n    but your tracker is not capable of recognizing this (resulting in\n    track switch events). The default is that there is no upper bound\n    on the timespan. In units of frame timestamps. When using auto_id\n    in units of count.",
          "signature": "def __init__(self, auto_id=False, max_switch_time=float('inf')):",
          "type": "Method",
          "class_signature": "class MOTAccumulator(object):"
        },
        "MOTAccumulator.update": {
          "code": "    def update(self, oids, hids, dists, frameid=None, vf='', similartiy_matrix=None, th=None):\n        \"\"\"Updates the accumulator with frame specific objects/detections.\n\n        This method generates events based on the following algorithm [1]:\n        1. Try to carry forward already established tracks. If any paired object / hypothesis\n        from previous timestamps are still visible in the current frame, create a 'MATCH'\n        event between them.\n        2. For the remaining constellations minimize the total object / hypothesis distance\n        error (Kuhn-Munkres algorithm). If a correspondence made contradicts a previous\n        match create a 'SWITCH' else a 'MATCH' event.\n        3. Create 'MISS' events for all remaining unassigned objects.\n        4. Create 'FP' events for all remaining unassigned hypotheses.\n\n        Params\n        ------\n        oids : N array\n            Array of object ids.\n        hids : M array\n            Array of hypothesis ids.\n        dists: NxM array\n            Distance matrix. np.nan values to signal do-not-pair constellations.\n            See `distances` module for support methods.\n\n        Kwargs\n        ------\n        frameId : id\n            Unique frame id. Optional when MOTAccumulator.auto_id is specified during\n            construction.\n        vf: file to log details\n        Returns\n        -------\n        frame_events : pd.DataFrame\n            Dataframe containing generated events\n\n        References\n        ----------\n        1. Bernardin, Keni, and Rainer Stiefelhagen. \"Evaluating multiple object tracking performance: the CLEAR MOT metrics.\"\n        EURASIP Journal on Image and Video Processing 2008.1 (2008): 1-10.\n        \"\"\"\n        # pylint: disable=too-many-locals, too-many-statements\n\n        self.dirty_events = True\n        oids = np.asarray(oids)\n        oids_masked = np.zeros_like(oids, dtype=np.bool_)\n        hids = np.asarray(hids)\n        hids_masked = np.zeros_like(hids, dtype=np.bool_)\n        dists = np.atleast_2d(dists).astype(float).reshape(oids.shape[0], hids.shape[0]).copy()\n\n        if frameid is None:\n            assert self.auto_id, 'auto-id is not enabled'\n            if len(self._indices['FrameId']) > 0:\n                frameid = self._indices['FrameId'][-1] + 1\n            else:\n                frameid = 0\n        else:\n            assert not self.auto_id, 'Cannot provide frame id when auto-id is enabled'\n\n        eid = itertools.count()\n\n        # 0. Record raw events\n\n        no = len(oids)\n        nh = len(hids)\n\n        # Add a RAW event simply to ensure the frame is counted.\n        self._append_to_indices(frameid, next(eid))\n        self._append_to_events('RAW', np.nan, np.nan, np.nan)\n\n        # Postcompute the distance matrix if necessary. (e.g., HOTA)\n        cost_for_matching = dists.copy()\n        if similartiy_matrix is not None and th is not None:\n            dists = 1 - similartiy_matrix\n            dists = np.where(similartiy_matrix < th - np.finfo(\"float\").eps, np.nan, dists)\n\n        # There must be at least one RAW event per object and hypothesis.\n        # Record all finite distances as RAW events.\n        valid_i, valid_j = np.where(np.isfinite(dists))\n        valid_dists = dists[valid_i, valid_j]\n        for i, j, dist_ij in zip(valid_i, valid_j, valid_dists):\n            self._append_to_indices(frameid, next(eid))\n            self._append_to_events('RAW', oids[i], hids[j], dist_ij)\n        # Add a RAW event for objects and hypotheses that were present but did\n        # not overlap with anything.\n        used_i = np.unique(valid_i)\n        used_j = np.unique(valid_j)\n        unused_i = np.setdiff1d(np.arange(no), used_i)\n        unused_j = np.setdiff1d(np.arange(nh), used_j)\n        for oid in oids[unused_i]:\n            self._append_to_indices(frameid, next(eid))\n            self._append_to_events('RAW', oid, np.nan, np.nan)\n        for hid in hids[unused_j]:\n            self._append_to_indices(frameid, next(eid))\n            self._append_to_events('RAW', np.nan, hid, np.nan)\n\n        if oids.size * hids.size > 0:\n            # 1. Try to re-establish tracks from correspondences in last update\n            #    ignore this if post processing is performed (e.g., HOTA)\n            if similartiy_matrix is None or th is None:\n                for i in range(oids.shape[0]):\n                    # No need to check oids_masked[i] here.\n                    if not (oids[i] in self.m and self.last_match[oids[i]] == self.last_update_frameid):\n                        continue\n\n                    hprev = self.m[oids[i]]\n                    j, = np.where(~hids_masked & (hids == hprev))\n                    if j.shape[0] == 0:\n                        continue\n                    j = j[0]\n\n                    if np.isfinite(dists[i, j]):\n                        o = oids[i]\n                        h = hids[j]\n                        oids_masked[i] = True\n                        hids_masked[j] = True\n                        self.m[oids[i]] = hids[j]\n\n                        self._append_to_indices(frameid, next(eid))\n                        self._append_to_events('MATCH', oids[i], hids[j], dists[i, j])\n                        self.last_match[o] = frameid\n                        self.hypHistory[h] = frameid\n\n            # 2. Try to remaining objects/hypotheses\n            dists[oids_masked, :] = np.nan\n            dists[:, hids_masked] = np.nan\n\n            rids, cids = linear_sum_assignment(cost_for_matching)\n\n            for i, j in zip(rids, cids):\n                if not np.isfinite(dists[i, j]):\n                    continue\n\n                o = oids[i]\n                h = hids[j]\n                ######################################################################\n                # todo - fixed a bug:\n                # is_switch = (o in self.m and\n                #              self.m[o] != h and\n                #              abs(frameid - self.last_occurrence[o]) <= self.max_switch_time)\n                switch_condition = (\n                    o in self.m and\n                    self.m[o] != h and\n                    o in self.last_occurrence and  # Ensure the object ID 'o' is initialized in last_occurrence\n                    abs(frameid - self.last_occurrence[o]) <= self.max_switch_time\n                )\n                is_switch = switch_condition\n                ######################################################################\n                cat1 = 'SWITCH' if is_switch else 'MATCH'\n                if cat1 == 'SWITCH':\n                    if h not in self.hypHistory:\n                        subcat = 'ASCEND'\n                        self._append_to_indices(frameid, next(eid))\n                        self._append_to_events(subcat, oids[i], hids[j], dists[i, j])\n                # ignore the last condition temporarily\n                is_transfer = (h in self.res_m and\n                               self.res_m[h] != o)\n                # is_transfer = (h in self.res_m and\n                #                self.res_m[h] != o and\n                #                abs(frameid - self.last_occurrence[o]) <= self.max_switch_time)\n                cat2 = 'TRANSFER' if is_transfer else 'MATCH'\n                if cat2 == 'TRANSFER':\n                    if o not in self.last_match:\n                        subcat = 'MIGRATE'\n                        self._append_to_indices(frameid, next(eid))\n                        self._append_to_events(subcat, oids[i], hids[j], dists[i, j])\n                    self._append_to_indices(frameid, next(eid))\n                    self._append_to_events(cat2, oids[i], hids[j], dists[i, j])\n                if vf != '' and (cat1 != 'MATCH' or cat2 != 'MATCH'):\n                    if cat1 == 'SWITCH':\n                        vf.write('%s %d %d %d %d %d\\n' % (subcat[:2], o, self.last_match[o], self.m[o], frameid, h))\n                    if cat2 == 'TRANSFER':\n                        vf.write('%s %d %d %d %d %d\\n' % (subcat[:2], h, self.hypHistory[h], self.res_m[h], frameid, o))\n                self.hypHistory[h] = frameid\n                self.last_match[o] = frameid\n                self._append_to_indices(frameid, next(eid))\n                self._append_to_events(cat1, oids[i], hids[j], dists[i, j])\n                oids_masked[i] = True\n                hids_masked[j] = True\n                self.m[o] = h\n                self.res_m[h] = o\n\n        # 3. All remaining objects are missed\n        for o in oids[~oids_masked]:\n            self._append_to_indices(frameid, next(eid))\n            self._append_to_events('MISS', o, np.nan, np.nan)\n            if vf != '':\n                vf.write('FN %d %d\\n' % (frameid, o))\n\n        # 4. All remaining hypotheses are false alarms\n        for h in hids[~hids_masked]:\n            self._append_to_indices(frameid, next(eid))\n            self._append_to_events('FP', np.nan, h, np.nan)\n            if vf != '':\n                vf.write('FP %d %d\\n' % (frameid, h))\n\n        # 5. Update occurance state\n        for o in oids:\n            self.last_occurrence[o] = frameid\n\n        self.last_update_frameid = frameid\n\n        return frameid",
          "docstring": "Updates the accumulator with frame specific objects/detections.\n\nThis method generates events based on the following algorithm [1]:\n1. Try to carry forward already established tracks. If any paired object / hypothesis\nfrom previous timestamps are still visible in the current frame, create a 'MATCH'\nevent between them.\n2. For the remaining constellations minimize the total object / hypothesis distance\nerror (Kuhn-Munkres algorithm). If a correspondence made contradicts a previous\nmatch create a 'SWITCH' else a 'MATCH' event.\n3. Create 'MISS' events for all remaining unassigned objects.\n4. Create 'FP' events for all remaining unassigned hypotheses.\n\nParams\n------\noids : N array\n    Array of object ids.\nhids : M array\n    Array of hypothesis ids.\ndists: NxM array\n    Distance matrix. np.nan values to signal do-not-pair constellations.\n    See `distances` module for support methods.\n\nKwargs\n------\nframeId : id\n    Unique frame id. Optional when MOTAccumulator.auto_id is specified during\n    construction.\nvf: file to log details\nReturns\n-------\nframe_events : pd.DataFrame\n    Dataframe containing generated events\n\nReferences\n----------\n1. Bernardin, Keni, and Rainer Stiefelhagen. \"Evaluating multiple object tracking performance: the CLEAR MOT metrics.\"\nEURASIP Journal on Image and Video Processing 2008.1 (2008): 1-10.",
          "signature": "def update(self, oids, hids, dists, frameid=None, vf='', similartiy_matrix=None, th=None):",
          "type": "Method",
          "class_signature": "class MOTAccumulator(object):"
        }
      },
      "motmetrics/utils.py": {
        "compute_euc": {
          "code": "    def compute_euc(a, b):\n        return norm2squared_matrix(a, b, max_d2=distth)",
          "docstring": "",
          "signature": "def compute_euc(a, b):",
          "type": "Function",
          "class_signature": null
        }
      }
    },
    "motmetrics/metrics.py:create": {
      "motmetrics/metrics.py": {
        "MetricsHost.__init__": {
          "code": "    def __init__(self):\n        self.metrics = OrderedDict()",
          "docstring": "",
          "signature": "def __init__(self):",
          "type": "Method",
          "class_signature": "class MetricsHost:"
        },
        "MetricsHost.register": {
          "code": "    def register(self, fnc, deps='auto', name=None, helpstr=None, formatter=None, fnc_m=None, deps_m='auto'):\n        \"\"\"Register a new metric.\n\n        Params\n        ------\n        fnc : Function\n            Function that computes the metric to be registered. The number of arguments\n            is 1 + N, where N is the number of dependencies of the metric to be registered.\n            The order of the argument passed is `df, result_dep1, result_dep2, ...`.\n\n        Kwargs\n        ------\n        deps : string, list of strings or None, optional\n            The dependencies of this metric. Each dependency is evaluated and the result\n            is passed as argument to `fnc` as described above. If None is specified, the\n            function does not have any dependencies. If a list of strings is given, dependencies\n            for these metric strings are registered. If 'auto' is passed, the dependencies\n            are deduced from argument inspection of the method. For this to work the argument\n            names have to be equal to the intended dependencies.\n        name : string or None, optional\n            Name identifier of this metric. If None is passed the name is deduced from\n            function inspection.\n        helpstr : string or None, optional\n            A description of what the metric computes. If no help message is given it\n            is deduced from the docstring of the function.\n        formatter: Format object, optional\n            An optional default formatter when rendering metric results as string. I.e to\n            render the result `0.35` as `35%` one would pass `{:.2%}.format`\n        fnc_m : Function or None, optional\n            Function that merges metric results. The number of arguments\n            is 1 + N, where N is the number of dependencies of the metric to be registered.\n            The order of the argument passed is `df, result_dep1, result_dep2, ...`.\n        \"\"\"\n        assert fnc is not None, 'No function given for metric {}'.format(name)\n        if deps is None:\n            deps = []\n        elif deps == 'auto':\n            if _getargspec(fnc).defaults is not None:\n                k = -len(_getargspec(fnc).defaults)\n            else:\n                k = len(_getargspec(fnc).args)\n            deps = _getargspec(fnc).args[1:k]\n        if name is None:\n            name = fnc.__name__\n        if helpstr is None:\n            helpstr = inspect.getdoc(fnc) if inspect.getdoc(fnc) else 'No description.'\n            helpstr = ' '.join(helpstr.split())\n        if fnc_m is None and name + '_m' in globals():\n            fnc_m = globals()[name + '_m']\n        if fnc_m is not None:\n            if deps_m is None:\n                deps_m = []\n            elif deps_m == 'auto':\n                if _getargspec(fnc_m).defaults is not None:\n                    k = -len(_getargspec(fnc_m).defaults)\n                else:\n                    k = len(_getargspec(fnc_m).args)\n                deps_m = _getargspec(fnc_m).args[1:k]\n        else:\n            deps_m = None\n        self.metrics[name] = {'name': name, 'fnc': fnc, 'fnc_m': fnc_m, 'deps': deps, 'deps_m': deps_m, 'help': helpstr, 'formatter': formatter}",
          "docstring": "Register a new metric.\n\nParams\n------\nfnc : Function\n    Function that computes the metric to be registered. The number of arguments\n    is 1 + N, where N is the number of dependencies of the metric to be registered.\n    The order of the argument passed is `df, result_dep1, result_dep2, ...`.\n\nKwargs\n------\ndeps : string, list of strings or None, optional\n    The dependencies of this metric. Each dependency is evaluated and the result\n    is passed as argument to `fnc` as described above. If None is specified, the\n    function does not have any dependencies. If a list of strings is given, dependencies\n    for these metric strings are registered. If 'auto' is passed, the dependencies\n    are deduced from argument inspection of the method. For this to work the argument\n    names have to be equal to the intended dependencies.\nname : string or None, optional\n    Name identifier of this metric. If None is passed the name is deduced from\n    function inspection.\nhelpstr : string or None, optional\n    A description of what the metric computes. If no help message is given it\n    is deduced from the docstring of the function.\nformatter: Format object, optional\n    An optional default formatter when rendering metric results as string. I.e to\n    render the result `0.35` as `35%` one would pass `{:.2%}.format`\nfnc_m : Function or None, optional\n    Function that merges metric results. The number of arguments\n    is 1 + N, where N is the number of dependencies of the metric to be registered.\n    The order of the argument passed is `df, result_dep1, result_dep2, ...`.",
          "signature": "def register(self, fnc, deps='auto', name=None, helpstr=None, formatter=None, fnc_m=None, deps_m='auto'):",
          "type": "Method",
          "class_signature": "class MetricsHost:"
        }
      }
    },
    "motmetrics/metrics.py:MetricsHost:compute": {
      "motmetrics/metrics.py": {
        "MetricsHost._compute": {
          "code": "    def _compute(self, df_map, name, cache, options, parent=None):\n        \"\"\"Compute metric and resolve dependencies.\"\"\"\n        assert name in self.metrics, 'Cannot find metric {} required by {}.'.format(name, parent)\n        already = cache.get(name, None)\n        if already is not None:\n            return already\n        minfo = self.metrics[name]\n        vals = []\n        for depname in minfo['deps']:\n            v = cache.get(depname, None)\n            if v is None:\n                v = cache[depname] = self._compute(df_map, depname, cache, options, parent=name)\n            vals.append(v)\n        if _getargspec(minfo['fnc']).defaults is None:\n            return minfo['fnc'](df_map, *vals)\n        else:\n            return minfo['fnc'](df_map, *vals, **options)",
          "docstring": "Compute metric and resolve dependencies.",
          "signature": "def _compute(self, df_map, name, cache, options, parent=None):",
          "type": "Method",
          "class_signature": "class MetricsHost:"
        },
        "events_to_df_map": {
          "code": "def events_to_df_map(df):\n    raw = df[df.Type == 'RAW']\n    noraw = df[(df.Type != 'RAW') & (df.Type != 'ASCEND') & (df.Type != 'TRANSFER') & (df.Type != 'MIGRATE')]\n    extra = df[df.Type != 'RAW']\n    df_map = DataFrameMap(full=df, raw=raw, noraw=noraw, extra=extra)\n    return df_map",
          "docstring": "",
          "signature": "def events_to_df_map(df):",
          "type": "Function",
          "class_signature": null
        }
      },
      "motmetrics/mot.py": {
        "MOTAccumulator.events": {
          "code": "    def events(self):\n        if self.dirty_events:\n            self.cached_events_df = MOTAccumulator.new_event_dataframe_with_data(self._indices, self._events)\n            self.dirty_events = False\n        return self.cached_events_df",
          "docstring": "",
          "signature": "def events(self):",
          "type": "Method",
          "class_signature": "class MOTAccumulator(object):"
        }
      }
    }
  },
  "call_tree": {
    "modified_testcases/test_utils.py:test_annotations_xor_predictions_present": {
      "modified_testcases/test_utils.py:_tracks_to_dataframe": {},
      "motmetrics/utils.py:compare_to_groundtruth": {
        "motmetrics/mot.py:MOTAccumulator:__init__": {
          "motmetrics/mot.py:MOTAccumulator:reset": {}
        },
        "motmetrics/mot.py:MOTAccumulator:update": {
          "motmetrics/mot.py:MOTAccumulator:_append_to_indices": {},
          "motmetrics/mot.py:MOTAccumulator:_append_to_events": {},
          "motmetrics/lap.py:linear_sum_assignment": {
            "motmetrics/lap.py:lsa_solve_scipy": {
              "motmetrics/lap.py:add_expensive_edges": {},
              "motmetrics/lap.py:_exclude_missing_edges": {}
            }
          }
        },
        "motmetrics/utils.py:compute_euc": {
          "motmetrics/distances.py:norm2squared_matrix": {}
        }
      },
      "motmetrics/metrics.py:create": {
        "motmetrics/metrics.py:MetricsHost:__init__": {},
        "motmetrics/metrics.py:MetricsHost:register": {}
      },
      "motmetrics/metrics.py:MetricsHost:compute": {
        "motmetrics/mot.py:MOTAccumulator:MOTAccumulator": {},
        "motmetrics/mot.py:MOTAccumulator:events": {
          "motmetrics/mot.py:MOTAccumulator:new_event_dataframe_with_data": {}
        },
        "motmetrics/metrics.py:events_to_df_map": {
          "motmetrics/metrics.py:DataFrameMap:__init__": {}
        },
        "motmetrics/metrics.py:MetricsHost:_compute": {
          "motmetrics/metrics.py:MetricsHost:_compute": {
            "[ignored_or_cut_off]": "..."
          },
          "motmetrics/metrics.py:num_objects": {},
          "motmetrics/metrics.py:num_predictions": {},
          "motmetrics/metrics.py:num_unique_objects": {}
        }
      }
    }
  },
  "PRD": "# PROJECT NAME: motmetrics-test_utils\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 motmetrics/\n    \u251c\u2500\u2500 metrics.py\n    \u2502   \u251c\u2500\u2500 MetricsHost.compute\n    \u2502   \u2514\u2500\u2500 create\n    \u2514\u2500\u2500 utils.py\n        \u2514\u2500\u2500 compare_to_groundtruth\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThe module supports the evaluation and benchmarking of multiple object tracking (MOT) systems by providing robust functionality for comparing object tracking predictions against ground truth annotations. It enables the accumulation and analysis of MOT-related metrics, such as the number of objects, predictions, and unique objects, using distance thresholds and customizable comparison methods. By transforming tracking data into a standardized format and facilitating the calculation of performance metrics, the module addresses the need for precise, reproducible MOT performance evaluation, aiding developers and researchers in assessing tracker accuracy and efficiency. This ensures actionable insights into tracker performance across different datasets and scenarios.\n\n## FILE 1: motmetrics/metrics.py\n\n- FUNCTION NAME: create\n  - SIGNATURE: def create():\n  - DOCSTRING: \n```python\n\"\"\"\nCreates and initializes an instance of the `MetricsHost` class, populating it with a predefined set of metrics related to multiple object tracking performance. This function registers various metrics such as the number of frames, false positives, unique objects, and several accuracy metrics related to object tracking.\n\nParameters\n----------\nNone\n\nReturns\n-------\nMetricsHost\n    An instance of the `MetricsHost` class populated with default metrics used for evaluating tracking performance.\n\nDependencies\n------------\n- `MetricsHost`: A class that manages the registration and computation of metrics.\n- Several metric functions such as `num_frames`, `obj_frequencies`, `num_matches`, etc., which provide specific calculations for performance evaluation.\n- Formatting options for displaying the metrics, for example, using \"{:d}\".format for integer outputs and \"{:.1%}\".format for percentage outputs.\n\nConstants\n---------\n- `motchallenge_metrics`: A predefined list of metric identifiers that include various accuracy measures like `idf1`, `idp`, `idr`, etc. This list is used for specifying which metrics to compute during evaluation.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - motmetrics/metrics.py:MetricsHost:register\n    - motmetrics/metrics.py:MetricsHost:__init__\n\n- CLASS METHOD: MetricsHost.compute\n  - CLASS SIGNATURE: class MetricsHost:\n  - SIGNATURE: def compute(self, df, ana=None, metrics=None, return_dataframe=True, return_cached=False, name=None):\n  - DOCSTRING: \n```python\n\"\"\"\nCompute metrics on a given dataframe or MOTAccumulator.\n\nThis method takes a dataframe or an MOTAccumulator containing event data, computes specified metrics (or all registered metrics if none are specified), and returns the results in the desired format (either as a pandas DataFrame or a dictionary). The method handles dependencies between metrics automatically, caching results for efficiency.\n\nParameters\n----------\ndf : MOTAccumulator or pandas.DataFrame\n    The dataframe or accumulator to compute the metrics on.\nana : dict or None, optional\n    A cache for fast computation of results.\nmetrics : string, list of string or None, optional\n    Identifiers for the metrics to compute. If None, all registered metrics are computed.\nreturn_dataframe : bool, optional\n    If True, returns the results as a pandas DataFrame (default); otherwise, returns a dictionary.\nreturn_cached : bool, optional\n    If True, all intermediate metrics required to compute the desired metrics are returned as well.\nname : string, optional\n    The index of the row containing the computed metric values when returning a DataFrame.\n\nReturns\n-------\npd.DataFrame or dict\n    If return_dataframe is True, returns a DataFrame with the computed metrics; otherwise, returns a dictionary with metric names as keys and computed values as values.\n\nNotes\n-----\nIf `df` is an instance of MOTAccumulator, its `events` attribute is used for metric computation. The `events_to_df_map` function is utilized to categorize events into different dataframes (full, raw, noraw, and extra) for metric calculations. The `motchallenge_metrics` constant lists all metrics registered for computation when no specific metrics are provided.\n\"\"\"\n```\n\n## FILE 2: motmetrics/utils.py\n\n- FUNCTION NAME: compare_to_groundtruth\n  - SIGNATURE: def compare_to_groundtruth(gt, dt, dist='iou', distfields=None, distth=0.5):\n  - DOCSTRING: \n```python\n\"\"\"\nCompare groundtruth and detector results using specified distance metrics.\n\nThis function evaluates the performance of a multi-object tracker by comparing its output (detector results) against groundtruth data. It uses various distance measures to compute the similarity between detected objects and groundtruth objects based on provided spatial fields (defaulting to 'X', 'Y', 'Width', and 'Height'). The function is particularly helpful in estimating tracking accuracy and identifying potential false positives or negatives.\n\nParameters\n----------\ngt : pd.DataFrame\n    The groundtruth data containing information about the actual tracked objects, which must include 'FrameId' and 'Id' as indices.\ndt : pd.DataFrame\n    The detection results from the tracker, following the same structure as the groundtruth DataFrame.\ndist : str, optional\n    The distance measure to use for comparisons. Acceptable values are 'iou' (Intersection over Union), 'euc' (Euclidean), and 'seuc' (Squared Euclidean). Defaults to 'iou'.\ndistfields : array, optional\n    Relevant spatial fields for calculating distance. Defaults to ['X', 'Y', 'Width', 'Height'].\ndistth : float, optional\n    The maximum tolerable distance for considering a detection valid, dictating the robustness of matches. Defaults to 0.5.\n\nReturns\n-------\nMOTAccumulator\n    An instance of MOTAccumulator containing the recorded performance metrics based on the computed distances.\n\nNotes\n-----\nThe function identifies the union of all frame IDs present in both groundtruth and detection data, ensuring that missing frames are appropriately accounted for as false positives or negatives. The computed distance matrices leverage the `iou_matrix` and `norm2squared_matrix` functions imported from the `motmetrics` library, highlighting its dependency on these distance calculation implementations.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - motmetrics/mot.py:MOTAccumulator:__init__\n    - motmetrics/utils.py:compute_euc\n    - motmetrics/mot.py:MOTAccumulator:update\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "motmetrics/metrics.py": "\"\"\"Obtain metrics from event logs.\"\"\"\nfrom __future__ import absolute_import, division, print_function\nimport inspect\nimport logging\nimport time\nfrom collections import OrderedDict\nimport numpy as np\nimport pandas as pd\nfrom motmetrics import math_util\nfrom motmetrics.lap import linear_sum_assignment\nfrom motmetrics.mot import MOTAccumulator\ntry:\n    _getargspec = inspect.getfullargspec\nexcept AttributeError:\n    _getargspec = inspect.getargspec\n\nclass MetricsHost:\n    \"\"\"Keeps track of metrics and intra metric dependencies.\"\"\"\n\n    def __init__(self):\n        self.metrics = OrderedDict()\n\n    def register(self, fnc, deps='auto', name=None, helpstr=None, formatter=None, fnc_m=None, deps_m='auto'):\n        \"\"\"Register a new metric.\n\n        Params\n        ------\n        fnc : Function\n            Function that computes the metric to be registered. The number of arguments\n            is 1 + N, where N is the number of dependencies of the metric to be registered.\n            The order of the argument passed is `df, result_dep1, result_dep2, ...`.\n\n        Kwargs\n        ------\n        deps : string, list of strings or None, optional\n            The dependencies of this metric. Each dependency is evaluated and the result\n            is passed as argument to `fnc` as described above. If None is specified, the\n            function does not have any dependencies. If a list of strings is given, dependencies\n            for these metric strings are registered. If 'auto' is passed, the dependencies\n            are deduced from argument inspection of the method. For this to work the argument\n            names have to be equal to the intended dependencies.\n        name : string or None, optional\n            Name identifier of this metric. If None is passed the name is deduced from\n            function inspection.\n        helpstr : string or None, optional\n            A description of what the metric computes. If no help message is given it\n            is deduced from the docstring of the function.\n        formatter: Format object, optional\n            An optional default formatter when rendering metric results as string. I.e to\n            render the result `0.35` as `35%` one would pass `{:.2%}.format`\n        fnc_m : Function or None, optional\n            Function that merges metric results. The number of arguments\n            is 1 + N, where N is the number of dependencies of the metric to be registered.\n            The order of the argument passed is `df, result_dep1, result_dep2, ...`.\n        \"\"\"\n        assert fnc is not None, 'No function given for metric {}'.format(name)\n        if deps is None:\n            deps = []\n        elif deps == 'auto':\n            if _getargspec(fnc).defaults is not None:\n                k = -len(_getargspec(fnc).defaults)\n            else:\n                k = len(_getargspec(fnc).args)\n            deps = _getargspec(fnc).args[1:k]\n        if name is None:\n            name = fnc.__name__\n        if helpstr is None:\n            helpstr = inspect.getdoc(fnc) if inspect.getdoc(fnc) else 'No description.'\n            helpstr = ' '.join(helpstr.split())\n        if fnc_m is None and name + '_m' in globals():\n            fnc_m = globals()[name + '_m']\n        if fnc_m is not None:\n            if deps_m is None:\n                deps_m = []\n            elif deps_m == 'auto':\n                if _getargspec(fnc_m).defaults is not None:\n                    k = -len(_getargspec(fnc_m).defaults)\n                else:\n                    k = len(_getargspec(fnc_m).args)\n                deps_m = _getargspec(fnc_m).args[1:k]\n        else:\n            deps_m = None\n        self.metrics[name] = {'name': name, 'fnc': fnc, 'fnc_m': fnc_m, 'deps': deps, 'deps_m': deps_m, 'help': helpstr, 'formatter': formatter}\n\n    @property\n    def names(self):\n        \"\"\"Returns the name identifiers of all registered metrics.\"\"\"\n        return [v['name'] for v in self.metrics.values()]\n\n    @property\n    def formatters(self):\n        \"\"\"Returns the formatters for all metrics that have associated formatters.\"\"\"\n        return {k: v['formatter'] for k, v in self.metrics.items() if v['formatter'] is not None}\n\n    def list_metrics(self, include_deps=False):\n        \"\"\"Returns a dataframe containing names, descriptions and optionally dependencies for each metric.\"\"\"\n        cols = ['Name', 'Description', 'Dependencies']\n        if include_deps:\n            data = [(m['name'], m['help'], m['deps']) for m in self.metrics.values()]\n        else:\n            data = [(m['name'], m['help']) for m in self.metrics.values()]\n            cols = cols[:-1]\n        return pd.DataFrame(data, columns=cols)\n\n    def list_metrics_markdown(self, include_deps=False):\n        \"\"\"Returns a markdown ready version of `list_metrics`.\"\"\"\n        df = self.list_metrics(include_deps=include_deps)\n        fmt = [':---' for i in range(len(df.columns))]\n        df_fmt = pd.DataFrame([fmt], columns=df.columns)\n        df_formatted = pd.concat([df_fmt, df])\n        return df_formatted.to_csv(sep='|', index=False)\n\n    def compute_overall(self, partials, metrics=None, return_dataframe=True, return_cached=False, name=None):\n        \"\"\"Compute overall metrics based on multiple results.\n\n        Params\n        ------\n        partials : list of metric results to combine overall\n\n        Kwargs\n        ------\n        metrics : string, list of string or None, optional\n            The identifiers of the metrics to be computed. This method will only\n            compute the minimal set of necessary metrics to fullfill the request.\n            If None is passed all registered metrics are computed.\n        return_dataframe : bool, optional\n            Return the result as pandas.DataFrame (default) or dict.\n        return_cached : bool, optional\n           If true all intermediate metrics required to compute the desired metrics are returned as well.\n        name : string, optional\n            When returning a pandas.DataFrame this is the index of the row containing\n            the computed metric values.\n\n        Returns\n        -------\n        df : pandas.DataFrame\n            A datafrom containing the metrics in columns and names in rows.\n        \"\"\"\n        if metrics is None:\n            metrics = motchallenge_metrics\n        elif isinstance(metrics, str):\n            metrics = [metrics]\n        cache = {}\n        for mname in metrics:\n            cache[mname] = self._compute_overall(partials, mname, cache, parent='summarize')\n        if name is None:\n            name = 0\n        if return_cached:\n            data = cache\n        else:\n            data = OrderedDict([(k, cache[k]) for k in metrics])\n        return pd.DataFrame(data, index=[name]) if return_dataframe else data\n\n    def compute_many(self, dfs, anas=None, metrics=None, names=None, generate_overall=False):\n        \"\"\"Compute metrics on multiple dataframe / accumulators.\n\n        Params\n        ------\n        dfs : list of MOTAccumulator or list of pandas.DataFrame\n            The data to compute metrics on.\n\n        Kwargs\n        ------\n        anas: dict or None, optional\n            To cache results for fast computation.\n        metrics : string, list of string or None, optional\n            The identifiers of the metrics to be computed. This method will only\n            compute the minimal set of necessary metrics to fullfill the request.\n            If None is passed all registered metrics are computed.\n        names : list of string, optional\n            The names of individual rows in the resulting dataframe.\n        generate_overall : boolean, optional\n            If true resulting dataframe will contain a summary row that is computed\n            using the same metrics over an accumulator that is the concatentation of\n            all input containers. In creating this temporary accumulator, care is taken\n            to offset frame indices avoid object id collisions.\n\n        Returns\n        -------\n        df : pandas.DataFrame\n            A datafrom containing the metrics in columns and names in rows.\n        \"\"\"\n        if metrics is None:\n            metrics = motchallenge_metrics\n        elif isinstance(metrics, str):\n            metrics = [metrics]\n        assert names is None or len(names) == len(dfs)\n        st = time.time()\n        if names is None:\n            names = list(range(len(dfs)))\n        if anas is None:\n            anas = [None] * len(dfs)\n        partials = [self.compute(acc, ana=analysis, metrics=metrics, name=name, return_cached=True, return_dataframe=False) for acc, analysis, name in zip(dfs, anas, names)]\n        logging.info('partials: %.3f seconds.', time.time() - st)\n        details = partials\n        partials = [pd.DataFrame(OrderedDict([(k, i[k]) for k in metrics]), index=[name]) for i, name in zip(partials, names)]\n        if generate_overall:\n            names = 'OVERALL'\n            partials.append(self.compute_overall(details, metrics=metrics, name=names))\n        logging.info('mergeOverall: %.3f seconds.', time.time() - st)\n        return pd.concat(partials)\n\n    def _compute(self, df_map, name, cache, options, parent=None):\n        \"\"\"Compute metric and resolve dependencies.\"\"\"\n        assert name in self.metrics, 'Cannot find metric {} required by {}.'.format(name, parent)\n        already = cache.get(name, None)\n        if already is not None:\n            return already\n        minfo = self.metrics[name]\n        vals = []\n        for depname in minfo['deps']:\n            v = cache.get(depname, None)\n            if v is None:\n                v = cache[depname] = self._compute(df_map, depname, cache, options, parent=name)\n            vals.append(v)\n        if _getargspec(minfo['fnc']).defaults is None:\n            return minfo['fnc'](df_map, *vals)\n        else:\n            return minfo['fnc'](df_map, *vals, **options)\n\n    def _compute_overall(self, partials, name, cache, parent=None):\n        assert name in self.metrics, 'Cannot find metric {} required by {}.'.format(name, parent)\n        already = cache.get(name, None)\n        if already is not None:\n            return already\n        minfo = self.metrics[name]\n        vals = []\n        for depname in minfo['deps_m']:\n            v = cache.get(depname, None)\n            if v is None:\n                v = cache[depname] = self._compute_overall(partials, depname, cache, parent=name)\n            vals.append(v)\n        assert minfo['fnc_m'] is not None, 'merge function for metric %s is None' % name\n        return minfo['fnc_m'](partials, *vals)\nsimple_add_func = []\n\ndef num_frames(df):\n    \"\"\"Total number of frames.\"\"\"\n    return df.full.index.get_level_values(0).unique().shape[0]\nsimple_add_func.append(num_frames)\n\ndef obj_frequencies(df):\n    \"\"\"Total number of occurrences of individual objects over all frames.\"\"\"\n    return df.noraw.OId.value_counts()\n\ndef pred_frequencies(df):\n    \"\"\"Total number of occurrences of individual predictions over all frames.\"\"\"\n    return df.noraw.HId.value_counts()\n\ndef num_unique_objects(df, obj_frequencies):\n    \"\"\"Total number of unique object ids encountered.\"\"\"\n    del df\n    return len(obj_frequencies)\nsimple_add_func.append(num_unique_objects)\n\ndef num_matches(df):\n    \"\"\"Total number matches.\"\"\"\n    return df.noraw.Type.isin(['MATCH']).sum()\nsimple_add_func.append(num_matches)\n\ndef num_switches(df):\n    \"\"\"Total number of track switches.\"\"\"\n    return df.noraw.Type.isin(['SWITCH']).sum()\nsimple_add_func.append(num_switches)\n\ndef num_transfer(df):\n    \"\"\"Total number of track transfer.\"\"\"\n    return df.extra.Type.isin(['TRANSFER']).sum()\nsimple_add_func.append(num_transfer)\n\ndef num_ascend(df):\n    \"\"\"Total number of track ascend.\"\"\"\n    return df.extra.Type.isin(['ASCEND']).sum()\nsimple_add_func.append(num_ascend)\n\ndef num_migrate(df):\n    \"\"\"Total number of track migrate.\"\"\"\n    return df.extra.Type.isin(['MIGRATE']).sum()\nsimple_add_func.append(num_migrate)\n\ndef num_false_positives(df):\n    \"\"\"Total number of false positives (false-alarms).\"\"\"\n    return df.noraw.Type.isin(['FP']).sum()\nsimple_add_func.append(num_false_positives)\n\ndef num_misses(df):\n    \"\"\"Total number of misses.\"\"\"\n    return df.noraw.Type.isin(['MISS']).sum()\nsimple_add_func.append(num_misses)\n\ndef num_detections(df, num_matches, num_switches):\n    \"\"\"Total number of detected objects including matches and switches.\"\"\"\n    del df\n    return num_matches + num_switches\nsimple_add_func.append(num_detections)\n\ndef num_objects(df, obj_frequencies):\n    \"\"\"Total number of unique object appearances over all frames.\"\"\"\n    del df\n    return obj_frequencies.sum()\nsimple_add_func.append(num_objects)\n\ndef num_predictions(df, pred_frequencies):\n    \"\"\"Total number of unique prediction appearances over all frames.\"\"\"\n    del df\n    return pred_frequencies.sum()\nsimple_add_func.append(num_predictions)\n\ndef num_gt_ids(df):\n    \"\"\"Number of unique gt ids.\"\"\"\n    return df.full['OId'].dropna().unique().shape[0]\nsimple_add_func.append(num_gt_ids)\n\ndef num_dt_ids(df):\n    \"\"\"Number of unique dt ids.\"\"\"\n    return df.full['HId'].dropna().unique().shape[0]\nsimple_add_func.append(num_dt_ids)\n\ndef track_ratios(df, obj_frequencies):\n    \"\"\"Ratio of assigned to total appearance count per unique object id.\"\"\"\n    tracked = df.noraw[df.noraw.Type != 'MISS']['OId'].value_counts()\n    return tracked.div(obj_frequencies).fillna(0.0)\n\ndef mostly_tracked(df, track_ratios):\n    \"\"\"Number of objects tracked for at least 80 percent of lifespan.\"\"\"\n    del df\n    return track_ratios[track_ratios >= 0.8].count()\nsimple_add_func.append(mostly_tracked)\n\ndef partially_tracked(df, track_ratios):\n    \"\"\"Number of objects tracked between 20 and 80 percent of lifespan.\"\"\"\n    del df\n    return track_ratios[(track_ratios >= 0.2) & (track_ratios < 0.8)].count()\nsimple_add_func.append(partially_tracked)\n\ndef mostly_lost(df, track_ratios):\n    \"\"\"Number of objects tracked less than 20 percent of lifespan.\"\"\"\n    del df\n    return track_ratios[track_ratios < 0.2].count()\nsimple_add_func.append(mostly_lost)\n\ndef num_fragmentations(df, obj_frequencies):\n    \"\"\"Total number of switches from tracked to not tracked.\"\"\"\n    fra = 0\n    for o in obj_frequencies.index:\n        dfo = df.noraw[df.noraw.OId == o]\n        notmiss = dfo[dfo.Type != 'MISS']\n        if len(notmiss) == 0:\n            continue\n        first = notmiss.index[0]\n        last = notmiss.index[-1]\n        diffs = dfo.loc[first:last].Type.apply(lambda x: 1 if x == 'MISS' else 0).diff()\n        fra += diffs[diffs == 1].count()\n    return fra\nsimple_add_func.append(num_fragmentations)\n\ndef motp(df, num_detections):\n    \"\"\"Multiple object tracker precision.\"\"\"\n    return math_util.quiet_divide(df.noraw['D'].sum(), num_detections)\n\ndef motp_m(partials, num_detections):\n    res = 0\n    for v in partials:\n        res += v['motp'] * v['num_detections']\n    return math_util.quiet_divide(res, num_detections)\n\ndef mota(df, num_misses, num_switches, num_false_positives, num_objects):\n    \"\"\"Multiple object tracker accuracy.\"\"\"\n    del df\n    return 1.0 - math_util.quiet_divide(num_misses + num_switches + num_false_positives, num_objects)\n\ndef mota_m(partials, num_misses, num_switches, num_false_positives, num_objects):\n    del partials\n    return 1.0 - math_util.quiet_divide(num_misses + num_switches + num_false_positives, num_objects)\n\ndef precision(df, num_detections, num_false_positives):\n    \"\"\"Number of detected objects over sum of detected and false positives.\"\"\"\n    del df\n    return math_util.quiet_divide(num_detections, num_false_positives + num_detections)\n\ndef precision_m(partials, num_detections, num_false_positives):\n    del partials\n    return math_util.quiet_divide(num_detections, num_false_positives + num_detections)\n\ndef recall(df, num_detections, num_objects):\n    \"\"\"Number of detections over number of objects.\"\"\"\n    del df\n    return math_util.quiet_divide(num_detections, num_objects)\n\ndef recall_m(partials, num_detections, num_objects):\n    del partials\n    return math_util.quiet_divide(num_detections, num_objects)\n\ndef deta_alpha(df, num_detections, num_objects, num_false_positives):\n    \"\"\"DeTA under specific threshold $\\\\alpha$\n    Source: https://jonathonluiten.medium.com/how-to-evaluate-tracking-with-the-hota-metrics-754036d183e1\n    \"\"\"\n    del df\n    return math_util.quiet_divide(num_detections, max(1, num_objects + num_false_positives))\n\ndef deta_alpha_m(partials):\n    res = 0\n    for v in partials:\n        res += v['deta_alpha']\n    return math_util.quiet_divide(res, len(partials))\n\ndef assa_alpha(df, num_detections, num_gt_ids, num_dt_ids):\n    \"\"\"AssA under specific threshold $\\\\alpha$\n    Source: https://github.com/JonathonLuiten/TrackEval/blob/12c8791b303e0a0b50f753af204249e622d0281a/trackeval/metrics/hota.py#L107-L108\n    \"\"\"\n    max_gt_ids = int(df.noraw.OId.max())\n    max_dt_ids = int(df.noraw.HId.max())\n    match_count_array = np.zeros((max_gt_ids, max_dt_ids))\n    gt_id_counts = np.zeros((max_gt_ids, 1))\n    tracker_id_counts = np.zeros((1, max_dt_ids))\n    for idx in range(len(df.noraw)):\n        oid, hid = (df.noraw.iloc[idx, 1], df.noraw.iloc[idx, 2])\n        if df.noraw.iloc[idx, 0] in ['SWITCH', 'MATCH']:\n            match_count_array[int(oid) - 1, int(hid) - 1] += 1\n        if oid == oid:\n            gt_id_counts[int(oid) - 1] += 1\n        if hid == hid:\n            tracker_id_counts[0, int(hid) - 1] += 1\n    ass_a = match_count_array / np.maximum(1, gt_id_counts + tracker_id_counts - match_count_array)\n    return math_util.quiet_divide((ass_a * match_count_array).sum(), max(1, num_detections))\n\ndef assa_alpha_m(partials):\n    res = 0\n    for v in partials:\n        res += v['assa_alpha']\n    return math_util.quiet_divide(res, len(partials))\n\ndef hota_alpha(df, deta_alpha, assa_alpha):\n    \"\"\"HOTA under specific threshold $\\\\alpha$\"\"\"\n    del df\n    return (deta_alpha * assa_alpha) ** 0.5\n\ndef hota_alpha_m(partials):\n    res = 0\n    for v in partials:\n        res += v['hota_alpha']\n    return math_util.quiet_divide(res, len(partials))\n\nclass DataFrameMap:\n\n    def __init__(self, full, raw, noraw, extra):\n        self.full = full\n        self.raw = raw\n        self.noraw = noraw\n        self.extra = extra\n\ndef events_to_df_map(df):\n    raw = df[df.Type == 'RAW']\n    noraw = df[(df.Type != 'RAW') & (df.Type != 'ASCEND') & (df.Type != 'TRANSFER') & (df.Type != 'MIGRATE')]\n    extra = df[df.Type != 'RAW']\n    df_map = DataFrameMap(full=df, raw=raw, noraw=noraw, extra=extra)\n    return df_map\n\ndef extract_counts_from_df_map(df):\n    \"\"\"\n    Returns:\n        Tuple (ocs, hcs, tps).\n        ocs: Dict from object id to count.\n        hcs: Dict from hypothesis id to count.\n        tps: Dict from (object id, hypothesis id) to true-positive count.\n        The ids are arbitrary, they might NOT be consecutive integers from 0.\n    \"\"\"\n    oids = df.full['OId'].dropna().unique()\n    hids = df.full['HId'].dropna().unique()\n    flat = df.raw.reset_index()\n    flat = flat[flat['OId'].isin(oids) | flat['HId'].isin(hids)]\n    ocs = flat.set_index('OId')['FrameId'].groupby('OId').nunique().to_dict()\n    hcs = flat.set_index('HId')['FrameId'].groupby('HId').nunique().to_dict()\n    dists = flat[['OId', 'HId', 'D']].set_index(['OId', 'HId']).dropna()\n    tps = dists.groupby(['OId', 'HId'])['D'].count().to_dict()\n    return (ocs, hcs, tps)\n\ndef id_global_assignment(df, ana=None):\n    \"\"\"ID measures: Global min-cost assignment for ID measures.\"\"\"\n    del ana\n    ocs, hcs, tps = extract_counts_from_df_map(df)\n    oids = sorted(ocs.keys())\n    hids = sorted(hcs.keys())\n    oids_idx = dict(((o, i) for i, o in enumerate(oids)))\n    hids_idx = dict(((h, i) for i, h in enumerate(hids)))\n    no = len(ocs)\n    nh = len(hcs)\n    fpmatrix = np.full((no + nh, no + nh), 0.0)\n    fnmatrix = np.full((no + nh, no + nh), 0.0)\n    fpmatrix[no:, :nh] = np.nan\n    fnmatrix[:no, nh:] = np.nan\n    for oid, oc in ocs.items():\n        r = oids_idx[oid]\n        fnmatrix[r, :nh] = oc\n        fnmatrix[r, nh + r] = oc\n    for hid, hc in hcs.items():\n        c = hids_idx[hid]\n        fpmatrix[:no, c] = hc\n        fpmatrix[c + no, c] = hc\n    for (oid, hid), ex in tps.items():\n        r = oids_idx[oid]\n        c = hids_idx[hid]\n        fpmatrix[r, c] -= ex\n        fnmatrix[r, c] -= ex\n    costs = fpmatrix + fnmatrix\n    rids, cids = linear_sum_assignment(costs)\n    return {'fpmatrix': fpmatrix, 'fnmatrix': fnmatrix, 'rids': rids, 'cids': cids, 'costs': costs, 'min_cost': costs[rids, cids].sum()}\n\ndef idfp(df, id_global_assignment):\n    \"\"\"ID measures: Number of false positive matches after global min-cost matching.\"\"\"\n    del df\n    rids, cids = (id_global_assignment['rids'], id_global_assignment['cids'])\n    return id_global_assignment['fpmatrix'][rids, cids].sum()\nsimple_add_func.append(idfp)\n\ndef idfn(df, id_global_assignment):\n    \"\"\"ID measures: Number of false negatives matches after global min-cost matching.\"\"\"\n    del df\n    rids, cids = (id_global_assignment['rids'], id_global_assignment['cids'])\n    return id_global_assignment['fnmatrix'][rids, cids].sum()\nsimple_add_func.append(idfn)\n\ndef idtp(df, id_global_assignment, num_objects, idfn):\n    \"\"\"ID measures: Number of true positives matches after global min-cost matching.\"\"\"\n    del df, id_global_assignment\n    return num_objects - idfn\nsimple_add_func.append(idtp)\n\ndef idp(df, idtp, idfp):\n    \"\"\"ID measures: global min-cost precision.\"\"\"\n    del df\n    return math_util.quiet_divide(idtp, idtp + idfp)\n\ndef idp_m(partials, idtp, idfp):\n    del partials\n    return math_util.quiet_divide(idtp, idtp + idfp)\n\ndef idr(df, idtp, idfn):\n    \"\"\"ID measures: global min-cost recall.\"\"\"\n    del df\n    return math_util.quiet_divide(idtp, idtp + idfn)\n\ndef idr_m(partials, idtp, idfn):\n    del partials\n    return math_util.quiet_divide(idtp, idtp + idfn)\n\ndef idf1(df, idtp, num_objects, num_predictions):\n    \"\"\"ID measures: global min-cost F1 score.\"\"\"\n    del df\n    return math_util.quiet_divide(2 * idtp, num_objects + num_predictions)\n\ndef idf1_m(partials, idtp, num_objects, num_predictions):\n    del partials\n    return math_util.quiet_divide(2 * idtp, num_objects + num_predictions)\nfor one in simple_add_func:\n    name = one.__name__\n\n    def getSimpleAdd(nm):\n\n        def simpleAddHolder(partials):\n            res = 0\n            for v in partials:\n                res += v[nm]\n            return res\n        return simpleAddHolder\n    locals()[name + '_m'] = getSimpleAdd(name)\nmotchallenge_metrics = ['idf1', 'idp', 'idr', 'recall', 'precision', 'num_unique_objects', 'mostly_tracked', 'partially_tracked', 'mostly_lost', 'num_false_positives', 'num_misses', 'num_switches', 'num_fragmentations', 'mota', 'motp', 'num_transfer', 'num_ascend', 'num_migrate']\n'A list of all metrics from MOTChallenge.'",
    "motmetrics/utils.py": "\"\"\"Functions for populating event accumulators.\"\"\"\nfrom __future__ import absolute_import, division, print_function\nimport numpy as np\nfrom motmetrics.distances import iou_matrix, norm2squared_matrix\nfrom motmetrics.mot import MOTAccumulator\nfrom motmetrics.preprocess import preprocessResult\n\ndef compute_global_aligment_score(allframeids, fid_to_fgt, fid_to_fdt, num_gt_id, num_det_id, dist_func):\n    \"\"\"Taken from https://github.com/JonathonLuiten/TrackEval/blob/12c8791b303e0a0b50f753af204249e622d0281a/trackeval/metrics/hota.py\"\"\"\n    potential_matches_count = np.zeros((num_gt_id, num_det_id))\n    gt_id_count = np.zeros((num_gt_id, 1))\n    tracker_id_count = np.zeros((1, num_det_id))\n    for fid in allframeids:\n        oids = np.empty(0)\n        hids = np.empty(0)\n        if fid in fid_to_fgt:\n            fgt = fid_to_fgt[fid]\n            oids = fgt.index.get_level_values('Id')\n        if fid in fid_to_fdt:\n            fdt = fid_to_fdt[fid]\n            hids = fdt.index.get_level_values('Id')\n        if len(oids) > 0 and len(hids) > 0:\n            gt_ids = np.array(oids.values) - 1\n            dt_ids = np.array(hids.values) - 1\n            similarity = dist_func(fgt.values, fdt.values, return_dist=False)\n            sim_iou_denom = similarity.sum(0)[np.newaxis, :] + similarity.sum(1)[:, np.newaxis] - similarity\n            sim_iou = np.zeros_like(similarity)\n            sim_iou_mask = sim_iou_denom > 0 + np.finfo('float').eps\n            sim_iou[sim_iou_mask] = similarity[sim_iou_mask] / sim_iou_denom[sim_iou_mask]\n            potential_matches_count[gt_ids[:, np.newaxis], dt_ids[np.newaxis, :]] += sim_iou\n            gt_id_count[gt_ids] += 1\n            tracker_id_count[0, dt_ids] += 1\n    global_alignment_score = potential_matches_count / np.maximum(1, gt_id_count + tracker_id_count - potential_matches_count)\n    return global_alignment_score\n\ndef compare_to_groundtruth_reweighting(gt, dt, dist='iou', distfields=None, distth=0.5):\n    \"\"\"Compare groundtruth and detector results with global alignment score.\n\n    This method assumes both results are given in terms of DataFrames with at least the following fields\n     - `FrameId` First level index used for matching ground-truth and test frames.\n     - `Id` Secondary level index marking available object / hypothesis ids\n\n    Depending on the distance to be used relevant distfields need to be specified.\n\n    Params\n    ------\n    gt : pd.DataFrame\n        Dataframe for ground-truth\n    test : pd.DataFrame\n        Dataframe for detector results\n\n    Kwargs\n    ------\n    dist : str, optional\n        String identifying distance to be used. Defaults to intersection over union ('iou'). Euclidean\n        distance ('euclidean') and squared euclidean distance ('seuc') are also supported.\n    distfields: array, optional\n        Fields relevant for extracting distance information. Defaults to ['X', 'Y', 'Width', 'Height']\n    distth: Union(float, array_like), optional\n        Maximum tolerable distance. Pairs exceeding this threshold are marked 'do-not-pair'.\n        If a list of thresholds is given, multiple accumulators are returned.\n    \"\"\"\n    if distfields is None:\n        distfields = ['X', 'Y', 'Width', 'Height']\n\n    def compute_iou(a, b, return_dist):\n        return iou_matrix(a, b, max_iou=distth, return_dist=return_dist)\n\n    def compute_euc(a, b, *args, **kwargs):\n        return np.sqrt(norm2squared_matrix(a, b, max_d2=distth ** 2))\n\n    def compute_seuc(a, b, *args, **kwargs):\n        return norm2squared_matrix(a, b, max_d2=distth)\n    if dist.upper() == 'IOU':\n        compute_dist = compute_iou\n    elif dist.upper() == 'EUC':\n        compute_dist = compute_euc\n        import warnings\n        warnings.warn(f\"'euc' flag changed its behavior. The euclidean distance is now used instead of the squared euclidean distance. Make sure the used threshold (distth={distth}) is not squared. Use 'euclidean' flag to avoid this warning.\")\n    elif dist.upper() == 'EUCLIDEAN':\n        compute_dist = compute_euc\n    elif dist.upper() == 'SEUC':\n        compute_dist = compute_seuc\n    else:\n        raise f'Unknown distance metric {dist}. Use \"IOU\", \"EUCLIDEAN\",  or \"SEUC\"'\n    return_single = False\n    if isinstance(distth, float):\n        distth = [distth]\n        return_single = True\n    acc_list = [MOTAccumulator() for _ in range(len(distth))]\n    num_gt_id = gt.index.get_level_values('Id').max()\n    num_det_id = dt.index.get_level_values('Id').max()\n    allframeids = gt.index.union(dt.index).levels[0]\n    gt = gt[distfields]\n    dt = dt[distfields]\n    fid_to_fgt = dict(iter(gt.groupby('FrameId')))\n    fid_to_fdt = dict(iter(dt.groupby('FrameId')))\n    global_alignment_score = compute_global_aligment_score(allframeids, fid_to_fgt, fid_to_fdt, num_gt_id, num_det_id, compute_dist)\n    for fid in allframeids:\n        oids = np.empty(0)\n        hids = np.empty(0)\n        weighted_dists = np.empty((0, 0))\n        if fid in fid_to_fgt:\n            fgt = fid_to_fgt[fid]\n            oids = fgt.index.get_level_values('Id')\n        if fid in fid_to_fdt:\n            fdt = fid_to_fdt[fid]\n            hids = fdt.index.get_level_values('Id')\n        if len(oids) > 0 and len(hids) > 0:\n            gt_ids = np.array(oids.values) - 1\n            dt_ids = np.array(hids.values) - 1\n            dists = compute_dist(fgt.values, fdt.values, return_dist=False)\n            weighted_dists = dists * global_alignment_score[gt_ids[:, np.newaxis], dt_ids[np.newaxis, :]]\n        for acc, th in zip(acc_list, distth):\n            acc.update(oids, hids, 1 - weighted_dists, frameid=fid, similartiy_matrix=dists, th=th)\n    return acc_list[0] if return_single else acc_list\n\ndef CLEAR_MOT_M(gt, dt, inifile, dist='iou', distfields=None, distth=0.5, include_all=False, vflag=''):\n    \"\"\"Compare groundtruth and detector results.\n\n    This method assumes both results are given in terms of DataFrames with at least the following fields\n     - `FrameId` First level index used for matching ground-truth and test frames.\n     - `Id` Secondary level index marking available object / hypothesis ids\n\n    Depending on the distance to be used relevant distfields need to be specified.\n\n    Params\n    ------\n    gt : pd.DataFrame\n        Dataframe for ground-truth\n    test : pd.DataFrame\n        Dataframe for detector results\n\n    Kwargs\n    ------\n    dist : str, optional\n        String identifying distance to be used. Defaults to intersection over union.\n    distfields: array, optional\n        Fields relevant for extracting distance information. Defaults to ['X', 'Y', 'Width', 'Height']\n    distth: float, optional\n        Maximum tolerable distance. Pairs exceeding this threshold are marked 'do-not-pair'.\n    \"\"\"\n    if distfields is None:\n        distfields = ['X', 'Y', 'Width', 'Height']\n\n    def compute_iou(a, b):\n        return iou_matrix(a, b, max_iou=distth)\n\n    def compute_euc(a, b):\n        return norm2squared_matrix(a, b, max_d2=distth)\n    compute_dist = compute_iou if dist.upper() == 'IOU' else compute_euc\n    acc = MOTAccumulator()\n    dt = preprocessResult(dt, gt, inifile)\n    if include_all:\n        gt = gt[gt['Confidence'] >= 0.99]\n    else:\n        gt = gt[(gt['Confidence'] >= 0.99) & (gt['ClassId'] == 1)]\n    allframeids = gt.index.union(dt.index).levels[0]\n    analysis = {'hyp': {}, 'obj': {}}\n    for fid in allframeids:\n        oids = np.empty(0)\n        hids = np.empty(0)\n        dists = np.empty((0, 0))\n        if fid in gt.index:\n            fgt = gt.loc[fid]\n            oids = fgt.index.values\n            for oid in oids:\n                oid = int(oid)\n                if oid not in analysis['obj']:\n                    analysis['obj'][oid] = 0\n                analysis['obj'][oid] += 1\n        if fid in dt.index:\n            fdt = dt.loc[fid]\n            hids = fdt.index.values\n            for hid in hids:\n                hid = int(hid)\n                if hid not in analysis['hyp']:\n                    analysis['hyp'][hid] = 0\n                analysis['hyp'][hid] += 1\n        if oids.shape[0] > 0 and hids.shape[0] > 0:\n            dists = compute_dist(fgt[distfields].values, fdt[distfields].values)\n        acc.update(oids, hids, dists, frameid=fid, vf=vflag)\n    return (acc, analysis)"
  }
}