{
  "dir_path": "/app/stestr",
  "package_name": "stestr",
  "sample_name": "stestr-test_run",
  "src_dir": "stestr/",
  "test_dir": "stestr/tests/",
  "test_file": "modified_testcases/test_run.py",
  "test_code": "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nimport io\n\nfrom stestr.commands import run\nfrom stestr.tests import base\n\n\nclass TestRunCommand(base.TestCase):\n    def test_to_int_positive_int(self):\n        self.assertEqual(29, run._to_int(29))\n\n    def test_to_int_positive_int_str(self):\n        self.assertEqual(42, run._to_int(\"42\"))\n\n    def test_to_int_negative_int(self):\n        self.assertEqual(-2, run._to_int(-2))\n\n    def test_to_int_negative_int_str(self):\n        self.assertEqual(-45, run._to_int(\"-45\"))\n\n    def test_to_int_invalid_str(self):\n        fake_stderr = io.StringIO()\n        out = run._to_int(\"I am not an int\", out=fake_stderr)\n        expected = 'Unable to convert \"I am not an int\" to an integer.  ' \"Using 0.\\n\"\n        self.assertEqual(fake_stderr.getvalue(), expected)\n        self.assertEqual(0, out)\n\n    def test_to_int_none(self):\n        fake_stderr = io.StringIO()\n        out = run._to_int(None, out=fake_stderr)\n        expected = 'Unable to convert \"None\" to an integer.  ' \"Using 0.\\n\"\n        self.assertEqual(fake_stderr.getvalue(), expected)\n        self.assertEqual(0, out)\n",
  "GT_file_code": {
    "stestr/commands/run.py": "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\n\"\"\"Run a projects tests and load them into stestr.\"\"\"\n\nimport errno\nimport functools\nimport io\nimport os\nimport os.path\nimport subprocess\nimport sys\n\nfrom cliff import command\nimport subunit\nimport testtools\n\nfrom stestr import bisect_tests\nfrom stestr.commands import load\nfrom stestr.commands import slowest\nfrom stestr import config_file\nfrom stestr import output\nfrom stestr.repository import abstract as repository\nfrom stestr.repository import util\nfrom stestr import results\nfrom stestr.subunit_runner import program\nfrom stestr.subunit_runner import run as subunit_run\nfrom stestr.testlist import parse_list\nfrom stestr import user_config\n\n\ndef _to_int(possible, default=0, out=sys.stderr):\n    try:\n        i = int(possible)\n    except (ValueError, TypeError):\n        i = default\n        msg = 'Unable to convert \"%s\" to an integer.  Using %d.\\n' % (possible, default)\n        out.write(str(msg))\n    return i\n\n\nclass Run(command.Command):\n    \"\"\"Run the tests for a project and store them into the repository.\n\n    Without --subunit, the process exit code will be non-zero if the test\n    run was not successful. However, with --subunit, the process exit code\n    is non-zero only if the subunit stream could not be generated\n    successfully. The test results and run status are included in the\n    subunit stream, so the stream should be used to determining the result\n    of the run instead of the exit code when using the --subunit flag.\n    \"\"\"\n\n    def get_parser(self, prog_name):\n        parser = super().get_parser(prog_name)\n        parser.add_argument(\n            \"filters\",\n            nargs=\"*\",\n            default=None,\n            help=\"A list of string regex filters to initially \"\n            \"apply on the test list. Tests that match any of \"\n            \"the regexes will be used. (assuming any other \"\n            \"filtering specified also uses it)\",\n        )\n        parser.add_argument(\n            \"--failing\",\n            action=\"store_true\",\n            default=False,\n            help=\"Run only tests known to be failing.\",\n        )\n        parser.add_argument(\n            \"--serial\",\n            action=\"store_true\",\n            default=False,\n            help=\"Run tests in a serial process.\",\n        )\n        parser.add_argument(\n            \"--concurrency\",\n            action=\"store\",\n            default=None,\n            type=int,\n            help=\"How many processes to use. The default (0) \"\n            \"autodetects your CPU count.\",\n        )\n        parser.add_argument(\n            \"--load-list\", default=None, help=\"Only run tests listed in the named file.\"\n        ),\n        parser.add_argument(\n            \"--subunit\",\n            action=\"store_true\",\n            default=False,\n            help=\"Display results in subunit format.\",\n        )\n        parser.add_argument(\n            \"--until-failure\",\n            action=\"store_true\",\n            default=False,\n            help=\"Repeat the run again and again until \" \"failure occurs.\",\n        )\n        parser.add_argument(\n            \"--analyze-isolation\",\n            action=\"store_true\",\n            default=False,\n            help=\"Search the last test run for 2-test test \" \"isolation interactions.\",\n        )\n        parser.add_argument(\n            \"--isolated\",\n            action=\"store_true\",\n            default=False,\n            help=\"Run each test id in a separate test runner.\",\n        )\n        parser.add_argument(\n            \"--worker-file\",\n            action=\"store\",\n            default=None,\n            dest=\"worker_path\",\n            help=\"Optional path of a manual worker grouping \" \"file to use for the run\",\n        )\n        parser.add_argument(\n            \"--exclude-list\",\n            \"-e\",\n            default=None,\n            dest=\"exclude_list\",\n            help=\"Path to an exclusion list file, this file \"\n            \"contains a separate regex exclude on each \"\n            \"newline\",\n        )\n        parser.add_argument(\n            \"--include-list\",\n            \"-i\",\n            default=None,\n            dest=\"include_list\",\n            help=\"Path to an inclusion list file, this file \"\n            \"contains a separate regex on each newline.\",\n        )\n        parser.add_argument(\n            \"--exclude-regex\",\n            \"-E\",\n            default=None,\n            dest=\"exclude_regex\",\n            help=\"Test rejection regex. If a test cases name \"\n            \"matches on re.search() operation , \"\n            \"it will be removed from the final test list. \"\n            \"Effectively the exclusion-regexp is added to \"\n            \"exclusion regexp list, but you do need to edit a \"\n            \"file. The exclusion filtering happens after the \"\n            \"initial safe list selection, which by default is \"\n            \"everything.\",\n        )\n        parser.add_argument(\n            \"--no-discover\",\n            \"-n\",\n            default=None,\n            metavar=\"TEST_ID\",\n            help=\"Takes in a single test to bypasses test \"\n            \"discover and just execute the test specified. A \"\n            \"file may be used in place of a test name.\",\n        )\n        parser.add_argument(\n            \"--pdb\",\n            default=None,\n            metavar=\"TEST_ID\",\n            help=\"Run a single test id with the intent of \"\n            \"using pdb. This does not launch any separate \"\n            \"processes to ensure pdb works as expected. It \"\n            \"will bypass test discovery and just execute the \"\n            \"test specified. A file may be used in place of a \"\n            \"test name.\",\n        )\n        parser.add_argument(\n            \"--random\",\n            action=\"store_true\",\n            default=False,\n            help=\"Randomize the test order after they are \"\n            \"partitioned into separate workers\",\n        )\n        parser.add_argument(\n            \"--combine\",\n            action=\"store_true\",\n            default=False,\n            help=\"Combine the results from the test run with \"\n            \"the last run in the repository\",\n        )\n        parser.add_argument(\n            \"--no-subunit-trace\",\n            action=\"store_true\",\n            default=False,\n            help=\"Disable the default subunit-trace output \" \"filter\",\n        )\n        parser.add_argument(\n            \"--force-subunit-trace\",\n            action=\"store_true\",\n            default=False,\n            help=\"Force subunit-trace output regardless of any\"\n            \"other options or config settings\",\n        )\n        parser.add_argument(\n            \"--color\",\n            action=\"store_true\",\n            default=False,\n            help=\"Enable color output in the subunit-trace \"\n            \"output, if subunit-trace output is enabled. \"\n            \"(this is the default). If subunit-trace is \"\n            \"disable this does nothing.\",\n        )\n        parser.add_argument(\n            \"--slowest\",\n            action=\"store_true\",\n            default=False,\n            help=\"After the test run, print the slowest \" \"tests.\",\n        )\n        parser.add_argument(\n            \"--abbreviate\",\n            action=\"store_true\",\n            dest=\"abbreviate\",\n            help=\"Print one character status for each test\",\n        )\n        parser.add_argument(\n            \"--suppress-attachments\",\n            action=\"store_true\",\n            dest=\"suppress_attachments\",\n            help=\"If set do not print stdout or stderr \"\n            \"attachment contents on a successful test \"\n            \"execution\",\n        )\n        parser.add_argument(\n            \"--all-attachments\",\n            action=\"store_true\",\n            dest=\"all_attachments\",\n            help=\"If set print all text attachment contents on\"\n            \" a successful test execution\",\n        )\n        parser.add_argument(\n            \"--show-binary-attachments\",\n            action=\"store_true\",\n            dest=\"show_binary_attachments\",\n            help=\"If set, show non-text attachments. This is \"\n            \"generally only useful for debug purposes.\",\n        )\n        return parser\n\n    def take_action(self, parsed_args):\n        user_conf = user_config.get_user_config(self.app_args.user_config)\n        filters = parsed_args.filters\n        args = parsed_args\n        if args.suppress_attachments and args.all_attachments:\n            msg = (\n                \"The --suppress-attachments and --all-attachments \"\n                \"options are mutually exclusive, you can not use both \"\n                \"at the same time\"\n            )\n            print(msg)\n            sys.exit(1)\n        if getattr(user_conf, \"run\", False):\n            if not user_conf.run.get(\"no-subunit-trace\"):\n                if not args.no_subunit_trace:\n                    pretty_out = True\n                else:\n                    pretty_out = False\n            else:\n                pretty_out = False\n\n            pretty_out = args.force_subunit_trace or pretty_out\n            if args.concurrency is None:\n                concurrency = user_conf.run.get(\"concurrency\", 0)\n            else:\n                concurrency = args.concurrency\n            random = args.random or user_conf.run.get(\"random\", False)\n            color = args.color or user_conf.run.get(\"color\", False)\n            abbreviate = args.abbreviate or user_conf.run.get(\"abbreviate\", False)\n            suppress_attachments_conf = user_conf.run.get(\"suppress-attachments\", False)\n            all_attachments_conf = user_conf.run.get(\"all-attachments\", False)\n            if not args.suppress_attachments and not args.all_attachments:\n                suppress_attachments = suppress_attachments_conf\n                all_attachments = all_attachments_conf\n            elif args.suppress_attachments:\n                all_attachments = False\n                suppress_attachments = args.suppress_attachments\n            elif args.all_attachments:\n                suppress_attachments = False\n                all_attachments = args.all_attachments\n        else:\n            pretty_out = args.force_subunit_trace or not args.no_subunit_trace\n            concurrency = args.concurrency or 0\n            random = args.random\n            color = args.color\n            abbreviate = args.abbreviate\n            suppress_attachments = args.suppress_attachments\n            all_attachments = args.all_attachments\n        verbose_level = self.app.options.verbose_level\n        stdout = open(os.devnull, \"w\") if verbose_level == 0 else sys.stdout\n        # Make sure all (python) callers have provided an int()\n        concurrency = _to_int(concurrency)\n        if concurrency and concurrency < 0:\n            msg = (\n                \"The provided concurrency value: %s is not valid. An \"\n                \"integer >= 0 must be used.\\n\" % concurrency\n            )\n            stdout.write(msg)\n            return 2\n        result = run_command(\n            config=self.app_args.config,\n            repo_url=self.app_args.repo_url,\n            test_path=self.app_args.test_path,\n            top_dir=self.app_args.top_dir,\n            group_regex=self.app_args.group_regex,\n            failing=args.failing,\n            serial=args.serial,\n            concurrency=concurrency,\n            load_list=args.load_list,\n            subunit_out=args.subunit,\n            until_failure=args.until_failure,\n            analyze_isolation=args.analyze_isolation,\n            isolated=args.isolated,\n            worker_path=args.worker_path,\n            exclude_list=args.exclude_list,\n            include_list=args.include_list,\n            exclude_regex=args.exclude_regex,\n            no_discover=args.no_discover,\n            random=random,\n            combine=args.combine,\n            filters=filters,\n            pretty_out=pretty_out,\n            color=color,\n            stdout=stdout,\n            abbreviate=abbreviate,\n            suppress_attachments=suppress_attachments,\n            all_attachments=all_attachments,\n            show_binary_attachments=args.show_binary_attachments,\n            pdb=args.pdb,\n        )\n\n        # Always output slowest test info if requested, regardless of other\n        # test run options\n        user_slowest = False\n        if getattr(user_conf, \"run\", False):\n            user_slowest = user_conf.run.get(\"slowest\", False)\n        if args.slowest or user_slowest:\n            slowest.slowest(repo_url=self.app_args.repo_url)\n\n        return result\n\n\ndef _find_failing(repo):\n    run = repo.get_failing()\n    case = run.get_test()\n    ids = []\n\n    def gather_errors(test_dict):\n        if test_dict[\"status\"] == \"fail\":\n            ids.append(test_dict[\"id\"])\n\n    result = testtools.StreamToDict(gather_errors)\n    result.startTestRun()\n    try:\n        case.run(result)\n    finally:\n        result.stopTestRun()\n    return ids\n\n\ndef run_command(\n    config=config_file.TestrConf.DEFAULT_CONFIG_FILENAME,\n    repo_url=None,\n    test_path=None,\n    top_dir=None,\n    group_regex=None,\n    failing=False,\n    serial=False,\n    concurrency=0,\n    load_list=None,\n    subunit_out=False,\n    until_failure=False,\n    analyze_isolation=False,\n    isolated=False,\n    worker_path=None,\n    exclude_list=None,\n    include_list=None,\n    exclude_regex=None,\n    no_discover=False,\n    random=False,\n    combine=False,\n    filters=None,\n    pretty_out=True,\n    color=False,\n    stdout=sys.stdout,\n    abbreviate=False,\n    suppress_attachments=False,\n    all_attachments=False,\n    show_binary_attachments=True,\n    pdb=False,\n):\n    \"\"\"Function to execute the run command\n\n    This function implements the run command. It will run the tests specified\n    in the parameters based on the provided config file and/or arguments\n    specified in the way specified by the arguments. The results will be\n    printed to STDOUT and loaded into the repository.\n\n    :param str config: The path to the stestr config file. Must be a string.\n    :param str repo_url: The url of the repository to use.\n    :param str test_path: Set the test path to use for unittest discovery.\n        If both this and the corresponding config file option are set, this\n        value will be used.\n    :param str top_dir: The top dir to use for unittest discovery. This takes\n        precedence over the value in the config file. (if one is present in\n        the config file)\n    :param str group_regex: Set a group regex to use for grouping tests\n        together in the stestr scheduler. If both this and the corresponding\n        config file option are set this value will be used.\n    :param bool failing: Run only tests known to be failing.\n    :param bool serial: Run tests serially\n    :param int concurrency: \"How many processes to use. The default (0)\n        autodetects your CPU count and uses that.\n    :param str load_list: The path to a list of test_ids. If specified only\n        tests listed in the named file will be run.\n    :param bool subunit_out: Display results in subunit format.\n    :param bool until_failure: Repeat the run again and again until failure\n        occurs.\n    :param bool analyze_isolation: Search the last test run for 2-test test\n        isolation interactions.\n    :param bool isolated: Run each test id in a separate test runner.\n    :param str worker_path: Optional path of a manual worker grouping file\n        to use for the run.\n    :param str exclude_list: Path to an exclusion list file, this file\n        contains a separate regex exclude on each newline.\n    :param str include_list: Path to a inclusion list file, this file\n        contains a separate regex on each newline.\n    :param str exclude_regex: Test rejection regex. If a test cases name\n        matches on re.search() operation, it will be removed from the final\n        test list.\n    :param str no_discover: Takes in a single test_id to bypasses test\n        discover and just execute the test specified. A file name may be used\n        in place of a test name.\n    :param bool random: Randomize the test order after they are partitioned\n        into separate workers\n    :param bool combine: Combine the results from the test run with the\n        last run in the repository\n    :param list filters: A list of string regex filters to initially apply on\n        the test list. Tests that match any of the regexes will be used.\n        (assuming any other filtering specified also uses it)\n    :param bool pretty_out: Use the subunit-trace output filter\n    :param bool color: Enable colorized output in subunit-trace\n    :param file stdout: The file object to write all output to. By default this\n        is sys.stdout\n    :param bool abbreviate: Use abbreviated output if set true\n    :param bool suppress_attachments: When set true attachments subunit_trace\n        will not print attachments on successful test execution.\n    :param bool all_attachments: When set true subunit_trace will print all\n        text attachments on successful test execution.\n    :param bool show_binary_attachments: When set to true, subunit_trace will\n        print binary attachments in addition to text attachments.\n    :param str pdb: Takes in a single test_id to bypasses test\n        discover and just execute the test specified without launching any\n        additional processes. A file name may be used in place of a test name.\n\n    :return return_code: The exit code for the command. 0 for success and > 0\n        for failures.\n    :rtype: int\n    \"\"\"\n    try:\n        repo = util.get_repo_open(repo_url=repo_url)\n    # If a repo is not found, and there a stestr config exists just create it\n    except repository.RepositoryNotFound:\n        conf = config_file.TestrConf.load_from_file(config)\n        if not conf.test_path and not test_path:\n            msg = (\n                \"No config file found and --test-path not specified. \"\n                \"Either create or specify a .stestr.conf, tox.ini, \"\n                \"or pyproject.toml, or use --test-path\"\n            )\n            stdout.write(msg)\n            exit(1)\n        try:\n            repo = util.get_repo_initialise(repo_url=repo_url)\n        except OSError as e:\n            if e.errno != errno.EEXIST:\n                raise\n            repo_path = repo_url or \"./stestr\"\n            stdout.write(\n                \"The specified repository directory %s already \"\n                \"exists. Please check if the repository already \"\n                \"exists or select a different path\\n\" % repo_path\n            )\n            return 1\n\n    combine_id = None\n    concurrency = _to_int(concurrency)\n\n    if concurrency and concurrency < 0:\n        msg = (\n            \"The provided concurrency value: %s is not valid. An integer \"\n            \">= 0 must be used.\\n\" % concurrency\n        )\n        stdout.write(msg)\n        return 2\n    if combine:\n        latest_id = repo.latest_id()\n        combine_id = str(latest_id)\n    if no_discover and pdb:\n        msg = (\n            \"--no-discover and --pdb are mutually exclusive options, \"\n            \"only specify one at a time\"\n        )\n        stdout.write(msg)\n        return 2\n    if pdb and until_failure:\n        msg = (\n            \"pdb mode does not function with the --until-failure flag, \"\n            \"only specify one at a time\"\n        )\n        stdout.write(msg)\n        return 2\n\n    if no_discover:\n        ids = no_discover\n        klass = None\n        if \"::\" in ids:\n            ids, klass = ids.split(\"::\", 1)\n            klass = \".\".join(klass.split(\"::\"))\n        if ids.find(os.path.sep) != -1:\n            root, _ = os.path.splitext(os.path.normpath(ids))\n            ids = \".\".join(root.split(os.path.sep))\n        if klass:\n            ids = f\"{ids}.{klass}\"\n        stestr_python = sys.executable\n        if os.environ.get(\"PYTHON\"):\n            python_bin = os.environ.get(\"PYTHON\")\n        elif stestr_python:\n            python_bin = stestr_python\n        else:\n            raise RuntimeError(\n                \"The Python interpreter was not found and \" \"PYTHON is not set\"\n            )\n        run_cmd = python_bin + \" -m stestr.subunit_runner.run \" + ids\n\n        def run_tests():\n            run_proc = [\n                (\n                    \"subunit\",\n                    output.ReturnCodeToSubunit(\n                        subprocess.Popen(run_cmd, shell=True, stdout=subprocess.PIPE)\n                    ),\n                )\n            ]\n            return load.load(\n                in_streams=run_proc,\n                subunit_out=subunit_out,\n                repo_url=repo_url,\n                run_id=combine_id,\n                pretty_out=pretty_out,\n                color=color,\n                stdout=stdout,\n                abbreviate=abbreviate,\n                suppress_attachments=suppress_attachments,\n                all_attachments=all_attachments,\n                show_binary_attachments=show_binary_attachments,\n            )\n\n        if not until_failure:\n            return run_tests()\n        else:\n            while True:\n                result = run_tests()\n                # If we're using subunit output we want to make sure to check\n                # the result from the repository because load() returns 0\n                # always on subunit output\n                if subunit:\n                    summary = testtools.StreamSummary()\n                    last_run = repo.get_latest_run().get_subunit_stream()\n                    stream = subunit.ByteStreamToStreamResult(last_run)\n                    summary.startTestRun()\n                    try:\n                        stream.run(summary)\n                    finally:\n                        summary.stopTestRun()\n                    if not results.wasSuccessful(summary):\n                        result = 1\n                if result:\n                    return result\n\n    if pdb:\n        ids = pdb\n        klass = None\n        if \"::\" in ids:\n            ids, klass = ids.split(\"::\", 1)\n            klass = \".\".join(klass.split(\"::\"))\n        if ids.find(os.path.sep) != -1:\n            root, _ = os.path.splitext(os.path.normpath(ids))\n            ids = \".\".join(root.split(os.path.sep))\n        if klass:\n            ids = f\"{ids}.{klass}\"\n        runner = subunit_run.SubunitTestRunner\n        stream = io.BytesIO()\n        program.TestProgram(\n            module=None,\n            argv=[\"stestr\", ids],\n            testRunner=functools.partial(runner, stdout=stream),\n        )\n        stream.seek(0)\n        run_proc = [(\"subunit\", stream)]\n        return load.load(\n            in_streams=run_proc,\n            subunit_out=subunit_out,\n            repo_url=repo_url,\n            run_id=combine_id,\n            pretty_out=pretty_out,\n            color=color,\n            stdout=stdout,\n            abbreviate=abbreviate,\n            suppress_attachments=suppress_attachments,\n            all_attachments=all_attachments,\n            show_binary_attachments=show_binary_attachments,\n        )\n\n    if failing or analyze_isolation:\n        ids = _find_failing(repo)\n    else:\n        ids = None\n    if load_list:\n        list_ids = set()\n        # Should perhaps be text.. currently does its own decode.\n        with open(load_list, \"rb\") as list_file:\n            list_ids = set(parse_list(list_file.read()))\n        if ids is None:\n            # Use the supplied list verbatim\n            ids = list_ids\n        else:\n            # We have some already limited set of ids, just reduce to ids\n            # that are both failing and listed.\n            ids = list_ids.intersection(ids)\n\n    conf = config_file.TestrConf.load_from_file(config)\n    if not analyze_isolation:\n        cmd = conf.get_run_command(\n            ids,\n            regexes=filters,\n            group_regex=group_regex,\n            repo_url=repo_url,\n            serial=serial,\n            worker_path=worker_path,\n            concurrency=concurrency,\n            exclude_list=exclude_list,\n            include_list=include_list,\n            exclude_regex=exclude_regex,\n            top_dir=top_dir,\n            test_path=test_path,\n            randomize=random,\n        )\n        if isolated:\n            result = 0\n            cmd.setUp()\n            try:\n                ids = cmd.list_tests()\n            finally:\n                cmd.cleanUp()\n            for test_id in ids:\n                # TODO(mtreinish): add regex\n                cmd = conf.get_run_command(\n                    [test_id],\n                    filters,\n                    group_regex=group_regex,\n                    repo_url=repo_url,\n                    serial=serial,\n                    worker_path=worker_path,\n                    concurrency=concurrency,\n                    exclude_list=exclude_list,\n                    include_list=include_list,\n                    exclude_regex=exclude_regex,\n                    randomize=random,\n                    test_path=test_path,\n                    top_dir=top_dir,\n                )\n\n                run_result = _run_tests(\n                    cmd,\n                    until_failure,\n                    subunit_out=subunit_out,\n                    combine_id=combine_id,\n                    repo_url=repo_url,\n                    pretty_out=pretty_out,\n                    color=color,\n                    abbreviate=abbreviate,\n                    stdout=stdout,\n                    suppress_attachments=suppress_attachments,\n                    all_attachments=all_attachments,\n                    show_binary_attachments=show_binary_attachments,\n                )\n                if run_result > result:\n                    result = run_result\n            return result\n        else:\n            return _run_tests(\n                cmd,\n                until_failure,\n                subunit_out=subunit_out,\n                combine_id=combine_id,\n                repo_url=repo_url,\n                pretty_out=pretty_out,\n                color=color,\n                stdout=stdout,\n                abbreviate=abbreviate,\n                suppress_attachments=suppress_attachments,\n                all_attachments=all_attachments,\n                show_binary_attachments=show_binary_attachments,\n            )\n    else:\n        # Where do we source data about the cause of conflicts.\n        latest_run = repo.get_latest_run()\n        # Stage one: reduce the list of failing tests (possibly further\n        # reduced by testfilters) to eliminate fails-on-own tests.\n        spurious_failures = set()\n        for test_id in ids:\n            # TODO(mtrienish): Add regex\n            cmd = conf.get_run_command(\n                [test_id],\n                group_regex=group_regex,\n                repo_url=repo_url,\n                serial=serial,\n                worker_path=worker_path,\n                concurrency=concurrency,\n                exclude_list=exclude_list,\n                include_list=include_list,\n                exclude_regex=exclude_regex,\n                randomize=random,\n                test_path=test_path,\n                top_dir=top_dir,\n            )\n            if not _run_tests(cmd, until_failure):\n                # If the test was filtered, it won't have been run.\n                if test_id in repo.get_test_ids(repo.latest_id()):\n                    spurious_failures.add(test_id)\n                # This is arguably ugly, why not just tell the system that\n                # a pass here isn't a real pass? [so that when we find a\n                # test that is spuriously failing, we don't forget\n                # that it is actually failing.\n                # Alternatively, perhaps this is a case for data mining:\n                # when a test starts passing, keep a journal, and allow\n                # digging back in time to see that it was a failure,\n                # what it failed with etc...\n                # The current solution is to just let it get marked as\n                # a pass temporarily.\n        if not spurious_failures:\n            # All done.\n            return 0\n        bisect_runner = bisect_tests.IsolationAnalyzer(\n            latest_run,\n            conf,\n            _run_tests,\n            repo,\n            test_path=test_path,\n            top_dir=top_dir,\n            group_regex=group_regex,\n            repo_url=repo_url,\n            serial=serial,\n            concurrency=concurrency,\n        )\n        # spurious-failure -> cause.\n        return bisect_runner.bisect_tests(spurious_failures)\n\n\ndef _run_tests(\n    cmd,\n    until_failure,\n    subunit_out=False,\n    combine_id=None,\n    repo_url=None,\n    pretty_out=True,\n    color=False,\n    stdout=sys.stdout,\n    abbreviate=False,\n    suppress_attachments=False,\n    all_attachments=False,\n    show_binary_attachments=False,\n):\n    \"\"\"Run the tests cmd was parameterised with.\"\"\"\n    cmd.setUp()\n    try:\n\n        def run_tests():\n            run_procs = [\n                (\"subunit\", output.ReturnCodeToSubunit(proc))\n                for proc in cmd.run_tests()\n            ]\n            if not run_procs:\n                stdout.write(\"The specified regex doesn't match with anything\")\n                return 1\n            return load.load(\n                (None, None),\n                in_streams=run_procs,\n                subunit_out=subunit_out,\n                repo_url=repo_url,\n                run_id=combine_id,\n                pretty_out=pretty_out,\n                color=color,\n                stdout=stdout,\n                abbreviate=abbreviate,\n                suppress_attachments=suppress_attachments,\n                all_attachments=all_attachments,\n                show_binary_attachments=show_binary_attachments,\n            )\n\n        if not until_failure:\n            return run_tests()\n        else:\n            while True:\n                result = run_tests()\n                # If we're using subunit output we want to make sure to check\n                # the result from the repository because load() returns 0\n                # always on subunit output\n                if subunit_out:\n                    repo = util.get_repo_open(repo_url=repo_url)\n                    summary = testtools.StreamSummary()\n                    last_run = repo.get_latest_run().get_subunit_stream()\n                    stream = subunit.ByteStreamToStreamResult(last_run)\n                    summary.startTestRun()\n                    try:\n                        stream.run(summary)\n                    finally:\n                        summary.stopTestRun()\n                    if not results.wasSuccessful(summary):\n                        result = 1\n                if result:\n                    return result\n    finally:\n        cmd.cleanUp()\n"
  },
  "GT_src_dict": {
    "stestr/commands/run.py": {
      "_to_int": {
        "code": "def _to_int(possible, default=0, out=sys.stderr):\n    \"\"\"Convert a value to an integer, providing a default in case of failure.\n\nParameters:\n- possible: The value to convert, which can be of any type.\n- default: The default integer value to return if the conversion fails (default is 0).\n- out: The output stream to write error messages (default is sys.stderr).\n\nReturns:\n- int: The converted integer value if successful; otherwise, the default value.\n\nSide Effects:\n- If the conversion fails, an error message is written to the specified output stream describing the failure and indicating the default value used.\n\nThis function is utilized in the `take_action` method of the `Run` class to ensure the provided concurrency setting is a valid integer, defaulting to 0 when necessary.\"\"\"\n    try:\n        i = int(possible)\n    except (ValueError, TypeError):\n        i = default\n        msg = 'Unable to convert \"%s\" to an integer.  Using %d.\\n' % (possible, default)\n        out.write(str(msg))\n    return i",
        "docstring": "Convert a value to an integer, providing a default in case of failure.\n\nParameters:\n- possible: The value to convert, which can be of any type.\n- default: The default integer value to return if the conversion fails (default is 0).\n- out: The output stream to write error messages (default is sys.stderr).\n\nReturns:\n- int: The converted integer value if successful; otherwise, the default value.\n\nSide Effects:\n- If the conversion fails, an error message is written to the specified output stream describing the failure and indicating the default value used.\n\nThis function is utilized in the `take_action` method of the `Run` class to ensure the provided concurrency setting is a valid integer, defaulting to 0 when necessary.",
        "signature": "def _to_int(possible, default=0, out=sys.stderr):",
        "type": "Function",
        "class_signature": null
      }
    }
  },
  "dependency_dict": {},
  "PRD": "# PROJECT NAME: stestr-test_run\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 stestr/\n    \u2514\u2500\u2500 commands/\n        \u2514\u2500\u2500 run.py\n            \u2514\u2500\u2500 _to_int\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThe module facilitates the validation and conversion of input data into integer values for use within the broader context of a system that executes and manages testing workflows. Its core functionality ensures that input values, including strings and `None` types, are safely converted into integers, providing default handling and user feedback in cases where conversion is not possible. By standardizing input formats and addressing potential conversion errors, the module enhances system reliability, minimizes runtime issues, and simplifies data processing for developers and automated testing processes. This ensures that testing pipelines can operate smoothly, even when faced with invalid or unexpected input data.\n\n## FILE 1: stestr/commands/run.py\n\n- FUNCTION NAME: _to_int\n  - SIGNATURE: def _to_int(possible, default=0, out=sys.stderr):\n  - DOCSTRING: \n```python\n\"\"\"\nConvert a value to an integer, providing a default in case of failure.\n\nParameters:\n- possible: The value to convert, which can be of any type.\n- default: The default integer value to return if the conversion fails (default is 0).\n- out: The output stream to write error messages (default is sys.stderr).\n\nReturns:\n- int: The converted integer value if successful; otherwise, the default value.\n\nSide Effects:\n- If the conversion fails, an error message is written to the specified output stream describing the failure and indicating the default value used.\n\nThis function is utilized in the `take_action` method of the `Run` class to ensure the provided concurrency setting is a valid integer, defaulting to 0 when necessary.\n\"\"\"\n```\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "stestr/commands/run.py": "\"\"\"Run a projects tests and load them into stestr.\"\"\"\nimport errno\nimport functools\nimport io\nimport os\nimport os.path\nimport subprocess\nimport sys\nfrom cliff import command\nimport subunit\nimport testtools\nfrom stestr import bisect_tests\nfrom stestr.commands import load\nfrom stestr.commands import slowest\nfrom stestr import config_file\nfrom stestr import output\nfrom stestr.repository import abstract as repository\nfrom stestr.repository import util\nfrom stestr import results\nfrom stestr.subunit_runner import program\nfrom stestr.subunit_runner import run as subunit_run\nfrom stestr.testlist import parse_list\nfrom stestr import user_config\n\nclass Run(command.Command):\n    \"\"\"Run the tests for a project and store them into the repository.\n\n    Without --subunit, the process exit code will be non-zero if the test\n    run was not successful. However, with --subunit, the process exit code\n    is non-zero only if the subunit stream could not be generated\n    successfully. The test results and run status are included in the\n    subunit stream, so the stream should be used to determining the result\n    of the run instead of the exit code when using the --subunit flag.\n    \"\"\"\n\n    def get_parser(self, prog_name):\n        parser = super().get_parser(prog_name)\n        parser.add_argument('filters', nargs='*', default=None, help='A list of string regex filters to initially apply on the test list. Tests that match any of the regexes will be used. (assuming any other filtering specified also uses it)')\n        parser.add_argument('--failing', action='store_true', default=False, help='Run only tests known to be failing.')\n        parser.add_argument('--serial', action='store_true', default=False, help='Run tests in a serial process.')\n        parser.add_argument('--concurrency', action='store', default=None, type=int, help='How many processes to use. The default (0) autodetects your CPU count.')\n        (parser.add_argument('--load-list', default=None, help='Only run tests listed in the named file.'),)\n        parser.add_argument('--subunit', action='store_true', default=False, help='Display results in subunit format.')\n        parser.add_argument('--until-failure', action='store_true', default=False, help='Repeat the run again and again until failure occurs.')\n        parser.add_argument('--analyze-isolation', action='store_true', default=False, help='Search the last test run for 2-test test isolation interactions.')\n        parser.add_argument('--isolated', action='store_true', default=False, help='Run each test id in a separate test runner.')\n        parser.add_argument('--worker-file', action='store', default=None, dest='worker_path', help='Optional path of a manual worker grouping file to use for the run')\n        parser.add_argument('--exclude-list', '-e', default=None, dest='exclude_list', help='Path to an exclusion list file, this file contains a separate regex exclude on each newline')\n        parser.add_argument('--include-list', '-i', default=None, dest='include_list', help='Path to an inclusion list file, this file contains a separate regex on each newline.')\n        parser.add_argument('--exclude-regex', '-E', default=None, dest='exclude_regex', help='Test rejection regex. If a test cases name matches on re.search() operation , it will be removed from the final test list. Effectively the exclusion-regexp is added to exclusion regexp list, but you do need to edit a file. The exclusion filtering happens after the initial safe list selection, which by default is everything.')\n        parser.add_argument('--no-discover', '-n', default=None, metavar='TEST_ID', help='Takes in a single test to bypasses test discover and just execute the test specified. A file may be used in place of a test name.')\n        parser.add_argument('--pdb', default=None, metavar='TEST_ID', help='Run a single test id with the intent of using pdb. This does not launch any separate processes to ensure pdb works as expected. It will bypass test discovery and just execute the test specified. A file may be used in place of a test name.')\n        parser.add_argument('--random', action='store_true', default=False, help='Randomize the test order after they are partitioned into separate workers')\n        parser.add_argument('--combine', action='store_true', default=False, help='Combine the results from the test run with the last run in the repository')\n        parser.add_argument('--no-subunit-trace', action='store_true', default=False, help='Disable the default subunit-trace output filter')\n        parser.add_argument('--force-subunit-trace', action='store_true', default=False, help='Force subunit-trace output regardless of anyother options or config settings')\n        parser.add_argument('--color', action='store_true', default=False, help='Enable color output in the subunit-trace output, if subunit-trace output is enabled. (this is the default). If subunit-trace is disable this does nothing.')\n        parser.add_argument('--slowest', action='store_true', default=False, help='After the test run, print the slowest tests.')\n        parser.add_argument('--abbreviate', action='store_true', dest='abbreviate', help='Print one character status for each test')\n        parser.add_argument('--suppress-attachments', action='store_true', dest='suppress_attachments', help='If set do not print stdout or stderr attachment contents on a successful test execution')\n        parser.add_argument('--all-attachments', action='store_true', dest='all_attachments', help='If set print all text attachment contents on a successful test execution')\n        parser.add_argument('--show-binary-attachments', action='store_true', dest='show_binary_attachments', help='If set, show non-text attachments. This is generally only useful for debug purposes.')\n        return parser\n\n    def take_action(self, parsed_args):\n        user_conf = user_config.get_user_config(self.app_args.user_config)\n        filters = parsed_args.filters\n        args = parsed_args\n        if args.suppress_attachments and args.all_attachments:\n            msg = 'The --suppress-attachments and --all-attachments options are mutually exclusive, you can not use both at the same time'\n            print(msg)\n            sys.exit(1)\n        if getattr(user_conf, 'run', False):\n            if not user_conf.run.get('no-subunit-trace'):\n                if not args.no_subunit_trace:\n                    pretty_out = True\n                else:\n                    pretty_out = False\n            else:\n                pretty_out = False\n            pretty_out = args.force_subunit_trace or pretty_out\n            if args.concurrency is None:\n                concurrency = user_conf.run.get('concurrency', 0)\n            else:\n                concurrency = args.concurrency\n            random = args.random or user_conf.run.get('random', False)\n            color = args.color or user_conf.run.get('color', False)\n            abbreviate = args.abbreviate or user_conf.run.get('abbreviate', False)\n            suppress_attachments_conf = user_conf.run.get('suppress-attachments', False)\n            all_attachments_conf = user_conf.run.get('all-attachments', False)\n            if not args.suppress_attachments and (not args.all_attachments):\n                suppress_attachments = suppress_attachments_conf\n                all_attachments = all_attachments_conf\n            elif args.suppress_attachments:\n                all_attachments = False\n                suppress_attachments = args.suppress_attachments\n            elif args.all_attachments:\n                suppress_attachments = False\n                all_attachments = args.all_attachments\n        else:\n            pretty_out = args.force_subunit_trace or not args.no_subunit_trace\n            concurrency = args.concurrency or 0\n            random = args.random\n            color = args.color\n            abbreviate = args.abbreviate\n            suppress_attachments = args.suppress_attachments\n            all_attachments = args.all_attachments\n        verbose_level = self.app.options.verbose_level\n        stdout = open(os.devnull, 'w') if verbose_level == 0 else sys.stdout\n        concurrency = _to_int(concurrency)\n        if concurrency and concurrency < 0:\n            msg = 'The provided concurrency value: %s is not valid. An integer >= 0 must be used.\\n' % concurrency\n            stdout.write(msg)\n            return 2\n        result = run_command(config=self.app_args.config, repo_url=self.app_args.repo_url, test_path=self.app_args.test_path, top_dir=self.app_args.top_dir, group_regex=self.app_args.group_regex, failing=args.failing, serial=args.serial, concurrency=concurrency, load_list=args.load_list, subunit_out=args.subunit, until_failure=args.until_failure, analyze_isolation=args.analyze_isolation, isolated=args.isolated, worker_path=args.worker_path, exclude_list=args.exclude_list, include_list=args.include_list, exclude_regex=args.exclude_regex, no_discover=args.no_discover, random=random, combine=args.combine, filters=filters, pretty_out=pretty_out, color=color, stdout=stdout, abbreviate=abbreviate, suppress_attachments=suppress_attachments, all_attachments=all_attachments, show_binary_attachments=args.show_binary_attachments, pdb=args.pdb)\n        user_slowest = False\n        if getattr(user_conf, 'run', False):\n            user_slowest = user_conf.run.get('slowest', False)\n        if args.slowest or user_slowest:\n            slowest.slowest(repo_url=self.app_args.repo_url)\n        return result\n\ndef _find_failing(repo):\n    run = repo.get_failing()\n    case = run.get_test()\n    ids = []\n\n    def gather_errors(test_dict):\n        if test_dict['status'] == 'fail':\n            ids.append(test_dict['id'])\n    result = testtools.StreamToDict(gather_errors)\n    result.startTestRun()\n    try:\n        case.run(result)\n    finally:\n        result.stopTestRun()\n    return ids\n\ndef run_command(config=config_file.TestrConf.DEFAULT_CONFIG_FILENAME, repo_url=None, test_path=None, top_dir=None, group_regex=None, failing=False, serial=False, concurrency=0, load_list=None, subunit_out=False, until_failure=False, analyze_isolation=False, isolated=False, worker_path=None, exclude_list=None, include_list=None, exclude_regex=None, no_discover=False, random=False, combine=False, filters=None, pretty_out=True, color=False, stdout=sys.stdout, abbreviate=False, suppress_attachments=False, all_attachments=False, show_binary_attachments=True, pdb=False):\n    \"\"\"Function to execute the run command\n\n    This function implements the run command. It will run the tests specified\n    in the parameters based on the provided config file and/or arguments\n    specified in the way specified by the arguments. The results will be\n    printed to STDOUT and loaded into the repository.\n\n    :param str config: The path to the stestr config file. Must be a string.\n    :param str repo_url: The url of the repository to use.\n    :param str test_path: Set the test path to use for unittest discovery.\n        If both this and the corresponding config file option are set, this\n        value will be used.\n    :param str top_dir: The top dir to use for unittest discovery. This takes\n        precedence over the value in the config file. (if one is present in\n        the config file)\n    :param str group_regex: Set a group regex to use for grouping tests\n        together in the stestr scheduler. If both this and the corresponding\n        config file option are set this value will be used.\n    :param bool failing: Run only tests known to be failing.\n    :param bool serial: Run tests serially\n    :param int concurrency: \"How many processes to use. The default (0)\n        autodetects your CPU count and uses that.\n    :param str load_list: The path to a list of test_ids. If specified only\n        tests listed in the named file will be run.\n    :param bool subunit_out: Display results in subunit format.\n    :param bool until_failure: Repeat the run again and again until failure\n        occurs.\n    :param bool analyze_isolation: Search the last test run for 2-test test\n        isolation interactions.\n    :param bool isolated: Run each test id in a separate test runner.\n    :param str worker_path: Optional path of a manual worker grouping file\n        to use for the run.\n    :param str exclude_list: Path to an exclusion list file, this file\n        contains a separate regex exclude on each newline.\n    :param str include_list: Path to a inclusion list file, this file\n        contains a separate regex on each newline.\n    :param str exclude_regex: Test rejection regex. If a test cases name\n        matches on re.search() operation, it will be removed from the final\n        test list.\n    :param str no_discover: Takes in a single test_id to bypasses test\n        discover and just execute the test specified. A file name may be used\n        in place of a test name.\n    :param bool random: Randomize the test order after they are partitioned\n        into separate workers\n    :param bool combine: Combine the results from the test run with the\n        last run in the repository\n    :param list filters: A list of string regex filters to initially apply on\n        the test list. Tests that match any of the regexes will be used.\n        (assuming any other filtering specified also uses it)\n    :param bool pretty_out: Use the subunit-trace output filter\n    :param bool color: Enable colorized output in subunit-trace\n    :param file stdout: The file object to write all output to. By default this\n        is sys.stdout\n    :param bool abbreviate: Use abbreviated output if set true\n    :param bool suppress_attachments: When set true attachments subunit_trace\n        will not print attachments on successful test execution.\n    :param bool all_attachments: When set true subunit_trace will print all\n        text attachments on successful test execution.\n    :param bool show_binary_attachments: When set to true, subunit_trace will\n        print binary attachments in addition to text attachments.\n    :param str pdb: Takes in a single test_id to bypasses test\n        discover and just execute the test specified without launching any\n        additional processes. A file name may be used in place of a test name.\n\n    :return return_code: The exit code for the command. 0 for success and > 0\n        for failures.\n    :rtype: int\n    \"\"\"\n    try:\n        repo = util.get_repo_open(repo_url=repo_url)\n    except repository.RepositoryNotFound:\n        conf = config_file.TestrConf.load_from_file(config)\n        if not conf.test_path and (not test_path):\n            msg = 'No config file found and --test-path not specified. Either create or specify a .stestr.conf, tox.ini, or pyproject.toml, or use --test-path'\n            stdout.write(msg)\n            exit(1)\n        try:\n            repo = util.get_repo_initialise(repo_url=repo_url)\n        except OSError as e:\n            if e.errno != errno.EEXIST:\n                raise\n            repo_path = repo_url or './stestr'\n            stdout.write('The specified repository directory %s already exists. Please check if the repository already exists or select a different path\\n' % repo_path)\n            return 1\n    combine_id = None\n    concurrency = _to_int(concurrency)\n    if concurrency and concurrency < 0:\n        msg = 'The provided concurrency value: %s is not valid. An integer >= 0 must be used.\\n' % concurrency\n        stdout.write(msg)\n        return 2\n    if combine:\n        latest_id = repo.latest_id()\n        combine_id = str(latest_id)\n    if no_discover and pdb:\n        msg = '--no-discover and --pdb are mutually exclusive options, only specify one at a time'\n        stdout.write(msg)\n        return 2\n    if pdb and until_failure:\n        msg = 'pdb mode does not function with the --until-failure flag, only specify one at a time'\n        stdout.write(msg)\n        return 2\n    if no_discover:\n        ids = no_discover\n        klass = None\n        if '::' in ids:\n            ids, klass = ids.split('::', 1)\n            klass = '.'.join(klass.split('::'))\n        if ids.find(os.path.sep) != -1:\n            root, _ = os.path.splitext(os.path.normpath(ids))\n            ids = '.'.join(root.split(os.path.sep))\n        if klass:\n            ids = f'{ids}.{klass}'\n        stestr_python = sys.executable\n        if os.environ.get('PYTHON'):\n            python_bin = os.environ.get('PYTHON')\n        elif stestr_python:\n            python_bin = stestr_python\n        else:\n            raise RuntimeError('The Python interpreter was not found and PYTHON is not set')\n        run_cmd = python_bin + ' -m stestr.subunit_runner.run ' + ids\n\n        def run_tests():\n            run_proc = [('subunit', output.ReturnCodeToSubunit(subprocess.Popen(run_cmd, shell=True, stdout=subprocess.PIPE)))]\n            return load.load(in_streams=run_proc, subunit_out=subunit_out, repo_url=repo_url, run_id=combine_id, pretty_out=pretty_out, color=color, stdout=stdout, abbreviate=abbreviate, suppress_attachments=suppress_attachments, all_attachments=all_attachments, show_binary_attachments=show_binary_attachments)\n        if not until_failure:\n            return run_tests()\n        else:\n            while True:\n                result = run_tests()\n                if subunit:\n                    summary = testtools.StreamSummary()\n                    last_run = repo.get_latest_run().get_subunit_stream()\n                    stream = subunit.ByteStreamToStreamResult(last_run)\n                    summary.startTestRun()\n                    try:\n                        stream.run(summary)\n                    finally:\n                        summary.stopTestRun()\n                    if not results.wasSuccessful(summary):\n                        result = 1\n                if result:\n                    return result\n    if pdb:\n        ids = pdb\n        klass = None\n        if '::' in ids:\n            ids, klass = ids.split('::', 1)\n            klass = '.'.join(klass.split('::'))\n        if ids.find(os.path.sep) != -1:\n            root, _ = os.path.splitext(os.path.normpath(ids))\n            ids = '.'.join(root.split(os.path.sep))\n        if klass:\n            ids = f'{ids}.{klass}'\n        runner = subunit_run.SubunitTestRunner\n        stream = io.BytesIO()\n        program.TestProgram(module=None, argv=['stestr', ids], testRunner=functools.partial(runner, stdout=stream))\n        stream.seek(0)\n        run_proc = [('subunit', stream)]\n        return load.load(in_streams=run_proc, subunit_out=subunit_out, repo_url=repo_url, run_id=combine_id, pretty_out=pretty_out, color=color, stdout=stdout, abbreviate=abbreviate, suppress_attachments=suppress_attachments, all_attachments=all_attachments, show_binary_attachments=show_binary_attachments)\n    if failing or analyze_isolation:\n        ids = _find_failing(repo)\n    else:\n        ids = None\n    if load_list:\n        list_ids = set()\n        with open(load_list, 'rb') as list_file:\n            list_ids = set(parse_list(list_file.read()))\n        if ids is None:\n            ids = list_ids\n        else:\n            ids = list_ids.intersection(ids)\n    conf = config_file.TestrConf.load_from_file(config)\n    if not analyze_isolation:\n        cmd = conf.get_run_command(ids, regexes=filters, group_regex=group_regex, repo_url=repo_url, serial=serial, worker_path=worker_path, concurrency=concurrency, exclude_list=exclude_list, include_list=include_list, exclude_regex=exclude_regex, top_dir=top_dir, test_path=test_path, randomize=random)\n        if isolated:\n            result = 0\n            cmd.setUp()\n            try:\n                ids = cmd.list_tests()\n            finally:\n                cmd.cleanUp()\n            for test_id in ids:\n                cmd = conf.get_run_command([test_id], filters, group_regex=group_regex, repo_url=repo_url, serial=serial, worker_path=worker_path, concurrency=concurrency, exclude_list=exclude_list, include_list=include_list, exclude_regex=exclude_regex, randomize=random, test_path=test_path, top_dir=top_dir)\n                run_result = _run_tests(cmd, until_failure, subunit_out=subunit_out, combine_id=combine_id, repo_url=repo_url, pretty_out=pretty_out, color=color, abbreviate=abbreviate, stdout=stdout, suppress_attachments=suppress_attachments, all_attachments=all_attachments, show_binary_attachments=show_binary_attachments)\n                if run_result > result:\n                    result = run_result\n            return result\n        else:\n            return _run_tests(cmd, until_failure, subunit_out=subunit_out, combine_id=combine_id, repo_url=repo_url, pretty_out=pretty_out, color=color, stdout=stdout, abbreviate=abbreviate, suppress_attachments=suppress_attachments, all_attachments=all_attachments, show_binary_attachments=show_binary_attachments)\n    else:\n        latest_run = repo.get_latest_run()\n        spurious_failures = set()\n        for test_id in ids:\n            cmd = conf.get_run_command([test_id], group_regex=group_regex, repo_url=repo_url, serial=serial, worker_path=worker_path, concurrency=concurrency, exclude_list=exclude_list, include_list=include_list, exclude_regex=exclude_regex, randomize=random, test_path=test_path, top_dir=top_dir)\n            if not _run_tests(cmd, until_failure):\n                if test_id in repo.get_test_ids(repo.latest_id()):\n                    spurious_failures.add(test_id)\n        if not spurious_failures:\n            return 0\n        bisect_runner = bisect_tests.IsolationAnalyzer(latest_run, conf, _run_tests, repo, test_path=test_path, top_dir=top_dir, group_regex=group_regex, repo_url=repo_url, serial=serial, concurrency=concurrency)\n        return bisect_runner.bisect_tests(spurious_failures)\n\ndef _run_tests(cmd, until_failure, subunit_out=False, combine_id=None, repo_url=None, pretty_out=True, color=False, stdout=sys.stdout, abbreviate=False, suppress_attachments=False, all_attachments=False, show_binary_attachments=False):\n    \"\"\"Run the tests cmd was parameterised with.\"\"\"\n    cmd.setUp()\n    try:\n\n        def run_tests():\n            run_procs = [('subunit', output.ReturnCodeToSubunit(proc)) for proc in cmd.run_tests()]\n            if not run_procs:\n                stdout.write(\"The specified regex doesn't match with anything\")\n                return 1\n            return load.load((None, None), in_streams=run_procs, subunit_out=subunit_out, repo_url=repo_url, run_id=combine_id, pretty_out=pretty_out, color=color, stdout=stdout, abbreviate=abbreviate, suppress_attachments=suppress_attachments, all_attachments=all_attachments, show_binary_attachments=show_binary_attachments)\n        if not until_failure:\n            return run_tests()\n        else:\n            while True:\n                result = run_tests()\n                if subunit_out:\n                    repo = util.get_repo_open(repo_url=repo_url)\n                    summary = testtools.StreamSummary()\n                    last_run = repo.get_latest_run().get_subunit_stream()\n                    stream = subunit.ByteStreamToStreamResult(last_run)\n                    summary.startTestRun()\n                    try:\n                        stream.run(summary)\n                    finally:\n                        summary.stopTestRun()\n                    if not results.wasSuccessful(summary):\n                        result = 1\n                if result:\n                    return result\n    finally:\n        cmd.cleanUp()"
  },
  "call_tree": {
    "modified_testcases/test_run.py:TestRunCommand:test_to_int_invalid_str": {
      "stestr/commands/run.py:_to_int": {}
    },
    "modified_testcases/test_run.py:TestRunCommand:test_to_int_negative_int": {
      "stestr/commands/run.py:_to_int": {}
    },
    "modified_testcases/test_run.py:TestRunCommand:test_to_int_negative_int_str": {
      "stestr/commands/run.py:_to_int": {}
    },
    "modified_testcases/test_run.py:TestRunCommand:test_to_int_none": {
      "stestr/commands/run.py:_to_int": {}
    },
    "modified_testcases/test_run.py:TestRunCommand:test_to_int_positive_int": {
      "stestr/commands/run.py:_to_int": {}
    },
    "modified_testcases/test_run.py:TestRunCommand:test_to_int_positive_int_str": {
      "stestr/commands/run.py:_to_int": {}
    }
  }
}