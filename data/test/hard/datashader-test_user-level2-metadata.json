{
  "dir_path": "/app/datashader",
  "package_name": "datashader",
  "sample_name": "datashader-test_user",
  "src_dir": "datashader/",
  "test_dir": "datashader/tests/",
  "test_file": "datashader/datashape/tests/test_user.py",
  "test_code": "import pytest\n\nfrom datashader.datashape.user import issubschema, validate\nfrom datashader.datashape import dshape\nfrom datetime import date, time, datetime\nimport numpy as np\n\n\nmin_np = pytest.mark.skipif(\n    np.__version__ > '1.14',\n    reason=\"issubdtype no longer downcasts\"\n)\n\n\n@min_np\ndef test_validate():\n    assert validate(int, 1)\n    assert validate('int', 1)\n    assert validate(str, 'Alice')\n    assert validate(dshape('string'), 'Alice')\n    assert validate(dshape('int'), 1)\n    assert validate(dshape('int')[0], 1)\n    assert validate('real', 2.0)\n    assert validate('2 * int', (1, 2))\n    assert not validate('3 * int', (1, 2))\n    assert not validate('2 * int', 2)\n\n\n@min_np\ndef test_nested_iteratables():\n    assert validate('2 * 3 * int', [(1, 2, 3), (4, 5, 6)])\n\n\ndef test_numeric_tower():\n    assert validate(np.integer, np.int32(1))\n    assert validate(np.number, np.int32(1))\n\n\n@min_np\ndef test_validate_dicts():\n    assert validate('{x: int, y: int}', {'x': 1, 'y': 2})\n    assert not validate('{x: int, y: int}', {'x': 1, 'y': 2.0})\n    assert not validate('{x: int, y: int}', {'x': 1, 'z': 2})\n\n    assert validate('var * {x: int, y: int}', [{'x': 1, 'y': 2}])\n\n    assert validate('var * {x: int, y: int}', [{'x': 1, 'y': 2},\n                                               {'x': 3, 'y': 4}])\n\n\n@min_np\ndef test_tuples_can_be_records_too():\n    assert validate('{x: int, y: real}', (1, 2.0))\n    assert not validate('{x: int, y: real}', (1.0, 2))\n\n\ndef test_datetimes():\n    assert validate('time', time(12, 0, 0))\n    assert validate('date', date(1999, 1, 20))\n    assert validate('datetime', datetime(1999, 1, 20, 12, 0, 0))\n\n\ndef test_numpy():\n    assert validate('2 * int32', np.array([1, 2], dtype='int32'))\n\n\ndef test_issubschema():\n    assert issubschema('int', 'int')\n    assert not issubschema('int', 'float32')\n\n    assert issubschema('2 * int', '2 * int')\n    assert not issubschema('2 * int', '3 * int')\n\n    # assert issubschema('float32', 'real')\n\n\ndef test_integration():\n    assert validate('{name: string, arrived: date}',\n                    {'name': 'Alice', 'arrived': date(2012, 1, 5)})\n",
  "GT_file_code": {
    "datashader/datashape/coretypes.py": "\"\"\"\nThis defines the DataShape type system, with unified\nshape and data type.\n\"\"\"\n\nimport ctypes\nimport operator\n\nfrom collections import OrderedDict\nfrom math import ceil\n\nfrom datashader import datashape\n\nimport numpy as np\n\nfrom .internal_utils import IndexCallable, isidentifier\n\n\n# Classes of unit types.\nDIMENSION = 1\nMEASURE = 2\n\n\nclass Type(type):\n    _registry = {}\n\n    def __new__(meta, name, bases, dct):\n        cls = super(Type, meta).__new__(meta, name, bases, dct)  # noqa: UP008\n        # Don't register abstract classes\n        if not dct.get('abstract'):\n            Type._registry[name] = cls\n        return cls\n\n    @classmethod\n    def register(cls, name, type):\n        # Don't clobber existing types.\n        if name in cls._registry:\n            raise TypeError('There is another type registered with name %s'\n                            % name)\n\n        cls._registry[name] = type\n\n    @classmethod\n    def lookup_type(cls, name):\n        return cls._registry[name]\n\n\nclass Mono(metaclass=Type):\n\n    \"\"\"\n    Monotype are unqualified 0 parameters.\n\n    Each type must be reconstructable using its parameters:\n\n        type(datashape_type)(*type.parameters)\n    \"\"\"\n\n    composite = False\n\n    def __init__(self, *params):\n        self._parameters = params\n\n    @property\n    def _slotted(self):\n        return hasattr(self, '__slots__')\n\n    @property\n    def parameters(self):\n        if self._slotted:\n            return tuple(getattr(self, slot) for slot in self.__slots__)\n        else:\n            return self._parameters\n\n    def info(self):\n        return type(self), self.parameters\n\n    def __eq__(self, other):\n        return (isinstance(other, Mono) and\n                self.shape == other.shape and\n                self.measure.info() == other.measure.info())\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __hash__(self):\n        try:\n            h = self._hash\n        except AttributeError:\n            h = self._hash = hash(self.shape) ^ hash(self.measure.info())\n        return h\n\n    @property\n    def shape(self):\n        return ()\n\n    def __len__(self):\n        return 1\n\n    def __getitem__(self, key):\n        return [self][key]\n\n    def __repr__(self):\n        return '%s(%s)' % (\n            type(self).__name__,\n            ', '.join(\n                (\n                    '%s=%r' % (slot, getattr(self, slot))\n                    for slot in self.__slots__\n                ) if self._slotted else\n                map(repr, self.parameters),\n            ),\n        )\n\n    # Monotypes are their own measure\n    @property\n    def measure(self):\n        return self\n\n    def subarray(self, leading):\n        \"\"\"Returns a data shape object of the subarray with 'leading'\n        dimensions removed. In the case of a measure such as CType,\n        'leading' must be 0, and self is returned.\n        \"\"\"\n        if leading >= 1:\n            raise IndexError(('Not enough dimensions in data shape '\n                              'to remove %d leading dimensions.') % leading)\n        else:\n            return self\n\n    def __mul__(self, other):\n        if isinstance(other, str):\n            from datashader import datashape\n            return datashape.dshape(other).__rmul__(self)\n        if isinstance(other, int):\n            other = Fixed(other)\n        if isinstance(other, DataShape):\n            return other.__rmul__(self)\n\n        return DataShape(self, other)\n\n    def __rmul__(self, other):\n        if isinstance(other, str):\n            from datashader import datashape\n            return self * datashape.dshape(other)\n        if isinstance(other, int):\n            other = Fixed(other)\n\n        return DataShape(other, self)\n\n    def __getstate__(self):\n        return self.parameters\n\n    def __setstate__(self, state):\n        if self._slotted:\n            for slot, val in zip(self.__slots__, state):\n                setattr(self, slot, val)\n        else:\n            self._parameters = state\n\n    def to_numpy_dtype(self):\n        raise TypeError('DataShape %s is not NumPy-compatible' % self)\n\n\nclass Unit(Mono):\n\n    \"\"\"\n    Unit type that does not need to be reconstructed.\n    \"\"\"\n\n    def __str__(self):\n        return type(self).__name__.lower()\n\n\nclass Ellipsis(Mono):\n\n    \"\"\"Ellipsis (...). Used to indicate a variable number of dimensions.\n\n    E.g.:\n\n        ... * float32    # float32 array w/ any number of dimensions\n        A... * float32   # float32 array w/ any number of dimensions,\n                        # associated with type variable A\n    \"\"\"\n    __slots__ = 'typevar',\n\n    def __init__(self, typevar=None):\n        self.typevar = typevar\n\n    def __str__(self):\n        return str(self.typevar) + '...' if self.typevar else '...'\n\n    def __repr__(self):\n        return '%s(%r)' % (type(self).__name__, str(self))\n\n\nclass Null(Unit):\n\n    \"\"\"The null datashape.\"\"\"\n    pass\n\n\nclass Date(Unit):\n\n    \"\"\" Date type \"\"\"\n    cls = MEASURE\n    __slots__ = ()\n\n    def to_numpy_dtype(self):\n        return np.dtype('datetime64[D]')\n\n\nclass Time(Unit):\n\n    \"\"\" Time type \"\"\"\n    cls = MEASURE\n    __slots__ = 'tz',\n\n    def __init__(self, tz=None):\n        if tz is not None and not isinstance(tz, str):\n            raise TypeError('tz parameter to time datashape must be a string')\n        # TODO validate against Olson tz database\n        self.tz = tz\n\n    def __str__(self):\n        basename = super().__str__()\n        if self.tz is None:\n            return basename\n        else:\n            return '%s[tz=%r]' % (basename, str(self.tz))\n\n\nclass DateTime(Unit):\n\n    \"\"\" DateTime type \"\"\"\n    cls = MEASURE\n    __slots__ = 'tz',\n\n    def __init__(self, tz=None):\n        if tz is not None and not isinstance(tz, str):\n            raise TypeError('tz parameter to datetime datashape must be a '\n                            'string')\n        # TODO validate against Olson tz database\n        self.tz = tz\n\n    def __str__(self):\n        basename = super().__str__()\n        if self.tz is None:\n            return basename\n        else:\n            return '%s[tz=%r]' % (basename, str(self.tz))\n\n    def to_numpy_dtype(self):\n        return np.dtype('datetime64[us]')\n\n\n_units = ('ns', 'us', 'ms', 's', 'm', 'h', 'D', 'W', 'M', 'Y')\n\n\n_unit_aliases = {\n    'year': 'Y',\n    'week': 'W',\n    'day': 'D',\n    'date': 'D',\n    'hour': 'h',\n    'second': 's',\n    'millisecond': 'ms',\n    'microsecond': 'us',\n    'nanosecond': 'ns'\n}\n\n\ndef normalize_time_unit(s):\n    \"\"\" Normalize time input to one of 'year', 'second', 'millisecond', etc..\n    Example\n    -------\n    >>> normalize_time_unit('milliseconds')\n    'ms'\n    >>> normalize_time_unit('ms')\n    'ms'\n    >>> normalize_time_unit('nanoseconds')\n    'ns'\n    >>> normalize_time_unit('nanosecond')\n    'ns'\n    \"\"\"\n    s = s.strip()\n    if s in _units:\n        return s\n    if s in _unit_aliases:\n        return _unit_aliases[s]\n    if s[-1] == 's' and len(s) > 2:\n        return normalize_time_unit(s.rstrip('s'))\n\n    raise ValueError(\"Do not understand time unit %s\" % s)\n\n\nclass TimeDelta(Unit):\n    cls = MEASURE\n    __slots__ = 'unit',\n\n    def __init__(self, unit='us'):\n        self.unit = normalize_time_unit(str(unit))\n\n    def __str__(self):\n        return 'timedelta[unit=%r]' % self.unit\n\n    def to_numpy_dtype(self):\n        return np.dtype('timedelta64[%s]' % self.unit)\n\n\nclass Units(Unit):\n    \"\"\" Units type for values with physical units \"\"\"\n    cls = MEASURE\n    __slots__ = 'unit', 'tp'\n\n    def __init__(self, unit, tp=None):\n        if not isinstance(unit, str):\n            raise TypeError('unit parameter to units datashape must be a '\n                            'string')\n        if tp is None:\n            tp = DataShape(float64)\n        elif not isinstance(tp, DataShape):\n            raise TypeError('tp parameter to units datashape must be a '\n                            'datashape type')\n        self.unit = unit\n        self.tp = tp\n\n    def __str__(self):\n        if self.tp == DataShape(float64):\n            return 'units[%r]' % (self.unit)\n        else:\n            return 'units[%r, %s]' % (self.unit, self.tp)\n\n\nclass Bytes(Unit):\n\n    \"\"\" Bytes type \"\"\"\n    cls = MEASURE\n    __slots__ = ()\n\n\n_canonical_string_encodings = {\n    'A': 'A',\n    'ascii': 'A',\n    'U8': 'U8',\n    'utf-8': 'U8',\n    'utf_8': 'U8',\n    'utf8': 'U8',\n    'U16': 'U16',\n    'utf-16': 'U16',\n    'utf_16': 'U16',\n    'utf16': 'U16',\n    'U32': 'U32',\n    'utf-32': 'U32',\n    'utf_32': 'U32',\n    'utf32': 'U32',\n}\n\n\nclass String(Unit):\n\n    \"\"\" String container\n\n    >>> String()\n    ctype(\"string\")\n    >>> String(10, 'ascii')\n    ctype(\"string[10, 'A']\")\n    \"\"\"\n    cls = MEASURE\n    __slots__ = 'fixlen', 'encoding'\n\n    def __init__(self, *args):\n        if len(args) == 0:\n            fixlen, encoding = None, None\n        if len(args) == 1:\n            if isinstance(args[0], str):\n                fixlen, encoding = None, args[0]\n            if isinstance(args[0], int):\n                fixlen, encoding = args[0], None\n        elif len(args) == 2:\n            fixlen, encoding = args\n\n        encoding = encoding or 'U8'\n        if isinstance(encoding, str):\n            encoding = str(encoding)\n        try:\n            encoding = _canonical_string_encodings[encoding]\n        except KeyError:\n            raise ValueError('Unsupported string encoding %s' %\n                             repr(encoding))\n\n        self.encoding = encoding\n        self.fixlen = fixlen\n\n        # Put it in a canonical form\n\n    def __str__(self):\n        if self.fixlen is None and self.encoding == 'U8':\n            return 'string'\n        elif self.fixlen is not None and self.encoding == 'U8':\n            return 'string[%i]' % self.fixlen\n        elif self.fixlen is None and self.encoding != 'U8':\n            return 'string[%s]' % repr(self.encoding).strip('u')\n        else:\n            return 'string[%i, %s]' % (self.fixlen,\n                                       repr(self.encoding).strip('u'))\n\n    def __repr__(self):\n        s = str(self)\n        return 'ctype(\"%s\")' % s.encode('unicode_escape').decode('ascii')\n\n    def to_numpy_dtype(self):\n        \"\"\"\n        >>> String().to_numpy_dtype()\n        dtype('O')\n        >>> String(30).to_numpy_dtype()\n        dtype('<U30')\n        >>> String(30, 'A').to_numpy_dtype()\n        dtype('S30')\n        \"\"\"\n        if self.fixlen:\n            if self.encoding == 'A':\n                return np.dtype('S%d' % self.fixlen)\n            else:\n                return np.dtype('U%d' % self.fixlen)\n\n        # Create a dtype with metadata indicating it's\n        # a string in the same style as the h5py special_dtype\n        return np.dtype('O', metadata={'vlen': str})\n\n\nclass Decimal(Unit):\n\n    \"\"\"Decimal type corresponding to SQL Decimal/Numeric types.\n\n    The first parameter passed specifies the number of digits of precision that\n    the Decimal contains. If an additional parameter is given, it represents\n    the scale, or number of digits of precision that are after the decimal\n    point.\n\n    The Decimal type makes no requirement of how it is to be stored in memory,\n    therefore, the number of bytes needed to store a Decimal for a given\n    precision will vary based on the platform where it is used.\n\n    Examples\n    --------\n    >>> Decimal(18)\n    Decimal(precision=18, scale=0)\n    >>> Decimal(7, 4)\n    Decimal(precision=7, scale=4)\n    >>> Decimal(precision=11, scale=2)\n    Decimal(precision=11, scale=2)\n    \"\"\"\n\n    cls = MEASURE\n    __slots__ = 'precision', 'scale'\n\n    def __init__(self, precision, scale=0):\n        self.precision = precision\n        self.scale = scale\n\n    def __str__(self):\n        return 'decimal[precision={precision}, scale={scale}]'.format(\n            precision=self.precision, scale=self.scale\n        )\n\n    def to_numpy_dtype(self):\n        \"\"\"Convert a decimal datashape to a NumPy dtype.\n\n        Note that floating-point (scale > 0) precision will be lost converting\n        to NumPy floats.\n\n        Examples\n        --------\n        >>> Decimal(18).to_numpy_dtype()\n        dtype('int64')\n        >>> Decimal(7,4).to_numpy_dtype()\n        dtype('float64')\n        \"\"\"\n\n        if self.scale == 0:\n            if self.precision <= 2:\n                return np.dtype(np.int8)\n            elif self.precision <= 4:\n                return np.dtype(np.int16)\n            elif self.precision <= 9:\n                return np.dtype(np.int32)\n            elif self.precision <= 18:\n                return np.dtype(np.int64)\n            else:\n                raise TypeError(\n                    'Integer Decimal precision > 18 is not NumPy-compatible')\n        else:\n            return np.dtype(np.float64)\n\n\nclass DataShape(Mono):\n\n    \"\"\"\n    Composite container for datashape elements.\n\n    Elements of a datashape like ``Fixed(3)``, ``Var()`` or ``int32`` are on,\n    on their own, valid datashapes.  These elements are collected together into\n    a composite ``DataShape`` to be complete.\n\n    This class is not intended to be used directly.  Instead, use the utility\n    ``dshape`` function to create datashapes from strings or datashape\n    elements.\n\n    Examples\n    --------\n\n    >>> from datashader.datashape import Fixed, int32, DataShape, dshape\n\n    >>> DataShape(Fixed(5), int32)  # Rare to DataShape directly\n    dshape(\"5 * int32\")\n\n    >>> dshape('5 * int32')         # Instead use the dshape function\n    dshape(\"5 * int32\")\n\n    >>> dshape([Fixed(5), int32])   # It can even do construction from elements\n    dshape(\"5 * int32\")\n\n    See Also\n    --------\n    datashape.dshape\n    \"\"\"\n    composite = False\n\n    def __init__(self, *parameters, **kwds):\n        if len(parameters) == 1 and isinstance(parameters[0], str):\n            raise TypeError(\"DataShape constructor for internal use.\\n\"\n                            \"Use dshape function to convert strings into \"\n                            \"datashapes.\\nTry:\\n\\tdshape('%s')\"\n                            % parameters[0])\n        if len(parameters) > 0:\n            self._parameters = tuple(map(_launder, parameters))\n            if getattr(self._parameters[-1], 'cls', MEASURE) != MEASURE:\n                raise TypeError(('Only a measure can appear on the'\n                                 ' last position of a datashape, not %s') %\n                                repr(self._parameters[-1]))\n            for dim in self._parameters[:-1]:\n                if getattr(dim, 'cls', DIMENSION) != DIMENSION:\n                    raise TypeError(('Only dimensions can appear before the'\n                                     ' last position of a datashape, not %s') %\n                                    repr(dim))\n        else:\n            raise ValueError('the data shape should be constructed from 2 or'\n                             ' more parameters, only got %s' % len(parameters))\n        self.composite = True\n        self.name = kwds.get('name')\n\n        if self.name:\n            type(type(self))._registry[self.name] = self\n\n    def __len__(self):\n        return len(self.parameters)\n\n    def __getitem__(self, index):\n        return self.parameters[index]\n\n    def __str__(self):\n        return self.name or ' * '.join(map(str, self.parameters))\n\n    def __repr__(self):\n        s = pprint(self)\n        if '\\n' in s:\n            return 'dshape(\"\"\"%s\"\"\")' % s\n        else:\n            return 'dshape(\"%s\")' % s\n\n    @property\n    def shape(self):\n        return self.parameters[:-1]\n\n    @property\n    def measure(self):\n        return self.parameters[-1]\n\n    def subarray(self, leading):\n        \"\"\"Returns a data shape object of the subarray with 'leading'\n        dimensions removed.\n\n        >>> from datashader.datashape import dshape\n        >>> dshape('1 * 2 * 3 * int32').subarray(1)\n        dshape(\"2 * 3 * int32\")\n        >>> dshape('1 * 2 * 3 * int32').subarray(2)\n        dshape(\"3 * int32\")\n        \"\"\"\n        if leading >= len(self.parameters):\n            raise IndexError('Not enough dimensions in data shape '\n                             'to remove %d leading dimensions.' % leading)\n        elif leading in [len(self.parameters) - 1, -1]:\n            return DataShape(self.parameters[-1])\n        else:\n            return DataShape(*self.parameters[leading:])\n\n    def __rmul__(self, other):\n        if isinstance(other, int):\n            other = Fixed(other)\n        return DataShape(other, *self)\n\n    @property\n    def subshape(self):\n        return IndexCallable(self._subshape)\n\n    def _subshape(self, index):\n        \"\"\" The DataShape of an indexed subarray\n\n        >>> from datashader.datashape import dshape\n\n        >>> ds = dshape('var * {name: string, amount: int32}')\n        >>> print(ds.subshape[0])\n        {name: string, amount: int32}\n\n        >>> print(ds.subshape[0:3])\n        3 * {name: string, amount: int32}\n\n        >>> print(ds.subshape[0:7:2, 'amount'])\n        4 * int32\n\n        >>> print(ds.subshape[[1, 10, 15]])\n        3 * {name: string, amount: int32}\n\n        >>> ds = dshape('{x: int, y: int}')\n        >>> print(ds.subshape['x'])\n        int32\n\n        >>> ds = dshape('10 * var * 10 * int32')\n        >>> print(ds.subshape[0:5, 0:3, 5])\n        5 * 3 * int32\n\n        >>> ds = dshape('var * {name: string, amount: int32, id: int32}')\n        >>> print(ds.subshape[:, [0, 2]])\n        var * {name: string, id: int32}\n\n        >>> ds = dshape('var * {name: string, amount: int32, id: int32}')\n        >>> print(ds.subshape[:, ['name', 'id']])\n        var * {name: string, id: int32}\n\n        >>> print(ds.subshape[0, 1:])\n        {amount: int32, id: int32}\n        \"\"\"\n        from .predicates import isdimension\n        if isinstance(index, int) and isdimension(self[0]):\n            return self.subarray(1)\n        if isinstance(self[0], Record) and isinstance(index, str):\n            return self[0][index]\n        if isinstance(self[0], Record) and isinstance(index, int):\n            return self[0].parameters[0][index][1]\n        if isinstance(self[0], Record) and isinstance(index, list):\n            rec = self[0]\n            # Translate strings to corresponding integers\n            index = [self[0].names.index(i) if isinstance(i, str) else i\n                     for i in index]\n            return DataShape(Record([rec.parameters[0][i] for i in index]))\n        if isinstance(self[0], Record) and isinstance(index, slice):\n            rec = self[0]\n            return DataShape(Record(rec.parameters[0][index]))\n        if isinstance(index, list) and isdimension(self[0]):\n            return len(index) * self.subarray(1)\n        if isinstance(index, slice):\n            if isinstance(self[0], Fixed):\n                n = int(self[0])\n                start = index.start or 0\n                stop = index.stop or n\n                if start < 0:\n                    start = n + start\n                if stop < 0:\n                    stop = n + stop\n                count = stop - start\n            else:\n                start = index.start or 0\n                stop = index.stop\n                if not stop:\n                    count = -start if start < 0 else var\n                if (stop is not None and start is not None and stop >= 0 and\n                        start >= 0):\n                    count = stop - start\n                else:\n                    count = var\n\n            if count != var and index.step is not None:\n                count = int(ceil(count / index.step))\n\n            return count * self.subarray(1)\n        if isinstance(index, tuple):\n            if not index:\n                return self\n            elif index[0] is None:\n                return 1 * self._subshape(index[1:])\n            elif len(index) == 1:\n                return self._subshape(index[0])\n            else:\n                ds = self.subarray(1)._subshape(index[1:])\n                return (self[0] * ds)._subshape(index[0])\n        raise TypeError('invalid index value %s of type %r' %\n                        (index, type(index).__name__))\n\n    def __setstate__(self, state):\n        self._parameters = state\n        self.composite = True\n        self.name = None\n\n\nnumpy_provides_missing = frozenset((Date, DateTime, TimeDelta))\n\n\nclass Option(Mono):\n\n    \"\"\"\n    Measure types which may or may not hold data. Makes no\n    indication of how this is implemented in memory.\n    \"\"\"\n    __slots__ = 'ty',\n\n    def __init__(self, ds):\n        self.ty = _launder(ds)\n\n    @property\n    def shape(self):\n        return self.ty.shape\n\n    @property\n    def itemsize(self):\n        return self.ty.itemsize\n\n    def __str__(self):\n        return '?%s' % self.ty\n\n    def to_numpy_dtype(self):\n        if type(self.ty) in numpy_provides_missing:\n            return self.ty.to_numpy_dtype()\n        raise TypeError('DataShape measure %s is not NumPy-compatible' % self)\n\n\nclass CType(Unit):\n\n    \"\"\"\n    Symbol for a sized type mapping uniquely to a native type.\n    \"\"\"\n    cls = MEASURE\n    __slots__ = 'name', '_itemsize', '_alignment'\n\n    def __init__(self, name, itemsize, alignment):\n        self.name = name\n        self._itemsize = itemsize\n        self._alignment = alignment\n        Type.register(name, self)\n\n    @classmethod\n    def from_numpy_dtype(self, dt):\n        \"\"\"\n        From Numpy dtype.\n\n        >>> from datashader.datashape import CType\n        >>> from numpy import dtype\n        >>> CType.from_numpy_dtype(dtype('int32'))\n        ctype(\"int32\")\n        >>> CType.from_numpy_dtype(dtype('i8'))\n        ctype(\"int64\")\n        >>> CType.from_numpy_dtype(dtype('M8'))\n        DateTime(tz=None)\n        >>> CType.from_numpy_dtype(dtype('U30'))  # doctest: +SKIP\n        ctype(\"string[30, 'U32']\")\n        \"\"\"\n        try:\n            return Type.lookup_type(dt.name)\n        except KeyError:\n            pass\n        if np.issubdtype(dt, np.datetime64):\n            unit, _ = np.datetime_data(dt)\n            defaults = {'D': date_, 'Y': date_, 'M': date_, 'W': date_}\n            return defaults.get(unit, datetime_)\n        elif np.issubdtype(dt, np.timedelta64):\n            unit, _ = np.datetime_data(dt)\n            return TimeDelta(unit=unit)\n        elif np.__version__[0] < \"2\" and np.issubdtype(dt, np.unicode_):  # noqa: NPY201\n            return String(dt.itemsize // 4, 'U32')\n        elif np.issubdtype(dt, np.str_) or np.issubdtype(dt, np.bytes_):\n            return String(dt.itemsize, 'ascii')\n        raise NotImplementedError(\"NumPy datatype %s not supported\" % dt)\n\n    @property\n    def itemsize(self):\n        \"\"\"The size of one element of this type.\"\"\"\n        return self._itemsize\n\n    @property\n    def alignment(self):\n        \"\"\"The alignment of one element of this type.\"\"\"\n        return self._alignment\n\n    def to_numpy_dtype(self):\n        \"\"\"\n        To Numpy dtype.\n        \"\"\"\n        # TODO: Fixup the complex type to how numpy does it\n        name = self.name\n        return np.dtype({\n            'complex[float32]': 'complex64',\n            'complex[float64]': 'complex128'\n        }.get(name, name))\n\n    def __str__(self):\n        return self.name\n\n    def __repr__(self):\n        s = str(self)\n        return 'ctype(\"%s\")' % s.encode('unicode_escape').decode('ascii')\n\n\nclass Fixed(Unit):\n\n    \"\"\"\n    Fixed dimension.\n    \"\"\"\n    cls = DIMENSION\n    __slots__ = 'val',\n\n    def __init__(self, i):\n        # Use operator.index, so Python integers, numpy int scalars, etc work\n        i = operator.index(i)\n\n        if i < 0:\n            raise ValueError('Fixed dimensions must be positive')\n\n        self.val = i\n\n    def __index__(self):\n        return self.val\n\n    def __int__(self):\n        return self.val\n\n    def __eq__(self, other):\n        return (type(other) is Fixed and self.val == other.val or\n                isinstance(other, int) and self.val == other)\n\n    __hash__ = Mono.__hash__\n\n    def __str__(self):\n        return str(self.val)\n\n\nclass Var(Unit):\n\n    \"\"\" Variable dimension \"\"\"\n    cls = DIMENSION\n    __slots__ = ()\n\n\nclass TypeVar(Unit):\n\n    \"\"\"\n    A free variable in the signature. Not user facing.\n    \"\"\"\n    # cls could be MEASURE or DIMENSION, depending on context\n    __slots__ = 'symbol',\n\n    def __init__(self, symbol):\n        if not symbol[0].isupper():\n            raise ValueError(('TypeVar symbol %r does not '\n                              'begin with a capital') % symbol)\n        self.symbol = symbol\n\n    def __str__(self):\n        return str(self.symbol)\n\n\nclass Function(Mono):\n    \"\"\"Function signature type\n    \"\"\"\n    @property\n    def restype(self):\n        return self.parameters[-1]\n\n    @property\n    def argtypes(self):\n        return self.parameters[:-1]\n\n    def __str__(self):\n        return '(%s) -> %s' % (\n            ', '.join(map(str, self.argtypes)), self.restype\n        )\n\n\nclass Map(Mono):\n    __slots__ = 'key', 'value'\n\n    def __init__(self, key, value):\n        self.key = _launder(key)\n        self.value = _launder(value)\n\n    def __str__(self):\n        return '%s[%s, %s]' % (type(self).__name__.lower(),\n                               self.key,\n                               self.value)\n\n    def to_numpy_dtype(self):\n        return to_numpy_dtype(self)\n\n\ndef _launder(x):\n    \"\"\" Clean up types prior to insertion into DataShape\n\n    >>> from datashader.datashape import dshape\n    >>> _launder(5)         # convert ints to Fixed\n    Fixed(val=5)\n    >>> _launder('int32')   # parse strings\n    ctype(\"int32\")\n    >>> _launder(dshape('int32'))\n    ctype(\"int32\")\n    >>> _launder(Fixed(5))  # No-op on valid parameters\n    Fixed(val=5)\n    \"\"\"\n    if isinstance(x, int):\n        x = Fixed(x)\n    if isinstance(x, str):\n        x = datashape.dshape(x)\n    if isinstance(x, DataShape) and len(x) == 1:\n        return x[0]\n    if isinstance(x, Mono):\n        return x\n    return x\n\n\nclass CollectionPrinter:\n\n    def __repr__(self):\n        s = str(self)\n        strs = ('\"\"\"%s\"\"\"' if '\\n' in s else '\"%s\"') % s\n        return 'dshape(%s)' % strs\n\n\nclass RecordMeta(Type):\n    @staticmethod\n    def _unpack_slice(s, idx):\n        if not isinstance(s, slice):\n            raise TypeError(\n                'invalid field specification at position %d.\\n'\n                'fields must be formatted like: {name}:{type}' % idx,\n            )\n\n        name, type_ = packed = s.start, s.stop\n        if name is None:\n            raise TypeError('missing field name at position %d' % idx)\n        if not isinstance(name, str):\n            raise TypeError(\n                \"field name at position %d ('%s') was not a string\" % (\n                    idx, name,\n                ),\n            )\n        if type_ is None and s.step is None:\n            raise TypeError(\n                \"missing type for field '%s' at position %d\" % (name, idx))\n        if s.step is not None:\n            raise TypeError(\n                \"unexpected slice step for field '%s' at position %d.\\n\"\n                \"hint: you might have a second ':'\" % (name, idx),\n            )\n\n        return packed\n\n    def __getitem__(self, types):\n        if not isinstance(types, tuple):\n            types = types,\n\n        return self(list(map(self._unpack_slice, types, range(len(types)))))\n\n\nclass Record(CollectionPrinter, Mono, metaclass=RecordMeta):\n    \"\"\"\n    A composite data structure of ordered fields mapped to types.\n\n    Properties\n    ----------\n\n    fields: tuple of (name, type) pairs\n        The only stored data, also the input to ``__init__``\n    dict: dict\n        A dictionary view of ``fields``\n    names: list of strings\n        A list of the names\n    types: list of datashapes\n        A list of the datashapes\n\n    Example\n    -------\n\n    >>> Record([['id', 'int'], ['name', 'string'], ['amount', 'real']])\n    dshape(\"{id: int32, name: string, amount: float64}\")\n    \"\"\"\n    cls = MEASURE\n\n    def __init__(self, fields):\n        \"\"\"\n        Parameters\n        ----------\n        fields : list/OrderedDict of (name, type) entries\n            The fields which make up the record.\n        \"\"\"\n        if isinstance(fields, OrderedDict):\n            fields = fields.items()\n        fields = list(fields)\n        names = [\n            str(name) if not isinstance(name, str) else name\n            for name, _ in fields\n        ]\n        types = [_launder(v) for _, v in fields]\n\n        if len(set(names)) != len(names):\n            for name in set(names):\n                names.remove(name)\n            raise ValueError(\"duplicate field names found: %s\" % names)\n\n        self._parameters = tuple(zip(names, types)),\n\n    @property\n    def fields(self):\n        return self._parameters[0]\n\n    @property\n    def dict(self):\n        return dict(self.fields)\n\n    @property\n    def names(self):\n        return [n for n, t in self.fields]\n\n    @property\n    def types(self):\n        return [t for n, t in self.fields]\n\n    def to_numpy_dtype(self):\n        \"\"\"\n        To Numpy record dtype.\n        \"\"\"\n        return np.dtype([(str(name), to_numpy_dtype(typ))\n                         for name, typ in self.fields])\n\n    def __getitem__(self, key):\n        return self.dict[key]\n\n    def __str__(self):\n        return pprint(self)\n\n\nR = Record  # Alias for record literals\n\n\ndef _format_categories(cats, n=10):\n    return '[%s%s]' % (\n        ', '.join(map(repr, cats[:n])),\n        ', ...' if len(cats) > n else ''\n    )\n\n\nclass Categorical(Mono):\n    \"\"\"Unordered categorical type.\n    \"\"\"\n\n    __slots__ = 'categories', 'type', 'ordered'\n    cls = MEASURE\n\n    def __init__(self, categories, type=None, ordered=False):\n        self.categories = tuple(categories)\n        self.type = (type or datashape.discover(self.categories)).measure\n        self.ordered = ordered\n\n    def __str__(self):\n        return '%s[%s, type=%s, ordered=%s]' % (\n            type(self).__name__.lower(),\n            _format_categories(self.categories),\n            self.type,\n            self.ordered\n        )\n\n    def __repr__(self):\n        return '%s(categories=%s, type=%r, ordered=%s)' % (\n            type(self).__name__,\n            _format_categories(self.categories),\n            self.type,\n            self.ordered\n        )\n\n\nclass Tuple(CollectionPrinter, Mono):\n\n    \"\"\"\n    A product type.\n    \"\"\"\n    __slots__ = 'dshapes',\n    cls = MEASURE\n\n    def __init__(self, dshapes):\n        \"\"\"\n        Parameters\n        ----------\n        dshapes : list of dshapes\n            The datashapes which make up the tuple.\n        \"\"\"\n        dshapes = [DataShape(ds) if not isinstance(ds, DataShape) else ds\n                   for ds in dshapes]\n        self.dshapes = tuple(dshapes)\n\n    def __str__(self):\n        return '(%s)' % ', '.join(map(str, self.dshapes))\n\n    def to_numpy_dtype(self):\n        \"\"\"\n        To Numpy record dtype.\n        \"\"\"\n        return np.dtype([('f%d' % i, to_numpy_dtype(typ))\n                         for i, typ in enumerate(self.parameters[0])])\n\n\nclass JSON(Mono):\n\n    \"\"\" JSON measure \"\"\"\n    cls = MEASURE\n    __slots__ = ()\n\n    def __str__(self):\n        return 'json'\n\n\nbool_ = CType('bool', 1, 1)\nchar = CType('char', 1, 1)\n\nint8 = CType('int8', 1, 1)\nint16 = CType('int16', 2, ctypes.alignment(ctypes.c_int16))\nint32 = CType('int32', 4, ctypes.alignment(ctypes.c_int32))\nint64 = CType('int64', 8, ctypes.alignment(ctypes.c_int64))\n\n# int is an alias for int32\nint_ = int32\nType.register('int', int_)\n\nuint8 = CType('uint8', 1, 1)\nuint16 = CType('uint16', 2, ctypes.alignment(ctypes.c_uint16))\nuint32 = CType('uint32', 4, ctypes.alignment(ctypes.c_uint32))\nuint64 = CType('uint64', 8, ctypes.alignment(ctypes.c_uint64))\n\nfloat16 = CType('float16', 2, ctypes.alignment(ctypes.c_uint16))\nfloat32 = CType('float32', 4, ctypes.alignment(ctypes.c_float))\nfloat64 = CType('float64', 8, ctypes.alignment(ctypes.c_double))\n# float128 = CType('float128', 16)\n\n# real is an alias for float64\nreal = float64\nType.register('real', real)\n\ncomplex_float32 = CType('complex[float32]', 8,\n                        ctypes.alignment(ctypes.c_float))\ncomplex_float64 = CType('complex[float64]', 16,\n                        ctypes.alignment(ctypes.c_double))\nType.register('complex64', complex_float32)\ncomplex64 = complex_float32\n\nType.register('complex128', complex_float64)\ncomplex128 = complex_float64\n# complex256 = CType('complex256', 32)\n\n# complex is an alias for complex[float64]\ncomplex_ = complex_float64\n\ndate_ = Date()\ntime_ = Time()\ndatetime_ = DateTime()\ntimedelta_ = TimeDelta()\nType.register('date', date_)\nType.register('time', time_)\nType.register('datetime', datetime_)\nType.register('timedelta', timedelta_)\n\nnull = Null()\nType.register('null', null)\n\nc_byte = int8\nc_short = int16\nc_int = int32\nc_longlong = int64\n\nc_ubyte = uint8\nc_ushort = uint16\nc_ulonglong = uint64\n\nif ctypes.sizeof(ctypes.c_long) == 4:\n    c_long = int32\n    c_ulong = uint32\nelse:\n    c_long = int64\n    c_ulong = uint64\n\nif ctypes.sizeof(ctypes.c_void_p) == 4:\n    intptr = c_ssize_t = int32\n    uintptr = c_size_t = uint32\nelse:\n    intptr = c_ssize_t = int64\n    uintptr = c_size_t = uint64\nType.register('intptr', intptr)\nType.register('uintptr', uintptr)\n\nc_half = float16\nc_float = float32\nc_double = float64\n\n# TODO: Deal with the longdouble == one of float64/float80/float96/float128\n# situation\n\n# c_longdouble = float128\n\nhalf = float16\nsingle = float32\ndouble = float64\n\nvoid = CType('void', 0, 1)\nobject_ = pyobj = CType('object',\n                        ctypes.sizeof(ctypes.py_object),\n                        ctypes.alignment(ctypes.py_object))\n\nna = Null\nNullRecord = Record(())\nbytes_ = Bytes()\n\nstring = String()\njson = JSON()\n\nType.register('float', c_float)\nType.register('double', c_double)\n\nType.register('bytes', bytes_)\n\nType.register('string', String())\n\nvar = Var()\n\n\ndef to_numpy_dtype(ds):\n    \"\"\" Throw away the shape information and just return the\n    measure as NumPy dtype instance.\"\"\"\n    if isinstance(ds.measure, datashape.coretypes.Map):\n        ds = ds.measure.key\n    return to_numpy(ds.measure)[1]\n\n\ndef to_numpy(ds):\n    \"\"\"\n    Downcast a datashape object into a Numpy (shape, dtype) tuple if\n    possible.\n\n    >>> from datashader.datashape import dshape, to_numpy\n    >>> to_numpy(dshape('5 * 5 * int32'))\n    ((5, 5), dtype('int32'))\n    >>> to_numpy(dshape('10 * string[30]'))\n    ((10,), dtype('<U30'))\n    >>> to_numpy(dshape('N * int32'))\n    ((-1,), dtype('int32'))\n    \"\"\"\n    shape = []\n    if isinstance(ds, DataShape):\n        # The datashape dimensions\n        for dim in ds[:-1]:\n            if isinstance(dim, Fixed):\n                shape.append(int(dim))\n            elif isinstance(dim, TypeVar):\n                shape.append(-1)\n            else:\n                raise TypeError('DataShape dimension %s is not '\n                                'NumPy-compatible' % dim)\n\n        # The datashape measure\n        msr = ds[-1]\n    else:\n        msr = ds\n\n    return tuple(shape), msr.to_numpy_dtype()\n\n\ndef from_numpy(shape, dt):\n    \"\"\"\n    Upcast a (shape, dtype) tuple if possible.\n\n    >>> from datashader.datashape import from_numpy\n    >>> from numpy import dtype\n    >>> from_numpy((5, 5), dtype('int32'))\n    dshape(\"5 * 5 * int32\")\n\n    >>> from_numpy((10,), dtype('S10'))\n    dshape(\"10 * string[10, 'A']\")\n    \"\"\"\n    dtype = np.dtype(dt)\n\n    if dtype.kind == 'S':\n        measure = String(dtype.itemsize, 'A')\n    elif dtype.kind == 'U':\n        measure = String(dtype.itemsize // 4, 'U32')\n    elif dtype.fields:\n        fields = [(name, dtype.fields[name]) for name in dtype.names]\n        rec = [(name, from_numpy(t.shape, t.base))  # recurse into nested dtype\n               for name, (t, _) in fields]  # _ is the byte offset: ignore it\n        measure = Record(rec)\n    else:\n        measure = CType.from_numpy_dtype(dtype)\n\n    if not shape:\n        return measure\n    return DataShape(*tuple(map(Fixed, shape)) + (measure,))\n\n\ndef print_unicode_string(s):\n    try:\n        return s.decode('unicode_escape').encode('ascii')\n    except AttributeError:\n        return s\n\n\ndef pprint(ds, width=80):\n    ''' Pretty print a datashape\n\n    >>> from datashader.datashape import dshape, pprint\n    >>> print(pprint(dshape('5 * 3 * int32')))\n    5 * 3 * int32\n\n    >>> ds = dshape(\"\"\"\n    ... 5000000000 * {\n    ...     a: (int, float32, real, string, datetime),\n    ...     b: {c: 5 * int, d: var * 100 * float32}\n    ... }\"\"\")\n    >>> print(pprint(ds))\n    5000000000 * {\n      a: (int32, float32, float64, string, datetime),\n      b: {c: 5 * int32, d: var * 100 * float32}\n      }\n\n    Record measures print like full datashapes\n    >>> print(pprint(ds.measure, width=30))\n    {\n      a: (\n        int32,\n        float32,\n        float64,\n        string,\n        datetime\n        ),\n      b: {\n        c: 5 * int32,\n        d: var * 100 * float32\n        }\n      }\n\n    Control width of the result\n    >>> print(pprint(ds, width=30))\n    5000000000 * {\n      a: (\n        int32,\n        float32,\n        float64,\n        string,\n        datetime\n        ),\n      b: {\n        c: 5 * int32,\n        d: var * 100 * float32\n        }\n      }\n    >>>\n    '''\n    result = ''\n\n    if isinstance(ds, DataShape):\n        if ds.shape:\n            result += ' * '.join(map(str, ds.shape))\n            result += ' * '\n        ds = ds[-1]\n\n    if isinstance(ds, Record):\n        pairs = ['%s: %s' % (name if isidentifier(name) else\n                             repr(print_unicode_string(name)),\n                             pprint(typ, width - len(result) - len(name)))\n                 for name, typ in zip(ds.names, ds.types)]\n        short = '{%s}' % ', '.join(pairs)\n\n        if len(result + short) < width:\n            return result + short\n        else:\n            long = '{\\n%s\\n}' % ',\\n'.join(pairs)\n            return result + long.replace('\\n', '\\n  ')\n\n    elif isinstance(ds, Tuple):\n        typs = [pprint(typ, width-len(result))\n                for typ in ds.dshapes]\n        short = '(%s)' % ', '.join(typs)\n        if len(result + short) < width:\n            return result + short\n        else:\n            long = '(\\n%s\\n)' % ',\\n'.join(typs)\n            return result + long.replace('\\n', '\\n  ')\n    else:\n        result += str(ds)\n    return result\n",
    "datashader/datatypes.py": "from __future__ import annotations\n\nimport re\n\nfrom functools import total_ordering\nfrom packaging.version import Version\n\nimport numpy as np\nimport pandas as pd\n\nfrom numba import jit\nfrom pandas.api.extensions import (\n    ExtensionDtype, ExtensionArray, register_extension_dtype)\nfrom numbers import Integral\n\nfrom pandas.api.types import pandas_dtype, is_extension_array_dtype\n\n\ntry:\n    # See if we can register extension type with dask >= 1.1.0\n    from dask.dataframe.extensions import make_array_nonempty\nexcept ImportError:\n    make_array_nonempty = None\n\n\ndef _validate_ragged_properties(start_indices, flat_array):\n    \"\"\"\n    Validate that start_indices are flat_array arrays that may be used to\n    represent a valid RaggedArray.\n\n    Parameters\n    ----------\n    flat_array: numpy array containing concatenation\n                of all nested arrays to be represented\n                by this ragged array\n    start_indices: unsigned integer numpy array the same\n                   length as the ragged array where values\n                   represent the index into flat_array where\n                   the corresponding ragged array element\n                   begins\n    Raises\n    ------\n    ValueError:\n        if input arguments are invalid or incompatible properties\n    \"\"\"\n\n    # Validate start_indices\n    if (not isinstance(start_indices, np.ndarray) or\n            start_indices.dtype.kind != 'u' or\n            start_indices.ndim != 1):\n        raise ValueError(\"\"\"\nThe start_indices property of a RaggedArray must be a 1D numpy array of\nunsigned integers (start_indices.dtype.kind == 'u')\n    Received value of type {typ}: {v}\"\"\".format(\n            typ=type(start_indices), v=repr(start_indices)))\n\n    # Validate flat_array\n    if (not isinstance(flat_array, np.ndarray) or\n            flat_array.ndim != 1):\n        raise ValueError(\"\"\"\nThe flat_array property of a RaggedArray must be a 1D numpy array\n    Received value of type {typ}: {v}\"\"\".format(\n            typ=type(flat_array), v=repr(flat_array)))\n\n    # Validate start_indices values\n    # We don't need to check start_indices < 0 because we already know that it\n    # has an unsigned integer datatype\n    #\n    # Note that start_indices[i] == len(flat_array) is valid as it represents\n    # and empty array element at the end of the ragged array.\n    invalid_inds = start_indices > len(flat_array)\n\n    if invalid_inds.any():\n        some_invalid_vals = start_indices[invalid_inds[:10]]\n\n        raise ValueError(\"\"\"\nElements of start_indices must be less than the length of flat_array ({m})\n    Invalid values include: {vals}\"\"\".format(\n            m=len(flat_array), vals=repr(some_invalid_vals)))\n\n\n# Internal ragged element array wrapper that provides\n# equality, ordering, and hashing.\n@total_ordering\nclass _RaggedElement:\n\n    @staticmethod\n    def ragged_or_nan(a):\n        if np.isscalar(a) and np.isnan(a):\n            return a\n        else:\n            return _RaggedElement(a)\n\n    @staticmethod\n    def array_or_nan(a):\n        if np.isscalar(a) and np.isnan(a):\n            return a\n        else:\n            return a.array\n\n    def __init__(self, array):\n        self.array = array\n\n    def __hash__(self):\n        return hash(self.array.tobytes())\n\n    def __eq__(self, other):\n        if not isinstance(other, _RaggedElement):\n            return False\n        return np.array_equal(self.array, other.array)\n\n    def __lt__(self, other):\n        if not isinstance(other, _RaggedElement):\n            return NotImplemented\n        return _lexograph_lt(self.array, other.array)\n\n    def __repr__(self):\n        array_repr = repr(self.array)\n        return array_repr.replace('array', 'ragged_element')\n\n\n@register_extension_dtype\nclass RaggedDtype(ExtensionDtype):\n    \"\"\"\n    Pandas ExtensionDtype to represent a ragged array datatype\n\n    Methods not otherwise documented here are inherited from ExtensionDtype;\n    please see the corresponding method on that class for the docstring\n    \"\"\"\n    type = np.ndarray\n    base = np.dtype('O')\n    _subtype_re = re.compile(r\"^ragged\\[(?P<subtype>\\w+)\\]$\")\n    _metadata = ('_dtype',)\n\n    @property\n    def name(self):\n        return 'Ragged[{subtype}]'.format(subtype=self.subtype)\n\n    def __repr__(self):\n        return self.name\n\n    @classmethod\n    def construct_array_type(cls):\n        return RaggedArray\n\n    @classmethod\n    def construct_from_string(cls, string):\n        if not isinstance(string, str):\n            raise TypeError(\"'construct_from_string' expects a string, got %s\" % type(string))\n\n        # lowercase string\n        string = string.lower()\n\n        msg = \"Cannot construct a 'RaggedDtype' from '{}'\"\n        if string.startswith('ragged'):\n            # Extract subtype\n            try:\n                subtype_string = cls._parse_subtype(string)\n                return RaggedDtype(dtype=subtype_string)\n            except Exception:\n                raise TypeError(msg.format(string))\n        else:\n            raise TypeError(msg.format(string))\n\n    def __init__(self, dtype=np.float64):\n        if isinstance(dtype, RaggedDtype):\n            self._dtype = dtype.subtype\n        else:\n            self._dtype = np.dtype(dtype)\n\n    @property\n    def subtype(self):\n        return self._dtype\n\n    @classmethod\n    def _parse_subtype(cls, dtype_string):\n        \"\"\"\n        Parse a datatype string to get the subtype\n\n        Parameters\n        ----------\n        dtype_string: str\n            A string like Ragged[subtype]\n\n        Returns\n        -------\n        subtype: str\n\n        Raises\n        ------\n        ValueError\n            When the subtype cannot be extracted\n        \"\"\"\n        # Be case insensitive\n        dtype_string = dtype_string.lower()\n\n        match = cls._subtype_re.match(dtype_string)\n        if match:\n            subtype_string = match.groupdict()['subtype']\n        elif dtype_string == 'ragged':\n            subtype_string = 'float64'\n        else:\n            raise ValueError(\"Cannot parse {dtype_string}\".format(\n                dtype_string=dtype_string))\n        return subtype_string\n\n\ndef missing(v):\n    return v is None or (np.isscalar(v) and np.isnan(v))\n\n\nclass RaggedArray(ExtensionArray):\n    \"\"\"\n    Pandas ExtensionArray to represent ragged arrays\n\n    Methods not otherwise documented here are inherited from ExtensionArray;\n    please see the corresponding method on that class for the docstring\n    \"\"\"\n    def __init__(self, data, dtype=None, copy=False):\n        \"\"\"\n        Construct a RaggedArray\n\n        Parameters\n        ----------\n        data: list or array or dict or RaggedArray\n            * list or 1D-array: A List or 1D array of lists or 1D arrays that\n                                should be represented by the RaggedArray\n\n            * dict: A dict containing 'start_indices' and 'flat_array' keys\n                    with numpy array values where:\n                    - flat_array:  numpy array containing concatenation\n                                   of all nested arrays to be represented\n                                   by this ragged array\n                    - start_indices: unsigned integer numpy array the same\n                                     length as the ragged array where values\n                                     represent the index into flat_array where\n                                     the corresponding ragged array element\n                                     begins\n            * RaggedArray: A RaggedArray instance to copy\n\n        dtype: RaggedDtype or np.dtype or str or None (default None)\n            Datatype to use to store underlying values from data.\n            If none (the default) then dtype will be determined using the\n            numpy.result_type function.\n        copy : bool (default False)\n            Whether to deep copy the input arrays. Only relevant when `data`\n            has type `dict` or `RaggedArray`. When data is a `list` or\n            `array`, input arrays are always copied.\n        \"\"\"\n        if (isinstance(data, dict) and\n                all(k in data for k in\n                    ['start_indices', 'flat_array'])):\n\n            _validate_ragged_properties(\n                start_indices=data['start_indices'],\n                flat_array=data['flat_array'])\n\n            self._start_indices = data['start_indices']\n            self._flat_array = data['flat_array']\n            dtype = self._flat_array.dtype\n\n            if copy:\n                self._start_indices = self._start_indices.copy()\n                self._flat_array = self._flat_array.copy()\n\n        elif isinstance(data, RaggedArray):\n            self._flat_array = data.flat_array\n            self._start_indices = data.start_indices\n            dtype = self._flat_array.dtype\n\n            if copy:\n                self._start_indices = self._start_indices.copy()\n                self._flat_array = self._flat_array.copy()\n        else:\n            # Compute lengths\n            index_len = len(data)\n            buffer_len = sum(len(datum)\n                             if not missing(datum)\n                             else 0 for datum in data)\n\n            # Compute necessary precision of start_indices array\n            for nbits in [8, 16, 32, 64]:\n                start_indices_dtype = 'uint' + str(nbits)\n                max_supported = np.iinfo(start_indices_dtype).max\n                if buffer_len <= max_supported:\n                    break\n\n            # infer dtype if not provided\n            if dtype is None:\n                non_missing = [np.atleast_1d(v)\n                               for v in data if not missing(v)]\n                if non_missing:\n                    dtype = np.result_type(*non_missing)\n                else:\n                    dtype = 'float64'\n            elif isinstance(dtype, RaggedDtype):\n                dtype = dtype.subtype\n\n            # Initialize representation arrays\n            self._start_indices = np.zeros(index_len, dtype=start_indices_dtype)\n            self._flat_array = np.zeros(buffer_len, dtype=dtype)\n\n            # Populate arrays\n            next_start_ind = 0\n            for i, array_el in enumerate(data):\n                # Compute element length\n                n = len(array_el) if not missing(array_el) else 0\n\n                # Update start indices\n                self._start_indices[i] = next_start_ind\n\n                # Do not assign when slice is empty avoiding possible\n                # nan assignment to integer array\n                if not n:\n                    continue\n\n                # Update flat array\n                self._flat_array[next_start_ind:next_start_ind+n] = array_el\n\n                # increment next start index\n                next_start_ind += n\n\n        self._dtype = RaggedDtype(dtype=dtype)\n\n    def __eq__(self, other):\n        if isinstance(other, RaggedArray):\n            if len(other) != len(self):\n                raise ValueError(\"\"\"\nCannot check equality of RaggedArray values of unequal length\n    len(ra1) == {len_ra1}\n    len(ra2) == {len_ra2}\"\"\".format(\n                    len_ra1=len(self),\n                    len_ra2=len(other)))\n\n            result = _eq_ragged_ragged(\n                self.start_indices, self.flat_array,\n                other.start_indices, other.flat_array)\n        else:\n            # Convert other to numpy array\n            if not isinstance(other, np.ndarray):\n                other_array = np.asarray(other)\n            else:\n                other_array = other\n\n            if other_array.ndim == 1 and other_array.dtype.kind != 'O':\n\n                # Treat as ragged scalar\n                result = _eq_ragged_scalar(\n                    self.start_indices, self.flat_array, other_array)\n            elif (other_array.ndim == 1 and\n                  other_array.dtype.kind == 'O' and\n                  len(other_array) == len(self)):\n\n                # Treat as vector\n                result = _eq_ragged_ndarray1d(\n                    self.start_indices, self.flat_array, other_array)\n            elif (other_array.ndim == 2 and\n                  other_array.dtype.kind != 'O' and\n                  other_array.shape[0] == len(self)):\n\n                # Treat rows as ragged elements\n                result = _eq_ragged_ndarray2d(\n                    self.start_indices, self.flat_array, other_array)\n            else:\n                raise ValueError(\"\"\"\nCannot check equality of RaggedArray of length {ra_len} with:\n    {other}\"\"\".format(ra_len=len(self), other=repr(other)))\n\n        return result\n\n    def __ne__(self, other):\n        return np.logical_not(self == other)\n\n    @property\n    def flat_array(self):\n        \"\"\"\n        numpy array containing concatenation of all nested arrays\n\n        Returns\n        -------\n        np.ndarray\n        \"\"\"\n        return self._flat_array\n\n    @property\n    def start_indices(self):\n        \"\"\"\n        unsigned integer numpy array the same length as the ragged array where\n        values represent the index into flat_array where the corresponding\n        ragged array element begins\n\n        Returns\n        -------\n        np.ndarray\n        \"\"\"\n        return self._start_indices\n\n    def __len__(self):\n        return len(self._start_indices)\n\n    def __getitem__(self, item):\n        err_msg = (\"Only integers, slices and integer or boolean\"\n                   \"arrays are valid indices.\")\n        if isinstance(item, Integral):\n            if item < -len(self) or item >= len(self):\n                raise IndexError(\"{item} is out of bounds\".format(item=item))\n            else:\n                # Convert negative item index\n                if item < 0:\n                    item += len(self)\n\n                slice_start = self.start_indices[item]\n                slice_end = (self.start_indices[item+1]\n                             if item + 1 <= len(self) - 1\n                             else len(self.flat_array))\n\n                return (self.flat_array[slice_start:slice_end]\n                        if slice_end!=slice_start\n                        else np.nan)\n\n        elif type(item) is slice:\n            data = []\n            selected_indices = np.arange(len(self))[item]\n\n            for selected_index in selected_indices:\n                data.append(self[selected_index])\n\n            return RaggedArray(data, dtype=self.flat_array.dtype)\n\n        elif isinstance(item, (np.ndarray, ExtensionArray, list, tuple)):\n            if isinstance(item, (np.ndarray, ExtensionArray)):\n                # Leave numpy and pandas arrays alone\n                kind = item.dtype.kind\n            else:\n                # Convert others to pandas arrays\n                item = pd.array(item)\n                kind = item.dtype.kind\n\n            if len(item) == 0:\n                return self.take([], allow_fill=False)\n            elif kind == 'b':\n                # Check mask length is compatible\n                if len(item) != len(self):\n                    raise IndexError(\n                        \"Boolean index has wrong length: {} instead of {}\"\n                        .format(len(item), len(self))\n                    )\n\n                # check for NA values\n                isna = pd.isna(item)\n                if isna.any():\n                    if Version(pd.__version__) > Version('1.0.1'):\n                        item[isna] = False\n                    else:\n                        raise ValueError(\n                            \"Cannot mask with a boolean indexer containing NA values\"\n                        )\n\n                data = []\n\n                for i, m in enumerate(item):\n                    if m:\n                        data.append(self[i])\n\n                return RaggedArray(data, dtype=self.flat_array.dtype)\n            elif kind in ('i', 'u'):\n                if any(pd.isna(item)):\n                    raise ValueError(\n                        \"Cannot index with an integer indexer containing NA values\"\n                    )\n                return self.take(item, allow_fill=False)\n            else:\n                raise IndexError(err_msg)\n        else:\n            raise IndexError(err_msg)\n\n    @classmethod\n    def _from_sequence(cls, scalars, dtype=None, copy=False):\n        return RaggedArray(scalars, dtype=dtype)\n\n    @classmethod\n    def _from_factorized(cls, values, original):\n        return RaggedArray(\n            [_RaggedElement.array_or_nan(v) for v in values],\n            dtype=original.flat_array.dtype)\n\n    def _as_ragged_element_array(self):\n        return np.array([_RaggedElement.ragged_or_nan(self[i])\n                         for i in range(len(self))])\n\n    def _values_for_factorize(self):\n        return self._as_ragged_element_array(), np.nan\n\n    def _values_for_argsort(self):\n        return self._as_ragged_element_array()\n\n    def unique(self):\n        from pandas import unique\n\n        uniques = unique(self._as_ragged_element_array())\n        return self._from_sequence(\n            [_RaggedElement.array_or_nan(v) for v in uniques],\n            dtype=self.dtype)\n\n    def fillna(self, value=None, method=None, limit=None):\n        # Override in RaggedArray to handle ndarray fill values\n        from pandas.util._validators import validate_fillna_kwargs\n        from pandas.core.missing import get_fill_func\n\n        value, method = validate_fillna_kwargs(value, method)\n\n        mask = self.isna()\n\n        if isinstance(value, RaggedArray):\n            if len(value) != len(self):\n                raise ValueError(\"Length of 'value' does not match. Got ({}) \"\n                                 \" expected {}\".format(len(value), len(self)))\n            value = value[mask]\n\n        if mask.any():\n            if method is not None:\n                func = get_fill_func(method)\n                new_values = func(self.astype(object), limit=limit,\n                                  mask=mask)\n                new_values = self._from_sequence(new_values, dtype=self.dtype)\n            else:\n                # fill with value\n                new_values = list(self)\n                mask_indices, = np.where(mask)\n                for ind in mask_indices:\n                    new_values[ind] = value\n\n                new_values = self._from_sequence(new_values, dtype=self.dtype)\n        else:\n            new_values = self.copy()\n        return new_values\n\n    def shift(self, periods=1, fill_value=None):\n        # Override in RaggedArray to handle ndarray fill values\n\n        # Note: this implementation assumes that `self.dtype.na_value` can be\n        # stored in an instance of your ExtensionArray with `self.dtype`.\n        if not len(self) or periods == 0:\n            return self.copy()\n\n        if fill_value is None:\n            fill_value = np.nan\n\n        empty = self._from_sequence(\n            [fill_value] * min(abs(periods), len(self)),\n            dtype=self.dtype\n        )\n        if periods > 0:\n            a = empty\n            b = self[:-periods]\n        else:\n            a = self[abs(periods):]\n            b = empty\n        return self._concat_same_type([a, b])\n\n    def searchsorted(self, value, side=\"left\", sorter=None):\n        arr = self._as_ragged_element_array()\n        if isinstance(value, RaggedArray):\n            search_value = value._as_ragged_element_array()\n        else:\n            search_value = _RaggedElement(value)\n        return arr.searchsorted(search_value, side=side, sorter=sorter)\n\n    def isna(self):\n        stop_indices = np.hstack([self.start_indices[1:],\n                                  [len(self.flat_array)]])\n\n        element_lengths = stop_indices - self.start_indices\n        return element_lengths == 0\n\n    def take(self, indices, allow_fill=False, fill_value=None):\n        if allow_fill:\n            invalid_inds = [i for i in indices if i < -1]\n            if invalid_inds:\n                raise ValueError(\"\"\"\nInvalid indices for take with allow_fill True: {inds}\"\"\".format(\n                    inds=invalid_inds[:9]))\n            sequence = [self[i] if i >= 0 else fill_value\n                        for i in indices]\n        else:\n            if len(self) == 0 and len(indices) > 0:\n                raise IndexError(\n                    \"cannot do a non-empty take from an empty axis|out of bounds\"\n                )\n\n            sequence = [self[i] for i in indices]\n\n        return RaggedArray(sequence, dtype=self.flat_array.dtype)\n\n    def copy(self, deep=False):\n        data = dict(\n            flat_array=self.flat_array,\n            start_indices=self.start_indices)\n\n        return RaggedArray(data, copy=deep)\n\n    @classmethod\n    def _concat_same_type(cls, to_concat):\n        # concat flat_arrays\n        flat_array = np.hstack([ra.flat_array for ra in to_concat])\n\n        # offset and concat start_indices\n        offsets = np.hstack([\n            [0], np.cumsum([len(ra.flat_array) for ra in to_concat[:-1]])\n        ]).astype('uint64')\n\n        start_indices = np.hstack([ra.start_indices + offset\n                                   for offset, ra in zip(offsets, to_concat)])\n\n        return RaggedArray(dict(\n            flat_array=flat_array, start_indices=start_indices),\n            copy=False)\n\n    @property\n    def dtype(self):\n        return self._dtype\n\n    @property\n    def nbytes(self):\n        return (self._flat_array.nbytes +\n                self._start_indices.nbytes)\n\n    def astype(self, dtype, copy=True):\n        dtype = pandas_dtype(dtype)\n        if isinstance(dtype, RaggedDtype):\n            if copy:\n                return self.copy()\n            return self\n\n        elif is_extension_array_dtype(dtype):\n            return dtype.construct_array_type()._from_sequence(\n                np.asarray(self))\n\n        return np.array([v for v in self], dtype=dtype)\n\n    def tolist(self):\n        # Based on pandas ExtensionArray.tolist\n        if self.ndim > 1:\n            return [item.tolist() for item in self]\n        else:\n            return list(self)\n\n    def __array__(self, dtype=None, copy=True):\n        dtype = np.dtype(object) if dtype is None else np.dtype(dtype)\n        if copy:\n            return np.array(self.tolist(), dtype=dtype)\n        else:\n            return np.array(self, dtype=dtype)\n\n    def duplicated(self, *args, **kwargs):\n        msg = \"duplicated is not implemented for RaggedArray\"\n        raise NotImplementedError(msg)\n\n\n@jit(nopython=True, nogil=True)\ndef _eq_ragged_ragged(start_indices1,\n                      flat_array1,\n                      start_indices2,\n                      flat_array2):\n    \"\"\"\n    Compare elements of two ragged arrays of the same length\n\n    Parameters\n    ----------\n    start_indices1: ndarray\n        start indices of a RaggedArray 1\n    flat_array1: ndarray\n        flat_array property of a RaggedArray 1\n    start_indices2: ndarray\n        start indices of a RaggedArray 2\n    flat_array2: ndarray\n        flat_array property of a RaggedArray 2\n\n    Returns\n    -------\n    mask: ndarray\n        1D bool array of same length as inputs with elements True when\n        corresponding elements are equal, False otherwise\n    \"\"\"\n    n = len(start_indices1)\n    m1 = len(flat_array1)\n    m2 = len(flat_array2)\n\n    result = np.zeros(n, dtype=np.bool_)\n\n    for i in range(n):\n        # Extract inds for ra1\n        start_index1 = start_indices1[i]\n        stop_index1 = start_indices1[i + 1] if i < n - 1 else m1\n        len_1 = stop_index1 - start_index1\n\n        # Extract inds for ra2\n        start_index2 = start_indices2[i]\n        stop_index2 = start_indices2[i + 1] if i < n - 1 else m2\n        len_2 = stop_index2 - start_index2\n\n        if len_1 != len_2:\n            el_equal = False\n        else:\n            el_equal = True\n            for flat_index1, flat_index2 in \\\n                    zip(range(start_index1, stop_index1),\n                        range(start_index2, stop_index2)):\n                el_1 = flat_array1[flat_index1]\n                el_2 = flat_array2[flat_index2]\n                el_equal &= el_1 == el_2\n\n        result[i] = el_equal\n\n    return result\n\n\n@jit(nopython=True, nogil=True)\ndef _eq_ragged_scalar(start_indices, flat_array, val):\n    \"\"\"\n    Compare elements of a RaggedArray with a scalar array\n\n    Parameters\n    ----------\n    start_indices: ndarray\n        start indices of a RaggedArray\n    flat_array: ndarray\n        flat_array property of a RaggedArray\n    val: ndarray\n\n    Returns\n    -------\n    mask: ndarray\n        1D bool array of same length as inputs with elements True when\n        ragged element equals scalar val, False otherwise.\n    \"\"\"\n    n = len(start_indices)\n    m = len(flat_array)\n    cols = len(val)\n    result = np.zeros(n, dtype=np.bool_)\n    for i in range(n):\n        start_index = start_indices[i]\n        stop_index = start_indices[i+1] if i < n - 1 else m\n\n        if stop_index - start_index != cols:\n            el_equal = False\n        else:\n            el_equal = True\n            for val_index, flat_index in \\\n                    enumerate(range(start_index, stop_index)):\n                el_equal &= flat_array[flat_index] == val[val_index]\n        result[i] = el_equal\n\n    return result\n\n\ndef _eq_ragged_ndarray1d(start_indices, flat_array, a):\n    \"\"\"\n    Compare a RaggedArray with a 1D numpy object array of the same length\n\n    Parameters\n    ----------\n    start_indices: ndarray\n        start indices of a RaggedArray\n    flat_array: ndarray\n        flat_array property of a RaggedArray\n    a: ndarray\n        1D numpy array of same length as ra\n\n    Returns\n    -------\n    mask: ndarray\n        1D bool array of same length as input with elements True when\n        corresponding elements are equal, False otherwise\n\n    Notes\n    -----\n    This function is not numba accelerated because it, by design, inputs\n    a numpy object array\n    \"\"\"\n\n    n = len(start_indices)\n    m = len(flat_array)\n    result = np.zeros(n, dtype=np.bool_)\n    for i in range(n):\n        start_index = start_indices[i]\n        stop_index = start_indices[i + 1] if i < n - 1 else m\n        a_val = a[i]\n        if (a_val is None or\n                (np.isscalar(a_val) and np.isnan(a_val)) or\n                len(a_val) == 0):\n            result[i] = start_index == stop_index\n        else:\n            result[i] = np.array_equal(flat_array[start_index:stop_index],\n                                       a_val)\n\n    return result\n\n\n@jit(nopython=True, nogil=True)\ndef _eq_ragged_ndarray2d(start_indices, flat_array, a):\n    \"\"\"\n    Compare a RaggedArray with rows of a 2D numpy object array\n\n    Parameters\n    ----------\n    start_indices: ndarray\n        start indices of a RaggedArray\n    flat_array: ndarray\n        flat_array property of a RaggedArray\n    a: ndarray\n        A 2D numpy array where the length of the first dimension matches the\n        length of the RaggedArray\n\n    Returns\n    -------\n    mask: ndarray\n        1D bool array of same length as input RaggedArray with elements True\n        when corresponding elements of ra equal corresponding row of `a`\n    \"\"\"\n    n = len(start_indices)\n    m = len(flat_array)\n    cols = a.shape[1]\n\n    # np.bool is an alias for Python's built-in bool type, np.bool_ is the\n    # numpy type that numba recognizes\n    result = np.zeros(n, dtype=np.bool_)\n    for row in range(n):\n        start_index = start_indices[row]\n        stop_index = start_indices[row + 1] if row < n - 1 else m\n\n        # Check equality\n        if stop_index - start_index != cols:\n            el_equal = False\n        else:\n            el_equal = True\n            for col, flat_index in enumerate(range(start_index, stop_index)):\n                el_equal &= flat_array[flat_index] == a[row, col]\n        result[row] = el_equal\n    return result\n\n\n@jit(nopython=True, nogil=True)\ndef _lexograph_lt(a1, a2):\n    \"\"\"\n    Compare two 1D numpy arrays lexographically\n    Parameters\n    ----------\n    a1: ndarray\n        1D numpy array\n    a2: ndarray\n        1D numpy array\n\n    Returns\n    -------\n    comparison:\n        True if a1 < a2, False otherwise\n    \"\"\"\n    for e1, e2 in zip(a1, a2):\n        if e1 < e2:\n            return True\n        elif e1 > e2:\n            return False\n    return len(a1) < len(a2)\n\n\ndef ragged_array_non_empty(dtype):\n    return RaggedArray([[1], [1, 2]], dtype=dtype)\n\n\nif make_array_nonempty:\n    make_array_nonempty.register(RaggedDtype)(ragged_array_non_empty)\n",
    "datashader/datashape/user.py": "from .dispatch import dispatch\nfrom .coretypes import (\n    CType, Date, DateTime, DataShape, Record, String, Time, Var, from_numpy, to_numpy_dtype)\nfrom .predicates import isdimension\nfrom .util import dshape\nfrom datetime import date, time, datetime\nimport numpy as np\n\n\n__all__ = ['validate', 'issubschema']\n\n\nbasetypes = np.generic, int, float, str, date, time, datetime\n\n\n@dispatch(np.dtype, basetypes)\ndef validate(schema, value):\n    return np.issubdtype(type(value), schema)\n\n\n@dispatch(CType, basetypes)\ndef validate(schema, value):  # noqa: F811\n    return validate(to_numpy_dtype(schema), value)\n\n\n@dispatch(DataShape, (tuple, list))\ndef validate(schema, value):  # noqa: F811\n    head = schema[0]\n    return ((len(schema) == 1 and validate(head, value))\n        or (isdimension(head)\n       and (isinstance(head, Var) or int(head) == len(value))\n       and all(validate(DataShape(*schema[1:]), item) for item in value)))\n\n\n@dispatch(DataShape, object)\ndef validate(schema, value):  # noqa: F811\n    if len(schema) == 1:\n        return validate(schema[0], value)\n\n\n@dispatch(Record, dict)\ndef validate(schema, d):  # noqa: F811\n    return all(validate(sch, d.get(k)) for k, sch in schema.parameters[0])\n\n\n@dispatch(Record, (tuple, list))\ndef validate(schema, seq):  # noqa: F811\n    return all(validate(sch, item) for (k, sch), item\n                                    in zip(schema.parameters[0], seq))\n\n\n@dispatch(str, object)\ndef validate(schema, value):  # noqa: F811\n    return validate(dshape(schema), value)\n\n\n@dispatch(type, object)\ndef validate(schema, value):  # noqa: F811\n    return isinstance(value, schema)\n\n\n@dispatch(tuple, object)\ndef validate(schemas, value):  # noqa: F811\n    return any(validate(schema, value) for schema in schemas)\n\n\n@dispatch(object, object)\ndef validate(schema, value):  # noqa: F811\n    return False\n\n\n@validate.register(String, str)\n@validate.register(Time, time)\n@validate.register(Date, date)\n@validate.register(DateTime, datetime)\ndef validate_always_true(schema, value):\n    return True\n\n\n@dispatch(DataShape, np.ndarray)\ndef validate(schema, value):\n    return issubschema(from_numpy(value.shape, value.dtype), schema)\n\n\n@dispatch(object, object)\ndef issubschema(a, b):\n    return issubschema(dshape(a), dshape(b))\n\n\n@dispatch(DataShape, DataShape)\ndef issubschema(a, b):  # noqa: F811\n    if a == b:\n        return True\n    # TODO, handle cases like float < real\n    # TODO, handle records {x: int, y: int, z: int} < {x: int, y: int}\n\n    return None  # We don't know, return something falsey\n",
    "datashader/datashape/util/__init__.py": "\nfrom itertools import chain\nimport operator\n\nfrom .. import parser\nfrom .. import type_symbol_table\nfrom ..validation import validate\nfrom .. import coretypes\n\n\n__all__ = 'dshape', 'dshapes', 'has_var_dim', 'has_ellipsis', 'cat_dshapes'\n\nsubclasses = operator.methodcaller('__subclasses__')\n\n#------------------------------------------------------------------------\n# Utility Functions for DataShapes\n#------------------------------------------------------------------------\n\ndef dshapes(*args):\n    \"\"\"\n    Parse a bunch of datashapes all at once.\n\n    >>> a, b = dshapes('3 * int32', '2 * var * float64')\n    \"\"\"\n    return [dshape(arg) for arg in args]\n\n\ndef dshape(o):\n    \"\"\"\n    Parse a datashape. For a thorough description see\n    https://datashape.readthedocs.io/en/latest/\n\n    >>> ds = dshape('2 * int32')\n    >>> ds[1]\n    ctype(\"int32\")\n    \"\"\"\n    if isinstance(o, coretypes.DataShape):\n        return o\n    if isinstance(o, str):\n        ds = parser.parse(o, type_symbol_table.sym)\n    elif isinstance(o, (coretypes.CType, coretypes.String,\n                        coretypes.Record, coretypes.JSON,\n                        coretypes.Date, coretypes.Time, coretypes.DateTime,\n                        coretypes.Unit)):\n        ds = coretypes.DataShape(o)\n    elif isinstance(o, coretypes.Mono):\n        ds = o\n    elif isinstance(o, (list, tuple)):\n        ds = coretypes.DataShape(*o)\n    else:\n        raise TypeError('Cannot create dshape from object of type %s' % type(o))\n    validate(ds)\n    return ds\n\n\ndef cat_dshapes(dslist):\n    \"\"\"\n    Concatenates a list of dshapes together along\n    the first axis. Raises an error if there is\n    a mismatch along another axis or the measures\n    are different.\n\n    Requires that the leading dimension be a known\n    size for all data shapes.\n    TODO: Relax this restriction to support\n          streaming dimensions.\n\n    >>> cat_dshapes(dshapes('10 * int32', '5 * int32'))\n    dshape(\"15 * int32\")\n    \"\"\"\n    if len(dslist) == 0:\n        raise ValueError('Cannot concatenate an empty list of dshapes')\n    elif len(dslist) == 1:\n        return dslist[0]\n\n    outer_dim_size = operator.index(dslist[0][0])\n    inner_ds = dslist[0][1:]\n    for ds in dslist[1:]:\n        outer_dim_size += operator.index(ds[0])\n        if ds[1:] != inner_ds:\n            raise ValueError(('The datashapes to concatenate much'\n                              ' all match after'\n                              ' the first dimension (%s vs %s)') %\n                              (inner_ds, ds[1:]))\n    return coretypes.DataShape(*[coretypes.Fixed(outer_dim_size)] + list(inner_ds))\n\n\ndef collect(pred, expr):\n    \"\"\" Collect terms in expression that match predicate\n\n    >>> from datashader.datashape import Unit, dshape\n    >>> predicate = lambda term: isinstance(term, Unit)\n    >>> dshape = dshape('var * {value: int64, loc: 2 * int32}')\n    >>> sorted(set(collect(predicate, dshape)), key=str)\n    [Fixed(val=2), ctype(\"int32\"), ctype(\"int64\"), Var()]\n    >>> from datashader.datashape import var, int64\n    >>> sorted(set(collect(predicate, [var, int64])), key=str)\n    [ctype(\"int64\"), Var()]\n    \"\"\"\n    if pred(expr):\n        return [expr]\n    if isinstance(expr, coretypes.Record):\n        return chain.from_iterable(collect(pred, typ) for typ in expr.types)\n    if isinstance(expr, coretypes.Mono):\n        return chain.from_iterable(collect(pred, typ) for typ in expr.parameters)\n    if isinstance(expr, (list, tuple)):\n        return chain.from_iterable(collect(pred, item) for item in expr)\n\n\ndef has_var_dim(ds):\n    \"\"\"Returns True if datashape has a variable dimension\n\n    Note currently treats variable length string as scalars.\n\n    >>> has_var_dim(dshape('2 * int32'))\n    False\n    >>> has_var_dim(dshape('var * 2 * int32'))\n    True\n    \"\"\"\n    return has((coretypes.Ellipsis, coretypes.Var), ds)\n\n\ndef has(typ, ds):\n    if isinstance(ds, typ):\n        return True\n    if isinstance(ds, coretypes.Record):\n        return any(has(typ, t) for t in ds.types)\n    if isinstance(ds, coretypes.Mono):\n        return any(has(typ, p) for p in ds.parameters)\n    if isinstance(ds, (list, tuple)):\n        return any(has(typ, item) for item in ds)\n    return False\n\n\ndef has_ellipsis(ds):\n    \"\"\"Returns True if the datashape has an ellipsis\n\n    >>> has_ellipsis(dshape('2 * int'))\n    False\n    >>> has_ellipsis(dshape('... * int'))\n    True\n    \"\"\"\n    return has(coretypes.Ellipsis, ds)\n"
  },
  "GT_src_dict": {
    "datashader/datashape/coretypes.py": {},
    "datashader/datatypes.py": {},
    "datashader/datashape/user.py": {
      "validate": {
        "code": "def validate(schema, value):\n    \"\"\"Validates whether a given value conforms to a specified schema, particularly focusing on NumPy array structures.\n\nParameters:\n- schema (DataShape): The expected shape and data type of the value, defined in terms of NumPy dtype and shape.\n- value (np.ndarray): The value to be validated against the schema, expected to be a NumPy array.\n\nReturns:\n- bool: True if the schema matches the value's shape and data type; otherwise, False.\n\nThis function utilizes the `from_numpy` utility to convert the NumPy array's shape and dtype into a format suitable for validation against the provided schema. The function also depends on the `issubschema` function to determine if the structure of the array matches the expected schema. This integration allows for comprehensive validation of various data structures as defined in the context of the overall code.\"\"\"\n    return issubschema(from_numpy(value.shape, value.dtype), schema)",
        "docstring": "Validates whether a given value conforms to a specified schema, particularly focusing on NumPy array structures.\n\nParameters:\n- schema (DataShape): The expected shape and data type of the value, defined in terms of NumPy dtype and shape.\n- value (np.ndarray): The value to be validated against the schema, expected to be a NumPy array.\n\nReturns:\n- bool: True if the schema matches the value's shape and data type; otherwise, False.\n\nThis function utilizes the `from_numpy` utility to convert the NumPy array's shape and dtype into a format suitable for validation against the provided schema. The function also depends on the `issubschema` function to determine if the structure of the array matches the expected schema. This integration allows for comprehensive validation of various data structures as defined in the context of the overall code.",
        "signature": "def validate(schema, value):",
        "type": "Function",
        "class_signature": null
      },
      "issubschema": {
        "code": "def issubschema(a, b):\n    \"\"\"Checks if the data shape `a` is a subschema of the data shape `b`.\n\nParameters:\n- a: DataShape or object representing the schema of a data structure.\n- b: DataShape or object representing the schema against which `a` is being validated.\n\nReturns:\n- True if `a` is equivalent to `b`.\n- None if the relationship cannot be determined.\n\nThe function interacts with the `dshape` utility to convert schemas into a comparable format. The function has TODO items for handling specific cases, such as type hierarchies (e.g., float < real) and nested records. These cases need to be addressed to implement complete subschema validation.\"\"\"\n    if a == b:\n        return True\n    return None",
        "docstring": "Checks if the data shape `a` is a subschema of the data shape `b`.\n\nParameters:\n- a: DataShape or object representing the schema of a data structure.\n- b: DataShape or object representing the schema against which `a` is being validated.\n\nReturns:\n- True if `a` is equivalent to `b`.\n- None if the relationship cannot be determined.\n\nThe function interacts with the `dshape` utility to convert schemas into a comparable format. The function has TODO items for handling specific cases, such as type hierarchies (e.g., float < real) and nested records. These cases need to be addressed to implement complete subschema validation.",
        "signature": "def issubschema(a, b):",
        "type": "Function",
        "class_signature": null
      }
    },
    "datashader/datashape/util/__init__.py": {
      "dshape": {
        "code": "def dshape(o):\n    \"\"\"Parse a datashape representation into a DataShape object.\n\nThis function accepts various types of inputs, including strings, coretypes representing data shapes, and collections of types. It validates the resulting DataShape before returning it, ensuring correctness.\n\nParameters:\n- o (str, coretypes.DataShape, coretypes.CType, coretypes.String, \n      coretypes.Record, coretypes.JSON, coretypes.Date, \n      coretypes.Time, coretypes.DateTime, coretypes.Unit, \n      coretypes.Mono, list, tuple): The input representing a datashape, which can be a string or various compatible coretype instances.\n\nReturns:\n- coretypes.DataShape: An instance of DataShape that represents the parsed structure.\n\nRaises:\n- TypeError: If the input type is not supported for creating a datashape.\n\nDependencies:\n- `parser`: Used to parse string representations into datashapes.\n- `type_symbol_table`: Provides symbol definitions for parsing.\n- `validate`: Ensures that the constructed DataShape adheres to required validations.\n- `coretypes`: Contains various data structure definitions needed for creating a DataShape instance.\"\"\"\n    '\\n    Parse a datashape. For a thorough description see\\n    https://datashape.readthedocs.io/en/latest/\\n\\n    >>> ds = dshape(\\'2 * int32\\')\\n    >>> ds[1]\\n    ctype(\"int32\")\\n    '\n    if isinstance(o, coretypes.DataShape):\n        return o\n    if isinstance(o, str):\n        ds = parser.parse(o, type_symbol_table.sym)\n    elif isinstance(o, (coretypes.CType, coretypes.String, coretypes.Record, coretypes.JSON, coretypes.Date, coretypes.Time, coretypes.DateTime, coretypes.Unit)):\n        ds = coretypes.DataShape(o)\n    elif isinstance(o, coretypes.Mono):\n        ds = o\n    elif isinstance(o, (list, tuple)):\n        ds = coretypes.DataShape(*o)\n    else:\n        raise TypeError('Cannot create dshape from object of type %s' % type(o))\n    validate(ds)\n    return ds",
        "docstring": "Parse a datashape representation into a DataShape object.\n\nThis function accepts various types of inputs, including strings, coretypes representing data shapes, and collections of types. It validates the resulting DataShape before returning it, ensuring correctness.\n\nParameters:\n- o (str, coretypes.DataShape, coretypes.CType, coretypes.String, \n      coretypes.Record, coretypes.JSON, coretypes.Date, \n      coretypes.Time, coretypes.DateTime, coretypes.Unit, \n      coretypes.Mono, list, tuple): The input representing a datashape, which can be a string or various compatible coretype instances.\n\nReturns:\n- coretypes.DataShape: An instance of DataShape that represents the parsed structure.\n\nRaises:\n- TypeError: If the input type is not supported for creating a datashape.\n\nDependencies:\n- `parser`: Used to parse string representations into datashapes.\n- `type_symbol_table`: Provides symbol definitions for parsing.\n- `validate`: Ensures that the constructed DataShape adheres to required validations.\n- `coretypes`: Contains various data structure definitions needed for creating a DataShape instance.",
        "signature": "def dshape(o):",
        "type": "Function",
        "class_signature": null
      }
    }
  },
  "dependency_dict": {
    "datashader/datashape/user.py:validate": {},
    "datashader/datashape/util/__init__.py:dshape": {
      "datashader/datashape/validation.py": {
        "validate": {
          "code": "def validate(ds):\n    \"\"\"\n    Validate a datashape to see whether it is well-formed.\n\n    Parameters\n    ----------\n    ds : DataShape\n\n    Examples\n    --------\n    >>> from datashader.datashape import dshape\n    >>> dshape('10 * int32')\n    dshape(\"10 * int32\")\n    >>> dshape('... * int32')\n    dshape(\"... * int32\")\n    >>> dshape('... * ... * int32') # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    TypeError: Can only use a single wildcard\n    >>> dshape('T * ... * X * ... * X') # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    TypeError: Can only use a single wildcard\n    >>> dshape('T * ...') # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    DataShapeSyntaxError: Expected a dtype\n    \"\"\"\n    traverse(_validate, ds)",
          "docstring": "Validate a datashape to see whether it is well-formed.\n\nParameters\n----------\nds : DataShape\n\nExamples\n--------\n>>> from datashader.datashape import dshape\n>>> dshape('10 * int32')\ndshape(\"10 * int32\")\n>>> dshape('... * int32')\ndshape(\"... * int32\")\n>>> dshape('... * ... * int32') # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\n    ...\nTypeError: Can only use a single wildcard\n>>> dshape('T * ... * X * ... * X') # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\n    ...\nTypeError: Can only use a single wildcard\n>>> dshape('T * ...') # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\n    ...\nDataShapeSyntaxError: Expected a dtype",
          "signature": "def validate(ds):",
          "type": "Function",
          "class_signature": null
        }
      },
      "datashader/datashape/parser.py": {
        "parse": {
          "code": "def parse(ds_str, sym):\n    \"\"\"Parses a single datashape from a string.\n\n    Parameters\n    ----------\n    ds_str : string\n        The datashape string to parse.\n    sym : TypeSymbolTable\n        The symbol tables of dimensions, dtypes, and type constructors for each.\n\n    \"\"\"\n    dsp = DataShapeParser(ds_str, sym)\n    ds = dsp.parse_datashape()\n    # If no datashape could be found\n    if ds is None:\n        dsp.raise_error('Invalid datashape')\n\n    # Make sure there's no garbage at the end\n    if dsp.pos != dsp.end_pos:\n        dsp.raise_error('Unexpected token in datashape')\n    return ds",
          "docstring": "Parses a single datashape from a string.\n\nParameters\n----------\nds_str : string\n    The datashape string to parse.\nsym : TypeSymbolTable\n    The symbol tables of dimensions, dtypes, and type constructors for each.",
          "signature": "def parse(ds_str, sym):",
          "type": "Function",
          "class_signature": null
        }
      }
    },
    "datashader/datashape/user.py:issubschema": {}
  },
  "call_tree": {
    "datashader/datashape/tests/test_user.py:test_numeric_tower": {
      "datashader/datashape/user.py:validate": {}
    },
    "datashader/datashape/tests/test_user.py:test_datetimes": {
      "datashader/datashape/user.py:validate": {
        "datashader/datashape/util/__init__.py:dshape": {
          "datashader/datashape/parser.py:parse": {
            "datashader/datashape/parser.py:DataShapeParser:__init__": {
              "datashader/datashape/parser.py:DataShapeParser:advance_tok": {
                "datashader/datashape/lexer.py:lex": {}
              }
            },
            "datashader/datashape/parser.py:DataShapeParser:parse_datashape": {
              "datashader/datashape/parser.py:DataShapeParser:tok": {},
              "datashader/datashape/parser.py:DataShapeParser:parse_datashape_nooption": {
                "datashader/datashape/parser.py:DataShapeParser:parse_dim": {
                  "datashader/datashape/parser.py:DataShapeParser:tok": {},
                  "datashader/datashape/parser.py:DataShapeParser:advance_tok": {
                    "datashader/datashape/lexer.py:lex": {}
                  }
                },
                "datashader/datashape/parser.py:DataShapeParser:parse_dtype": {
                  "datashader/datashape/parser.py:DataShapeParser:tok": {},
                  "datashader/datashape/parser.py:DataShapeParser:advance_tok": {}
                },
                "datashader/datashape/coretypes.py:Mono:__len__": {},
                "datashader/datashape/coretypes.py:DataShape:__init__": {
                  "datashader/datashape/coretypes.py:_launder": {
                    "datashader/datashape/coretypes.py:DataShape:DataShape": {},
                    "datashader/datashape/coretypes.py:Mono:Mono": {}
                  }
                }
              }
            }
          },
          "datashader/datashape/validation.py:validate": {
            "datashader/datashape/coretypes.py:Var:Var": {},
            "datashader/datashape/validation.py:traverse": {
              "datashader/datashape/coretypes.py:Mono:parameters": {
                "datashader/datashape/coretypes.py:Mono:_slotted": {}
              },
              "datashader/datashape/validation.py:traverse": {
                "[ignored_or_cut_off]": "..."
              },
              "datashader/datashape/validation.py:_validate": {
                "datashader/datashape/coretypes.py:Mono:parameters": {
                  "datashader/datashape/coretypes.py:Mono:_slotted": {}
                }
              }
            }
          }
        },
        "datashader/datashape/user.py:validate": {
          "[ignored_or_cut_off]": "..."
        }
      }
    },
    "datashader/datashape/tests/test_user.py:test_numpy": {
      "datashader/datashape/user.py:validate": {
        "datashader/datashape/util/__init__.py:dshape": {
          "datashader/datashape/parser.py:parse": {
            "datashader/datashape/parser.py:DataShapeParser:__init__": {
              "datashader/datashape/parser.py:DataShapeParser:advance_tok": {
                "datashader/datashape/lexer.py:lex": {}
              }
            },
            "datashader/datashape/parser.py:DataShapeParser:parse_datashape": {
              "datashader/datashape/parser.py:DataShapeParser:tok": {},
              "datashader/datashape/parser.py:DataShapeParser:parse_datashape_nooption": {
                "datashader/datashape/parser.py:DataShapeParser:parse_dim": {
                  "datashader/datashape/parser.py:DataShapeParser:tok": {},
                  "datashader/datashape/parser.py:DataShapeParser:advance_tok": {
                    "datashader/datashape/lexer.py:lex": {}
                  },
                  "datashader/datashape/parser.py:DataShapeParser:syntactic_sugar": {},
                  "datashader/datashape/coretypes.py:Fixed:__init__": {}
                },
                "datashader/datashape/parser.py:DataShapeParser:tok": {},
                "datashader/datashape/parser.py:DataShapeParser:advance_tok": {
                  "datashader/datashape/lexer.py:lex": {}
                },
                "datashader/datashape/parser.py:DataShapeParser:parse_datashape": {
                  "[ignored_or_cut_off]": "..."
                },
                "datashader/datashape/coretypes.py:Mono:parameters": {
                  "datashader/datashape/coretypes.py:Mono:_slotted": {}
                },
                "datashader/datashape/coretypes.py:DataShape:__init__": {
                  "datashader/datashape/coretypes.py:_launder": {}
                }
              }
            }
          },
          "datashader/datashape/validation.py:validate": {
            "datashader/datashape/validation.py:traverse": {
              "datashader/datashape/coretypes.py:Mono:parameters": {
                "datashader/datashape/coretypes.py:Mono:_slotted": {}
              },
              "datashader/datashape/validation.py:traverse": {
                "[ignored_or_cut_off]": "..."
              },
              "datashader/datashape/validation.py:_validate": {
                "datashader/datashape/coretypes.py:Mono:parameters": {
                  "datashader/datashape/coretypes.py:Mono:_slotted": {}
                }
              }
            }
          }
        },
        "datashader/datashape/user.py:validate": {
          "[ignored_or_cut_off]": "..."
        }
      }
    },
    "datashader/datashape/tests/test_user.py:test_issubschema": {
      "datashader/datashape/user.py:issubschema": {
        "datashader/datashape/util/__init__.py:dshape": {
          "datashader/datashape/parser.py:parse": {
            "datashader/datashape/parser.py:DataShapeParser:__init__": {
              "datashader/datashape/parser.py:DataShapeParser:advance_tok": {
                "datashader/datashape/lexer.py:lex": {}
              }
            },
            "datashader/datashape/parser.py:DataShapeParser:parse_datashape": {
              "datashader/datashape/parser.py:DataShapeParser:tok": {},
              "datashader/datashape/parser.py:DataShapeParser:parse_datashape_nooption": {
                "datashader/datashape/parser.py:DataShapeParser:parse_dim": {
                  "datashader/datashape/parser.py:DataShapeParser:tok": {},
                  "datashader/datashape/parser.py:DataShapeParser:advance_tok": {
                    "datashader/datashape/lexer.py:lex": {}
                  },
                  "datashader/datashape/parser.py:DataShapeParser:syntactic_sugar": {},
                  "datashader/datashape/coretypes.py:Fixed:__init__": {}
                },
                "datashader/datashape/parser.py:DataShapeParser:parse_dtype": {
                  "datashader/datashape/parser.py:DataShapeParser:tok": {},
                  "datashader/datashape/parser.py:DataShapeParser:advance_tok": {}
                },
                "datashader/datashape/coretypes.py:Mono:__len__": {},
                "datashader/datashape/coretypes.py:DataShape:__init__": {
                  "datashader/datashape/coretypes.py:_launder": {}
                },
                "datashader/datashape/parser.py:DataShapeParser:tok": {},
                "datashader/datashape/parser.py:DataShapeParser:advance_tok": {
                  "datashader/datashape/lexer.py:lex": {}
                },
                "datashader/datashape/parser.py:DataShapeParser:parse_datashape": {
                  "[ignored_or_cut_off]": "..."
                },
                "datashader/datashape/coretypes.py:Mono:parameters": {
                  "datashader/datashape/coretypes.py:Mono:_slotted": {}
                }
              }
            }
          },
          "datashader/datashape/validation.py:validate": {
            "datashader/datashape/validation.py:traverse": {
              "datashader/datashape/coretypes.py:Mono:parameters": {
                "datashader/datashape/coretypes.py:Mono:_slotted": {}
              },
              "datashader/datashape/validation.py:traverse": {
                "[ignored_or_cut_off]": "..."
              },
              "datashader/datashape/validation.py:_validate": {
                "datashader/datashape/coretypes.py:Mono:parameters": {
                  "datashader/datashape/coretypes.py:Mono:_slotted": {}
                }
              }
            }
          }
        },
        "datashader/datashape/user.py:issubschema": {
          "[ignored_or_cut_off]": "..."
        }
      }
    },
    "datashader/datashape/tests/test_user.py:test_integration": {
      "datashader/datashape/user.py:validate": {
        "datashader/datashape/util/__init__.py:dshape": {
          "datashader/datashape/parser.py:parse": {
            "datashader/datashape/parser.py:DataShapeParser:__init__": {
              "datashader/datashape/parser.py:DataShapeParser:advance_tok": {
                "datashader/datashape/lexer.py:lex": {}
              }
            },
            "datashader/datashape/parser.py:DataShapeParser:parse_datashape": {
              "datashader/datashape/parser.py:DataShapeParser:tok": {},
              "datashader/datashape/parser.py:DataShapeParser:parse_datashape_nooption": {
                "datashader/datashape/parser.py:DataShapeParser:parse_dim": {
                  "datashader/datashape/parser.py:DataShapeParser:tok": {}
                },
                "datashader/datashape/parser.py:DataShapeParser:parse_dtype": {
                  "datashader/datashape/parser.py:DataShapeParser:tok": {},
                  "datashader/datashape/parser.py:DataShapeParser:parse_struct_type": {
                    "datashader/datashape/parser.py:DataShapeParser:tok": {},
                    "datashader/datashape/parser.py:DataShapeParser:advance_tok": {
                      "datashader/datashape/lexer.py:lex": {}
                    },
                    "datashader/datashape/parser.py:DataShapeParser:parse_homogeneous_list": {
                      "datashader/datashape/parser.py:DataShapeParser:parse_struct_field": {},
                      "datashader/datashape/parser.py:DataShapeParser:tok": {},
                      "datashader/datashape/parser.py:DataShapeParser:advance_tok": {}
                    },
                    "datashader/datashape/parser.py:DataShapeParser:syntactic_sugar": {},
                    "datashader/datashape/type_symbol_table.py:_struct": {
                      "datashader/datashape/coretypes.py:Record:__init__": {}
                    }
                  }
                },
                "datashader/datashape/coretypes.py:Mono:__len__": {},
                "datashader/datashape/coretypes.py:DataShape:__init__": {
                  "datashader/datashape/coretypes.py:_launder": {}
                }
              }
            }
          },
          "datashader/datashape/validation.py:validate": {
            "datashader/datashape/validation.py:traverse": {
              "datashader/datashape/coretypes.py:Mono:parameters": {
                "datashader/datashape/coretypes.py:Mono:_slotted": {}
              },
              "datashader/datashape/validation.py:traverse": {
                "[ignored_or_cut_off]": "..."
              },
              "datashader/datashape/validation.py:_validate": {
                "datashader/datashape/coretypes.py:Mono:parameters": {
                  "datashader/datashape/coretypes.py:Mono:_slotted": {}
                }
              }
            }
          }
        },
        "datashader/datashape/user.py:validate": {
          "[ignored_or_cut_off]": "..."
        }
      }
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_user/datashader-test_user/datashader/tests/test_pandas.py:test_line_manual_range": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_user/datashader-test_user/datashader/tests/test_pandas.py:test_line_autorange": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_user/datashader-test_user/datashader/tests/test_pandas.py:test_area_to_zero_fixedrange": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_user/datashader-test_user/datashader/tests/test_pandas.py:test_area_to_zero_autorange": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_user/datashader-test_user/datashader/tests/test_pandas.py:test_area_to_zero_autorange_gap": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_user/datashader-test_user/datashader/tests/test_pandas.py:test_area_to_line_autorange": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_user/datashader-test_user/datashader/datashape/tests/test_coretypes.py:test_record_parse_optional": {
      "datashader/datashape/coretypes.py:Option:Option": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_user/datashader-test_user/modified_testcases/test_pandas.py:test_line_manual_range": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_user/datashader-test_user/modified_testcases/test_pandas.py:test_line_autorange": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_user/datashader-test_user/modified_testcases/test_pandas.py:test_area_to_zero_fixedrange": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_user/datashader-test_user/modified_testcases/test_pandas.py:test_area_to_zero_autorange": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_user/datashader-test_user/modified_testcases/test_pandas.py:test_area_to_zero_autorange_gap": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_user/datashader-test_user/modified_testcases/test_pandas.py:test_area_to_line_autorange": {
      "datashader/datatypes.py:RaggedDtype": {}
    }
  },
  "PRD": "# PROJECT NAME: datashader-test_user\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 datashader/\n    \u251c\u2500\u2500 datashape/\n    \u2502   \u251c\u2500\u2500 coretypes.py\n    \u2502   \u2502   \u2514\u2500\u2500 Option.Option\n    \u2502   \u251c\u2500\u2500 user.py\n    \u2502   \u2502   \u251c\u2500\u2500 issubschema\n    \u2502   \u2502   \u2514\u2500\u2500 validate\n    \u2502   \u2514\u2500\u2500 util/\n    \u2502       \u2514\u2500\u2500 __init__.py\n    \u2502           \u2514\u2500\u2500 dshape\n    \u2514\u2500\u2500 datatypes.py\n        \u2514\u2500\u2500 RaggedDtype\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThe module is designed to validate data structures and type schemas, ensuring conformance between expected data definitions and their actual instances. It provides robust functionality for checking compatibility of values with expected types, nested iterable structures, dictionaries with specified field constraints, and numpy arrays, as well as datetime-specific types. By leveraging compatibility checks and schema validation, the module addresses the challenge of ensuring data integrity and consistency in applications that handle dynamic or complex data inputs, simplifying error detection and type verification for developers. This ensures reliable handling of structured data in a wide array of use cases, enabling smoother integration with data processing workflows.\n\n## FILE 1: datashader/datashape/coretypes.py\n\n## FILE 2: datashader/datatypes.py\n\n## FILE 3: datashader/datashape/user.py\n\n- FUNCTION NAME: validate\n  - SIGNATURE: def validate(schema, value):\n  - DOCSTRING: \n```python\n\"\"\"\nValidates whether a given value conforms to a specified schema, particularly focusing on NumPy array structures.\n\nParameters:\n- schema (DataShape): The expected shape and data type of the value, defined in terms of NumPy dtype and shape.\n- value (np.ndarray): The value to be validated against the schema, expected to be a NumPy array.\n\nReturns:\n- bool: True if the schema matches the value's shape and data type; otherwise, False.\n\nThis function utilizes the `from_numpy` utility to convert the NumPy array's shape and dtype into a format suitable for validation against the provided schema. The function also depends on the `issubschema` function to determine if the structure of the array matches the expected schema. This integration allows for comprehensive validation of various data structures as defined in the context of the overall code.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - datashader/datashape/user.py:validate\n    - datashader/datashape/util/__init__.py:dshape\n\n- FUNCTION NAME: issubschema\n  - SIGNATURE: def issubschema(a, b):\n  - DOCSTRING: \n```python\n\"\"\"\nChecks if the data shape `a` is a subschema of the data shape `b`.\n\nParameters:\n- a: DataShape or object representing the schema of a data structure.\n- b: DataShape or object representing the schema against which `a` is being validated.\n\nReturns:\n- True if `a` is equivalent to `b`.\n- None if the relationship cannot be determined.\n\nThe function interacts with the `dshape` utility to convert schemas into a comparable format. The function has TODO items for handling specific cases, such as type hierarchies (e.g., float < real) and nested records. These cases need to be addressed to implement complete subschema validation.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - datashader/datashape/util/__init__.py:dshape\n    - datashader/datashape/user.py:issubschema\n\n## FILE 4: datashader/datashape/util/__init__.py\n\n- FUNCTION NAME: dshape\n  - SIGNATURE: def dshape(o):\n  - DOCSTRING: \n```python\n\"\"\"\nParse a datashape representation into a DataShape object.\n\nThis function accepts various types of inputs, including strings, coretypes representing data shapes, and collections of types. It validates the resulting DataShape before returning it, ensuring correctness.\n\nParameters:\n- o (str, coretypes.DataShape, coretypes.CType, coretypes.String, \n      coretypes.Record, coretypes.JSON, coretypes.Date, \n      coretypes.Time, coretypes.DateTime, coretypes.Unit, \n      coretypes.Mono, list, tuple): The input representing a datashape, which can be a string or various compatible coretype instances.\n\nReturns:\n- coretypes.DataShape: An instance of DataShape that represents the parsed structure.\n\nRaises:\n- TypeError: If the input type is not supported for creating a datashape.\n\nDependencies:\n- `parser`: Used to parse string representations into datashapes.\n- `type_symbol_table`: Provides symbol definitions for parsing.\n- `validate`: Ensures that the constructed DataShape adheres to required validations.\n- `coretypes`: Contains various data structure definitions needed for creating a DataShape instance.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - datashader/datashape/validation.py:validate\n    - datashader/datashape/user.py:validate\n    - datashader/datashape/parser.py:parse\n    - datashader/datashape/user.py:issubschema\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "datashader/datashape/coretypes.py": "\"\"\"\nThis defines the DataShape type system, with unified\nshape and data type.\n\"\"\"\nimport ctypes\nimport operator\nfrom collections import OrderedDict\nfrom math import ceil\nfrom datashader import datashape\nimport numpy as np\nfrom .internal_utils import IndexCallable, isidentifier\nDIMENSION = 1\nMEASURE = 2\n\nclass Type(type):\n    _registry = {}\n\n    def __new__(meta, name, bases, dct):\n        cls = super(Type, meta).__new__(meta, name, bases, dct)\n        if not dct.get('abstract'):\n            Type._registry[name] = cls\n        return cls\n\n    @classmethod\n    def register(cls, name, type):\n        if name in cls._registry:\n            raise TypeError('There is another type registered with name %s' % name)\n        cls._registry[name] = type\n\n    @classmethod\n    def lookup_type(cls, name):\n        return cls._registry[name]\n\nclass Mono(metaclass=Type):\n    \"\"\"\n    Monotype are unqualified 0 parameters.\n\n    Each type must be reconstructable using its parameters:\n\n        type(datashape_type)(*type.parameters)\n    \"\"\"\n    composite = False\n\n    def __init__(self, *params):\n        self._parameters = params\n\n    @property\n    def _slotted(self):\n        return hasattr(self, '__slots__')\n\n    @property\n    def parameters(self):\n        if self._slotted:\n            return tuple((getattr(self, slot) for slot in self.__slots__))\n        else:\n            return self._parameters\n\n    def info(self):\n        return (type(self), self.parameters)\n\n    def __eq__(self, other):\n        return isinstance(other, Mono) and self.shape == other.shape and (self.measure.info() == other.measure.info())\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __hash__(self):\n        try:\n            h = self._hash\n        except AttributeError:\n            h = self._hash = hash(self.shape) ^ hash(self.measure.info())\n        return h\n\n    @property\n    def shape(self):\n        return ()\n\n    def __len__(self):\n        return 1\n\n    def __getitem__(self, key):\n        return [self][key]\n\n    def __repr__(self):\n        return '%s(%s)' % (type(self).__name__, ', '.join(('%s=%r' % (slot, getattr(self, slot)) for slot in self.__slots__) if self._slotted else map(repr, self.parameters)))\n\n    @property\n    def measure(self):\n        return self\n\n    def subarray(self, leading):\n        \"\"\"Returns a data shape object of the subarray with 'leading'\n        dimensions removed. In the case of a measure such as CType,\n        'leading' must be 0, and self is returned.\n        \"\"\"\n        if leading >= 1:\n            raise IndexError('Not enough dimensions in data shape to remove %d leading dimensions.' % leading)\n        else:\n            return self\n\n    def __mul__(self, other):\n        if isinstance(other, str):\n            from datashader import datashape\n            return datashape.dshape(other).__rmul__(self)\n        if isinstance(other, int):\n            other = Fixed(other)\n        if isinstance(other, DataShape):\n            return other.__rmul__(self)\n        return DataShape(self, other)\n\n    def __rmul__(self, other):\n        if isinstance(other, str):\n            from datashader import datashape\n            return self * datashape.dshape(other)\n        if isinstance(other, int):\n            other = Fixed(other)\n        return DataShape(other, self)\n\n    def __getstate__(self):\n        return self.parameters\n\n    def __setstate__(self, state):\n        if self._slotted:\n            for slot, val in zip(self.__slots__, state):\n                setattr(self, slot, val)\n        else:\n            self._parameters = state\n\n    def to_numpy_dtype(self):\n        raise TypeError('DataShape %s is not NumPy-compatible' % self)\n\nclass Unit(Mono):\n    \"\"\"\n    Unit type that does not need to be reconstructed.\n    \"\"\"\n\n    def __str__(self):\n        return type(self).__name__.lower()\n\nclass Ellipsis(Mono):\n    \"\"\"Ellipsis (...). Used to indicate a variable number of dimensions.\n\n    E.g.:\n\n        ... * float32    # float32 array w/ any number of dimensions\n        A... * float32   # float32 array w/ any number of dimensions,\n                        # associated with type variable A\n    \"\"\"\n    __slots__ = ('typevar',)\n\n    def __init__(self, typevar=None):\n        self.typevar = typevar\n\n    def __str__(self):\n        return str(self.typevar) + '...' if self.typevar else '...'\n\n    def __repr__(self):\n        return '%s(%r)' % (type(self).__name__, str(self))\n\nclass Null(Unit):\n    \"\"\"The null datashape.\"\"\"\n    pass\n\nclass Date(Unit):\n    \"\"\" Date type \"\"\"\n    cls = MEASURE\n    __slots__ = ()\n\n    def to_numpy_dtype(self):\n        return np.dtype('datetime64[D]')\n\nclass Time(Unit):\n    \"\"\" Time type \"\"\"\n    cls = MEASURE\n    __slots__ = ('tz',)\n\n    def __init__(self, tz=None):\n        if tz is not None and (not isinstance(tz, str)):\n            raise TypeError('tz parameter to time datashape must be a string')\n        self.tz = tz\n\n    def __str__(self):\n        basename = super().__str__()\n        if self.tz is None:\n            return basename\n        else:\n            return '%s[tz=%r]' % (basename, str(self.tz))\n\nclass DateTime(Unit):\n    \"\"\" DateTime type \"\"\"\n    cls = MEASURE\n    __slots__ = ('tz',)\n\n    def __init__(self, tz=None):\n        if tz is not None and (not isinstance(tz, str)):\n            raise TypeError('tz parameter to datetime datashape must be a string')\n        self.tz = tz\n\n    def __str__(self):\n        basename = super().__str__()\n        if self.tz is None:\n            return basename\n        else:\n            return '%s[tz=%r]' % (basename, str(self.tz))\n\n    def to_numpy_dtype(self):\n        return np.dtype('datetime64[us]')\n_units = ('ns', 'us', 'ms', 's', 'm', 'h', 'D', 'W', 'M', 'Y')\n_unit_aliases = {'year': 'Y', 'week': 'W', 'day': 'D', 'date': 'D', 'hour': 'h', 'second': 's', 'millisecond': 'ms', 'microsecond': 'us', 'nanosecond': 'ns'}\n\ndef normalize_time_unit(s):\n    \"\"\" Normalize time input to one of 'year', 'second', 'millisecond', etc..\n    Example\n    -------\n    >>> normalize_time_unit('milliseconds')\n    'ms'\n    >>> normalize_time_unit('ms')\n    'ms'\n    >>> normalize_time_unit('nanoseconds')\n    'ns'\n    >>> normalize_time_unit('nanosecond')\n    'ns'\n    \"\"\"\n    s = s.strip()\n    if s in _units:\n        return s\n    if s in _unit_aliases:\n        return _unit_aliases[s]\n    if s[-1] == 's' and len(s) > 2:\n        return normalize_time_unit(s.rstrip('s'))\n    raise ValueError('Do not understand time unit %s' % s)\n\nclass TimeDelta(Unit):\n    cls = MEASURE\n    __slots__ = ('unit',)\n\n    def __init__(self, unit='us'):\n        self.unit = normalize_time_unit(str(unit))\n\n    def __str__(self):\n        return 'timedelta[unit=%r]' % self.unit\n\n    def to_numpy_dtype(self):\n        return np.dtype('timedelta64[%s]' % self.unit)\n\nclass Units(Unit):\n    \"\"\" Units type for values with physical units \"\"\"\n    cls = MEASURE\n    __slots__ = ('unit', 'tp')\n\n    def __init__(self, unit, tp=None):\n        if not isinstance(unit, str):\n            raise TypeError('unit parameter to units datashape must be a string')\n        if tp is None:\n            tp = DataShape(float64)\n        elif not isinstance(tp, DataShape):\n            raise TypeError('tp parameter to units datashape must be a datashape type')\n        self.unit = unit\n        self.tp = tp\n\n    def __str__(self):\n        if self.tp == DataShape(float64):\n            return 'units[%r]' % self.unit\n        else:\n            return 'units[%r, %s]' % (self.unit, self.tp)\n\nclass Bytes(Unit):\n    \"\"\" Bytes type \"\"\"\n    cls = MEASURE\n    __slots__ = ()\n_canonical_string_encodings = {'A': 'A', 'ascii': 'A', 'U8': 'U8', 'utf-8': 'U8', 'utf_8': 'U8', 'utf8': 'U8', 'U16': 'U16', 'utf-16': 'U16', 'utf_16': 'U16', 'utf16': 'U16', 'U32': 'U32', 'utf-32': 'U32', 'utf_32': 'U32', 'utf32': 'U32'}\n\nclass String(Unit):\n    \"\"\" String container\n\n    >>> String()\n    ctype(\"string\")\n    >>> String(10, 'ascii')\n    ctype(\"string[10, 'A']\")\n    \"\"\"\n    cls = MEASURE\n    __slots__ = ('fixlen', 'encoding')\n\n    def __init__(self, *args):\n        if len(args) == 0:\n            fixlen, encoding = (None, None)\n        if len(args) == 1:\n            if isinstance(args[0], str):\n                fixlen, encoding = (None, args[0])\n            if isinstance(args[0], int):\n                fixlen, encoding = (args[0], None)\n        elif len(args) == 2:\n            fixlen, encoding = args\n        encoding = encoding or 'U8'\n        if isinstance(encoding, str):\n            encoding = str(encoding)\n        try:\n            encoding = _canonical_string_encodings[encoding]\n        except KeyError:\n            raise ValueError('Unsupported string encoding %s' % repr(encoding))\n        self.encoding = encoding\n        self.fixlen = fixlen\n\n    def __str__(self):\n        if self.fixlen is None and self.encoding == 'U8':\n            return 'string'\n        elif self.fixlen is not None and self.encoding == 'U8':\n            return 'string[%i]' % self.fixlen\n        elif self.fixlen is None and self.encoding != 'U8':\n            return 'string[%s]' % repr(self.encoding).strip('u')\n        else:\n            return 'string[%i, %s]' % (self.fixlen, repr(self.encoding).strip('u'))\n\n    def __repr__(self):\n        s = str(self)\n        return 'ctype(\"%s\")' % s.encode('unicode_escape').decode('ascii')\n\n    def to_numpy_dtype(self):\n        \"\"\"\n        >>> String().to_numpy_dtype()\n        dtype('O')\n        >>> String(30).to_numpy_dtype()\n        dtype('<U30')\n        >>> String(30, 'A').to_numpy_dtype()\n        dtype('S30')\n        \"\"\"\n        if self.fixlen:\n            if self.encoding == 'A':\n                return np.dtype('S%d' % self.fixlen)\n            else:\n                return np.dtype('U%d' % self.fixlen)\n        return np.dtype('O', metadata={'vlen': str})\n\nclass Decimal(Unit):\n    \"\"\"Decimal type corresponding to SQL Decimal/Numeric types.\n\n    The first parameter passed specifies the number of digits of precision that\n    the Decimal contains. If an additional parameter is given, it represents\n    the scale, or number of digits of precision that are after the decimal\n    point.\n\n    The Decimal type makes no requirement of how it is to be stored in memory,\n    therefore, the number of bytes needed to store a Decimal for a given\n    precision will vary based on the platform where it is used.\n\n    Examples\n    --------\n    >>> Decimal(18)\n    Decimal(precision=18, scale=0)\n    >>> Decimal(7, 4)\n    Decimal(precision=7, scale=4)\n    >>> Decimal(precision=11, scale=2)\n    Decimal(precision=11, scale=2)\n    \"\"\"\n    cls = MEASURE\n    __slots__ = ('precision', 'scale')\n\n    def __init__(self, precision, scale=0):\n        self.precision = precision\n        self.scale = scale\n\n    def __str__(self):\n        return 'decimal[precision={precision}, scale={scale}]'.format(precision=self.precision, scale=self.scale)\n\n    def to_numpy_dtype(self):\n        \"\"\"Convert a decimal datashape to a NumPy dtype.\n\n        Note that floating-point (scale > 0) precision will be lost converting\n        to NumPy floats.\n\n        Examples\n        --------\n        >>> Decimal(18).to_numpy_dtype()\n        dtype('int64')\n        >>> Decimal(7,4).to_numpy_dtype()\n        dtype('float64')\n        \"\"\"\n        if self.scale == 0:\n            if self.precision <= 2:\n                return np.dtype(np.int8)\n            elif self.precision <= 4:\n                return np.dtype(np.int16)\n            elif self.precision <= 9:\n                return np.dtype(np.int32)\n            elif self.precision <= 18:\n                return np.dtype(np.int64)\n            else:\n                raise TypeError('Integer Decimal precision > 18 is not NumPy-compatible')\n        else:\n            return np.dtype(np.float64)\n\nclass DataShape(Mono):\n    \"\"\"\n    Composite container for datashape elements.\n\n    Elements of a datashape like ``Fixed(3)``, ``Var()`` or ``int32`` are on,\n    on their own, valid datashapes.  These elements are collected together into\n    a composite ``DataShape`` to be complete.\n\n    This class is not intended to be used directly.  Instead, use the utility\n    ``dshape`` function to create datashapes from strings or datashape\n    elements.\n\n    Examples\n    --------\n\n    >>> from datashader.datashape import Fixed, int32, DataShape, dshape\n\n    >>> DataShape(Fixed(5), int32)  # Rare to DataShape directly\n    dshape(\"5 * int32\")\n\n    >>> dshape('5 * int32')         # Instead use the dshape function\n    dshape(\"5 * int32\")\n\n    >>> dshape([Fixed(5), int32])   # It can even do construction from elements\n    dshape(\"5 * int32\")\n\n    See Also\n    --------\n    datashape.dshape\n    \"\"\"\n    composite = False\n\n    def __init__(self, *parameters, **kwds):\n        if len(parameters) == 1 and isinstance(parameters[0], str):\n            raise TypeError(\"DataShape constructor for internal use.\\nUse dshape function to convert strings into datashapes.\\nTry:\\n\\tdshape('%s')\" % parameters[0])\n        if len(parameters) > 0:\n            self._parameters = tuple(map(_launder, parameters))\n            if getattr(self._parameters[-1], 'cls', MEASURE) != MEASURE:\n                raise TypeError('Only a measure can appear on the last position of a datashape, not %s' % repr(self._parameters[-1]))\n            for dim in self._parameters[:-1]:\n                if getattr(dim, 'cls', DIMENSION) != DIMENSION:\n                    raise TypeError('Only dimensions can appear before the last position of a datashape, not %s' % repr(dim))\n        else:\n            raise ValueError('the data shape should be constructed from 2 or more parameters, only got %s' % len(parameters))\n        self.composite = True\n        self.name = kwds.get('name')\n        if self.name:\n            type(type(self))._registry[self.name] = self\n\n    def __len__(self):\n        return len(self.parameters)\n\n    def __getitem__(self, index):\n        return self.parameters[index]\n\n    def __str__(self):\n        return self.name or ' * '.join(map(str, self.parameters))\n\n    def __repr__(self):\n        s = pprint(self)\n        if '\\n' in s:\n            return 'dshape(\"\"\"%s\"\"\")' % s\n        else:\n            return 'dshape(\"%s\")' % s\n\n    @property\n    def shape(self):\n        return self.parameters[:-1]\n\n    @property\n    def measure(self):\n        return self.parameters[-1]\n\n    def subarray(self, leading):\n        \"\"\"Returns a data shape object of the subarray with 'leading'\n        dimensions removed.\n\n        >>> from datashader.datashape import dshape\n        >>> dshape('1 * 2 * 3 * int32').subarray(1)\n        dshape(\"2 * 3 * int32\")\n        >>> dshape('1 * 2 * 3 * int32').subarray(2)\n        dshape(\"3 * int32\")\n        \"\"\"\n        if leading >= len(self.parameters):\n            raise IndexError('Not enough dimensions in data shape to remove %d leading dimensions.' % leading)\n        elif leading in [len(self.parameters) - 1, -1]:\n            return DataShape(self.parameters[-1])\n        else:\n            return DataShape(*self.parameters[leading:])\n\n    def __rmul__(self, other):\n        if isinstance(other, int):\n            other = Fixed(other)\n        return DataShape(other, *self)\n\n    @property\n    def subshape(self):\n        return IndexCallable(self._subshape)\n\n    def _subshape(self, index):\n        \"\"\" The DataShape of an indexed subarray\n\n        >>> from datashader.datashape import dshape\n\n        >>> ds = dshape('var * {name: string, amount: int32}')\n        >>> print(ds.subshape[0])\n        {name: string, amount: int32}\n\n        >>> print(ds.subshape[0:3])\n        3 * {name: string, amount: int32}\n\n        >>> print(ds.subshape[0:7:2, 'amount'])\n        4 * int32\n\n        >>> print(ds.subshape[[1, 10, 15]])\n        3 * {name: string, amount: int32}\n\n        >>> ds = dshape('{x: int, y: int}')\n        >>> print(ds.subshape['x'])\n        int32\n\n        >>> ds = dshape('10 * var * 10 * int32')\n        >>> print(ds.subshape[0:5, 0:3, 5])\n        5 * 3 * int32\n\n        >>> ds = dshape('var * {name: string, amount: int32, id: int32}')\n        >>> print(ds.subshape[:, [0, 2]])\n        var * {name: string, id: int32}\n\n        >>> ds = dshape('var * {name: string, amount: int32, id: int32}')\n        >>> print(ds.subshape[:, ['name', 'id']])\n        var * {name: string, id: int32}\n\n        >>> print(ds.subshape[0, 1:])\n        {amount: int32, id: int32}\n        \"\"\"\n        from .predicates import isdimension\n        if isinstance(index, int) and isdimension(self[0]):\n            return self.subarray(1)\n        if isinstance(self[0], Record) and isinstance(index, str):\n            return self[0][index]\n        if isinstance(self[0], Record) and isinstance(index, int):\n            return self[0].parameters[0][index][1]\n        if isinstance(self[0], Record) and isinstance(index, list):\n            rec = self[0]\n            index = [self[0].names.index(i) if isinstance(i, str) else i for i in index]\n            return DataShape(Record([rec.parameters[0][i] for i in index]))\n        if isinstance(self[0], Record) and isinstance(index, slice):\n            rec = self[0]\n            return DataShape(Record(rec.parameters[0][index]))\n        if isinstance(index, list) and isdimension(self[0]):\n            return len(index) * self.subarray(1)\n        if isinstance(index, slice):\n            if isinstance(self[0], Fixed):\n                n = int(self[0])\n                start = index.start or 0\n                stop = index.stop or n\n                if start < 0:\n                    start = n + start\n                if stop < 0:\n                    stop = n + stop\n                count = stop - start\n            else:\n                start = index.start or 0\n                stop = index.stop\n                if not stop:\n                    count = -start if start < 0 else var\n                if stop is not None and start is not None and (stop >= 0) and (start >= 0):\n                    count = stop - start\n                else:\n                    count = var\n            if count != var and index.step is not None:\n                count = int(ceil(count / index.step))\n            return count * self.subarray(1)\n        if isinstance(index, tuple):\n            if not index:\n                return self\n            elif index[0] is None:\n                return 1 * self._subshape(index[1:])\n            elif len(index) == 1:\n                return self._subshape(index[0])\n            else:\n                ds = self.subarray(1)._subshape(index[1:])\n                return (self[0] * ds)._subshape(index[0])\n        raise TypeError('invalid index value %s of type %r' % (index, type(index).__name__))\n\n    def __setstate__(self, state):\n        self._parameters = state\n        self.composite = True\n        self.name = None\nnumpy_provides_missing = frozenset((Date, DateTime, TimeDelta))\n\nclass Option(Mono):\n    \"\"\"\n    Measure types which may or may not hold data. Makes no\n    indication of how this is implemented in memory.\n    \"\"\"\n    __slots__ = ('ty',)\n\n    def __init__(self, ds):\n        self.ty = _launder(ds)\n\n    @property\n    def shape(self):\n        return self.ty.shape\n\n    @property\n    def itemsize(self):\n        return self.ty.itemsize\n\n    def __str__(self):\n        return '?%s' % self.ty\n\n    def to_numpy_dtype(self):\n        if type(self.ty) in numpy_provides_missing:\n            return self.ty.to_numpy_dtype()\n        raise TypeError('DataShape measure %s is not NumPy-compatible' % self)\n\nclass CType(Unit):\n    \"\"\"\n    Symbol for a sized type mapping uniquely to a native type.\n    \"\"\"\n    cls = MEASURE\n    __slots__ = ('name', '_itemsize', '_alignment')\n\n    def __init__(self, name, itemsize, alignment):\n        self.name = name\n        self._itemsize = itemsize\n        self._alignment = alignment\n        Type.register(name, self)\n\n    @classmethod\n    def from_numpy_dtype(self, dt):\n        \"\"\"\n        From Numpy dtype.\n\n        >>> from datashader.datashape import CType\n        >>> from numpy import dtype\n        >>> CType.from_numpy_dtype(dtype('int32'))\n        ctype(\"int32\")\n        >>> CType.from_numpy_dtype(dtype('i8'))\n        ctype(\"int64\")\n        >>> CType.from_numpy_dtype(dtype('M8'))\n        DateTime(tz=None)\n        >>> CType.from_numpy_dtype(dtype('U30'))  # doctest: +SKIP\n        ctype(\"string[30, 'U32']\")\n        \"\"\"\n        try:\n            return Type.lookup_type(dt.name)\n        except KeyError:\n            pass\n        if np.issubdtype(dt, np.datetime64):\n            unit, _ = np.datetime_data(dt)\n            defaults = {'D': date_, 'Y': date_, 'M': date_, 'W': date_}\n            return defaults.get(unit, datetime_)\n        elif np.issubdtype(dt, np.timedelta64):\n            unit, _ = np.datetime_data(dt)\n            return TimeDelta(unit=unit)\n        elif np.__version__[0] < '2' and np.issubdtype(dt, np.unicode_):\n            return String(dt.itemsize // 4, 'U32')\n        elif np.issubdtype(dt, np.str_) or np.issubdtype(dt, np.bytes_):\n            return String(dt.itemsize, 'ascii')\n        raise NotImplementedError('NumPy datatype %s not supported' % dt)\n\n    @property\n    def itemsize(self):\n        \"\"\"The size of one element of this type.\"\"\"\n        return self._itemsize\n\n    @property\n    def alignment(self):\n        \"\"\"The alignment of one element of this type.\"\"\"\n        return self._alignment\n\n    def to_numpy_dtype(self):\n        \"\"\"\n        To Numpy dtype.\n        \"\"\"\n        name = self.name\n        return np.dtype({'complex[float32]': 'complex64', 'complex[float64]': 'complex128'}.get(name, name))\n\n    def __str__(self):\n        return self.name\n\n    def __repr__(self):\n        s = str(self)\n        return 'ctype(\"%s\")' % s.encode('unicode_escape').decode('ascii')\n\nclass Fixed(Unit):\n    \"\"\"\n    Fixed dimension.\n    \"\"\"\n    cls = DIMENSION\n    __slots__ = ('val',)\n\n    def __init__(self, i):\n        i = operator.index(i)\n        if i < 0:\n            raise ValueError('Fixed dimensions must be positive')\n        self.val = i\n\n    def __index__(self):\n        return self.val\n\n    def __int__(self):\n        return self.val\n\n    def __eq__(self, other):\n        return type(other) is Fixed and self.val == other.val or (isinstance(other, int) and self.val == other)\n    __hash__ = Mono.__hash__\n\n    def __str__(self):\n        return str(self.val)\n\nclass Var(Unit):\n    \"\"\" Variable dimension \"\"\"\n    cls = DIMENSION\n    __slots__ = ()\n\nclass TypeVar(Unit):\n    \"\"\"\n    A free variable in the signature. Not user facing.\n    \"\"\"\n    __slots__ = ('symbol',)\n\n    def __init__(self, symbol):\n        if not symbol[0].isupper():\n            raise ValueError('TypeVar symbol %r does not begin with a capital' % symbol)\n        self.symbol = symbol\n\n    def __str__(self):\n        return str(self.symbol)\n\nclass Function(Mono):\n    \"\"\"Function signature type\n    \"\"\"\n\n    @property\n    def restype(self):\n        return self.parameters[-1]\n\n    @property\n    def argtypes(self):\n        return self.parameters[:-1]\n\n    def __str__(self):\n        return '(%s) -> %s' % (', '.join(map(str, self.argtypes)), self.restype)\n\nclass Map(Mono):\n    __slots__ = ('key', 'value')\n\n    def __init__(self, key, value):\n        self.key = _launder(key)\n        self.value = _launder(value)\n\n    def __str__(self):\n        return '%s[%s, %s]' % (type(self).__name__.lower(), self.key, self.value)\n\n    def to_numpy_dtype(self):\n        return to_numpy_dtype(self)\n\ndef _launder(x):\n    \"\"\" Clean up types prior to insertion into DataShape\n\n    >>> from datashader.datashape import dshape\n    >>> _launder(5)         # convert ints to Fixed\n    Fixed(val=5)\n    >>> _launder('int32')   # parse strings\n    ctype(\"int32\")\n    >>> _launder(dshape('int32'))\n    ctype(\"int32\")\n    >>> _launder(Fixed(5))  # No-op on valid parameters\n    Fixed(val=5)\n    \"\"\"\n    if isinstance(x, int):\n        x = Fixed(x)\n    if isinstance(x, str):\n        x = datashape.dshape(x)\n    if isinstance(x, DataShape) and len(x) == 1:\n        return x[0]\n    if isinstance(x, Mono):\n        return x\n    return x\n\nclass CollectionPrinter:\n\n    def __repr__(self):\n        s = str(self)\n        strs = ('\"\"\"%s\"\"\"' if '\\n' in s else '\"%s\"') % s\n        return 'dshape(%s)' % strs\n\nclass RecordMeta(Type):\n\n    @staticmethod\n    def _unpack_slice(s, idx):\n        if not isinstance(s, slice):\n            raise TypeError('invalid field specification at position %d.\\nfields must be formatted like: {name}:{type}' % idx)\n        name, type_ = packed = (s.start, s.stop)\n        if name is None:\n            raise TypeError('missing field name at position %d' % idx)\n        if not isinstance(name, str):\n            raise TypeError(\"field name at position %d ('%s') was not a string\" % (idx, name))\n        if type_ is None and s.step is None:\n            raise TypeError(\"missing type for field '%s' at position %d\" % (name, idx))\n        if s.step is not None:\n            raise TypeError(\"unexpected slice step for field '%s' at position %d.\\nhint: you might have a second ':'\" % (name, idx))\n        return packed\n\n    def __getitem__(self, types):\n        if not isinstance(types, tuple):\n            types = (types,)\n        return self(list(map(self._unpack_slice, types, range(len(types)))))\n\nclass Record(CollectionPrinter, Mono, metaclass=RecordMeta):\n    \"\"\"\n    A composite data structure of ordered fields mapped to types.\n\n    Properties\n    ----------\n\n    fields: tuple of (name, type) pairs\n        The only stored data, also the input to ``__init__``\n    dict: dict\n        A dictionary view of ``fields``\n    names: list of strings\n        A list of the names\n    types: list of datashapes\n        A list of the datashapes\n\n    Example\n    -------\n\n    >>> Record([['id', 'int'], ['name', 'string'], ['amount', 'real']])\n    dshape(\"{id: int32, name: string, amount: float64}\")\n    \"\"\"\n    cls = MEASURE\n\n    def __init__(self, fields):\n        \"\"\"\n        Parameters\n        ----------\n        fields : list/OrderedDict of (name, type) entries\n            The fields which make up the record.\n        \"\"\"\n        if isinstance(fields, OrderedDict):\n            fields = fields.items()\n        fields = list(fields)\n        names = [str(name) if not isinstance(name, str) else name for name, _ in fields]\n        types = [_launder(v) for _, v in fields]\n        if len(set(names)) != len(names):\n            for name in set(names):\n                names.remove(name)\n            raise ValueError('duplicate field names found: %s' % names)\n        self._parameters = (tuple(zip(names, types)),)\n\n    @property\n    def fields(self):\n        return self._parameters[0]\n\n    @property\n    def dict(self):\n        return dict(self.fields)\n\n    @property\n    def names(self):\n        return [n for n, t in self.fields]\n\n    @property\n    def types(self):\n        return [t for n, t in self.fields]\n\n    def to_numpy_dtype(self):\n        \"\"\"\n        To Numpy record dtype.\n        \"\"\"\n        return np.dtype([(str(name), to_numpy_dtype(typ)) for name, typ in self.fields])\n\n    def __getitem__(self, key):\n        return self.dict[key]\n\n    def __str__(self):\n        return pprint(self)\nR = Record\n\ndef _format_categories(cats, n=10):\n    return '[%s%s]' % (', '.join(map(repr, cats[:n])), ', ...' if len(cats) > n else '')\n\nclass Categorical(Mono):\n    \"\"\"Unordered categorical type.\n    \"\"\"\n    __slots__ = ('categories', 'type', 'ordered')\n    cls = MEASURE\n\n    def __init__(self, categories, type=None, ordered=False):\n        self.categories = tuple(categories)\n        self.type = (type or datashape.discover(self.categories)).measure\n        self.ordered = ordered\n\n    def __str__(self):\n        return '%s[%s, type=%s, ordered=%s]' % (type(self).__name__.lower(), _format_categories(self.categories), self.type, self.ordered)\n\n    def __repr__(self):\n        return '%s(categories=%s, type=%r, ordered=%s)' % (type(self).__name__, _format_categories(self.categories), self.type, self.ordered)\n\nclass Tuple(CollectionPrinter, Mono):\n    \"\"\"\n    A product type.\n    \"\"\"\n    __slots__ = ('dshapes',)\n    cls = MEASURE\n\n    def __init__(self, dshapes):\n        \"\"\"\n        Parameters\n        ----------\n        dshapes : list of dshapes\n            The datashapes which make up the tuple.\n        \"\"\"\n        dshapes = [DataShape(ds) if not isinstance(ds, DataShape) else ds for ds in dshapes]\n        self.dshapes = tuple(dshapes)\n\n    def __str__(self):\n        return '(%s)' % ', '.join(map(str, self.dshapes))\n\n    def to_numpy_dtype(self):\n        \"\"\"\n        To Numpy record dtype.\n        \"\"\"\n        return np.dtype([('f%d' % i, to_numpy_dtype(typ)) for i, typ in enumerate(self.parameters[0])])\n\nclass JSON(Mono):\n    \"\"\" JSON measure \"\"\"\n    cls = MEASURE\n    __slots__ = ()\n\n    def __str__(self):\n        return 'json'\nbool_ = CType('bool', 1, 1)\nchar = CType('char', 1, 1)\nint8 = CType('int8', 1, 1)\nint16 = CType('int16', 2, ctypes.alignment(ctypes.c_int16))\nint32 = CType('int32', 4, ctypes.alignment(ctypes.c_int32))\nint64 = CType('int64', 8, ctypes.alignment(ctypes.c_int64))\nint_ = int32\nType.register('int', int_)\nuint8 = CType('uint8', 1, 1)\nuint16 = CType('uint16', 2, ctypes.alignment(ctypes.c_uint16))\nuint32 = CType('uint32', 4, ctypes.alignment(ctypes.c_uint32))\nuint64 = CType('uint64', 8, ctypes.alignment(ctypes.c_uint64))\nfloat16 = CType('float16', 2, ctypes.alignment(ctypes.c_uint16))\nfloat32 = CType('float32', 4, ctypes.alignment(ctypes.c_float))\nfloat64 = CType('float64', 8, ctypes.alignment(ctypes.c_double))\nreal = float64\nType.register('real', real)\ncomplex_float32 = CType('complex[float32]', 8, ctypes.alignment(ctypes.c_float))\ncomplex_float64 = CType('complex[float64]', 16, ctypes.alignment(ctypes.c_double))\nType.register('complex64', complex_float32)\ncomplex64 = complex_float32\nType.register('complex128', complex_float64)\ncomplex128 = complex_float64\ncomplex_ = complex_float64\ndate_ = Date()\ntime_ = Time()\ndatetime_ = DateTime()\ntimedelta_ = TimeDelta()\nType.register('date', date_)\nType.register('time', time_)\nType.register('datetime', datetime_)\nType.register('timedelta', timedelta_)\nnull = Null()\nType.register('null', null)\nc_byte = int8\nc_short = int16\nc_int = int32\nc_longlong = int64\nc_ubyte = uint8\nc_ushort = uint16\nc_ulonglong = uint64\nif ctypes.sizeof(ctypes.c_long) == 4:\n    c_long = int32\n    c_ulong = uint32\nelse:\n    c_long = int64\n    c_ulong = uint64\nif ctypes.sizeof(ctypes.c_void_p) == 4:\n    intptr = c_ssize_t = int32\n    uintptr = c_size_t = uint32\nelse:\n    intptr = c_ssize_t = int64\n    uintptr = c_size_t = uint64\nType.register('intptr', intptr)\nType.register('uintptr', uintptr)\nc_half = float16\nc_float = float32\nc_double = float64\nhalf = float16\nsingle = float32\ndouble = float64\nvoid = CType('void', 0, 1)\nobject_ = pyobj = CType('object', ctypes.sizeof(ctypes.py_object), ctypes.alignment(ctypes.py_object))\nna = Null\nNullRecord = Record(())\nbytes_ = Bytes()\nstring = String()\njson = JSON()\nType.register('float', c_float)\nType.register('double', c_double)\nType.register('bytes', bytes_)\nType.register('string', String())\nvar = Var()\n\ndef to_numpy_dtype(ds):\n    \"\"\" Throw away the shape information and just return the\n    measure as NumPy dtype instance.\"\"\"\n    if isinstance(ds.measure, datashape.coretypes.Map):\n        ds = ds.measure.key\n    return to_numpy(ds.measure)[1]\n\ndef to_numpy(ds):\n    \"\"\"\n    Downcast a datashape object into a Numpy (shape, dtype) tuple if\n    possible.\n\n    >>> from datashader.datashape import dshape, to_numpy\n    >>> to_numpy(dshape('5 * 5 * int32'))\n    ((5, 5), dtype('int32'))\n    >>> to_numpy(dshape('10 * string[30]'))\n    ((10,), dtype('<U30'))\n    >>> to_numpy(dshape('N * int32'))\n    ((-1,), dtype('int32'))\n    \"\"\"\n    shape = []\n    if isinstance(ds, DataShape):\n        for dim in ds[:-1]:\n            if isinstance(dim, Fixed):\n                shape.append(int(dim))\n            elif isinstance(dim, TypeVar):\n                shape.append(-1)\n            else:\n                raise TypeError('DataShape dimension %s is not NumPy-compatible' % dim)\n        msr = ds[-1]\n    else:\n        msr = ds\n    return (tuple(shape), msr.to_numpy_dtype())\n\ndef from_numpy(shape, dt):\n    \"\"\"\n    Upcast a (shape, dtype) tuple if possible.\n\n    >>> from datashader.datashape import from_numpy\n    >>> from numpy import dtype\n    >>> from_numpy((5, 5), dtype('int32'))\n    dshape(\"5 * 5 * int32\")\n\n    >>> from_numpy((10,), dtype('S10'))\n    dshape(\"10 * string[10, 'A']\")\n    \"\"\"\n    dtype = np.dtype(dt)\n    if dtype.kind == 'S':\n        measure = String(dtype.itemsize, 'A')\n    elif dtype.kind == 'U':\n        measure = String(dtype.itemsize // 4, 'U32')\n    elif dtype.fields:\n        fields = [(name, dtype.fields[name]) for name in dtype.names]\n        rec = [(name, from_numpy(t.shape, t.base)) for name, (t, _) in fields]\n        measure = Record(rec)\n    else:\n        measure = CType.from_numpy_dtype(dtype)\n    if not shape:\n        return measure\n    return DataShape(*tuple(map(Fixed, shape)) + (measure,))\n\ndef print_unicode_string(s):\n    try:\n        return s.decode('unicode_escape').encode('ascii')\n    except AttributeError:\n        return s\n\ndef pprint(ds, width=80):\n    ''' Pretty print a datashape\n\n    >>> from datashader.datashape import dshape, pprint\n    >>> print(pprint(dshape('5 * 3 * int32')))\n    5 * 3 * int32\n\n    >>> ds = dshape(\"\"\"\n    ... 5000000000 * {\n    ...     a: (int, float32, real, string, datetime),\n    ...     b: {c: 5 * int, d: var * 100 * float32}\n    ... }\"\"\")\n    >>> print(pprint(ds))\n    5000000000 * {\n      a: (int32, float32, float64, string, datetime),\n      b: {c: 5 * int32, d: var * 100 * float32}\n      }\n\n    Record measures print like full datashapes\n    >>> print(pprint(ds.measure, width=30))\n    {\n      a: (\n        int32,\n        float32,\n        float64,\n        string,\n        datetime\n        ),\n      b: {\n        c: 5 * int32,\n        d: var * 100 * float32\n        }\n      }\n\n    Control width of the result\n    >>> print(pprint(ds, width=30))\n    5000000000 * {\n      a: (\n        int32,\n        float32,\n        float64,\n        string,\n        datetime\n        ),\n      b: {\n        c: 5 * int32,\n        d: var * 100 * float32\n        }\n      }\n    >>>\n    '''\n    result = ''\n    if isinstance(ds, DataShape):\n        if ds.shape:\n            result += ' * '.join(map(str, ds.shape))\n            result += ' * '\n        ds = ds[-1]\n    if isinstance(ds, Record):\n        pairs = ['%s: %s' % (name if isidentifier(name) else repr(print_unicode_string(name)), pprint(typ, width - len(result) - len(name))) for name, typ in zip(ds.names, ds.types)]\n        short = '{%s}' % ', '.join(pairs)\n        if len(result + short) < width:\n            return result + short\n        else:\n            long = '{\\n%s\\n}' % ',\\n'.join(pairs)\n            return result + long.replace('\\n', '\\n  ')\n    elif isinstance(ds, Tuple):\n        typs = [pprint(typ, width - len(result)) for typ in ds.dshapes]\n        short = '(%s)' % ', '.join(typs)\n        if len(result + short) < width:\n            return result + short\n        else:\n            long = '(\\n%s\\n)' % ',\\n'.join(typs)\n            return result + long.replace('\\n', '\\n  ')\n    else:\n        result += str(ds)\n    return result",
    "datashader/datatypes.py": "from __future__ import annotations\nimport re\nfrom functools import total_ordering\nfrom packaging.version import Version\nimport numpy as np\nimport pandas as pd\nfrom numba import jit\nfrom pandas.api.extensions import ExtensionDtype, ExtensionArray, register_extension_dtype\nfrom numbers import Integral\nfrom pandas.api.types import pandas_dtype, is_extension_array_dtype\ntry:\n    from dask.dataframe.extensions import make_array_nonempty\nexcept ImportError:\n    make_array_nonempty = None\n\ndef _validate_ragged_properties(start_indices, flat_array):\n    \"\"\"\n    Validate that start_indices are flat_array arrays that may be used to\n    represent a valid RaggedArray.\n\n    Parameters\n    ----------\n    flat_array: numpy array containing concatenation\n                of all nested arrays to be represented\n                by this ragged array\n    start_indices: unsigned integer numpy array the same\n                   length as the ragged array where values\n                   represent the index into flat_array where\n                   the corresponding ragged array element\n                   begins\n    Raises\n    ------\n    ValueError:\n        if input arguments are invalid or incompatible properties\n    \"\"\"\n    if not isinstance(start_indices, np.ndarray) or start_indices.dtype.kind != 'u' or start_indices.ndim != 1:\n        raise ValueError(\"\\nThe start_indices property of a RaggedArray must be a 1D numpy array of\\nunsigned integers (start_indices.dtype.kind == 'u')\\n    Received value of type {typ}: {v}\".format(typ=type(start_indices), v=repr(start_indices)))\n    if not isinstance(flat_array, np.ndarray) or flat_array.ndim != 1:\n        raise ValueError('\\nThe flat_array property of a RaggedArray must be a 1D numpy array\\n    Received value of type {typ}: {v}'.format(typ=type(flat_array), v=repr(flat_array)))\n    invalid_inds = start_indices > len(flat_array)\n    if invalid_inds.any():\n        some_invalid_vals = start_indices[invalid_inds[:10]]\n        raise ValueError('\\nElements of start_indices must be less than the length of flat_array ({m})\\n    Invalid values include: {vals}'.format(m=len(flat_array), vals=repr(some_invalid_vals)))\n\n@total_ordering\nclass _RaggedElement:\n\n    @staticmethod\n    def ragged_or_nan(a):\n        if np.isscalar(a) and np.isnan(a):\n            return a\n        else:\n            return _RaggedElement(a)\n\n    @staticmethod\n    def array_or_nan(a):\n        if np.isscalar(a) and np.isnan(a):\n            return a\n        else:\n            return a.array\n\n    def __init__(self, array):\n        self.array = array\n\n    def __hash__(self):\n        return hash(self.array.tobytes())\n\n    def __eq__(self, other):\n        if not isinstance(other, _RaggedElement):\n            return False\n        return np.array_equal(self.array, other.array)\n\n    def __lt__(self, other):\n        if not isinstance(other, _RaggedElement):\n            return NotImplemented\n        return _lexograph_lt(self.array, other.array)\n\n    def __repr__(self):\n        array_repr = repr(self.array)\n        return array_repr.replace('array', 'ragged_element')\n\n@register_extension_dtype\nclass RaggedDtype(ExtensionDtype):\n    \"\"\"\n    Pandas ExtensionDtype to represent a ragged array datatype\n\n    Methods not otherwise documented here are inherited from ExtensionDtype;\n    please see the corresponding method on that class for the docstring\n    \"\"\"\n    type = np.ndarray\n    base = np.dtype('O')\n    _subtype_re = re.compile('^ragged\\\\[(?P<subtype>\\\\w+)\\\\]$')\n    _metadata = ('_dtype',)\n\n    @property\n    def name(self):\n        return 'Ragged[{subtype}]'.format(subtype=self.subtype)\n\n    def __repr__(self):\n        return self.name\n\n    @classmethod\n    def construct_array_type(cls):\n        return RaggedArray\n\n    @classmethod\n    def construct_from_string(cls, string):\n        if not isinstance(string, str):\n            raise TypeError(\"'construct_from_string' expects a string, got %s\" % type(string))\n        string = string.lower()\n        msg = \"Cannot construct a 'RaggedDtype' from '{}'\"\n        if string.startswith('ragged'):\n            try:\n                subtype_string = cls._parse_subtype(string)\n                return RaggedDtype(dtype=subtype_string)\n            except Exception:\n                raise TypeError(msg.format(string))\n        else:\n            raise TypeError(msg.format(string))\n\n    def __init__(self, dtype=np.float64):\n        if isinstance(dtype, RaggedDtype):\n            self._dtype = dtype.subtype\n        else:\n            self._dtype = np.dtype(dtype)\n\n    @property\n    def subtype(self):\n        return self._dtype\n\n    @classmethod\n    def _parse_subtype(cls, dtype_string):\n        \"\"\"\n        Parse a datatype string to get the subtype\n\n        Parameters\n        ----------\n        dtype_string: str\n            A string like Ragged[subtype]\n\n        Returns\n        -------\n        subtype: str\n\n        Raises\n        ------\n        ValueError\n            When the subtype cannot be extracted\n        \"\"\"\n        dtype_string = dtype_string.lower()\n        match = cls._subtype_re.match(dtype_string)\n        if match:\n            subtype_string = match.groupdict()['subtype']\n        elif dtype_string == 'ragged':\n            subtype_string = 'float64'\n        else:\n            raise ValueError('Cannot parse {dtype_string}'.format(dtype_string=dtype_string))\n        return subtype_string\n\ndef missing(v):\n    return v is None or (np.isscalar(v) and np.isnan(v))\n\nclass RaggedArray(ExtensionArray):\n    \"\"\"\n    Pandas ExtensionArray to represent ragged arrays\n\n    Methods not otherwise documented here are inherited from ExtensionArray;\n    please see the corresponding method on that class for the docstring\n    \"\"\"\n\n    def __init__(self, data, dtype=None, copy=False):\n        \"\"\"\n        Construct a RaggedArray\n\n        Parameters\n        ----------\n        data: list or array or dict or RaggedArray\n            * list or 1D-array: A List or 1D array of lists or 1D arrays that\n                                should be represented by the RaggedArray\n\n            * dict: A dict containing 'start_indices' and 'flat_array' keys\n                    with numpy array values where:\n                    - flat_array:  numpy array containing concatenation\n                                   of all nested arrays to be represented\n                                   by this ragged array\n                    - start_indices: unsigned integer numpy array the same\n                                     length as the ragged array where values\n                                     represent the index into flat_array where\n                                     the corresponding ragged array element\n                                     begins\n            * RaggedArray: A RaggedArray instance to copy\n\n        dtype: RaggedDtype or np.dtype or str or None (default None)\n            Datatype to use to store underlying values from data.\n            If none (the default) then dtype will be determined using the\n            numpy.result_type function.\n        copy : bool (default False)\n            Whether to deep copy the input arrays. Only relevant when `data`\n            has type `dict` or `RaggedArray`. When data is a `list` or\n            `array`, input arrays are always copied.\n        \"\"\"\n        if isinstance(data, dict) and all((k in data for k in ['start_indices', 'flat_array'])):\n            _validate_ragged_properties(start_indices=data['start_indices'], flat_array=data['flat_array'])\n            self._start_indices = data['start_indices']\n            self._flat_array = data['flat_array']\n            dtype = self._flat_array.dtype\n            if copy:\n                self._start_indices = self._start_indices.copy()\n                self._flat_array = self._flat_array.copy()\n        elif isinstance(data, RaggedArray):\n            self._flat_array = data.flat_array\n            self._start_indices = data.start_indices\n            dtype = self._flat_array.dtype\n            if copy:\n                self._start_indices = self._start_indices.copy()\n                self._flat_array = self._flat_array.copy()\n        else:\n            index_len = len(data)\n            buffer_len = sum((len(datum) if not missing(datum) else 0 for datum in data))\n            for nbits in [8, 16, 32, 64]:\n                start_indices_dtype = 'uint' + str(nbits)\n                max_supported = np.iinfo(start_indices_dtype).max\n                if buffer_len <= max_supported:\n                    break\n            if dtype is None:\n                non_missing = [np.atleast_1d(v) for v in data if not missing(v)]\n                if non_missing:\n                    dtype = np.result_type(*non_missing)\n                else:\n                    dtype = 'float64'\n            elif isinstance(dtype, RaggedDtype):\n                dtype = dtype.subtype\n            self._start_indices = np.zeros(index_len, dtype=start_indices_dtype)\n            self._flat_array = np.zeros(buffer_len, dtype=dtype)\n            next_start_ind = 0\n            for i, array_el in enumerate(data):\n                n = len(array_el) if not missing(array_el) else 0\n                self._start_indices[i] = next_start_ind\n                if not n:\n                    continue\n                self._flat_array[next_start_ind:next_start_ind + n] = array_el\n                next_start_ind += n\n        self._dtype = RaggedDtype(dtype=dtype)\n\n    def __eq__(self, other):\n        if isinstance(other, RaggedArray):\n            if len(other) != len(self):\n                raise ValueError('\\nCannot check equality of RaggedArray values of unequal length\\n    len(ra1) == {len_ra1}\\n    len(ra2) == {len_ra2}'.format(len_ra1=len(self), len_ra2=len(other)))\n            result = _eq_ragged_ragged(self.start_indices, self.flat_array, other.start_indices, other.flat_array)\n        else:\n            if not isinstance(other, np.ndarray):\n                other_array = np.asarray(other)\n            else:\n                other_array = other\n            if other_array.ndim == 1 and other_array.dtype.kind != 'O':\n                result = _eq_ragged_scalar(self.start_indices, self.flat_array, other_array)\n            elif other_array.ndim == 1 and other_array.dtype.kind == 'O' and (len(other_array) == len(self)):\n                result = _eq_ragged_ndarray1d(self.start_indices, self.flat_array, other_array)\n            elif other_array.ndim == 2 and other_array.dtype.kind != 'O' and (other_array.shape[0] == len(self)):\n                result = _eq_ragged_ndarray2d(self.start_indices, self.flat_array, other_array)\n            else:\n                raise ValueError('\\nCannot check equality of RaggedArray of length {ra_len} with:\\n    {other}'.format(ra_len=len(self), other=repr(other)))\n        return result\n\n    def __ne__(self, other):\n        return np.logical_not(self == other)\n\n    @property\n    def flat_array(self):\n        \"\"\"\n        numpy array containing concatenation of all nested arrays\n\n        Returns\n        -------\n        np.ndarray\n        \"\"\"\n        return self._flat_array\n\n    @property\n    def start_indices(self):\n        \"\"\"\n        unsigned integer numpy array the same length as the ragged array where\n        values represent the index into flat_array where the corresponding\n        ragged array element begins\n\n        Returns\n        -------\n        np.ndarray\n        \"\"\"\n        return self._start_indices\n\n    def __len__(self):\n        return len(self._start_indices)\n\n    def __getitem__(self, item):\n        err_msg = 'Only integers, slices and integer or booleanarrays are valid indices.'\n        if isinstance(item, Integral):\n            if item < -len(self) or item >= len(self):\n                raise IndexError('{item} is out of bounds'.format(item=item))\n            else:\n                if item < 0:\n                    item += len(self)\n                slice_start = self.start_indices[item]\n                slice_end = self.start_indices[item + 1] if item + 1 <= len(self) - 1 else len(self.flat_array)\n                return self.flat_array[slice_start:slice_end] if slice_end != slice_start else np.nan\n        elif type(item) is slice:\n            data = []\n            selected_indices = np.arange(len(self))[item]\n            for selected_index in selected_indices:\n                data.append(self[selected_index])\n            return RaggedArray(data, dtype=self.flat_array.dtype)\n        elif isinstance(item, (np.ndarray, ExtensionArray, list, tuple)):\n            if isinstance(item, (np.ndarray, ExtensionArray)):\n                kind = item.dtype.kind\n            else:\n                item = pd.array(item)\n                kind = item.dtype.kind\n            if len(item) == 0:\n                return self.take([], allow_fill=False)\n            elif kind == 'b':\n                if len(item) != len(self):\n                    raise IndexError('Boolean index has wrong length: {} instead of {}'.format(len(item), len(self)))\n                isna = pd.isna(item)\n                if isna.any():\n                    if Version(pd.__version__) > Version('1.0.1'):\n                        item[isna] = False\n                    else:\n                        raise ValueError('Cannot mask with a boolean indexer containing NA values')\n                data = []\n                for i, m in enumerate(item):\n                    if m:\n                        data.append(self[i])\n                return RaggedArray(data, dtype=self.flat_array.dtype)\n            elif kind in ('i', 'u'):\n                if any(pd.isna(item)):\n                    raise ValueError('Cannot index with an integer indexer containing NA values')\n                return self.take(item, allow_fill=False)\n            else:\n                raise IndexError(err_msg)\n        else:\n            raise IndexError(err_msg)\n\n    @classmethod\n    def _from_sequence(cls, scalars, dtype=None, copy=False):\n        return RaggedArray(scalars, dtype=dtype)\n\n    @classmethod\n    def _from_factorized(cls, values, original):\n        return RaggedArray([_RaggedElement.array_or_nan(v) for v in values], dtype=original.flat_array.dtype)\n\n    def _as_ragged_element_array(self):\n        return np.array([_RaggedElement.ragged_or_nan(self[i]) for i in range(len(self))])\n\n    def _values_for_factorize(self):\n        return (self._as_ragged_element_array(), np.nan)\n\n    def _values_for_argsort(self):\n        return self._as_ragged_element_array()\n\n    def unique(self):\n        from pandas import unique\n        uniques = unique(self._as_ragged_element_array())\n        return self._from_sequence([_RaggedElement.array_or_nan(v) for v in uniques], dtype=self.dtype)\n\n    def fillna(self, value=None, method=None, limit=None):\n        from pandas.util._validators import validate_fillna_kwargs\n        from pandas.core.missing import get_fill_func\n        value, method = validate_fillna_kwargs(value, method)\n        mask = self.isna()\n        if isinstance(value, RaggedArray):\n            if len(value) != len(self):\n                raise ValueError(\"Length of 'value' does not match. Got ({})  expected {}\".format(len(value), len(self)))\n            value = value[mask]\n        if mask.any():\n            if method is not None:\n                func = get_fill_func(method)\n                new_values = func(self.astype(object), limit=limit, mask=mask)\n                new_values = self._from_sequence(new_values, dtype=self.dtype)\n            else:\n                new_values = list(self)\n                mask_indices, = np.where(mask)\n                for ind in mask_indices:\n                    new_values[ind] = value\n                new_values = self._from_sequence(new_values, dtype=self.dtype)\n        else:\n            new_values = self.copy()\n        return new_values\n\n    def shift(self, periods=1, fill_value=None):\n        if not len(self) or periods == 0:\n            return self.copy()\n        if fill_value is None:\n            fill_value = np.nan\n        empty = self._from_sequence([fill_value] * min(abs(periods), len(self)), dtype=self.dtype)\n        if periods > 0:\n            a = empty\n            b = self[:-periods]\n        else:\n            a = self[abs(periods):]\n            b = empty\n        return self._concat_same_type([a, b])\n\n    def searchsorted(self, value, side='left', sorter=None):\n        arr = self._as_ragged_element_array()\n        if isinstance(value, RaggedArray):\n            search_value = value._as_ragged_element_array()\n        else:\n            search_value = _RaggedElement(value)\n        return arr.searchsorted(search_value, side=side, sorter=sorter)\n\n    def isna(self):\n        stop_indices = np.hstack([self.start_indices[1:], [len(self.flat_array)]])\n        element_lengths = stop_indices - self.start_indices\n        return element_lengths == 0\n\n    def take(self, indices, allow_fill=False, fill_value=None):\n        if allow_fill:\n            invalid_inds = [i for i in indices if i < -1]\n            if invalid_inds:\n                raise ValueError('\\nInvalid indices for take with allow_fill True: {inds}'.format(inds=invalid_inds[:9]))\n            sequence = [self[i] if i >= 0 else fill_value for i in indices]\n        else:\n            if len(self) == 0 and len(indices) > 0:\n                raise IndexError('cannot do a non-empty take from an empty axis|out of bounds')\n            sequence = [self[i] for i in indices]\n        return RaggedArray(sequence, dtype=self.flat_array.dtype)\n\n    def copy(self, deep=False):\n        data = dict(flat_array=self.flat_array, start_indices=self.start_indices)\n        return RaggedArray(data, copy=deep)\n\n    @classmethod\n    def _concat_same_type(cls, to_concat):\n        flat_array = np.hstack([ra.flat_array for ra in to_concat])\n        offsets = np.hstack([[0], np.cumsum([len(ra.flat_array) for ra in to_concat[:-1]])]).astype('uint64')\n        start_indices = np.hstack([ra.start_indices + offset for offset, ra in zip(offsets, to_concat)])\n        return RaggedArray(dict(flat_array=flat_array, start_indices=start_indices), copy=False)\n\n    @property\n    def dtype(self):\n        return self._dtype\n\n    @property\n    def nbytes(self):\n        return self._flat_array.nbytes + self._start_indices.nbytes\n\n    def astype(self, dtype, copy=True):\n        dtype = pandas_dtype(dtype)\n        if isinstance(dtype, RaggedDtype):\n            if copy:\n                return self.copy()\n            return self\n        elif is_extension_array_dtype(dtype):\n            return dtype.construct_array_type()._from_sequence(np.asarray(self))\n        return np.array([v for v in self], dtype=dtype)\n\n    def tolist(self):\n        if self.ndim > 1:\n            return [item.tolist() for item in self]\n        else:\n            return list(self)\n\n    def __array__(self, dtype=None, copy=True):\n        dtype = np.dtype(object) if dtype is None else np.dtype(dtype)\n        if copy:\n            return np.array(self.tolist(), dtype=dtype)\n        else:\n            return np.array(self, dtype=dtype)\n\n    def duplicated(self, *args, **kwargs):\n        msg = 'duplicated is not implemented for RaggedArray'\n        raise NotImplementedError(msg)\n\n@jit(nopython=True, nogil=True)\ndef _eq_ragged_ragged(start_indices1, flat_array1, start_indices2, flat_array2):\n    \"\"\"\n    Compare elements of two ragged arrays of the same length\n\n    Parameters\n    ----------\n    start_indices1: ndarray\n        start indices of a RaggedArray 1\n    flat_array1: ndarray\n        flat_array property of a RaggedArray 1\n    start_indices2: ndarray\n        start indices of a RaggedArray 2\n    flat_array2: ndarray\n        flat_array property of a RaggedArray 2\n\n    Returns\n    -------\n    mask: ndarray\n        1D bool array of same length as inputs with elements True when\n        corresponding elements are equal, False otherwise\n    \"\"\"\n    n = len(start_indices1)\n    m1 = len(flat_array1)\n    m2 = len(flat_array2)\n    result = np.zeros(n, dtype=np.bool_)\n    for i in range(n):\n        start_index1 = start_indices1[i]\n        stop_index1 = start_indices1[i + 1] if i < n - 1 else m1\n        len_1 = stop_index1 - start_index1\n        start_index2 = start_indices2[i]\n        stop_index2 = start_indices2[i + 1] if i < n - 1 else m2\n        len_2 = stop_index2 - start_index2\n        if len_1 != len_2:\n            el_equal = False\n        else:\n            el_equal = True\n            for flat_index1, flat_index2 in zip(range(start_index1, stop_index1), range(start_index2, stop_index2)):\n                el_1 = flat_array1[flat_index1]\n                el_2 = flat_array2[flat_index2]\n                el_equal &= el_1 == el_2\n        result[i] = el_equal\n    return result\n\n@jit(nopython=True, nogil=True)\ndef _eq_ragged_scalar(start_indices, flat_array, val):\n    \"\"\"\n    Compare elements of a RaggedArray with a scalar array\n\n    Parameters\n    ----------\n    start_indices: ndarray\n        start indices of a RaggedArray\n    flat_array: ndarray\n        flat_array property of a RaggedArray\n    val: ndarray\n\n    Returns\n    -------\n    mask: ndarray\n        1D bool array of same length as inputs with elements True when\n        ragged element equals scalar val, False otherwise.\n    \"\"\"\n    n = len(start_indices)\n    m = len(flat_array)\n    cols = len(val)\n    result = np.zeros(n, dtype=np.bool_)\n    for i in range(n):\n        start_index = start_indices[i]\n        stop_index = start_indices[i + 1] if i < n - 1 else m\n        if stop_index - start_index != cols:\n            el_equal = False\n        else:\n            el_equal = True\n            for val_index, flat_index in enumerate(range(start_index, stop_index)):\n                el_equal &= flat_array[flat_index] == val[val_index]\n        result[i] = el_equal\n    return result\n\ndef _eq_ragged_ndarray1d(start_indices, flat_array, a):\n    \"\"\"\n    Compare a RaggedArray with a 1D numpy object array of the same length\n\n    Parameters\n    ----------\n    start_indices: ndarray\n        start indices of a RaggedArray\n    flat_array: ndarray\n        flat_array property of a RaggedArray\n    a: ndarray\n        1D numpy array of same length as ra\n\n    Returns\n    -------\n    mask: ndarray\n        1D bool array of same length as input with elements True when\n        corresponding elements are equal, False otherwise\n\n    Notes\n    -----\n    This function is not numba accelerated because it, by design, inputs\n    a numpy object array\n    \"\"\"\n    n = len(start_indices)\n    m = len(flat_array)\n    result = np.zeros(n, dtype=np.bool_)\n    for i in range(n):\n        start_index = start_indices[i]\n        stop_index = start_indices[i + 1] if i < n - 1 else m\n        a_val = a[i]\n        if a_val is None or (np.isscalar(a_val) and np.isnan(a_val)) or len(a_val) == 0:\n            result[i] = start_index == stop_index\n        else:\n            result[i] = np.array_equal(flat_array[start_index:stop_index], a_val)\n    return result\n\n@jit(nopython=True, nogil=True)\ndef _eq_ragged_ndarray2d(start_indices, flat_array, a):\n    \"\"\"\n    Compare a RaggedArray with rows of a 2D numpy object array\n\n    Parameters\n    ----------\n    start_indices: ndarray\n        start indices of a RaggedArray\n    flat_array: ndarray\n        flat_array property of a RaggedArray\n    a: ndarray\n        A 2D numpy array where the length of the first dimension matches the\n        length of the RaggedArray\n\n    Returns\n    -------\n    mask: ndarray\n        1D bool array of same length as input RaggedArray with elements True\n        when corresponding elements of ra equal corresponding row of `a`\n    \"\"\"\n    n = len(start_indices)\n    m = len(flat_array)\n    cols = a.shape[1]\n    result = np.zeros(n, dtype=np.bool_)\n    for row in range(n):\n        start_index = start_indices[row]\n        stop_index = start_indices[row + 1] if row < n - 1 else m\n        if stop_index - start_index != cols:\n            el_equal = False\n        else:\n            el_equal = True\n            for col, flat_index in enumerate(range(start_index, stop_index)):\n                el_equal &= flat_array[flat_index] == a[row, col]\n        result[row] = el_equal\n    return result\n\n@jit(nopython=True, nogil=True)\ndef _lexograph_lt(a1, a2):\n    \"\"\"\n    Compare two 1D numpy arrays lexographically\n    Parameters\n    ----------\n    a1: ndarray\n        1D numpy array\n    a2: ndarray\n        1D numpy array\n\n    Returns\n    -------\n    comparison:\n        True if a1 < a2, False otherwise\n    \"\"\"\n    for e1, e2 in zip(a1, a2):\n        if e1 < e2:\n            return True\n        elif e1 > e2:\n            return False\n    return len(a1) < len(a2)\n\ndef ragged_array_non_empty(dtype):\n    return RaggedArray([[1], [1, 2]], dtype=dtype)\nif make_array_nonempty:\n    make_array_nonempty.register(RaggedDtype)(ragged_array_non_empty)",
    "datashader/datashape/user.py": "from .dispatch import dispatch\nfrom .coretypes import CType, Date, DateTime, DataShape, Record, String, Time, Var, from_numpy, to_numpy_dtype\nfrom .predicates import isdimension\nfrom .util import dshape\nfrom datetime import date, time, datetime\nimport numpy as np\n__all__ = ['validate', 'issubschema']\nbasetypes = (np.generic, int, float, str, date, time, datetime)\n\n@validate.register(String, str)\n@validate.register(Time, time)\n@validate.register(Date, date)\n@validate.register(DateTime, datetime)\ndef validate_always_true(schema, value):\n    return True",
    "datashader/datashape/util/__init__.py": "from itertools import chain\nimport operator\nfrom .. import parser\nfrom .. import type_symbol_table\nfrom ..validation import validate\nfrom .. import coretypes\n__all__ = ('dshape', 'dshapes', 'has_var_dim', 'has_ellipsis', 'cat_dshapes')\nsubclasses = operator.methodcaller('__subclasses__')\n\ndef dshapes(*args):\n    \"\"\"\n    Parse a bunch of datashapes all at once.\n\n    >>> a, b = dshapes('3 * int32', '2 * var * float64')\n    \"\"\"\n    return [dshape(arg) for arg in args]\n\ndef cat_dshapes(dslist):\n    \"\"\"\n    Concatenates a list of dshapes together along\n    the first axis. Raises an error if there is\n    a mismatch along another axis or the measures\n    are different.\n\n    Requires that the leading dimension be a known\n    size for all data shapes.\n    TODO: Relax this restriction to support\n          streaming dimensions.\n\n    >>> cat_dshapes(dshapes('10 * int32', '5 * int32'))\n    dshape(\"15 * int32\")\n    \"\"\"\n    if len(dslist) == 0:\n        raise ValueError('Cannot concatenate an empty list of dshapes')\n    elif len(dslist) == 1:\n        return dslist[0]\n    outer_dim_size = operator.index(dslist[0][0])\n    inner_ds = dslist[0][1:]\n    for ds in dslist[1:]:\n        outer_dim_size += operator.index(ds[0])\n        if ds[1:] != inner_ds:\n            raise ValueError('The datashapes to concatenate much all match after the first dimension (%s vs %s)' % (inner_ds, ds[1:]))\n    return coretypes.DataShape(*[coretypes.Fixed(outer_dim_size)] + list(inner_ds))\n\ndef collect(pred, expr):\n    \"\"\" Collect terms in expression that match predicate\n\n    >>> from datashader.datashape import Unit, dshape\n    >>> predicate = lambda term: isinstance(term, Unit)\n    >>> dshape = dshape('var * {value: int64, loc: 2 * int32}')\n    >>> sorted(set(collect(predicate, dshape)), key=str)\n    [Fixed(val=2), ctype(\"int32\"), ctype(\"int64\"), Var()]\n    >>> from datashader.datashape import var, int64\n    >>> sorted(set(collect(predicate, [var, int64])), key=str)\n    [ctype(\"int64\"), Var()]\n    \"\"\"\n    if pred(expr):\n        return [expr]\n    if isinstance(expr, coretypes.Record):\n        return chain.from_iterable((collect(pred, typ) for typ in expr.types))\n    if isinstance(expr, coretypes.Mono):\n        return chain.from_iterable((collect(pred, typ) for typ in expr.parameters))\n    if isinstance(expr, (list, tuple)):\n        return chain.from_iterable((collect(pred, item) for item in expr))\n\ndef has_var_dim(ds):\n    \"\"\"Returns True if datashape has a variable dimension\n\n    Note currently treats variable length string as scalars.\n\n    >>> has_var_dim(dshape('2 * int32'))\n    False\n    >>> has_var_dim(dshape('var * 2 * int32'))\n    True\n    \"\"\"\n    return has((coretypes.Ellipsis, coretypes.Var), ds)\n\ndef has(typ, ds):\n    if isinstance(ds, typ):\n        return True\n    if isinstance(ds, coretypes.Record):\n        return any((has(typ, t) for t in ds.types))\n    if isinstance(ds, coretypes.Mono):\n        return any((has(typ, p) for p in ds.parameters))\n    if isinstance(ds, (list, tuple)):\n        return any((has(typ, item) for item in ds))\n    return False\n\ndef has_ellipsis(ds):\n    \"\"\"Returns True if the datashape has an ellipsis\n\n    >>> has_ellipsis(dshape('2 * int'))\n    False\n    >>> has_ellipsis(dshape('... * int'))\n    True\n    \"\"\"\n    return has(coretypes.Ellipsis, ds)"
  }
}