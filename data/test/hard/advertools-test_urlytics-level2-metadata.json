{
  "dir_path": "/app/advertools",
  "package_name": "advertools",
  "sample_name": "advertools-test_urlytics",
  "src_dir": "advertools/",
  "test_dir": "tests/",
  "test_file": "tests/test_urlytics.py",
  "test_code": "from secrets import token_hex\nfrom tempfile import TemporaryDirectory\n\nimport pandas as pd\nimport pytest\n\n\nfrom advertools.urlytics import url_to_df\n\ndomain = \"http://example.com\"\ndomain_path = \"https://example.com/path\"\npath_rel = \"/path_rel\"\npath_rel_noslash = \"no_slash_no_nothing\"\ndomain_query = \"https://www.example.com?one=1&two=2\"\ndomain_query_rel = \"/?one=1&two=2\"\nport = \"https://www.example.com:80\"\nfragment = \"https://example.com/#fragment\"\nfragment_rel = \"/#fragment_rel\"\nfull = \"ftp://example.com:20/cat/sub_cat?one=10&three=30#frag_2\"\n\nordered_q_param_urls = [\n    \"https://example.com?a=a&b=b&c=c\",\n    \"https://example.com?b=b&c=c\",\n    \"https://example.com?c=c\",\n    \"https://example.com?a=a&c=c\",\n]\n\n\ndef test_urltodf_raises_on_wrong_file_ext():\n    with pytest.raises(ValueError):\n        url_to_df([\"https://example.com\"], output_file=\"output.wrong\")\n\n\ndef test_urltodf_convert_str_tolist():\n    result = url_to_df(\"https://www.example.com\")\n    assert isinstance(result, pd.DataFrame)\n\n\ndef test_path_rel_noslash():\n    result = url_to_df(path_rel_noslash)\n    assert pd.isna(result[\"scheme\"][0])\n    assert pd.isna(result[\"netloc\"][0])\n\n\ndef test_abs_and_rel():\n    result = url_to_df([domain, path_rel])\n    assert \"dir_1\" in result\n    assert len(result) == 2\n    assert result[\"scheme\"].iloc[-1] is None\n    assert result[\"query\"].isna().all()\n    assert result[\"fragment\"].isna().all()\n\n\ndef test_domainpath_fragrel_full():\n    result = url_to_df([domain_path, fragment_rel, full])\n    assert len(result) == 3\n    assert \"dir_2\" in result\n    query_set = {\"query_one\", \"query_three\"}\n    assert query_set.intersection(result.columns) == query_set\n\n\ndef test_no_path_has_no_last_dir():\n    result = url_to_df(domain_query)\n    assert \"last_dir\" not in result\n\n\ndef test_all():\n    result = url_to_df(\n        [\n            domain,\n            domain_path,\n            path_rel,\n            domain_query,\n            domain_query_rel,\n            port,\n            fragment,\n            fragment_rel,\n            full,\n        ]\n    )\n    assert len(result) == 9\n    assert \"port\" in result\n    assert \"hostname\" in result\n    assert \"last_dir\" in result\n\n\ndef test_query_params_are_ordered_by_fullness():\n    result = url_to_df(ordered_q_param_urls)\n    query_df = result.filter(regex=\"^query_\")\n    sorted_q_params = query_df.notna().mean().sort_values(ascending=False).index\n    assert (query_df.columns == sorted_q_params).all()\n\n\ndef test_urltodf_produces_outputfile():\n    with TemporaryDirectory() as tmpdir:\n        url_to_df(\n            [\"https://example.com/one/two\", \"https://example.com/one/two\"],\n            output_file=f\"{tmpdir}/output.parquet\",\n        )\n        df = pd.read_parquet(f\"{tmpdir}/output.parquet\")\n        assert isinstance(df, pd.DataFrame)\n\n\ndef test_urltodf_preserves_order_of_supplied_urls():\n    with TemporaryDirectory() as tmpdir:\n        urls = [f\"https://example.com/one/two/{token_hex(16)}\" for i in range(2300)]\n        url_to_df(urls, output_file=f\"{tmpdir}/output.parquet\")\n        df = pd.read_parquet(f\"{tmpdir}/output.parquet\")\n        assert df[\"url\"].eq(urls).all()\n",
  "GT_file_code": {
    "advertools/urlytics.py": "\"\"\"\n.. _urlytics:\n\nSplit, Parse, and Analyze URL Structure\n=======================================\n\nExtracting information from URLs can be a little tedious, yet very important.\nUsing the standard for URLs we can extract a lot of information in a fairly\nstructured manner.\n\nThere are many situations in which you have many URLs that you want to better\nunderstand:\n\n* **Analytics reports**: Whichever analytics system you use, whether Google\n  Analytics, search console, or any other reporting tool that reports on URLs,\n  your reports can be enhanced by splitting URLs, and in effect becoming four\n  or five data points as opposed to one.\n* :ref:`Crawl datasets <crawl>`: The result of any crawl you run typically\n  contains the URLs, which can benefit from the same enhancement.\n* :ref:`SERP datasets <serp>`: Which are basically about URLs.\n* :ref:`Extracted URLs <extract>`: Extracting URLs from social media posts is\n  one thing you might want to do to better understand those posts, and further\n  splitting URLs can also help.\n* :ref:`XML sitemaps <sitemaps>`: Right after downloading a sitemap(s)\n  splitting it further can help in giving a better perspective on the dataset.\n\nThe main function here is :func:`url_to_df`, which as the name suggests,\nconverts URLs to DataFrames.\n\n\n.. thebe-button::\n    Run this code\n\n.. code-block::\n    :class: thebe, thebe-init\n\n    import advertools as adv\n\n    urls = ['https://netloc.com/path_1/path_2?price=10&color=blue#frag_1',\n            'https://netloc.com/path_1/path_2?price=15&color=red#frag_2',\n            'https://netloc.com/path_1/path_2/path_3?size=sm&color=blue#frag_1',\n            'https://netloc.com/path_1?price=10&color=blue']\n    adv.url_to_df(urls)\n\n====  =================================================================  ========  ==========  =====================  ===================  ==========  =======  =======  =======  ==========  =============  =============  ============\n  ..  url                                                                scheme    netloc      path                   query                fragment    dir_1    dir_2    dir_3    last_dir    query_color      query_price  query_size\n====  =================================================================  ========  ==========  =====================  ===================  ==========  =======  =======  =======  ==========  =============  =============  ============\n   0  https://netloc.com/path_1/path_2?price=10&color=blue#frag_1        https     netloc.com  /path_1/path_2         price=10&color=blue  frag_1      path_1   path_2   nan      path_2      blue                      10  nan\n   1  https://netloc.com/path_1/path_2?price=15&color=red#frag_2         https     netloc.com  /path_1/path_2         price=15&color=red   frag_2      path_1   path_2   nan      path_2      red                       15  nan\n   2  https://netloc.com/path_1/path_2/path_3?size=sm&color=blue#frag_1  https     netloc.com  /path_1/path_2/path_3  size=sm&color=blue   frag_1      path_1   path_2   path_3   path_3      blue                     nan  sm\n   3  https://netloc.com/path_1?price=10&color=blue                      https     netloc.com  /path_1                price=10&color=blue              path_1   nan      nan      path_1      blue                      10  nan\n====  =================================================================  ========  ==========  =====================  ===================  ==========  =======  =======  =======  ==========  =============  =============  ============\n\n\u0650A more elaborate exmaple on :ref:`how to analyze URLs <sitemaps>` shows how you\nmight use this function after obtaining a set of URLs.\n\nThe resulting DataFrame contains the following columns:\n\n* **url**: The original URLs are listed as a reference. They are decoded for\n  easier reading, and you can set ``decode=False`` if you want to retain the\n  original encoding.\n* **scheme**: Self-explanatory. Note that you can also provide relative URLs\n  `/category/sub-category?one=1&two=2` in which case the `url`, `scheme` and\n  `netloc` columns would be empty. You can mix relative and absolute URLs as\n  well.\n* **netloc**: The network location is the sub-domain (optional) together with\n  the domain and top-level domain and/or the country domain.\n* **path**: The slug of the URL, excluding the query parameters and fragments\n  if any. The path is also split into directories ``dir_1/dir_2/dir_3/...`` to\n  make it easier to categorize and analyze the URLs.\n* **last_dir**: The last directory of each of the URLs. This is usually the\n  part that contains information about the page itself (blog post title,\n  product name, etc.) with previous directories providing meta data (category,\n  sub-category, author name, etc.). In many cases you don't have all URLs with\n  the same number of directories, so they end up unaligned. This extracts all\n  ``last_dir``'s in one column.\n* **query**: If query parameters are available they are given in this column,\n  but more importantly they are parsed and included in separate columns, where\n  each parameter has its own column (with the keys being the names). As in the\n  example above, the query `price=10&color=blue` becomes two columns, one for\n  price and the other for color. If any other URLs in the dataset contain the\n  same parameters, their values will be populated in the same column, and `NA`\n  otherwise.\n* **fragment**: The final part of the URL after the hash mark `#`, linking to a\n  part in the page.\n* **query_***: The query parameter names are prepended with `query_` to make\n  it easy to filter them out, and to avoid any name collissions with other\n  columns, if some URL contains a query parameter called \"url\" for example.\n  In the unlikely event of having a repeated parameter in the same URL, then\n  their values would be delimited by two \"@\" signs `one@@two@@three`. It's\n  unusual, but it happens.\n* **hostname and port**: If available a column for ports will be shown, and if\n  the hostname is different from `netloc` it would also have its own column.\n\nQuery Parameters\n----------------\nThe great thing about parameters is that the names are descriptive (mostly!)\nand once given a certain column you can easily understand what data they\ncontain. Once this is done, you can sort the products by price, filter by\ndestination, get the red and blue items, and so on.\n\nThe URL Path (Directories):\n---------------------------\nHere things are not as straightforward, and there is no way to know what the\nfirst or second directory is supposed to indicate. In general, I can think of\nthree main situations that you can encounter while analyzing directories.\n\n* **Consistent URLs**: This is the simplest case, where all URLs follow the\n  same structure. `/en/product1` clearly shows that the first directory\n  indicates the language of the page. So it can also make sense to rename those\n  columns once you have discovered their meaning.\n\n* **Inconsistent URLs**: This is similar to the previous situation. All URLs\n  follow the same pattern with a few exceptions. Take the following URLs for\n  example:\n\n    * /topic1/title-of-article-1\n    * /es/topic1/title-of-article-2\n    * /es/topic2/title-of-article-3\n    * /topic2/title-of-artilce-4\n\n  You can see that they follow the pattern `/language/topic/article-title`,\n  except for English, which is not explicitly mentioned, but its articles can\n  be identified by having two instead of three directories, as we have for\n  \"/es/\". If URLs are split in this case, yout will end up with `dir_1` having\n  \"topic1\", \"es\", \"es\", and \"topic2\", which distorts the data. Actually you\n  want to have \"en\", \"es\", \"es\", \"en\". In such cases, after making sure you\n  have the right rules and patterns, you might create special columns or\n  replace/insert values to make them consistent, and get them to a state\n  similar to the first example.\n\n* **URLs of different types**: In many cases you will find that sites have\n  different types of pages with completely different roles on the site.\n\n    * /blog/post-1-title.html\n    * /community/help/topic_1\n    * /community/help/topic_2\n\n  Here, once you split the directories, you will see that they don't align\n  properly (because of different lengths), and they can't be compared easily. A\n  good approach is to split your dataset into one for blog posts and another\n  for community content for example.\n\nThe ideal case for the `path` part of the URL is to be split into directories\nof equal length across the dataset, having the right data in the right columns\nand `NA` otherwise. Or, splitting the dataset and analyzing separately.\n\nAnalyzing a large number of URLs\n--------------------------------\n\nHaving a very long list of URLs is a thing that you might encounter with log files,\nbig XML sitemaps, crawling a big website, and so on.\nYou can still use ``url_to_df`` but you might consume a massive amount of memory, in\nsome cases making impossible to process the data. For these cases you can use the\n``output_file`` parameter.\nAll you have to do is provide a path for this output file, and it has to have the\n.parquet extension. This allows you to compress the data, analyze it way more\nefficiently, and you can refer back to the same dataset without having to go through\nthe process again (it can take a few minutes with big datasets).\n\n.. code-block:: python\n   :linenos:\n\n    import advertools as adv\n    import pandas as pd\n\n    adv.url_to_df([url_1, url_2, ...], ouput_file=\"output_file.parquet\")\n    pd.read_parquet(\"output_file.parquet\", columns=[\"scheme\"])\n    pd.read_parquet(\"output_file.parquet\", columns=[\"dir_1\", \"dir_2\"])\n    pd.read_parquet(\n        \"output_file.parquet\",\n        columns=[\"dir_1\", \"dir_2\"],\n        filters=[(\"dir_1\", \"in\", [\"news\", \"politics\"])],\n    )\n\n\"\"\"  # noqa: E501\n\nimport os\nfrom tempfile import TemporaryDirectory\nfrom urllib.parse import parse_qs, unquote, urlsplit\n\nimport numpy as np\nimport pandas as pd\n\n\ndef _url_to_df(urls, decode=True):\n    \"\"\"Split the given URLs into their components to a DataFrame.\n\n    Each column will have its own component, and query parameters and\n    directories will also be parsed and given special columns each.\n\n    :param url urls: A list of URLs to split into components\n    :param bool decode: Whether or not to decode the given URLs\n    :return DataFrame split: A DataFrame with a column for each component\n    \"\"\"\n    if isinstance(urls, str):\n        urls = [urls]\n    decode = unquote if decode else lambda x: x\n    split_list = []\n    for url in urls:\n        split = urlsplit(decode(url))\n        port = split.port\n        hostname = split.hostname if split.hostname != split.netloc else None\n        split = split._asdict()\n        if hostname:\n            split[\"hostname\"] = hostname\n        if port:\n            split[\"port\"] = port\n        parsed_query = parse_qs(split[\"query\"])\n        parsed_query = {\n            \"query_\" + key: \"@@\".join(val) for key, val in parsed_query.items()\n        }\n        split.update(**parsed_query)\n        dirs = split[\"path\"].strip(\"/\").split(\"/\")\n        if dirs[0]:\n            dir_cols = {\"dir_{}\".format(n): d for n, d in enumerate(dirs, 1)}\n            split.update(**dir_cols)\n        split_list.append(split)\n    df = pd.DataFrame(split_list)\n\n    query_df = df.filter(regex=\"query_\")\n    if not query_df.empty:\n        sorted_q_params = query_df.notna().mean().sort_values(ascending=False).index\n        query_df = query_df[sorted_q_params]\n        df = df.drop(query_df.columns, axis=1)\n    dirs_df = df.filter(regex=\"^dir_\")\n    if not dirs_df.empty:\n        df = df.drop(dirs_df.columns, axis=1)\n        dirs_df = dirs_df.assign(last_dir=dirs_df.ffill(axis=1).iloc[:, -1:].squeeze())\n    df = pd.concat([df, dirs_df, query_df], axis=1).replace(\"\", pd.NA)\n    url_list_df = pd.DataFrame({\"url\": [decode(url) for url in urls]})\n    final_df = pd.concat([url_list_df, df], axis=1)\n    return final_df\n\n\ndef url_to_df(urls, decode=True, output_file=None):\n    \"\"\"Split the given URLs into their components to a DataFrame.\n\n    Each column will have its own component, and query parameters and\n    directories will also be parsed and given special columns each.\n\n    Parameters\n    ----------\n    urls : list,pandas.Series\n      A list of URLs to split into components\n    decode : bool, default True\n      Whether or not to decode the given URLs\n    output_file : str\n      The path where to save the output DataFrame with a .parquet extension\n\n    Returns\n    -------\n    urldf : pandas.DataFrame\n      A DataFrame with a column for each URL component\n    \"\"\"\n    if output_file is not None:\n        if output_file.rsplit(\".\")[-1] != \"parquet\":\n            raise ValueError(\"Your output_file has to have a .parquet extension.\")\n    step = 1000\n    sublists = (urls[sub : sub + step] for sub in range(0, len(urls), step))\n\n    with TemporaryDirectory() as tmpdir:\n        for i, sublist in enumerate(sublists):\n            urldf = _url_to_df(sublist, decode=decode)\n            urldf.index = range(i * step, (i * step) + len(urldf))\n            urldf.to_parquet(f\"{tmpdir}/{i:08}.parquet\", index=True, version=\"2.6\")\n        final_df_list = [\n            pd.read_parquet(f\"{tmpdir}/{tmpfile}\") for tmpfile in os.listdir(tmpdir)\n        ]\n        final_df = pd.concat(final_df_list).sort_index()\n    if output_file is not None:\n        final_df.to_parquet(output_file, index=False, version=\"2.6\")\n    else:\n        return final_df\n"
  },
  "GT_src_dict": {
    "advertools/urlytics.py": {
      "_url_to_df": {
        "code": "def _url_to_df(urls, decode=True):\n    \"\"\"Split URLs into their components and return a DataFrame containing parsed elements along with query parameters and directory structures.\n\nParameters\n----------\nurls : list or str\n    A list of URLs to split into components. If a single string is provided, it will be converted into a list.\ndecode : bool, default True\n    Whether to decode the given URLs. If True, `unquote` from `urllib.parse` is used to decode them; otherwise, it retains the original encoding.\n\nReturns\n-------\nDataFrame\n    A DataFrame with each component of the URLs in its own column, including scheme, netloc, path, query parameters (with keys prefixed by 'query_'), and directories split into separate columns. \n\nNotes\n-----\nThe function utilizes `urlsplit` to parse the URLs and `parse_qs` to handle query parameters. Additionally, it constructs dynamic columns for directories labeled as dir_1, dir_2, etc., and a last_dir column representing the final path segment. The result replaces empty strings with `pd.NA` for clarity. The usage of `TemporaryDirectory` from the `tempfile` module facilitates efficient handling of large datasets by processing them in chunks.\"\"\"\n    'Split the given URLs into their components to a DataFrame.\\n\\n    Each column will have its own component, and query parameters and\\n    directories will also be parsed and given special columns each.\\n\\n    :param url urls: A list of URLs to split into components\\n    :param bool decode: Whether or not to decode the given URLs\\n    :return DataFrame split: A DataFrame with a column for each component\\n    '\n    if isinstance(urls, str):\n        urls = [urls]\n    decode = unquote if decode else lambda x: x\n    split_list = []\n    for url in urls:\n        split = urlsplit(decode(url))\n        port = split.port\n        hostname = split.hostname if split.hostname != split.netloc else None\n        split = split._asdict()\n        if hostname:\n            split['hostname'] = hostname\n        if port:\n            split['port'] = port\n        parsed_query = parse_qs(split['query'])\n        parsed_query = {'query_' + key: '@@'.join(val) for key, val in parsed_query.items()}\n        split.update(**parsed_query)\n        dirs = split['path'].strip('/').split('/')\n        if dirs[0]:\n            dir_cols = {'dir_{}'.format(n): d for n, d in enumerate(dirs, 1)}\n            split.update(**dir_cols)\n        split_list.append(split)\n    df = pd.DataFrame(split_list)\n    query_df = df.filter(regex='query_')\n    if not query_df.empty:\n        sorted_q_params = query_df.notna().mean().sort_values(ascending=False).index\n        query_df = query_df[sorted_q_params]\n        df = df.drop(query_df.columns, axis=1)\n    dirs_df = df.filter(regex='^dir_')\n    if not dirs_df.empty:\n        df = df.drop(dirs_df.columns, axis=1)\n        dirs_df = dirs_df.assign(last_dir=dirs_df.ffill(axis=1).iloc[:, -1:].squeeze())\n    df = pd.concat([df, dirs_df, query_df], axis=1).replace('', pd.NA)\n    url_list_df = pd.DataFrame({'url': [decode(url) for url in urls]})\n    final_df = pd.concat([url_list_df, df], axis=1)\n    return final_df",
        "docstring": "Split URLs into their components and return a DataFrame containing parsed elements along with query parameters and directory structures.\n\nParameters\n----------\nurls : list or str\n    A list of URLs to split into components. If a single string is provided, it will be converted into a list.\ndecode : bool, default True\n    Whether to decode the given URLs. If True, `unquote` from `urllib.parse` is used to decode them; otherwise, it retains the original encoding.\n\nReturns\n-------\nDataFrame\n    A DataFrame with each component of the URLs in its own column, including scheme, netloc, path, query parameters (with keys prefixed by 'query_'), and directories split into separate columns. \n\nNotes\n-----\nThe function utilizes `urlsplit` to parse the URLs and `parse_qs` to handle query parameters. Additionally, it constructs dynamic columns for directories labeled as dir_1, dir_2, etc., and a last_dir column representing the final path segment. The result replaces empty strings with `pd.NA` for clarity. The usage of `TemporaryDirectory` from the `tempfile` module facilitates efficient handling of large datasets by processing them in chunks.",
        "signature": "def _url_to_df(urls, decode=True):",
        "type": "Function",
        "class_signature": null
      },
      "url_to_df": {
        "code": "def url_to_df(urls, decode=True, output_file=None):\n    \"\"\"Split a list of URLs into their individual components and return as a DataFrame.\n\nThis function processes the provided URLs, extracting elements such as scheme, netloc, path, query parameters, directories, and fragments. Each component is returned in a structured DataFrame format, making it easier to analyze and manipulate URL data. The function can also save the resulting DataFrame as a Parquet file for efficient storage and retrieval.\n\nParameters\n----------\nurls : list or pandas.Series\n    A list of URLs to split into components.\ndecode : bool, default True\n    If True, the URLs will be decoded for readability; otherwise, the original encoding is preserved.\noutput_file : str, optional\n    The file path to save the output DataFrame, which must have a .parquet extension. If not provided, the function returns the DataFrame.\n\nReturns\n-------\nDataFrame\n    A pandas DataFrame containing individual columns for each URL component, including additional columns for parsed query parameters and path directories.\n\nRaises\n------\nValueError\n    If the output_file does not have a .parquet extension.\n\nNotes\n-----\nThe function processes URLs in batches of 1000 for memory efficiency using TemporaryDirectory to manage intermediate files. The `_url_to_df` utility function is invoked to handle the core parsing logic, which utilizes the `urlsplit` function from the `urllib.parse` module to decompose the URLs. Query parameters are included as separate columns, prefixed with 'query_' to avoid naming collisions.\n\nDependencies\n------------\nThis function relies on the following external libraries: `pandas`, `numpy`, and `urllib.parse`.\"\"\"\n    'Split the given URLs into their components to a DataFrame.\\n\\n    Each column will have its own component, and query parameters and\\n    directories will also be parsed and given special columns each.\\n\\n    Parameters\\n    ----------\\n    urls : list,pandas.Series\\n      A list of URLs to split into components\\n    decode : bool, default True\\n      Whether or not to decode the given URLs\\n    output_file : str\\n      The path where to save the output DataFrame with a .parquet extension\\n\\n    Returns\\n    -------\\n    urldf : pandas.DataFrame\\n      A DataFrame with a column for each URL component\\n    '\n    if output_file is not None:\n        if output_file.rsplit('.')[-1] != 'parquet':\n            raise ValueError('Your output_file has to have a .parquet extension.')\n    step = 1000\n    sublists = (urls[sub:sub + step] for sub in range(0, len(urls), step))\n    with TemporaryDirectory() as tmpdir:\n        for i, sublist in enumerate(sublists):\n            urldf = _url_to_df(sublist, decode=decode)\n            urldf.index = range(i * step, i * step + len(urldf))\n            urldf.to_parquet(f'{tmpdir}/{i:08}.parquet', index=True, version='2.6')\n        final_df_list = [pd.read_parquet(f'{tmpdir}/{tmpfile}') for tmpfile in os.listdir(tmpdir)]\n        final_df = pd.concat(final_df_list).sort_index()\n    if output_file is not None:\n        final_df.to_parquet(output_file, index=False, version='2.6')\n    else:\n        return final_df",
        "docstring": "Split a list of URLs into their individual components and return as a DataFrame.\n\nThis function processes the provided URLs, extracting elements such as scheme, netloc, path, query parameters, directories, and fragments. Each component is returned in a structured DataFrame format, making it easier to analyze and manipulate URL data. The function can also save the resulting DataFrame as a Parquet file for efficient storage and retrieval.\n\nParameters\n----------\nurls : list or pandas.Series\n    A list of URLs to split into components.\ndecode : bool, default True\n    If True, the URLs will be decoded for readability; otherwise, the original encoding is preserved.\noutput_file : str, optional\n    The file path to save the output DataFrame, which must have a .parquet extension. If not provided, the function returns the DataFrame.\n\nReturns\n-------\nDataFrame\n    A pandas DataFrame containing individual columns for each URL component, including additional columns for parsed query parameters and path directories.\n\nRaises\n------\nValueError\n    If the output_file does not have a .parquet extension.\n\nNotes\n-----\nThe function processes URLs in batches of 1000 for memory efficiency using TemporaryDirectory to manage intermediate files. The `_url_to_df` utility function is invoked to handle the core parsing logic, which utilizes the `urlsplit` function from the `urllib.parse` module to decompose the URLs. Query parameters are included as separate columns, prefixed with 'query_' to avoid naming collisions.\n\nDependencies\n------------\nThis function relies on the following external libraries: `pandas`, `numpy`, and `urllib.parse`.",
        "signature": "def url_to_df(urls, decode=True, output_file=None):",
        "type": "Function",
        "class_signature": null
      }
    }
  },
  "dependency_dict": {},
  "PRD": "# PROJECT NAME: advertools-test_urlytics\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 advertools/\n    \u2514\u2500\u2500 urlytics.py\n        \u251c\u2500\u2500 _url_to_df\n        \u2514\u2500\u2500 url_to_df\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThe module facilitates comprehensive parsing and analysis of URLs by converting them into structured data representations, such as dataframes. It provides functionalities to extract and standardize key URL components, including scheme, hostname, query parameters, path segments, fragments, ports, and directories. Additionally, the module supports preserving the order of URL lists, managing relative and absolute path variations, and exporting parsed data into output files in formats such as Parquet. By automating URL decomposition and ensuring consistent handling of complex URL patterns, the module simplifies URL manipulation, analysis, and debugging for developers, enhancing efficiency and accuracy in working with web data.\n\n## FILE 1: advertools/urlytics.py\n\n- FUNCTION NAME: _url_to_df\n  - SIGNATURE: def _url_to_df(urls, decode=True):\n  - DOCSTRING: \n```python\n\"\"\"\nSplit URLs into their components and return a DataFrame containing parsed elements along with query parameters and directory structures.\n\nParameters\n----------\nurls : list or str\n    A list of URLs to split into components. If a single string is provided, it will be converted into a list.\ndecode : bool, default True\n    Whether to decode the given URLs. If True, `unquote` from `urllib.parse` is used to decode them; otherwise, it retains the original encoding.\n\nReturns\n-------\nDataFrame\n    A DataFrame with each component of the URLs in its own column, including scheme, netloc, path, query parameters (with keys prefixed by 'query_'), and directories split into separate columns. \n\nNotes\n-----\nThe function utilizes `urlsplit` to parse the URLs and `parse_qs` to handle query parameters. Additionally, it constructs dynamic columns for directories labeled as dir_1, dir_2, etc., and a last_dir column representing the final path segment. The result replaces empty strings with `pd.NA` for clarity. The usage of `TemporaryDirectory` from the `tempfile` module facilitates efficient handling of large datasets by processing them in chunks.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - advertools/urlytics.py:url_to_df\n\n- FUNCTION NAME: url_to_df\n  - SIGNATURE: def url_to_df(urls, decode=True, output_file=None):\n  - DOCSTRING: \n```python\n\"\"\"\nSplit a list of URLs into their individual components and return as a DataFrame.\n\nThis function processes the provided URLs, extracting elements such as scheme, netloc, path, query parameters, directories, and fragments. Each component is returned in a structured DataFrame format, making it easier to analyze and manipulate URL data. The function can also save the resulting DataFrame as a Parquet file for efficient storage and retrieval.\n\nParameters\n----------\nurls : list or pandas.Series\n    A list of URLs to split into components.\ndecode : bool, default True\n    If True, the URLs will be decoded for readability; otherwise, the original encoding is preserved.\noutput_file : str, optional\n    The file path to save the output DataFrame, which must have a .parquet extension. If not provided, the function returns the DataFrame.\n\nReturns\n-------\nDataFrame\n    A pandas DataFrame containing individual columns for each URL component, including additional columns for parsed query parameters and path directories.\n\nRaises\n------\nValueError\n    If the output_file does not have a .parquet extension.\n\nNotes\n-----\nThe function processes URLs in batches of 1000 for memory efficiency using TemporaryDirectory to manage intermediate files. The `_url_to_df` utility function is invoked to handle the core parsing logic, which utilizes the `urlsplit` function from the `urllib.parse` module to decompose the URLs. Query parameters are included as separate columns, prefixed with 'query_' to avoid naming collisions.\n\nDependencies\n------------\nThis function relies on the following external libraries: `pandas`, `numpy`, and `urllib.parse`.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - advertools/urlytics.py:_url_to_df\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "advertools/urlytics.py": "\"\"\"\n.. _urlytics:\n\nSplit, Parse, and Analyze URL Structure\n=======================================\n\nExtracting information from URLs can be a little tedious, yet very important.\nUsing the standard for URLs we can extract a lot of information in a fairly\nstructured manner.\n\nThere are many situations in which you have many URLs that you want to better\nunderstand:\n\n* **Analytics reports**: Whichever analytics system you use, whether Google\n  Analytics, search console, or any other reporting tool that reports on URLs,\n  your reports can be enhanced by splitting URLs, and in effect becoming four\n  or five data points as opposed to one.\n* :ref:`Crawl datasets <crawl>`: The result of any crawl you run typically\n  contains the URLs, which can benefit from the same enhancement.\n* :ref:`SERP datasets <serp>`: Which are basically about URLs.\n* :ref:`Extracted URLs <extract>`: Extracting URLs from social media posts is\n  one thing you might want to do to better understand those posts, and further\n  splitting URLs can also help.\n* :ref:`XML sitemaps <sitemaps>`: Right after downloading a sitemap(s)\n  splitting it further can help in giving a better perspective on the dataset.\n\nThe main function here is :func:`url_to_df`, which as the name suggests,\nconverts URLs to DataFrames.\n\n\n.. thebe-button::\n    Run this code\n\n.. code-block::\n    :class: thebe, thebe-init\n\n    import advertools as adv\n\n    urls = ['https://netloc.com/path_1/path_2?price=10&color=blue#frag_1',\n            'https://netloc.com/path_1/path_2?price=15&color=red#frag_2',\n            'https://netloc.com/path_1/path_2/path_3?size=sm&color=blue#frag_1',\n            'https://netloc.com/path_1?price=10&color=blue']\n    adv.url_to_df(urls)\n\n====  =================================================================  ========  ==========  =====================  ===================  ==========  =======  =======  =======  ==========  =============  =============  ============\n  ..  url                                                                scheme    netloc      path                   query                fragment    dir_1    dir_2    dir_3    last_dir    query_color      query_price  query_size\n====  =================================================================  ========  ==========  =====================  ===================  ==========  =======  =======  =======  ==========  =============  =============  ============\n   0  https://netloc.com/path_1/path_2?price=10&color=blue#frag_1        https     netloc.com  /path_1/path_2         price=10&color=blue  frag_1      path_1   path_2   nan      path_2      blue                      10  nan\n   1  https://netloc.com/path_1/path_2?price=15&color=red#frag_2         https     netloc.com  /path_1/path_2         price=15&color=red   frag_2      path_1   path_2   nan      path_2      red                       15  nan\n   2  https://netloc.com/path_1/path_2/path_3?size=sm&color=blue#frag_1  https     netloc.com  /path_1/path_2/path_3  size=sm&color=blue   frag_1      path_1   path_2   path_3   path_3      blue                     nan  sm\n   3  https://netloc.com/path_1?price=10&color=blue                      https     netloc.com  /path_1                price=10&color=blue              path_1   nan      nan      path_1      blue                      10  nan\n====  =================================================================  ========  ==========  =====================  ===================  ==========  =======  =======  =======  ==========  =============  =============  ============\n\n\u0650A more elaborate exmaple on :ref:`how to analyze URLs <sitemaps>` shows how you\nmight use this function after obtaining a set of URLs.\n\nThe resulting DataFrame contains the following columns:\n\n* **url**: The original URLs are listed as a reference. They are decoded for\n  easier reading, and you can set ``decode=False`` if you want to retain the\n  original encoding.\n* **scheme**: Self-explanatory. Note that you can also provide relative URLs\n  `/category/sub-category?one=1&two=2` in which case the `url`, `scheme` and\n  `netloc` columns would be empty. You can mix relative and absolute URLs as\n  well.\n* **netloc**: The network location is the sub-domain (optional) together with\n  the domain and top-level domain and/or the country domain.\n* **path**: The slug of the URL, excluding the query parameters and fragments\n  if any. The path is also split into directories ``dir_1/dir_2/dir_3/...`` to\n  make it easier to categorize and analyze the URLs.\n* **last_dir**: The last directory of each of the URLs. This is usually the\n  part that contains information about the page itself (blog post title,\n  product name, etc.) with previous directories providing meta data (category,\n  sub-category, author name, etc.). In many cases you don't have all URLs with\n  the same number of directories, so they end up unaligned. This extracts all\n  ``last_dir``'s in one column.\n* **query**: If query parameters are available they are given in this column,\n  but more importantly they are parsed and included in separate columns, where\n  each parameter has its own column (with the keys being the names). As in the\n  example above, the query `price=10&color=blue` becomes two columns, one for\n  price and the other for color. If any other URLs in the dataset contain the\n  same parameters, their values will be populated in the same column, and `NA`\n  otherwise.\n* **fragment**: The final part of the URL after the hash mark `#`, linking to a\n  part in the page.\n* **query_***: The query parameter names are prepended with `query_` to make\n  it easy to filter them out, and to avoid any name collissions with other\n  columns, if some URL contains a query parameter called \"url\" for example.\n  In the unlikely event of having a repeated parameter in the same URL, then\n  their values would be delimited by two \"@\" signs `one@@two@@three`. It's\n  unusual, but it happens.\n* **hostname and port**: If available a column for ports will be shown, and if\n  the hostname is different from `netloc` it would also have its own column.\n\nQuery Parameters\n----------------\nThe great thing about parameters is that the names are descriptive (mostly!)\nand once given a certain column you can easily understand what data they\ncontain. Once this is done, you can sort the products by price, filter by\ndestination, get the red and blue items, and so on.\n\nThe URL Path (Directories):\n---------------------------\nHere things are not as straightforward, and there is no way to know what the\nfirst or second directory is supposed to indicate. In general, I can think of\nthree main situations that you can encounter while analyzing directories.\n\n* **Consistent URLs**: This is the simplest case, where all URLs follow the\n  same structure. `/en/product1` clearly shows that the first directory\n  indicates the language of the page. So it can also make sense to rename those\n  columns once you have discovered their meaning.\n\n* **Inconsistent URLs**: This is similar to the previous situation. All URLs\n  follow the same pattern with a few exceptions. Take the following URLs for\n  example:\n\n    * /topic1/title-of-article-1\n    * /es/topic1/title-of-article-2\n    * /es/topic2/title-of-article-3\n    * /topic2/title-of-artilce-4\n\n  You can see that they follow the pattern `/language/topic/article-title`,\n  except for English, which is not explicitly mentioned, but its articles can\n  be identified by having two instead of three directories, as we have for\n  \"/es/\". If URLs are split in this case, yout will end up with `dir_1` having\n  \"topic1\", \"es\", \"es\", and \"topic2\", which distorts the data. Actually you\n  want to have \"en\", \"es\", \"es\", \"en\". In such cases, after making sure you\n  have the right rules and patterns, you might create special columns or\n  replace/insert values to make them consistent, and get them to a state\n  similar to the first example.\n\n* **URLs of different types**: In many cases you will find that sites have\n  different types of pages with completely different roles on the site.\n\n    * /blog/post-1-title.html\n    * /community/help/topic_1\n    * /community/help/topic_2\n\n  Here, once you split the directories, you will see that they don't align\n  properly (because of different lengths), and they can't be compared easily. A\n  good approach is to split your dataset into one for blog posts and another\n  for community content for example.\n\nThe ideal case for the `path` part of the URL is to be split into directories\nof equal length across the dataset, having the right data in the right columns\nand `NA` otherwise. Or, splitting the dataset and analyzing separately.\n\nAnalyzing a large number of URLs\n--------------------------------\n\nHaving a very long list of URLs is a thing that you might encounter with log files,\nbig XML sitemaps, crawling a big website, and so on.\nYou can still use ``url_to_df`` but you might consume a massive amount of memory, in\nsome cases making impossible to process the data. For these cases you can use the\n``output_file`` parameter.\nAll you have to do is provide a path for this output file, and it has to have the\n.parquet extension. This allows you to compress the data, analyze it way more\nefficiently, and you can refer back to the same dataset without having to go through\nthe process again (it can take a few minutes with big datasets).\n\n.. code-block:: python\n   :linenos:\n\n    import advertools as adv\n    import pandas as pd\n\n    adv.url_to_df([url_1, url_2, ...], ouput_file=\"output_file.parquet\")\n    pd.read_parquet(\"output_file.parquet\", columns=[\"scheme\"])\n    pd.read_parquet(\"output_file.parquet\", columns=[\"dir_1\", \"dir_2\"])\n    pd.read_parquet(\n        \"output_file.parquet\",\n        columns=[\"dir_1\", \"dir_2\"],\n        filters=[(\"dir_1\", \"in\", [\"news\", \"politics\"])],\n    )\n\n\"\"\"\nimport os\nfrom tempfile import TemporaryDirectory\nfrom urllib.parse import parse_qs, unquote, urlsplit\nimport numpy as np\nimport pandas as pd"
  },
  "call_tree": {
    "tests/test_urlytics.py:test_urltodf_raises_on_wrong_file_ext": {
      "advertools/urlytics.py:url_to_df": {}
    },
    "tests/test_urlytics.py:test_urltodf_convert_str_tolist": {
      "advertools/urlytics.py:url_to_df": {
        "advertools/urlytics.py:_url_to_df": {}
      }
    },
    "tests/test_urlytics.py:test_path_rel_noslash": {
      "advertools/urlytics.py:url_to_df": {
        "advertools/urlytics.py:_url_to_df": {}
      }
    },
    "tests/test_urlytics.py:test_abs_and_rel": {
      "advertools/urlytics.py:url_to_df": {
        "advertools/urlytics.py:_url_to_df": {}
      }
    },
    "tests/test_urlytics.py:test_domainpath_fragrel_full": {
      "advertools/urlytics.py:url_to_df": {
        "advertools/urlytics.py:_url_to_df": {}
      }
    },
    "tests/test_urlytics.py:test_no_path_has_no_last_dir": {
      "advertools/urlytics.py:url_to_df": {
        "advertools/urlytics.py:_url_to_df": {}
      }
    },
    "tests/test_urlytics.py:test_all": {
      "advertools/urlytics.py:url_to_df": {
        "advertools/urlytics.py:_url_to_df": {}
      }
    },
    "tests/test_urlytics.py:test_query_params_are_ordered_by_fullness": {
      "advertools/urlytics.py:url_to_df": {
        "advertools/urlytics.py:_url_to_df": {}
      }
    },
    "tests/test_urlytics.py:test_urltodf_produces_outputfile": {
      "advertools/urlytics.py:url_to_df": {
        "advertools/urlytics.py:_url_to_df": {}
      }
    },
    "tests/test_urlytics.py:test_urltodf_preserves_order_of_supplied_urls": {
      "advertools/urlytics.py:url_to_df": {
        "advertools/urlytics.py:_url_to_df": {}
      }
    }
  }
}