{
  "dir_path": "/app/datashader",
  "package_name": "datashader",
  "sample_name": "datashader-test_tiles",
  "src_dir": "datashader/",
  "test_dir": "datashader/tests/",
  "test_file": "datashader/tests/test_tiles.py",
  "test_code": "from __future__ import annotations\nimport datashader as ds\nimport datashader.transfer_functions as tf\n\nfrom datashader.colors import viridis\n\nfrom datashader.tiles import render_tiles\nfrom datashader.tiles import gen_super_tiles\nfrom datashader.tiles import _get_super_tile_min_max\nfrom datashader.tiles import calculate_zoom_level_stats\nfrom datashader.tiles import MercatorTileDefinition\n\nfrom datashader.tests.utils import dask_skip\n\nimport numpy as np\nimport pandas as pd\n\nTOLERANCE = 0.01\n\nMERCATOR_CONST = 20037508.34\n\ndf = None\ndef mock_load_data_func(x_range, y_range):\n    global df\n    if df is None:\n        rng = np.random.default_rng()\n        xs = rng.normal(loc=0, scale=500000, size=10000000)\n        ys = rng.normal(loc=0, scale=500000, size=10000000)\n        df = pd.DataFrame(dict(x=xs, y=ys))\n\n    return df.loc[df['x'].between(*x_range) & df['y'].between(*y_range)]\n\n\ndef mock_rasterize_func(df, x_range, y_range, height, width):\n    cvs = ds.Canvas(x_range=x_range, y_range=y_range,\n                    plot_height=height, plot_width=width)\n    agg = cvs.points(df, 'x', 'y')\n    return agg\n\n\ndef mock_shader_func(agg, span=None):\n    img = tf.shade(agg, cmap=viridis, span=span, how='log')\n    img = tf.set_background(img, 'black')\n    return img\n\n\ndef mock_post_render_func(img, **kwargs):\n    from PIL import ImageDraw\n\n    (x, y) = (5, 5)\n    info = \"x={} / y={} / z={}, w={}, h={}\".format(kwargs['x'],\n                                                   kwargs['y'],\n                                                   kwargs['z'],\n                                                   img.width,\n                                                   img.height)\n\n    draw = ImageDraw.Draw(img)\n    draw.text((x, y), info, fill='rgb(255, 255, 255)')\n    return img\n\n\n# TODO: mark with slow_test\n@dask_skip\ndef test_render_tiles():\n    full_extent_of_data = (-500000, -500000,\n                           500000, 500000)\n    levels = list(range(2))\n    output_path = 'test_tiles_output'\n    results = render_tiles(full_extent_of_data,\n                           levels,\n                           load_data_func=mock_load_data_func,\n                           rasterize_func=mock_rasterize_func,\n                           shader_func=mock_shader_func,\n                           post_render_func=mock_post_render_func,\n                           output_path=output_path)\n\n    assert results\n    assert isinstance(results, dict)\n\n    for level in levels:\n        assert level in results\n        assert isinstance(results[level], dict)\n\n    assert results[0]['success']\n    assert results[0]['stats']\n    assert results[0]['supertile_count']\n\n\ndef assert_is_numeric(value):\n    is_int_or_float = isinstance(value, (int, float))\n    type_name = type(value).__name__\n    is_numpy_int_or_float = 'int' in type_name or 'float' in type_name\n    assert any([is_int_or_float, is_numpy_int_or_float])\n\n\n\ndef test_get_super_tile_min_max():\n\n    tile_info = {'level': 0,\n                'x_range': (-MERCATOR_CONST, MERCATOR_CONST),\n                'y_range': (-MERCATOR_CONST, MERCATOR_CONST),\n                'tile_size': 256,\n                'span': (0, 1000)}\n\n    agg = _get_super_tile_min_max(tile_info, mock_load_data_func, mock_rasterize_func)\n\n    result = [np.nanmin(agg.data), np.nanmax(agg.data)]\n\n    assert isinstance(result, list)\n    assert len(result) == 2\n    assert_is_numeric(result[0])\n    assert_is_numeric(result[1])\n\n@dask_skip\ndef test_calculate_zoom_level_stats_with_fullscan_ranging_strategy():\n    full_extent = (-MERCATOR_CONST, -MERCATOR_CONST,\n                   MERCATOR_CONST, MERCATOR_CONST)\n    level = 0\n    color_ranging_strategy = 'fullscan'\n    super_tiles, span = calculate_zoom_level_stats(list(gen_super_tiles(full_extent, level)),\n                                        mock_load_data_func,\n                                        mock_rasterize_func,\n                                        color_ranging_strategy=color_ranging_strategy)\n\n    assert isinstance(span, (list, tuple))\n    assert len(span) == 2\n    assert_is_numeric(span[0])\n    assert_is_numeric(span[1])\n\ndef test_meters_to_tile():\n    # Part of NYC (used in taxi demo)\n    full_extent_of_data = (-8243206.93436, 4968192.04221, -8226510.539480001, 4982886.20438)\n    xmin, ymin, xmax, ymax = full_extent_of_data\n    zoom = 12\n    tile_def = MercatorTileDefinition((xmin, xmax), (ymin, ymax), tile_size=256)\n    tile = tile_def.meters_to_tile(xmin, ymin, zoom)\n    assert tile == (1205, 1540) # using Google tile coordinates, not TMS\n",
  "GT_file_code": {
    "datashader/tiles.py": "from __future__ import annotations\nfrom io import BytesIO\n\nimport math\nimport os\n\n\nimport numpy as np\n\nfrom PIL.Image import fromarray\ntry:\n    import dask\n    import dask.bag as db\nexcept ImportError:\n    dask, db = None, None\n\n__all__ = ['render_tiles', 'MercatorTileDefinition']\n\n\n# helpers ---------------------------------------------------------------------\ndef _create_dir(path):\n    import errno\n    import os\n\n    try:\n        os.makedirs(path)\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise\n\n\ndef _get_super_tile_min_max(tile_info, load_data_func, rasterize_func):\n    tile_size = tile_info['tile_size']\n    df = load_data_func(tile_info['x_range'], tile_info['y_range'])\n    agg = rasterize_func(df, x_range=tile_info['x_range'],\n                         y_range=tile_info['y_range'],\n                         height=tile_size, width=tile_size)\n    return agg\n\n\ndef calculate_zoom_level_stats(super_tiles, load_data_func,\n                               rasterize_func,\n                               color_ranging_strategy='fullscan'):\n    if color_ranging_strategy == 'fullscan':\n        stats = []\n        is_bool = False\n        for super_tile in super_tiles:\n            agg = _get_super_tile_min_max(super_tile, load_data_func, rasterize_func)\n            super_tile['agg'] = agg\n            if agg.dtype.kind == 'b':\n                is_bool = True\n            else:\n                stats.append(np.nanmin(agg.data))\n                stats.append(np.nanmax(agg.data))\n        if is_bool:\n            span = (0, 1)\n        elif dask:\n            b = db.from_sequence(stats)\n            span = dask.compute(b.min(), b.max())\n        else:\n            raise ValueError('Dask is required for non-boolean data')\n        return super_tiles, span\n    else:\n        raise ValueError('Invalid color_ranging_strategy option')\n\n\ndef render_tiles(full_extent, levels, load_data_func,\n                 rasterize_func, shader_func,\n                 post_render_func, output_path, color_ranging_strategy='fullscan'):\n    if not dask:\n        raise ImportError('Dask is required for rendering tiles')\n    results = {}\n    for level in levels:\n        print('calculating statistics for level {}'.format(level))\n        super_tiles, span = calculate_zoom_level_stats(list(gen_super_tiles(full_extent, level)),\n                                                       load_data_func, rasterize_func,\n                                                       color_ranging_strategy=color_ranging_strategy)\n        print('rendering {} supertiles for zoom level {} with span={}'.format(len(super_tiles),\n                                                                              level, span))\n        b = db.from_sequence(super_tiles)\n        b.map(render_super_tile, span, output_path, shader_func, post_render_func).compute()\n        results[level] = dict(success=True, stats=span, supertile_count=len(super_tiles))\n\n    return results\n\n\ndef gen_super_tiles(extent, zoom_level, span=None):\n    xmin, ymin, xmax, ymax = extent\n    super_tile_size = min(2 ** 4 * 256,\n                          (2 ** zoom_level) * 256)\n    super_tile_def = MercatorTileDefinition(x_range=(xmin, xmax), y_range=(ymin, ymax),\n                                            tile_size=super_tile_size)\n    super_tiles = super_tile_def.get_tiles_by_extent(extent, zoom_level)\n    for s in super_tiles:\n        st_extent = s[3]\n        x_range = (st_extent[0], st_extent[2])\n        y_range = (st_extent[1], st_extent[3])\n        yield {'level': zoom_level,\n               'x_range': x_range,\n               'y_range': y_range,\n               'tile_size': super_tile_def.tile_size,\n               'span': span}\n\n\ndef render_super_tile(tile_info, span, output_path, shader_func, post_render_func):\n    level = tile_info['level']\n    ds_img = shader_func(tile_info['agg'], span=span)\n    return create_sub_tiles(ds_img, level, tile_info, output_path, post_render_func)\n\n\ndef create_sub_tiles(data_array, level, tile_info, output_path, post_render_func=None):\n    # validate / createoutput_dir\n    _create_dir(output_path)\n\n    # create tile source\n    tile_def = MercatorTileDefinition(x_range=tile_info['x_range'],\n                                      y_range=tile_info['y_range'],\n                                      tile_size=256)\n\n    # create Tile Renderer\n    if output_path.startswith('s3:'):\n        renderer = S3TileRenderer(tile_def, output_location=output_path,\n                                  post_render_func=post_render_func)\n    else:\n        renderer = FileSystemTileRenderer(tile_def, output_location=output_path,\n                                          post_render_func=post_render_func)\n\n    return renderer.render(data_array, level=level)\n\n\ndef invert_y_tile(y, z):\n    # Convert from TMS to Google tile y coordinate, and vice versa\n    return (2 ** z) - 1 - y\n\n\n# TODO: change name from source to definition\nclass MercatorTileDefinition:\n    ''' Implementation of mercator tile source\n    In general, tile sources are used as a required input for ``TileRenderer``.\n\n    Parameters\n    ----------\n\n    x_range : tuple\n      full extent of x dimension in data units\n\n    y_range : tuple\n      full extent of y dimension in data units\n\n    max_zoom : int\n      A maximum zoom level for the tile layer. This is the most zoomed-in level.\n\n    min_zoom : int\n      A minimum zoom level for the tile layer. This is the most zoomed-out level.\n\n    max_zoom : int\n      A maximum zoom level for the tile layer. This is the most zoomed-in level.\n\n    x_origin_offset : int\n      An x-offset in plot coordinates.\n\n    y_origin_offset : int\n      An y-offset in plot coordinates.\n\n    initial_resolution : int\n      Resolution (plot_units / pixels) of minimum zoom level of tileset\n      projection. None to auto-compute.\n\n    format : int\n      An y-offset in plot coordinates.\n\n    Output\n    ------\n    tileScheme: MercatorTileSource\n\n    '''\n\n    def __init__(self, x_range, y_range, tile_size=256, min_zoom=0, max_zoom=30,\n                 x_origin_offset=20037508.34, y_origin_offset=20037508.34,\n                 initial_resolution=156543.03392804097):\n        self.x_range = x_range\n        self.y_range = y_range\n        self.tile_size = tile_size\n        self.min_zoom = min_zoom\n        self.max_zoom = max_zoom\n        self.x_origin_offset = x_origin_offset\n        self.y_origin_offset = y_origin_offset\n        self.initial_resolution = initial_resolution\n        self._resolutions = [\n            self._get_resolution(z) for z in range(self.min_zoom, self.max_zoom + 1)]\n\n    def to_ogc_tile_metadata(self, output_file_path):\n        '''\n        Create OGC tile metadata XML\n        '''\n        pass\n\n    def to_esri_tile_metadata(self, output_file_path):\n        '''\n        Create ESRI tile metadata JSON\n        '''\n        pass\n\n    def is_valid_tile(self, x, y, z):\n\n        if x < 0 or x >= math.pow(2, z):\n            return False\n\n        if y < 0 or y >= math.pow(2, z):\n            return False\n\n        return True\n\n    # TODO ngjit?\n    def _get_resolution(self, z):\n        return self.initial_resolution / (2 ** z)\n\n    def get_resolution_by_extent(self, extent, height, width):\n        x_rs = (extent[2] - extent[0]) / width\n        y_rs = (extent[3] - extent[1]) / height\n        return [x_rs, y_rs]\n\n    def get_level_by_extent(self, extent, height, width):\n        x_rs = (extent[2] - extent[0]) / width\n        y_rs = (extent[3] - extent[1]) / height\n        resolution = max(x_rs, y_rs)\n\n        # TODO: refactor this...\n        i = 0\n        for r in self._resolutions:\n            if resolution > r:\n                if i == 0:\n                    return 0\n                if i > 0:\n                    return i - 1\n            i += 1\n        return (i - 1)\n\n    def pixels_to_meters(self, px, py, level):\n        res = self._get_resolution(level)\n        mx = (px * res) - self.x_origin_offset\n        my = (py * res) - self.y_origin_offset\n        return (mx, my)\n\n    def meters_to_pixels(self, mx, my, level):\n        res = self._get_resolution(level)\n        px = (mx + self.x_origin_offset) / res\n        py = (my + self.y_origin_offset) / res\n        return (px, py)\n\n    def pixels_to_tile(self, px, py, level):\n        tx = math.ceil(px / self.tile_size)\n        tx = tx if tx == 0 else tx - 1\n        ty = max(math.ceil(py / self.tile_size) - 1, 0)\n        # convert from TMS y coordinate\n        return (int(tx), invert_y_tile(int(ty), level))\n\n    def pixels_to_raster(self, px, py, level):\n        map_size = self.tile_size << level\n        return (px, map_size - py)\n\n    def meters_to_tile(self, mx, my, level):\n        px, py = self.meters_to_pixels(mx, my, level)\n        return self.pixels_to_tile(px, py, level)\n\n    def get_tiles_by_extent(self, extent, level):\n\n        # unpack extent and convert to tile coordinates\n        xmin, ymin, xmax, ymax = extent\n        # note y coordinates are reversed since they are in opposite direction to meters\n        txmin, tymax = self.meters_to_tile(xmin, ymin, level)\n        txmax, tymin = self.meters_to_tile(xmax, ymax, level)\n\n        # TODO: vectorize?\n        tiles = []\n        for ty in range(tymin, tymax + 1):\n            for tx in range(txmin, txmax + 1):\n                if self.is_valid_tile(tx, ty, level):\n                    t = (tx, ty, level, self.get_tile_meters(tx, ty, level))\n                    tiles.append(t)\n\n        return tiles\n\n    def get_tile_meters(self, tx, ty, level):\n        ty = invert_y_tile(ty, level)  # convert to TMS for conversion to meters\n        xmin, ymin = self.pixels_to_meters(tx * self.tile_size, ty * self.tile_size, level)\n        xmax, ymax = self.pixels_to_meters((tx + 1) * self.tile_size,\n                                           (ty + 1) * self.tile_size, level)\n        return (xmin, ymin, xmax, ymax)\n\n\nclass TileRenderer:\n\n    def __init__(self, tile_definition, output_location, tile_format='PNG',\n                 post_render_func=None):\n\n        self.tile_def = tile_definition\n        self.output_location = output_location\n        self.tile_format = tile_format\n        self.post_render_func = post_render_func\n\n        # TODO: add all the formats supported by PIL\n        if self.tile_format not in ('PNG', 'JPG'):\n            raise ValueError('Invalid output format')\n\n    def render(self, da, level):\n        xmin, xmax = self.tile_def.x_range\n        ymin, ymax = self.tile_def.y_range\n        extent = xmin, ymin, xmax, ymax\n\n        tiles = self.tile_def.get_tiles_by_extent(extent, level)\n        for t in tiles:\n            x, y, z, data_extent = t\n            dxmin, dymin, dxmax, dymax = data_extent\n            arr = da.loc[{'x': slice(dxmin, dxmax), 'y': slice(dymin, dymax)}]\n\n            if 0 in arr.shape:\n                continue\n\n            # flip since y tiles go down (Google map tiles\n            img = fromarray(np.flip(arr.data, 0), 'RGBA')\n\n            if self.post_render_func:\n                extras = dict(x=x, y=y, z=z)\n                img = self.post_render_func(img, **extras)\n\n            yield (img, x, y, z)\n\n\ndef tile_previewer(full_extent, tileset_url,\n                   output_dir=None,\n                   filename='index.html',\n                   title='Datashader Tileset',\n                   min_zoom=0, max_zoom=40,\n                   height=None, width=None,\n                   **kwargs):\n    '''Helper function for creating a simple Bokeh figure with\n    a WMTS Tile Source.\n\n    Notes\n    -----\n    - if you don't supply height / width, stretch_both sizing_mode is used.\n    - supply an output_dir to write figure to disk.\n    '''\n\n    try:\n        from bokeh.plotting import figure\n        from bokeh.models.tiles import WMTSTileSource\n        from bokeh.io import output_file, save\n        from os import path\n    except ImportError:\n        raise ImportError('install bokeh to enable creation of simple tile viewer')\n\n    if output_dir:\n        output_file(filename=path.join(output_dir, filename),\n                    title=title)\n\n    xmin, ymin, xmax, ymax = full_extent\n\n    if height and width:\n        p = figure(width=width, height=height,\n                   x_range=(xmin, xmax),\n                   y_range=(ymin, ymax),\n                   tools=\"pan,wheel_zoom,reset\", **kwargs)\n    else:\n        p = figure(sizing_mode='stretch_both',\n                   x_range=(xmin, xmax),\n                   y_range=(ymin, ymax),\n                   tools=\"pan,wheel_zoom,reset\", **kwargs)\n\n    p.background_fill_color = 'black'\n    p.grid.grid_line_alpha = 0\n    p.axis.visible = True\n\n    tile_source = WMTSTileSource(url=tileset_url,\n                                 min_zoom=min_zoom,\n                                 max_zoom=max_zoom)\n    p.add_tile(tile_source, render_parents=False)\n\n    if output_dir:\n        save(p)\n\n    return p\n\n\nclass FileSystemTileRenderer(TileRenderer):\n\n    def render(self, da, level):\n        for img, x, y, z in super().render(da, level):\n            tile_file_name = '{}.{}'.format(y, self.tile_format.lower())\n            tile_directory = os.path.join(self.output_location, str(z), str(x))\n            output_file = os.path.join(tile_directory, tile_file_name)\n            _create_dir(tile_directory)\n            img.save(output_file, self.tile_format)\n\n\nclass S3TileRenderer(TileRenderer):\n\n    def render(self, da, level):\n\n        try:\n            import boto3\n        except ImportError:\n            raise ImportError('install boto3 to enable rendering to S3')\n\n        from urllib.parse import urlparse\n\n        s3_info = urlparse(self.output_location)\n        bucket = s3_info.netloc\n        client = boto3.client('s3')\n        for img, x, y, z in super().render(da, level):\n            tile_file_name = '{}.{}'.format(y, self.tile_format.lower())\n            key = os.path.join(s3_info.path, str(z), str(x), tile_file_name).lstrip('/')\n            output_buf = BytesIO()\n            img.save(output_buf, self.tile_format)\n            output_buf.seek(0)\n            client.put_object(Body=output_buf, Bucket=bucket, Key=key, ACL='public-read')\n\n        return 'https://{}.s3.amazonaws.com/{}'.format(bucket, s3_info.path)\n",
    "datashader/core.py": "from __future__ import annotations\n\nfrom numbers import Number\nfrom math import log10\nimport warnings\nimport contextlib\n\nimport numpy as np\nimport pandas as pd\nfrom packaging.version import Version\nfrom xarray import DataArray, Dataset\n\nfrom .utils import Dispatcher, ngjit, calc_res, calc_bbox, orient_array, \\\n    dshape_from_xarray_dataset\nfrom .utils import get_indices, dshape_from_pandas, dshape_from_dask\nfrom .utils import Expr # noqa (API import)\nfrom .resampling import resample_2d, resample_2d_distributed\nfrom . import reductions as rd\n\ntry:\n    import dask.dataframe as dd\n    import dask.array as da\nexcept ImportError:\n    dd, da = None, None\n\ntry:\n    import cudf\nexcept Exception:\n    cudf = None\n\ntry:\n    import dask_cudf\nexcept Exception:\n    dask_cudf = None\n\ntry:\n    import spatialpandas\nexcept Exception:\n    spatialpandas = None\n\nclass Axis:\n    \"\"\"Interface for implementing axis transformations.\n\n    Instances hold implementations of transformations to and from axis space.\n    The default implementation is equivalent to:\n\n    >>> def forward_transform(data_x):\n    ...     scale * mapper(data_x) + t\n    >>> def inverse_transform(axis_x):\n    ...     inverse_mapper((axis_x - t)/s)\n\n    Where ``mapper`` and ``inverse_mapper`` are elementwise functions mapping\n    to and from axis-space respectively, and ``scale`` and ``transform`` are\n    parameters describing a linear scale and translate transformation, computed\n    by the ``compute_scale_and_translate`` method.\n    \"\"\"\n\n    def compute_scale_and_translate(self, range, n):\n        \"\"\"Compute the scale and translate parameters for a linear transformation\n        ``output = s * input + t``, mapping from data space to axis space.\n\n        Parameters\n        ----------\n        range : tuple\n            A tuple representing the range ``[min, max]`` along the axis, in\n            data space. Both min and max are inclusive.\n        n : int\n            The number of bins along the axis.\n\n        Returns\n        -------\n        s, t : floats\n        \"\"\"\n        start, end = map(self.mapper, range)\n        s = n/(end - start)\n        t = -start * s\n        return s, t\n\n    def compute_index(self, st, n):\n        \"\"\"Compute a 1D array representing the axis index.\n\n        Parameters\n        ----------\n        st : tuple\n            A tuple of ``(scale, translate)`` parameters.\n        n : int\n            The number of bins along the dimension.\n\n        Returns\n        -------\n        index : ndarray\n        \"\"\"\n        px = np.arange(n)+0.5\n        s, t = st\n        return self.inverse_mapper((px - t)/s)\n\n    def mapper(val):\n        \"\"\"A mapping from data space to axis space\"\"\"\n        raise NotImplementedError\n\n    def inverse_mapper(val):\n        \"\"\"A mapping from axis space to data space\"\"\"\n        raise NotImplementedError\n\n    def validate(self, range):\n        \"\"\"Given a range (low,high), raise an error if the range is invalid for this axis\"\"\"\n        pass\n\n\nclass LinearAxis(Axis):\n    \"\"\"A linear Axis\"\"\"\n    @staticmethod\n    @ngjit\n    def mapper(val):\n        return val\n\n    @staticmethod\n    @ngjit\n    def inverse_mapper(val):\n        return val\n\n\nclass LogAxis(Axis):\n    \"\"\"A base-10 logarithmic Axis\"\"\"\n    @staticmethod\n    @ngjit\n    def mapper(val):\n        return log10(float(val))\n\n    @staticmethod\n    @ngjit\n    def inverse_mapper(val):\n        y = 10 # temporary workaround for https://github.com/numba/numba/issues/3135 (numba 0.39.0)\n        return y**val\n\n    def validate(self, range):\n        if range is None:\n            # Nothing to check if no range\n            return\n        if range[0] <= 0 or range[1] <= 0:\n            raise ValueError('Range values must be >0 for logarithmic axes')\n\n\n_axis_lookup = {'linear': LinearAxis(), 'log': LogAxis()}\n\n\ndef validate_xy_or_geometry(glyph, x, y, geometry):\n    if (geometry is None and (x is None or y is None) or\n            geometry is not None and (x is not None or y is not None)):\n        raise ValueError(\"\"\"\n{glyph} coordinates may be specified by providing both the x and y arguments, or by\nproviding the geometry argument. Received:\n    x: {x}\n    y: {y}\n    geometry: {geometry}\n\"\"\".format(glyph=glyph, x=repr(x), y=repr(y), geometry=repr(geometry)))\n\n\nclass Canvas:\n    \"\"\"An abstract canvas representing the space in which to bin.\n\n    Parameters\n    ----------\n    plot_width, plot_height : int, optional\n        Width and height of the output aggregate in pixels.\n    x_range, y_range : tuple, optional\n        A tuple representing the bounds inclusive space ``[min, max]`` along\n        the axis.\n    x_axis_type, y_axis_type : str, optional\n        The type of the axis. Valid options are ``'linear'`` [default], and\n        ``'log'``.\n    \"\"\"\n    def __init__(self, plot_width=600, plot_height=600,\n                 x_range=None, y_range=None,\n                 x_axis_type='linear', y_axis_type='linear'):\n        self.plot_width = plot_width\n        self.plot_height = plot_height\n        self.x_range = None if x_range is None else tuple(x_range)\n        self.y_range = None if y_range is None else tuple(y_range)\n        self.x_axis = _axis_lookup[x_axis_type]\n        self.y_axis = _axis_lookup[y_axis_type]\n\n    def points(self, source, x=None, y=None, agg=None, geometry=None):\n        \"\"\"Compute a reduction by pixel, mapping data to pixels as points.\n\n        Parameters\n        ----------\n        source : pandas.DataFrame, dask.DataFrame, or xarray.DataArray/Dataset\n            The input datasource.\n        x, y : str\n            Column names for the x and y coordinates of each point. If provided,\n            the geometry argument may not also be provided.\n        agg : Reduction, optional\n            Reduction to compute. Default is ``count()``.\n        geometry: str\n            Column name of a PointsArray of the coordinates of each point. If provided,\n            the x and y arguments may not also be provided.\n        \"\"\"\n        from .glyphs import Point, MultiPointGeometry\n        from .reductions import count as count_rdn\n\n        validate_xy_or_geometry('Point', x, y, geometry)\n\n        if agg is None:\n            agg = count_rdn()\n\n        if geometry is None:\n            glyph = Point(x, y)\n        else:\n            if spatialpandas and isinstance(source, spatialpandas.dask.DaskGeoDataFrame):\n                # Downselect partitions to those that may contain points in viewport\n                x_range = self.x_range if self.x_range is not None else (None, None)\n                y_range = self.y_range if self.y_range is not None else (None, None)\n                source = source.cx_partitions[slice(*x_range), slice(*y_range)]\n                glyph = MultiPointGeometry(geometry)\n            elif spatialpandas and isinstance(source, spatialpandas.GeoDataFrame):\n                glyph = MultiPointGeometry(geometry)\n            elif (geopandas_source := self._source_from_geopandas(source)) is not None:\n                source = geopandas_source\n                from datashader.glyphs.points import MultiPointGeoPandas\n                glyph = MultiPointGeoPandas(geometry)\n            else:\n                raise ValueError(\n                    \"source must be an instance of spatialpandas.GeoDataFrame, \"\n                    \"spatialpandas.dask.DaskGeoDataFrame, geopandas.GeoDataFrame, or \"\n                    \"dask_geopandas.GeoDataFrame. Received objects of type {typ}\".format(\n                        typ=type(source)))\n\n        return bypixel(source, self, glyph, agg)\n\n    def line(self, source, x=None, y=None, agg=None, axis=0, geometry=None,\n             line_width=0, antialias=False):\n        \"\"\"Compute a reduction by pixel, mapping data to pixels as one or\n        more lines.\n\n        For aggregates that take in extra fields, the interpolated bins will\n        receive the fields from the previous point. In pseudocode:\n\n        >>> for i in range(len(rows) - 1):    # doctest: +SKIP\n        ...     row0 = rows[i]\n        ...     row1 = rows[i + 1]\n        ...     for xi, yi in interpolate(row0.x, row0.y, row1.x, row1.y):\n        ...         add_to_aggregate(xi, yi, row0)\n\n        Parameters\n        ----------\n        source : pandas.DataFrame, dask.DataFrame, or xarray.DataArray/Dataset\n            The input datasource.\n        x, y : str or number or list or tuple or np.ndarray\n            Specification of the x and y coordinates of each vertex\n            * str or number: Column labels in source\n            * list or tuple: List or tuple of column labels in source\n            * np.ndarray: When axis=1, a literal array of the\n              coordinates to be used for every row\n        agg : Reduction, optional\n            Reduction to compute. Default is ``any()``.\n        axis : 0 or 1, default 0\n            Axis in source to draw lines along\n            * 0: Draw lines using data from the specified columns across\n                 all rows in source\n            * 1: Draw one line per row in source using data from the\n                 specified columns\n        geometry : str\n            Column name of a LinesArray of the coordinates of each line. If provided,\n            the x and y arguments may not also be provided.\n        line_width : number, optional\n            Width of the line to draw, in pixels. If zero, the\n            default, lines are drawn using a simple algorithm with a\n            blocky single-pixel width based on whether the line passes\n            through each pixel or does not. If greater than one, lines\n            are drawn with the specified width using a slower and\n            more complex antialiasing algorithm with fractional values\n            along each edge, so that lines have a more uniform visual\n            appearance across all angles. Line widths between 0 and 1\n            effectively use a line_width of 1 pixel but with a\n            proportionate reduction in the strength of each pixel,\n            approximating the visual appearance of a subpixel line\n            width.\n        antialias : bool, optional\n            This option is kept for backward compatibility only.\n            ``True`` is equivalent to ``line_width=1`` and\n            ``False`` (the default) to ``line_width=0``. Do not specify\n            both ``antialias`` and ``line_width`` in the same call as a\n            ``ValueError`` will be raised if they disagree.\n\n        Examples\n        --------\n        Define a canvas and a pandas DataFrame with 6 rows\n        >>> import pandas as pd  # doctest: +SKIP\n        ... import numpy as np\n        ... import datashader as ds\n        ... from datashader import Canvas\n        ... import datashader.transfer_functions as tf\n        ... cvs = Canvas()\n        ... df = pd.DataFrame({\n        ...    'A1': [1, 1.5, 2, 2.5, 3, 4],\n        ...    'A2': [1.5, 2, 3, 3.2, 4, 5],\n        ...    'B1': [10, 12, 11, 14, 13, 15],\n        ...    'B2': [11, 9, 10, 7, 8, 12],\n        ... }, dtype='float64')\n\n        Aggregate one line across all rows, with coordinates df.A1 by df.B1\n        >>> agg = cvs.line(df, x='A1', y='B1', axis=0)  # doctest: +SKIP\n        ... tf.spread(tf.shade(agg))\n\n        Aggregate two lines across all rows. The first with coordinates\n        df.A1 by df.B1 and the second with coordinates df.A2 by df.B2\n        >>> agg = cvs.line(df, x=['A1', 'A2'], y=['B1', 'B2'], axis=0)  # doctest: +SKIP\n        ... tf.spread(tf.shade(agg))\n\n        Aggregate two lines across all rows where the lines share the same\n        x coordinates. The first line will have coordinates df.A1 by df.B1\n        and the second will have coordinates df.A1 by df.B2\n        >>> agg = cvs.line(df, x='A1', y=['B1', 'B2'], axis=0)  # doctest: +SKIP\n        ... tf.spread(tf.shade(agg))\n\n        Aggregate 6 length-2 lines, one per row, where the ith line has\n        coordinates [df.A1[i], df.A2[i]] by [df.B1[i], df.B2[i]]\n        >>> agg = cvs.line(df, x=['A1', 'A2'], y=['B1', 'B2'], axis=1)  # doctest: +SKIP\n        ... tf.spread(tf.shade(agg))\n\n        Aggregate 6 length-4 lines, one per row, where the x coordinates\n        of every line are [0, 1, 2, 3] and the y coordinates of the ith line\n        are [df.A1[i], df.A2[i], df.B1[i], df.B2[i]].\n        >>> agg = cvs.line(df,  # doctest: +SKIP\n        ...                x=np.arange(4),\n        ...                y=['A1', 'A2', 'B1', 'B2'],\n        ...                axis=1)\n        ... tf.spread(tf.shade(agg))\n\n        Aggregate RaggedArrays of variable length lines, one per row\n        (requires pandas >= 0.24.0)\n        >>> df_ragged = pd.DataFrame({  # doctest: +SKIP\n        ...    'A1': pd.array([\n        ...        [1, 1.5], [2, 2.5, 3], [1.5, 2, 3, 4], [3.2, 4, 5]],\n        ...        dtype='Ragged[float32]'),\n        ...    'B1': pd.array([\n        ...        [10, 12], [11, 14, 13], [10, 7, 9, 10], [7, 8, 12]],\n        ...        dtype='Ragged[float32]'),\n        ...    'group': pd.Categorical([0, 1, 2, 1])\n        ... })\n        ...\n        ... agg = cvs.line(df_ragged, x='A1', y='B1', axis=1)\n        ... tf.spread(tf.shade(agg))\n\n        Aggregate RaggedArrays of variable length lines by group column,\n        one per row (requires pandas >= 0.24.0)\n        >>> agg = cvs.line(df_ragged, x='A1', y='B1',  # doctest: +SKIP\n        ...                agg=ds.count_cat('group'), axis=1)\n        ... tf.spread(tf.shade(agg))\n        \"\"\"\n        from .glyphs import (LineAxis0, LinesAxis1, LinesAxis1XConstant,\n                             LinesAxis1YConstant, LineAxis0Multi,\n                             LinesAxis1Ragged, LineAxis1Geometry, LinesXarrayCommonX)\n\n        validate_xy_or_geometry('Line', x, y, geometry)\n\n        if agg is None:\n            agg = rd.any()\n\n        if line_width is None:\n            line_width = 0\n\n        # Check and convert antialias kwarg to line_width.\n        if antialias and line_width != 0:\n            raise ValueError(\n                \"Do not specify values for both the line_width and \\n\"\n                \"antialias keyword arguments; use line_width instead.\")\n        if antialias:\n            line_width = 1.0\n\n        if geometry is not None:\n            if spatialpandas and isinstance(source, spatialpandas.dask.DaskGeoDataFrame):\n                # Downselect partitions to those that may contain lines in viewport\n                x_range = self.x_range if self.x_range is not None else (None, None)\n                y_range = self.y_range if self.y_range is not None else (None, None)\n                source = source.cx_partitions[slice(*x_range), slice(*y_range)]\n                glyph = LineAxis1Geometry(geometry)\n            elif spatialpandas and isinstance(source, spatialpandas.GeoDataFrame):\n                glyph = LineAxis1Geometry(geometry)\n            elif (geopandas_source := self._source_from_geopandas(source)) is not None:\n                source = geopandas_source\n                from datashader.glyphs.line import LineAxis1GeoPandas\n                glyph = LineAxis1GeoPandas(geometry)\n            else:\n                raise ValueError(\n                    \"source must be an instance of spatialpandas.GeoDataFrame, \"\n                    \"spatialpandas.dask.DaskGeoDataFrame, geopandas.GeoDataFrame, or \"\n                    \"dask_geopandas.GeoDataFrame. Received objects of type {typ}\".format(\n                        typ=type(source)))\n\n        elif isinstance(source, Dataset) and isinstance(x, str) and isinstance(y, str):\n            x_arr = source[x]\n            y_arr = source[y]\n            if x_arr.ndim != 1:\n                raise ValueError(f\"x array must have 1 dimension not {x_arr.ndim}\")\n\n            if y_arr.ndim != 2:\n                raise ValueError(f\"y array must have 2 dimensions not {y_arr.ndim}\")\n            if x not in y_arr.dims:\n                raise ValueError(\"x must be one of the coordinate dimensions of y\")\n\n            y_coord_dims = list(y_arr.coords.dims)\n            x_dim_index = y_coord_dims.index(x)\n            glyph = LinesXarrayCommonX(x, y, x_dim_index)\n        else:\n            # Broadcast column specifications to handle cases where\n            # x is a list and y is a string or vice versa\n            orig_x, orig_y = x, y\n            x, y = _broadcast_column_specifications(x, y)\n\n            if axis == 0:\n                if (isinstance(x, (Number, str)) and\n                        isinstance(y, (Number, str))):\n                    glyph = LineAxis0(x, y)\n                elif (isinstance(x, (list, tuple)) and\n                        isinstance(y, (list, tuple))):\n                    glyph = LineAxis0Multi(tuple(x), tuple(y))\n                else:\n                    raise ValueError(\"\"\"\nInvalid combination of x and y arguments to Canvas.line when axis=0.\n    Received:\n        x: {x}\n        y: {y}\nSee docstring for more information on valid usage\"\"\".format(\n                        x=repr(orig_x), y=repr(orig_y)))\n\n            elif axis == 1:\n                if isinstance(x, (list, tuple)) and isinstance(y, (list, tuple)):\n                    glyph = LinesAxis1(tuple(x), tuple(y))\n                elif (isinstance(x, np.ndarray) and\n                      isinstance(y,  (list, tuple))):\n                    glyph = LinesAxis1XConstant(x, tuple(y))\n                elif (isinstance(x, (list, tuple)) and\n                      isinstance(y, np.ndarray)):\n                    glyph = LinesAxis1YConstant(tuple(x), y)\n                elif (isinstance(x, (Number, str)) and\n                        isinstance(y, (Number, str))):\n                    glyph = LinesAxis1Ragged(x, y)\n                else:\n                    raise ValueError(\"\"\"\nInvalid combination of x and y arguments to Canvas.line when axis=1.\n    Received:\n        x: {x}\n        y: {y}\nSee docstring for more information on valid usage\"\"\".format(\n                        x=repr(orig_x), y=repr(orig_y)))\n\n            else:\n                raise ValueError(\"\"\"\nThe axis argument to Canvas.line must be 0 or 1\n    Received: {axis}\"\"\".format(axis=axis))\n\n        if (line_width > 0 and ((cudf and isinstance(source, cudf.DataFrame)) or\n                               (dask_cudf and isinstance(source, dask_cudf.DataFrame)))):\n            warnings.warn(\n                \"Antialiased lines are not supported for CUDA-backed sources, \"\n                \"so reverting to line_width=0\")\n            line_width = 0\n\n        glyph.set_line_width(line_width)\n\n        if glyph.antialiased:\n            # This is required to identify and report use of reductions that do\n            # not yet support antialiasing.\n            non_cat_agg = agg\n            if isinstance(non_cat_agg, rd.by):\n                non_cat_agg = non_cat_agg.reduction\n\n            if not isinstance(non_cat_agg, (\n                rd.any, rd.count, rd.max, rd.min, rd.sum, rd.summary, rd._sum_zero,\n                rd._first_or_last, rd.mean, rd.max_n, rd.min_n, rd._first_n_or_last_n,\n                rd._max_or_min_row_index, rd._max_n_or_min_n_row_index, rd.where,\n            )):\n                raise NotImplementedError(\n                    f\"{type(non_cat_agg)} reduction not implemented for antialiased lines\")\n\n        return bypixel(source, self, glyph, agg, antialias=glyph.antialiased)\n\n    def area(self, source, x, y, agg=None, axis=0, y_stack=None):\n        \"\"\"Compute a reduction by pixel, mapping data to pixels as a filled\n        area region\n\n        Parameters\n        ----------\n        source : pandas.DataFrame, dask.DataFrame, or xarray.DataArray/Dataset\n            The input datasource.\n        x, y : str or number or list or tuple or np.ndarray\n            Specification of the x and y coordinates of each vertex of the\n            line defining the starting edge of the area region.\n            * str or number: Column labels in source\n            * list or tuple: List or tuple of column labels in source\n            * np.ndarray: When axis=1, a literal array of the\n              coordinates to be used for every row\n        agg : Reduction, optional\n            Reduction to compute. Default is ``count()``.\n        axis : 0 or 1, default 0\n            Axis in source to draw lines along\n            * 0: Draw area regions using data from the specified columns\n                 across all rows in source\n            * 1: Draw one area region per row in source using data from the\n                 specified columns\n        y_stack: str or number or list or tuple or np.ndarray or None\n            Specification of the y coordinates of each vertex of the line\n            defining the ending edge of the area region, where the x\n            coordinate is given by the x argument described above.\n\n            If y_stack is None, then the area region is filled to the y=0 line\n\n            If y_stack is not None, then the form of y_stack must match the\n            form of y.\n\n        Examples\n        --------\n        Define a canvas and a pandas DataFrame with 6 rows\n        >>> import pandas as pd  # doctest: +SKIP\n        ... import numpy as np\n        ... import datashader as ds\n        ... from datashader import Canvas\n        ... import datashader.transfer_functions as tf\n        ... cvs = Canvas()\n        ... df = pd.DataFrame({\n        ...    'A1': [1, 1.5, 2, 2.5, 3, 4],\n        ...    'A2': [1.6, 2.1, 2.9, 3.2, 4.2, 5],\n        ...    'B1': [10, 12, 11, 14, 13, 15],\n        ...    'B2': [11, 9, 10, 7, 8, 12],\n        ... }, dtype='float64')\n\n        Aggregate one area region across all rows, that starts with\n        coordinates df.A1 by df.B1 and is filled to the y=0 line\n        >>> agg = cvs.area(df, x='A1', y='B1',  # doctest: +SKIP\n        ...                agg=ds.count(), axis=0)\n        ... tf.shade(agg)\n\n        Aggregate one area region across all rows, that starts with\n        coordinates df.A1 by df.B1 and is filled to the line with coordinates\n        df.A1 by df.B2\n        >>> agg = cvs.area(df, x='A1', y='B1', y_stack='B2', # doctest: +SKIP\n        ...                agg=ds.count(), axis=0)\n        ... tf.shade(agg)\n\n        Aggregate two area regions across all rows. The first starting\n        with coordinates df.A1 by df.B1 and the second with coordinates\n        df.A2 by df.B2. Both regions are filled to the y=0 line\n        >>> agg = cvs.area(df, x=['A1', 'A2'], y=['B1', 'B2'],  # doctest: +SKIP\n                           agg=ds.count(), axis=0)\n        ... tf.shade(agg)\n\n        Aggregate two area regions across all rows where the regions share the\n        same x coordinates. The first region will start with coordinates\n        df.A1 by df.B1 and the second will start with coordinates\n        df.A1 by df.B2. Both regions are filled to the y=0 line\n        >>> agg = cvs.area(df, x='A1', y=['B1', 'B2'], agg=ds.count(), axis=0)  # doctest: +SKIP\n        ... tf.shade(agg)\n\n        Aggregate 6 length-2 area regions, one per row, where the ith region\n        starts with coordinates [df.A1[i], df.A2[i]] by [df.B1[i], df.B2[i]]\n        and is filled to the y=0 line\n        >>> agg = cvs.area(df, x=['A1', 'A2'], y=['B1', 'B2'],  # doctest: +SKIP\n                           agg=ds.count(), axis=1)\n        ... tf.shade(agg)\n\n        Aggregate 6 length-4 area regions, one per row, where the\n        starting x coordinates of every region are [0, 1, 2, 3] and\n        the starting y coordinates of the ith region are\n        [df.A1[i], df.A2[i], df.B1[i], df.B2[i]].  All regions are filled to\n        the y=0 line\n        >>> agg = cvs.area(df,  # doctest: +SKIP\n        ...                x=np.arange(4),\n        ...                y=['A1', 'A2', 'B1', 'B2'],\n        ...                agg=ds.count(),\n        ...                axis=1)\n        ... tf.shade(agg)\n\n        Aggregate RaggedArrays of variable length area regions, one per row.\n        The starting coordinates of the ith region are df_ragged.A1 by\n        df_ragged.B1 and the regions are filled to the y=0 line.\n        (requires pandas >= 0.24.0)\n        >>> df_ragged = pd.DataFrame({  # doctest: +SKIP\n        ...    'A1': pd.array([\n        ...        [1, 1.5], [2, 2.5, 3], [1.5, 2, 3, 4], [3.2, 4, 5]],\n        ...        dtype='Ragged[float32]'),\n        ...    'B1': pd.array([\n        ...        [10, 12], [11, 14, 13], [10, 7, 9, 10], [7, 8, 12]],\n        ...        dtype='Ragged[float32]'),\n        ...    'B2': pd.array([\n        ...        [6, 10], [9, 10, 18], [9, 5, 6, 8], [4, 5, 11]],\n        ...        dtype='Ragged[float32]'),\n        ...    'group': pd.Categorical([0, 1, 2, 1])\n        ... })\n        ...\n        ... agg = cvs.area(df_ragged, x='A1', y='B1', agg=ds.count(), axis=1)\n        ... tf.shade(agg)\n\n        Instead of filling regions to the y=0 line, fill to the line with\n        coordinates df_ragged.A1 by df_ragged.B2\n        >>> agg = cvs.area(df_ragged, x='A1', y='B1', y_stack='B2', # doctest: +SKIP\n        ...                agg=ds.count(), axis=1)\n        ... tf.shade(agg)\n\n        (requires pandas >= 0.24.0)\n        \"\"\"\n        from .glyphs import (\n            AreaToZeroAxis0, AreaToLineAxis0,\n            AreaToZeroAxis0Multi, AreaToLineAxis0Multi,\n            AreaToZeroAxis1, AreaToLineAxis1,\n            AreaToZeroAxis1XConstant, AreaToLineAxis1XConstant,\n            AreaToZeroAxis1YConstant, AreaToLineAxis1YConstant,\n            AreaToZeroAxis1Ragged, AreaToLineAxis1Ragged,\n        )\n        from .reductions import any as any_rdn\n        if agg is None:\n            agg = any_rdn()\n\n        # Broadcast column specifications to handle cases where\n        # x is a list and y is a string or vice versa\n        orig_x, orig_y, orig_y_stack = x, y, y_stack\n        x, y, y_stack = _broadcast_column_specifications(x, y, y_stack)\n\n        if axis == 0:\n            if y_stack is None:\n                if (isinstance(x, (Number, str)) and\n                        isinstance(y, (Number, str))):\n                    glyph = AreaToZeroAxis0(x, y)\n                elif (isinstance(x, (list, tuple)) and\n                      isinstance(y, (list, tuple))):\n                    glyph = AreaToZeroAxis0Multi(tuple(x), tuple(y))\n                else:\n                    raise ValueError(\"\"\"\nInvalid combination of x and y arguments to Canvas.area when axis=0.\n    Received:\n        x: {x}\n        y: {y}\nSee docstring for more information on valid usage\"\"\".format(\n                        x=repr(x), y=repr(y)))\n            else:\n                # y_stack is not None\n                if (isinstance(x, (Number, str)) and\n                        isinstance(y, (Number, str)) and\n                        isinstance(y_stack, (Number, str))):\n\n                    glyph = AreaToLineAxis0(x, y, y_stack)\n                elif (isinstance(x, (list, tuple)) and\n                      isinstance(y, (list, tuple)) and\n                      isinstance(y_stack, (list, tuple))):\n                    glyph = AreaToLineAxis0Multi(\n                        tuple(x), tuple(y), tuple(y_stack))\n                else:\n                    raise ValueError(\"\"\"\nInvalid combination of x, y, and y_stack arguments to Canvas.area when axis=0.\n    Received:\n        x: {x}\n        y: {y}\n        y_stack: {y_stack}\nSee docstring for more information on valid usage\"\"\".format(\n                        x=repr(orig_x),\n                        y=repr(orig_y),\n                        y_stack=repr(orig_y_stack)))\n\n        elif axis == 1:\n            if y_stack is None:\n                if (isinstance(x, (list, tuple)) and\n                        isinstance(y, (list, tuple))):\n                    glyph = AreaToZeroAxis1(tuple(x), tuple(y))\n                elif (isinstance(x, np.ndarray) and\n                      isinstance(y, (list, tuple))):\n                    glyph = AreaToZeroAxis1XConstant(x, tuple(y))\n                elif (isinstance(x, (list, tuple)) and\n                      isinstance(y, np.ndarray)):\n                    glyph = AreaToZeroAxis1YConstant(tuple(x), y)\n                elif (isinstance(x, (Number, str)) and\n                      isinstance(y, (Number, str))):\n                    glyph = AreaToZeroAxis1Ragged(x, y)\n                else:\n                    raise ValueError(\"\"\"\nInvalid combination of x and y arguments to Canvas.area when axis=1.\n    Received:\n        x: {x}\n        y: {y}\nSee docstring for more information on valid usage\"\"\".format(\n                        x=repr(x), y=repr(y)))\n            else:\n                if (isinstance(x, (list, tuple)) and\n                        isinstance(y, (list, tuple)) and\n                        isinstance(y_stack, (list, tuple))):\n                    glyph = AreaToLineAxis1(\n                        tuple(x), tuple(y), tuple(y_stack))\n                elif (isinstance(x, np.ndarray) and\n                      isinstance(y, (list, tuple)) and\n                      isinstance(y_stack, (list, tuple))):\n                    glyph = AreaToLineAxis1XConstant(\n                        x, tuple(y), tuple(y_stack))\n                elif (isinstance(x, (list, tuple)) and\n                      isinstance(y, np.ndarray) and\n                      isinstance(y_stack, np.ndarray)):\n                    glyph = AreaToLineAxis1YConstant(tuple(x), y, y_stack)\n                elif (isinstance(x, (Number, str)) and\n                      isinstance(y, (Number, str)) and\n                      isinstance(y_stack, (Number, str))):\n                    glyph = AreaToLineAxis1Ragged(x, y, y_stack)\n                else:\n                    raise ValueError(\"\"\"\nInvalid combination of x, y, and y_stack arguments to Canvas.area when axis=1.\n    Received:\n        x: {x}\n        y: {y}\n        y_stack: {y_stack}\nSee docstring for more information on valid usage\"\"\".format(\n                        x=repr(orig_x),\n                        y=repr(orig_y),\n                        y_stack=repr(orig_y_stack)))\n        else:\n            raise ValueError(\"\"\"\nThe axis argument to Canvas.area must be 0 or 1\n    Received: {axis}\"\"\".format(axis=axis))\n\n        return bypixel(source, self, glyph, agg)\n\n    def polygons(self, source, geometry, agg=None):\n        \"\"\"Compute a reduction by pixel, mapping data to pixels as one or\n        more filled polygons.\n\n        Parameters\n        ----------\n        source : xarray.DataArray or Dataset\n            The input datasource.\n        geometry : str\n            Column name of a PolygonsArray of the coordinates of each line.\n        agg : Reduction, optional\n            Reduction to compute. Default is ``any()``.\n\n        Returns\n        -------\n        data : xarray.DataArray\n\n        Examples\n        --------\n        >>> import datashader as ds  # doctest: +SKIP\n        ... import datashader.transfer_functions as tf\n        ... from spatialpandas.geometry import PolygonArray\n        ... from spatialpandas import GeoDataFrame\n        ... import pandas as pd\n        ...\n        ... polygons = PolygonArray([\n        ...     # First Element\n        ...     [[0, 0, 1, 0, 2, 2, -1, 4, 0, 0],  # Filled quadrilateral (CCW order)\n        ...      [0.5, 1,  1, 2,  1.5, 1.5,  0.5, 1],     # Triangular hole (CW order)\n        ...      [0, 2, 0, 2.5, 0.5, 2.5, 0.5, 2, 0, 2],  # Rectangular hole (CW order)\n        ...      [2.5, 3, 3.5, 3, 3.5, 4, 2.5, 3],  # Filled triangle\n        ...     ],\n        ...\n        ...     # Second Element\n        ...     [[3, 0, 3, 2, 4, 2, 4, 0, 3, 0],  # Filled rectangle (CCW order)\n        ...      # Rectangular hole (CW order)\n        ...      [3.25, 0.25, 3.75, 0.25, 3.75, 1.75, 3.25, 1.75, 3.25, 0.25],\n        ...     ]\n        ... ])\n        ...\n        ... df = GeoDataFrame({'polygons': polygons, 'v': range(len(polygons))})\n        ...\n        ... cvs = ds.Canvas()\n        ... agg = cvs.polygons(df, geometry='polygons', agg=ds.sum('v'))\n        ... tf.shade(agg)\n        \"\"\"\n        from .glyphs import PolygonGeom\n        from .reductions import any as any_rdn\n\n        if spatialpandas and isinstance(source, spatialpandas.dask.DaskGeoDataFrame):\n            # Downselect partitions to those that may contain polygons in viewport\n            x_range = self.x_range if self.x_range is not None else (None, None)\n            y_range = self.y_range if self.y_range is not None else (None, None)\n            source = source.cx_partitions[slice(*x_range), slice(*y_range)]\n            glyph = PolygonGeom(geometry)\n        elif spatialpandas and isinstance(source, spatialpandas.GeoDataFrame):\n            glyph = PolygonGeom(geometry)\n        elif (geopandas_source := self._source_from_geopandas(source)) is not None:\n            source = geopandas_source\n            from .glyphs.polygon import GeopandasPolygonGeom\n            glyph = GeopandasPolygonGeom(geometry)\n        else:\n            raise ValueError(\n                \"source must be an instance of spatialpandas.GeoDataFrame, \"\n                \"spatialpandas.dask.DaskGeoDataFrame, geopandas.GeoDataFrame or \"\n                f\"dask_geopandas.GeoDataFrame, not {type(source)}\")\n\n        if agg is None:\n            agg = any_rdn()\n        return bypixel(source, self, glyph, agg)\n\n    def quadmesh(self, source, x=None, y=None, agg=None):\n        \"\"\"Samples a recti- or curvi-linear quadmesh by canvas size and bounds.\n\n        Parameters\n        ----------\n        source : xarray.DataArray or Dataset\n            The input datasource.\n        x, y : str\n            Column names for the x and y coordinates of each point.\n        agg : Reduction, optional\n            Reduction to compute. Default is ``mean()``. Note that agg is ignored when\n            upsampling.\n\n        Returns\n        -------\n        data : xarray.DataArray\n        \"\"\"\n        from .glyphs import QuadMeshRaster, QuadMeshRectilinear, QuadMeshCurvilinear\n\n        # Determine reduction operation\n        from .reductions import mean as mean_rnd\n\n        if isinstance(source, Dataset):\n            if agg is None or agg.column is None:\n                name = list(source.data_vars)[0]\n            else:\n                name = agg.column\n            # Keep as dataset so that source[agg.column] works\n            source = source[[name]]\n        elif isinstance(source, DataArray):\n            # Make dataset so that source[agg.column] works\n            name = source.name\n            source = source.to_dataset()\n        else:\n            raise ValueError(\"Invalid input type\")\n\n        if agg is None:\n            agg = mean_rnd(name)\n\n        if x is None and y is None:\n            y, x = source[name].dims\n        elif not x or not y:\n            raise ValueError(\"Either specify both x and y coordinates\"\n                             \"or allow them to be inferred.\")\n        yarr, xarr = source[y], source[x]\n\n        if (yarr.ndim > 1 or xarr.ndim > 1) and xarr.dims != yarr.dims:\n            raise ValueError(\"Ensure that x- and y-coordinate arrays \"\n                             \"share the same dimensions. x-coordinates \"\n                             \"are indexed by %s dims while \"\n                             \"y-coordinates are indexed by %s dims.\" %\n                             (xarr.dims, yarr.dims))\n\n        if (name is not None\n                and agg.column is not None\n                and agg.column != name):\n            raise ValueError('DataArray name %r does not match '\n                             'supplied reduction %s.' %\n                             (source.name, agg))\n\n        if xarr.ndim == 1:\n            xaxis_linear = self.x_axis is _axis_lookup[\"linear\"]\n            yaxis_linear = self.y_axis is _axis_lookup[\"linear\"]\n            even_yspacing = np.allclose(\n                yarr, np.linspace(yarr[0].data, yarr[-1].data, len(yarr))\n            )\n            even_xspacing = np.allclose(\n                xarr, np.linspace(xarr[0].data, xarr[-1].data, len(xarr))\n            )\n\n            if xaxis_linear and yaxis_linear and even_xspacing and even_yspacing:\n                # Source is a raster, where all x and y coordinates are evenly spaced\n                glyph = QuadMeshRaster(x, y, name)\n                upsample_width, upsample_height = glyph.is_upsample(\n                        source, x, y, name, self.x_range, self.y_range,\n                        self.plot_width, self.plot_height\n                )\n                if upsample_width and upsample_height:\n                    # Override aggregate with more efficient one for upsampling\n                    agg = rd._upsample(name)\n                    return bypixel(source, self, glyph, agg)\n                elif not upsample_width and not upsample_height:\n                    # Downsample both width and height\n                    return bypixel(source, self, glyph, agg)\n                else:\n                    # Mix of upsampling and downsampling\n                    # Use general rectilinear quadmesh implementation\n                    glyph = QuadMeshRectilinear(x, y, name)\n                    return bypixel(source, self, glyph, agg)\n            else:\n                # Source is a general rectilinear quadmesh\n                glyph = QuadMeshRectilinear(x, y, name)\n                return bypixel(source, self, glyph, agg)\n        elif xarr.ndim == 2:\n            glyph = QuadMeshCurvilinear(x, y, name)\n            return bypixel(source, self, glyph, agg)\n        else:\n            raise ValueError(\"\"\"\\\nx- and y-coordinate arrays must have 1 or 2 dimensions.\n    Received arrays with dimensions: {dims}\"\"\".format(\n                dims=list(xarr.dims)))\n\n    # TODO re 'untested', below: Consider replacing with e.g. a 3x3\n    # array in the call to Canvas (plot_height=3,plot_width=3), then\n    # show the output as a numpy array that has a compact\n    # representation\n    def trimesh(self, vertices, simplices, mesh=None, agg=None, interp=True, interpolate=None):\n        \"\"\"Compute a reduction by pixel, mapping data to pixels as a triangle.\n\n        >>> import datashader as ds\n        >>> verts = pd.DataFrame({'x': [0, 5, 10],\n        ...                         'y': [0, 10, 0],\n        ...                         'weight': [1, 5, 3]},\n        ...                        columns=['x', 'y', 'weight'])\n        >>> tris = pd.DataFrame({'v0': [2], 'v1': [0], 'v2': [1]},\n        ...                       columns=['v0', 'v1', 'v2'])\n        >>> cvs = ds.Canvas(x_range=(verts.x.min(), verts.x.max()),\n        ...                 y_range=(verts.y.min(), verts.y.max()))\n        >>> untested = cvs.trimesh(verts, tris)\n\n        Parameters\n        ----------\n        vertices : pandas.DataFrame, dask.DataFrame\n            The input datasource for triangle vertex coordinates. These can be\n            interpreted as the x/y coordinates of the vertices, with optional\n            weights for value interpolation. Columns should be ordered\n            corresponding to 'x', 'y', followed by zero or more (optional)\n            columns containing vertex values. The rows need not be ordered.\n            The column data types must be floating point or integer.\n        simplices : pandas.DataFrame, dask.DataFrame\n            The input datasource for triangle (simplex) definitions. These can\n            be interpreted as rows of ``vertices``, aka positions in the\n            ``vertices`` index. Columns should be ordered corresponding to\n            'vertex0', 'vertex1', and 'vertex2'. Order of the vertices can be\n            clockwise or counter-clockwise; it does not matter as long as the\n            data is consistent for all simplices in the dataframe. The\n            rows need not be ordered.  The data type for the first\n            three columns in the dataframe must be integer.\n        agg : Reduction, optional\n            Reduction to compute. Default is ``mean()``.\n        mesh : pandas.DataFrame, optional\n            An ordered triangle mesh in tabular form, used for optimization\n            purposes. This dataframe is expected to have come from\n            ``datashader.utils.mesh()``. If this argument is not None, the first\n            two arguments are ignored.\n        interpolate : str, optional default=linear\n            Method to use for interpolation between specified values. ``nearest``\n            means to use a single value for the whole triangle, and ``linear``\n            means to do bilinear interpolation of the pixels within each\n            triangle (a weighted average of the vertex values). For\n            backwards compatibility, also accepts ``interp=True`` for ``linear``\n            and ``interp=False`` for ``nearest``.\n        \"\"\"\n        from .glyphs import Triangles\n        from .reductions import mean as mean_rdn\n        from .utils import mesh as create_mesh\n\n        source = mesh\n\n        # 'interp' argument is deprecated as of datashader=0.6.4\n        if interpolate is not None:\n            if interpolate == 'linear':\n                interp = True\n            elif interpolate == 'nearest':\n                interp = False\n            else:\n                raise ValueError('Invalid interpolate method: options include {}'.format(\n                    ['linear','nearest']))\n\n        # Validation is done inside the [pd]d_mesh utility functions\n        if source is None:\n            source = create_mesh(vertices, simplices)\n\n        verts_have_weights = len(vertices.columns) > 2\n        if verts_have_weights:\n            weight_col = vertices.columns[2]\n        else:\n            weight_col = simplices.columns[3]\n\n        if agg is None:\n            agg = mean_rdn(weight_col)\n        elif agg.column is None:\n            agg.column = weight_col\n\n        cols = source.columns\n        x, y, weights = cols[0], cols[1], cols[2:]\n\n        return bypixel(source, self, Triangles(x, y, weights, weight_type=verts_have_weights,\n                                               interp=interp), agg)\n\n    def raster(self,\n               source,\n               layer=None,\n               upsample_method='linear',    # Deprecated as of datashader=0.6.4\n               downsample_method=rd.mean(), # Deprecated as of datashader=0.6.4\n               nan_value=None,\n               agg=None,\n               interpolate=None,\n               chunksize=None,\n               max_mem=None):\n        \"\"\"Sample a raster dataset by canvas size and bounds.\n\n        Handles 2D or 3D xarray DataArrays, assuming that the last two\n        array dimensions are the y- and x-axis that are to be\n        resampled. If a 3D array is supplied a layer may be specified\n        to resample to select the layer along the first dimension to\n        resample.\n\n        Missing values (those having the value indicated by the\n        \"nodata\" attribute of the raster) are replaced with `NaN` if\n        floats, and 0 if int.\n\n        Also supports resampling out-of-core DataArrays backed by dask\n        Arrays. By default it will try to maintain the same chunksize\n        in the output array but a custom chunksize may be provided.\n        If there are memory constraints they may be defined using the\n        max_mem parameter, which determines how large the chunks in\n        memory may be.\n\n        Parameters\n        ----------\n        source : xarray.DataArray or xr.Dataset\n            2D or 3D labelled array (if Dataset, the agg reduction must\n            define the data variable).\n        layer : float\n            For a 3D array, value along the z dimension : optional default=None\n        ds_method : str (optional)\n            Grid cell aggregation method for a possible downsampling.\n        us_method : str (optional)\n            Grid cell interpolation method for a possible upsampling.\n        nan_value : int or float, optional\n            Optional nan_value which will be masked out when applying\n            the resampling.\n        agg : Reduction, optional default=mean()\n            Resampling mode when downsampling raster. The supported\n            options include: first, last, mean, mode, var, std, min,\n            The agg can be specified as either a string name or as a\n            reduction function, but note that the function object will\n            be used only to extract the agg type (mean, max, etc.) and\n            the optional column name; the hardcoded raster code\n            supports only a fixed set of reductions and ignores the\n            actual code of the provided agg.\n        interpolate : str, optional  default=linear\n            Resampling mode when upsampling raster.\n            options include: nearest, linear.\n        chunksize : tuple(int, int) (optional)\n            Size of the output chunks. By default this the chunk size is\n            inherited from the *src* array.\n        max_mem : int (optional)\n            The maximum number of bytes that should be loaded into memory\n            during the regridding operation.\n\n        Returns\n        -------\n        data : xarray.Dataset\n        \"\"\"\n        # For backwards compatibility\n        if agg         is None:\n            agg=downsample_method\n        if interpolate is None:\n            interpolate=upsample_method\n\n        upsample_methods = ['nearest','linear']\n\n        downsample_methods = {'first':'first', rd.first:'first',\n                              'last':'last',   rd.last:'last',\n                              'mode':'mode',   rd.mode:'mode',\n                              'mean':'mean',   rd.mean:'mean',\n                              'var':'var',     rd.var:'var',\n                              'std':'std',     rd.std:'std',\n                              'min':'min',     rd.min:'min',\n                              'max':'max',     rd.max:'max'}\n\n        if interpolate not in upsample_methods:\n            raise ValueError('Invalid interpolate method: options include {}'.format(\n                upsample_methods))\n\n        if not isinstance(source, (DataArray, Dataset)):\n            raise ValueError('Expected xarray DataArray or Dataset as '\n                             'the data source, found %s.'\n                             % type(source).__name__)\n\n        column = None\n        if isinstance(agg, rd.Reduction):\n            agg, column = type(agg), agg.column\n            if (isinstance(source, DataArray) and column is not None\n                and source.name != column):\n                agg_repr = '%s(%r)' % (agg.__name__, column)\n                raise ValueError('DataArray name %r does not match '\n                                 'supplied reduction %s.' %\n                                 (source.name, agg_repr))\n\n        if isinstance(source, Dataset):\n            data_vars = list(source.data_vars)\n            if column is None:\n                raise ValueError('When supplying a Dataset the agg reduction '\n                                 'must specify the variable to aggregate. '\n                                 'Available data_vars include: %r.' % data_vars)\n            elif column not in source.data_vars:\n                raise KeyError('Supplied reduction column %r not found '\n                               'in Dataset, expected one of the following '\n                               'data variables: %r.' % (column, data_vars))\n            source = source[column]\n\n        if agg not in downsample_methods.keys():\n            raise ValueError('Invalid aggregation method: options include {}'.format(\n                list(downsample_methods.keys())))\n        ds_method = downsample_methods[agg]\n\n        if source.ndim not in [2, 3]:\n            raise ValueError('Raster aggregation expects a 2D or 3D '\n                             'DataArray, found %s dimensions' % source.ndim)\n\n        res = calc_res(source)\n        ydim, xdim = source.dims[-2:]\n        xvals, yvals = source[xdim].values, source[ydim].values\n        left, bottom, right, top = calc_bbox(xvals, yvals, res)\n        if layer is not None:\n            source=source.sel(**{source.dims[0]: layer})\n        array = orient_array(source, res)\n\n        if nan_value is not None:\n            mask = array==nan_value\n            array = np.ma.masked_array(array, mask=mask, fill_value=nan_value)\n            fill_value = nan_value\n        elif np.issubdtype(source.dtype, np.integer):\n            fill_value = 0\n        else:\n            fill_value = np.nan\n\n        if self.x_range is None:\n            self.x_range = (left,right)\n        if self.y_range is None:\n            self.y_range = (bottom,top)\n\n        # window coordinates\n        xmin = max(self.x_range[0], left)\n        ymin = max(self.y_range[0], bottom)\n        xmax = min(self.x_range[1], right)\n        ymax = min(self.y_range[1], top)\n\n        width_ratio = min((xmax - xmin) / (self.x_range[1] - self.x_range[0]), 1)\n        height_ratio = min((ymax - ymin) / (self.y_range[1] - self.y_range[0]), 1)\n\n        if np.isclose(width_ratio, 0) or np.isclose(height_ratio, 0):\n            raise ValueError('Canvas x_range or y_range values do not match closely enough '\n                             'with the data source to be able to accurately rasterize. '\n                             'Please provide ranges that are more accurate.')\n\n        w = max(int(round(self.plot_width * width_ratio)), 1)\n        h = max(int(round(self.plot_height * height_ratio)), 1)\n        cmin, cmax = get_indices(xmin, xmax, xvals, res[0])\n        rmin, rmax = get_indices(ymin, ymax, yvals, res[1])\n\n        kwargs = dict(w=w, h=h, ds_method=ds_method,\n                      us_method=interpolate, fill_value=fill_value)\n        if array.ndim == 2:\n            source_window = array[rmin:rmax+1, cmin:cmax+1]\n            if ds_method in ['var', 'std']:\n                source_window = source_window.astype('f')\n            if da and isinstance(source_window, da.Array):\n                data = resample_2d_distributed(\n                    source_window, chunksize=chunksize, max_mem=max_mem,\n                    **kwargs)\n            else:\n                data = resample_2d(source_window, **kwargs)\n            layers = 1\n        else:\n            source_window = array[:, rmin:rmax+1, cmin:cmax+1]\n            if ds_method in ['var', 'std']:\n                source_window = source_window.astype('f')\n            arrays = []\n            for arr in source_window:\n                if da and isinstance(arr, da.Array):\n                    arr = resample_2d_distributed(\n                        arr, chunksize=chunksize, max_mem=max_mem,\n                        **kwargs)\n                else:\n                    arr = resample_2d(arr, **kwargs)\n                arrays.append(arr)\n            data = np.dstack(arrays)\n            layers = len(arrays)\n\n        if w != self.plot_width or h != self.plot_height:\n            num_height = self.plot_height - h\n            num_width = self.plot_width - w\n\n            lpad = xmin - self.x_range[0]\n            rpad = self.x_range[1] - xmax\n            lpct = lpad / (lpad + rpad) if lpad + rpad > 0 else 0\n            left = max(int(np.ceil(num_width * lpct)), 0)\n            right = max(num_width - left, 0)\n            lshape, rshape = (self.plot_height, left), (self.plot_height, right)\n            if layers > 1:\n                lshape, rshape = lshape + (layers,), rshape + (layers,)\n            left_pad = np.full(lshape, fill_value, source_window.dtype)\n            right_pad = np.full(rshape, fill_value, source_window.dtype)\n\n            tpad = ymin - self.y_range[0]\n            bpad = self.y_range[1] - ymax\n            tpct = tpad / (tpad + bpad) if tpad + bpad > 0 else 0\n            top = max(int(np.ceil(num_height * tpct)), 0)\n            bottom = max(num_height - top, 0)\n            tshape, bshape = (top, w), (bottom, w)\n            if layers > 1:\n                tshape, bshape = tshape + (layers,), bshape + (layers,)\n            top_pad = np.full(tshape, fill_value, source_window.dtype)\n            bottom_pad = np.full(bshape, fill_value, source_window.dtype)\n\n            concat = da.concatenate if da and isinstance(data, da.Array) else np.concatenate\n            arrays = (top_pad, data) if top_pad.shape[0] > 0 else (data,)\n            if bottom_pad.shape[0] > 0:\n                arrays += (bottom_pad,)\n            data = concat(arrays, axis=0) if len(arrays) > 1 else arrays[0]\n            arrays = (left_pad, data) if left_pad.shape[1] > 0 else (data,)\n            if right_pad.shape[1] > 0:\n                arrays += (right_pad,)\n            data = concat(arrays, axis=1) if len(arrays) > 1 else arrays[0]\n\n        # Reorient array to original orientation\n        if res[1] > 0:\n            data = data[::-1]\n        if res[0] < 0:\n            data = data[:, ::-1]\n\n        # Compute DataArray metadata\n\n        # To avoid floating point representation error,\n        # do not recompute x coords if same x_range and same plot_width,\n        # do not recompute y coords if same y_range and same plot_height\n        close_x = np.allclose([left, right], self.x_range) and np.size(xvals) == self.plot_width\n        close_y = np.allclose([bottom, top], self.y_range) and np.size(yvals) == self.plot_height\n\n        if close_x:\n            xs = xvals\n        else:\n            x_st = self.x_axis.compute_scale_and_translate(self.x_range, self.plot_width)\n            xs = self.x_axis.compute_index(x_st, self.plot_width)\n            if res[0] < 0:\n                xs = xs[::-1]\n\n        if close_y:\n            ys = yvals\n        else:\n            y_st = self.y_axis.compute_scale_and_translate(self.y_range, self.plot_height)\n            ys = self.y_axis.compute_index(y_st, self.plot_height)\n            if res[1] > 0:\n                ys = ys[::-1]\n\n        coords = {xdim: xs, ydim: ys}\n        dims = [ydim, xdim]\n        attrs = dict(res=res[0], x_range=self.x_range, y_range=self.y_range)\n\n        # Find nodata value if available in any of the common conventional locations\n        # See https://corteva.github.io/rioxarray/stable/getting_started/nodata_management.html\n        # and https://github.com/holoviz/datashader/issues/990\n        for a in ['_FillValue', 'missing_value', 'fill_value', 'nodata', 'NODATA']:\n            if a in source.attrs:\n                attrs['nodata'] = source.attrs[a]\n                break\n        if 'nodata' not in attrs:\n            try:\n                attrs['nodata'] = source.attrs['nodatavals'][0]\n            except Exception:\n                pass\n\n        # Handle DataArray with layers\n        if data.ndim == 3:\n            data = data.transpose([2, 0, 1])\n            layer_dim = source.dims[0]\n            coords[layer_dim] = source.coords[layer_dim]\n            dims = [layer_dim]+dims\n        return DataArray(data, coords=coords, dims=dims, attrs=attrs)\n\n    def validate_ranges(self, x_range, y_range):\n        self.x_axis.validate(x_range)\n        self.y_axis.validate(y_range)\n\n    def validate_size(self, width, height):\n        if width <= 0 or height <= 0:\n            raise ValueError(\"Invalid size: plot_width and plot_height must be bigger than 0\")\n\n    def validate(self):\n        \"\"\"Check that parameter settings are valid for this object\"\"\"\n        self.validate_ranges(self.x_range, self.y_range)\n        self.validate_size(self.plot_width, self.plot_height)\n\n    def _source_from_geopandas(self, source):\n        \"\"\"\n        Check if the specified source is a geopandas or dask-geopandas GeoDataFrame.\n        If so, spatially filter the source and return it.\n        If not, return None.\n        \"\"\"\n        dfs = []\n        with contextlib.suppress(ImportError):\n            import geopandas\n            dfs.append(geopandas.GeoDataFrame)\n\n        with contextlib.suppress(ImportError):\n            import dask_geopandas\n            if Version(dask_geopandas.__version__) >= Version(\"0.4.0\"):\n                from dask_geopandas.core import GeoDataFrame as gdf1\n                dfs.append(gdf1)\n\n                # See https://github.com/geopandas/dask-geopandas/issues/311\n                with contextlib.suppress(TypeError):\n                    from dask_geopandas.expr import GeoDataFrame as gdf2\n                    dfs.append(gdf2)\n            else:\n                dfs.append(dask_geopandas.GeoDataFrame)\n\n        if isinstance(source, tuple(dfs)):\n            # Explicit shapely version check as cannot continue unless shapely >= 2\n            from shapely import __version__ as shapely_version\n            if Version(shapely_version) < Version('2.0.0'):\n                raise ImportError(\"Use of GeoPandas in Datashader requires Shapely >= 2.0.0\")\n\n            if isinstance(source, geopandas.GeoDataFrame):\n                x_range = self.x_range if self.x_range is not None else (-np.inf, np.inf)\n                y_range = self.y_range if self.y_range is not None else (-np.inf, np.inf)\n                from shapely import box\n                query = source.sindex.query(box(x_range[0], y_range[0], x_range[1], y_range[1]))\n                source = source.iloc[query]\n            else:\n                x_range = self.x_range if self.x_range is not None else (None, None)\n                y_range = self.y_range if self.y_range is not None else (None, None)\n                source = source.cx[slice(*x_range), slice(*y_range)]\n            return source\n        else:\n            return None\n\n\ndef bypixel(source, canvas, glyph, agg, *, antialias=False):\n    \"\"\"Compute an aggregate grouped by pixel sized bins.\n\n    Aggregate input data ``source`` into a grid with shape and axis matching\n    ``canvas``, mapping data to bins by ``glyph``, and aggregating by reduction\n    ``agg``.\n\n    Parameters\n    ----------\n    source : pandas.DataFrame, dask.DataFrame\n        Input datasource\n    canvas : Canvas\n    glyph : Glyph\n    agg : Reduction\n    \"\"\"\n    source, dshape = _bypixel_sanitise(source, glyph, agg)\n\n    schema = dshape.measure\n    glyph.validate(schema)\n    agg.validate(schema)\n    canvas.validate()\n\n    # All-NaN objects (e.g. chunks of arrays with no data) are valid in Datashader\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n        return bypixel.pipeline(source, schema, canvas, glyph, agg, antialias=antialias)\n\n\ndef _bypixel_sanitise(source, glyph, agg):\n    # Convert 1D xarray DataArrays and DataSets into Dask DataFrames\n    if isinstance(source, DataArray) and source.ndim == 1:\n        if not source.name:\n            source.name = 'value'\n        source = source.reset_coords()\n    if isinstance(source, Dataset) and len(source.dims) == 1:\n        columns = list(source.coords.keys()) + list(source.data_vars.keys())\n        cols_to_keep = _cols_to_keep(columns, glyph, agg)\n        source = source.drop_vars([col for col in columns if col not in cols_to_keep])\n        if dd:\n            source = source.to_dask_dataframe()\n        else:\n            source = source.to_dataframe()\n\n    if (isinstance(source, pd.DataFrame) or\n            (cudf and isinstance(source, cudf.DataFrame))):\n        # Avoid datashape.Categorical instantiation bottleneck\n        # by only retaining the necessary columns:\n        # https://github.com/bokeh/datashader/issues/396\n        # Preserve column ordering without duplicates\n\n        cols_to_keep = _cols_to_keep(source.columns, glyph, agg)\n        if len(cols_to_keep) < len(source.columns):\n            # If _sindex is set, ensure it is not dropped\n            # https://github.com/holoviz/datashader/issues/1121\n            sindex = None\n            from .glyphs.polygon import PolygonGeom\n            if isinstance(glyph, PolygonGeom):\n                sindex = getattr(source[glyph.geometry].array, \"_sindex\", None)\n            source = source[cols_to_keep]\n            if (sindex is not None and\n                    getattr(source[glyph.geometry].array, \"_sindex\", None) is None):\n                source[glyph.geometry].array._sindex = sindex\n        dshape = dshape_from_pandas(source)\n    elif dd and isinstance(source, dd.DataFrame):\n        dshape, source = dshape_from_dask(source)\n    elif isinstance(source, Dataset):\n        # Multi-dimensional Dataset\n        dshape = dshape_from_xarray_dataset(source)\n    else:\n        raise ValueError(\"source must be a pandas or dask DataFrame\")\n\n    return source, dshape\n\n\ndef _cols_to_keep(columns, glyph, agg):\n    \"\"\"\n    Return which columns from the supplied data source are kept as they are\n    needed by the specified agg. Excludes any SpecialColumn.\n    \"\"\"\n    cols_to_keep = dict({col: False for col in columns})\n    for col in glyph.required_columns():\n        cols_to_keep[col] = True\n\n    def recurse(cols_to_keep, agg):\n        if hasattr(agg, 'values'):\n            for subagg in agg.values:\n                recurse(cols_to_keep, subagg)\n        elif hasattr(agg, 'columns'):\n            for column in agg.columns:\n                if column not in (None, rd.SpecialColumn.RowIndex):\n                    cols_to_keep[column] = True\n        elif agg.column not in (None, rd.SpecialColumn.RowIndex):\n            cols_to_keep[agg.column] = True\n\n    recurse(cols_to_keep, agg)\n\n    return [col for col, keepit in cols_to_keep.items() if keepit]\n\n\ndef _broadcast_column_specifications(*args):\n    lengths = {len(a) for a in args if isinstance(a, (list, tuple))}\n    if len(lengths) != 1:\n        # None of the inputs are lists/tuples, return them as is\n        return args\n    else:\n        n = lengths.pop()\n        return tuple(\n            (arg,) * n if isinstance(arg, (Number, str)) else arg\n            for arg in args\n        )\n\n\nbypixel.pipeline = Dispatcher()\n"
  },
  "GT_src_dict": {
    "datashader/tiles.py": {
      "_get_super_tile_min_max": {
        "code": "def _get_super_tile_min_max(tile_info, load_data_func, rasterize_func):\n    \"\"\"Calculates the aggregated data for a given super tile by loading relevant data and applying a rasterization function.\n\nParameters\n----------\ntile_info : dict\n    A dictionary containing the range of x and y coordinates (`'x_range'`, `'y_range'`) and the size of the tile (`'tile_size'`).\nload_data_func : callable\n    A function that loads the data based on the provided x and y ranges.\nrasterize_func : callable\n    A function that rasterizes the loaded data into an aggregated format with specified dimensions.\n\nReturns\n-------\nnumpy.ndarray\n    The aggregated data for the specified super tile.\n\nNotes\n-----\nThis function utilizes the `load_data_func` to fetch necessary data within the specified x and y ranges and then employs the `rasterize_func` to convert that data into a raster format, defined by `tile_size`. It plays a crucial role in generating statistics for the super tiles in the `calculate_zoom_level_stats` function.\"\"\"\n    tile_size = tile_info['tile_size']\n    df = load_data_func(tile_info['x_range'], tile_info['y_range'])\n    agg = rasterize_func(df, x_range=tile_info['x_range'], y_range=tile_info['y_range'], height=tile_size, width=tile_size)\n    return agg",
        "docstring": "Calculates the aggregated data for a given super tile by loading relevant data and applying a rasterization function.\n\nParameters\n----------\ntile_info : dict\n    A dictionary containing the range of x and y coordinates (`'x_range'`, `'y_range'`) and the size of the tile (`'tile_size'`).\nload_data_func : callable\n    A function that loads the data based on the provided x and y ranges.\nrasterize_func : callable\n    A function that rasterizes the loaded data into an aggregated format with specified dimensions.\n\nReturns\n-------\nnumpy.ndarray\n    The aggregated data for the specified super tile.\n\nNotes\n-----\nThis function utilizes the `load_data_func` to fetch necessary data within the specified x and y ranges and then employs the `rasterize_func` to convert that data into a raster format, defined by `tile_size`. It plays a crucial role in generating statistics for the super tiles in the `calculate_zoom_level_stats` function.",
        "signature": "def _get_super_tile_min_max(tile_info, load_data_func, rasterize_func):",
        "type": "Function",
        "class_signature": null
      },
      "MercatorTileDefinition.__init__": {
        "code": "    def __init__(self, x_range, y_range, tile_size=256, min_zoom=0, max_zoom=30, x_origin_offset=20037508.34, y_origin_offset=20037508.34, initial_resolution=156543.03392804097):\n        \"\"\"Initialization for the MercatorTileDefinition class, which defines parameters necessary for managing Mercator tile sources used by tile rendering systems.\n\nParameters\n----------\nx_range : tuple\n    The spatial extent of the data in the x-dimension, defining the bounds for tile generation.\n    \ny_range : tuple\n    The spatial extent of the data in the y-dimension, similar to x_range.\n    \ntile_size : int, optional\n    The size of each tile in pixels, defaulting to 256 pixels. This dictates the resolution of generated tiles.\n    \nmin_zoom : int, optional\n    The minimum zoom level for the tile layer, defaulting to 0. It determines the most zoomed-out view.\n    \nmax_zoom : int, optional\n    The maximum zoom level for the tile layer, defaulting to 30. It sets the most zoomed-in detail available.\n\nx_origin_offset : float, optional\n    An x-coordinate origin offset in plot coordinates, defaulting to 20037508.34, common for Web Mercator projections.\n\ny_origin_offset : float, optional\n    A y-coordinate origin offset in plot coordinates, defaulting to 20037508.34, corresponding to the same projection conventions.\n    \ninitial_resolution : float, optional\n    The initial resolution (in plot units per pixel) at the minimum zoom level. Default is defined for standard Web Mercator.\n\nAttributes\n----------\nself._resolutions : list\n    Computed list of resolutions for each zoom level between min_zoom and max_zoom, which will be used when determining tile resolutions for rendering tasks.\n\nThis method prepares the instance for use in rendering tiles by establishing the necessary parameters and calculated attributes to facilitate map tiling based on the Mercator projection.\"\"\"\n        self.x_range = x_range\n        self.y_range = y_range\n        self.tile_size = tile_size\n        self.min_zoom = min_zoom\n        self.max_zoom = max_zoom\n        self.x_origin_offset = x_origin_offset\n        self.y_origin_offset = y_origin_offset\n        self.initial_resolution = initial_resolution\n        self._resolutions = [self._get_resolution(z) for z in range(self.min_zoom, self.max_zoom + 1)]",
        "docstring": "Initialization for the MercatorTileDefinition class, which defines parameters necessary for managing Mercator tile sources used by tile rendering systems.\n\nParameters\n----------\nx_range : tuple\n    The spatial extent of the data in the x-dimension, defining the bounds for tile generation.\n    \ny_range : tuple\n    The spatial extent of the data in the y-dimension, similar to x_range.\n    \ntile_size : int, optional\n    The size of each tile in pixels, defaulting to 256 pixels. This dictates the resolution of generated tiles.\n    \nmin_zoom : int, optional\n    The minimum zoom level for the tile layer, defaulting to 0. It determines the most zoomed-out view.\n    \nmax_zoom : int, optional\n    The maximum zoom level for the tile layer, defaulting to 30. It sets the most zoomed-in detail available.\n\nx_origin_offset : float, optional\n    An x-coordinate origin offset in plot coordinates, defaulting to 20037508.34, common for Web Mercator projections.\n\ny_origin_offset : float, optional\n    A y-coordinate origin offset in plot coordinates, defaulting to 20037508.34, corresponding to the same projection conventions.\n    \ninitial_resolution : float, optional\n    The initial resolution (in plot units per pixel) at the minimum zoom level. Default is defined for standard Web Mercator.\n\nAttributes\n----------\nself._resolutions : list\n    Computed list of resolutions for each zoom level between min_zoom and max_zoom, which will be used when determining tile resolutions for rendering tasks.\n\nThis method prepares the instance for use in rendering tiles by establishing the necessary parameters and calculated attributes to facilitate map tiling based on the Mercator projection.",
        "signature": "def __init__(self, x_range, y_range, tile_size=256, min_zoom=0, max_zoom=30, x_origin_offset=20037508.34, y_origin_offset=20037508.34, initial_resolution=156543.03392804097):",
        "type": "Method",
        "class_signature": "class MercatorTileDefinition:"
      },
      "MercatorTileDefinition.meters_to_tile": {
        "code": "    def meters_to_tile(self, mx, my, level):\n        \"\"\"Converts meters to tile coordinates for a specified zoom level.\n\nParameters\n----------\nmx : float\n    The x-coordinate in meters.\nmy : float\n    The y-coordinate in meters.\nlevel : int\n    The zoom level for which the tile coordinates are to be calculated.\n\nReturns\n-------\ntuple\n    A tuple representing the tile coordinates (tx, ty), where tx is the tile's x-coordinate and ty is its y-coordinate.\n\nNotes\n-----\nThis method first converts the provided meters into pixel coordinates using the `meters_to_pixels` method, which relies on the tile's resolution\u2014computed through the class's internal method `_get_resolution`. Finally, it translates the pixel coordinates into tile coordinates using the `pixels_to_tile` method, which also interacts with the class's `tile_size` attribute to determine the appropriate tile indexing.\"\"\"\n        px, py = self.meters_to_pixels(mx, my, level)\n        return self.pixels_to_tile(px, py, level)",
        "docstring": "Converts meters to tile coordinates for a specified zoom level.\n\nParameters\n----------\nmx : float\n    The x-coordinate in meters.\nmy : float\n    The y-coordinate in meters.\nlevel : int\n    The zoom level for which the tile coordinates are to be calculated.\n\nReturns\n-------\ntuple\n    A tuple representing the tile coordinates (tx, ty), where tx is the tile's x-coordinate and ty is its y-coordinate.\n\nNotes\n-----\nThis method first converts the provided meters into pixel coordinates using the `meters_to_pixels` method, which relies on the tile's resolution\u2014computed through the class's internal method `_get_resolution`. Finally, it translates the pixel coordinates into tile coordinates using the `pixels_to_tile` method, which also interacts with the class's `tile_size` attribute to determine the appropriate tile indexing.",
        "signature": "def meters_to_tile(self, mx, my, level):",
        "type": "Method",
        "class_signature": "class MercatorTileDefinition:"
      }
    },
    "datashader/core.py": {
      "Canvas.points": {
        "code": "    def points(self, source, x=None, y=None, agg=None, geometry=None):\n        \"\"\"Compute a reduction by pixel, mapping data to pixels as points.\n\nParameters\n----------\nsource : pandas.DataFrame, dask.DataFrame, or xarray.DataArray/Dataset\n    The input datasource containing coordinates to plot.\nx, y : str, optional\n    Column names for the x and y coordinates of each point. If provided,\n    the geometry argument must not be provided.\nagg : Reduction, optional\n    Reduction operation to compute. Defaults to count().\ngeometry : str, optional\n    Column name of a PointsArray, representing coordinates of each point. \n    If provided, the x and y arguments must not be supplied.\n\nReturns\n-------\ndata : The processed pixelated data, aggregated according to the specified reduction.\n\nDependencies\n------------\nThe function uses the `validate_xy_or_geometry` helper to ensure proper configuration of x, y, and geometry parameters. \nIt also interacts with the `bypixel` function to execute the aggregation logic and requires the presence of `Point`, \n`MultiPointGeometry`, and `MultiPointGeoPandas` glyph functionalities for handling geometric data representation. \nIt leverages features of the `spatialpandas` and `geopandas` libraries for specialized data types applicable in spatial contexts.\n\nConstants\n---------\n- `count_rdn`: Uses the count reduction as the default operation if none is specified.\n- `_axis_lookup`: A dictionary that references different types of axes (e.g., linear, logarithmic) used for transformations in Canvas.\"\"\"\n        'Compute a reduction by pixel, mapping data to pixels as points.\\n\\n        Parameters\\n        ----------\\n        source : pandas.DataFrame, dask.DataFrame, or xarray.DataArray/Dataset\\n            The input datasource.\\n        x, y : str\\n            Column names for the x and y coordinates of each point. If provided,\\n            the geometry argument may not also be provided.\\n        agg : Reduction, optional\\n            Reduction to compute. Default is ``count()``.\\n        geometry: str\\n            Column name of a PointsArray of the coordinates of each point. If provided,\\n            the x and y arguments may not also be provided.\\n        '\n        from .glyphs import Point, MultiPointGeometry\n        from .reductions import count as count_rdn\n        validate_xy_or_geometry('Point', x, y, geometry)\n        if agg is None:\n            agg = count_rdn()\n        if geometry is None:\n            glyph = Point(x, y)\n        elif spatialpandas and isinstance(source, spatialpandas.dask.DaskGeoDataFrame):\n            x_range = self.x_range if self.x_range is not None else (None, None)\n            y_range = self.y_range if self.y_range is not None else (None, None)\n            source = source.cx_partitions[slice(*x_range), slice(*y_range)]\n            glyph = MultiPointGeometry(geometry)\n        elif spatialpandas and isinstance(source, spatialpandas.GeoDataFrame):\n            glyph = MultiPointGeometry(geometry)\n        elif (geopandas_source := self._source_from_geopandas(source)) is not None:\n            source = geopandas_source\n            from datashader.glyphs.points import MultiPointGeoPandas\n            glyph = MultiPointGeoPandas(geometry)\n        else:\n            raise ValueError('source must be an instance of spatialpandas.GeoDataFrame, spatialpandas.dask.DaskGeoDataFrame, geopandas.GeoDataFrame, or dask_geopandas.GeoDataFrame. Received objects of type {typ}'.format(typ=type(source)))\n        return bypixel(source, self, glyph, agg)",
        "docstring": "Compute a reduction by pixel, mapping data to pixels as points.\n\nParameters\n----------\nsource : pandas.DataFrame, dask.DataFrame, or xarray.DataArray/Dataset\n    The input datasource containing coordinates to plot.\nx, y : str, optional\n    Column names for the x and y coordinates of each point. If provided,\n    the geometry argument must not be provided.\nagg : Reduction, optional\n    Reduction operation to compute. Defaults to count().\ngeometry : str, optional\n    Column name of a PointsArray, representing coordinates of each point. \n    If provided, the x and y arguments must not be supplied.\n\nReturns\n-------\ndata : The processed pixelated data, aggregated according to the specified reduction.\n\nDependencies\n------------\nThe function uses the `validate_xy_or_geometry` helper to ensure proper configuration of x, y, and geometry parameters. \nIt also interacts with the `bypixel` function to execute the aggregation logic and requires the presence of `Point`, \n`MultiPointGeometry`, and `MultiPointGeoPandas` glyph functionalities for handling geometric data representation. \nIt leverages features of the `spatialpandas` and `geopandas` libraries for specialized data types applicable in spatial contexts.\n\nConstants\n---------\n- `count_rdn`: Uses the count reduction as the default operation if none is specified.\n- `_axis_lookup`: A dictionary that references different types of axes (e.g., linear, logarithmic) used for transformations in Canvas.",
        "signature": "def points(self, source, x=None, y=None, agg=None, geometry=None):",
        "type": "Method",
        "class_signature": "class Canvas:"
      }
    }
  },
  "dependency_dict": {
    "datashader/tiles.py:_get_super_tile_min_max": {
      "datashader/tests/test_tiles.py": {
        "mock_load_data_func": {
          "code": "def mock_load_data_func(x_range, y_range):\n    global df\n    if df is None:\n        rng = np.random.default_rng()\n        xs = rng.normal(loc=0, scale=500000, size=10000000)\n        ys = rng.normal(loc=0, scale=500000, size=10000000)\n        df = pd.DataFrame(dict(x=xs, y=ys))\n\n    return df.loc[df['x'].between(*x_range) & df['y'].between(*y_range)]",
          "docstring": "",
          "signature": "def mock_load_data_func(x_range, y_range):",
          "type": "Function",
          "class_signature": null
        },
        "mock_rasterize_func": {
          "code": "def mock_rasterize_func(df, x_range, y_range, height, width):\n    cvs = ds.Canvas(x_range=x_range, y_range=y_range,\n                    plot_height=height, plot_width=width)\n    agg = cvs.points(df, 'x', 'y')\n    return agg",
          "docstring": "",
          "signature": "def mock_rasterize_func(df, x_range, y_range, height, width):",
          "type": "Function",
          "class_signature": null
        }
      }
    },
    "datashader/core.py:Canvas:points": {
      "datashader/core.py": {
        "bypixel": {
          "code": "def bypixel(source, canvas, glyph, agg, *, antialias=False):\n    \"\"\"Compute an aggregate grouped by pixel sized bins.\n\n    Aggregate input data ``source`` into a grid with shape and axis matching\n    ``canvas``, mapping data to bins by ``glyph``, and aggregating by reduction\n    ``agg``.\n\n    Parameters\n    ----------\n    source : pandas.DataFrame, dask.DataFrame\n        Input datasource\n    canvas : Canvas\n    glyph : Glyph\n    agg : Reduction\n    \"\"\"\n    source, dshape = _bypixel_sanitise(source, glyph, agg)\n    schema = dshape.measure\n    glyph.validate(schema)\n    agg.validate(schema)\n    canvas.validate()\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', 'All-NaN (slice|axis) encountered')\n        return bypixel.pipeline(source, schema, canvas, glyph, agg, antialias=antialias)",
          "docstring": "Compute an aggregate grouped by pixel sized bins.\n\nAggregate input data ``source`` into a grid with shape and axis matching\n``canvas``, mapping data to bins by ``glyph``, and aggregating by reduction\n``agg``.\n\nParameters\n----------\nsource : pandas.DataFrame, dask.DataFrame\n    Input datasource\ncanvas : Canvas\nglyph : Glyph\nagg : Reduction",
          "signature": "def bypixel(source, canvas, glyph, agg, *, antialias=False):",
          "type": "Function",
          "class_signature": null
        }
      }
    },
    "datashader/tiles.py:MercatorTileDefinition:__init__": {
      "datashader/tiles.py": {
        "MercatorTileDefinition._get_resolution": {
          "code": "    def _get_resolution(self, z):\n        return self.initial_resolution / 2 ** z",
          "docstring": "",
          "signature": "def _get_resolution(self, z):",
          "type": "Method",
          "class_signature": "class MercatorTileDefinition:"
        }
      }
    },
    "datashader/tiles.py:MercatorTileDefinition:meters_to_tile": {
      "datashader/tiles.py": {
        "MercatorTileDefinition.meters_to_pixels": {
          "code": "    def meters_to_pixels(self, mx, my, level):\n        res = self._get_resolution(level)\n        px = (mx + self.x_origin_offset) / res\n        py = (my + self.y_origin_offset) / res\n        return (px, py)",
          "docstring": "",
          "signature": "def meters_to_pixels(self, mx, my, level):",
          "type": "Method",
          "class_signature": "class MercatorTileDefinition:"
        },
        "MercatorTileDefinition.pixels_to_tile": {
          "code": "    def pixels_to_tile(self, px, py, level):\n        tx = math.ceil(px / self.tile_size)\n        tx = tx if tx == 0 else tx - 1\n        ty = max(math.ceil(py / self.tile_size) - 1, 0)\n        return (int(tx), invert_y_tile(int(ty), level))",
          "docstring": "",
          "signature": "def pixels_to_tile(self, px, py, level):",
          "type": "Method",
          "class_signature": "class MercatorTileDefinition:"
        }
      }
    }
  },
  "call_tree": {
    "datashader/tests/test_tiles.py:test_get_super_tile_min_max": {
      "datashader/tiles.py:_get_super_tile_min_max": {
        "datashader/tests/test_tiles.py:mock_load_data_func": {},
        "datashader/tests/test_tiles.py:mock_rasterize_func": {
          "datashader/core.py:Canvas:__init__": {},
          "datashader/core.py:Canvas:points": {
            "datashader/core.py:validate_xy_or_geometry": {},
            "datashader/reductions.py:SelfIntersectingOptionalFieldReduction:__init__": {
              "datashader/reductions.py:OptionalFieldReduction:__init__": {
                "datashader/reductions.py:Reduction:__init__": {}
              }
            },
            "datashader/glyphs/points.py:_PointLike:__init__": {},
            "datashader/core.py:bypixel": {
              "datashader/core.py:_bypixel_sanitise": {
                "datashader/core.py:_cols_to_keep": {
                  "datashader/glyphs/points.py:_PointLike:required_columns": {},
                  "datashader/core.py:recurse": {}
                },
                "datashader/utils.py:dshape_from_pandas": {
                  "datashader/utils.py:dshape_from_pandas_helper": {
                    "datashader/datashape/coretypes.py:CType:from_numpy_dtype": {
                      "datashader/datashape/coretypes.py:Type:lookup_type": {}
                    },
                    "datashader/datashape/coretypes.py:Mono:__eq__": {
                      "datashader/datashape/coretypes.py:Mono:shape": {},
                      "datashader/datashape/coretypes.py:Mono:measure": {},
                      "datashader/datashape/coretypes.py:Mono:info": {}
                    }
                  },
                  "datashader/datashape/coretypes.py:Record:__init__": {
                    "datashader/datashape/coretypes.py:_launder": {}
                  },
                  "datashader/datashape/coretypes.py:Mono:__rmul__": {
                    "datashader/datashape/coretypes.py:Fixed:__init__": {},
                    "datashader/datashape/coretypes.py:DataShape:__init__": {
                      "datashader/datashape/coretypes.py:_launder": {}
                    }
                  }
                }
              },
              "datashader/datashape/coretypes.py:DataShape:measure": {
                "datashader/datashape/coretypes.py:Mono:parameters": {
                  "datashader/datashape/coretypes.py:Mono:_slotted": {}
                }
              },
              "datashader/glyphs/points.py:_PointLike:validate": {
                "datashader/datashape/coretypes.py:Mono:measure": {},
                "datashader/datashape/coretypes.py:Record:__getitem__": {
                  "datashader/datashape/coretypes.py:Record:dict": {
                    "datashader/datashape/coretypes.py:Record:fields": {}
                  }
                },
                "datashader/utils.py:isreal": {
                  "datashader/datashape/predicates.py:launder": {},
                  "datashader/datashape/typesets.py:TypeSet:__contains__": {
                    "datashader/datashape/typesets.py:TypeSet:_set": {
                      "datashader/datashape/coretypes.py:Mono:__hash__": {}
                    },
                    "datashader/datashape/coretypes.py:Mono:__hash__": {}
                  }
                }
              },
              "datashader/reductions.py:OptionalFieldReduction:validate": {},
              "datashader/core.py:Canvas:validate": {
                "datashader/core.py:Canvas:validate_ranges": {
                  "datashader/core.py:Axis:validate": {}
                },
                "datashader/core.py:Canvas:validate_size": {}
              },
              "datashader/utils.py:Dispatcher:__call__": {
                "datashader/data_libraries/pandas.py:pandas_pipeline": {
                  "datashader/utils.py:Dispatcher:__call__": {
                    "[ignored_or_cut_off]": "..."
                  }
                }
              }
            }
          }
        }
      },
      "datashader/tests/test_tiles.py:assert_is_numeric": {}
    },
    "datashader/tests/test_tiles.py:mock_rasterize_func": {
      "datashader/core.py:Canvas:points": {
        "datashader/core.py:bypixel": {
          "datashader/core.py:_bypixel_sanitise": {
            "datashader/utils.py:dshape_from_pandas": {
              "datashader/utils.py:dshape_from_pandas_helper": {
                "datashader/datashape/coretypes.py:Mono:__eq__": {
                  "datashader/datashape/coretypes.py:Mono:info": {
                    "datashader/datashape/coretypes.py:Mono:parameters": {
                      "datashader/datashape/coretypes.py:Mono:_slotted": {}
                    }
                  }
                }
              }
            }
          },
          "datashader/utils.py:Dispatcher:__call__": {
            "datashader/data_libraries/pandas.py:pandas_pipeline": {
              "datashader/utils.py:Dispatcher:__call__": {
                "[ignored_or_cut_off]": "..."
              }
            }
          }
        }
      }
    },
    "datashader/tests/test_tiles.py:test_meters_to_tile": {
      "datashader/tiles.py:MercatorTileDefinition:__init__": {
        "datashader/tiles.py:MercatorTileDefinition:_get_resolution": {}
      },
      "datashader/tiles.py:MercatorTileDefinition:meters_to_tile": {
        "datashader/tiles.py:MercatorTileDefinition:meters_to_pixels": {
          "datashader/tiles.py:MercatorTileDefinition:_get_resolution": {}
        },
        "datashader/tiles.py:MercatorTileDefinition:pixels_to_tile": {
          "datashader/tiles.py:invert_y_tile": {}
        }
      }
    }
  },
  "PRD": "# PROJECT NAME: datashader-test_tiles\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 datashader/\n    \u251c\u2500\u2500 core.py\n    \u2502   \u2514\u2500\u2500 Canvas.points\n    \u2514\u2500\u2500 tiles.py\n        \u251c\u2500\u2500 MercatorTileDefinition.__init__\n        \u251c\u2500\u2500 MercatorTileDefinition.meters_to_tile\n        \u2514\u2500\u2500 _get_super_tile_min_max\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThe module provides functionality for generating and rendering tiled visualizations of large-scale spatial data using Mercator-based projections. It enables efficient tiling, data aggregation, rasterization, and dynamic image shading, while supporting features like multi-zoom level statistics calculation and custom rendering pipelines. By supporting tiling workflows and efficient handling of large datasets through functions for generating super tiles, calculating zoom-level statistics, and applying visualization shaders, it facilitates the creation of scalable map-based visual representations. This module addresses challenges in visualizing vast spatial datasets by optimizing performance and providing tools for generating rich, zoomable, and visually detailed maps.\n\n## FILE 1: datashader/tiles.py\n\n- FUNCTION NAME: _get_super_tile_min_max\n  - SIGNATURE: def _get_super_tile_min_max(tile_info, load_data_func, rasterize_func):\n  - DOCSTRING: \n```python\n\"\"\"\nCalculates the aggregated data for a given super tile by loading relevant data and applying a rasterization function.\n\nParameters\n----------\ntile_info : dict\n    A dictionary containing the range of x and y coordinates (`'x_range'`, `'y_range'`) and the size of the tile (`'tile_size'`).\nload_data_func : callable\n    A function that loads the data based on the provided x and y ranges.\nrasterize_func : callable\n    A function that rasterizes the loaded data into an aggregated format with specified dimensions.\n\nReturns\n-------\nnumpy.ndarray\n    The aggregated data for the specified super tile.\n\nNotes\n-----\nThis function utilizes the `load_data_func` to fetch necessary data within the specified x and y ranges and then employs the `rasterize_func` to convert that data into a raster format, defined by `tile_size`. It plays a crucial role in generating statistics for the super tiles in the `calculate_zoom_level_stats` function.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - datashader/tests/test_tiles.py:mock_rasterize_func\n    - datashader/tests/test_tiles.py:mock_load_data_func\n\n- CLASS METHOD: MercatorTileDefinition.__init__\n  - CLASS SIGNATURE: class MercatorTileDefinition:\n  - SIGNATURE: def __init__(self, x_range, y_range, tile_size=256, min_zoom=0, max_zoom=30, x_origin_offset=20037508.34, y_origin_offset=20037508.34, initial_resolution=156543.03392804097):\n  - DOCSTRING: \n```python\n\"\"\"\nInitialization for the MercatorTileDefinition class, which defines parameters necessary for managing Mercator tile sources used by tile rendering systems.\n\nParameters\n----------\nx_range : tuple\n    The spatial extent of the data in the x-dimension, defining the bounds for tile generation.\n    \ny_range : tuple\n    The spatial extent of the data in the y-dimension, similar to x_range.\n    \ntile_size : int, optional\n    The size of each tile in pixels, defaulting to 256 pixels. This dictates the resolution of generated tiles.\n    \nmin_zoom : int, optional\n    The minimum zoom level for the tile layer, defaulting to 0. It determines the most zoomed-out view.\n    \nmax_zoom : int, optional\n    The maximum zoom level for the tile layer, defaulting to 30. It sets the most zoomed-in detail available.\n\nx_origin_offset : float, optional\n    An x-coordinate origin offset in plot coordinates, defaulting to 20037508.34, common for Web Mercator projections.\n\ny_origin_offset : float, optional\n    A y-coordinate origin offset in plot coordinates, defaulting to 20037508.34, corresponding to the same projection conventions.\n    \ninitial_resolution : float, optional\n    The initial resolution (in plot units per pixel) at the minimum zoom level. Default is defined for standard Web Mercator.\n\nAttributes\n----------\nself._resolutions : list\n    Computed list of resolutions for each zoom level between min_zoom and max_zoom, which will be used when determining tile resolutions for rendering tasks.\n\nThis method prepares the instance for use in rendering tiles by establishing the necessary parameters and calculated attributes to facilitate map tiling based on the Mercator projection.\n\"\"\"\n```\n\n- CLASS METHOD: MercatorTileDefinition.meters_to_tile\n  - CLASS SIGNATURE: class MercatorTileDefinition:\n  - SIGNATURE: def meters_to_tile(self, mx, my, level):\n  - DOCSTRING: \n```python\n\"\"\"\nConverts meters to tile coordinates for a specified zoom level.\n\nParameters\n----------\nmx : float\n    The x-coordinate in meters.\nmy : float\n    The y-coordinate in meters.\nlevel : int\n    The zoom level for which the tile coordinates are to be calculated.\n\nReturns\n-------\ntuple\n    A tuple representing the tile coordinates (tx, ty), where tx is the tile's x-coordinate and ty is its y-coordinate.\n\nNotes\n-----\nThis method first converts the provided meters into pixel coordinates using the `meters_to_pixels` method, which relies on the tile's resolution\u2014computed through the class's internal method `_get_resolution`. Finally, it translates the pixel coordinates into tile coordinates using the `pixels_to_tile` method, which also interacts with the class's `tile_size` attribute to determine the appropriate tile indexing.\n\"\"\"\n```\n\n## FILE 2: datashader/core.py\n\n- CLASS METHOD: Canvas.points\n  - CLASS SIGNATURE: class Canvas:\n  - SIGNATURE: def points(self, source, x=None, y=None, agg=None, geometry=None):\n  - DOCSTRING: \n```python\n\"\"\"\nCompute a reduction by pixel, mapping data to pixels as points.\n\nParameters\n----------\nsource : pandas.DataFrame, dask.DataFrame, or xarray.DataArray/Dataset\n    The input datasource containing coordinates to plot.\nx, y : str, optional\n    Column names for the x and y coordinates of each point. If provided,\n    the geometry argument must not be provided.\nagg : Reduction, optional\n    Reduction operation to compute. Defaults to count().\ngeometry : str, optional\n    Column name of a PointsArray, representing coordinates of each point. \n    If provided, the x and y arguments must not be supplied.\n\nReturns\n-------\ndata : The processed pixelated data, aggregated according to the specified reduction.\n\nDependencies\n------------\nThe function uses the `validate_xy_or_geometry` helper to ensure proper configuration of x, y, and geometry parameters. \nIt also interacts with the `bypixel` function to execute the aggregation logic and requires the presence of `Point`, \n`MultiPointGeometry`, and `MultiPointGeoPandas` glyph functionalities for handling geometric data representation. \nIt leverages features of the `spatialpandas` and `geopandas` libraries for specialized data types applicable in spatial contexts.\n\nConstants\n---------\n- `count_rdn`: Uses the count reduction as the default operation if none is specified.\n- `_axis_lookup`: A dictionary that references different types of axes (e.g., linear, logarithmic) used for transformations in Canvas.\n\"\"\"\n```\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "datashader/tiles.py": "from __future__ import annotations\nfrom io import BytesIO\nimport math\nimport os\nimport numpy as np\nfrom PIL.Image import fromarray\ntry:\n    import dask\n    import dask.bag as db\nexcept ImportError:\n    dask, db = (None, None)\n__all__ = ['render_tiles', 'MercatorTileDefinition']\n\ndef _create_dir(path):\n    import errno\n    import os\n    try:\n        os.makedirs(path)\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise\n\ndef calculate_zoom_level_stats(super_tiles, load_data_func, rasterize_func, color_ranging_strategy='fullscan'):\n    if color_ranging_strategy == 'fullscan':\n        stats = []\n        is_bool = False\n        for super_tile in super_tiles:\n            agg = _get_super_tile_min_max(super_tile, load_data_func, rasterize_func)\n            super_tile['agg'] = agg\n            if agg.dtype.kind == 'b':\n                is_bool = True\n            else:\n                stats.append(np.nanmin(agg.data))\n                stats.append(np.nanmax(agg.data))\n        if is_bool:\n            span = (0, 1)\n        elif dask:\n            b = db.from_sequence(stats)\n            span = dask.compute(b.min(), b.max())\n        else:\n            raise ValueError('Dask is required for non-boolean data')\n        return (super_tiles, span)\n    else:\n        raise ValueError('Invalid color_ranging_strategy option')\n\ndef render_tiles(full_extent, levels, load_data_func, rasterize_func, shader_func, post_render_func, output_path, color_ranging_strategy='fullscan'):\n    if not dask:\n        raise ImportError('Dask is required for rendering tiles')\n    results = {}\n    for level in levels:\n        print('calculating statistics for level {}'.format(level))\n        super_tiles, span = calculate_zoom_level_stats(list(gen_super_tiles(full_extent, level)), load_data_func, rasterize_func, color_ranging_strategy=color_ranging_strategy)\n        print('rendering {} supertiles for zoom level {} with span={}'.format(len(super_tiles), level, span))\n        b = db.from_sequence(super_tiles)\n        b.map(render_super_tile, span, output_path, shader_func, post_render_func).compute()\n        results[level] = dict(success=True, stats=span, supertile_count=len(super_tiles))\n    return results\n\ndef gen_super_tiles(extent, zoom_level, span=None):\n    xmin, ymin, xmax, ymax = extent\n    super_tile_size = min(2 ** 4 * 256, 2 ** zoom_level * 256)\n    super_tile_def = MercatorTileDefinition(x_range=(xmin, xmax), y_range=(ymin, ymax), tile_size=super_tile_size)\n    super_tiles = super_tile_def.get_tiles_by_extent(extent, zoom_level)\n    for s in super_tiles:\n        st_extent = s[3]\n        x_range = (st_extent[0], st_extent[2])\n        y_range = (st_extent[1], st_extent[3])\n        yield {'level': zoom_level, 'x_range': x_range, 'y_range': y_range, 'tile_size': super_tile_def.tile_size, 'span': span}\n\ndef render_super_tile(tile_info, span, output_path, shader_func, post_render_func):\n    level = tile_info['level']\n    ds_img = shader_func(tile_info['agg'], span=span)\n    return create_sub_tiles(ds_img, level, tile_info, output_path, post_render_func)\n\ndef create_sub_tiles(data_array, level, tile_info, output_path, post_render_func=None):\n    _create_dir(output_path)\n    tile_def = MercatorTileDefinition(x_range=tile_info['x_range'], y_range=tile_info['y_range'], tile_size=256)\n    if output_path.startswith('s3:'):\n        renderer = S3TileRenderer(tile_def, output_location=output_path, post_render_func=post_render_func)\n    else:\n        renderer = FileSystemTileRenderer(tile_def, output_location=output_path, post_render_func=post_render_func)\n    return renderer.render(data_array, level=level)\n\ndef invert_y_tile(y, z):\n    return 2 ** z - 1 - y\n\nclass MercatorTileDefinition:\n    \"\"\" Implementation of mercator tile source\n    In general, tile sources are used as a required input for ``TileRenderer``.\n\n    Parameters\n    ----------\n\n    x_range : tuple\n      full extent of x dimension in data units\n\n    y_range : tuple\n      full extent of y dimension in data units\n\n    max_zoom : int\n      A maximum zoom level for the tile layer. This is the most zoomed-in level.\n\n    min_zoom : int\n      A minimum zoom level for the tile layer. This is the most zoomed-out level.\n\n    max_zoom : int\n      A maximum zoom level for the tile layer. This is the most zoomed-in level.\n\n    x_origin_offset : int\n      An x-offset in plot coordinates.\n\n    y_origin_offset : int\n      An y-offset in plot coordinates.\n\n    initial_resolution : int\n      Resolution (plot_units / pixels) of minimum zoom level of tileset\n      projection. None to auto-compute.\n\n    format : int\n      An y-offset in plot coordinates.\n\n    Output\n    ------\n    tileScheme: MercatorTileSource\n\n    \"\"\"\n\n    def to_ogc_tile_metadata(self, output_file_path):\n        \"\"\"\n        Create OGC tile metadata XML\n        \"\"\"\n        pass\n\n    def to_esri_tile_metadata(self, output_file_path):\n        \"\"\"\n        Create ESRI tile metadata JSON\n        \"\"\"\n        pass\n\n    def is_valid_tile(self, x, y, z):\n        if x < 0 or x >= math.pow(2, z):\n            return False\n        if y < 0 or y >= math.pow(2, z):\n            return False\n        return True\n\n    def _get_resolution(self, z):\n        return self.initial_resolution / 2 ** z\n\n    def get_resolution_by_extent(self, extent, height, width):\n        x_rs = (extent[2] - extent[0]) / width\n        y_rs = (extent[3] - extent[1]) / height\n        return [x_rs, y_rs]\n\n    def get_level_by_extent(self, extent, height, width):\n        x_rs = (extent[2] - extent[0]) / width\n        y_rs = (extent[3] - extent[1]) / height\n        resolution = max(x_rs, y_rs)\n        i = 0\n        for r in self._resolutions:\n            if resolution > r:\n                if i == 0:\n                    return 0\n                if i > 0:\n                    return i - 1\n            i += 1\n        return i - 1\n\n    def pixels_to_meters(self, px, py, level):\n        res = self._get_resolution(level)\n        mx = px * res - self.x_origin_offset\n        my = py * res - self.y_origin_offset\n        return (mx, my)\n\n    def meters_to_pixels(self, mx, my, level):\n        res = self._get_resolution(level)\n        px = (mx + self.x_origin_offset) / res\n        py = (my + self.y_origin_offset) / res\n        return (px, py)\n\n    def pixels_to_tile(self, px, py, level):\n        tx = math.ceil(px / self.tile_size)\n        tx = tx if tx == 0 else tx - 1\n        ty = max(math.ceil(py / self.tile_size) - 1, 0)\n        return (int(tx), invert_y_tile(int(ty), level))\n\n    def pixels_to_raster(self, px, py, level):\n        map_size = self.tile_size << level\n        return (px, map_size - py)\n\n    def get_tiles_by_extent(self, extent, level):\n        xmin, ymin, xmax, ymax = extent\n        txmin, tymax = self.meters_to_tile(xmin, ymin, level)\n        txmax, tymin = self.meters_to_tile(xmax, ymax, level)\n        tiles = []\n        for ty in range(tymin, tymax + 1):\n            for tx in range(txmin, txmax + 1):\n                if self.is_valid_tile(tx, ty, level):\n                    t = (tx, ty, level, self.get_tile_meters(tx, ty, level))\n                    tiles.append(t)\n        return tiles\n\n    def get_tile_meters(self, tx, ty, level):\n        ty = invert_y_tile(ty, level)\n        xmin, ymin = self.pixels_to_meters(tx * self.tile_size, ty * self.tile_size, level)\n        xmax, ymax = self.pixels_to_meters((tx + 1) * self.tile_size, (ty + 1) * self.tile_size, level)\n        return (xmin, ymin, xmax, ymax)\n\nclass TileRenderer:\n\n    def __init__(self, tile_definition, output_location, tile_format='PNG', post_render_func=None):\n        self.tile_def = tile_definition\n        self.output_location = output_location\n        self.tile_format = tile_format\n        self.post_render_func = post_render_func\n        if self.tile_format not in ('PNG', 'JPG'):\n            raise ValueError('Invalid output format')\n\n    def render(self, da, level):\n        xmin, xmax = self.tile_def.x_range\n        ymin, ymax = self.tile_def.y_range\n        extent = (xmin, ymin, xmax, ymax)\n        tiles = self.tile_def.get_tiles_by_extent(extent, level)\n        for t in tiles:\n            x, y, z, data_extent = t\n            dxmin, dymin, dxmax, dymax = data_extent\n            arr = da.loc[{'x': slice(dxmin, dxmax), 'y': slice(dymin, dymax)}]\n            if 0 in arr.shape:\n                continue\n            img = fromarray(np.flip(arr.data, 0), 'RGBA')\n            if self.post_render_func:\n                extras = dict(x=x, y=y, z=z)\n                img = self.post_render_func(img, **extras)\n            yield (img, x, y, z)\n\ndef tile_previewer(full_extent, tileset_url, output_dir=None, filename='index.html', title='Datashader Tileset', min_zoom=0, max_zoom=40, height=None, width=None, **kwargs):\n    \"\"\"Helper function for creating a simple Bokeh figure with\n    a WMTS Tile Source.\n\n    Notes\n    -----\n    - if you don't supply height / width, stretch_both sizing_mode is used.\n    - supply an output_dir to write figure to disk.\n    \"\"\"\n    try:\n        from bokeh.plotting import figure\n        from bokeh.models.tiles import WMTSTileSource\n        from bokeh.io import output_file, save\n        from os import path\n    except ImportError:\n        raise ImportError('install bokeh to enable creation of simple tile viewer')\n    if output_dir:\n        output_file(filename=path.join(output_dir, filename), title=title)\n    xmin, ymin, xmax, ymax = full_extent\n    if height and width:\n        p = figure(width=width, height=height, x_range=(xmin, xmax), y_range=(ymin, ymax), tools='pan,wheel_zoom,reset', **kwargs)\n    else:\n        p = figure(sizing_mode='stretch_both', x_range=(xmin, xmax), y_range=(ymin, ymax), tools='pan,wheel_zoom,reset', **kwargs)\n    p.background_fill_color = 'black'\n    p.grid.grid_line_alpha = 0\n    p.axis.visible = True\n    tile_source = WMTSTileSource(url=tileset_url, min_zoom=min_zoom, max_zoom=max_zoom)\n    p.add_tile(tile_source, render_parents=False)\n    if output_dir:\n        save(p)\n    return p\n\nclass FileSystemTileRenderer(TileRenderer):\n\n    def render(self, da, level):\n        for img, x, y, z in super().render(da, level):\n            tile_file_name = '{}.{}'.format(y, self.tile_format.lower())\n            tile_directory = os.path.join(self.output_location, str(z), str(x))\n            output_file = os.path.join(tile_directory, tile_file_name)\n            _create_dir(tile_directory)\n            img.save(output_file, self.tile_format)\n\nclass S3TileRenderer(TileRenderer):\n\n    def render(self, da, level):\n        try:\n            import boto3\n        except ImportError:\n            raise ImportError('install boto3 to enable rendering to S3')\n        from urllib.parse import urlparse\n        s3_info = urlparse(self.output_location)\n        bucket = s3_info.netloc\n        client = boto3.client('s3')\n        for img, x, y, z in super().render(da, level):\n            tile_file_name = '{}.{}'.format(y, self.tile_format.lower())\n            key = os.path.join(s3_info.path, str(z), str(x), tile_file_name).lstrip('/')\n            output_buf = BytesIO()\n            img.save(output_buf, self.tile_format)\n            output_buf.seek(0)\n            client.put_object(Body=output_buf, Bucket=bucket, Key=key, ACL='public-read')\n        return 'https://{}.s3.amazonaws.com/{}'.format(bucket, s3_info.path)",
    "datashader/core.py": "from __future__ import annotations\nfrom numbers import Number\nfrom math import log10\nimport warnings\nimport contextlib\nimport numpy as np\nimport pandas as pd\nfrom packaging.version import Version\nfrom xarray import DataArray, Dataset\nfrom .utils import Dispatcher, ngjit, calc_res, calc_bbox, orient_array, dshape_from_xarray_dataset\nfrom .utils import get_indices, dshape_from_pandas, dshape_from_dask\nfrom .utils import Expr\nfrom .resampling import resample_2d, resample_2d_distributed\nfrom . import reductions as rd\ntry:\n    import dask.dataframe as dd\n    import dask.array as da\nexcept ImportError:\n    dd, da = (None, None)\ntry:\n    import cudf\nexcept Exception:\n    cudf = None\ntry:\n    import dask_cudf\nexcept Exception:\n    dask_cudf = None\ntry:\n    import spatialpandas\nexcept Exception:\n    spatialpandas = None\n\nclass Axis:\n    \"\"\"Interface for implementing axis transformations.\n\n    Instances hold implementations of transformations to and from axis space.\n    The default implementation is equivalent to:\n\n    >>> def forward_transform(data_x):\n    ...     scale * mapper(data_x) + t\n    >>> def inverse_transform(axis_x):\n    ...     inverse_mapper((axis_x - t)/s)\n\n    Where ``mapper`` and ``inverse_mapper`` are elementwise functions mapping\n    to and from axis-space respectively, and ``scale`` and ``transform`` are\n    parameters describing a linear scale and translate transformation, computed\n    by the ``compute_scale_and_translate`` method.\n    \"\"\"\n\n    def compute_scale_and_translate(self, range, n):\n        \"\"\"Compute the scale and translate parameters for a linear transformation\n        ``output = s * input + t``, mapping from data space to axis space.\n\n        Parameters\n        ----------\n        range : tuple\n            A tuple representing the range ``[min, max]`` along the axis, in\n            data space. Both min and max are inclusive.\n        n : int\n            The number of bins along the axis.\n\n        Returns\n        -------\n        s, t : floats\n        \"\"\"\n        start, end = map(self.mapper, range)\n        s = n / (end - start)\n        t = -start * s\n        return (s, t)\n\n    def compute_index(self, st, n):\n        \"\"\"Compute a 1D array representing the axis index.\n\n        Parameters\n        ----------\n        st : tuple\n            A tuple of ``(scale, translate)`` parameters.\n        n : int\n            The number of bins along the dimension.\n\n        Returns\n        -------\n        index : ndarray\n        \"\"\"\n        px = np.arange(n) + 0.5\n        s, t = st\n        return self.inverse_mapper((px - t) / s)\n\n    def mapper(val):\n        \"\"\"A mapping from data space to axis space\"\"\"\n        raise NotImplementedError\n\n    def inverse_mapper(val):\n        \"\"\"A mapping from axis space to data space\"\"\"\n        raise NotImplementedError\n\n    def validate(self, range):\n        \"\"\"Given a range (low,high), raise an error if the range is invalid for this axis\"\"\"\n        pass\n\nclass LinearAxis(Axis):\n    \"\"\"A linear Axis\"\"\"\n\n    @staticmethod\n    @ngjit\n    def mapper(val):\n        return val\n\n    @staticmethod\n    @ngjit\n    def inverse_mapper(val):\n        return val\n\nclass LogAxis(Axis):\n    \"\"\"A base-10 logarithmic Axis\"\"\"\n\n    @staticmethod\n    @ngjit\n    def mapper(val):\n        return log10(float(val))\n\n    @staticmethod\n    @ngjit\n    def inverse_mapper(val):\n        y = 10\n        return y ** val\n\n    def validate(self, range):\n        if range is None:\n            return\n        if range[0] <= 0 or range[1] <= 0:\n            raise ValueError('Range values must be >0 for logarithmic axes')\n_axis_lookup = {'linear': LinearAxis(), 'log': LogAxis()}\n\ndef validate_xy_or_geometry(glyph, x, y, geometry):\n    if geometry is None and (x is None or y is None) or (geometry is not None and (x is not None or y is not None)):\n        raise ValueError('\\n{glyph} coordinates may be specified by providing both the x and y arguments, or by\\nproviding the geometry argument. Received:\\n    x: {x}\\n    y: {y}\\n    geometry: {geometry}\\n'.format(glyph=glyph, x=repr(x), y=repr(y), geometry=repr(geometry)))\n\nclass Canvas:\n    \"\"\"An abstract canvas representing the space in which to bin.\n\n    Parameters\n    ----------\n    plot_width, plot_height : int, optional\n        Width and height of the output aggregate in pixels.\n    x_range, y_range : tuple, optional\n        A tuple representing the bounds inclusive space ``[min, max]`` along\n        the axis.\n    x_axis_type, y_axis_type : str, optional\n        The type of the axis. Valid options are ``'linear'`` [default], and\n        ``'log'``.\n    \"\"\"\n\n    def __init__(self, plot_width=600, plot_height=600, x_range=None, y_range=None, x_axis_type='linear', y_axis_type='linear'):\n        self.plot_width = plot_width\n        self.plot_height = plot_height\n        self.x_range = None if x_range is None else tuple(x_range)\n        self.y_range = None if y_range is None else tuple(y_range)\n        self.x_axis = _axis_lookup[x_axis_type]\n        self.y_axis = _axis_lookup[y_axis_type]\n\n    def line(self, source, x=None, y=None, agg=None, axis=0, geometry=None, line_width=0, antialias=False):\n        \"\"\"Compute a reduction by pixel, mapping data to pixels as one or\n        more lines.\n\n        For aggregates that take in extra fields, the interpolated bins will\n        receive the fields from the previous point. In pseudocode:\n\n        >>> for i in range(len(rows) - 1):    # doctest: +SKIP\n        ...     row0 = rows[i]\n        ...     row1 = rows[i + 1]\n        ...     for xi, yi in interpolate(row0.x, row0.y, row1.x, row1.y):\n        ...         add_to_aggregate(xi, yi, row0)\n\n        Parameters\n        ----------\n        source : pandas.DataFrame, dask.DataFrame, or xarray.DataArray/Dataset\n            The input datasource.\n        x, y : str or number or list or tuple or np.ndarray\n            Specification of the x and y coordinates of each vertex\n            * str or number: Column labels in source\n            * list or tuple: List or tuple of column labels in source\n            * np.ndarray: When axis=1, a literal array of the\n              coordinates to be used for every row\n        agg : Reduction, optional\n            Reduction to compute. Default is ``any()``.\n        axis : 0 or 1, default 0\n            Axis in source to draw lines along\n            * 0: Draw lines using data from the specified columns across\n                 all rows in source\n            * 1: Draw one line per row in source using data from the\n                 specified columns\n        geometry : str\n            Column name of a LinesArray of the coordinates of each line. If provided,\n            the x and y arguments may not also be provided.\n        line_width : number, optional\n            Width of the line to draw, in pixels. If zero, the\n            default, lines are drawn using a simple algorithm with a\n            blocky single-pixel width based on whether the line passes\n            through each pixel or does not. If greater than one, lines\n            are drawn with the specified width using a slower and\n            more complex antialiasing algorithm with fractional values\n            along each edge, so that lines have a more uniform visual\n            appearance across all angles. Line widths between 0 and 1\n            effectively use a line_width of 1 pixel but with a\n            proportionate reduction in the strength of each pixel,\n            approximating the visual appearance of a subpixel line\n            width.\n        antialias : bool, optional\n            This option is kept for backward compatibility only.\n            ``True`` is equivalent to ``line_width=1`` and\n            ``False`` (the default) to ``line_width=0``. Do not specify\n            both ``antialias`` and ``line_width`` in the same call as a\n            ``ValueError`` will be raised if they disagree.\n\n        Examples\n        --------\n        Define a canvas and a pandas DataFrame with 6 rows\n        >>> import pandas as pd  # doctest: +SKIP\n        ... import numpy as np\n        ... import datashader as ds\n        ... from datashader import Canvas\n        ... import datashader.transfer_functions as tf\n        ... cvs = Canvas()\n        ... df = pd.DataFrame({\n        ...    'A1': [1, 1.5, 2, 2.5, 3, 4],\n        ...    'A2': [1.5, 2, 3, 3.2, 4, 5],\n        ...    'B1': [10, 12, 11, 14, 13, 15],\n        ...    'B2': [11, 9, 10, 7, 8, 12],\n        ... }, dtype='float64')\n\n        Aggregate one line across all rows, with coordinates df.A1 by df.B1\n        >>> agg = cvs.line(df, x='A1', y='B1', axis=0)  # doctest: +SKIP\n        ... tf.spread(tf.shade(agg))\n\n        Aggregate two lines across all rows. The first with coordinates\n        df.A1 by df.B1 and the second with coordinates df.A2 by df.B2\n        >>> agg = cvs.line(df, x=['A1', 'A2'], y=['B1', 'B2'], axis=0)  # doctest: +SKIP\n        ... tf.spread(tf.shade(agg))\n\n        Aggregate two lines across all rows where the lines share the same\n        x coordinates. The first line will have coordinates df.A1 by df.B1\n        and the second will have coordinates df.A1 by df.B2\n        >>> agg = cvs.line(df, x='A1', y=['B1', 'B2'], axis=0)  # doctest: +SKIP\n        ... tf.spread(tf.shade(agg))\n\n        Aggregate 6 length-2 lines, one per row, where the ith line has\n        coordinates [df.A1[i], df.A2[i]] by [df.B1[i], df.B2[i]]\n        >>> agg = cvs.line(df, x=['A1', 'A2'], y=['B1', 'B2'], axis=1)  # doctest: +SKIP\n        ... tf.spread(tf.shade(agg))\n\n        Aggregate 6 length-4 lines, one per row, where the x coordinates\n        of every line are [0, 1, 2, 3] and the y coordinates of the ith line\n        are [df.A1[i], df.A2[i], df.B1[i], df.B2[i]].\n        >>> agg = cvs.line(df,  # doctest: +SKIP\n        ...                x=np.arange(4),\n        ...                y=['A1', 'A2', 'B1', 'B2'],\n        ...                axis=1)\n        ... tf.spread(tf.shade(agg))\n\n        Aggregate RaggedArrays of variable length lines, one per row\n        (requires pandas >= 0.24.0)\n        >>> df_ragged = pd.DataFrame({  # doctest: +SKIP\n        ...    'A1': pd.array([\n        ...        [1, 1.5], [2, 2.5, 3], [1.5, 2, 3, 4], [3.2, 4, 5]],\n        ...        dtype='Ragged[float32]'),\n        ...    'B1': pd.array([\n        ...        [10, 12], [11, 14, 13], [10, 7, 9, 10], [7, 8, 12]],\n        ...        dtype='Ragged[float32]'),\n        ...    'group': pd.Categorical([0, 1, 2, 1])\n        ... })\n        ...\n        ... agg = cvs.line(df_ragged, x='A1', y='B1', axis=1)\n        ... tf.spread(tf.shade(agg))\n\n        Aggregate RaggedArrays of variable length lines by group column,\n        one per row (requires pandas >= 0.24.0)\n        >>> agg = cvs.line(df_ragged, x='A1', y='B1',  # doctest: +SKIP\n        ...                agg=ds.count_cat('group'), axis=1)\n        ... tf.spread(tf.shade(agg))\n        \"\"\"\n        from .glyphs import LineAxis0, LinesAxis1, LinesAxis1XConstant, LinesAxis1YConstant, LineAxis0Multi, LinesAxis1Ragged, LineAxis1Geometry, LinesXarrayCommonX\n        validate_xy_or_geometry('Line', x, y, geometry)\n        if agg is None:\n            agg = rd.any()\n        if line_width is None:\n            line_width = 0\n        if antialias and line_width != 0:\n            raise ValueError('Do not specify values for both the line_width and \\nantialias keyword arguments; use line_width instead.')\n        if antialias:\n            line_width = 1.0\n        if geometry is not None:\n            if spatialpandas and isinstance(source, spatialpandas.dask.DaskGeoDataFrame):\n                x_range = self.x_range if self.x_range is not None else (None, None)\n                y_range = self.y_range if self.y_range is not None else (None, None)\n                source = source.cx_partitions[slice(*x_range), slice(*y_range)]\n                glyph = LineAxis1Geometry(geometry)\n            elif spatialpandas and isinstance(source, spatialpandas.GeoDataFrame):\n                glyph = LineAxis1Geometry(geometry)\n            elif (geopandas_source := self._source_from_geopandas(source)) is not None:\n                source = geopandas_source\n                from datashader.glyphs.line import LineAxis1GeoPandas\n                glyph = LineAxis1GeoPandas(geometry)\n            else:\n                raise ValueError('source must be an instance of spatialpandas.GeoDataFrame, spatialpandas.dask.DaskGeoDataFrame, geopandas.GeoDataFrame, or dask_geopandas.GeoDataFrame. Received objects of type {typ}'.format(typ=type(source)))\n        elif isinstance(source, Dataset) and isinstance(x, str) and isinstance(y, str):\n            x_arr = source[x]\n            y_arr = source[y]\n            if x_arr.ndim != 1:\n                raise ValueError(f'x array must have 1 dimension not {x_arr.ndim}')\n            if y_arr.ndim != 2:\n                raise ValueError(f'y array must have 2 dimensions not {y_arr.ndim}')\n            if x not in y_arr.dims:\n                raise ValueError('x must be one of the coordinate dimensions of y')\n            y_coord_dims = list(y_arr.coords.dims)\n            x_dim_index = y_coord_dims.index(x)\n            glyph = LinesXarrayCommonX(x, y, x_dim_index)\n        else:\n            orig_x, orig_y = (x, y)\n            x, y = _broadcast_column_specifications(x, y)\n            if axis == 0:\n                if isinstance(x, (Number, str)) and isinstance(y, (Number, str)):\n                    glyph = LineAxis0(x, y)\n                elif isinstance(x, (list, tuple)) and isinstance(y, (list, tuple)):\n                    glyph = LineAxis0Multi(tuple(x), tuple(y))\n                else:\n                    raise ValueError('\\nInvalid combination of x and y arguments to Canvas.line when axis=0.\\n    Received:\\n        x: {x}\\n        y: {y}\\nSee docstring for more information on valid usage'.format(x=repr(orig_x), y=repr(orig_y)))\n            elif axis == 1:\n                if isinstance(x, (list, tuple)) and isinstance(y, (list, tuple)):\n                    glyph = LinesAxis1(tuple(x), tuple(y))\n                elif isinstance(x, np.ndarray) and isinstance(y, (list, tuple)):\n                    glyph = LinesAxis1XConstant(x, tuple(y))\n                elif isinstance(x, (list, tuple)) and isinstance(y, np.ndarray):\n                    glyph = LinesAxis1YConstant(tuple(x), y)\n                elif isinstance(x, (Number, str)) and isinstance(y, (Number, str)):\n                    glyph = LinesAxis1Ragged(x, y)\n                else:\n                    raise ValueError('\\nInvalid combination of x and y arguments to Canvas.line when axis=1.\\n    Received:\\n        x: {x}\\n        y: {y}\\nSee docstring for more information on valid usage'.format(x=repr(orig_x), y=repr(orig_y)))\n            else:\n                raise ValueError('\\nThe axis argument to Canvas.line must be 0 or 1\\n    Received: {axis}'.format(axis=axis))\n        if line_width > 0 and (cudf and isinstance(source, cudf.DataFrame) or (dask_cudf and isinstance(source, dask_cudf.DataFrame))):\n            warnings.warn('Antialiased lines are not supported for CUDA-backed sources, so reverting to line_width=0')\n            line_width = 0\n        glyph.set_line_width(line_width)\n        if glyph.antialiased:\n            non_cat_agg = agg\n            if isinstance(non_cat_agg, rd.by):\n                non_cat_agg = non_cat_agg.reduction\n            if not isinstance(non_cat_agg, (rd.any, rd.count, rd.max, rd.min, rd.sum, rd.summary, rd._sum_zero, rd._first_or_last, rd.mean, rd.max_n, rd.min_n, rd._first_n_or_last_n, rd._max_or_min_row_index, rd._max_n_or_min_n_row_index, rd.where)):\n                raise NotImplementedError(f'{type(non_cat_agg)} reduction not implemented for antialiased lines')\n        return bypixel(source, self, glyph, agg, antialias=glyph.antialiased)\n\n    def area(self, source, x, y, agg=None, axis=0, y_stack=None):\n        \"\"\"Compute a reduction by pixel, mapping data to pixels as a filled\n        area region\n\n        Parameters\n        ----------\n        source : pandas.DataFrame, dask.DataFrame, or xarray.DataArray/Dataset\n            The input datasource.\n        x, y : str or number or list or tuple or np.ndarray\n            Specification of the x and y coordinates of each vertex of the\n            line defining the starting edge of the area region.\n            * str or number: Column labels in source\n            * list or tuple: List or tuple of column labels in source\n            * np.ndarray: When axis=1, a literal array of the\n              coordinates to be used for every row\n        agg : Reduction, optional\n            Reduction to compute. Default is ``count()``.\n        axis : 0 or 1, default 0\n            Axis in source to draw lines along\n            * 0: Draw area regions using data from the specified columns\n                 across all rows in source\n            * 1: Draw one area region per row in source using data from the\n                 specified columns\n        y_stack: str or number or list or tuple or np.ndarray or None\n            Specification of the y coordinates of each vertex of the line\n            defining the ending edge of the area region, where the x\n            coordinate is given by the x argument described above.\n\n            If y_stack is None, then the area region is filled to the y=0 line\n\n            If y_stack is not None, then the form of y_stack must match the\n            form of y.\n\n        Examples\n        --------\n        Define a canvas and a pandas DataFrame with 6 rows\n        >>> import pandas as pd  # doctest: +SKIP\n        ... import numpy as np\n        ... import datashader as ds\n        ... from datashader import Canvas\n        ... import datashader.transfer_functions as tf\n        ... cvs = Canvas()\n        ... df = pd.DataFrame({\n        ...    'A1': [1, 1.5, 2, 2.5, 3, 4],\n        ...    'A2': [1.6, 2.1, 2.9, 3.2, 4.2, 5],\n        ...    'B1': [10, 12, 11, 14, 13, 15],\n        ...    'B2': [11, 9, 10, 7, 8, 12],\n        ... }, dtype='float64')\n\n        Aggregate one area region across all rows, that starts with\n        coordinates df.A1 by df.B1 and is filled to the y=0 line\n        >>> agg = cvs.area(df, x='A1', y='B1',  # doctest: +SKIP\n        ...                agg=ds.count(), axis=0)\n        ... tf.shade(agg)\n\n        Aggregate one area region across all rows, that starts with\n        coordinates df.A1 by df.B1 and is filled to the line with coordinates\n        df.A1 by df.B2\n        >>> agg = cvs.area(df, x='A1', y='B1', y_stack='B2', # doctest: +SKIP\n        ...                agg=ds.count(), axis=0)\n        ... tf.shade(agg)\n\n        Aggregate two area regions across all rows. The first starting\n        with coordinates df.A1 by df.B1 and the second with coordinates\n        df.A2 by df.B2. Both regions are filled to the y=0 line\n        >>> agg = cvs.area(df, x=['A1', 'A2'], y=['B1', 'B2'],  # doctest: +SKIP\n                           agg=ds.count(), axis=0)\n        ... tf.shade(agg)\n\n        Aggregate two area regions across all rows where the regions share the\n        same x coordinates. The first region will start with coordinates\n        df.A1 by df.B1 and the second will start with coordinates\n        df.A1 by df.B2. Both regions are filled to the y=0 line\n        >>> agg = cvs.area(df, x='A1', y=['B1', 'B2'], agg=ds.count(), axis=0)  # doctest: +SKIP\n        ... tf.shade(agg)\n\n        Aggregate 6 length-2 area regions, one per row, where the ith region\n        starts with coordinates [df.A1[i], df.A2[i]] by [df.B1[i], df.B2[i]]\n        and is filled to the y=0 line\n        >>> agg = cvs.area(df, x=['A1', 'A2'], y=['B1', 'B2'],  # doctest: +SKIP\n                           agg=ds.count(), axis=1)\n        ... tf.shade(agg)\n\n        Aggregate 6 length-4 area regions, one per row, where the\n        starting x coordinates of every region are [0, 1, 2, 3] and\n        the starting y coordinates of the ith region are\n        [df.A1[i], df.A2[i], df.B1[i], df.B2[i]].  All regions are filled to\n        the y=0 line\n        >>> agg = cvs.area(df,  # doctest: +SKIP\n        ...                x=np.arange(4),\n        ...                y=['A1', 'A2', 'B1', 'B2'],\n        ...                agg=ds.count(),\n        ...                axis=1)\n        ... tf.shade(agg)\n\n        Aggregate RaggedArrays of variable length area regions, one per row.\n        The starting coordinates of the ith region are df_ragged.A1 by\n        df_ragged.B1 and the regions are filled to the y=0 line.\n        (requires pandas >= 0.24.0)\n        >>> df_ragged = pd.DataFrame({  # doctest: +SKIP\n        ...    'A1': pd.array([\n        ...        [1, 1.5], [2, 2.5, 3], [1.5, 2, 3, 4], [3.2, 4, 5]],\n        ...        dtype='Ragged[float32]'),\n        ...    'B1': pd.array([\n        ...        [10, 12], [11, 14, 13], [10, 7, 9, 10], [7, 8, 12]],\n        ...        dtype='Ragged[float32]'),\n        ...    'B2': pd.array([\n        ...        [6, 10], [9, 10, 18], [9, 5, 6, 8], [4, 5, 11]],\n        ...        dtype='Ragged[float32]'),\n        ...    'group': pd.Categorical([0, 1, 2, 1])\n        ... })\n        ...\n        ... agg = cvs.area(df_ragged, x='A1', y='B1', agg=ds.count(), axis=1)\n        ... tf.shade(agg)\n\n        Instead of filling regions to the y=0 line, fill to the line with\n        coordinates df_ragged.A1 by df_ragged.B2\n        >>> agg = cvs.area(df_ragged, x='A1', y='B1', y_stack='B2', # doctest: +SKIP\n        ...                agg=ds.count(), axis=1)\n        ... tf.shade(agg)\n\n        (requires pandas >= 0.24.0)\n        \"\"\"\n        from .glyphs import AreaToZeroAxis0, AreaToLineAxis0, AreaToZeroAxis0Multi, AreaToLineAxis0Multi, AreaToZeroAxis1, AreaToLineAxis1, AreaToZeroAxis1XConstant, AreaToLineAxis1XConstant, AreaToZeroAxis1YConstant, AreaToLineAxis1YConstant, AreaToZeroAxis1Ragged, AreaToLineAxis1Ragged\n        from .reductions import any as any_rdn\n        if agg is None:\n            agg = any_rdn()\n        orig_x, orig_y, orig_y_stack = (x, y, y_stack)\n        x, y, y_stack = _broadcast_column_specifications(x, y, y_stack)\n        if axis == 0:\n            if y_stack is None:\n                if isinstance(x, (Number, str)) and isinstance(y, (Number, str)):\n                    glyph = AreaToZeroAxis0(x, y)\n                elif isinstance(x, (list, tuple)) and isinstance(y, (list, tuple)):\n                    glyph = AreaToZeroAxis0Multi(tuple(x), tuple(y))\n                else:\n                    raise ValueError('\\nInvalid combination of x and y arguments to Canvas.area when axis=0.\\n    Received:\\n        x: {x}\\n        y: {y}\\nSee docstring for more information on valid usage'.format(x=repr(x), y=repr(y)))\n            elif isinstance(x, (Number, str)) and isinstance(y, (Number, str)) and isinstance(y_stack, (Number, str)):\n                glyph = AreaToLineAxis0(x, y, y_stack)\n            elif isinstance(x, (list, tuple)) and isinstance(y, (list, tuple)) and isinstance(y_stack, (list, tuple)):\n                glyph = AreaToLineAxis0Multi(tuple(x), tuple(y), tuple(y_stack))\n            else:\n                raise ValueError('\\nInvalid combination of x, y, and y_stack arguments to Canvas.area when axis=0.\\n    Received:\\n        x: {x}\\n        y: {y}\\n        y_stack: {y_stack}\\nSee docstring for more information on valid usage'.format(x=repr(orig_x), y=repr(orig_y), y_stack=repr(orig_y_stack)))\n        elif axis == 1:\n            if y_stack is None:\n                if isinstance(x, (list, tuple)) and isinstance(y, (list, tuple)):\n                    glyph = AreaToZeroAxis1(tuple(x), tuple(y))\n                elif isinstance(x, np.ndarray) and isinstance(y, (list, tuple)):\n                    glyph = AreaToZeroAxis1XConstant(x, tuple(y))\n                elif isinstance(x, (list, tuple)) and isinstance(y, np.ndarray):\n                    glyph = AreaToZeroAxis1YConstant(tuple(x), y)\n                elif isinstance(x, (Number, str)) and isinstance(y, (Number, str)):\n                    glyph = AreaToZeroAxis1Ragged(x, y)\n                else:\n                    raise ValueError('\\nInvalid combination of x and y arguments to Canvas.area when axis=1.\\n    Received:\\n        x: {x}\\n        y: {y}\\nSee docstring for more information on valid usage'.format(x=repr(x), y=repr(y)))\n            elif isinstance(x, (list, tuple)) and isinstance(y, (list, tuple)) and isinstance(y_stack, (list, tuple)):\n                glyph = AreaToLineAxis1(tuple(x), tuple(y), tuple(y_stack))\n            elif isinstance(x, np.ndarray) and isinstance(y, (list, tuple)) and isinstance(y_stack, (list, tuple)):\n                glyph = AreaToLineAxis1XConstant(x, tuple(y), tuple(y_stack))\n            elif isinstance(x, (list, tuple)) and isinstance(y, np.ndarray) and isinstance(y_stack, np.ndarray):\n                glyph = AreaToLineAxis1YConstant(tuple(x), y, y_stack)\n            elif isinstance(x, (Number, str)) and isinstance(y, (Number, str)) and isinstance(y_stack, (Number, str)):\n                glyph = AreaToLineAxis1Ragged(x, y, y_stack)\n            else:\n                raise ValueError('\\nInvalid combination of x, y, and y_stack arguments to Canvas.area when axis=1.\\n    Received:\\n        x: {x}\\n        y: {y}\\n        y_stack: {y_stack}\\nSee docstring for more information on valid usage'.format(x=repr(orig_x), y=repr(orig_y), y_stack=repr(orig_y_stack)))\n        else:\n            raise ValueError('\\nThe axis argument to Canvas.area must be 0 or 1\\n    Received: {axis}'.format(axis=axis))\n        return bypixel(source, self, glyph, agg)\n\n    def polygons(self, source, geometry, agg=None):\n        \"\"\"Compute a reduction by pixel, mapping data to pixels as one or\n        more filled polygons.\n\n        Parameters\n        ----------\n        source : xarray.DataArray or Dataset\n            The input datasource.\n        geometry : str\n            Column name of a PolygonsArray of the coordinates of each line.\n        agg : Reduction, optional\n            Reduction to compute. Default is ``any()``.\n\n        Returns\n        -------\n        data : xarray.DataArray\n\n        Examples\n        --------\n        >>> import datashader as ds  # doctest: +SKIP\n        ... import datashader.transfer_functions as tf\n        ... from spatialpandas.geometry import PolygonArray\n        ... from spatialpandas import GeoDataFrame\n        ... import pandas as pd\n        ...\n        ... polygons = PolygonArray([\n        ...     # First Element\n        ...     [[0, 0, 1, 0, 2, 2, -1, 4, 0, 0],  # Filled quadrilateral (CCW order)\n        ...      [0.5, 1,  1, 2,  1.5, 1.5,  0.5, 1],     # Triangular hole (CW order)\n        ...      [0, 2, 0, 2.5, 0.5, 2.5, 0.5, 2, 0, 2],  # Rectangular hole (CW order)\n        ...      [2.5, 3, 3.5, 3, 3.5, 4, 2.5, 3],  # Filled triangle\n        ...     ],\n        ...\n        ...     # Second Element\n        ...     [[3, 0, 3, 2, 4, 2, 4, 0, 3, 0],  # Filled rectangle (CCW order)\n        ...      # Rectangular hole (CW order)\n        ...      [3.25, 0.25, 3.75, 0.25, 3.75, 1.75, 3.25, 1.75, 3.25, 0.25],\n        ...     ]\n        ... ])\n        ...\n        ... df = GeoDataFrame({'polygons': polygons, 'v': range(len(polygons))})\n        ...\n        ... cvs = ds.Canvas()\n        ... agg = cvs.polygons(df, geometry='polygons', agg=ds.sum('v'))\n        ... tf.shade(agg)\n        \"\"\"\n        from .glyphs import PolygonGeom\n        from .reductions import any as any_rdn\n        if spatialpandas and isinstance(source, spatialpandas.dask.DaskGeoDataFrame):\n            x_range = self.x_range if self.x_range is not None else (None, None)\n            y_range = self.y_range if self.y_range is not None else (None, None)\n            source = source.cx_partitions[slice(*x_range), slice(*y_range)]\n            glyph = PolygonGeom(geometry)\n        elif spatialpandas and isinstance(source, spatialpandas.GeoDataFrame):\n            glyph = PolygonGeom(geometry)\n        elif (geopandas_source := self._source_from_geopandas(source)) is not None:\n            source = geopandas_source\n            from .glyphs.polygon import GeopandasPolygonGeom\n            glyph = GeopandasPolygonGeom(geometry)\n        else:\n            raise ValueError(f'source must be an instance of spatialpandas.GeoDataFrame, spatialpandas.dask.DaskGeoDataFrame, geopandas.GeoDataFrame or dask_geopandas.GeoDataFrame, not {type(source)}')\n        if agg is None:\n            agg = any_rdn()\n        return bypixel(source, self, glyph, agg)\n\n    def quadmesh(self, source, x=None, y=None, agg=None):\n        \"\"\"Samples a recti- or curvi-linear quadmesh by canvas size and bounds.\n\n        Parameters\n        ----------\n        source : xarray.DataArray or Dataset\n            The input datasource.\n        x, y : str\n            Column names for the x and y coordinates of each point.\n        agg : Reduction, optional\n            Reduction to compute. Default is ``mean()``. Note that agg is ignored when\n            upsampling.\n\n        Returns\n        -------\n        data : xarray.DataArray\n        \"\"\"\n        from .glyphs import QuadMeshRaster, QuadMeshRectilinear, QuadMeshCurvilinear\n        from .reductions import mean as mean_rnd\n        if isinstance(source, Dataset):\n            if agg is None or agg.column is None:\n                name = list(source.data_vars)[0]\n            else:\n                name = agg.column\n            source = source[[name]]\n        elif isinstance(source, DataArray):\n            name = source.name\n            source = source.to_dataset()\n        else:\n            raise ValueError('Invalid input type')\n        if agg is None:\n            agg = mean_rnd(name)\n        if x is None and y is None:\n            y, x = source[name].dims\n        elif not x or not y:\n            raise ValueError('Either specify both x and y coordinatesor allow them to be inferred.')\n        yarr, xarr = (source[y], source[x])\n        if (yarr.ndim > 1 or xarr.ndim > 1) and xarr.dims != yarr.dims:\n            raise ValueError('Ensure that x- and y-coordinate arrays share the same dimensions. x-coordinates are indexed by %s dims while y-coordinates are indexed by %s dims.' % (xarr.dims, yarr.dims))\n        if name is not None and agg.column is not None and (agg.column != name):\n            raise ValueError('DataArray name %r does not match supplied reduction %s.' % (source.name, agg))\n        if xarr.ndim == 1:\n            xaxis_linear = self.x_axis is _axis_lookup['linear']\n            yaxis_linear = self.y_axis is _axis_lookup['linear']\n            even_yspacing = np.allclose(yarr, np.linspace(yarr[0].data, yarr[-1].data, len(yarr)))\n            even_xspacing = np.allclose(xarr, np.linspace(xarr[0].data, xarr[-1].data, len(xarr)))\n            if xaxis_linear and yaxis_linear and even_xspacing and even_yspacing:\n                glyph = QuadMeshRaster(x, y, name)\n                upsample_width, upsample_height = glyph.is_upsample(source, x, y, name, self.x_range, self.y_range, self.plot_width, self.plot_height)\n                if upsample_width and upsample_height:\n                    agg = rd._upsample(name)\n                    return bypixel(source, self, glyph, agg)\n                elif not upsample_width and (not upsample_height):\n                    return bypixel(source, self, glyph, agg)\n                else:\n                    glyph = QuadMeshRectilinear(x, y, name)\n                    return bypixel(source, self, glyph, agg)\n            else:\n                glyph = QuadMeshRectilinear(x, y, name)\n                return bypixel(source, self, glyph, agg)\n        elif xarr.ndim == 2:\n            glyph = QuadMeshCurvilinear(x, y, name)\n            return bypixel(source, self, glyph, agg)\n        else:\n            raise ValueError('x- and y-coordinate arrays must have 1 or 2 dimensions.\\n    Received arrays with dimensions: {dims}'.format(dims=list(xarr.dims)))\n\n    def trimesh(self, vertices, simplices, mesh=None, agg=None, interp=True, interpolate=None):\n        \"\"\"Compute a reduction by pixel, mapping data to pixels as a triangle.\n\n        >>> import datashader as ds\n        >>> verts = pd.DataFrame({'x': [0, 5, 10],\n        ...                         'y': [0, 10, 0],\n        ...                         'weight': [1, 5, 3]},\n        ...                        columns=['x', 'y', 'weight'])\n        >>> tris = pd.DataFrame({'v0': [2], 'v1': [0], 'v2': [1]},\n        ...                       columns=['v0', 'v1', 'v2'])\n        >>> cvs = ds.Canvas(x_range=(verts.x.min(), verts.x.max()),\n        ...                 y_range=(verts.y.min(), verts.y.max()))\n        >>> untested = cvs.trimesh(verts, tris)\n\n        Parameters\n        ----------\n        vertices : pandas.DataFrame, dask.DataFrame\n            The input datasource for triangle vertex coordinates. These can be\n            interpreted as the x/y coordinates of the vertices, with optional\n            weights for value interpolation. Columns should be ordered\n            corresponding to 'x', 'y', followed by zero or more (optional)\n            columns containing vertex values. The rows need not be ordered.\n            The column data types must be floating point or integer.\n        simplices : pandas.DataFrame, dask.DataFrame\n            The input datasource for triangle (simplex) definitions. These can\n            be interpreted as rows of ``vertices``, aka positions in the\n            ``vertices`` index. Columns should be ordered corresponding to\n            'vertex0', 'vertex1', and 'vertex2'. Order of the vertices can be\n            clockwise or counter-clockwise; it does not matter as long as the\n            data is consistent for all simplices in the dataframe. The\n            rows need not be ordered.  The data type for the first\n            three columns in the dataframe must be integer.\n        agg : Reduction, optional\n            Reduction to compute. Default is ``mean()``.\n        mesh : pandas.DataFrame, optional\n            An ordered triangle mesh in tabular form, used for optimization\n            purposes. This dataframe is expected to have come from\n            ``datashader.utils.mesh()``. If this argument is not None, the first\n            two arguments are ignored.\n        interpolate : str, optional default=linear\n            Method to use for interpolation between specified values. ``nearest``\n            means to use a single value for the whole triangle, and ``linear``\n            means to do bilinear interpolation of the pixels within each\n            triangle (a weighted average of the vertex values). For\n            backwards compatibility, also accepts ``interp=True`` for ``linear``\n            and ``interp=False`` for ``nearest``.\n        \"\"\"\n        from .glyphs import Triangles\n        from .reductions import mean as mean_rdn\n        from .utils import mesh as create_mesh\n        source = mesh\n        if interpolate is not None:\n            if interpolate == 'linear':\n                interp = True\n            elif interpolate == 'nearest':\n                interp = False\n            else:\n                raise ValueError('Invalid interpolate method: options include {}'.format(['linear', 'nearest']))\n        if source is None:\n            source = create_mesh(vertices, simplices)\n        verts_have_weights = len(vertices.columns) > 2\n        if verts_have_weights:\n            weight_col = vertices.columns[2]\n        else:\n            weight_col = simplices.columns[3]\n        if agg is None:\n            agg = mean_rdn(weight_col)\n        elif agg.column is None:\n            agg.column = weight_col\n        cols = source.columns\n        x, y, weights = (cols[0], cols[1], cols[2:])\n        return bypixel(source, self, Triangles(x, y, weights, weight_type=verts_have_weights, interp=interp), agg)\n\n    def raster(self, source, layer=None, upsample_method='linear', downsample_method=rd.mean(), nan_value=None, agg=None, interpolate=None, chunksize=None, max_mem=None):\n        \"\"\"Sample a raster dataset by canvas size and bounds.\n\n        Handles 2D or 3D xarray DataArrays, assuming that the last two\n        array dimensions are the y- and x-axis that are to be\n        resampled. If a 3D array is supplied a layer may be specified\n        to resample to select the layer along the first dimension to\n        resample.\n\n        Missing values (those having the value indicated by the\n        \"nodata\" attribute of the raster) are replaced with `NaN` if\n        floats, and 0 if int.\n\n        Also supports resampling out-of-core DataArrays backed by dask\n        Arrays. By default it will try to maintain the same chunksize\n        in the output array but a custom chunksize may be provided.\n        If there are memory constraints they may be defined using the\n        max_mem parameter, which determines how large the chunks in\n        memory may be.\n\n        Parameters\n        ----------\n        source : xarray.DataArray or xr.Dataset\n            2D or 3D labelled array (if Dataset, the agg reduction must\n            define the data variable).\n        layer : float\n            For a 3D array, value along the z dimension : optional default=None\n        ds_method : str (optional)\n            Grid cell aggregation method for a possible downsampling.\n        us_method : str (optional)\n            Grid cell interpolation method for a possible upsampling.\n        nan_value : int or float, optional\n            Optional nan_value which will be masked out when applying\n            the resampling.\n        agg : Reduction, optional default=mean()\n            Resampling mode when downsampling raster. The supported\n            options include: first, last, mean, mode, var, std, min,\n            The agg can be specified as either a string name or as a\n            reduction function, but note that the function object will\n            be used only to extract the agg type (mean, max, etc.) and\n            the optional column name; the hardcoded raster code\n            supports only a fixed set of reductions and ignores the\n            actual code of the provided agg.\n        interpolate : str, optional  default=linear\n            Resampling mode when upsampling raster.\n            options include: nearest, linear.\n        chunksize : tuple(int, int) (optional)\n            Size of the output chunks. By default this the chunk size is\n            inherited from the *src* array.\n        max_mem : int (optional)\n            The maximum number of bytes that should be loaded into memory\n            during the regridding operation.\n\n        Returns\n        -------\n        data : xarray.Dataset\n        \"\"\"\n        if agg is None:\n            agg = downsample_method\n        if interpolate is None:\n            interpolate = upsample_method\n        upsample_methods = ['nearest', 'linear']\n        downsample_methods = {'first': 'first', rd.first: 'first', 'last': 'last', rd.last: 'last', 'mode': 'mode', rd.mode: 'mode', 'mean': 'mean', rd.mean: 'mean', 'var': 'var', rd.var: 'var', 'std': 'std', rd.std: 'std', 'min': 'min', rd.min: 'min', 'max': 'max', rd.max: 'max'}\n        if interpolate not in upsample_methods:\n            raise ValueError('Invalid interpolate method: options include {}'.format(upsample_methods))\n        if not isinstance(source, (DataArray, Dataset)):\n            raise ValueError('Expected xarray DataArray or Dataset as the data source, found %s.' % type(source).__name__)\n        column = None\n        if isinstance(agg, rd.Reduction):\n            agg, column = (type(agg), agg.column)\n            if isinstance(source, DataArray) and column is not None and (source.name != column):\n                agg_repr = '%s(%r)' % (agg.__name__, column)\n                raise ValueError('DataArray name %r does not match supplied reduction %s.' % (source.name, agg_repr))\n        if isinstance(source, Dataset):\n            data_vars = list(source.data_vars)\n            if column is None:\n                raise ValueError('When supplying a Dataset the agg reduction must specify the variable to aggregate. Available data_vars include: %r.' % data_vars)\n            elif column not in source.data_vars:\n                raise KeyError('Supplied reduction column %r not found in Dataset, expected one of the following data variables: %r.' % (column, data_vars))\n            source = source[column]\n        if agg not in downsample_methods.keys():\n            raise ValueError('Invalid aggregation method: options include {}'.format(list(downsample_methods.keys())))\n        ds_method = downsample_methods[agg]\n        if source.ndim not in [2, 3]:\n            raise ValueError('Raster aggregation expects a 2D or 3D DataArray, found %s dimensions' % source.ndim)\n        res = calc_res(source)\n        ydim, xdim = source.dims[-2:]\n        xvals, yvals = (source[xdim].values, source[ydim].values)\n        left, bottom, right, top = calc_bbox(xvals, yvals, res)\n        if layer is not None:\n            source = source.sel(**{source.dims[0]: layer})\n        array = orient_array(source, res)\n        if nan_value is not None:\n            mask = array == nan_value\n            array = np.ma.masked_array(array, mask=mask, fill_value=nan_value)\n            fill_value = nan_value\n        elif np.issubdtype(source.dtype, np.integer):\n            fill_value = 0\n        else:\n            fill_value = np.nan\n        if self.x_range is None:\n            self.x_range = (left, right)\n        if self.y_range is None:\n            self.y_range = (bottom, top)\n        xmin = max(self.x_range[0], left)\n        ymin = max(self.y_range[0], bottom)\n        xmax = min(self.x_range[1], right)\n        ymax = min(self.y_range[1], top)\n        width_ratio = min((xmax - xmin) / (self.x_range[1] - self.x_range[0]), 1)\n        height_ratio = min((ymax - ymin) / (self.y_range[1] - self.y_range[0]), 1)\n        if np.isclose(width_ratio, 0) or np.isclose(height_ratio, 0):\n            raise ValueError('Canvas x_range or y_range values do not match closely enough with the data source to be able to accurately rasterize. Please provide ranges that are more accurate.')\n        w = max(int(round(self.plot_width * width_ratio)), 1)\n        h = max(int(round(self.plot_height * height_ratio)), 1)\n        cmin, cmax = get_indices(xmin, xmax, xvals, res[0])\n        rmin, rmax = get_indices(ymin, ymax, yvals, res[1])\n        kwargs = dict(w=w, h=h, ds_method=ds_method, us_method=interpolate, fill_value=fill_value)\n        if array.ndim == 2:\n            source_window = array[rmin:rmax + 1, cmin:cmax + 1]\n            if ds_method in ['var', 'std']:\n                source_window = source_window.astype('f')\n            if da and isinstance(source_window, da.Array):\n                data = resample_2d_distributed(source_window, chunksize=chunksize, max_mem=max_mem, **kwargs)\n            else:\n                data = resample_2d(source_window, **kwargs)\n            layers = 1\n        else:\n            source_window = array[:, rmin:rmax + 1, cmin:cmax + 1]\n            if ds_method in ['var', 'std']:\n                source_window = source_window.astype('f')\n            arrays = []\n            for arr in source_window:\n                if da and isinstance(arr, da.Array):\n                    arr = resample_2d_distributed(arr, chunksize=chunksize, max_mem=max_mem, **kwargs)\n                else:\n                    arr = resample_2d(arr, **kwargs)\n                arrays.append(arr)\n            data = np.dstack(arrays)\n            layers = len(arrays)\n        if w != self.plot_width or h != self.plot_height:\n            num_height = self.plot_height - h\n            num_width = self.plot_width - w\n            lpad = xmin - self.x_range[0]\n            rpad = self.x_range[1] - xmax\n            lpct = lpad / (lpad + rpad) if lpad + rpad > 0 else 0\n            left = max(int(np.ceil(num_width * lpct)), 0)\n            right = max(num_width - left, 0)\n            lshape, rshape = ((self.plot_height, left), (self.plot_height, right))\n            if layers > 1:\n                lshape, rshape = (lshape + (layers,), rshape + (layers,))\n            left_pad = np.full(lshape, fill_value, source_window.dtype)\n            right_pad = np.full(rshape, fill_value, source_window.dtype)\n            tpad = ymin - self.y_range[0]\n            bpad = self.y_range[1] - ymax\n            tpct = tpad / (tpad + bpad) if tpad + bpad > 0 else 0\n            top = max(int(np.ceil(num_height * tpct)), 0)\n            bottom = max(num_height - top, 0)\n            tshape, bshape = ((top, w), (bottom, w))\n            if layers > 1:\n                tshape, bshape = (tshape + (layers,), bshape + (layers,))\n            top_pad = np.full(tshape, fill_value, source_window.dtype)\n            bottom_pad = np.full(bshape, fill_value, source_window.dtype)\n            concat = da.concatenate if da and isinstance(data, da.Array) else np.concatenate\n            arrays = (top_pad, data) if top_pad.shape[0] > 0 else (data,)\n            if bottom_pad.shape[0] > 0:\n                arrays += (bottom_pad,)\n            data = concat(arrays, axis=0) if len(arrays) > 1 else arrays[0]\n            arrays = (left_pad, data) if left_pad.shape[1] > 0 else (data,)\n            if right_pad.shape[1] > 0:\n                arrays += (right_pad,)\n            data = concat(arrays, axis=1) if len(arrays) > 1 else arrays[0]\n        if res[1] > 0:\n            data = data[::-1]\n        if res[0] < 0:\n            data = data[:, ::-1]\n        close_x = np.allclose([left, right], self.x_range) and np.size(xvals) == self.plot_width\n        close_y = np.allclose([bottom, top], self.y_range) and np.size(yvals) == self.plot_height\n        if close_x:\n            xs = xvals\n        else:\n            x_st = self.x_axis.compute_scale_and_translate(self.x_range, self.plot_width)\n            xs = self.x_axis.compute_index(x_st, self.plot_width)\n            if res[0] < 0:\n                xs = xs[::-1]\n        if close_y:\n            ys = yvals\n        else:\n            y_st = self.y_axis.compute_scale_and_translate(self.y_range, self.plot_height)\n            ys = self.y_axis.compute_index(y_st, self.plot_height)\n            if res[1] > 0:\n                ys = ys[::-1]\n        coords = {xdim: xs, ydim: ys}\n        dims = [ydim, xdim]\n        attrs = dict(res=res[0], x_range=self.x_range, y_range=self.y_range)\n        for a in ['_FillValue', 'missing_value', 'fill_value', 'nodata', 'NODATA']:\n            if a in source.attrs:\n                attrs['nodata'] = source.attrs[a]\n                break\n        if 'nodata' not in attrs:\n            try:\n                attrs['nodata'] = source.attrs['nodatavals'][0]\n            except Exception:\n                pass\n        if data.ndim == 3:\n            data = data.transpose([2, 0, 1])\n            layer_dim = source.dims[0]\n            coords[layer_dim] = source.coords[layer_dim]\n            dims = [layer_dim] + dims\n        return DataArray(data, coords=coords, dims=dims, attrs=attrs)\n\n    def validate_ranges(self, x_range, y_range):\n        self.x_axis.validate(x_range)\n        self.y_axis.validate(y_range)\n\n    def validate_size(self, width, height):\n        if width <= 0 or height <= 0:\n            raise ValueError('Invalid size: plot_width and plot_height must be bigger than 0')\n\n    def validate(self):\n        \"\"\"Check that parameter settings are valid for this object\"\"\"\n        self.validate_ranges(self.x_range, self.y_range)\n        self.validate_size(self.plot_width, self.plot_height)\n\n    def _source_from_geopandas(self, source):\n        \"\"\"\n        Check if the specified source is a geopandas or dask-geopandas GeoDataFrame.\n        If so, spatially filter the source and return it.\n        If not, return None.\n        \"\"\"\n        dfs = []\n        with contextlib.suppress(ImportError):\n            import geopandas\n            dfs.append(geopandas.GeoDataFrame)\n        with contextlib.suppress(ImportError):\n            import dask_geopandas\n            if Version(dask_geopandas.__version__) >= Version('0.4.0'):\n                from dask_geopandas.core import GeoDataFrame as gdf1\n                dfs.append(gdf1)\n                with contextlib.suppress(TypeError):\n                    from dask_geopandas.expr import GeoDataFrame as gdf2\n                    dfs.append(gdf2)\n            else:\n                dfs.append(dask_geopandas.GeoDataFrame)\n        if isinstance(source, tuple(dfs)):\n            from shapely import __version__ as shapely_version\n            if Version(shapely_version) < Version('2.0.0'):\n                raise ImportError('Use of GeoPandas in Datashader requires Shapely >= 2.0.0')\n            if isinstance(source, geopandas.GeoDataFrame):\n                x_range = self.x_range if self.x_range is not None else (-np.inf, np.inf)\n                y_range = self.y_range if self.y_range is not None else (-np.inf, np.inf)\n                from shapely import box\n                query = source.sindex.query(box(x_range[0], y_range[0], x_range[1], y_range[1]))\n                source = source.iloc[query]\n            else:\n                x_range = self.x_range if self.x_range is not None else (None, None)\n                y_range = self.y_range if self.y_range is not None else (None, None)\n                source = source.cx[slice(*x_range), slice(*y_range)]\n            return source\n        else:\n            return None\n\ndef bypixel(source, canvas, glyph, agg, *, antialias=False):\n    \"\"\"Compute an aggregate grouped by pixel sized bins.\n\n    Aggregate input data ``source`` into a grid with shape and axis matching\n    ``canvas``, mapping data to bins by ``glyph``, and aggregating by reduction\n    ``agg``.\n\n    Parameters\n    ----------\n    source : pandas.DataFrame, dask.DataFrame\n        Input datasource\n    canvas : Canvas\n    glyph : Glyph\n    agg : Reduction\n    \"\"\"\n    source, dshape = _bypixel_sanitise(source, glyph, agg)\n    schema = dshape.measure\n    glyph.validate(schema)\n    agg.validate(schema)\n    canvas.validate()\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', 'All-NaN (slice|axis) encountered')\n        return bypixel.pipeline(source, schema, canvas, glyph, agg, antialias=antialias)\n\ndef _bypixel_sanitise(source, glyph, agg):\n    if isinstance(source, DataArray) and source.ndim == 1:\n        if not source.name:\n            source.name = 'value'\n        source = source.reset_coords()\n    if isinstance(source, Dataset) and len(source.dims) == 1:\n        columns = list(source.coords.keys()) + list(source.data_vars.keys())\n        cols_to_keep = _cols_to_keep(columns, glyph, agg)\n        source = source.drop_vars([col for col in columns if col not in cols_to_keep])\n        if dd:\n            source = source.to_dask_dataframe()\n        else:\n            source = source.to_dataframe()\n    if isinstance(source, pd.DataFrame) or (cudf and isinstance(source, cudf.DataFrame)):\n        cols_to_keep = _cols_to_keep(source.columns, glyph, agg)\n        if len(cols_to_keep) < len(source.columns):\n            sindex = None\n            from .glyphs.polygon import PolygonGeom\n            if isinstance(glyph, PolygonGeom):\n                sindex = getattr(source[glyph.geometry].array, '_sindex', None)\n            source = source[cols_to_keep]\n            if sindex is not None and getattr(source[glyph.geometry].array, '_sindex', None) is None:\n                source[glyph.geometry].array._sindex = sindex\n        dshape = dshape_from_pandas(source)\n    elif dd and isinstance(source, dd.DataFrame):\n        dshape, source = dshape_from_dask(source)\n    elif isinstance(source, Dataset):\n        dshape = dshape_from_xarray_dataset(source)\n    else:\n        raise ValueError('source must be a pandas or dask DataFrame')\n    return (source, dshape)\n\ndef _cols_to_keep(columns, glyph, agg):\n    \"\"\"\n    Return which columns from the supplied data source are kept as they are\n    needed by the specified agg. Excludes any SpecialColumn.\n    \"\"\"\n    cols_to_keep = dict({col: False for col in columns})\n    for col in glyph.required_columns():\n        cols_to_keep[col] = True\n\n    def recurse(cols_to_keep, agg):\n        if hasattr(agg, 'values'):\n            for subagg in agg.values:\n                recurse(cols_to_keep, subagg)\n        elif hasattr(agg, 'columns'):\n            for column in agg.columns:\n                if column not in (None, rd.SpecialColumn.RowIndex):\n                    cols_to_keep[column] = True\n        elif agg.column not in (None, rd.SpecialColumn.RowIndex):\n            cols_to_keep[agg.column] = True\n    recurse(cols_to_keep, agg)\n    return [col for col, keepit in cols_to_keep.items() if keepit]\n\ndef _broadcast_column_specifications(*args):\n    lengths = {len(a) for a in args if isinstance(a, (list, tuple))}\n    if len(lengths) != 1:\n        return args\n    else:\n        n = lengths.pop()\n        return tuple(((arg,) * n if isinstance(arg, (Number, str)) else arg for arg in args))\nbypixel.pipeline = Dispatcher()"
  }
}