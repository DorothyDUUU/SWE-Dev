{
  "dir_path": "/app/elasticsearch_curator",
  "package_name": "elasticsearch_curator",
  "sample_name": "elasticsearch_curator-test_helpers_testers",
  "src_dir": "curator/",
  "test_dir": "tests/",
  "test_file": "tests/unit/test_helpers_testers.py",
  "test_code": "\"\"\"Unit tests for utils\"\"\"\nfrom unittest import TestCase\nimport pytest\nfrom unittest.mock import Mock\nfrom elastic_transport import ApiResponseMeta\nfrom elasticsearch8 import Elasticsearch\nfrom elasticsearch8.exceptions import AuthenticationException, NotFoundError\nfrom curator.exceptions import (\n     ConfigurationError, FailedExecution, MissingArgument, RepositoryException,\n     SearchableSnapshotException)\nfrom curator.helpers.testers import (\n    has_lifecycle_name, is_idx_partial, repository_exists, rollable_alias, snapshot_running,\n    validate_filters, verify_client_object, verify_repository)\n\nFAKE_FAIL = Exception('Simulated Failure')\n\nclass TestRepositoryExists(TestCase):\n    \"\"\"TestRepositoryExists\n\n    Test helpers.testers.repository_exists functionality.\n    \"\"\"\n    def test_missing_arg(self):\n        \"\"\"test_missing_arg\n\n        Should raise an exception if the repository isn't passed as an arg\n        \"\"\"\n        client = Mock()\n        with pytest.raises(MissingArgument, match=r'No value for \"repository\" provided'):\n            repository_exists(client)\n    def test_repository_in_results(self):\n        \"\"\"test_repository_in_results\n\n        Should return ``True`` if the passed repository exists\n        \"\"\"\n        client = Mock()\n        client.snapshot.get_repository.return_value = {'repo':{'foo':'bar'}}\n        assert repository_exists(client, repository=\"repo\")\n    def test_repo_not_in_results(self):\n        \"\"\"test_repo_not_in_results\n\n        Should return ``False`` if the passed repository does not exist\n        \"\"\"\n        client = Mock()\n        client.snapshot.get_repository.return_value = {'not_your_repo':{'foo':'bar'}}\n        assert not repository_exists(client, repository=\"repo\")\n\nclass TestRollableAlias(TestCase):\n    \"\"\"TestRollableAlias\n\n    Test helpers.testers.rollable_alias functionality.\n    \"\"\"\n    def test_return_false_if_no_alias(self):\n        \"\"\"test_return_false_if_no_alias\n\n        Should return ``False`` with a simulated Exception being raised\n        \"\"\"\n        err = 'simulated error'\n        client = Mock()\n        client.info.return_value = {'version': {'number': '8.3.3'} }\n        client.indices.get_alias.return_value = {}\n        client.indices.get_alias.side_effect = NotFoundError(404, err, err)\n        assert not rollable_alias(client, 'foo')\n    def test_return_false_too_many_indices(self):\n        \"\"\"test_return_false_too_many_indices\n\n        Should return ``False`` if alias associated with too many indices\n        \"\"\"\n        retval = {'index-a': {'aliases': {'foo': {}}}, 'index-b': {'aliases': {'foo': {}}}}\n        client = Mock()\n        client.info.return_value = {'version': {'number': '8.3.3'} }\n        client.indices.get_alias.return_value = retval\n        assert not rollable_alias(client, 'foo')\n    def test_return_false_non_numeric(self):\n        \"\"\"test_return_false_non_numeric\n\n        Should return ``False`` if index name doesn't end in rollable digits\n        \"\"\"\n        retval = {'index-a': {'aliases': {'foo': {}}}}\n        client = Mock()\n        client.info.return_value = {'version': {'number': '8.3.3'} }\n        client.indices.get_alias.return_value = retval\n        assert not rollable_alias(client, 'foo')\n    def test_return_true_two_digits(self):\n        \"\"\"test_return_true_two_digits\n\n        Should return ``True`` if the index ends in rollable digits\n        \"\"\"\n        retval = {'index-00001': {'aliases': {'foo': {}}}}\n        client = Mock()\n        client.info.return_value = {'version': {'number': '8.3.3'} }\n        client.indices.get_alias.return_value = retval\n        assert rollable_alias(client, 'foo')\n    def test_return_true_hyphenated(self):\n        \"\"\"test_return_true_hyphenated\n\n        Should return ``True`` if the index has a rollable, hyphenated name\n        \"\"\"\n        retval = {'index-2017.03.07-1': {'aliases': {'foo': {}}}}\n        client = Mock()\n        client.info.return_value = {'version': {'number': '8.3.3'} }\n        client.indices.get_alias.return_value = retval\n        assert rollable_alias(client, 'foo')\n\nclass TestSnapshotRunning(TestCase):\n    \"\"\"TestSnapshotRunning\n\n    Test helpers.testers.snapshot_running functionality\n    \"\"\"\n    # :pylint disable=line-too-long\n    def test_true(self):\n        \"\"\"test_true\n\n        Should return ``True`` when a snapshot is in progress/running.\n        \"\"\"\n        client = Mock()\n        client.snapshot.status.return_value = {'snapshots': ['running']}\n        # self.assertTrue(snapshot_running(client))\n        assert snapshot_running(client)\n    def test_false(self):\n        \"\"\"test_False\n\n        Should return ``False`` when a snapshot is not in progress/running.\n        \"\"\"\n        client = Mock()\n        client.snapshot.status.return_value = {'snapshots': []}\n        # self.assertFalse(snapshot_running(client))\n        assert not snapshot_running(client)\n    def test_raises_exception(self):\n        \"\"\"test_raises_exception\n\n        Should raise a ``FailedExecution`` exception when an exception happens upstream\n        \"\"\"\n        client = Mock()\n        client.snapshot.status.return_value = {'snapshots': []}\n        client.snapshot.status.side_effect = FAKE_FAIL\n        # self.assertRaises(FailedExecution, snapshot_running, client)\n        with pytest.raises(FailedExecution, match=r'Rerun with loglevel DEBUG'):\n            snapshot_running(client)\n\nclass TestValidateFilters(TestCase):\n    \"\"\"TestValidateFilters\n\n    Test helpers.testers.validate_filters functionality.\n    \"\"\"\n    def test_snapshot_with_index_filter(self):\n        \"\"\"test_snapshot_with_index_filter\n\n        Should raise ConfigurationError with improper filter for filtertype\n        In this case, an index filtertype (``kibana``) for the ``delete_snapshots`` filter\n        \"\"\"\n        with pytest.raises(ConfigurationError, match=r'filtertype is not compatible with action'):\n            validate_filters('delete_snapshots', [{'filtertype': 'kibana'}])\n    def test_index_with_snapshot_filter(self):\n        \"\"\"test_index_with_snapshot_filter\n\n        Should raise ConfigurationError with improper filter for filtertype\n        In this case, a snapshot filtertype (``state``) for the ``delete_indices`` filter\n        \"\"\"\n        with pytest.raises(ConfigurationError, match=r'filtertype is not compatible with action'):\n            validate_filters('delete_indices', [{'filtertype': 'state', 'state': 'SUCCESS'}])\n\nclass TestVerifyClientObject(TestCase):\n    \"\"\"TestVerifyClientObject\n\n    Test helpers.testers.verify_client_object functionality.\n    \"\"\"\n    def test_is_client_object(self):\n        \"\"\"test_is_client_object\n\n        Should return a ``None`` value for a valid client object.\n        \"\"\"\n        test = Elasticsearch(hosts=[\"http://127.0.0.1:9200\"])\n        assert None is verify_client_object(test)\n\n    def test_is_not_client_object(self):\n        \"\"\"test_is_not_client_object\n\n        Should raise a ``TypeError`` exception with an invalid client object.\n        \"\"\"\n        test = 'not a client object'\n        with pytest.raises(TypeError, match=r'Not a valid client object'):\n            verify_client_object(test)\n\nclass TestVerifyRepository(TestCase):\n    \"\"\"TestVerifyRepository\n\n    Test helpers.testers.verify_repository functionality\n    \"\"\"\n    VERIFIED_NODES = {'nodes': {'nodeid1': {'name': 'node1'}, 'nodeid2': {'name': 'node2'}}}\n    REPO_NAME = 'repo_name'\n    def test_passing(self):\n        \"\"\"test_passing\n\n        Should return ``None`` and raise no Exception on success\n        \"\"\"\n        client = Mock()\n        client.snapshot.verify_repository.return_value = self.VERIFIED_NODES\n        assert None is verify_repository(client, repository=self.REPO_NAME)\n    def test_raises_404(self):\n        \"\"\"test_raises_404\n\n        Should raise ``RepositoryException`` when a 404 ``TransportError`` raises first\n        \"\"\"\n        client = Mock()\n        client.snapshot.verify_repository.return_value = self.VERIFIED_NODES\n        # 5 positional args for meta: status, http_version, headers, duration, node\n        meta = ApiResponseMeta(404, '1.1', {}, 0.01, None)\n        body = f'[{self.REPO_NAME}] missing'\n        msg = 'repository_missing_exception'\n        # 3 positional args for NotFoundError: message, meta, body\n        effect = NotFoundError(msg, meta, body)\n        client.snapshot.verify_repository.side_effect = effect\n        with pytest.raises(RepositoryException, match=r'Repository \"repo_name\" not found'):\n            verify_repository(client, repository=self.REPO_NAME)\n    def test_raises_401(self):\n        \"\"\"test_raises_401\n\n        Should raise ``RepositoryException`` when a 401 AuthenticationException raises first\n        \"\"\"\n        client = Mock()\n        client.snapshot.verify_repository.return_value = self.VERIFIED_NODES\n        # 5 positional args for meta: status, http_version, headers, duration, node\n        meta = ApiResponseMeta(401, '1.1', {}, 0.01, None)\n        body = 'No authentication'\n        msg = 'authentication error'\n        # 3 positional args for NotFoundError: message, meta, body\n        effect = AuthenticationException(msg, meta, body)\n        client.snapshot.verify_repository.side_effect = effect\n        with pytest.raises(RepositoryException, match=r'Got a 401 response from Elasticsearch'):\n            verify_repository(client, repository=self.REPO_NAME)\n    def test_raises_other(self):\n        \"\"\"test_raises_other\n\n        Should raise ``RepositoryException`` when any other Exception raises first\n        \"\"\"\n        client = Mock()\n        client.snapshot.verify_repository.return_value = self.VERIFIED_NODES\n        client.snapshot.verify_repository.side_effect = FAKE_FAIL\n        with pytest.raises(RepositoryException, match=r'Failed to verify'):\n            verify_repository(client, repository=self.REPO_NAME)\n\nclass TestHasLifecycleName(TestCase):\n    \"\"\"TestHasLifecycleName\n\n    Test helpers.testers.has_lifecycle_name functionality\n    \"\"\"\n    def test_has_lifecycle_name(self):\n        \"\"\"test_has_lifecycle_name\"\"\"\n        testval = {'lifecycle': {'name': 'ilm_policy'}}\n        assert has_lifecycle_name(testval)\n    def test_has_no_lifecycle_name(self):\n        \"\"\"test_has_no_lifecycle_name\"\"\"\n        testval = {'lifecycle': {'nothere': 'nope'}}\n        assert not has_lifecycle_name(testval)\n\nclass TestIsIdxPartial(TestCase):\n    \"\"\"TestIsIdxPartial\n\n    Test helpers.testers.is_idx_partial functionality\n    \"\"\"\n    def test_is_idx_partial(self):\n        \"\"\"test_is_idx_partial\"\"\"\n        testval = {'store': {'snapshot': {'partial': True}}}\n        assert is_idx_partial(testval)\n    def test_is_idx_partial_false1(self):\n        \"\"\"test_is_idx_partial_false1\"\"\"\n        testval = {'store': {'snapshot': {'partial': False}}}\n        assert not is_idx_partial(testval)\n    def test_is_idx_partial_false2(self):\n        \"\"\"test_is_idx_partial_false2\"\"\"\n        testval = {'store': {'snapshot': {'nothere': 'nope'}}}\n        assert not is_idx_partial(testval)\n    def test_is_idx_partial_raises1(self):\n        \"\"\"test_is_idx_partial_raises1\"\"\"\n        testval = {'store': {'nothere': 'nope'}}\n        with pytest.raises(SearchableSnapshotException, match='not a mounted searchable snapshot'):\n            is_idx_partial(testval)\n    def test_is_idx_partial_raises2(self):\n        \"\"\"test_is_idx_partial_raises2\"\"\"\n        testval = {'nothere': 'nope'}\n        with pytest.raises(SearchableSnapshotException, match='not a mounted searchable snapshot'):\n            is_idx_partial(testval)\n",
  "GT_file_code": {
    "curator/helpers/testers.py": "\"\"\"Utility functions that get things\"\"\"\n\nimport logging\nfrom voluptuous import Schema\nfrom elasticsearch8 import Elasticsearch\nfrom elasticsearch8.exceptions import NotFoundError\nfrom es_client.helpers.schemacheck import SchemaCheck\nfrom es_client.helpers.utils import prune_nones\nfrom curator.helpers.getters import get_repository, get_write_index\nfrom curator.exceptions import (\n    ConfigurationError,\n    MissingArgument,\n    RepositoryException,\n    SearchableSnapshotException,\n)\nfrom curator.defaults.settings import (\n    index_filtertypes,\n    snapshot_actions,\n    snapshot_filtertypes,\n)\nfrom curator.validators import actions, options\nfrom curator.validators.filter_functions import validfilters\nfrom curator.helpers.utils import report_failure\n\n\ndef has_lifecycle_name(idx_settings):\n    \"\"\"\n    :param idx_settings: The settings for an index being tested\n    :type idx_settings: dict\n\n    :returns: ``True`` if a lifecycle name exists in settings, else ``False``\n    :rtype: bool\n    \"\"\"\n    if 'lifecycle' in idx_settings:\n        if 'name' in idx_settings['lifecycle']:\n            return True\n    return False\n\n\ndef is_idx_partial(idx_settings):\n    \"\"\"\n    :param idx_settings: The settings for an index being tested\n    :type idx_settings: dict\n\n    :returns: ``True`` if store.snapshot.partial exists in settings, else ``False``\n    :rtype: bool\n    \"\"\"\n    if 'store' in idx_settings:\n        if 'snapshot' in idx_settings['store']:\n            if 'partial' in idx_settings['store']['snapshot']:\n                if idx_settings['store']['snapshot']['partial']:\n                    return True\n                # store.snapshot.partial exists but is False -- Not a frozen tier mount\n                return False\n            # store.snapshot exists, but partial isn't there --\n            # Possibly a cold tier mount\n            return False\n        raise SearchableSnapshotException('Index not a mounted searchable snapshot')\n    raise SearchableSnapshotException('Index not a mounted searchable snapshot')\n\n\ndef ilm_policy_check(client, alias):\n    \"\"\"Test if alias is associated with an ILM policy\n\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.get_settings`\n\n    :param client: A client connection object\n    :param alias: The alias name\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type alias: str\n    :rtype: bool\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    # alias = action_obj.options['name']\n    write_index = get_write_index(client, alias)\n    try:\n        idx_settings = client.indices.get_settings(index=write_index)\n        if 'name' in idx_settings[write_index]['settings']['index']['lifecycle']:\n            # logger.info('Alias %s is associated with ILM policy.', alias)\n            # logger.info('Skipping action %s because allow_ilm_indices is false.', idx)\n            return True\n    except KeyError:\n        logger.debug('No ILM policies associated with %s', alias)\n    return False\n\n\ndef repository_exists(client, repository=None):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get_repository`\n\n    :param client: A client connection object\n    :param repository: The Elasticsearch snapshot repository to use\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type repository: str\n\n    :returns: ``True`` if ``repository`` exists, else ``False``\n    :rtype: bool\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    if not repository:\n        raise MissingArgument('No value for \"repository\" provided')\n    try:\n        test_result = get_repository(client, repository)\n        if repository in test_result:\n            logger.debug(\"Repository %s exists.\", repository)\n            response = True\n        else:\n            logger.debug(\"Repository %s not found...\", repository)\n            response = False\n    # pylint: disable=broad-except\n    except Exception as err:\n        logger.debug('Unable to find repository \"%s\": Error: %s', repository, err)\n        response = False\n    return response\n\n\ndef rollable_alias(client, alias):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.get_alias`\n\n    :param client: A client connection object\n    :param alias: An Elasticsearch alias\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type alias: str\n\n\n    :returns: ``True`` or ``False`` depending on whether ``alias`` is an alias that\n        points to an index that can be used by the ``_rollover`` API.\n    :rtype: bool\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    try:\n        response = client.indices.get_alias(name=alias)\n    except NotFoundError:\n        logger.error('Alias \"%s\" not found.', alias)\n        return False\n    # Response should be like:\n    # {'there_should_be_only_one': {'aliases': {'value of \"alias\" here': {}}}}\n    # where 'there_should_be_only_one' is a single index name that ends in a number,\n    # and 'value of \"alias\" here' reflects the value of the passed parameter, except\n    # where the ``is_write_index`` setting makes it possible to have more than one\n    # index associated with a rollover index\n    for idx in response:\n        if 'is_write_index' in response[idx]['aliases'][alias]:\n            if response[idx]['aliases'][alias]['is_write_index']:\n                return True\n    # implied ``else``: If not ``is_write_index``, it has to fit the following criteria:\n    if len(response) > 1:\n        logger.error(\n            '\"alias\" must only reference one index, but points to %s', response\n        )\n        return False\n    index = list(response.keys())[0]\n    rollable = False\n    # In order for `rollable` to be True, the last 2 digits of the index\n    # must be digits, or a hyphen followed by a digit.\n    # NOTE: This is not a guarantee that the rest of the index name is\n    # necessarily correctly formatted.\n    if index[-2:][1].isdigit():\n        if index[-2:][0].isdigit():\n            rollable = True\n        elif index[-2:][0] == '-':\n            rollable = True\n    return rollable\n\n\ndef snapshot_running(client):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get_repository`\n\n    Return ``True`` if a snapshot is in progress, and ``False`` if not\n\n    :param client: A client connection object\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n\n    :rtype: bool\n    \"\"\"\n    try:\n        status = client.snapshot.status()['snapshots']\n    # pylint: disable=broad-except\n    except Exception as exc:\n        report_failure(exc)\n    # We will only accept a positively identified False.  Anything else is\n    # suspect. That's why this statement, rather than just ``return status``\n    # pylint: disable=simplifiable-if-expression\n    return False if not status else True\n\n\ndef validate_actions(data):\n    \"\"\"\n    Validate the ``actions`` configuration dictionary, as imported from actions.yml,\n    for example.\n\n    :param data: The configuration dictionary\n\n    :type data: dict\n\n    :returns: The validated and sanitized configuration dictionary.\n    :rtype: dict\n    \"\"\"\n    # data is the ENTIRE schema...\n    clean_config = {}\n    # Let's break it down into smaller chunks...\n    # First, let's make sure it has \"actions\" as a key, with a subdictionary\n    root = SchemaCheck(data, actions.root(), 'Actions File', 'root').result()\n    # We've passed the first step.  Now let's iterate over the actions...\n    for action_id in root['actions']:\n        # Now, let's ensure that the basic action structure is correct, with\n        # the proper possibilities for 'action'\n        action_dict = root['actions'][action_id]\n        loc = f'Action ID \"{action_id}\"'\n        valid_structure = SchemaCheck(\n            action_dict, actions.structure(action_dict, loc), 'structure', loc\n        ).result()\n        # With the basic structure validated, now we extract the action name\n        current_action = valid_structure['action']\n        # And let's update the location with the action.\n        loc = f'Action ID \"{action_id}\", action \"{current_action}\"'\n        clean_options = SchemaCheck(\n            prune_nones(valid_structure['options']),\n            options.get_schema(current_action),\n            'options',\n            loc,\n        ).result()\n        clean_config[action_id] = {\n            'action': current_action,\n            'description': valid_structure['description'],\n            'options': clean_options,\n        }\n        if current_action == 'alias':\n            add_remove = {}\n            for k in ['add', 'remove']:\n                if k in valid_structure:\n                    current_filters = SchemaCheck(\n                        valid_structure[k]['filters'],\n                        Schema(validfilters(current_action, location=loc)),\n                        f'\"{k}\" filters',\n                        f'{loc}, \"filters\"',\n                    ).result()\n                    add_remove.update(\n                        {\n                            k: {\n                                'filters': SchemaCheck(\n                                    current_filters,\n                                    Schema(validfilters(current_action, location=loc)),\n                                    'filters',\n                                    f'{loc}, \"{k}\", \"filters\"',\n                                ).result()\n                            }\n                        }\n                    )\n            # Add/Remove here\n            clean_config[action_id].update(add_remove)\n        elif current_action in ['cluster_routing', 'create_index', 'rollover']:\n            # neither cluster_routing nor create_index should have filters\n            pass\n        else:  # Filters key only appears in non-alias actions\n            valid_filters = SchemaCheck(\n                valid_structure['filters'],\n                Schema(validfilters(current_action, location=loc)),\n                'filters',\n                f'{loc}, \"filters\"',\n            ).result()\n            clean_filters = validate_filters(current_action, valid_filters)\n            clean_config[action_id].update({'filters': clean_filters})\n        # This is a special case for remote reindex\n        if current_action == 'reindex':\n            # Check only if populated with something.\n            if 'remote_filters' in valid_structure['options']:\n                valid_filters = SchemaCheck(\n                    valid_structure['options']['remote_filters'],\n                    Schema(validfilters(current_action, location=loc)),\n                    'filters',\n                    f'{loc}, \"filters\"',\n                ).result()\n                clean_remote_filters = validate_filters(current_action, valid_filters)\n                clean_config[action_id]['options'].update(\n                    {'remote_filters': clean_remote_filters}\n                )\n\n    # if we've gotten this far without any Exceptions raised, it's valid!\n    return {'actions': clean_config}\n\n\ndef validate_filters(action, myfilters):\n    \"\"\"\n    Validate that myfilters are appropriate for the action type, e.g. no\n    index filters applied to a snapshot list.\n\n    :param action: An action name\n    :param myfilters: A list of filters to test.\n\n    :type action: str\n    :type myfilters: list\n\n    :returns: Validated list of filters\n    :rtype: list\n    \"\"\"\n    # Define which set of filtertypes to use for testing\n    if action in snapshot_actions():\n        filtertypes = snapshot_filtertypes()\n    else:\n        filtertypes = index_filtertypes()\n    for fil in myfilters:\n        if fil['filtertype'] not in filtertypes:\n            raise ConfigurationError(\n                f\"\\\"{fil['filtertype']}\\\" filtertype is not compatible with \"\n                f\"action \\\"{action}\\\"\"\n            )\n    # If we get to this point, we're still valid.  Return the original list\n    return myfilters\n\n\ndef verify_client_object(test):\n    \"\"\"\n    :param test: The variable or object to test\n\n    :type test: :py:class:`~.elasticsearch.Elasticsearch`\n\n    :returns: ``True`` if ``test`` is a proper :py:class:`~.elasticsearch.Elasticsearch`\n        client object and raise a :py:exc:`TypeError` exception if it is not.\n    :rtype: bool\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    # Ignore mock type for testing\n    if str(type(test)) == \"<class 'unittest.mock.Mock'>\":\n        pass\n    elif not isinstance(test, Elasticsearch):\n        msg = f'Not a valid client object. Type: {type(test)} was passed'\n        logger.error(msg)\n        raise TypeError(msg)\n\n\ndef verify_index_list(test):\n    \"\"\"\n    :param test: The variable or object to test\n\n    :type test: :py:class:`~.curator.IndexList`\n\n    :returns: ``None`` if ``test`` is a proper :py:class:`~.curator.indexlist.IndexList`\n        object, else raise a :py:class:`TypeError` exception.\n    :rtype: None\n    \"\"\"\n    # It breaks if this import isn't local to this function:\n    # ImportError: cannot import name 'IndexList' from partially initialized module\n    # 'curator.indexlist' (most likely due to a circular import)\n    # pylint: disable=import-outside-toplevel\n    from curator.indexlist import IndexList\n\n    logger = logging.getLogger(__name__)\n    if not isinstance(test, IndexList):\n        msg = f'Not a valid IndexList object. Type: {type(test)} was passed'\n        logger.error(msg)\n        raise TypeError(msg)\n\n\ndef verify_repository(client, repository=None):\n    \"\"\"\n    Do :py:meth:`~.elasticsearch.snapshot.verify_repository` call. If it fails, raise a\n    :py:exc:`~.curator.exceptions.RepositoryException`.\n\n    :param client: A client connection object\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :param repository: A repository name\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type repository: str\n\n    :rtype: None\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    try:\n        nodes = client.snapshot.verify_repository(name=repository)['nodes']\n        logger.debug('All nodes can write to the repository')\n        logger.debug('Nodes with verified repository access: %s', nodes)\n    except Exception as err:\n        try:\n            if err.status_code == 404:\n                msg = (\n                    f'--- Repository \"{repository}\" not found. Error: '\n                    f'{err.meta.status}, {err.error}'\n                )\n            else:\n                msg = (\n                    f'--- Got a {err.meta.status} response from Elasticsearch.  '\n                    f'Error message: {err.error}'\n                )\n        except AttributeError:\n            msg = f'--- Error message: {err}'.format()\n        report = f'Failed to verify all nodes have repository access: {msg}'\n        raise RepositoryException(report) from err\n\n\ndef verify_snapshot_list(test):\n    \"\"\"\n    :param test: The variable or object to test\n\n    :type test: :py:class:`~.curator.SnapshotList`\n\n    :returns: ``None`` if ``test`` is a proper\n        :py:class:`~.curator.snapshotlist.SnapshotList` object, else raise a\n        :py:class:`TypeError` exception.\n    :rtype: None\n    \"\"\"\n    # It breaks if this import isn't local to this function:\n    # ImportError: cannot import name 'SnapshotList' from partially initialized module\n    # 'curator.snapshotlist' (most likely due to a circular import)\n    # pylint: disable=import-outside-toplevel\n    from curator.snapshotlist import SnapshotList\n\n    logger = logging.getLogger(__name__)\n    if not isinstance(test, SnapshotList):\n        msg = f'Not a valid SnapshotList object. Type: {type(test)} was passed'\n        logger.error(msg)\n        raise TypeError(msg)\n",
    "curator/defaults/settings.py": "\"\"\"Utilities/Helpers for defaults and schemas\"\"\"\n\nfrom os import path\nfrom voluptuous import Any, Boolean, Coerce, Optional\nfrom curator.exceptions import CuratorException\n\n# pylint: disable=E1120\n\nCURATOR_DOCS = 'https://www.elastic.co/guide/en/elasticsearch/client/curator'\nCLICK_DRYRUN = {\n    'dry-run': {'help': 'Do not perform any changes.', 'is_flag': True},\n}\nDATA_NODE_ROLES = ['data', 'data_content', 'data_hot', 'data_warm']\n\n# Click specifics\n\n\ndef footer(version, tail='index.html'):\n    \"\"\"\n    Generate a footer linking to Curator docs based on Curator version\n\n    :param version: The Curator version\n\n    :type version: str\n\n    :returns: An epilog/footer suitable for Click\n    \"\"\"\n    if not isinstance(version, str):\n        raise CuratorException('Parameter version is not a string: {type(version)}')\n    majmin = ''\n    try:\n        ver = version.split('.')\n        majmin = f'{ver[0]}.{ver[1]}'\n    except Exception as exc:\n        msg = f'Could not determine Curator version from provided value: {version}'\n        raise CuratorException(msg) from exc\n    return f'Learn more at {CURATOR_DOCS}/{majmin}/{tail}'\n\n\n# Default Config file location\ndef default_config_file():\n    \"\"\"\n    :returns: The default configuration file location:\n        path.join(path.expanduser('~'), '.curator', 'curator.yml')\n    \"\"\"\n    default = path.join(path.expanduser('~'), '.curator', 'curator.yml')\n    if path.isfile(default):\n        return default\n\n\n# Default filter patterns (regular expressions)\ndef regex_map():\n    \"\"\"\n    :returns: A dictionary of pattern filter 'kind's with their associated regular\n        expression: {'timestring': r'^.*{0}.*$', 'regex': r'{0}',\n        'prefix': r'^{0}.*$', 'suffix': r'^.*{0}$'}\n    \"\"\"\n    return {\n        'timestring': r'^.*{0}.*$',\n        'regex': r'{0}',\n        'prefix': r'^{0}.*$',\n        'suffix': r'^.*{0}$',\n    }\n\n\ndef date_regex():\n    \"\"\"\n    :returns: A dictionary/map of the strftime string characters and their string\n        lengths: {'Y':'4', 'G':'4', 'y':'2', 'm':'2', 'W':'2', 'V':'2', 'U':'2',\n        'd':'2', 'H':'2', 'M':'2', 'S':'2', 'j':'3'}\n    \"\"\"\n    return {\n        'Y': '4',\n        'G': '4',\n        'y': '2',\n        'm': '2',\n        'W': '2',\n        'V': '2',\n        'U': '2',\n        'd': '2',\n        'H': '2',\n        'M': '2',\n        'S': '2',\n        'j': '3',\n    }\n\n\n# Actions\n\n\ndef cluster_actions():\n    \"\"\"\n    :returns: A list of supported cluster actions (right now, that's only\n        ['cluster_routing'])\n    \"\"\"\n    return ['cluster_routing']\n\n\ndef index_actions():\n    \"\"\"\n    :returns: The list of supported index actions:\n        [ 'alias', 'allocation', 'close', 'create_index', 'delete_indices',\n        'forcemerge', 'index_settings', 'open', 'reindex', 'replicas',\n        'rollover', 'shrink', 'snapshot']\n    \"\"\"\n    return [\n        'alias',\n        'allocation',\n        'close',\n        'cold2frozen',\n        'create_index',\n        'delete_indices',\n        'forcemerge',\n        'index_settings',\n        'open',\n        'reindex',\n        'replicas',\n        'rollover',\n        'shrink',\n        'snapshot',\n    ]\n\n\ndef snapshot_actions():\n    \"\"\"\n    :returns: The list of supported snapshot actions: ['delete_snapshots', 'restore']\n    \"\"\"\n    return ['delete_snapshots', 'restore']\n\n\ndef all_actions():\n    \"\"\"\n    :returns: A sorted list of all supported actions: cluster, index, and snapshot\n    \"\"\"\n    return sorted(cluster_actions() + index_actions() + snapshot_actions())\n\n\ndef index_filtertypes():\n    \"\"\"\n    :returns: The list of supported index filter types:\n        ['alias', 'allocated', 'age', 'closed', 'count', 'empty', 'forcemerged',\n        'ilm', 'kibana', 'none', 'opened', 'pattern', 'period', 'space',\n        'shards', 'size']\n    \"\"\"\n\n    return [\n        'alias',\n        'allocated',\n        'age',\n        'closed',\n        'count',\n        'empty',\n        'forcemerged',\n        'ilm',\n        'kibana',\n        'none',\n        'opened',\n        'pattern',\n        'period',\n        'space',\n        'shards',\n        'size',\n    ]\n\n\ndef snapshot_filtertypes():\n    \"\"\"\n    :returns: The list of supported snapshot filter types: ['age', 'count', 'none',\n        'pattern', 'period', 'state']\n    \"\"\"\n    return ['age', 'count', 'none', 'pattern', 'period', 'state']\n\n\ndef all_filtertypes():\n    \"\"\"\n    :returns: A sorted list of all supported filter types (both snapshot and index)\n    \"\"\"\n    return sorted(list(set(index_filtertypes() + snapshot_filtertypes())))\n\n\ndef default_options():\n    \"\"\"\n    :returns: The default values for these options:\n        {'allow_ilm_indices': False, 'continue_if_exception': False,\n        'disable_action': False, 'ignore_empty_list': False,\n        'timeout_override': None}\n    \"\"\"\n    return {\n        'allow_ilm_indices': False,\n        'continue_if_exception': False,\n        'disable_action': False,\n        'ignore_empty_list': False,\n        'timeout_override': None,\n    }\n\n\ndef default_filters():\n    \"\"\"\n    If no filters are set, add a 'none' filter\n\n    :returns: {'filters': [{'filtertype': 'none'}]}\n    \"\"\"\n    return {'filters': [{'filtertype': 'none'}]}\n\n\ndef structural_filter_elements():\n    \"\"\"\n    :returns: Barebones schemas for initial validation of filters\n    \"\"\"\n\n    return {\n        Optional('aliases'): Any(list, str),\n        Optional('allocation_type'): Any(str),\n        Optional('count'): Coerce(int),\n        Optional('date_from'): Any(None, str),\n        Optional('date_from_format'): Any(None, str),\n        Optional('date_to'): Any(None, str),\n        Optional('date_to_format'): Any(None, str),\n        Optional('direction'): Any(str),\n        Optional('disk_space'): float,\n        Optional('epoch'): Any(Coerce(int), None),\n        Optional('exclude'): Any(None, bool, int, str),\n        Optional('field'): Any(None, str),\n        Optional('intersect'): Any(None, bool, int, str),\n        Optional('key'): Any(str),\n        Optional('kind'): Any(str),\n        Optional('max_num_segments'): Coerce(int),\n        Optional('number_of_shards'): Coerce(int),\n        Optional('pattern'): Any(str),\n        Optional('period_type'): Any(str),\n        Optional('reverse'): Any(None, bool, int, str),\n        Optional('range_from'): Coerce(int),\n        Optional('range_to'): Coerce(int),\n        Optional('shard_filter_behavior'): Any(str),\n        Optional('size_behavior'): Any(str),\n        Optional('size_threshold'): Any(Coerce(float)),\n        Optional('source'): Any(str),\n        Optional('state'): Any(str),\n        Optional('stats_result'): Any(None, str),\n        Optional('timestring'): Any(None, str),\n        Optional('threshold_behavior'): Any(str),\n        Optional('unit'): Any(str),\n        Optional('unit_count'): Coerce(int),\n        Optional('unit_count_pattern'): Any(str),\n        Optional('use_age'): Boolean(),\n        Optional('value'): Any(int, float, bool, str),\n        Optional('week_starts_on'): Any(None, str),\n    }\n",
    "curator/helpers/utils.py": "\"\"\"Helper utilities\n\nThe kind that don't fit in testers, getters, date_ops, or converters\n\"\"\"\nimport logging\nfrom es_client.helpers.utils import ensure_list\nfrom curator.exceptions import FailedExecution\n\ndef chunk_index_list(indices):\n    \"\"\"\n    This utility chunks very large index lists into 3KB chunks.\n    It measures the size as a csv string, then converts back into a list for the return value.\n\n    :param indices: The list of indices\n\n    :type indices: list\n\n    :returns: A list of lists (each a piece of the original ``indices``)\n    :rtype: list\n    \"\"\"\n    chunks = []\n    chunk = \"\"\n    for index in indices:\n        if len(chunk) < 3072:\n            if not chunk:\n                chunk = index\n            else:\n                chunk += \",\" + index\n        else:\n            chunks.append(chunk.split(','))\n            chunk = index\n    chunks.append(chunk.split(','))\n    return chunks\n\ndef report_failure(exception):\n    \"\"\"\n    Raise a :py:exc:`~.curator.exceptions.FailedExecution` exception and include the original error\n    message.\n\n    :param exception: The upstream exception.\n\n    :type exception: :py:exc:Exception\n\n    :rtype: None\n    \"\"\"\n    raise FailedExecution(\n        f'Exception encountered.  Rerun with loglevel DEBUG and/or check Elasticsearch logs for'\n        f'more information. Exception: {exception}'\n    )\n\ndef show_dry_run(ilo, action, **kwargs):\n    \"\"\"\n    Log dry run output with the action which would have been executed.\n\n    :param ilo: An IndexList Object\n    :param action: The ``action`` to be performed.\n    :param kwargs: Any other args to show in the log output\n\n\n    :type ilo: :py:class:`~.curator.indexlist.IndexList`\n    :type action: str\n    :type kwargs: dict\n\n    :rtype: None\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info('DRY-RUN MODE.  No changes will be made.')\n    msg = f'(CLOSED) indices may be shown that may not be acted on by action \"{action}\".'\n    logger.info(msg)\n    indices = sorted(ilo.indices)\n    for idx in indices:\n        # Dry runs need index state, so we collect it here if it's not present.\n        try:\n            index_closed = ilo.index_info[idx]['state'] == 'close'\n        except KeyError:\n            ilo.get_index_state()\n            index_closed = ilo.index_info[idx]['state'] == 'close'\n        var = ' (CLOSED)' if index_closed else ''\n        msg = f'DRY-RUN: {action}: {idx}{var} with arguments: {kwargs}'\n        logger.info(msg)\n\ndef to_csv(indices):\n    \"\"\"\n    :param indices: A list of indices to act on, or a single value, which could be\n        in the format of a csv string already.\n\n    :type indices: list\n\n    :returns: A csv string from a list of indices, or a single value if only one value is present\n    :rtype: str\n    \"\"\"\n    indices = ensure_list(indices) # in case of a single value passed\n    if indices:\n        return ','.join(sorted(indices))\n    return None\n",
    "curator/helpers/getters.py": "\"\"\"Utility functions that get things\"\"\"\n\nimport logging\nfrom elasticsearch8 import exceptions as es8exc\nfrom curator.exceptions import (\n    ConfigurationError,\n    CuratorException,\n    FailedExecution,\n    MissingArgument,\n)\n\n\ndef byte_size(num, suffix='B'):\n    \"\"\"\n    :param num: The number of byte\n    :param suffix: An arbitrary suffix, like ``Bytes``\n\n    :type num: int\n    :type suffix: str\n\n    :returns: A formatted string indicating the size in bytes, with the proper unit,\n        e.g. KB, MB, GB, TB, etc.\n    :rtype: float\n    \"\"\"\n    for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:\n        if abs(num) < 1024.0:\n            return f'{num:3.1f}{unit}{suffix}'\n        num /= 1024.0\n    return f'{num:.1f}Y{suffix}'\n\n\ndef escape_dots(stringval):\n    \"\"\"\n    Escape any dots (periods) in ``stringval``.\n\n    Primarily used for ``filter_path`` where dots are indicators of path nesting\n\n    :param stringval: A string, ostensibly an index name\n\n    :type stringval: str\n\n    :returns: ``stringval``, but with any periods escaped with a backslash\n    :retval: str\n    \"\"\"\n    return stringval.replace('.', r'\\.')\n\n\ndef get_alias_actions(oldidx, newidx, aliases):\n    \"\"\"\n    :param oldidx: The old index name\n    :param newidx: The new index name\n    :param aliases: The aliases\n\n    :type oldidx: str\n    :type newidx: str\n    :type aliases: dict\n\n    :returns: A list of actions suitable for\n        :py:meth:`~.elasticsearch.client.IndicesClient.update_aliases` ``actions``\n        kwarg.\n    :rtype: list\n    \"\"\"\n    actions = []\n    for alias in aliases.keys():\n        actions.append({'remove': {'index': oldidx, 'alias': alias}})\n        actions.append({'add': {'index': newidx, 'alias': alias}})\n    return actions\n\n\ndef get_data_tiers(client):\n    \"\"\"\n    Get all valid data tiers from the node roles of each node in the cluster by\n    polling each node\n\n    :param client: A client connection object\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n\n    :returns: The available data tiers in ``tier: bool`` form.\n    :rtype: dict\n    \"\"\"\n\n    def role_check(role, node_info):\n        if role in node_info['roles']:\n            return True\n        return False\n\n    info = client.nodes.info()['nodes']\n    retval = {\n        'data_hot': False,\n        'data_warm': False,\n        'data_cold': False,\n        'data_frozen': False,\n    }\n    for node in info:\n        for role in ['data_hot', 'data_warm', 'data_cold', 'data_frozen']:\n            # This guarantees we don't overwrite a True with a False.\n            # We only add True values\n            if role_check(role, info[node]):\n                retval[role] = True\n    return retval\n\n\ndef get_indices(client, search_pattern='_all'):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.CatClient.indices`\n\n    :param client: A client connection object\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n\n    :returns: The current list of indices from the cluster\n    :rtype: list\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    indices = []\n    try:\n        # Doing this in two stages because IndexList also calls for these args,\n        # and the unit tests need to Mock this call the same exact way.\n        resp = client.cat.indices(\n            index=search_pattern,\n            expand_wildcards='open,closed',\n            h='index,status',\n            format='json',\n        )\n    except Exception as err:\n        raise FailedExecution(f'Failed to get indices. Error: {err}') from err\n    if not resp:\n        return indices\n    for entry in resp:\n        indices.append(entry['index'])\n    logger.debug('All indices: %s', indices)\n    return indices\n\n\ndef get_repository(client, repository=''):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get_repository`\n\n    :param client: A client connection object\n    :param repository: The Elasticsearch snapshot repository to use\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type repository: str\n\n    :returns: Configuration information for ``repository``.\n    :rtype: dict\n    \"\"\"\n    try:\n        return client.snapshot.get_repository(name=repository)\n    except (es8exc.TransportError, es8exc.NotFoundError) as err:\n        msg = (\n            f'Unable to get repository {repository}.  Error: {err} Check Elasticsearch '\n            f'logs for more information.'\n        )\n        raise CuratorException(msg) from err\n\n\ndef get_snapshot(client, repository=None, snapshot=''):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get`\n\n    :param client: A client connection object\n    :param repository: The Elasticsearch snapshot repository to use\n    :param snapshot: The snapshot name, or a comma-separated list of snapshots\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type repository: str\n    :type snapshot: str\n\n    :returns: Information about the provided ``snapshot``, a snapshot (or a\n        comma-separated list of snapshots). If no snapshot specified, it will\n        collect info for all snapshots.  If none exist, an empty :py:class:`dict`\n        will be returned.\n    :rtype: dict\n    \"\"\"\n    if not repository:\n        raise MissingArgument('No value for \"repository\" provided')\n    snapname = '*' if snapshot == '' else snapshot\n    try:\n        return client.snapshot.get(repository=repository, snapshot=snapshot)\n    except (es8exc.TransportError, es8exc.NotFoundError) as err:\n        msg = (\n            f'Unable to get information about snapshot {snapname} from repository: '\n            f'{repository}.  Error: {err}'\n        )\n        raise FailedExecution(msg) from err\n\n\ndef get_snapshot_data(client, repository=None):\n    \"\"\"\n    Get all snapshots from repository and return a list.\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get`\n\n    :param client: A client connection object\n    :param repository: The Elasticsearch snapshot repository to use\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type repository: str\n\n    :returns: The list of all snapshots from ``repository``\n    :rtype: list\n    \"\"\"\n    if not repository:\n        raise MissingArgument('No value for \"repository\" provided')\n    try:\n        return client.snapshot.get(repository=repository, snapshot=\"*\")['snapshots']\n    except (es8exc.TransportError, es8exc.NotFoundError) as err:\n        msg = (\n            f'Unable to get snapshot information from repository: '\n            f'{repository}. Error: {err}'\n        )\n        raise FailedExecution(msg) from err\n\n\ndef get_tier_preference(client, target_tier='data_frozen'):\n    \"\"\"Do the tier preference thing in reverse order from coldest to hottest\n    Based on the value of ``target_tier``, build out the list to use.\n\n    :param client: A client connection object\n    :param target_tier: The target data tier, e.g. data_warm.\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type target_tier: str\n\n    :returns: A suitable tier preference string in csv format\n    :rtype: str\n    \"\"\"\n    tiermap = {\n        'data_content': 0,\n        'data_hot': 1,\n        'data_warm': 2,\n        'data_cold': 3,\n        'data_frozen': 4,\n    }\n    tiers = get_data_tiers(client)\n    test_list = []\n    for tier in ['data_hot', 'data_warm', 'data_cold', 'data_frozen']:\n        if tier in tiers and tiermap[tier] <= tiermap[target_tier]:\n            test_list.insert(0, tier)\n    if target_tier == 'data_frozen':\n        # We're migrating to frozen here. If a frozen tier exists, frozen searchable\n        # snapshot mounts should only ever go to the frozen tier.\n        if 'data_frozen' in tiers and tiers['data_frozen']:\n            return 'data_frozen'\n    # If there are no  nodes with the 'data_frozen' role...\n    preflist = []\n    for key in test_list:\n        # This ordering ensures that colder tiers are prioritized\n        if key in tiers and tiers[key]:\n            preflist.append(key)\n    # If all of these are false, then we have no data tiers and must use 'data_content'\n    if not preflist:\n        return 'data_content'\n    # This will join from coldest to hottest as csv string,\n    # e.g. 'data_cold,data_warm,data_hot'\n    return ','.join(preflist)\n\n\ndef get_write_index(client, alias):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.get_alias`\n\n    :param client: A client connection object\n    :param alias: An alias name\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type alias: str\n\n    :returns: The the index name associated with the alias that is designated\n        ``is_write_index``\n    :rtype: str\n    \"\"\"\n    try:\n        response = client.indices.get_alias(index=alias)\n    except Exception as exc:\n        raise CuratorException(f'Alias {alias} not found') from exc\n    # If there are more than one in the list, one needs to be the write index\n    # otherwise the alias is a one to many, and can't do rollover.\n    retval = None\n    if len(list(response.keys())) > 1:\n        for index in list(response.keys()):\n            try:\n                if response[index]['aliases'][alias]['is_write_index']:\n                    retval = index\n            except KeyError as exc:\n                raise FailedExecution(\n                    'Invalid alias: is_write_index not found in 1 to many alias'\n                ) from exc\n    else:\n        # There's only one, so this is it\n        retval = list(response.keys())[0]\n    return retval\n\n\ndef index_size(client, idx, value='total'):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.stats`\n\n    :param client: A client connection object\n    :param idx: An index name\n    :param value: One of either ``primaries`` or ``total``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type idx: str\n    :type value: str\n\n    :returns: The sum of either ``primaries`` or ``total`` shards for index ``idx``\n    :rtype: integer\n    \"\"\"\n    fpath = f'indices.{escape_dots(idx)}.{value}.store.size_in_bytes'\n    return client.indices.stats(index=idx, filter_path=fpath)['indices'][idx][value][\n        'store'\n    ]['size_in_bytes']\n\n\ndef meta_getter(client, idx, get=None):\n    \"\"\"Meta Getter\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.get_settings` or\n    :py:meth:`~.elasticsearch.client.IndicesClient.get_alias`\n\n    :param client: A client connection object\n    :param idx: An Elasticsearch index\n    :param get: The kind of get to perform, e.g. settings or alias\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type idx: str\n    :type get: str\n\n    :returns: The settings from the get call to the named index\n    :rtype: dict\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    acceptable = ['settings', 'alias']\n    if not get:\n        raise ConfigurationError('\"get\" can not be a NoneType')\n    if get not in acceptable:\n        raise ConfigurationError(f'\"get\" must be one of {acceptable}')\n    retval = {}\n    try:\n        if get == 'settings':\n            retval = client.indices.get_settings(index=idx)[idx]['settings']['index']\n        elif get == 'alias':\n            retval = client.indices.get_alias(index=idx)[idx]['aliases']\n    except es8exc.NotFoundError as missing:\n        logger.error('Index %s was not found!', idx)\n        raise es8exc.NotFoundError from missing\n    except KeyError as err:\n        logger.error('Key not found: %s', err)\n        raise KeyError from err\n    # pylint: disable=broad-except\n    except Exception as exc:\n        logger.error('Exception encountered: %s', exc)\n    return retval\n\n\ndef name_to_node_id(client, name):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.NodesClient.info`\n\n    :param client: A client connection object\n    :param name: The node ``name``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type name: str\n\n    :returns: The node_id of the node identified by ``name``\n    :rtype: str\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    fpath = 'nodes'\n    info = client.nodes.info(filter_path=fpath)\n    for node in info['nodes']:\n        if info['nodes'][node]['name'] == name:\n            logger.debug('Found node_id \"%s\" for name \"%s\".', node, name)\n            return node\n    logger.error('No node_id found matching name: \"%s\"', name)\n    return None\n\n\ndef node_id_to_name(client, node_id):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.NodesClient.info`\n\n    :param client: A client connection object\n    :param node_id: The node ``node_id``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type node_id: str\n\n    :returns: The name of the node identified by ``node_id``\n    :rtype: str\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    fpath = f'nodes.{node_id}.name'\n    info = client.nodes.info(filter_path=fpath)\n    name = None\n    if node_id in info['nodes']:\n        name = info['nodes'][node_id]['name']\n    else:\n        logger.error('No node_id found matching: \"%s\"', node_id)\n    logger.debug('Name associated with node_id \"%s\": %s', node_id, name)\n    return name\n\n\ndef node_roles(client, node_id):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.NodesClient.info`\n\n    :param client: A client connection object\n    :param node_id: The node ``node_id``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type node_id: str\n\n    :returns: The list of roles assigned to the node identified by ``node_id``\n    :rtype: list\n    \"\"\"\n    fpath = f'nodes.{node_id}.roles'\n    return client.nodes.info(filter_path=fpath)['nodes'][node_id]['roles']\n\n\ndef single_data_path(client, node_id):\n    \"\"\"\n    In order for a shrink to work, it should be on a single filesystem, as shards\n    cannot span filesystems. Calls :py:meth:`~.elasticsearch.client.NodesClient.stats`\n\n    :param client: A client connection object\n    :param node_id: The node ``node_id``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type node_id: str\n\n    :returns: ``True`` if the node has a single filesystem, else ``False``\n    :rtype: bool\n    \"\"\"\n    fpath = f'nodes.{node_id}.fs.data'\n    response = client.nodes.stats(filter_path=fpath)\n    return len(response['nodes'][node_id]['fs']['data']) == 1\n",
    "curator/exceptions.py": "\"\"\"Curator Exceptions\"\"\"\nclass CuratorException(Exception):\n    \"\"\"\n    Base class for all exceptions raised by Curator which are not Elasticsearch\n    exceptions.\n    \"\"\"\n\nclass ConfigurationError(CuratorException):\n    \"\"\"\n    Exception raised when a misconfiguration is detected\n    \"\"\"\n\nclass MissingArgument(CuratorException):\n    \"\"\"\n    Exception raised when a needed argument is not passed.\n    \"\"\"\n\nclass NoIndices(CuratorException):\n    \"\"\"\n    Exception raised when an operation is attempted against an empty index_list\n    \"\"\"\n\nclass NoSnapshots(CuratorException):\n    \"\"\"\n    Exception raised when an operation is attempted against an empty snapshot_list\n    \"\"\"\n\nclass ActionError(CuratorException):\n    \"\"\"\n    Exception raised when an action (against an index_list or snapshot_list) cannot be taken.\n    \"\"\"\n\nclass FailedExecution(CuratorException):\n    \"\"\"\n    Exception raised when an action fails to execute for some reason.\n    \"\"\"\n\nclass SnapshotInProgress(ActionError):\n    \"\"\"\n    Exception raised when a snapshot is already in progress\n    \"\"\"\n\nclass ActionTimeout(CuratorException):\n    \"\"\"\n    Exception raised when an action fails to complete in the allotted time\n    \"\"\"\n\nclass FailedSnapshot(CuratorException):\n    \"\"\"\n    Exception raised when a snapshot does not complete with state SUCCESS\n    \"\"\"\n\nclass FailedRestore(CuratorException):\n    \"\"\"\n    Exception raised when a Snapshot Restore does not restore all selected indices\n    \"\"\"\n\nclass FailedReindex(CuratorException):\n    \"\"\"\n    Exception raised when failures are found in the reindex task response\n    \"\"\"\n\nclass ClientException(CuratorException):\n    \"\"\"\n    Exception raised when the Elasticsearch client and/or connection is the source of the problem.\n    \"\"\"\n\nclass LoggingException(CuratorException):\n    \"\"\"\n    Exception raised when Curator cannot either log or configure logging\n    \"\"\"\n\nclass RepositoryException(CuratorException):\n    \"\"\"\n    Exception raised when Curator cannot verify a snapshot repository\n    \"\"\"\n\nclass SearchableSnapshotException(CuratorException):\n    \"\"\"\n    Exception raised when Curator finds something out of order with a Searchable Snapshot\n    \"\"\"\n"
  },
  "GT_src_dict": {
    "curator/helpers/testers.py": {
      "has_lifecycle_name": {
        "code": "def has_lifecycle_name(idx_settings):\n    \"\"\"Checks if the provided index settings contain a defined lifecycle name.\n\n:param idx_settings: A dictionary containing the settings for an index being evaluated.\n:type idx_settings: dict\n\n:returns: `True` if the index settings include a lifecycle name; otherwise, returns `False`.\n:rtype: bool\n\nThis function primarily interacts with the indexing settings of an Elasticsearch cluster. It looks for the presence of the 'lifecycle' key and subsequently checks for a 'name' key under it, which indicates that an Index Lifecycle Management (ILM) policy is associated with the index. This function does not modify any data and solely returns a boolean based on the structure of the input dictionary.\"\"\"\n    '\\n    :param idx_settings: The settings for an index being tested\\n    :type idx_settings: dict\\n\\n    :returns: ``True`` if a lifecycle name exists in settings, else ``False``\\n    :rtype: bool\\n    '\n    if 'lifecycle' in idx_settings:\n        if 'name' in idx_settings['lifecycle']:\n            return True\n    return False",
        "docstring": "Checks if the provided index settings contain a defined lifecycle name.\n\n:param idx_settings: A dictionary containing the settings for an index being evaluated.\n:type idx_settings: dict\n\n:returns: `True` if the index settings include a lifecycle name; otherwise, returns `False`.\n:rtype: bool\n\nThis function primarily interacts with the indexing settings of an Elasticsearch cluster. It looks for the presence of the 'lifecycle' key and subsequently checks for a 'name' key under it, which indicates that an Index Lifecycle Management (ILM) policy is associated with the index. This function does not modify any data and solely returns a boolean based on the structure of the input dictionary.",
        "signature": "def has_lifecycle_name(idx_settings):",
        "type": "Function",
        "class_signature": null
      },
      "is_idx_partial": {
        "code": "def is_idx_partial(idx_settings):\n    \"\"\"Checks if an index is configured as a partial snapshot in its settings.\n\n:param idx_settings: A dictionary containing the settings of the index to be tested.\n:type idx_settings: dict\n\n:returns: ``True`` if the settings indicate that the index supports partial snapshots (i.e., `store.snapshot.partial` exists and is set to `True`), otherwise returns ``False``.\n:rtype: bool\n\n:raises SearchableSnapshotException: If the index is not a mounted searchable snapshot according to the defined settings structure, indicating either that `store` or `store.snapshot` is missing.\n\nThis function interacts with the `SearchableSnapshotException`, which is defined in the `curator.exceptions` module, and is used to signify issues related to the expected structure of index settings related to searchable snapshots.\"\"\"\n    '\\n    :param idx_settings: The settings for an index being tested\\n    :type idx_settings: dict\\n\\n    :returns: ``True`` if store.snapshot.partial exists in settings, else ``False``\\n    :rtype: bool\\n    '\n    if 'store' in idx_settings:\n        if 'snapshot' in idx_settings['store']:\n            if 'partial' in idx_settings['store']['snapshot']:\n                if idx_settings['store']['snapshot']['partial']:\n                    return True\n                return False\n            return False\n        raise SearchableSnapshotException('Index not a mounted searchable snapshot')\n    raise SearchableSnapshotException('Index not a mounted searchable snapshot')",
        "docstring": "Checks if an index is configured as a partial snapshot in its settings.\n\n:param idx_settings: A dictionary containing the settings of the index to be tested.\n:type idx_settings: dict\n\n:returns: ``True`` if the settings indicate that the index supports partial snapshots (i.e., `store.snapshot.partial` exists and is set to `True`), otherwise returns ``False``.\n:rtype: bool\n\n:raises SearchableSnapshotException: If the index is not a mounted searchable snapshot according to the defined settings structure, indicating either that `store` or `store.snapshot` is missing.\n\nThis function interacts with the `SearchableSnapshotException`, which is defined in the `curator.exceptions` module, and is used to signify issues related to the expected structure of index settings related to searchable snapshots.",
        "signature": "def is_idx_partial(idx_settings):",
        "type": "Function",
        "class_signature": null
      },
      "repository_exists": {
        "code": "def repository_exists(client, repository=None):\n    \"\"\"Check the existence of a specified Elasticsearch snapshot repository.\n\nThis function uses the Elasticsearch client to verify whether a given snapshot repository exists. If the repository name is not provided, a MissingArgument exception is raised. It utilizes the `get_repository` function to query the repository and checks for its presence in the returned result.\n\nParameters:\n- client (Elasticsearch): A client connection object to interact with the Elasticsearch instance.\n- repository (str, optional): The name of the Elasticsearch snapshot repository to check.\n\nReturns:\n- bool: Returns `True` if the specified repository exists; otherwise, returns `False`.\n\nRaises:\n- MissingArgument: If no repository name is provided.\n\nDependencies:\n- Requires the `get_repository` function from the `curator.helpers.getters` module to access repository information.\n- Utilizes the `MissingArgument` exception from `curator.exceptions` for error handling.\n- Logging is handled through the Python standard logging library.\"\"\"\n    '\\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get_repository`\\n\\n    :param client: A client connection object\\n    :param repository: The Elasticsearch snapshot repository to use\\n\\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\\n    :type repository: str\\n\\n    :returns: ``True`` if ``repository`` exists, else ``False``\\n    :rtype: bool\\n    '\n    logger = logging.getLogger(__name__)\n    if not repository:\n        raise MissingArgument('No value for \"repository\" provided')\n    try:\n        test_result = get_repository(client, repository)\n        if repository in test_result:\n            logger.debug('Repository %s exists.', repository)\n            response = True\n        else:\n            logger.debug('Repository %s not found...', repository)\n            response = False\n    except Exception as err:\n        logger.debug('Unable to find repository \"%s\": Error: %s', repository, err)\n        response = False\n    return response",
        "docstring": "Check the existence of a specified Elasticsearch snapshot repository.\n\nThis function uses the Elasticsearch client to verify whether a given snapshot repository exists. If the repository name is not provided, a MissingArgument exception is raised. It utilizes the `get_repository` function to query the repository and checks for its presence in the returned result.\n\nParameters:\n- client (Elasticsearch): A client connection object to interact with the Elasticsearch instance.\n- repository (str, optional): The name of the Elasticsearch snapshot repository to check.\n\nReturns:\n- bool: Returns `True` if the specified repository exists; otherwise, returns `False`.\n\nRaises:\n- MissingArgument: If no repository name is provided.\n\nDependencies:\n- Requires the `get_repository` function from the `curator.helpers.getters` module to access repository information.\n- Utilizes the `MissingArgument` exception from `curator.exceptions` for error handling.\n- Logging is handled through the Python standard logging library.",
        "signature": "def repository_exists(client, repository=None):",
        "type": "Function",
        "class_signature": null
      },
      "rollable_alias": {
        "code": "def rollable_alias(client, alias):\n    \"\"\"Checks if an Elasticsearch alias can be used with the `_rollover` API. This function verifies if the given alias points to a valid index, and if that index qualifies for a rollover operation.\n\nParameters:\n- client (Elasticsearch): A client connection object to interact with the Elasticsearch cluster.\n- alias (str): The name of the Elasticsearch alias to be validated.\n\nReturns:\n- bool: `True` if the alias is associated with an index that can be rolled over, otherwise `False`.\n\nInteraction:\nThe function uses the `get_alias` method from the Elasticsearch IndicesClient to retrieve alias information. It checks if the alias is a write index and confirms that the index name ends with two digits or a hyphen followed by a digit, which indicates its rollable status. If the alias is not found, it logs an error message and returns `False`. The function also imports and handles `NotFoundError` from the `elasticsearch8.exceptions` module to catch missing alias scenarios.\"\"\"\n    '\\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.get_alias`\\n\\n    :param client: A client connection object\\n    :param alias: An Elasticsearch alias\\n\\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\\n    :type alias: str\\n\\n\\n    :returns: ``True`` or ``False`` depending on whether ``alias`` is an alias that\\n        points to an index that can be used by the ``_rollover`` API.\\n    :rtype: bool\\n    '\n    logger = logging.getLogger(__name__)\n    try:\n        response = client.indices.get_alias(name=alias)\n    except NotFoundError:\n        logger.error('Alias \"%s\" not found.', alias)\n        return False\n    for idx in response:\n        if 'is_write_index' in response[idx]['aliases'][alias]:\n            if response[idx]['aliases'][alias]['is_write_index']:\n                return True\n    if len(response) > 1:\n        logger.error('\"alias\" must only reference one index, but points to %s', response)\n        return False\n    index = list(response.keys())[0]\n    rollable = False\n    if index[-2:][1].isdigit():\n        if index[-2:][0].isdigit():\n            rollable = True\n        elif index[-2:][0] == '-':\n            rollable = True\n    return rollable",
        "docstring": "Checks if an Elasticsearch alias can be used with the `_rollover` API. This function verifies if the given alias points to a valid index, and if that index qualifies for a rollover operation.\n\nParameters:\n- client (Elasticsearch): A client connection object to interact with the Elasticsearch cluster.\n- alias (str): The name of the Elasticsearch alias to be validated.\n\nReturns:\n- bool: `True` if the alias is associated with an index that can be rolled over, otherwise `False`.\n\nInteraction:\nThe function uses the `get_alias` method from the Elasticsearch IndicesClient to retrieve alias information. It checks if the alias is a write index and confirms that the index name ends with two digits or a hyphen followed by a digit, which indicates its rollable status. If the alias is not found, it logs an error message and returns `False`. The function also imports and handles `NotFoundError` from the `elasticsearch8.exceptions` module to catch missing alias scenarios.",
        "signature": "def rollable_alias(client, alias):",
        "type": "Function",
        "class_signature": null
      },
      "snapshot_running": {
        "code": "def snapshot_running(client):\n    \"\"\"Returns `True` if a snapshot is currently in progress in the Elasticsearch cluster, and `False` otherwise.\n\n:param client: An instance of the Elasticsearch client used to communicate with the cluster.\n:type client: :py:class:`~.elasticsearch.Elasticsearch`\n\n:return: `True` if there are running snapshots, otherwise `False`.\n:rtype: bool\n\nThe function calls the `snapshot.status()` method from the Elasticsearch client to retrieve the snapshot status. It checks the `'snapshots'` field in the response; if it is empty, it indicates that no snapshots are in progress. Any exceptions during this call are handled by invoking the `report_failure` function, which logs the error without disrupting the execution flow.\"\"\"\n    '\\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get_repository`\\n\\n    Return ``True`` if a snapshot is in progress, and ``False`` if not\\n\\n    :param client: A client connection object\\n\\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\\n\\n    :rtype: bool\\n    '\n    try:\n        status = client.snapshot.status()['snapshots']\n    except Exception as exc:\n        report_failure(exc)\n    return False if not status else True",
        "docstring": "Returns `True` if a snapshot is currently in progress in the Elasticsearch cluster, and `False` otherwise.\n\n:param client: An instance of the Elasticsearch client used to communicate with the cluster.\n:type client: :py:class:`~.elasticsearch.Elasticsearch`\n\n:return: `True` if there are running snapshots, otherwise `False`.\n:rtype: bool\n\nThe function calls the `snapshot.status()` method from the Elasticsearch client to retrieve the snapshot status. It checks the `'snapshots'` field in the response; if it is empty, it indicates that no snapshots are in progress. Any exceptions during this call are handled by invoking the `report_failure` function, which logs the error without disrupting the execution flow.",
        "signature": "def snapshot_running(client):",
        "type": "Function",
        "class_signature": null
      },
      "validate_filters": {
        "code": "def validate_filters(action, myfilters):\n    \"\"\"Validate the appropriateness of filter specifications for a given action type, ensuring that only valid filter types are used.\n\n:param action: The name of the action to validate filters against.\n:type action: str\n:param myfilters: A list of filter specifications to be validated.\n:type myfilters: list\n\n:returns: The original list of filters if they are valid according to the action type.\n:rtype: list\n\n:raises ConfigurationError: If any filtertype in myfilters is not compatible with the specified action.\n\nThis function leverages the `snapshot_actions()` and `index_filtertypes()` functions from the `curator.defaults.settings` module. It determines the appropriate set of filter types based on the action. If an invalid filtertype is detected in myfilters, a ConfigurationError is raised, notifying the user of the incompatibility.\"\"\"\n    '\\n    Validate that myfilters are appropriate for the action type, e.g. no\\n    index filters applied to a snapshot list.\\n\\n    :param action: An action name\\n    :param myfilters: A list of filters to test.\\n\\n    :type action: str\\n    :type myfilters: list\\n\\n    :returns: Validated list of filters\\n    :rtype: list\\n    '\n    if action in snapshot_actions():\n        filtertypes = snapshot_filtertypes()\n    else:\n        filtertypes = index_filtertypes()\n    for fil in myfilters:\n        if fil['filtertype'] not in filtertypes:\n            raise ConfigurationError(f'\"{fil['filtertype']}\" filtertype is not compatible with action \"{action}\"')\n    return myfilters",
        "docstring": "Validate the appropriateness of filter specifications for a given action type, ensuring that only valid filter types are used.\n\n:param action: The name of the action to validate filters against.\n:type action: str\n:param myfilters: A list of filter specifications to be validated.\n:type myfilters: list\n\n:returns: The original list of filters if they are valid according to the action type.\n:rtype: list\n\n:raises ConfigurationError: If any filtertype in myfilters is not compatible with the specified action.\n\nThis function leverages the `snapshot_actions()` and `index_filtertypes()` functions from the `curator.defaults.settings` module. It determines the appropriate set of filter types based on the action. If an invalid filtertype is detected in myfilters, a ConfigurationError is raised, notifying the user of the incompatibility.",
        "signature": "def validate_filters(action, myfilters):",
        "type": "Function",
        "class_signature": null
      },
      "verify_client_object": {
        "code": "def verify_client_object(test):\n    \"\"\"Verify that the provided object is an instance of the Elasticsearch client.\n\n:param test: The object to be tested for validity as an Elasticsearch client.\n:type test: :py:class:`~.elasticsearch.Elasticsearch`\n\n:raises TypeError: If the provided object is not a valid Elasticsearch client instance or if it is a mock object used for testing.\n\n:returns: None if the object is valid; otherwise, an exception is raised.\n\nThis function utilizes the `logging` module to log errors in case of invalid input. It checks if the provided `test` is an instance of `Elasticsearch` from the `elasticsearch8` package, which represents a client connection to an Elasticsearch cluster. If the test variable is a mock object (commonly used in unit tests), it is ignored for the purpose of verification.\"\"\"\n    '\\n    :param test: The variable or object to test\\n\\n    :type test: :py:class:`~.elasticsearch.Elasticsearch`\\n\\n    :returns: ``True`` if ``test`` is a proper :py:class:`~.elasticsearch.Elasticsearch`\\n        client object and raise a :py:exc:`TypeError` exception if it is not.\\n    :rtype: bool\\n    '\n    logger = logging.getLogger(__name__)\n    if str(type(test)) == \"<class 'unittest.mock.Mock'>\":\n        pass\n    elif not isinstance(test, Elasticsearch):\n        msg = f'Not a valid client object. Type: {type(test)} was passed'\n        logger.error(msg)\n        raise TypeError(msg)",
        "docstring": "Verify that the provided object is an instance of the Elasticsearch client.\n\n:param test: The object to be tested for validity as an Elasticsearch client.\n:type test: :py:class:`~.elasticsearch.Elasticsearch`\n\n:raises TypeError: If the provided object is not a valid Elasticsearch client instance or if it is a mock object used for testing.\n\n:returns: None if the object is valid; otherwise, an exception is raised.\n\nThis function utilizes the `logging` module to log errors in case of invalid input. It checks if the provided `test` is an instance of `Elasticsearch` from the `elasticsearch8` package, which represents a client connection to an Elasticsearch cluster. If the test variable is a mock object (commonly used in unit tests), it is ignored for the purpose of verification.",
        "signature": "def verify_client_object(test):",
        "type": "Function",
        "class_signature": null
      },
      "verify_repository": {
        "code": "def verify_repository(client, repository=None):\n    \"\"\"Verify the existence and accessibility of an Elasticsearch snapshot repository.\n\nThis function attempts to verify whether the specified repository exists and if all nodes can write to it. If the verification fails, it raises a `RepositoryException` with a descriptive error message.\n\nParameters:\n- client (Elasticsearch): An instance of the Elasticsearch client, used to interact with the Elasticsearch cluster.\n- repository (str, optional): The name of the snapshot repository to verify. Must be provided.\n\nReturns:\n- None: This function does not return a value. It raises an exception if verification fails.\n\nDependencies:\n- This function utilizes the `verify_repository` method from the `elasticsearch` client and manages errors that may arise during the process. Any issue encountered will be logged via the standard logger.\n- Raises `RepositoryException` from `curator.exceptions` if the verification fails or if the repository does not exist, ensuring appropriate error handling.\"\"\"\n    '\\n    Do :py:meth:`~.elasticsearch.snapshot.verify_repository` call. If it fails, raise a\\n    :py:exc:`~.curator.exceptions.RepositoryException`.\\n\\n    :param client: A client connection object\\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\\n    :param repository: A repository name\\n\\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\\n    :type repository: str\\n\\n    :rtype: None\\n    '\n    logger = logging.getLogger(__name__)\n    try:\n        nodes = client.snapshot.verify_repository(name=repository)['nodes']\n        logger.debug('All nodes can write to the repository')\n        logger.debug('Nodes with verified repository access: %s', nodes)\n    except Exception as err:\n        try:\n            if err.status_code == 404:\n                msg = f'--- Repository \"{repository}\" not found. Error: {err.meta.status}, {err.error}'\n            else:\n                msg = f'--- Got a {err.meta.status} response from Elasticsearch.  Error message: {err.error}'\n        except AttributeError:\n            msg = f'--- Error message: {err}'.format()\n        report = f'Failed to verify all nodes have repository access: {msg}'\n        raise RepositoryException(report) from err",
        "docstring": "Verify the existence and accessibility of an Elasticsearch snapshot repository.\n\nThis function attempts to verify whether the specified repository exists and if all nodes can write to it. If the verification fails, it raises a `RepositoryException` with a descriptive error message.\n\nParameters:\n- client (Elasticsearch): An instance of the Elasticsearch client, used to interact with the Elasticsearch cluster.\n- repository (str, optional): The name of the snapshot repository to verify. Must be provided.\n\nReturns:\n- None: This function does not return a value. It raises an exception if verification fails.\n\nDependencies:\n- This function utilizes the `verify_repository` method from the `elasticsearch` client and manages errors that may arise during the process. Any issue encountered will be logged via the standard logger.\n- Raises `RepositoryException` from `curator.exceptions` if the verification fails or if the repository does not exist, ensuring appropriate error handling.",
        "signature": "def verify_repository(client, repository=None):",
        "type": "Function",
        "class_signature": null
      }
    },
    "curator/defaults/settings.py": {
      "snapshot_actions": {
        "code": "def snapshot_actions():\n    \"\"\"Returns a list of supported snapshot actions for managing snapshots in Elasticsearch. Currently, the actions included are 'delete_snapshots' to remove existing snapshots and 'restore' to retrieve snapshots back into the cluster. This function can be utilized for determining available operations in scripts managing Elasticsearch snapshot lifecycle. No parameters are needed, and it does not have any side effects. The function interacts with the overarching context of the code, which defines various actions (including cluster and index actions) helpful for Elasticsearch operations.\"\"\"\n    \"\\n    :returns: The list of supported snapshot actions: ['delete_snapshots', 'restore']\\n    \"\n    return ['delete_snapshots', 'restore']",
        "docstring": "Returns a list of supported snapshot actions for managing snapshots in Elasticsearch. Currently, the actions included are 'delete_snapshots' to remove existing snapshots and 'restore' to retrieve snapshots back into the cluster. This function can be utilized for determining available operations in scripts managing Elasticsearch snapshot lifecycle. No parameters are needed, and it does not have any side effects. The function interacts with the overarching context of the code, which defines various actions (including cluster and index actions) helpful for Elasticsearch operations.",
        "signature": "def snapshot_actions():",
        "type": "Function",
        "class_signature": null
      },
      "index_filtertypes": {
        "code": "def index_filtertypes():\n    \"\"\"Returns a list of supported index filter types used in the Curator utility for managing Elasticsearch indices. The function provides a predefined set of filter types that can be applied to indices for various operations like aliasing, allocation, age verification, and more.\n\n:returns: A list of strings representing supported index filter types, including:\n    - 'alias'\n    - 'allocated'\n    - 'age'\n    - 'closed'\n    - 'count'\n    - 'empty'\n    - 'forcemerged'\n    - 'ilm'\n    - 'kibana'\n    - 'none'\n    - 'opened'\n    - 'pattern'\n    - 'period'\n    - 'space'\n    - 'shards'\n    - 'size'\n\nThis function interacts with other parts of the code, such as the `all_filtertypes()` function, which combines both index and snapshot filter types. The availability of various filter types enables users to customize index management operations based on specific criteria.\"\"\"\n    \"\\n    :returns: The list of supported index filter types:\\n        ['alias', 'allocated', 'age', 'closed', 'count', 'empty', 'forcemerged',\\n        'ilm', 'kibana', 'none', 'opened', 'pattern', 'period', 'space',\\n        'shards', 'size']\\n    \"\n    return ['alias', 'allocated', 'age', 'closed', 'count', 'empty', 'forcemerged', 'ilm', 'kibana', 'none', 'opened', 'pattern', 'period', 'space', 'shards', 'size']",
        "docstring": "Returns a list of supported index filter types used in the Curator utility for managing Elasticsearch indices. The function provides a predefined set of filter types that can be applied to indices for various operations like aliasing, allocation, age verification, and more.\n\n:returns: A list of strings representing supported index filter types, including:\n    - 'alias'\n    - 'allocated'\n    - 'age'\n    - 'closed'\n    - 'count'\n    - 'empty'\n    - 'forcemerged'\n    - 'ilm'\n    - 'kibana'\n    - 'none'\n    - 'opened'\n    - 'pattern'\n    - 'period'\n    - 'space'\n    - 'shards'\n    - 'size'\n\nThis function interacts with other parts of the code, such as the `all_filtertypes()` function, which combines both index and snapshot filter types. The availability of various filter types enables users to customize index management operations based on specific criteria.",
        "signature": "def index_filtertypes():",
        "type": "Function",
        "class_signature": null
      },
      "snapshot_filtertypes": {
        "code": "def snapshot_filtertypes():\n    \"\"\"Retrieves the list of supported snapshot filter types for use in managing Elasticsearch snapshots.\n\nThis function does not take any parameters and returns a list of strings representing the available filter types that can be applied to snapshots: \n['age', 'count', 'none', 'pattern', 'period', 'state']. \n\nThe returned filter types are utilized elsewhere in the code, primarily in operations related to filtering snapshot data based on various criteria. This function is part of a larger suite of functions that manage index and snapshot actions in an Elasticsearch curator context.\"\"\"\n    \"\\n    :returns: The list of supported snapshot filter types: ['age', 'count', 'none',\\n        'pattern', 'period', 'state']\\n    \"\n    return ['age', 'count', 'none', 'pattern', 'period', 'state']",
        "docstring": "Retrieves the list of supported snapshot filter types for use in managing Elasticsearch snapshots.\n\nThis function does not take any parameters and returns a list of strings representing the available filter types that can be applied to snapshots: \n['age', 'count', 'none', 'pattern', 'period', 'state']. \n\nThe returned filter types are utilized elsewhere in the code, primarily in operations related to filtering snapshot data based on various criteria. This function is part of a larger suite of functions that manage index and snapshot actions in an Elasticsearch curator context.",
        "signature": "def snapshot_filtertypes():",
        "type": "Function",
        "class_signature": null
      }
    },
    "curator/helpers/utils.py": {
      "report_failure": {
        "code": "def report_failure(exception):\n    \"\"\"Raises a `FailedExecution` exception to signal a failure in the execution of an operation, including the original error message for debugging purposes. \n\n:param exception: The upstream exception that caused the failure.\n:type exception: Exception\n\n:raises FailedExecution: Always raises this specific exception with a standardized error message that includes the original exception details.\n\nThis function is dependent on the `FailedExecution` class from the `curator.exceptions` module, which provides a structured way to handle execution failures within the broader context of the application.\"\"\"\n    '\\n    Raise a :py:exc:`~.curator.exceptions.FailedExecution` exception and include the original error\\n    message.\\n\\n    :param exception: The upstream exception.\\n\\n    :type exception: :py:exc:Exception\\n\\n    :rtype: None\\n    '\n    raise FailedExecution(f'Exception encountered.  Rerun with loglevel DEBUG and/or check Elasticsearch logs formore information. Exception: {exception}')",
        "docstring": "Raises a `FailedExecution` exception to signal a failure in the execution of an operation, including the original error message for debugging purposes. \n\n:param exception: The upstream exception that caused the failure.\n:type exception: Exception\n\n:raises FailedExecution: Always raises this specific exception with a standardized error message that includes the original exception details.\n\nThis function is dependent on the `FailedExecution` class from the `curator.exceptions` module, which provides a structured way to handle execution failures within the broader context of the application.",
        "signature": "def report_failure(exception):",
        "type": "Function",
        "class_signature": null
      }
    },
    "curator/helpers/getters.py": {
      "get_repository": {
        "code": "def get_repository(client, repository=''):\n    \"\"\"Retrieve the configuration information for a specified Elasticsearch snapshot repository.\n\nThis function interfaces with the Elasticsearch SnapshotClient to fetch details about a given repository by its name. It requires a client connection to the Elasticsearch cluster and the name of the repository as parameters. If the repository cannot be found or accessed, it raises a CuratorException with a relevant error message.\n\nParameters:\n- client (Elasticsearch): An Elasticsearch client connection object used to communicate with the cluster.\n- repository (str): The name of the snapshot repository to retrieve information for. Defaults to an empty string.\n\nReturns:\n- dict: A dictionary containing the configuration details of the specified snapshot repository.\n\nRaises:\n- CuratorException: If there is a TransportError or NotFoundError when trying to access the repository, providing context about the failure.\n\nDependencies:\n- `es8exc` from the `elasticsearch8` module is used to handle exceptions related to the Elasticsearch transport layer.\n- `CuratorException` from `curator.exceptions` is raised for error handling.\"\"\"\n    '\\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get_repository`\\n\\n    :param client: A client connection object\\n    :param repository: The Elasticsearch snapshot repository to use\\n\\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\\n    :type repository: str\\n\\n    :returns: Configuration information for ``repository``.\\n    :rtype: dict\\n    '\n    try:\n        return client.snapshot.get_repository(name=repository)\n    except (es8exc.TransportError, es8exc.NotFoundError) as err:\n        msg = f'Unable to get repository {repository}.  Error: {err} Check Elasticsearch logs for more information.'\n        raise CuratorException(msg) from err",
        "docstring": "Retrieve the configuration information for a specified Elasticsearch snapshot repository.\n\nThis function interfaces with the Elasticsearch SnapshotClient to fetch details about a given repository by its name. It requires a client connection to the Elasticsearch cluster and the name of the repository as parameters. If the repository cannot be found or accessed, it raises a CuratorException with a relevant error message.\n\nParameters:\n- client (Elasticsearch): An Elasticsearch client connection object used to communicate with the cluster.\n- repository (str): The name of the snapshot repository to retrieve information for. Defaults to an empty string.\n\nReturns:\n- dict: A dictionary containing the configuration details of the specified snapshot repository.\n\nRaises:\n- CuratorException: If there is a TransportError or NotFoundError when trying to access the repository, providing context about the failure.\n\nDependencies:\n- `es8exc` from the `elasticsearch8` module is used to handle exceptions related to the Elasticsearch transport layer.\n- `CuratorException` from `curator.exceptions` is raised for error handling.",
        "signature": "def get_repository(client, repository=''):",
        "type": "Function",
        "class_signature": null
      }
    },
    "curator/exceptions.py": {}
  },
  "dependency_dict": {
    "curator/helpers/testers.py:repository_exists": {},
    "curator/helpers/getters.py:get_repository": {},
    "curator/helpers/testers.py:snapshot_running": {},
    "curator/helpers/utils.py:report_failure": {},
    "curator/helpers/testers.py:validate_filters": {},
    "curator/defaults/settings.py:snapshot_actions": {},
    "curator/defaults/settings.py:index_filtertypes": {},
    "curator/defaults/settings.py:snapshot_filtertypes": {}
  },
  "call_tree": {
    "tests/unit/test_helpers_testers.py:TestRepositoryExists:test_missing_arg": {
      "curator/helpers/testers.py:repository_exists": {}
    },
    "tests/unit/test_helpers_testers.py:TestRepositoryExists:test_repo_not_in_results": {
      "curator/helpers/testers.py:repository_exists": {
        "curator/helpers/getters.py:get_repository": {}
      }
    },
    "tests/unit/test_helpers_testers.py:TestRepositoryExists:test_repository_in_results": {
      "curator/helpers/testers.py:repository_exists": {
        "curator/helpers/getters.py:get_repository": {}
      }
    },
    "tests/unit/test_helpers_testers.py:TestRollableAlias:test_return_false_if_no_alias": {
      "curator/helpers/testers.py:rollable_alias": {}
    },
    "tests/unit/test_helpers_testers.py:TestRollableAlias:test_return_false_non_numeric": {
      "curator/helpers/testers.py:rollable_alias": {}
    },
    "tests/unit/test_helpers_testers.py:TestRollableAlias:test_return_false_too_many_indices": {
      "curator/helpers/testers.py:rollable_alias": {}
    },
    "tests/unit/test_helpers_testers.py:TestRollableAlias:test_return_true_hyphenated": {
      "curator/helpers/testers.py:rollable_alias": {}
    },
    "tests/unit/test_helpers_testers.py:TestRollableAlias:test_return_true_two_digits": {
      "curator/helpers/testers.py:rollable_alias": {}
    },
    "tests/unit/test_helpers_testers.py:TestSnapshotRunning:test_false": {
      "curator/helpers/testers.py:snapshot_running": {}
    },
    "tests/unit/test_helpers_testers.py:TestSnapshotRunning:test_raises_exception": {
      "curator/helpers/testers.py:snapshot_running": {
        "curator/helpers/utils.py:report_failure": {}
      }
    },
    "tests/unit/test_helpers_testers.py:TestSnapshotRunning:test_true": {
      "curator/helpers/testers.py:snapshot_running": {}
    },
    "tests/unit/test_helpers_testers.py:TestValidateFilters:test_index_with_snapshot_filter": {
      "curator/helpers/testers.py:validate_filters": {
        "curator/defaults/settings.py:snapshot_actions": {},
        "curator/defaults/settings.py:index_filtertypes": {}
      }
    },
    "tests/unit/test_helpers_testers.py:TestValidateFilters:test_snapshot_with_index_filter": {
      "curator/helpers/testers.py:validate_filters": {
        "curator/defaults/settings.py:snapshot_actions": {},
        "curator/defaults/settings.py:snapshot_filtertypes": {}
      }
    },
    "tests/unit/test_helpers_testers.py:TestVerifyClientObject:test_is_client_object": {
      "curator/helpers/testers.py:verify_client_object": {}
    },
    "tests/unit/test_helpers_testers.py:TestVerifyClientObject:test_is_not_client_object": {
      "curator/helpers/testers.py:verify_client_object": {}
    },
    "tests/unit/test_helpers_testers.py:TestVerifyRepository:test_passing": {
      "curator/helpers/testers.py:verify_repository": {}
    },
    "tests/unit/test_helpers_testers.py:TestVerifyRepository:test_raises_401": {
      "curator/helpers/testers.py:verify_repository": {}
    },
    "tests/unit/test_helpers_testers.py:TestVerifyRepository:test_raises_404": {
      "curator/helpers/testers.py:verify_repository": {}
    },
    "tests/unit/test_helpers_testers.py:TestVerifyRepository:test_raises_other": {
      "curator/helpers/testers.py:verify_repository": {}
    },
    "tests/unit/test_helpers_testers.py:TestHasLifecycleName:test_has_lifecycle_name": {
      "curator/helpers/testers.py:has_lifecycle_name": {}
    },
    "tests/unit/test_helpers_testers.py:TestHasLifecycleName:test_has_no_lifecycle_name": {
      "curator/helpers/testers.py:has_lifecycle_name": {}
    },
    "tests/unit/test_helpers_testers.py:TestIsIdxPartial:test_is_idx_partial": {
      "curator/helpers/testers.py:is_idx_partial": {}
    },
    "tests/unit/test_helpers_testers.py:TestIsIdxPartial:test_is_idx_partial_false1": {
      "curator/helpers/testers.py:is_idx_partial": {}
    },
    "tests/unit/test_helpers_testers.py:TestIsIdxPartial:test_is_idx_partial_false2": {
      "curator/helpers/testers.py:is_idx_partial": {}
    },
    "tests/unit/test_helpers_testers.py:TestIsIdxPartial:test_is_idx_partial_raises1": {
      "curator/helpers/testers.py:is_idx_partial": {}
    },
    "tests/unit/test_helpers_testers.py:TestIsIdxPartial:test_is_idx_partial_raises2": {
      "curator/helpers/testers.py:is_idx_partial": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/elasticsearch_curator-image-test_helpers_testers/elasticsearch_curator-test_helpers_testers/tests/integration/test_cli.py:TestCLIMethods:test_action_is_none": {
      "curator/exceptions.py:ConfigurationError:ConfigurationError": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/elasticsearch_curator-image-test_helpers_testers/elasticsearch_curator-test_helpers_testers/tests/integration/test_cli.py:TestCLIMethods:test_no_action": {
      "curator/exceptions.py:ConfigurationError:ConfigurationError": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/elasticsearch_curator-image-test_helpers_testers/elasticsearch_curator-test_helpers_testers/tests/integration/test_integrations.py:TestFilters:test_filter_by_alias_bad_aliases": {
      "curator/exceptions.py:ConfigurationError:ConfigurationError": {}
    }
  },
  "PRD": "# PROJECT NAME: elasticsearch_curator-test_helpers_testers\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 curator/\n    \u251c\u2500\u2500 defaults/\n    \u2502   \u2514\u2500\u2500 settings.py\n    \u2502       \u251c\u2500\u2500 index_filtertypes\n    \u2502       \u251c\u2500\u2500 snapshot_actions\n    \u2502       \u2514\u2500\u2500 snapshot_filtertypes\n    \u251c\u2500\u2500 exceptions.py\n    \u2502   \u2514\u2500\u2500 ConfigurationError.ConfigurationError\n    \u2514\u2500\u2500 helpers/\n        \u251c\u2500\u2500 getters.py\n        \u2502   \u2514\u2500\u2500 get_repository\n        \u251c\u2500\u2500 testers.py\n        \u2502   \u251c\u2500\u2500 has_lifecycle_name\n        \u2502   \u251c\u2500\u2500 is_idx_partial\n        \u2502   \u251c\u2500\u2500 repository_exists\n        \u2502   \u251c\u2500\u2500 rollable_alias\n        \u2502   \u251c\u2500\u2500 snapshot_running\n        \u2502   \u251c\u2500\u2500 validate_filters\n        \u2502   \u251c\u2500\u2500 verify_client_object\n        \u2502   \u2514\u2500\u2500 verify_repository\n        \u2514\u2500\u2500 utils.py\n            \u2514\u2500\u2500 report_failure\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThis module is designed to test the functionality and reliability of utility methods that support operations within Elasticsearch environments, particularly for snapshot management, index aliasing, configuration validation, and repository verification. It ensures that critical helper functions can accurately determine the existence of Elasticsearch repositories, validate client and repository configurations, assess the status of aliases and snapshots, and handle common edge cases, such as missing parameters or invalid configurations. By providing robust testing for these utilities, the module helps developers ensure consistency, reliability, and error-handling in Elasticsearch-related operations, streamlining the management of complex workflows and reducing the risk of misconfigurations or execution failures.\n\n## FILE 1: curator/helpers/testers.py\n\n- FUNCTION NAME: validate_filters\n  - SIGNATURE: def validate_filters(action, myfilters):\n  - DOCSTRING: \n```python\n\"\"\"\nValidate the appropriateness of filter specifications for a given action type, ensuring that only valid filter types are used.\n\n:param action: The name of the action to validate filters against.\n:type action: str\n:param myfilters: A list of filter specifications to be validated.\n:type myfilters: list\n\n:returns: The original list of filters if they are valid according to the action type.\n:rtype: list\n\n:raises ConfigurationError: If any filtertype in myfilters is not compatible with the specified action.\n\nThis function leverages the `snapshot_actions()` and `index_filtertypes()` functions from the `curator.defaults.settings` module. It determines the appropriate set of filter types based on the action. If an invalid filtertype is detected in myfilters, a ConfigurationError is raised, notifying the user of the incompatibility.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - curator/defaults/settings.py:index_filtertypes\n    - curator/defaults/settings.py:snapshot_filtertypes\n    - curator/defaults/settings.py:snapshot_actions\n\n- FUNCTION NAME: repository_exists\n  - SIGNATURE: def repository_exists(client, repository=None):\n  - DOCSTRING: \n```python\n\"\"\"\nCheck the existence of a specified Elasticsearch snapshot repository.\n\nThis function uses the Elasticsearch client to verify whether a given snapshot repository exists. If the repository name is not provided, a MissingArgument exception is raised. It utilizes the `get_repository` function to query the repository and checks for its presence in the returned result.\n\nParameters:\n- client (Elasticsearch): A client connection object to interact with the Elasticsearch instance.\n- repository (str, optional): The name of the Elasticsearch snapshot repository to check.\n\nReturns:\n- bool: Returns `True` if the specified repository exists; otherwise, returns `False`.\n\nRaises:\n- MissingArgument: If no repository name is provided.\n\nDependencies:\n- Requires the `get_repository` function from the `curator.helpers.getters` module to access repository information.\n- Utilizes the `MissingArgument` exception from `curator.exceptions` for error handling.\n- Logging is handled through the Python standard logging library.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - curator/helpers/getters.py:get_repository\n\n- FUNCTION NAME: snapshot_running\n  - SIGNATURE: def snapshot_running(client):\n  - DOCSTRING: \n```python\n\"\"\"\nReturns `True` if a snapshot is currently in progress in the Elasticsearch cluster, and `False` otherwise.\n\n:param client: An instance of the Elasticsearch client used to communicate with the cluster.\n:type client: :py:class:`~.elasticsearch.Elasticsearch`\n\n:return: `True` if there are running snapshots, otherwise `False`.\n:rtype: bool\n\nThe function calls the `snapshot.status()` method from the Elasticsearch client to retrieve the snapshot status. It checks the `'snapshots'` field in the response; if it is empty, it indicates that no snapshots are in progress. Any exceptions during this call are handled by invoking the `report_failure` function, which logs the error without disrupting the execution flow.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - curator/helpers/utils.py:report_failure\n\n- FUNCTION NAME: verify_repository\n  - SIGNATURE: def verify_repository(client, repository=None):\n  - DOCSTRING: \n```python\n\"\"\"\nVerify the existence and accessibility of an Elasticsearch snapshot repository.\n\nThis function attempts to verify whether the specified repository exists and if all nodes can write to it. If the verification fails, it raises a `RepositoryException` with a descriptive error message.\n\nParameters:\n- client (Elasticsearch): An instance of the Elasticsearch client, used to interact with the Elasticsearch cluster.\n- repository (str, optional): The name of the snapshot repository to verify. Must be provided.\n\nReturns:\n- None: This function does not return a value. It raises an exception if verification fails.\n\nDependencies:\n- This function utilizes the `verify_repository` method from the `elasticsearch` client and manages errors that may arise during the process. Any issue encountered will be logged via the standard logger.\n- Raises `RepositoryException` from `curator.exceptions` if the verification fails or if the repository does not exist, ensuring appropriate error handling.\n\"\"\"\n```\n\n- FUNCTION NAME: is_idx_partial\n  - SIGNATURE: def is_idx_partial(idx_settings):\n  - DOCSTRING: \n```python\n\"\"\"\nChecks if an index is configured as a partial snapshot in its settings.\n\n:param idx_settings: A dictionary containing the settings of the index to be tested.\n:type idx_settings: dict\n\n:returns: ``True`` if the settings indicate that the index supports partial snapshots (i.e., `store.snapshot.partial` exists and is set to `True`), otherwise returns ``False``.\n:rtype: bool\n\n:raises SearchableSnapshotException: If the index is not a mounted searchable snapshot according to the defined settings structure, indicating either that `store` or `store.snapshot` is missing.\n\nThis function interacts with the `SearchableSnapshotException`, which is defined in the `curator.exceptions` module, and is used to signify issues related to the expected structure of index settings related to searchable snapshots.\n\"\"\"\n```\n\n- FUNCTION NAME: verify_client_object\n  - SIGNATURE: def verify_client_object(test):\n  - DOCSTRING: \n```python\n\"\"\"\nVerify that the provided object is an instance of the Elasticsearch client.\n\n:param test: The object to be tested for validity as an Elasticsearch client.\n:type test: :py:class:`~.elasticsearch.Elasticsearch`\n\n:raises TypeError: If the provided object is not a valid Elasticsearch client instance or if it is a mock object used for testing.\n\n:returns: None if the object is valid; otherwise, an exception is raised.\n\nThis function utilizes the `logging` module to log errors in case of invalid input. It checks if the provided `test` is an instance of `Elasticsearch` from the `elasticsearch8` package, which represents a client connection to an Elasticsearch cluster. If the test variable is a mock object (commonly used in unit tests), it is ignored for the purpose of verification.\n\"\"\"\n```\n\n- FUNCTION NAME: rollable_alias\n  - SIGNATURE: def rollable_alias(client, alias):\n  - DOCSTRING: \n```python\n\"\"\"\nChecks if an Elasticsearch alias can be used with the `_rollover` API. This function verifies if the given alias points to a valid index, and if that index qualifies for a rollover operation.\n\nParameters:\n- client (Elasticsearch): A client connection object to interact with the Elasticsearch cluster.\n- alias (str): The name of the Elasticsearch alias to be validated.\n\nReturns:\n- bool: `True` if the alias is associated with an index that can be rolled over, otherwise `False`.\n\nInteraction:\nThe function uses the `get_alias` method from the Elasticsearch IndicesClient to retrieve alias information. It checks if the alias is a write index and confirms that the index name ends with two digits or a hyphen followed by a digit, which indicates its rollable status. If the alias is not found, it logs an error message and returns `False`. The function also imports and handles `NotFoundError` from the `elasticsearch8.exceptions` module to catch missing alias scenarios.\n\"\"\"\n```\n\n- FUNCTION NAME: has_lifecycle_name\n  - SIGNATURE: def has_lifecycle_name(idx_settings):\n  - DOCSTRING: \n```python\n\"\"\"\nChecks if the provided index settings contain a defined lifecycle name.\n\n:param idx_settings: A dictionary containing the settings for an index being evaluated.\n:type idx_settings: dict\n\n:returns: `True` if the index settings include a lifecycle name; otherwise, returns `False`.\n:rtype: bool\n\nThis function primarily interacts with the indexing settings of an Elasticsearch cluster. It looks for the presence of the 'lifecycle' key and subsequently checks for a 'name' key under it, which indicates that an Index Lifecycle Management (ILM) policy is associated with the index. This function does not modify any data and solely returns a boolean based on the structure of the input dictionary.\n\"\"\"\n```\n\n## FILE 2: curator/defaults/settings.py\n\n- FUNCTION NAME: index_filtertypes\n  - SIGNATURE: def index_filtertypes():\n  - DOCSTRING: \n```python\n\"\"\"\nReturns a list of supported index filter types used in the Curator utility for managing Elasticsearch indices. The function provides a predefined set of filter types that can be applied to indices for various operations like aliasing, allocation, age verification, and more.\n\n:returns: A list of strings representing supported index filter types, including:\n    - 'alias'\n    - 'allocated'\n    - 'age'\n    - 'closed'\n    - 'count'\n    - 'empty'\n    - 'forcemerged'\n    - 'ilm'\n    - 'kibana'\n    - 'none'\n    - 'opened'\n    - 'pattern'\n    - 'period'\n    - 'space'\n    - 'shards'\n    - 'size'\n\nThis function interacts with other parts of the code, such as the `all_filtertypes()` function, which combines both index and snapshot filter types. The availability of various filter types enables users to customize index management operations based on specific criteria.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - curator/helpers/testers.py:validate_filters\n\n- FUNCTION NAME: snapshot_filtertypes\n  - SIGNATURE: def snapshot_filtertypes():\n  - DOCSTRING: \n```python\n\"\"\"\nRetrieves the list of supported snapshot filter types for use in managing Elasticsearch snapshots.\n\nThis function does not take any parameters and returns a list of strings representing the available filter types that can be applied to snapshots: \n['age', 'count', 'none', 'pattern', 'period', 'state']. \n\nThe returned filter types are utilized elsewhere in the code, primarily in operations related to filtering snapshot data based on various criteria. This function is part of a larger suite of functions that manage index and snapshot actions in an Elasticsearch curator context.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - curator/helpers/testers.py:validate_filters\n\n- FUNCTION NAME: snapshot_actions\n  - SIGNATURE: def snapshot_actions():\n  - DOCSTRING: \n```python\n\"\"\"\nReturns a list of supported snapshot actions for managing snapshots in Elasticsearch. Currently, the actions included are 'delete_snapshots' to remove existing snapshots and 'restore' to retrieve snapshots back into the cluster. This function can be utilized for determining available operations in scripts managing Elasticsearch snapshot lifecycle. No parameters are needed, and it does not have any side effects. The function interacts with the overarching context of the code, which defines various actions (including cluster and index actions) helpful for Elasticsearch operations.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - curator/helpers/testers.py:validate_filters\n\n## FILE 3: curator/helpers/utils.py\n\n- FUNCTION NAME: report_failure\n  - SIGNATURE: def report_failure(exception):\n  - DOCSTRING: \n```python\n\"\"\"\nRaises a `FailedExecution` exception to signal a failure in the execution of an operation, including the original error message for debugging purposes. \n\n:param exception: The upstream exception that caused the failure.\n:type exception: Exception\n\n:raises FailedExecution: Always raises this specific exception with a standardized error message that includes the original exception details.\n\nThis function is dependent on the `FailedExecution` class from the `curator.exceptions` module, which provides a structured way to handle execution failures within the broader context of the application.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - curator/helpers/testers.py:snapshot_running\n\n## FILE 4: curator/helpers/getters.py\n\n- FUNCTION NAME: get_repository\n  - SIGNATURE: def get_repository(client, repository=''):\n  - DOCSTRING: \n```python\n\"\"\"\nRetrieve the configuration information for a specified Elasticsearch snapshot repository.\n\nThis function interfaces with the Elasticsearch SnapshotClient to fetch details about a given repository by its name. It requires a client connection to the Elasticsearch cluster and the name of the repository as parameters. If the repository cannot be found or accessed, it raises a CuratorException with a relevant error message.\n\nParameters:\n- client (Elasticsearch): An Elasticsearch client connection object used to communicate with the cluster.\n- repository (str): The name of the snapshot repository to retrieve information for. Defaults to an empty string.\n\nReturns:\n- dict: A dictionary containing the configuration details of the specified snapshot repository.\n\nRaises:\n- CuratorException: If there is a TransportError or NotFoundError when trying to access the repository, providing context about the failure.\n\nDependencies:\n- `es8exc` from the `elasticsearch8` module is used to handle exceptions related to the Elasticsearch transport layer.\n- `CuratorException` from `curator.exceptions` is raised for error handling.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - curator/helpers/testers.py:repository_exists\n\n## FILE 5: curator/exceptions.py\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "curator/helpers/testers.py": "\"\"\"Utility functions that get things\"\"\"\nimport logging\nfrom voluptuous import Schema\nfrom elasticsearch8 import Elasticsearch\nfrom elasticsearch8.exceptions import NotFoundError\nfrom es_client.helpers.schemacheck import SchemaCheck\nfrom es_client.helpers.utils import prune_nones\nfrom curator.helpers.getters import get_repository, get_write_index\nfrom curator.exceptions import ConfigurationError, MissingArgument, RepositoryException, SearchableSnapshotException\nfrom curator.defaults.settings import index_filtertypes, snapshot_actions, snapshot_filtertypes\nfrom curator.validators import actions, options\nfrom curator.validators.filter_functions import validfilters\nfrom curator.helpers.utils import report_failure\n\ndef ilm_policy_check(client, alias):\n    \"\"\"Test if alias is associated with an ILM policy\n\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.get_settings`\n\n    :param client: A client connection object\n    :param alias: The alias name\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type alias: str\n    :rtype: bool\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    write_index = get_write_index(client, alias)\n    try:\n        idx_settings = client.indices.get_settings(index=write_index)\n        if 'name' in idx_settings[write_index]['settings']['index']['lifecycle']:\n            return True\n    except KeyError:\n        logger.debug('No ILM policies associated with %s', alias)\n    return False\n\ndef validate_actions(data):\n    \"\"\"\n    Validate the ``actions`` configuration dictionary, as imported from actions.yml,\n    for example.\n\n    :param data: The configuration dictionary\n\n    :type data: dict\n\n    :returns: The validated and sanitized configuration dictionary.\n    :rtype: dict\n    \"\"\"\n    clean_config = {}\n    root = SchemaCheck(data, actions.root(), 'Actions File', 'root').result()\n    for action_id in root['actions']:\n        action_dict = root['actions'][action_id]\n        loc = f'Action ID \"{action_id}\"'\n        valid_structure = SchemaCheck(action_dict, actions.structure(action_dict, loc), 'structure', loc).result()\n        current_action = valid_structure['action']\n        loc = f'Action ID \"{action_id}\", action \"{current_action}\"'\n        clean_options = SchemaCheck(prune_nones(valid_structure['options']), options.get_schema(current_action), 'options', loc).result()\n        clean_config[action_id] = {'action': current_action, 'description': valid_structure['description'], 'options': clean_options}\n        if current_action == 'alias':\n            add_remove = {}\n            for k in ['add', 'remove']:\n                if k in valid_structure:\n                    current_filters = SchemaCheck(valid_structure[k]['filters'], Schema(validfilters(current_action, location=loc)), f'\"{k}\" filters', f'{loc}, \"filters\"').result()\n                    add_remove.update({k: {'filters': SchemaCheck(current_filters, Schema(validfilters(current_action, location=loc)), 'filters', f'{loc}, \"{k}\", \"filters\"').result()}})\n            clean_config[action_id].update(add_remove)\n        elif current_action in ['cluster_routing', 'create_index', 'rollover']:\n            pass\n        else:\n            valid_filters = SchemaCheck(valid_structure['filters'], Schema(validfilters(current_action, location=loc)), 'filters', f'{loc}, \"filters\"').result()\n            clean_filters = validate_filters(current_action, valid_filters)\n            clean_config[action_id].update({'filters': clean_filters})\n        if current_action == 'reindex':\n            if 'remote_filters' in valid_structure['options']:\n                valid_filters = SchemaCheck(valid_structure['options']['remote_filters'], Schema(validfilters(current_action, location=loc)), 'filters', f'{loc}, \"filters\"').result()\n                clean_remote_filters = validate_filters(current_action, valid_filters)\n                clean_config[action_id]['options'].update({'remote_filters': clean_remote_filters})\n    return {'actions': clean_config}\n\ndef verify_index_list(test):\n    \"\"\"\n    :param test: The variable or object to test\n\n    :type test: :py:class:`~.curator.IndexList`\n\n    :returns: ``None`` if ``test`` is a proper :py:class:`~.curator.indexlist.IndexList`\n        object, else raise a :py:class:`TypeError` exception.\n    :rtype: None\n    \"\"\"\n    from curator.indexlist import IndexList\n    logger = logging.getLogger(__name__)\n    if not isinstance(test, IndexList):\n        msg = f'Not a valid IndexList object. Type: {type(test)} was passed'\n        logger.error(msg)\n        raise TypeError(msg)\n\ndef verify_snapshot_list(test):\n    \"\"\"\n    :param test: The variable or object to test\n\n    :type test: :py:class:`~.curator.SnapshotList`\n\n    :returns: ``None`` if ``test`` is a proper\n        :py:class:`~.curator.snapshotlist.SnapshotList` object, else raise a\n        :py:class:`TypeError` exception.\n    :rtype: None\n    \"\"\"\n    from curator.snapshotlist import SnapshotList\n    logger = logging.getLogger(__name__)\n    if not isinstance(test, SnapshotList):\n        msg = f'Not a valid SnapshotList object. Type: {type(test)} was passed'\n        logger.error(msg)\n        raise TypeError(msg)",
    "curator/defaults/settings.py": "\"\"\"Utilities/Helpers for defaults and schemas\"\"\"\nfrom os import path\nfrom voluptuous import Any, Boolean, Coerce, Optional\nfrom curator.exceptions import CuratorException\nCURATOR_DOCS = 'https://www.elastic.co/guide/en/elasticsearch/client/curator'\nCLICK_DRYRUN = {'dry-run': {'help': 'Do not perform any changes.', 'is_flag': True}}\nDATA_NODE_ROLES = ['data', 'data_content', 'data_hot', 'data_warm']\n\ndef footer(version, tail='index.html'):\n    \"\"\"\n    Generate a footer linking to Curator docs based on Curator version\n\n    :param version: The Curator version\n\n    :type version: str\n\n    :returns: An epilog/footer suitable for Click\n    \"\"\"\n    if not isinstance(version, str):\n        raise CuratorException('Parameter version is not a string: {type(version)}')\n    majmin = ''\n    try:\n        ver = version.split('.')\n        majmin = f'{ver[0]}.{ver[1]}'\n    except Exception as exc:\n        msg = f'Could not determine Curator version from provided value: {version}'\n        raise CuratorException(msg) from exc\n    return f'Learn more at {CURATOR_DOCS}/{majmin}/{tail}'\n\ndef default_config_file():\n    \"\"\"\n    :returns: The default configuration file location:\n        path.join(path.expanduser('~'), '.curator', 'curator.yml')\n    \"\"\"\n    default = path.join(path.expanduser('~'), '.curator', 'curator.yml')\n    if path.isfile(default):\n        return default\n\ndef regex_map():\n    \"\"\"\n    :returns: A dictionary of pattern filter 'kind's with their associated regular\n        expression: {'timestring': r'^.*{0}.*$', 'regex': r'{0}',\n        'prefix': r'^{0}.*$', 'suffix': r'^.*{0}$'}\n    \"\"\"\n    return {'timestring': '^.*{0}.*$', 'regex': '{0}', 'prefix': '^{0}.*$', 'suffix': '^.*{0}$'}\n\ndef date_regex():\n    \"\"\"\n    :returns: A dictionary/map of the strftime string characters and their string\n        lengths: {'Y':'4', 'G':'4', 'y':'2', 'm':'2', 'W':'2', 'V':'2', 'U':'2',\n        'd':'2', 'H':'2', 'M':'2', 'S':'2', 'j':'3'}\n    \"\"\"\n    return {'Y': '4', 'G': '4', 'y': '2', 'm': '2', 'W': '2', 'V': '2', 'U': '2', 'd': '2', 'H': '2', 'M': '2', 'S': '2', 'j': '3'}\n\ndef cluster_actions():\n    \"\"\"\n    :returns: A list of supported cluster actions (right now, that's only\n        ['cluster_routing'])\n    \"\"\"\n    return ['cluster_routing']\n\ndef index_actions():\n    \"\"\"\n    :returns: The list of supported index actions:\n        [ 'alias', 'allocation', 'close', 'create_index', 'delete_indices',\n        'forcemerge', 'index_settings', 'open', 'reindex', 'replicas',\n        'rollover', 'shrink', 'snapshot']\n    \"\"\"\n    return ['alias', 'allocation', 'close', 'cold2frozen', 'create_index', 'delete_indices', 'forcemerge', 'index_settings', 'open', 'reindex', 'replicas', 'rollover', 'shrink', 'snapshot']\n\ndef all_actions():\n    \"\"\"\n    :returns: A sorted list of all supported actions: cluster, index, and snapshot\n    \"\"\"\n    return sorted(cluster_actions() + index_actions() + snapshot_actions())\n\ndef all_filtertypes():\n    \"\"\"\n    :returns: A sorted list of all supported filter types (both snapshot and index)\n    \"\"\"\n    return sorted(list(set(index_filtertypes() + snapshot_filtertypes())))\n\ndef default_options():\n    \"\"\"\n    :returns: The default values for these options:\n        {'allow_ilm_indices': False, 'continue_if_exception': False,\n        'disable_action': False, 'ignore_empty_list': False,\n        'timeout_override': None}\n    \"\"\"\n    return {'allow_ilm_indices': False, 'continue_if_exception': False, 'disable_action': False, 'ignore_empty_list': False, 'timeout_override': None}\n\ndef default_filters():\n    \"\"\"\n    If no filters are set, add a 'none' filter\n\n    :returns: {'filters': [{'filtertype': 'none'}]}\n    \"\"\"\n    return {'filters': [{'filtertype': 'none'}]}\n\ndef structural_filter_elements():\n    \"\"\"\n    :returns: Barebones schemas for initial validation of filters\n    \"\"\"\n    return {Optional('aliases'): Any(list, str), Optional('allocation_type'): Any(str), Optional('count'): Coerce(int), Optional('date_from'): Any(None, str), Optional('date_from_format'): Any(None, str), Optional('date_to'): Any(None, str), Optional('date_to_format'): Any(None, str), Optional('direction'): Any(str), Optional('disk_space'): float, Optional('epoch'): Any(Coerce(int), None), Optional('exclude'): Any(None, bool, int, str), Optional('field'): Any(None, str), Optional('intersect'): Any(None, bool, int, str), Optional('key'): Any(str), Optional('kind'): Any(str), Optional('max_num_segments'): Coerce(int), Optional('number_of_shards'): Coerce(int), Optional('pattern'): Any(str), Optional('period_type'): Any(str), Optional('reverse'): Any(None, bool, int, str), Optional('range_from'): Coerce(int), Optional('range_to'): Coerce(int), Optional('shard_filter_behavior'): Any(str), Optional('size_behavior'): Any(str), Optional('size_threshold'): Any(Coerce(float)), Optional('source'): Any(str), Optional('state'): Any(str), Optional('stats_result'): Any(None, str), Optional('timestring'): Any(None, str), Optional('threshold_behavior'): Any(str), Optional('unit'): Any(str), Optional('unit_count'): Coerce(int), Optional('unit_count_pattern'): Any(str), Optional('use_age'): Boolean(), Optional('value'): Any(int, float, bool, str), Optional('week_starts_on'): Any(None, str)}",
    "curator/helpers/utils.py": "\"\"\"Helper utilities\n\nThe kind that don't fit in testers, getters, date_ops, or converters\n\"\"\"\nimport logging\nfrom es_client.helpers.utils import ensure_list\nfrom curator.exceptions import FailedExecution\n\ndef chunk_index_list(indices):\n    \"\"\"\n    This utility chunks very large index lists into 3KB chunks.\n    It measures the size as a csv string, then converts back into a list for the return value.\n\n    :param indices: The list of indices\n\n    :type indices: list\n\n    :returns: A list of lists (each a piece of the original ``indices``)\n    :rtype: list\n    \"\"\"\n    chunks = []\n    chunk = ''\n    for index in indices:\n        if len(chunk) < 3072:\n            if not chunk:\n                chunk = index\n            else:\n                chunk += ',' + index\n        else:\n            chunks.append(chunk.split(','))\n            chunk = index\n    chunks.append(chunk.split(','))\n    return chunks\n\ndef show_dry_run(ilo, action, **kwargs):\n    \"\"\"\n    Log dry run output with the action which would have been executed.\n\n    :param ilo: An IndexList Object\n    :param action: The ``action`` to be performed.\n    :param kwargs: Any other args to show in the log output\n\n\n    :type ilo: :py:class:`~.curator.indexlist.IndexList`\n    :type action: str\n    :type kwargs: dict\n\n    :rtype: None\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info('DRY-RUN MODE.  No changes will be made.')\n    msg = f'(CLOSED) indices may be shown that may not be acted on by action \"{action}\".'\n    logger.info(msg)\n    indices = sorted(ilo.indices)\n    for idx in indices:\n        try:\n            index_closed = ilo.index_info[idx]['state'] == 'close'\n        except KeyError:\n            ilo.get_index_state()\n            index_closed = ilo.index_info[idx]['state'] == 'close'\n        var = ' (CLOSED)' if index_closed else ''\n        msg = f'DRY-RUN: {action}: {idx}{var} with arguments: {kwargs}'\n        logger.info(msg)\n\ndef to_csv(indices):\n    \"\"\"\n    :param indices: A list of indices to act on, or a single value, which could be\n        in the format of a csv string already.\n\n    :type indices: list\n\n    :returns: A csv string from a list of indices, or a single value if only one value is present\n    :rtype: str\n    \"\"\"\n    indices = ensure_list(indices)\n    if indices:\n        return ','.join(sorted(indices))\n    return None",
    "curator/helpers/getters.py": "\"\"\"Utility functions that get things\"\"\"\nimport logging\nfrom elasticsearch8 import exceptions as es8exc\nfrom curator.exceptions import ConfigurationError, CuratorException, FailedExecution, MissingArgument\n\ndef byte_size(num, suffix='B'):\n    \"\"\"\n    :param num: The number of byte\n    :param suffix: An arbitrary suffix, like ``Bytes``\n\n    :type num: int\n    :type suffix: str\n\n    :returns: A formatted string indicating the size in bytes, with the proper unit,\n        e.g. KB, MB, GB, TB, etc.\n    :rtype: float\n    \"\"\"\n    for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:\n        if abs(num) < 1024.0:\n            return f'{num:3.1f}{unit}{suffix}'\n        num /= 1024.0\n    return f'{num:.1f}Y{suffix}'\n\ndef escape_dots(stringval):\n    \"\"\"\n    Escape any dots (periods) in ``stringval``.\n\n    Primarily used for ``filter_path`` where dots are indicators of path nesting\n\n    :param stringval: A string, ostensibly an index name\n\n    :type stringval: str\n\n    :returns: ``stringval``, but with any periods escaped with a backslash\n    :retval: str\n    \"\"\"\n    return stringval.replace('.', '\\\\.')\n\ndef get_alias_actions(oldidx, newidx, aliases):\n    \"\"\"\n    :param oldidx: The old index name\n    :param newidx: The new index name\n    :param aliases: The aliases\n\n    :type oldidx: str\n    :type newidx: str\n    :type aliases: dict\n\n    :returns: A list of actions suitable for\n        :py:meth:`~.elasticsearch.client.IndicesClient.update_aliases` ``actions``\n        kwarg.\n    :rtype: list\n    \"\"\"\n    actions = []\n    for alias in aliases.keys():\n        actions.append({'remove': {'index': oldidx, 'alias': alias}})\n        actions.append({'add': {'index': newidx, 'alias': alias}})\n    return actions\n\ndef get_data_tiers(client):\n    \"\"\"\n    Get all valid data tiers from the node roles of each node in the cluster by\n    polling each node\n\n    :param client: A client connection object\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n\n    :returns: The available data tiers in ``tier: bool`` form.\n    :rtype: dict\n    \"\"\"\n\n    def role_check(role, node_info):\n        if role in node_info['roles']:\n            return True\n        return False\n    info = client.nodes.info()['nodes']\n    retval = {'data_hot': False, 'data_warm': False, 'data_cold': False, 'data_frozen': False}\n    for node in info:\n        for role in ['data_hot', 'data_warm', 'data_cold', 'data_frozen']:\n            if role_check(role, info[node]):\n                retval[role] = True\n    return retval\n\ndef get_indices(client, search_pattern='_all'):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.CatClient.indices`\n\n    :param client: A client connection object\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n\n    :returns: The current list of indices from the cluster\n    :rtype: list\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    indices = []\n    try:\n        resp = client.cat.indices(index=search_pattern, expand_wildcards='open,closed', h='index,status', format='json')\n    except Exception as err:\n        raise FailedExecution(f'Failed to get indices. Error: {err}') from err\n    if not resp:\n        return indices\n    for entry in resp:\n        indices.append(entry['index'])\n    logger.debug('All indices: %s', indices)\n    return indices\n\ndef get_snapshot(client, repository=None, snapshot=''):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get`\n\n    :param client: A client connection object\n    :param repository: The Elasticsearch snapshot repository to use\n    :param snapshot: The snapshot name, or a comma-separated list of snapshots\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type repository: str\n    :type snapshot: str\n\n    :returns: Information about the provided ``snapshot``, a snapshot (or a\n        comma-separated list of snapshots). If no snapshot specified, it will\n        collect info for all snapshots.  If none exist, an empty :py:class:`dict`\n        will be returned.\n    :rtype: dict\n    \"\"\"\n    if not repository:\n        raise MissingArgument('No value for \"repository\" provided')\n    snapname = '*' if snapshot == '' else snapshot\n    try:\n        return client.snapshot.get(repository=repository, snapshot=snapshot)\n    except (es8exc.TransportError, es8exc.NotFoundError) as err:\n        msg = f'Unable to get information about snapshot {snapname} from repository: {repository}.  Error: {err}'\n        raise FailedExecution(msg) from err\n\ndef get_snapshot_data(client, repository=None):\n    \"\"\"\n    Get all snapshots from repository and return a list.\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get`\n\n    :param client: A client connection object\n    :param repository: The Elasticsearch snapshot repository to use\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type repository: str\n\n    :returns: The list of all snapshots from ``repository``\n    :rtype: list\n    \"\"\"\n    if not repository:\n        raise MissingArgument('No value for \"repository\" provided')\n    try:\n        return client.snapshot.get(repository=repository, snapshot='*')['snapshots']\n    except (es8exc.TransportError, es8exc.NotFoundError) as err:\n        msg = f'Unable to get snapshot information from repository: {repository}. Error: {err}'\n        raise FailedExecution(msg) from err\n\ndef get_tier_preference(client, target_tier='data_frozen'):\n    \"\"\"Do the tier preference thing in reverse order from coldest to hottest\n    Based on the value of ``target_tier``, build out the list to use.\n\n    :param client: A client connection object\n    :param target_tier: The target data tier, e.g. data_warm.\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type target_tier: str\n\n    :returns: A suitable tier preference string in csv format\n    :rtype: str\n    \"\"\"\n    tiermap = {'data_content': 0, 'data_hot': 1, 'data_warm': 2, 'data_cold': 3, 'data_frozen': 4}\n    tiers = get_data_tiers(client)\n    test_list = []\n    for tier in ['data_hot', 'data_warm', 'data_cold', 'data_frozen']:\n        if tier in tiers and tiermap[tier] <= tiermap[target_tier]:\n            test_list.insert(0, tier)\n    if target_tier == 'data_frozen':\n        if 'data_frozen' in tiers and tiers['data_frozen']:\n            return 'data_frozen'\n    preflist = []\n    for key in test_list:\n        if key in tiers and tiers[key]:\n            preflist.append(key)\n    if not preflist:\n        return 'data_content'\n    return ','.join(preflist)\n\ndef get_write_index(client, alias):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.get_alias`\n\n    :param client: A client connection object\n    :param alias: An alias name\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type alias: str\n\n    :returns: The the index name associated with the alias that is designated\n        ``is_write_index``\n    :rtype: str\n    \"\"\"\n    try:\n        response = client.indices.get_alias(index=alias)\n    except Exception as exc:\n        raise CuratorException(f'Alias {alias} not found') from exc\n    retval = None\n    if len(list(response.keys())) > 1:\n        for index in list(response.keys()):\n            try:\n                if response[index]['aliases'][alias]['is_write_index']:\n                    retval = index\n            except KeyError as exc:\n                raise FailedExecution('Invalid alias: is_write_index not found in 1 to many alias') from exc\n    else:\n        retval = list(response.keys())[0]\n    return retval\n\ndef index_size(client, idx, value='total'):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.stats`\n\n    :param client: A client connection object\n    :param idx: An index name\n    :param value: One of either ``primaries`` or ``total``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type idx: str\n    :type value: str\n\n    :returns: The sum of either ``primaries`` or ``total`` shards for index ``idx``\n    :rtype: integer\n    \"\"\"\n    fpath = f'indices.{escape_dots(idx)}.{value}.store.size_in_bytes'\n    return client.indices.stats(index=idx, filter_path=fpath)['indices'][idx][value]['store']['size_in_bytes']\n\ndef meta_getter(client, idx, get=None):\n    \"\"\"Meta Getter\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.get_settings` or\n    :py:meth:`~.elasticsearch.client.IndicesClient.get_alias`\n\n    :param client: A client connection object\n    :param idx: An Elasticsearch index\n    :param get: The kind of get to perform, e.g. settings or alias\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type idx: str\n    :type get: str\n\n    :returns: The settings from the get call to the named index\n    :rtype: dict\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    acceptable = ['settings', 'alias']\n    if not get:\n        raise ConfigurationError('\"get\" can not be a NoneType')\n    if get not in acceptable:\n        raise ConfigurationError(f'\"get\" must be one of {acceptable}')\n    retval = {}\n    try:\n        if get == 'settings':\n            retval = client.indices.get_settings(index=idx)[idx]['settings']['index']\n        elif get == 'alias':\n            retval = client.indices.get_alias(index=idx)[idx]['aliases']\n    except es8exc.NotFoundError as missing:\n        logger.error('Index %s was not found!', idx)\n        raise es8exc.NotFoundError from missing\n    except KeyError as err:\n        logger.error('Key not found: %s', err)\n        raise KeyError from err\n    except Exception as exc:\n        logger.error('Exception encountered: %s', exc)\n    return retval\n\ndef name_to_node_id(client, name):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.NodesClient.info`\n\n    :param client: A client connection object\n    :param name: The node ``name``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type name: str\n\n    :returns: The node_id of the node identified by ``name``\n    :rtype: str\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    fpath = 'nodes'\n    info = client.nodes.info(filter_path=fpath)\n    for node in info['nodes']:\n        if info['nodes'][node]['name'] == name:\n            logger.debug('Found node_id \"%s\" for name \"%s\".', node, name)\n            return node\n    logger.error('No node_id found matching name: \"%s\"', name)\n    return None\n\ndef node_id_to_name(client, node_id):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.NodesClient.info`\n\n    :param client: A client connection object\n    :param node_id: The node ``node_id``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type node_id: str\n\n    :returns: The name of the node identified by ``node_id``\n    :rtype: str\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    fpath = f'nodes.{node_id}.name'\n    info = client.nodes.info(filter_path=fpath)\n    name = None\n    if node_id in info['nodes']:\n        name = info['nodes'][node_id]['name']\n    else:\n        logger.error('No node_id found matching: \"%s\"', node_id)\n    logger.debug('Name associated with node_id \"%s\": %s', node_id, name)\n    return name\n\ndef node_roles(client, node_id):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.NodesClient.info`\n\n    :param client: A client connection object\n    :param node_id: The node ``node_id``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type node_id: str\n\n    :returns: The list of roles assigned to the node identified by ``node_id``\n    :rtype: list\n    \"\"\"\n    fpath = f'nodes.{node_id}.roles'\n    return client.nodes.info(filter_path=fpath)['nodes'][node_id]['roles']\n\ndef single_data_path(client, node_id):\n    \"\"\"\n    In order for a shrink to work, it should be on a single filesystem, as shards\n    cannot span filesystems. Calls :py:meth:`~.elasticsearch.client.NodesClient.stats`\n\n    :param client: A client connection object\n    :param node_id: The node ``node_id``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type node_id: str\n\n    :returns: ``True`` if the node has a single filesystem, else ``False``\n    :rtype: bool\n    \"\"\"\n    fpath = f'nodes.{node_id}.fs.data'\n    response = client.nodes.stats(filter_path=fpath)\n    return len(response['nodes'][node_id]['fs']['data']) == 1",
    "curator/exceptions.py": "\"\"\"Curator Exceptions\"\"\"\n\nclass CuratorException(Exception):\n    \"\"\"\n    Base class for all exceptions raised by Curator which are not Elasticsearch\n    exceptions.\n    \"\"\"\n\nclass ConfigurationError(CuratorException):\n    \"\"\"\n    Exception raised when a misconfiguration is detected\n    \"\"\"\n\nclass MissingArgument(CuratorException):\n    \"\"\"\n    Exception raised when a needed argument is not passed.\n    \"\"\"\n\nclass NoIndices(CuratorException):\n    \"\"\"\n    Exception raised when an operation is attempted against an empty index_list\n    \"\"\"\n\nclass NoSnapshots(CuratorException):\n    \"\"\"\n    Exception raised when an operation is attempted against an empty snapshot_list\n    \"\"\"\n\nclass ActionError(CuratorException):\n    \"\"\"\n    Exception raised when an action (against an index_list or snapshot_list) cannot be taken.\n    \"\"\"\n\nclass FailedExecution(CuratorException):\n    \"\"\"\n    Exception raised when an action fails to execute for some reason.\n    \"\"\"\n\nclass SnapshotInProgress(ActionError):\n    \"\"\"\n    Exception raised when a snapshot is already in progress\n    \"\"\"\n\nclass ActionTimeout(CuratorException):\n    \"\"\"\n    Exception raised when an action fails to complete in the allotted time\n    \"\"\"\n\nclass FailedSnapshot(CuratorException):\n    \"\"\"\n    Exception raised when a snapshot does not complete with state SUCCESS\n    \"\"\"\n\nclass FailedRestore(CuratorException):\n    \"\"\"\n    Exception raised when a Snapshot Restore does not restore all selected indices\n    \"\"\"\n\nclass FailedReindex(CuratorException):\n    \"\"\"\n    Exception raised when failures are found in the reindex task response\n    \"\"\"\n\nclass ClientException(CuratorException):\n    \"\"\"\n    Exception raised when the Elasticsearch client and/or connection is the source of the problem.\n    \"\"\"\n\nclass LoggingException(CuratorException):\n    \"\"\"\n    Exception raised when Curator cannot either log or configure logging\n    \"\"\"\n\nclass RepositoryException(CuratorException):\n    \"\"\"\n    Exception raised when Curator cannot verify a snapshot repository\n    \"\"\"\n\nclass SearchableSnapshotException(CuratorException):\n    \"\"\"\n    Exception raised when Curator finds something out of order with a Searchable Snapshot\n    \"\"\""
  }
}