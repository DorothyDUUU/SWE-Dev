{
  "dir_path": "/app/datashader",
  "package_name": "datashader",
  "sample_name": "datashader-test_util",
  "src_dir": "datashader/",
  "test_dir": "datashader/tests/",
  "test_file": "datashader/datashape/tests/test_util.py",
  "test_code": "import pytest\n\nfrom datashader import datashape\nfrom datashader.datashape import dshape, has_var_dim, has_ellipsis\n\n\ndef test_cat_dshapes():\n    # concatenating 1 dshape is a no-op\n    dslist = [dshape('3 * 10 * int32')]\n    assert datashape.cat_dshapes(dslist) == dslist[0]\n    # two dshapes\n    dslist = [dshape('3 * 10 * int32'),\n              dshape('7 * 10 * int32')]\n    assert datashape.cat_dshapes(dslist) == dshape('10 * 10 * int32')\n\n\ndef test_cat_dshapes_errors():\n    # need at least one dshape\n    with pytest.raises(ValueError):\n        datashape.cat_dshapes([])\n\n    # dshapes need to match after the first dimension\n    with pytest.raises(ValueError):\n        datashape.cat_dshapes([dshape('3 * 10 * int32'),\n                              dshape('3 * 1 * int32')])\n\n\n@pytest.mark.parametrize('ds_pos',\n                         [\"... * float32\",\n                          \"A... * float32\",\n                          \"var * float32\",\n                          \"10 * { f0: int32, f1: A... * float32 }\",\n                          \"{ f0 : { g0 : var * int }, f1: int32 }\",\n                          (dshape(\"var * int32\"),)])\ndef test_has_var_dim(ds_pos):\n    assert has_var_dim(dshape(ds_pos))\n\n\n@pytest.mark.parametrize('ds_neg',\n                         [dshape(\"float32\"),\n                          dshape(\"10 * float32\"),\n                          dshape(\"10 * { f0: int32, f1: 10 * float32 }\"),\n                          dshape(\"{ f0 : { g0 : 2 * int }, f1: int32 }\"),\n                          (dshape(\"int32\"),)])\ndef test_not_has_var_dim(ds_neg):\n    assert not has_var_dim(ds_neg)\n\n\n@pytest.mark.parametrize('ds',\n                         [dshape(\"... * float32\"),\n                          dshape(\"A... * float32\"),\n                          dshape(\"var * ... * float32\"),\n                          dshape(\"(int32, M... * int16) -> var * int8\"),\n                          dshape(\"(int32, var * int16) -> ... * int8\"),\n                          dshape(\"10 * { f0: int32, f1: A... * float32 }\"),\n                          dshape(\"{ f0 : { g0 : ... * int }, f1: int32 }\"),\n                          (dshape(\"... * int32\"),)])\ndef test_has_ellipsis(ds):\n    assert has_ellipsis(ds)\n\n\n@pytest.mark.parametrize('ds',\n                         [dshape(\"float32\"),\n                          dshape(\"10 * var * float32\"),\n                          dshape(\"M * float32\"),\n                          dshape(\"(int32, M * int16) -> var * int8\"),\n                          dshape(\"(int32, int16) -> var * int8\"),\n                          dshape(\"10 * { f0: int32, f1: 10 * float32 }\"),\n                          dshape(\"{ f0 : { g0 : 2 * int }, f1: int32 }\"),\n                          (dshape(\"M * int32\"),)])\ndef test_not_has_ellipsis(ds):\n    assert not has_ellipsis(ds)\n",
  "GT_file_code": {
    "datashader/datashape/coretypes.py": "\"\"\"\nThis defines the DataShape type system, with unified\nshape and data type.\n\"\"\"\n\nimport ctypes\nimport operator\n\nfrom collections import OrderedDict\nfrom math import ceil\n\nfrom datashader import datashape\n\nimport numpy as np\n\nfrom .internal_utils import IndexCallable, isidentifier\n\n\n# Classes of unit types.\nDIMENSION = 1\nMEASURE = 2\n\n\nclass Type(type):\n    _registry = {}\n\n    def __new__(meta, name, bases, dct):\n        cls = super(Type, meta).__new__(meta, name, bases, dct)  # noqa: UP008\n        # Don't register abstract classes\n        if not dct.get('abstract'):\n            Type._registry[name] = cls\n        return cls\n\n    @classmethod\n    def register(cls, name, type):\n        # Don't clobber existing types.\n        if name in cls._registry:\n            raise TypeError('There is another type registered with name %s'\n                            % name)\n\n        cls._registry[name] = type\n\n    @classmethod\n    def lookup_type(cls, name):\n        return cls._registry[name]\n\n\nclass Mono(metaclass=Type):\n\n    \"\"\"\n    Monotype are unqualified 0 parameters.\n\n    Each type must be reconstructable using its parameters:\n\n        type(datashape_type)(*type.parameters)\n    \"\"\"\n\n    composite = False\n\n    def __init__(self, *params):\n        self._parameters = params\n\n    @property\n    def _slotted(self):\n        return hasattr(self, '__slots__')\n\n    @property\n    def parameters(self):\n        if self._slotted:\n            return tuple(getattr(self, slot) for slot in self.__slots__)\n        else:\n            return self._parameters\n\n    def info(self):\n        return type(self), self.parameters\n\n    def __eq__(self, other):\n        return (isinstance(other, Mono) and\n                self.shape == other.shape and\n                self.measure.info() == other.measure.info())\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __hash__(self):\n        try:\n            h = self._hash\n        except AttributeError:\n            h = self._hash = hash(self.shape) ^ hash(self.measure.info())\n        return h\n\n    @property\n    def shape(self):\n        return ()\n\n    def __len__(self):\n        return 1\n\n    def __getitem__(self, key):\n        return [self][key]\n\n    def __repr__(self):\n        return '%s(%s)' % (\n            type(self).__name__,\n            ', '.join(\n                (\n                    '%s=%r' % (slot, getattr(self, slot))\n                    for slot in self.__slots__\n                ) if self._slotted else\n                map(repr, self.parameters),\n            ),\n        )\n\n    # Monotypes are their own measure\n    @property\n    def measure(self):\n        return self\n\n    def subarray(self, leading):\n        \"\"\"Returns a data shape object of the subarray with 'leading'\n        dimensions removed. In the case of a measure such as CType,\n        'leading' must be 0, and self is returned.\n        \"\"\"\n        if leading >= 1:\n            raise IndexError(('Not enough dimensions in data shape '\n                              'to remove %d leading dimensions.') % leading)\n        else:\n            return self\n\n    def __mul__(self, other):\n        if isinstance(other, str):\n            from datashader import datashape\n            return datashape.dshape(other).__rmul__(self)\n        if isinstance(other, int):\n            other = Fixed(other)\n        if isinstance(other, DataShape):\n            return other.__rmul__(self)\n\n        return DataShape(self, other)\n\n    def __rmul__(self, other):\n        if isinstance(other, str):\n            from datashader import datashape\n            return self * datashape.dshape(other)\n        if isinstance(other, int):\n            other = Fixed(other)\n\n        return DataShape(other, self)\n\n    def __getstate__(self):\n        return self.parameters\n\n    def __setstate__(self, state):\n        if self._slotted:\n            for slot, val in zip(self.__slots__, state):\n                setattr(self, slot, val)\n        else:\n            self._parameters = state\n\n    def to_numpy_dtype(self):\n        raise TypeError('DataShape %s is not NumPy-compatible' % self)\n\n\nclass Unit(Mono):\n\n    \"\"\"\n    Unit type that does not need to be reconstructed.\n    \"\"\"\n\n    def __str__(self):\n        return type(self).__name__.lower()\n\n\nclass Ellipsis(Mono):\n\n    \"\"\"Ellipsis (...). Used to indicate a variable number of dimensions.\n\n    E.g.:\n\n        ... * float32    # float32 array w/ any number of dimensions\n        A... * float32   # float32 array w/ any number of dimensions,\n                        # associated with type variable A\n    \"\"\"\n    __slots__ = 'typevar',\n\n    def __init__(self, typevar=None):\n        self.typevar = typevar\n\n    def __str__(self):\n        return str(self.typevar) + '...' if self.typevar else '...'\n\n    def __repr__(self):\n        return '%s(%r)' % (type(self).__name__, str(self))\n\n\nclass Null(Unit):\n\n    \"\"\"The null datashape.\"\"\"\n    pass\n\n\nclass Date(Unit):\n\n    \"\"\" Date type \"\"\"\n    cls = MEASURE\n    __slots__ = ()\n\n    def to_numpy_dtype(self):\n        return np.dtype('datetime64[D]')\n\n\nclass Time(Unit):\n\n    \"\"\" Time type \"\"\"\n    cls = MEASURE\n    __slots__ = 'tz',\n\n    def __init__(self, tz=None):\n        if tz is not None and not isinstance(tz, str):\n            raise TypeError('tz parameter to time datashape must be a string')\n        # TODO validate against Olson tz database\n        self.tz = tz\n\n    def __str__(self):\n        basename = super().__str__()\n        if self.tz is None:\n            return basename\n        else:\n            return '%s[tz=%r]' % (basename, str(self.tz))\n\n\nclass DateTime(Unit):\n\n    \"\"\" DateTime type \"\"\"\n    cls = MEASURE\n    __slots__ = 'tz',\n\n    def __init__(self, tz=None):\n        if tz is not None and not isinstance(tz, str):\n            raise TypeError('tz parameter to datetime datashape must be a '\n                            'string')\n        # TODO validate against Olson tz database\n        self.tz = tz\n\n    def __str__(self):\n        basename = super().__str__()\n        if self.tz is None:\n            return basename\n        else:\n            return '%s[tz=%r]' % (basename, str(self.tz))\n\n    def to_numpy_dtype(self):\n        return np.dtype('datetime64[us]')\n\n\n_units = ('ns', 'us', 'ms', 's', 'm', 'h', 'D', 'W', 'M', 'Y')\n\n\n_unit_aliases = {\n    'year': 'Y',\n    'week': 'W',\n    'day': 'D',\n    'date': 'D',\n    'hour': 'h',\n    'second': 's',\n    'millisecond': 'ms',\n    'microsecond': 'us',\n    'nanosecond': 'ns'\n}\n\n\ndef normalize_time_unit(s):\n    \"\"\" Normalize time input to one of 'year', 'second', 'millisecond', etc..\n    Example\n    -------\n    >>> normalize_time_unit('milliseconds')\n    'ms'\n    >>> normalize_time_unit('ms')\n    'ms'\n    >>> normalize_time_unit('nanoseconds')\n    'ns'\n    >>> normalize_time_unit('nanosecond')\n    'ns'\n    \"\"\"\n    s = s.strip()\n    if s in _units:\n        return s\n    if s in _unit_aliases:\n        return _unit_aliases[s]\n    if s[-1] == 's' and len(s) > 2:\n        return normalize_time_unit(s.rstrip('s'))\n\n    raise ValueError(\"Do not understand time unit %s\" % s)\n\n\nclass TimeDelta(Unit):\n    cls = MEASURE\n    __slots__ = 'unit',\n\n    def __init__(self, unit='us'):\n        self.unit = normalize_time_unit(str(unit))\n\n    def __str__(self):\n        return 'timedelta[unit=%r]' % self.unit\n\n    def to_numpy_dtype(self):\n        return np.dtype('timedelta64[%s]' % self.unit)\n\n\nclass Units(Unit):\n    \"\"\" Units type for values with physical units \"\"\"\n    cls = MEASURE\n    __slots__ = 'unit', 'tp'\n\n    def __init__(self, unit, tp=None):\n        if not isinstance(unit, str):\n            raise TypeError('unit parameter to units datashape must be a '\n                            'string')\n        if tp is None:\n            tp = DataShape(float64)\n        elif not isinstance(tp, DataShape):\n            raise TypeError('tp parameter to units datashape must be a '\n                            'datashape type')\n        self.unit = unit\n        self.tp = tp\n\n    def __str__(self):\n        if self.tp == DataShape(float64):\n            return 'units[%r]' % (self.unit)\n        else:\n            return 'units[%r, %s]' % (self.unit, self.tp)\n\n\nclass Bytes(Unit):\n\n    \"\"\" Bytes type \"\"\"\n    cls = MEASURE\n    __slots__ = ()\n\n\n_canonical_string_encodings = {\n    'A': 'A',\n    'ascii': 'A',\n    'U8': 'U8',\n    'utf-8': 'U8',\n    'utf_8': 'U8',\n    'utf8': 'U8',\n    'U16': 'U16',\n    'utf-16': 'U16',\n    'utf_16': 'U16',\n    'utf16': 'U16',\n    'U32': 'U32',\n    'utf-32': 'U32',\n    'utf_32': 'U32',\n    'utf32': 'U32',\n}\n\n\nclass String(Unit):\n\n    \"\"\" String container\n\n    >>> String()\n    ctype(\"string\")\n    >>> String(10, 'ascii')\n    ctype(\"string[10, 'A']\")\n    \"\"\"\n    cls = MEASURE\n    __slots__ = 'fixlen', 'encoding'\n\n    def __init__(self, *args):\n        if len(args) == 0:\n            fixlen, encoding = None, None\n        if len(args) == 1:\n            if isinstance(args[0], str):\n                fixlen, encoding = None, args[0]\n            if isinstance(args[0], int):\n                fixlen, encoding = args[0], None\n        elif len(args) == 2:\n            fixlen, encoding = args\n\n        encoding = encoding or 'U8'\n        if isinstance(encoding, str):\n            encoding = str(encoding)\n        try:\n            encoding = _canonical_string_encodings[encoding]\n        except KeyError:\n            raise ValueError('Unsupported string encoding %s' %\n                             repr(encoding))\n\n        self.encoding = encoding\n        self.fixlen = fixlen\n\n        # Put it in a canonical form\n\n    def __str__(self):\n        if self.fixlen is None and self.encoding == 'U8':\n            return 'string'\n        elif self.fixlen is not None and self.encoding == 'U8':\n            return 'string[%i]' % self.fixlen\n        elif self.fixlen is None and self.encoding != 'U8':\n            return 'string[%s]' % repr(self.encoding).strip('u')\n        else:\n            return 'string[%i, %s]' % (self.fixlen,\n                                       repr(self.encoding).strip('u'))\n\n    def __repr__(self):\n        s = str(self)\n        return 'ctype(\"%s\")' % s.encode('unicode_escape').decode('ascii')\n\n    def to_numpy_dtype(self):\n        \"\"\"\n        >>> String().to_numpy_dtype()\n        dtype('O')\n        >>> String(30).to_numpy_dtype()\n        dtype('<U30')\n        >>> String(30, 'A').to_numpy_dtype()\n        dtype('S30')\n        \"\"\"\n        if self.fixlen:\n            if self.encoding == 'A':\n                return np.dtype('S%d' % self.fixlen)\n            else:\n                return np.dtype('U%d' % self.fixlen)\n\n        # Create a dtype with metadata indicating it's\n        # a string in the same style as the h5py special_dtype\n        return np.dtype('O', metadata={'vlen': str})\n\n\nclass Decimal(Unit):\n\n    \"\"\"Decimal type corresponding to SQL Decimal/Numeric types.\n\n    The first parameter passed specifies the number of digits of precision that\n    the Decimal contains. If an additional parameter is given, it represents\n    the scale, or number of digits of precision that are after the decimal\n    point.\n\n    The Decimal type makes no requirement of how it is to be stored in memory,\n    therefore, the number of bytes needed to store a Decimal for a given\n    precision will vary based on the platform where it is used.\n\n    Examples\n    --------\n    >>> Decimal(18)\n    Decimal(precision=18, scale=0)\n    >>> Decimal(7, 4)\n    Decimal(precision=7, scale=4)\n    >>> Decimal(precision=11, scale=2)\n    Decimal(precision=11, scale=2)\n    \"\"\"\n\n    cls = MEASURE\n    __slots__ = 'precision', 'scale'\n\n    def __init__(self, precision, scale=0):\n        self.precision = precision\n        self.scale = scale\n\n    def __str__(self):\n        return 'decimal[precision={precision}, scale={scale}]'.format(\n            precision=self.precision, scale=self.scale\n        )\n\n    def to_numpy_dtype(self):\n        \"\"\"Convert a decimal datashape to a NumPy dtype.\n\n        Note that floating-point (scale > 0) precision will be lost converting\n        to NumPy floats.\n\n        Examples\n        --------\n        >>> Decimal(18).to_numpy_dtype()\n        dtype('int64')\n        >>> Decimal(7,4).to_numpy_dtype()\n        dtype('float64')\n        \"\"\"\n\n        if self.scale == 0:\n            if self.precision <= 2:\n                return np.dtype(np.int8)\n            elif self.precision <= 4:\n                return np.dtype(np.int16)\n            elif self.precision <= 9:\n                return np.dtype(np.int32)\n            elif self.precision <= 18:\n                return np.dtype(np.int64)\n            else:\n                raise TypeError(\n                    'Integer Decimal precision > 18 is not NumPy-compatible')\n        else:\n            return np.dtype(np.float64)\n\n\nclass DataShape(Mono):\n\n    \"\"\"\n    Composite container for datashape elements.\n\n    Elements of a datashape like ``Fixed(3)``, ``Var()`` or ``int32`` are on,\n    on their own, valid datashapes.  These elements are collected together into\n    a composite ``DataShape`` to be complete.\n\n    This class is not intended to be used directly.  Instead, use the utility\n    ``dshape`` function to create datashapes from strings or datashape\n    elements.\n\n    Examples\n    --------\n\n    >>> from datashader.datashape import Fixed, int32, DataShape, dshape\n\n    >>> DataShape(Fixed(5), int32)  # Rare to DataShape directly\n    dshape(\"5 * int32\")\n\n    >>> dshape('5 * int32')         # Instead use the dshape function\n    dshape(\"5 * int32\")\n\n    >>> dshape([Fixed(5), int32])   # It can even do construction from elements\n    dshape(\"5 * int32\")\n\n    See Also\n    --------\n    datashape.dshape\n    \"\"\"\n    composite = False\n\n    def __init__(self, *parameters, **kwds):\n        if len(parameters) == 1 and isinstance(parameters[0], str):\n            raise TypeError(\"DataShape constructor for internal use.\\n\"\n                            \"Use dshape function to convert strings into \"\n                            \"datashapes.\\nTry:\\n\\tdshape('%s')\"\n                            % parameters[0])\n        if len(parameters) > 0:\n            self._parameters = tuple(map(_launder, parameters))\n            if getattr(self._parameters[-1], 'cls', MEASURE) != MEASURE:\n                raise TypeError(('Only a measure can appear on the'\n                                 ' last position of a datashape, not %s') %\n                                repr(self._parameters[-1]))\n            for dim in self._parameters[:-1]:\n                if getattr(dim, 'cls', DIMENSION) != DIMENSION:\n                    raise TypeError(('Only dimensions can appear before the'\n                                     ' last position of a datashape, not %s') %\n                                    repr(dim))\n        else:\n            raise ValueError('the data shape should be constructed from 2 or'\n                             ' more parameters, only got %s' % len(parameters))\n        self.composite = True\n        self.name = kwds.get('name')\n\n        if self.name:\n            type(type(self))._registry[self.name] = self\n\n    def __len__(self):\n        return len(self.parameters)\n\n    def __getitem__(self, index):\n        return self.parameters[index]\n\n    def __str__(self):\n        return self.name or ' * '.join(map(str, self.parameters))\n\n    def __repr__(self):\n        s = pprint(self)\n        if '\\n' in s:\n            return 'dshape(\"\"\"%s\"\"\")' % s\n        else:\n            return 'dshape(\"%s\")' % s\n\n    @property\n    def shape(self):\n        return self.parameters[:-1]\n\n    @property\n    def measure(self):\n        return self.parameters[-1]\n\n    def subarray(self, leading):\n        \"\"\"Returns a data shape object of the subarray with 'leading'\n        dimensions removed.\n\n        >>> from datashader.datashape import dshape\n        >>> dshape('1 * 2 * 3 * int32').subarray(1)\n        dshape(\"2 * 3 * int32\")\n        >>> dshape('1 * 2 * 3 * int32').subarray(2)\n        dshape(\"3 * int32\")\n        \"\"\"\n        if leading >= len(self.parameters):\n            raise IndexError('Not enough dimensions in data shape '\n                             'to remove %d leading dimensions.' % leading)\n        elif leading in [len(self.parameters) - 1, -1]:\n            return DataShape(self.parameters[-1])\n        else:\n            return DataShape(*self.parameters[leading:])\n\n    def __rmul__(self, other):\n        if isinstance(other, int):\n            other = Fixed(other)\n        return DataShape(other, *self)\n\n    @property\n    def subshape(self):\n        return IndexCallable(self._subshape)\n\n    def _subshape(self, index):\n        \"\"\" The DataShape of an indexed subarray\n\n        >>> from datashader.datashape import dshape\n\n        >>> ds = dshape('var * {name: string, amount: int32}')\n        >>> print(ds.subshape[0])\n        {name: string, amount: int32}\n\n        >>> print(ds.subshape[0:3])\n        3 * {name: string, amount: int32}\n\n        >>> print(ds.subshape[0:7:2, 'amount'])\n        4 * int32\n\n        >>> print(ds.subshape[[1, 10, 15]])\n        3 * {name: string, amount: int32}\n\n        >>> ds = dshape('{x: int, y: int}')\n        >>> print(ds.subshape['x'])\n        int32\n\n        >>> ds = dshape('10 * var * 10 * int32')\n        >>> print(ds.subshape[0:5, 0:3, 5])\n        5 * 3 * int32\n\n        >>> ds = dshape('var * {name: string, amount: int32, id: int32}')\n        >>> print(ds.subshape[:, [0, 2]])\n        var * {name: string, id: int32}\n\n        >>> ds = dshape('var * {name: string, amount: int32, id: int32}')\n        >>> print(ds.subshape[:, ['name', 'id']])\n        var * {name: string, id: int32}\n\n        >>> print(ds.subshape[0, 1:])\n        {amount: int32, id: int32}\n        \"\"\"\n        from .predicates import isdimension\n        if isinstance(index, int) and isdimension(self[0]):\n            return self.subarray(1)\n        if isinstance(self[0], Record) and isinstance(index, str):\n            return self[0][index]\n        if isinstance(self[0], Record) and isinstance(index, int):\n            return self[0].parameters[0][index][1]\n        if isinstance(self[0], Record) and isinstance(index, list):\n            rec = self[0]\n            # Translate strings to corresponding integers\n            index = [self[0].names.index(i) if isinstance(i, str) else i\n                     for i in index]\n            return DataShape(Record([rec.parameters[0][i] for i in index]))\n        if isinstance(self[0], Record) and isinstance(index, slice):\n            rec = self[0]\n            return DataShape(Record(rec.parameters[0][index]))\n        if isinstance(index, list) and isdimension(self[0]):\n            return len(index) * self.subarray(1)\n        if isinstance(index, slice):\n            if isinstance(self[0], Fixed):\n                n = int(self[0])\n                start = index.start or 0\n                stop = index.stop or n\n                if start < 0:\n                    start = n + start\n                if stop < 0:\n                    stop = n + stop\n                count = stop - start\n            else:\n                start = index.start or 0\n                stop = index.stop\n                if not stop:\n                    count = -start if start < 0 else var\n                if (stop is not None and start is not None and stop >= 0 and\n                        start >= 0):\n                    count = stop - start\n                else:\n                    count = var\n\n            if count != var and index.step is not None:\n                count = int(ceil(count / index.step))\n\n            return count * self.subarray(1)\n        if isinstance(index, tuple):\n            if not index:\n                return self\n            elif index[0] is None:\n                return 1 * self._subshape(index[1:])\n            elif len(index) == 1:\n                return self._subshape(index[0])\n            else:\n                ds = self.subarray(1)._subshape(index[1:])\n                return (self[0] * ds)._subshape(index[0])\n        raise TypeError('invalid index value %s of type %r' %\n                        (index, type(index).__name__))\n\n    def __setstate__(self, state):\n        self._parameters = state\n        self.composite = True\n        self.name = None\n\n\nnumpy_provides_missing = frozenset((Date, DateTime, TimeDelta))\n\n\nclass Option(Mono):\n\n    \"\"\"\n    Measure types which may or may not hold data. Makes no\n    indication of how this is implemented in memory.\n    \"\"\"\n    __slots__ = 'ty',\n\n    def __init__(self, ds):\n        self.ty = _launder(ds)\n\n    @property\n    def shape(self):\n        return self.ty.shape\n\n    @property\n    def itemsize(self):\n        return self.ty.itemsize\n\n    def __str__(self):\n        return '?%s' % self.ty\n\n    def to_numpy_dtype(self):\n        if type(self.ty) in numpy_provides_missing:\n            return self.ty.to_numpy_dtype()\n        raise TypeError('DataShape measure %s is not NumPy-compatible' % self)\n\n\nclass CType(Unit):\n\n    \"\"\"\n    Symbol for a sized type mapping uniquely to a native type.\n    \"\"\"\n    cls = MEASURE\n    __slots__ = 'name', '_itemsize', '_alignment'\n\n    def __init__(self, name, itemsize, alignment):\n        self.name = name\n        self._itemsize = itemsize\n        self._alignment = alignment\n        Type.register(name, self)\n\n    @classmethod\n    def from_numpy_dtype(self, dt):\n        \"\"\"\n        From Numpy dtype.\n\n        >>> from datashader.datashape import CType\n        >>> from numpy import dtype\n        >>> CType.from_numpy_dtype(dtype('int32'))\n        ctype(\"int32\")\n        >>> CType.from_numpy_dtype(dtype('i8'))\n        ctype(\"int64\")\n        >>> CType.from_numpy_dtype(dtype('M8'))\n        DateTime(tz=None)\n        >>> CType.from_numpy_dtype(dtype('U30'))  # doctest: +SKIP\n        ctype(\"string[30, 'U32']\")\n        \"\"\"\n        try:\n            return Type.lookup_type(dt.name)\n        except KeyError:\n            pass\n        if np.issubdtype(dt, np.datetime64):\n            unit, _ = np.datetime_data(dt)\n            defaults = {'D': date_, 'Y': date_, 'M': date_, 'W': date_}\n            return defaults.get(unit, datetime_)\n        elif np.issubdtype(dt, np.timedelta64):\n            unit, _ = np.datetime_data(dt)\n            return TimeDelta(unit=unit)\n        elif np.__version__[0] < \"2\" and np.issubdtype(dt, np.unicode_):  # noqa: NPY201\n            return String(dt.itemsize // 4, 'U32')\n        elif np.issubdtype(dt, np.str_) or np.issubdtype(dt, np.bytes_):\n            return String(dt.itemsize, 'ascii')\n        raise NotImplementedError(\"NumPy datatype %s not supported\" % dt)\n\n    @property\n    def itemsize(self):\n        \"\"\"The size of one element of this type.\"\"\"\n        return self._itemsize\n\n    @property\n    def alignment(self):\n        \"\"\"The alignment of one element of this type.\"\"\"\n        return self._alignment\n\n    def to_numpy_dtype(self):\n        \"\"\"\n        To Numpy dtype.\n        \"\"\"\n        # TODO: Fixup the complex type to how numpy does it\n        name = self.name\n        return np.dtype({\n            'complex[float32]': 'complex64',\n            'complex[float64]': 'complex128'\n        }.get(name, name))\n\n    def __str__(self):\n        return self.name\n\n    def __repr__(self):\n        s = str(self)\n        return 'ctype(\"%s\")' % s.encode('unicode_escape').decode('ascii')\n\n\nclass Fixed(Unit):\n\n    \"\"\"\n    Fixed dimension.\n    \"\"\"\n    cls = DIMENSION\n    __slots__ = 'val',\n\n    def __init__(self, i):\n        # Use operator.index, so Python integers, numpy int scalars, etc work\n        i = operator.index(i)\n\n        if i < 0:\n            raise ValueError('Fixed dimensions must be positive')\n\n        self.val = i\n\n    def __index__(self):\n        return self.val\n\n    def __int__(self):\n        return self.val\n\n    def __eq__(self, other):\n        return (type(other) is Fixed and self.val == other.val or\n                isinstance(other, int) and self.val == other)\n\n    __hash__ = Mono.__hash__\n\n    def __str__(self):\n        return str(self.val)\n\n\nclass Var(Unit):\n\n    \"\"\" Variable dimension \"\"\"\n    cls = DIMENSION\n    __slots__ = ()\n\n\nclass TypeVar(Unit):\n\n    \"\"\"\n    A free variable in the signature. Not user facing.\n    \"\"\"\n    # cls could be MEASURE or DIMENSION, depending on context\n    __slots__ = 'symbol',\n\n    def __init__(self, symbol):\n        if not symbol[0].isupper():\n            raise ValueError(('TypeVar symbol %r does not '\n                              'begin with a capital') % symbol)\n        self.symbol = symbol\n\n    def __str__(self):\n        return str(self.symbol)\n\n\nclass Function(Mono):\n    \"\"\"Function signature type\n    \"\"\"\n    @property\n    def restype(self):\n        return self.parameters[-1]\n\n    @property\n    def argtypes(self):\n        return self.parameters[:-1]\n\n    def __str__(self):\n        return '(%s) -> %s' % (\n            ', '.join(map(str, self.argtypes)), self.restype\n        )\n\n\nclass Map(Mono):\n    __slots__ = 'key', 'value'\n\n    def __init__(self, key, value):\n        self.key = _launder(key)\n        self.value = _launder(value)\n\n    def __str__(self):\n        return '%s[%s, %s]' % (type(self).__name__.lower(),\n                               self.key,\n                               self.value)\n\n    def to_numpy_dtype(self):\n        return to_numpy_dtype(self)\n\n\ndef _launder(x):\n    \"\"\" Clean up types prior to insertion into DataShape\n\n    >>> from datashader.datashape import dshape\n    >>> _launder(5)         # convert ints to Fixed\n    Fixed(val=5)\n    >>> _launder('int32')   # parse strings\n    ctype(\"int32\")\n    >>> _launder(dshape('int32'))\n    ctype(\"int32\")\n    >>> _launder(Fixed(5))  # No-op on valid parameters\n    Fixed(val=5)\n    \"\"\"\n    if isinstance(x, int):\n        x = Fixed(x)\n    if isinstance(x, str):\n        x = datashape.dshape(x)\n    if isinstance(x, DataShape) and len(x) == 1:\n        return x[0]\n    if isinstance(x, Mono):\n        return x\n    return x\n\n\nclass CollectionPrinter:\n\n    def __repr__(self):\n        s = str(self)\n        strs = ('\"\"\"%s\"\"\"' if '\\n' in s else '\"%s\"') % s\n        return 'dshape(%s)' % strs\n\n\nclass RecordMeta(Type):\n    @staticmethod\n    def _unpack_slice(s, idx):\n        if not isinstance(s, slice):\n            raise TypeError(\n                'invalid field specification at position %d.\\n'\n                'fields must be formatted like: {name}:{type}' % idx,\n            )\n\n        name, type_ = packed = s.start, s.stop\n        if name is None:\n            raise TypeError('missing field name at position %d' % idx)\n        if not isinstance(name, str):\n            raise TypeError(\n                \"field name at position %d ('%s') was not a string\" % (\n                    idx, name,\n                ),\n            )\n        if type_ is None and s.step is None:\n            raise TypeError(\n                \"missing type for field '%s' at position %d\" % (name, idx))\n        if s.step is not None:\n            raise TypeError(\n                \"unexpected slice step for field '%s' at position %d.\\n\"\n                \"hint: you might have a second ':'\" % (name, idx),\n            )\n\n        return packed\n\n    def __getitem__(self, types):\n        if not isinstance(types, tuple):\n            types = types,\n\n        return self(list(map(self._unpack_slice, types, range(len(types)))))\n\n\nclass Record(CollectionPrinter, Mono, metaclass=RecordMeta):\n    \"\"\"\n    A composite data structure of ordered fields mapped to types.\n\n    Properties\n    ----------\n\n    fields: tuple of (name, type) pairs\n        The only stored data, also the input to ``__init__``\n    dict: dict\n        A dictionary view of ``fields``\n    names: list of strings\n        A list of the names\n    types: list of datashapes\n        A list of the datashapes\n\n    Example\n    -------\n\n    >>> Record([['id', 'int'], ['name', 'string'], ['amount', 'real']])\n    dshape(\"{id: int32, name: string, amount: float64}\")\n    \"\"\"\n    cls = MEASURE\n\n    def __init__(self, fields):\n        \"\"\"\n        Parameters\n        ----------\n        fields : list/OrderedDict of (name, type) entries\n            The fields which make up the record.\n        \"\"\"\n        if isinstance(fields, OrderedDict):\n            fields = fields.items()\n        fields = list(fields)\n        names = [\n            str(name) if not isinstance(name, str) else name\n            for name, _ in fields\n        ]\n        types = [_launder(v) for _, v in fields]\n\n        if len(set(names)) != len(names):\n            for name in set(names):\n                names.remove(name)\n            raise ValueError(\"duplicate field names found: %s\" % names)\n\n        self._parameters = tuple(zip(names, types)),\n\n    @property\n    def fields(self):\n        return self._parameters[0]\n\n    @property\n    def dict(self):\n        return dict(self.fields)\n\n    @property\n    def names(self):\n        return [n for n, t in self.fields]\n\n    @property\n    def types(self):\n        return [t for n, t in self.fields]\n\n    def to_numpy_dtype(self):\n        \"\"\"\n        To Numpy record dtype.\n        \"\"\"\n        return np.dtype([(str(name), to_numpy_dtype(typ))\n                         for name, typ in self.fields])\n\n    def __getitem__(self, key):\n        return self.dict[key]\n\n    def __str__(self):\n        return pprint(self)\n\n\nR = Record  # Alias for record literals\n\n\ndef _format_categories(cats, n=10):\n    return '[%s%s]' % (\n        ', '.join(map(repr, cats[:n])),\n        ', ...' if len(cats) > n else ''\n    )\n\n\nclass Categorical(Mono):\n    \"\"\"Unordered categorical type.\n    \"\"\"\n\n    __slots__ = 'categories', 'type', 'ordered'\n    cls = MEASURE\n\n    def __init__(self, categories, type=None, ordered=False):\n        self.categories = tuple(categories)\n        self.type = (type or datashape.discover(self.categories)).measure\n        self.ordered = ordered\n\n    def __str__(self):\n        return '%s[%s, type=%s, ordered=%s]' % (\n            type(self).__name__.lower(),\n            _format_categories(self.categories),\n            self.type,\n            self.ordered\n        )\n\n    def __repr__(self):\n        return '%s(categories=%s, type=%r, ordered=%s)' % (\n            type(self).__name__,\n            _format_categories(self.categories),\n            self.type,\n            self.ordered\n        )\n\n\nclass Tuple(CollectionPrinter, Mono):\n\n    \"\"\"\n    A product type.\n    \"\"\"\n    __slots__ = 'dshapes',\n    cls = MEASURE\n\n    def __init__(self, dshapes):\n        \"\"\"\n        Parameters\n        ----------\n        dshapes : list of dshapes\n            The datashapes which make up the tuple.\n        \"\"\"\n        dshapes = [DataShape(ds) if not isinstance(ds, DataShape) else ds\n                   for ds in dshapes]\n        self.dshapes = tuple(dshapes)\n\n    def __str__(self):\n        return '(%s)' % ', '.join(map(str, self.dshapes))\n\n    def to_numpy_dtype(self):\n        \"\"\"\n        To Numpy record dtype.\n        \"\"\"\n        return np.dtype([('f%d' % i, to_numpy_dtype(typ))\n                         for i, typ in enumerate(self.parameters[0])])\n\n\nclass JSON(Mono):\n\n    \"\"\" JSON measure \"\"\"\n    cls = MEASURE\n    __slots__ = ()\n\n    def __str__(self):\n        return 'json'\n\n\nbool_ = CType('bool', 1, 1)\nchar = CType('char', 1, 1)\n\nint8 = CType('int8', 1, 1)\nint16 = CType('int16', 2, ctypes.alignment(ctypes.c_int16))\nint32 = CType('int32', 4, ctypes.alignment(ctypes.c_int32))\nint64 = CType('int64', 8, ctypes.alignment(ctypes.c_int64))\n\n# int is an alias for int32\nint_ = int32\nType.register('int', int_)\n\nuint8 = CType('uint8', 1, 1)\nuint16 = CType('uint16', 2, ctypes.alignment(ctypes.c_uint16))\nuint32 = CType('uint32', 4, ctypes.alignment(ctypes.c_uint32))\nuint64 = CType('uint64', 8, ctypes.alignment(ctypes.c_uint64))\n\nfloat16 = CType('float16', 2, ctypes.alignment(ctypes.c_uint16))\nfloat32 = CType('float32', 4, ctypes.alignment(ctypes.c_float))\nfloat64 = CType('float64', 8, ctypes.alignment(ctypes.c_double))\n# float128 = CType('float128', 16)\n\n# real is an alias for float64\nreal = float64\nType.register('real', real)\n\ncomplex_float32 = CType('complex[float32]', 8,\n                        ctypes.alignment(ctypes.c_float))\ncomplex_float64 = CType('complex[float64]', 16,\n                        ctypes.alignment(ctypes.c_double))\nType.register('complex64', complex_float32)\ncomplex64 = complex_float32\n\nType.register('complex128', complex_float64)\ncomplex128 = complex_float64\n# complex256 = CType('complex256', 32)\n\n# complex is an alias for complex[float64]\ncomplex_ = complex_float64\n\ndate_ = Date()\ntime_ = Time()\ndatetime_ = DateTime()\ntimedelta_ = TimeDelta()\nType.register('date', date_)\nType.register('time', time_)\nType.register('datetime', datetime_)\nType.register('timedelta', timedelta_)\n\nnull = Null()\nType.register('null', null)\n\nc_byte = int8\nc_short = int16\nc_int = int32\nc_longlong = int64\n\nc_ubyte = uint8\nc_ushort = uint16\nc_ulonglong = uint64\n\nif ctypes.sizeof(ctypes.c_long) == 4:\n    c_long = int32\n    c_ulong = uint32\nelse:\n    c_long = int64\n    c_ulong = uint64\n\nif ctypes.sizeof(ctypes.c_void_p) == 4:\n    intptr = c_ssize_t = int32\n    uintptr = c_size_t = uint32\nelse:\n    intptr = c_ssize_t = int64\n    uintptr = c_size_t = uint64\nType.register('intptr', intptr)\nType.register('uintptr', uintptr)\n\nc_half = float16\nc_float = float32\nc_double = float64\n\n# TODO: Deal with the longdouble == one of float64/float80/float96/float128\n# situation\n\n# c_longdouble = float128\n\nhalf = float16\nsingle = float32\ndouble = float64\n\nvoid = CType('void', 0, 1)\nobject_ = pyobj = CType('object',\n                        ctypes.sizeof(ctypes.py_object),\n                        ctypes.alignment(ctypes.py_object))\n\nna = Null\nNullRecord = Record(())\nbytes_ = Bytes()\n\nstring = String()\njson = JSON()\n\nType.register('float', c_float)\nType.register('double', c_double)\n\nType.register('bytes', bytes_)\n\nType.register('string', String())\n\nvar = Var()\n\n\ndef to_numpy_dtype(ds):\n    \"\"\" Throw away the shape information and just return the\n    measure as NumPy dtype instance.\"\"\"\n    if isinstance(ds.measure, datashape.coretypes.Map):\n        ds = ds.measure.key\n    return to_numpy(ds.measure)[1]\n\n\ndef to_numpy(ds):\n    \"\"\"\n    Downcast a datashape object into a Numpy (shape, dtype) tuple if\n    possible.\n\n    >>> from datashader.datashape import dshape, to_numpy\n    >>> to_numpy(dshape('5 * 5 * int32'))\n    ((5, 5), dtype('int32'))\n    >>> to_numpy(dshape('10 * string[30]'))\n    ((10,), dtype('<U30'))\n    >>> to_numpy(dshape('N * int32'))\n    ((-1,), dtype('int32'))\n    \"\"\"\n    shape = []\n    if isinstance(ds, DataShape):\n        # The datashape dimensions\n        for dim in ds[:-1]:\n            if isinstance(dim, Fixed):\n                shape.append(int(dim))\n            elif isinstance(dim, TypeVar):\n                shape.append(-1)\n            else:\n                raise TypeError('DataShape dimension %s is not '\n                                'NumPy-compatible' % dim)\n\n        # The datashape measure\n        msr = ds[-1]\n    else:\n        msr = ds\n\n    return tuple(shape), msr.to_numpy_dtype()\n\n\ndef from_numpy(shape, dt):\n    \"\"\"\n    Upcast a (shape, dtype) tuple if possible.\n\n    >>> from datashader.datashape import from_numpy\n    >>> from numpy import dtype\n    >>> from_numpy((5, 5), dtype('int32'))\n    dshape(\"5 * 5 * int32\")\n\n    >>> from_numpy((10,), dtype('S10'))\n    dshape(\"10 * string[10, 'A']\")\n    \"\"\"\n    dtype = np.dtype(dt)\n\n    if dtype.kind == 'S':\n        measure = String(dtype.itemsize, 'A')\n    elif dtype.kind == 'U':\n        measure = String(dtype.itemsize // 4, 'U32')\n    elif dtype.fields:\n        fields = [(name, dtype.fields[name]) for name in dtype.names]\n        rec = [(name, from_numpy(t.shape, t.base))  # recurse into nested dtype\n               for name, (t, _) in fields]  # _ is the byte offset: ignore it\n        measure = Record(rec)\n    else:\n        measure = CType.from_numpy_dtype(dtype)\n\n    if not shape:\n        return measure\n    return DataShape(*tuple(map(Fixed, shape)) + (measure,))\n\n\ndef print_unicode_string(s):\n    try:\n        return s.decode('unicode_escape').encode('ascii')\n    except AttributeError:\n        return s\n\n\ndef pprint(ds, width=80):\n    ''' Pretty print a datashape\n\n    >>> from datashader.datashape import dshape, pprint\n    >>> print(pprint(dshape('5 * 3 * int32')))\n    5 * 3 * int32\n\n    >>> ds = dshape(\"\"\"\n    ... 5000000000 * {\n    ...     a: (int, float32, real, string, datetime),\n    ...     b: {c: 5 * int, d: var * 100 * float32}\n    ... }\"\"\")\n    >>> print(pprint(ds))\n    5000000000 * {\n      a: (int32, float32, float64, string, datetime),\n      b: {c: 5 * int32, d: var * 100 * float32}\n      }\n\n    Record measures print like full datashapes\n    >>> print(pprint(ds.measure, width=30))\n    {\n      a: (\n        int32,\n        float32,\n        float64,\n        string,\n        datetime\n        ),\n      b: {\n        c: 5 * int32,\n        d: var * 100 * float32\n        }\n      }\n\n    Control width of the result\n    >>> print(pprint(ds, width=30))\n    5000000000 * {\n      a: (\n        int32,\n        float32,\n        float64,\n        string,\n        datetime\n        ),\n      b: {\n        c: 5 * int32,\n        d: var * 100 * float32\n        }\n      }\n    >>>\n    '''\n    result = ''\n\n    if isinstance(ds, DataShape):\n        if ds.shape:\n            result += ' * '.join(map(str, ds.shape))\n            result += ' * '\n        ds = ds[-1]\n\n    if isinstance(ds, Record):\n        pairs = ['%s: %s' % (name if isidentifier(name) else\n                             repr(print_unicode_string(name)),\n                             pprint(typ, width - len(result) - len(name)))\n                 for name, typ in zip(ds.names, ds.types)]\n        short = '{%s}' % ', '.join(pairs)\n\n        if len(result + short) < width:\n            return result + short\n        else:\n            long = '{\\n%s\\n}' % ',\\n'.join(pairs)\n            return result + long.replace('\\n', '\\n  ')\n\n    elif isinstance(ds, Tuple):\n        typs = [pprint(typ, width-len(result))\n                for typ in ds.dshapes]\n        short = '(%s)' % ', '.join(typs)\n        if len(result + short) < width:\n            return result + short\n        else:\n            long = '(\\n%s\\n)' % ',\\n'.join(typs)\n            return result + long.replace('\\n', '\\n  ')\n    else:\n        result += str(ds)\n    return result\n",
    "datashader/datashape/util/__init__.py": "\nfrom itertools import chain\nimport operator\n\nfrom .. import parser\nfrom .. import type_symbol_table\nfrom ..validation import validate\nfrom .. import coretypes\n\n\n__all__ = 'dshape', 'dshapes', 'has_var_dim', 'has_ellipsis', 'cat_dshapes'\n\nsubclasses = operator.methodcaller('__subclasses__')\n\n#------------------------------------------------------------------------\n# Utility Functions for DataShapes\n#------------------------------------------------------------------------\n\ndef dshapes(*args):\n    \"\"\"\n    Parse a bunch of datashapes all at once.\n\n    >>> a, b = dshapes('3 * int32', '2 * var * float64')\n    \"\"\"\n    return [dshape(arg) for arg in args]\n\n\ndef dshape(o):\n    \"\"\"\n    Parse a datashape. For a thorough description see\n    https://datashape.readthedocs.io/en/latest/\n\n    >>> ds = dshape('2 * int32')\n    >>> ds[1]\n    ctype(\"int32\")\n    \"\"\"\n    if isinstance(o, coretypes.DataShape):\n        return o\n    if isinstance(o, str):\n        ds = parser.parse(o, type_symbol_table.sym)\n    elif isinstance(o, (coretypes.CType, coretypes.String,\n                        coretypes.Record, coretypes.JSON,\n                        coretypes.Date, coretypes.Time, coretypes.DateTime,\n                        coretypes.Unit)):\n        ds = coretypes.DataShape(o)\n    elif isinstance(o, coretypes.Mono):\n        ds = o\n    elif isinstance(o, (list, tuple)):\n        ds = coretypes.DataShape(*o)\n    else:\n        raise TypeError('Cannot create dshape from object of type %s' % type(o))\n    validate(ds)\n    return ds\n\n\ndef cat_dshapes(dslist):\n    \"\"\"\n    Concatenates a list of dshapes together along\n    the first axis. Raises an error if there is\n    a mismatch along another axis or the measures\n    are different.\n\n    Requires that the leading dimension be a known\n    size for all data shapes.\n    TODO: Relax this restriction to support\n          streaming dimensions.\n\n    >>> cat_dshapes(dshapes('10 * int32', '5 * int32'))\n    dshape(\"15 * int32\")\n    \"\"\"\n    if len(dslist) == 0:\n        raise ValueError('Cannot concatenate an empty list of dshapes')\n    elif len(dslist) == 1:\n        return dslist[0]\n\n    outer_dim_size = operator.index(dslist[0][0])\n    inner_ds = dslist[0][1:]\n    for ds in dslist[1:]:\n        outer_dim_size += operator.index(ds[0])\n        if ds[1:] != inner_ds:\n            raise ValueError(('The datashapes to concatenate much'\n                              ' all match after'\n                              ' the first dimension (%s vs %s)') %\n                              (inner_ds, ds[1:]))\n    return coretypes.DataShape(*[coretypes.Fixed(outer_dim_size)] + list(inner_ds))\n\n\ndef collect(pred, expr):\n    \"\"\" Collect terms in expression that match predicate\n\n    >>> from datashader.datashape import Unit, dshape\n    >>> predicate = lambda term: isinstance(term, Unit)\n    >>> dshape = dshape('var * {value: int64, loc: 2 * int32}')\n    >>> sorted(set(collect(predicate, dshape)), key=str)\n    [Fixed(val=2), ctype(\"int32\"), ctype(\"int64\"), Var()]\n    >>> from datashader.datashape import var, int64\n    >>> sorted(set(collect(predicate, [var, int64])), key=str)\n    [ctype(\"int64\"), Var()]\n    \"\"\"\n    if pred(expr):\n        return [expr]\n    if isinstance(expr, coretypes.Record):\n        return chain.from_iterable(collect(pred, typ) for typ in expr.types)\n    if isinstance(expr, coretypes.Mono):\n        return chain.from_iterable(collect(pred, typ) for typ in expr.parameters)\n    if isinstance(expr, (list, tuple)):\n        return chain.from_iterable(collect(pred, item) for item in expr)\n\n\ndef has_var_dim(ds):\n    \"\"\"Returns True if datashape has a variable dimension\n\n    Note currently treats variable length string as scalars.\n\n    >>> has_var_dim(dshape('2 * int32'))\n    False\n    >>> has_var_dim(dshape('var * 2 * int32'))\n    True\n    \"\"\"\n    return has((coretypes.Ellipsis, coretypes.Var), ds)\n\n\ndef has(typ, ds):\n    if isinstance(ds, typ):\n        return True\n    if isinstance(ds, coretypes.Record):\n        return any(has(typ, t) for t in ds.types)\n    if isinstance(ds, coretypes.Mono):\n        return any(has(typ, p) for p in ds.parameters)\n    if isinstance(ds, (list, tuple)):\n        return any(has(typ, item) for item in ds)\n    return False\n\n\ndef has_ellipsis(ds):\n    \"\"\"Returns True if the datashape has an ellipsis\n\n    >>> has_ellipsis(dshape('2 * int'))\n    False\n    >>> has_ellipsis(dshape('... * int'))\n    True\n    \"\"\"\n    return has(coretypes.Ellipsis, ds)\n",
    "datashader/datashape/parser.py": "\"\"\"\nParser for the datashape grammar.\n\"\"\"\n\n\nfrom . import lexer, error\n# TODO: Remove coretypes dependency, make 100% of interaction through\n#       the type symbol table\nfrom . import coretypes\n\n__all__ = ['parse']\n\n\nclass DataShapeParser:\n    \"\"\"A DataShape parser object.\"\"\"\n    def __init__(self, ds_str, sym):\n        # The datashape string being parsed\n        self.ds_str = ds_str\n        # Symbol tables for dimensions, dtypes, and type constructors for each\n        self.sym = sym\n        # The lexer\n        self.lex = lexer.lex(ds_str)\n        # The array of tokens self.lex has already produced\n        self.tokens = []\n        # The token currently being examined, and\n        # the end position, set when self.lex is exhausted\n        self.pos = -1\n        self.end_pos = None\n        # Advance to the first token\n        self.advance_tok()\n\n    def advance_tok(self):\n        \"\"\"Advances self.pos by one, if it is not already at the end.\"\"\"\n        if self.pos != self.end_pos:\n            self.pos = self.pos + 1\n            try:\n                # If self.pos has not been backtracked,\n                # we need to request a new token from the lexer\n                if self.pos >= len(self.tokens):\n                    self.tokens.append(next(self.lex))\n            except StopIteration:\n                # Create an EOF token, whose span starts at the\n                # end of the last token to use for error messages\n                if len(self.tokens) > 0:\n                    span = (self.tokens[self.pos-1].span[1],)*2\n                else:\n                    span = (0, 0)\n                self.tokens.append(lexer.Token(None, None, span, None))\n                self.end_pos = self.pos\n\n    @property\n    def tok(self):\n        return self.tokens[self.pos]\n\n    def raise_error(self, errmsg):\n        raise error.DataShapeSyntaxError(self.tok.span[0], '<nofile>',\n                                         self.ds_str, errmsg)\n\n    def parse_homogeneous_list(self, parse_item, sep_tok_id, errmsg,\n                               trailing_sep=False):\n        \"\"\"\n        <item>_list : <item> <SEP> <item>_list\n                    | <item>\n\n        Returns a list of <item>s, or None.\n        \"\"\"\n        saved_pos = self.pos\n        # Parse zero or more \"<item> <SEP>\" repetitions\n        items = []\n        item = True\n        while item is not None:\n            # Parse the <item>\n            item = parse_item()\n            if item is not None:\n                items.append(item)\n                if self.tok.id == sep_tok_id:\n                    # If a <SEP> is next, there are more items\n                    self.advance_tok()\n                else:\n                    # Otherwise we've reached the end\n                    return items\n            else:\n                if len(items) > 0:\n                    if trailing_sep:\n                        return items\n                    else:\n                        # If we already saw \"<item> <SEP>\" at least once,\n                        # we can point at the more specific position within\n                        # the list of <item>s where the error occurred\n                        self.raise_error(errmsg)\n                else:\n                    self.pos = saved_pos\n                    return None\n\n    def syntactic_sugar(self, symdict, name, dshapemsg, error_pos=None):\n        \"\"\"\n        Looks up a symbol in the provided symbol table dictionary for\n        syntactic sugar, raising a standard error message if the symbol\n        is missing.\n\n        Parameters\n        ----------\n        symdict : symbol table dictionary\n            One of self.sym.dtype, self.sym.dim,\n            self.sym.dtype_constr, or self.sym.dim_constr.\n        name : str\n            The name of the symbol to look up.\n        dshapemsg : str\n            The datashape construct this lookup is for, e.g.\n            '{...} dtype constructor'.\n        error_pos : int, optional\n            The position in the token stream at which to flag the error.\n        \"\"\"\n        entry = symdict.get(name)\n        if entry is not None:\n            return entry\n        else:\n            if error_pos is not None:\n                self.pos = error_pos\n            self.raise_error(('Symbol table missing \"%s\" ' +\n                             'entry for %s') % (name, dshapemsg))\n\n    def parse_datashape(self):\n        \"\"\"\n        datashape : datashape_nooption\n                  | QUESTIONMARK datashape_nooption\n                  | EXCLAMATIONMARK datashape_nooption\n\n        Returns a datashape object or None.\n        \"\"\"\n        tok = self.tok\n        constructors = {lexer.QUESTIONMARK: 'option'}\n        if tok.id in constructors:\n            self.advance_tok()\n            saved_pos = self.pos\n            ds = self.parse_datashape_nooption()\n            if ds is not None:\n                # Look in the dtype symbol table for the option type constructor\n                option = self.syntactic_sugar(self.sym.dtype_constr,\n                                              constructors[tok.id],\n                                              '%s dtype construction' %\n                                              constructors[tok.id],\n                                              saved_pos - 1)\n                return coretypes.DataShape(option(ds))\n        else:\n            return self.parse_datashape_nooption()\n\n    def parse_datashape_nooption(self):\n        \"\"\"\n        datashape_nooption : dim ASTERISK datashape\n                           | dtype\n\n        Returns a datashape object or None.\n        \"\"\"\n        saved_pos = self.pos\n        # Try dim ASTERISK datashape\n        dim = self.parse_dim()\n        if dim is not None:\n            if self.tok.id == lexer.ASTERISK:\n                # If an asterisk is next, we're good\n                self.advance_tok()\n                saved_pos = self.pos\n                dshape = self.parse_datashape()\n                if dshape is None:\n                    self.pos = saved_pos\n                    self.raise_error('Expected a dim or a dtype')\n                return coretypes.DataShape(dim, *dshape.parameters)\n        # Try dtype\n        dtype = self.parse_dtype()\n        if dtype:\n            return coretypes.DataShape(dtype)\n        else:\n            return None\n\n    def parse_dim(self):\n        \"\"\"\n        dim : typevar\n            | ellipsis_typevar\n            | type\n            | type_constr\n            | INTEGER\n            | ELLIPSIS\n        typevar : NAME_UPPER\n        ellipsis_typevar : NAME_UPPER ELLIPSIS\n        type : NAME_LOWER\n        type_constr : NAME_LOWER LBRACKET type_arg_list RBRACKET\n\n        Returns a the dim object, or None.\n        TODO: Support type constructors\n        \"\"\"\n        saved_pos = self.pos\n        tok = self.tok\n        if tok.id == lexer.NAME_UPPER:\n            val = tok.val\n            self.advance_tok()\n            if self.tok.id == lexer.ELLIPSIS:\n                self.advance_tok()\n                # TypeVars ellipses are treated as the \"ellipsis\" dim\n                tconstr = self.syntactic_sugar(self.sym.dim_constr, 'ellipsis',\n                                               'TypeVar... dim constructor',\n                                               saved_pos)\n                return tconstr(val)\n            elif self.tok.id == lexer.ASTERISK:\n                # Using a lookahead check for '*' after the TypeVar, so that\n                # the error message would be about a dtype problem instead\n                # of a dim problem when 'typevar' isn't in the symbol table\n                #\n                # TypeVars are treated as the \"typevar\" dim\n                tconstr = self.syntactic_sugar(self.sym.dim_constr, 'typevar',\n                                               'TypeVar dim constructor',\n                                               saved_pos)\n                return tconstr(val)\n            else:\n                self.pos = saved_pos\n                return None\n        elif tok.id == lexer.NAME_LOWER:\n            name = tok.val\n            self.advance_tok()\n            if self.tok.id == lexer.LBRACKET:\n                dim_constr = self.sym.dim_constr.get(name)\n                if dim_constr is None:\n                    self.pos = saved_pos\n                    return None\n                self.advance_tok()\n                args = self.parse_type_arg_list()   # noqa: F841\n                if self.tok.id == lexer.RBRACKET:\n                    self.advance_tok()\n                    raise NotImplementedError(\n                        'dim type constructors not actually supported yet')\n                else:\n                    self.raise_error('Expected a closing \"]\"')\n            else:\n                dim = self.sym.dim.get(name)\n                if dim is not None:\n                    return dim\n                else:\n                    self.pos = saved_pos\n                    return None\n        elif tok.id == lexer.INTEGER:\n            val = tok.val\n            self.advance_tok()\n            # If the token after the INTEGER is not ASTERISK,\n            # it cannot be a dim, so skip it\n            if self.tok.id != lexer.ASTERISK:\n                self.pos = saved_pos\n                return None\n            # Integers are treated as \"fixed\" dimensions\n            tconstr = self.syntactic_sugar(self.sym.dim_constr, 'fixed',\n                                           'integer dimensions')\n            return tconstr(val)\n        elif tok.id == lexer.ELLIPSIS:\n            self.advance_tok()\n            # Ellipses are treated as the \"ellipsis\" dim\n            dim = self.syntactic_sugar(self.sym.dim, 'ellipsis',\n                                           '... dim',\n                                           saved_pos)\n            return dim\n        else:\n            return None\n\n    def parse_dtype(self):\n        \"\"\"\n        dtype : typevar\n              | type\n              | type_constr\n              | struct_type\n              | funcproto_or_tuple_type\n        typevar : NAME_UPPER\n        ellipsis_typevar : NAME_UPPER ELLIPSIS\n        type : NAME_LOWER\n        type_constr : NAME_LOWER LBRACKET type_arg_list RBRACKET\n        struct_type : LBRACE ...\n        funcproto_or_tuple_type : LPAREN ...\n\n        Returns a the dtype object, or None.\n        \"\"\"\n        saved_pos = self.pos\n        tok = self.tok\n        if tok.id == lexer.NAME_UPPER:\n            val = tok.val\n            self.advance_tok()\n            # TypeVars are treated as the \"typevar\" dtype\n            tconstr = self.syntactic_sugar(self.sym.dtype_constr, 'typevar',\n                                           'TypeVar dtype constructor',\n                                           saved_pos)\n            return tconstr(val)\n        elif tok.id == lexer.NAME_LOWER:\n            name = tok.val\n            self.advance_tok()\n            if self.tok.id == lexer.LBRACKET:\n                dtype_constr = self.sym.dtype_constr.get(name)\n                if dtype_constr is None:\n                    self.pos = saved_pos\n                    return None\n                self.advance_tok()\n                args, kwargs = self.parse_type_arg_list()\n                if self.tok.id == lexer.RBRACKET:\n                    if len(args) == 0 and len(kwargs) == 0:\n                        self.raise_error('Expected at least one type ' +\n                                         'constructor argument')\n                    self.advance_tok()\n                    return dtype_constr(*args, **kwargs)\n                else:\n                    self.raise_error('Invalid type constructor argument')\n            else:\n                dtype = self.sym.dtype.get(name)\n                if dtype is not None:\n                    return dtype\n                else:\n                    self.pos = saved_pos\n                    return None\n        elif tok.id == lexer.LBRACE:\n            return self.parse_struct_type()\n        elif tok.id == lexer.LPAREN:\n            return self.parse_funcproto_or_tuple_type()\n        else:\n            return None\n\n    def parse_type_arg_list(self):\n        \"\"\"\n        type_arg_list : type_arg COMMA type_arg_list\n                      | type_kwarg_list\n                      | type_arg\n        type_kwarg_list : type_kwarg COMMA type_kwarg_list\n                        | type_kwarg\n\n        Returns a tuple (args, kwargs), or (None, None).\n        \"\"\"\n        # Parse zero or more \"type_arg COMMA\" repetitions\n        args = []\n        arg = True\n        while arg is not None:\n            # Parse the type_arg\n            arg = self.parse_type_arg()\n            if arg is not None:\n                if self.tok.id == lexer.COMMA:\n                    # If a comma is next, there are more args\n                    self.advance_tok()\n                    args.append(arg)\n                else:\n                    # Otherwise we've reached the end, and there\n                    # were no keyword args\n                    args.append(arg)\n                    return (args, {})\n            else:\n                break\n        kwargs = self.parse_homogeneous_list(self.parse_type_kwarg, lexer.COMMA,\n                                           'Expected another keyword argument, ' +\n                                           'positional arguments cannot follow ' +\n                                           'keyword arguments')\n        return (args, dict(kwargs) if kwargs else {})\n\n    def parse_type_arg(self):\n        \"\"\"\n        type_arg : datashape\n                 | INTEGER\n                 | STRING\n                 | BOOLEAN\n                 | list_type_arg\n        list_type_arg : LBRACKET RBRACKET\n                      | LBRACKET datashape_list RBRACKET\n                      | LBRACKET integer_list RBRACKET\n                      | LBRACKET string_list RBRACKET\n\n        Returns a type_arg value, or None.\n        \"\"\"\n        ds = self.parse_datashape()\n        if ds is not None:\n            return ds\n        if self.tok.id in [lexer.INTEGER, lexer.STRING, lexer.BOOLEAN]:\n            val = self.tok.val\n            self.advance_tok()\n            return val\n        elif self.tok.id == lexer.LBRACKET:\n            self.advance_tok()\n            val = self.parse_datashape_list()\n            if val is None:\n                val = self.parse_integer_list()\n            if val is None:\n                val = self.parse_string_list()\n            if val is None:\n                val = self.parse_boolean_list()\n            if self.tok.id == lexer.RBRACKET:\n                self.advance_tok()\n                return [] if val is None else val\n            else:\n                if val is None:\n                    self.raise_error('Expected a type constructor argument ' +\n                                     'or a closing \"]\"')\n                else:\n                    self.raise_error('Expected a \",\" or a closing \"]\"')\n        else:\n            return None\n\n    def parse_type_kwarg(self):\n        \"\"\"\n        type_kwarg : NAME_LOWER EQUAL type_arg\n\n        Returns a (name, type_arg) tuple, or None.\n        \"\"\"\n        if self.tok.id != lexer.NAME_LOWER:\n            return None\n        saved_pos = self.pos\n        name = self.tok.val\n        self.advance_tok()\n        if self.tok.id != lexer.EQUAL:\n            self.pos = saved_pos\n            return None\n        self.advance_tok()\n        arg = self.parse_type_arg()\n        if arg is not None:\n            return (name, arg)\n        else:\n            # After \"NAME_LOWER EQUAL\", a type_arg is required.\n            self.raise_error('Expected a type constructor argument')\n\n    def parse_datashape_list(self):\n        \"\"\"\n        datashape_list : datashape COMMA datashape_list\n                       | datashape\n\n        Returns a list of datashape type objects, or None.\n        \"\"\"\n        return self.parse_homogeneous_list(self.parse_datashape, lexer.COMMA,\n                                           'Expected another datashape, ' +\n                                           'type constructor parameter ' +\n                                           'lists must have uniform type')\n\n    def parse_integer(self):\n        \"\"\"\n        integer : INTEGER\n        \"\"\"\n        if self.tok.id == lexer.INTEGER:\n            val = self.tok.val\n            self.advance_tok()\n            return val\n        else:\n            return None\n\n    def parse_integer_list(self):\n        \"\"\"\n        integer_list : INTEGER COMMA integer_list\n                     | INTEGER\n\n        Returns a list of integers, or None.\n        \"\"\"\n        return self.parse_homogeneous_list(self.parse_integer, lexer.COMMA,\n                                           'Expected another integer, ' +\n                                           'type constructor parameter ' +\n                                           'lists must have uniform type')\n\n    def parse_boolean(self):\n        \"\"\"\n        boolean : BOOLEAN\n        \"\"\"\n        if self.tok.id == lexer.BOOLEAN:\n            val = self.tok.val\n            self.advance_tok()\n            return val\n        else:\n            return None\n\n    def parse_boolean_list(self):\n        \"\"\"\n        boolean_list : boolean COMMA boolean_list\n                     | boolean\n\n        Returns a list of booleans, or None.\n        \"\"\"\n        return self.parse_homogeneous_list(self.parse_boolean, lexer.COMMA,\n                                           'Expected another boolean, ' +\n                                           'type constructor parameter ' +\n                                           'lists must have uniform type')\n\n    def parse_string(self):\n        \"\"\"\n        string : STRING\n        \"\"\"\n        if self.tok.id == lexer.STRING:\n            val = self.tok.val\n            self.advance_tok()\n            return val\n        else:\n            return None\n\n    def parse_string_list(self):\n        \"\"\"\n        string_list : STRING COMMA string_list\n                    | STRING\n\n        Returns a list of strings, or None.\n        \"\"\"\n        return self.parse_homogeneous_list(self.parse_string, lexer.COMMA,\n                                           'Expected another string, ' +\n                                           'type constructor parameter ' +\n                                           'lists must have uniform type')\n\n    def parse_struct_type(self):\n        \"\"\"\n        struct_type : LBRACE struct_field_list RBRACE\n                    | LBRACE struct_field_list COMMA RBRACE\n\n        Returns a struct type, or None.\n        \"\"\"\n        if self.tok.id != lexer.LBRACE:\n            return None\n        saved_pos = self.pos\n        self.advance_tok()\n        fields = self.parse_homogeneous_list(self.parse_struct_field, lexer.COMMA,\n                                             'Invalid field in struct',\n                                             trailing_sep=True) or []\n        if self.tok.id != lexer.RBRACE:\n            self.raise_error('Invalid field in struct')\n        self.advance_tok()\n        # Split apart the names and types into separate lists,\n        # compatible with type constructor parameters\n        names = [f[0] for f in fields]\n        types = [f[1] for f in fields]\n        # Structs are treated as the \"struct\" dtype\n        tconstr = self.syntactic_sugar(self.sym.dtype_constr, 'struct',\n                                       '{...} dtype constructor', saved_pos)\n        return tconstr(names, types)\n\n    def parse_struct_field(self):\n        \"\"\"\n        struct_field : struct_field_name COLON datashape\n        struct_field_name : NAME_LOWER\n                          | NAME_UPPER\n                          | NAME_OTHER\n                          | STRING\n\n        Returns a tuple (name, datashape object) or None\n        \"\"\"\n        if self.tok.id not in [lexer.NAME_LOWER, lexer.NAME_UPPER,\n                               lexer.NAME_OTHER, lexer.STRING]:\n            return None\n        name = self.tok.val\n        self.advance_tok()\n        if self.tok.id != lexer.COLON:\n            self.raise_error('Expected a \":\" separating the field ' +\n                             'name from its datashape')\n        self.advance_tok()\n        ds = self.parse_datashape()\n        if ds is None:\n            self.raise_error('Expected the datashape of the field')\n        return (name, ds)\n\n    def parse_funcproto_or_tuple_type(self):\n        \"\"\"\n        funcproto_or_tuple_type : tuple_type RARROW datashape\n                                | tuple_type\n        tuple_type : LPAREN tuple_item_list RPAREN\n                   | LPAREN tuple_item_list COMMA RPAREN\n                   | LPAREN RPAREN\n        tuple_item_list : datashape COMMA tuple_item_list\n                        | datashape\n\n        Returns a tuple type object, a function prototype, or None.\n        \"\"\"\n        if self.tok.id != lexer.LPAREN:\n            return None\n        saved_pos = self.pos\n        self.advance_tok()\n        dshapes = self.parse_homogeneous_list(\n            self.parse_datashape,\n            lexer.COMMA,\n            'Invalid datashape in tuple',\n            trailing_sep=True,\n        ) or ()\n        if self.tok.id != lexer.RPAREN:\n            self.raise_error('Invalid datashape in tuple')\n        self.advance_tok()\n        if self.tok.id != lexer.RARROW:\n            # Tuples are treated as the \"tuple\" dtype\n            tconstr = self.syntactic_sugar(self.sym.dtype_constr, 'tuple',\n                                           '(...) dtype constructor', saved_pos)\n            return tconstr(dshapes)\n        else:\n            # Get the return datashape after the right arrow\n            self.advance_tok()\n            ret_dshape = self.parse_datashape()\n            if ret_dshape is None:\n                self.raise_error('Expected function prototype return ' +\n                                 'datashape')\n            # Function Prototypes are treated as the \"funcproto\" dtype\n            tconstr = self.syntactic_sugar(self.sym.dtype_constr, 'funcproto',\n                                           '(...) -> ... dtype constructor',\n                                           saved_pos)\n            return tconstr(dshapes, ret_dshape)\n\n\ndef parse(ds_str, sym):\n    \"\"\"Parses a single datashape from a string.\n\n    Parameters\n    ----------\n    ds_str : string\n        The datashape string to parse.\n    sym : TypeSymbolTable\n        The symbol tables of dimensions, dtypes, and type constructors for each.\n\n    \"\"\"\n    dsp = DataShapeParser(ds_str, sym)\n    ds = dsp.parse_datashape()\n    # If no datashape could be found\n    if ds is None:\n        dsp.raise_error('Invalid datashape')\n\n    # Make sure there's no garbage at the end\n    if dsp.pos != dsp.end_pos:\n        dsp.raise_error('Unexpected token in datashape')\n    return ds\n",
    "datashader/datatypes.py": "from __future__ import annotations\n\nimport re\n\nfrom functools import total_ordering\nfrom packaging.version import Version\n\nimport numpy as np\nimport pandas as pd\n\nfrom numba import jit\nfrom pandas.api.extensions import (\n    ExtensionDtype, ExtensionArray, register_extension_dtype)\nfrom numbers import Integral\n\nfrom pandas.api.types import pandas_dtype, is_extension_array_dtype\n\n\ntry:\n    # See if we can register extension type with dask >= 1.1.0\n    from dask.dataframe.extensions import make_array_nonempty\nexcept ImportError:\n    make_array_nonempty = None\n\n\ndef _validate_ragged_properties(start_indices, flat_array):\n    \"\"\"\n    Validate that start_indices are flat_array arrays that may be used to\n    represent a valid RaggedArray.\n\n    Parameters\n    ----------\n    flat_array: numpy array containing concatenation\n                of all nested arrays to be represented\n                by this ragged array\n    start_indices: unsigned integer numpy array the same\n                   length as the ragged array where values\n                   represent the index into flat_array where\n                   the corresponding ragged array element\n                   begins\n    Raises\n    ------\n    ValueError:\n        if input arguments are invalid or incompatible properties\n    \"\"\"\n\n    # Validate start_indices\n    if (not isinstance(start_indices, np.ndarray) or\n            start_indices.dtype.kind != 'u' or\n            start_indices.ndim != 1):\n        raise ValueError(\"\"\"\nThe start_indices property of a RaggedArray must be a 1D numpy array of\nunsigned integers (start_indices.dtype.kind == 'u')\n    Received value of type {typ}: {v}\"\"\".format(\n            typ=type(start_indices), v=repr(start_indices)))\n\n    # Validate flat_array\n    if (not isinstance(flat_array, np.ndarray) or\n            flat_array.ndim != 1):\n        raise ValueError(\"\"\"\nThe flat_array property of a RaggedArray must be a 1D numpy array\n    Received value of type {typ}: {v}\"\"\".format(\n            typ=type(flat_array), v=repr(flat_array)))\n\n    # Validate start_indices values\n    # We don't need to check start_indices < 0 because we already know that it\n    # has an unsigned integer datatype\n    #\n    # Note that start_indices[i] == len(flat_array) is valid as it represents\n    # and empty array element at the end of the ragged array.\n    invalid_inds = start_indices > len(flat_array)\n\n    if invalid_inds.any():\n        some_invalid_vals = start_indices[invalid_inds[:10]]\n\n        raise ValueError(\"\"\"\nElements of start_indices must be less than the length of flat_array ({m})\n    Invalid values include: {vals}\"\"\".format(\n            m=len(flat_array), vals=repr(some_invalid_vals)))\n\n\n# Internal ragged element array wrapper that provides\n# equality, ordering, and hashing.\n@total_ordering\nclass _RaggedElement:\n\n    @staticmethod\n    def ragged_or_nan(a):\n        if np.isscalar(a) and np.isnan(a):\n            return a\n        else:\n            return _RaggedElement(a)\n\n    @staticmethod\n    def array_or_nan(a):\n        if np.isscalar(a) and np.isnan(a):\n            return a\n        else:\n            return a.array\n\n    def __init__(self, array):\n        self.array = array\n\n    def __hash__(self):\n        return hash(self.array.tobytes())\n\n    def __eq__(self, other):\n        if not isinstance(other, _RaggedElement):\n            return False\n        return np.array_equal(self.array, other.array)\n\n    def __lt__(self, other):\n        if not isinstance(other, _RaggedElement):\n            return NotImplemented\n        return _lexograph_lt(self.array, other.array)\n\n    def __repr__(self):\n        array_repr = repr(self.array)\n        return array_repr.replace('array', 'ragged_element')\n\n\n@register_extension_dtype\nclass RaggedDtype(ExtensionDtype):\n    \"\"\"\n    Pandas ExtensionDtype to represent a ragged array datatype\n\n    Methods not otherwise documented here are inherited from ExtensionDtype;\n    please see the corresponding method on that class for the docstring\n    \"\"\"\n    type = np.ndarray\n    base = np.dtype('O')\n    _subtype_re = re.compile(r\"^ragged\\[(?P<subtype>\\w+)\\]$\")\n    _metadata = ('_dtype',)\n\n    @property\n    def name(self):\n        return 'Ragged[{subtype}]'.format(subtype=self.subtype)\n\n    def __repr__(self):\n        return self.name\n\n    @classmethod\n    def construct_array_type(cls):\n        return RaggedArray\n\n    @classmethod\n    def construct_from_string(cls, string):\n        if not isinstance(string, str):\n            raise TypeError(\"'construct_from_string' expects a string, got %s\" % type(string))\n\n        # lowercase string\n        string = string.lower()\n\n        msg = \"Cannot construct a 'RaggedDtype' from '{}'\"\n        if string.startswith('ragged'):\n            # Extract subtype\n            try:\n                subtype_string = cls._parse_subtype(string)\n                return RaggedDtype(dtype=subtype_string)\n            except Exception:\n                raise TypeError(msg.format(string))\n        else:\n            raise TypeError(msg.format(string))\n\n    def __init__(self, dtype=np.float64):\n        if isinstance(dtype, RaggedDtype):\n            self._dtype = dtype.subtype\n        else:\n            self._dtype = np.dtype(dtype)\n\n    @property\n    def subtype(self):\n        return self._dtype\n\n    @classmethod\n    def _parse_subtype(cls, dtype_string):\n        \"\"\"\n        Parse a datatype string to get the subtype\n\n        Parameters\n        ----------\n        dtype_string: str\n            A string like Ragged[subtype]\n\n        Returns\n        -------\n        subtype: str\n\n        Raises\n        ------\n        ValueError\n            When the subtype cannot be extracted\n        \"\"\"\n        # Be case insensitive\n        dtype_string = dtype_string.lower()\n\n        match = cls._subtype_re.match(dtype_string)\n        if match:\n            subtype_string = match.groupdict()['subtype']\n        elif dtype_string == 'ragged':\n            subtype_string = 'float64'\n        else:\n            raise ValueError(\"Cannot parse {dtype_string}\".format(\n                dtype_string=dtype_string))\n        return subtype_string\n\n\ndef missing(v):\n    return v is None or (np.isscalar(v) and np.isnan(v))\n\n\nclass RaggedArray(ExtensionArray):\n    \"\"\"\n    Pandas ExtensionArray to represent ragged arrays\n\n    Methods not otherwise documented here are inherited from ExtensionArray;\n    please see the corresponding method on that class for the docstring\n    \"\"\"\n    def __init__(self, data, dtype=None, copy=False):\n        \"\"\"\n        Construct a RaggedArray\n\n        Parameters\n        ----------\n        data: list or array or dict or RaggedArray\n            * list or 1D-array: A List or 1D array of lists or 1D arrays that\n                                should be represented by the RaggedArray\n\n            * dict: A dict containing 'start_indices' and 'flat_array' keys\n                    with numpy array values where:\n                    - flat_array:  numpy array containing concatenation\n                                   of all nested arrays to be represented\n                                   by this ragged array\n                    - start_indices: unsigned integer numpy array the same\n                                     length as the ragged array where values\n                                     represent the index into flat_array where\n                                     the corresponding ragged array element\n                                     begins\n            * RaggedArray: A RaggedArray instance to copy\n\n        dtype: RaggedDtype or np.dtype or str or None (default None)\n            Datatype to use to store underlying values from data.\n            If none (the default) then dtype will be determined using the\n            numpy.result_type function.\n        copy : bool (default False)\n            Whether to deep copy the input arrays. Only relevant when `data`\n            has type `dict` or `RaggedArray`. When data is a `list` or\n            `array`, input arrays are always copied.\n        \"\"\"\n        if (isinstance(data, dict) and\n                all(k in data for k in\n                    ['start_indices', 'flat_array'])):\n\n            _validate_ragged_properties(\n                start_indices=data['start_indices'],\n                flat_array=data['flat_array'])\n\n            self._start_indices = data['start_indices']\n            self._flat_array = data['flat_array']\n            dtype = self._flat_array.dtype\n\n            if copy:\n                self._start_indices = self._start_indices.copy()\n                self._flat_array = self._flat_array.copy()\n\n        elif isinstance(data, RaggedArray):\n            self._flat_array = data.flat_array\n            self._start_indices = data.start_indices\n            dtype = self._flat_array.dtype\n\n            if copy:\n                self._start_indices = self._start_indices.copy()\n                self._flat_array = self._flat_array.copy()\n        else:\n            # Compute lengths\n            index_len = len(data)\n            buffer_len = sum(len(datum)\n                             if not missing(datum)\n                             else 0 for datum in data)\n\n            # Compute necessary precision of start_indices array\n            for nbits in [8, 16, 32, 64]:\n                start_indices_dtype = 'uint' + str(nbits)\n                max_supported = np.iinfo(start_indices_dtype).max\n                if buffer_len <= max_supported:\n                    break\n\n            # infer dtype if not provided\n            if dtype is None:\n                non_missing = [np.atleast_1d(v)\n                               for v in data if not missing(v)]\n                if non_missing:\n                    dtype = np.result_type(*non_missing)\n                else:\n                    dtype = 'float64'\n            elif isinstance(dtype, RaggedDtype):\n                dtype = dtype.subtype\n\n            # Initialize representation arrays\n            self._start_indices = np.zeros(index_len, dtype=start_indices_dtype)\n            self._flat_array = np.zeros(buffer_len, dtype=dtype)\n\n            # Populate arrays\n            next_start_ind = 0\n            for i, array_el in enumerate(data):\n                # Compute element length\n                n = len(array_el) if not missing(array_el) else 0\n\n                # Update start indices\n                self._start_indices[i] = next_start_ind\n\n                # Do not assign when slice is empty avoiding possible\n                # nan assignment to integer array\n                if not n:\n                    continue\n\n                # Update flat array\n                self._flat_array[next_start_ind:next_start_ind+n] = array_el\n\n                # increment next start index\n                next_start_ind += n\n\n        self._dtype = RaggedDtype(dtype=dtype)\n\n    def __eq__(self, other):\n        if isinstance(other, RaggedArray):\n            if len(other) != len(self):\n                raise ValueError(\"\"\"\nCannot check equality of RaggedArray values of unequal length\n    len(ra1) == {len_ra1}\n    len(ra2) == {len_ra2}\"\"\".format(\n                    len_ra1=len(self),\n                    len_ra2=len(other)))\n\n            result = _eq_ragged_ragged(\n                self.start_indices, self.flat_array,\n                other.start_indices, other.flat_array)\n        else:\n            # Convert other to numpy array\n            if not isinstance(other, np.ndarray):\n                other_array = np.asarray(other)\n            else:\n                other_array = other\n\n            if other_array.ndim == 1 and other_array.dtype.kind != 'O':\n\n                # Treat as ragged scalar\n                result = _eq_ragged_scalar(\n                    self.start_indices, self.flat_array, other_array)\n            elif (other_array.ndim == 1 and\n                  other_array.dtype.kind == 'O' and\n                  len(other_array) == len(self)):\n\n                # Treat as vector\n                result = _eq_ragged_ndarray1d(\n                    self.start_indices, self.flat_array, other_array)\n            elif (other_array.ndim == 2 and\n                  other_array.dtype.kind != 'O' and\n                  other_array.shape[0] == len(self)):\n\n                # Treat rows as ragged elements\n                result = _eq_ragged_ndarray2d(\n                    self.start_indices, self.flat_array, other_array)\n            else:\n                raise ValueError(\"\"\"\nCannot check equality of RaggedArray of length {ra_len} with:\n    {other}\"\"\".format(ra_len=len(self), other=repr(other)))\n\n        return result\n\n    def __ne__(self, other):\n        return np.logical_not(self == other)\n\n    @property\n    def flat_array(self):\n        \"\"\"\n        numpy array containing concatenation of all nested arrays\n\n        Returns\n        -------\n        np.ndarray\n        \"\"\"\n        return self._flat_array\n\n    @property\n    def start_indices(self):\n        \"\"\"\n        unsigned integer numpy array the same length as the ragged array where\n        values represent the index into flat_array where the corresponding\n        ragged array element begins\n\n        Returns\n        -------\n        np.ndarray\n        \"\"\"\n        return self._start_indices\n\n    def __len__(self):\n        return len(self._start_indices)\n\n    def __getitem__(self, item):\n        err_msg = (\"Only integers, slices and integer or boolean\"\n                   \"arrays are valid indices.\")\n        if isinstance(item, Integral):\n            if item < -len(self) or item >= len(self):\n                raise IndexError(\"{item} is out of bounds\".format(item=item))\n            else:\n                # Convert negative item index\n                if item < 0:\n                    item += len(self)\n\n                slice_start = self.start_indices[item]\n                slice_end = (self.start_indices[item+1]\n                             if item + 1 <= len(self) - 1\n                             else len(self.flat_array))\n\n                return (self.flat_array[slice_start:slice_end]\n                        if slice_end!=slice_start\n                        else np.nan)\n\n        elif type(item) is slice:\n            data = []\n            selected_indices = np.arange(len(self))[item]\n\n            for selected_index in selected_indices:\n                data.append(self[selected_index])\n\n            return RaggedArray(data, dtype=self.flat_array.dtype)\n\n        elif isinstance(item, (np.ndarray, ExtensionArray, list, tuple)):\n            if isinstance(item, (np.ndarray, ExtensionArray)):\n                # Leave numpy and pandas arrays alone\n                kind = item.dtype.kind\n            else:\n                # Convert others to pandas arrays\n                item = pd.array(item)\n                kind = item.dtype.kind\n\n            if len(item) == 0:\n                return self.take([], allow_fill=False)\n            elif kind == 'b':\n                # Check mask length is compatible\n                if len(item) != len(self):\n                    raise IndexError(\n                        \"Boolean index has wrong length: {} instead of {}\"\n                        .format(len(item), len(self))\n                    )\n\n                # check for NA values\n                isna = pd.isna(item)\n                if isna.any():\n                    if Version(pd.__version__) > Version('1.0.1'):\n                        item[isna] = False\n                    else:\n                        raise ValueError(\n                            \"Cannot mask with a boolean indexer containing NA values\"\n                        )\n\n                data = []\n\n                for i, m in enumerate(item):\n                    if m:\n                        data.append(self[i])\n\n                return RaggedArray(data, dtype=self.flat_array.dtype)\n            elif kind in ('i', 'u'):\n                if any(pd.isna(item)):\n                    raise ValueError(\n                        \"Cannot index with an integer indexer containing NA values\"\n                    )\n                return self.take(item, allow_fill=False)\n            else:\n                raise IndexError(err_msg)\n        else:\n            raise IndexError(err_msg)\n\n    @classmethod\n    def _from_sequence(cls, scalars, dtype=None, copy=False):\n        return RaggedArray(scalars, dtype=dtype)\n\n    @classmethod\n    def _from_factorized(cls, values, original):\n        return RaggedArray(\n            [_RaggedElement.array_or_nan(v) for v in values],\n            dtype=original.flat_array.dtype)\n\n    def _as_ragged_element_array(self):\n        return np.array([_RaggedElement.ragged_or_nan(self[i])\n                         for i in range(len(self))])\n\n    def _values_for_factorize(self):\n        return self._as_ragged_element_array(), np.nan\n\n    def _values_for_argsort(self):\n        return self._as_ragged_element_array()\n\n    def unique(self):\n        from pandas import unique\n\n        uniques = unique(self._as_ragged_element_array())\n        return self._from_sequence(\n            [_RaggedElement.array_or_nan(v) for v in uniques],\n            dtype=self.dtype)\n\n    def fillna(self, value=None, method=None, limit=None):\n        # Override in RaggedArray to handle ndarray fill values\n        from pandas.util._validators import validate_fillna_kwargs\n        from pandas.core.missing import get_fill_func\n\n        value, method = validate_fillna_kwargs(value, method)\n\n        mask = self.isna()\n\n        if isinstance(value, RaggedArray):\n            if len(value) != len(self):\n                raise ValueError(\"Length of 'value' does not match. Got ({}) \"\n                                 \" expected {}\".format(len(value), len(self)))\n            value = value[mask]\n\n        if mask.any():\n            if method is not None:\n                func = get_fill_func(method)\n                new_values = func(self.astype(object), limit=limit,\n                                  mask=mask)\n                new_values = self._from_sequence(new_values, dtype=self.dtype)\n            else:\n                # fill with value\n                new_values = list(self)\n                mask_indices, = np.where(mask)\n                for ind in mask_indices:\n                    new_values[ind] = value\n\n                new_values = self._from_sequence(new_values, dtype=self.dtype)\n        else:\n            new_values = self.copy()\n        return new_values\n\n    def shift(self, periods=1, fill_value=None):\n        # Override in RaggedArray to handle ndarray fill values\n\n        # Note: this implementation assumes that `self.dtype.na_value` can be\n        # stored in an instance of your ExtensionArray with `self.dtype`.\n        if not len(self) or periods == 0:\n            return self.copy()\n\n        if fill_value is None:\n            fill_value = np.nan\n\n        empty = self._from_sequence(\n            [fill_value] * min(abs(periods), len(self)),\n            dtype=self.dtype\n        )\n        if periods > 0:\n            a = empty\n            b = self[:-periods]\n        else:\n            a = self[abs(periods):]\n            b = empty\n        return self._concat_same_type([a, b])\n\n    def searchsorted(self, value, side=\"left\", sorter=None):\n        arr = self._as_ragged_element_array()\n        if isinstance(value, RaggedArray):\n            search_value = value._as_ragged_element_array()\n        else:\n            search_value = _RaggedElement(value)\n        return arr.searchsorted(search_value, side=side, sorter=sorter)\n\n    def isna(self):\n        stop_indices = np.hstack([self.start_indices[1:],\n                                  [len(self.flat_array)]])\n\n        element_lengths = stop_indices - self.start_indices\n        return element_lengths == 0\n\n    def take(self, indices, allow_fill=False, fill_value=None):\n        if allow_fill:\n            invalid_inds = [i for i in indices if i < -1]\n            if invalid_inds:\n                raise ValueError(\"\"\"\nInvalid indices for take with allow_fill True: {inds}\"\"\".format(\n                    inds=invalid_inds[:9]))\n            sequence = [self[i] if i >= 0 else fill_value\n                        for i in indices]\n        else:\n            if len(self) == 0 and len(indices) > 0:\n                raise IndexError(\n                    \"cannot do a non-empty take from an empty axis|out of bounds\"\n                )\n\n            sequence = [self[i] for i in indices]\n\n        return RaggedArray(sequence, dtype=self.flat_array.dtype)\n\n    def copy(self, deep=False):\n        data = dict(\n            flat_array=self.flat_array,\n            start_indices=self.start_indices)\n\n        return RaggedArray(data, copy=deep)\n\n    @classmethod\n    def _concat_same_type(cls, to_concat):\n        # concat flat_arrays\n        flat_array = np.hstack([ra.flat_array for ra in to_concat])\n\n        # offset and concat start_indices\n        offsets = np.hstack([\n            [0], np.cumsum([len(ra.flat_array) for ra in to_concat[:-1]])\n        ]).astype('uint64')\n\n        start_indices = np.hstack([ra.start_indices + offset\n                                   for offset, ra in zip(offsets, to_concat)])\n\n        return RaggedArray(dict(\n            flat_array=flat_array, start_indices=start_indices),\n            copy=False)\n\n    @property\n    def dtype(self):\n        return self._dtype\n\n    @property\n    def nbytes(self):\n        return (self._flat_array.nbytes +\n                self._start_indices.nbytes)\n\n    def astype(self, dtype, copy=True):\n        dtype = pandas_dtype(dtype)\n        if isinstance(dtype, RaggedDtype):\n            if copy:\n                return self.copy()\n            return self\n\n        elif is_extension_array_dtype(dtype):\n            return dtype.construct_array_type()._from_sequence(\n                np.asarray(self))\n\n        return np.array([v for v in self], dtype=dtype)\n\n    def tolist(self):\n        # Based on pandas ExtensionArray.tolist\n        if self.ndim > 1:\n            return [item.tolist() for item in self]\n        else:\n            return list(self)\n\n    def __array__(self, dtype=None, copy=True):\n        dtype = np.dtype(object) if dtype is None else np.dtype(dtype)\n        if copy:\n            return np.array(self.tolist(), dtype=dtype)\n        else:\n            return np.array(self, dtype=dtype)\n\n    def duplicated(self, *args, **kwargs):\n        msg = \"duplicated is not implemented for RaggedArray\"\n        raise NotImplementedError(msg)\n\n\n@jit(nopython=True, nogil=True)\ndef _eq_ragged_ragged(start_indices1,\n                      flat_array1,\n                      start_indices2,\n                      flat_array2):\n    \"\"\"\n    Compare elements of two ragged arrays of the same length\n\n    Parameters\n    ----------\n    start_indices1: ndarray\n        start indices of a RaggedArray 1\n    flat_array1: ndarray\n        flat_array property of a RaggedArray 1\n    start_indices2: ndarray\n        start indices of a RaggedArray 2\n    flat_array2: ndarray\n        flat_array property of a RaggedArray 2\n\n    Returns\n    -------\n    mask: ndarray\n        1D bool array of same length as inputs with elements True when\n        corresponding elements are equal, False otherwise\n    \"\"\"\n    n = len(start_indices1)\n    m1 = len(flat_array1)\n    m2 = len(flat_array2)\n\n    result = np.zeros(n, dtype=np.bool_)\n\n    for i in range(n):\n        # Extract inds for ra1\n        start_index1 = start_indices1[i]\n        stop_index1 = start_indices1[i + 1] if i < n - 1 else m1\n        len_1 = stop_index1 - start_index1\n\n        # Extract inds for ra2\n        start_index2 = start_indices2[i]\n        stop_index2 = start_indices2[i + 1] if i < n - 1 else m2\n        len_2 = stop_index2 - start_index2\n\n        if len_1 != len_2:\n            el_equal = False\n        else:\n            el_equal = True\n            for flat_index1, flat_index2 in \\\n                    zip(range(start_index1, stop_index1),\n                        range(start_index2, stop_index2)):\n                el_1 = flat_array1[flat_index1]\n                el_2 = flat_array2[flat_index2]\n                el_equal &= el_1 == el_2\n\n        result[i] = el_equal\n\n    return result\n\n\n@jit(nopython=True, nogil=True)\ndef _eq_ragged_scalar(start_indices, flat_array, val):\n    \"\"\"\n    Compare elements of a RaggedArray with a scalar array\n\n    Parameters\n    ----------\n    start_indices: ndarray\n        start indices of a RaggedArray\n    flat_array: ndarray\n        flat_array property of a RaggedArray\n    val: ndarray\n\n    Returns\n    -------\n    mask: ndarray\n        1D bool array of same length as inputs with elements True when\n        ragged element equals scalar val, False otherwise.\n    \"\"\"\n    n = len(start_indices)\n    m = len(flat_array)\n    cols = len(val)\n    result = np.zeros(n, dtype=np.bool_)\n    for i in range(n):\n        start_index = start_indices[i]\n        stop_index = start_indices[i+1] if i < n - 1 else m\n\n        if stop_index - start_index != cols:\n            el_equal = False\n        else:\n            el_equal = True\n            for val_index, flat_index in \\\n                    enumerate(range(start_index, stop_index)):\n                el_equal &= flat_array[flat_index] == val[val_index]\n        result[i] = el_equal\n\n    return result\n\n\ndef _eq_ragged_ndarray1d(start_indices, flat_array, a):\n    \"\"\"\n    Compare a RaggedArray with a 1D numpy object array of the same length\n\n    Parameters\n    ----------\n    start_indices: ndarray\n        start indices of a RaggedArray\n    flat_array: ndarray\n        flat_array property of a RaggedArray\n    a: ndarray\n        1D numpy array of same length as ra\n\n    Returns\n    -------\n    mask: ndarray\n        1D bool array of same length as input with elements True when\n        corresponding elements are equal, False otherwise\n\n    Notes\n    -----\n    This function is not numba accelerated because it, by design, inputs\n    a numpy object array\n    \"\"\"\n\n    n = len(start_indices)\n    m = len(flat_array)\n    result = np.zeros(n, dtype=np.bool_)\n    for i in range(n):\n        start_index = start_indices[i]\n        stop_index = start_indices[i + 1] if i < n - 1 else m\n        a_val = a[i]\n        if (a_val is None or\n                (np.isscalar(a_val) and np.isnan(a_val)) or\n                len(a_val) == 0):\n            result[i] = start_index == stop_index\n        else:\n            result[i] = np.array_equal(flat_array[start_index:stop_index],\n                                       a_val)\n\n    return result\n\n\n@jit(nopython=True, nogil=True)\ndef _eq_ragged_ndarray2d(start_indices, flat_array, a):\n    \"\"\"\n    Compare a RaggedArray with rows of a 2D numpy object array\n\n    Parameters\n    ----------\n    start_indices: ndarray\n        start indices of a RaggedArray\n    flat_array: ndarray\n        flat_array property of a RaggedArray\n    a: ndarray\n        A 2D numpy array where the length of the first dimension matches the\n        length of the RaggedArray\n\n    Returns\n    -------\n    mask: ndarray\n        1D bool array of same length as input RaggedArray with elements True\n        when corresponding elements of ra equal corresponding row of `a`\n    \"\"\"\n    n = len(start_indices)\n    m = len(flat_array)\n    cols = a.shape[1]\n\n    # np.bool is an alias for Python's built-in bool type, np.bool_ is the\n    # numpy type that numba recognizes\n    result = np.zeros(n, dtype=np.bool_)\n    for row in range(n):\n        start_index = start_indices[row]\n        stop_index = start_indices[row + 1] if row < n - 1 else m\n\n        # Check equality\n        if stop_index - start_index != cols:\n            el_equal = False\n        else:\n            el_equal = True\n            for col, flat_index in enumerate(range(start_index, stop_index)):\n                el_equal &= flat_array[flat_index] == a[row, col]\n        result[row] = el_equal\n    return result\n\n\n@jit(nopython=True, nogil=True)\ndef _lexograph_lt(a1, a2):\n    \"\"\"\n    Compare two 1D numpy arrays lexographically\n    Parameters\n    ----------\n    a1: ndarray\n        1D numpy array\n    a2: ndarray\n        1D numpy array\n\n    Returns\n    -------\n    comparison:\n        True if a1 < a2, False otherwise\n    \"\"\"\n    for e1, e2 in zip(a1, a2):\n        if e1 < e2:\n            return True\n        elif e1 > e2:\n            return False\n    return len(a1) < len(a2)\n\n\ndef ragged_array_non_empty(dtype):\n    return RaggedArray([[1], [1, 2]], dtype=dtype)\n\n\nif make_array_nonempty:\n    make_array_nonempty.register(RaggedDtype)(ragged_array_non_empty)\n",
    "datashader/datashape/validation.py": "\"\"\"\nDatashape validation.\n\"\"\"\n\nfrom . import coretypes as T\n\n\ndef traverse(f, t):\n    \"\"\"\n    Map `f` over `t`, calling `f` with type `t` and the map result of the\n    mapping `f` over `t` 's parameters.\n\n    Parameters\n    ----------\n    f : callable\n    t : DataShape\n\n    Returns\n    -------\n    DataShape\n    \"\"\"\n    if isinstance(t, T.Mono) and not isinstance(t, T.Unit):\n        return f(t, [traverse(f, p) for p in t.parameters])\n    return t\n\n\ndef validate(ds):\n    \"\"\"\n    Validate a datashape to see whether it is well-formed.\n\n    Parameters\n    ----------\n    ds : DataShape\n\n    Examples\n    --------\n    >>> from datashader.datashape import dshape\n    >>> dshape('10 * int32')\n    dshape(\"10 * int32\")\n    >>> dshape('... * int32')\n    dshape(\"... * int32\")\n    >>> dshape('... * ... * int32') # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    TypeError: Can only use a single wildcard\n    >>> dshape('T * ... * X * ... * X') # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    TypeError: Can only use a single wildcard\n    >>> dshape('T * ...') # doctest: +IGNORE_EXCEPTION_DETAIL\n    Traceback (most recent call last):\n        ...\n    DataShapeSyntaxError: Expected a dtype\n    \"\"\"\n    traverse(_validate, ds)\n\n\ndef _validate(ds, params):\n    if isinstance(ds, T.DataShape):\n        # Check ellipses\n        ellipses = [x for x in ds.parameters if isinstance(x, T.Ellipsis)]\n        if len(ellipses) > 1:\n            raise TypeError(\"Can only use a single wildcard\")\n        elif isinstance(ds.parameters[-1], T.Ellipsis):\n            raise TypeError(\"Measure may not be an Ellipsis (...)\")\n"
  },
  "GT_src_dict": {
    "datashader/datashape/coretypes.py": {
      "Mono.info": {
        "code": "    def info(self):\n        \"\"\"Retrieve information about the Mono type.\n\n    This method returns a tuple containing the type of the current Mono instance \n    and its parameters. The parameters are provided during the initialization of \n    the Mono class and represent the characteristics or defining properties of \n    the type.\n\n    Returns\n    -------\n    tuple\n        A tuple where the first element is the type of the current instance \n        (inherited from the built-in type), and the second element is the \n        parameters associated with the instance, stored as an attribute \n        (self.parameters).\n\n    Notes\n    -----\n    This method is primarily used for introspection and debugging purposes to \n    gather type and parameter information about an instance of the Mono class \n    and its subclasses.\"\"\"\n        return (type(self), self.parameters)",
        "docstring": "Retrieve information about the Mono type.\n\nThis method returns a tuple containing the type of the current Mono instance \nand its parameters. The parameters are provided during the initialization of \nthe Mono class and represent the characteristics or defining properties of \nthe type.\n\nReturns\n-------\ntuple\n    A tuple where the first element is the type of the current instance \n    (inherited from the built-in type), and the second element is the \n    parameters associated with the instance, stored as an attribute \n    (self.parameters).\n\nNotes\n-----\nThis method is primarily used for introspection and debugging purposes to \ngather type and parameter information about an instance of the Mono class \nand its subclasses.",
        "signature": "def info(self):",
        "type": "Method",
        "class_signature": "class Mono(metaclass=Type):"
      },
      "Mono.__eq__": {
        "code": "    def __eq__(self, other):\n        \"\"\"Check for equality between two Mono instances.\n\nParameters\n----------\nother : Any\n    The object to compare with the current instance. It is expected to be a Mono or a subclass of Mono.\n\nReturns\n-------\nbool\n    True if the other object is an instance of Mono and both the shape and measure values are equal; False otherwise.\n\nNotes\n-----\n- The equality check relies on the `shape` property, which returns the dimensions of the Mono instance, and the `measure` property, which provides the associated measure.\n- The `measure.info()` method is called to compare the internal details of the measurement types between the two instances.\"\"\"\n        return isinstance(other, Mono) and self.shape == other.shape and (self.measure.info() == other.measure.info())",
        "docstring": "Check for equality between two Mono instances.\n\nParameters\n----------\nother : Any\n    The object to compare with the current instance. It is expected to be a Mono or a subclass of Mono.\n\nReturns\n-------\nbool\n    True if the other object is an instance of Mono and both the shape and measure values are equal; False otherwise.\n\nNotes\n-----\n- The equality check relies on the `shape` property, which returns the dimensions of the Mono instance, and the `measure` property, which provides the associated measure.\n- The `measure.info()` method is called to compare the internal details of the measurement types between the two instances.",
        "signature": "def __eq__(self, other):",
        "type": "Method",
        "class_signature": "class Mono(metaclass=Type):"
      },
      "Mono.__repr__": {
        "code": "    def __repr__(self):\n        \"\"\"Returns a string representation of the Mono instance, formatted to include the class name and either its slot attributes or parameters.\n\nThe method constructs a string that identifies the class type and details its attributes. If the instance has attributes defined in `__slots__`, it will return these in the form of keyword-value pairs. Otherwise, it will map the parameters of the instance to their string representations.\n\nReturns:\n    str: A string representing the Mono instance including class type and its attributes or parameters.\n\nDependencies:\n- `self.__slots__`: A tuple of attribute names if the class uses slots, which allows for more memory-efficient object representation.\n\nSide Effects:\nNone. The method purely generates a string representation of the instance without modifying any attributes.\"\"\"\n        return '%s(%s)' % (type(self).__name__, ', '.join(('%s=%r' % (slot, getattr(self, slot)) for slot in self.__slots__) if self._slotted else map(repr, self.parameters)))",
        "docstring": "Returns a string representation of the Mono instance, formatted to include the class name and either its slot attributes or parameters.\n\nThe method constructs a string that identifies the class type and details its attributes. If the instance has attributes defined in `__slots__`, it will return these in the form of keyword-value pairs. Otherwise, it will map the parameters of the instance to their string representations.\n\nReturns:\n    str: A string representing the Mono instance including class type and its attributes or parameters.\n\nDependencies:\n- `self.__slots__`: A tuple of attribute names if the class uses slots, which allows for more memory-efficient object representation.\n\nSide Effects:\nNone. The method purely generates a string representation of the instance without modifying any attributes.",
        "signature": "def __repr__(self):",
        "type": "Method",
        "class_signature": "class Mono(metaclass=Type):"
      },
      "DataShape.__init__": {
        "code": "    def __init__(self, *parameters, **kwds):\n        \"\"\"Initialize a DataShape object, which serves as a composite container for \ndatashape elements, such as dimensions and measures.\n\nParameters\n----------\n*parameters : Variable length argument list\n    Should contain at least two items: dimensions (instances of Fixed or Var) \n    followed by a measure (an instance of Unit or subclasses thereof). \n\n**kwds : Additional keyword arguments\n    - name (str, optional): If provided, registers the DataShape under this name \n    in the internal type registry.\n\nRaises\n------\nTypeError : If only one parameter is provided or if the last parameter is not a measure, \n    or if any of the preceding parameters are not dimensions.\nValueError : If no parameters are supplied or if only one is given.\n\nAttributes\n----------\n- _parameters : Tuple\n    A collection of cleaned parameters, processed by the _launder function to ensure \n    they are valid datashape types.\n- composite : bool\n    Set to True to indicate that this DataShape is a composite type.\n- name : str or None\n    Optional name under which this DataShape is registered in the type registry.\n\nConstants Used\n--------------\n- MEASURE : Indicates that a type is a measure (defined in the module, used to validate \n  the last parameter).\n- DIMENSION : Indicates that a type is a dimension (defined in the module, used to validate \n  preceding parameters).\n\nThis method relies on the _launder function to clean parameters, ensuring they conform \nto expected types before constructing the DataShape.\"\"\"\n        if len(parameters) == 1 and isinstance(parameters[0], str):\n            raise TypeError(\"DataShape constructor for internal use.\\nUse dshape function to convert strings into datashapes.\\nTry:\\n\\tdshape('%s')\" % parameters[0])\n        if len(parameters) > 0:\n            self._parameters = tuple(map(_launder, parameters))\n            if getattr(self._parameters[-1], 'cls', MEASURE) != MEASURE:\n                raise TypeError('Only a measure can appear on the last position of a datashape, not %s' % repr(self._parameters[-1]))\n            for dim in self._parameters[:-1]:\n                if getattr(dim, 'cls', DIMENSION) != DIMENSION:\n                    raise TypeError('Only dimensions can appear before the last position of a datashape, not %s' % repr(dim))\n        else:\n            raise ValueError('the data shape should be constructed from 2 or more parameters, only got %s' % len(parameters))\n        self.composite = True\n        self.name = kwds.get('name')\n        if self.name:\n            type(type(self))._registry[self.name] = self",
        "docstring": "Initialize a DataShape object, which serves as a composite container for \ndatashape elements, such as dimensions and measures.\n\nParameters\n----------\n*parameters : Variable length argument list\n    Should contain at least two items: dimensions (instances of Fixed or Var) \n    followed by a measure (an instance of Unit or subclasses thereof). \n\n**kwds : Additional keyword arguments\n    - name (str, optional): If provided, registers the DataShape under this name \n    in the internal type registry.\n\nRaises\n------\nTypeError : If only one parameter is provided or if the last parameter is not a measure, \n    or if any of the preceding parameters are not dimensions.\nValueError : If no parameters are supplied or if only one is given.\n\nAttributes\n----------\n- _parameters : Tuple\n    A collection of cleaned parameters, processed by the _launder function to ensure \n    they are valid datashape types.\n- composite : bool\n    Set to True to indicate that this DataShape is a composite type.\n- name : str or None\n    Optional name under which this DataShape is registered in the type registry.\n\nConstants Used\n--------------\n- MEASURE : Indicates that a type is a measure (defined in the module, used to validate \n  the last parameter).\n- DIMENSION : Indicates that a type is a dimension (defined in the module, used to validate \n  preceding parameters).\n\nThis method relies on the _launder function to clean parameters, ensuring they conform \nto expected types before constructing the DataShape.",
        "signature": "def __init__(self, *parameters, **kwds):",
        "type": "Method",
        "class_signature": "class DataShape(Mono):"
      },
      "DataShape.__getitem__": {
        "code": "    def __getitem__(self, index):\n        \"\"\"Retrieve a specific parameter from the `DataShape` instance.\n\nParameters\n----------\nindex : int\n    The index of the parameter to retrieve from the `parameters` tuple of the `DataShape` instance.\n\nReturns\n-------\nThe parameter at the specified index, which can be of various types depending on the structure of the `DataShape`.\n\nNotes\n-----\nThis method allows access to the individual components of the `DataShape`, such as dimensions and measures, which are stored in the `self.parameters` attribute. The `parameters` attribute is a tuple containing the elements of the data shape, and is initialized when the `DataShape` is constructed.\"\"\"\n        return self.parameters[index]",
        "docstring": "Retrieve a specific parameter from the `DataShape` instance.\n\nParameters\n----------\nindex : int\n    The index of the parameter to retrieve from the `parameters` tuple of the `DataShape` instance.\n\nReturns\n-------\nThe parameter at the specified index, which can be of various types depending on the structure of the `DataShape`.\n\nNotes\n-----\nThis method allows access to the individual components of the `DataShape`, such as dimensions and measures, which are stored in the `self.parameters` attribute. The `parameters` attribute is a tuple containing the elements of the data shape, and is initialized when the `DataShape` is constructed.",
        "signature": "def __getitem__(self, index):",
        "type": "Method",
        "class_signature": "class DataShape(Mono):"
      },
      "DataShape.shape": {
        "code": "    def shape(self):\n        \"\"\"Return the shape of the DataShape instance.\n\nThis method retrieves all but the last parameter of the DataShape instance, which represents a collection of dimensions. The last parameter typically denotes the measure (i.e., the data type). This method allows access to the shape of multi-dimensional data structures encapsulated by the DataShape, making it easier to work with data dimensions in a consistent manner.\n\nReturns:\n    tuple: A tuple containing all dimension parameters of the DataShape, excluding the measure. Each dimension is represented by an instance of Fixed or Var classes.\n\nDependencies:\n    This method relies on the `parameters` attribute, which is a tuple holding the components of the DataShape. The last item in this tuple is assumed to be the measure or data type, which is not included in the shape returned by this method.\"\"\"\n        return self.parameters[:-1]",
        "docstring": "Return the shape of the DataShape instance.\n\nThis method retrieves all but the last parameter of the DataShape instance, which represents a collection of dimensions. The last parameter typically denotes the measure (i.e., the data type). This method allows access to the shape of multi-dimensional data structures encapsulated by the DataShape, making it easier to work with data dimensions in a consistent manner.\n\nReturns:\n    tuple: A tuple containing all dimension parameters of the DataShape, excluding the measure. Each dimension is represented by an instance of Fixed or Var classes.\n\nDependencies:\n    This method relies on the `parameters` attribute, which is a tuple holding the components of the DataShape. The last item in this tuple is assumed to be the measure or data type, which is not included in the shape returned by this method.",
        "signature": "def shape(self):",
        "type": "Method",
        "class_signature": "class DataShape(Mono):"
      },
      "DataShape.measure": {
        "code": "    def measure(self):\n        \"\"\"Return the measure of the DataShape.\n\nThe measure corresponds to the last element in the parameters of the DataShape instance. This attribute is crucial as it defines the data type of the values contained within the DataShape, such as int32, float64, or user-defined types. The measure is typically a type that follows the conventions established in the datashape module.\n\nReturns:\n    Mono: The measure type of the DataShape, which is the last element in the parameters tuple.\n\nDependencies:\n    This method relies on the `parameters` attribute of the DataShape class, which is populated during initialization and contains the datatypes (measures) used throughout the structure.\"\"\"\n        return self.parameters[-1]",
        "docstring": "Return the measure of the DataShape.\n\nThe measure corresponds to the last element in the parameters of the DataShape instance. This attribute is crucial as it defines the data type of the values contained within the DataShape, such as int32, float64, or user-defined types. The measure is typically a type that follows the conventions established in the datashape module.\n\nReturns:\n    Mono: The measure type of the DataShape, which is the last element in the parameters tuple.\n\nDependencies:\n    This method relies on the `parameters` attribute of the DataShape class, which is populated during initialization and contains the datatypes (measures) used throughout the structure.",
        "signature": "def measure(self):",
        "type": "Method",
        "class_signature": "class DataShape(Mono):"
      },
      "CType.__repr__": {
        "code": "    def __repr__(self):\n        \"\"\"Generates a string representation of the CType instance.\n\nThis method converts the internal name of the CType into a format suitable for display. It encodes the string to handle any special Unicode characters, ensuring the output is correctly formatted for various character encodings. The result is a string in the format 'ctype(\"name\")', where 'name' is the string representation of the CType instance.\n\nReturns\n-------\nstr\n    A formatted string representing the CType instance, containing its name encoded for ASCII compatibility.\n\nDependencies\n------------\nThis method relies on the __str__ method to retrieve the name of the CType instance. It also uses the `encode` and `decode` methods to ensure proper Unicode handling.\"\"\"\n        s = str(self)\n        return 'ctype(\"%s\")' % s.encode('unicode_escape').decode('ascii')",
        "docstring": "Generates a string representation of the CType instance.\n\nThis method converts the internal name of the CType into a format suitable for display. It encodes the string to handle any special Unicode characters, ensuring the output is correctly formatted for various character encodings. The result is a string in the format 'ctype(\"name\")', where 'name' is the string representation of the CType instance.\n\nReturns\n-------\nstr\n    A formatted string representing the CType instance, containing its name encoded for ASCII compatibility.\n\nDependencies\n------------\nThis method relies on the __str__ method to retrieve the name of the CType instance. It also uses the `encode` and `decode` methods to ensure proper Unicode handling.",
        "signature": "def __repr__(self):",
        "type": "Method",
        "class_signature": "class CType(Unit):"
      },
      "Fixed.__init__": {
        "code": "    def __init__(self, i):\n        \"\"\"Initializes a Fixed dimension.\n\nParameters\n----------\ni : int\n    A non-negative integer representing the fixed dimension size. This can be a Python integer or a NumPy integer scalar, as the method utilizes `operator.index()` for compatibility.\n\nRaises\n------\nValueError\n    If the provided dimension size is negative, indicating that Fixed dimensions must be positive.\n\nAttributes\n----------\nval : int\n    Stores the fixed dimension size, ensuring that downstream processes utilize this value correctly to define the dimensions of data shapes.\n\nThis method interacts with the Fixed class's __index__ method, enabling instances of Fixed to be treated like native integers in contexts that require integer values. It relies on the operator module for converting various integer types to a standard Python integer format.\"\"\"\n        i = operator.index(i)\n        if i < 0:\n            raise ValueError('Fixed dimensions must be positive')\n        self.val = i",
        "docstring": "Initializes a Fixed dimension.\n\nParameters\n----------\ni : int\n    A non-negative integer representing the fixed dimension size. This can be a Python integer or a NumPy integer scalar, as the method utilizes `operator.index()` for compatibility.\n\nRaises\n------\nValueError\n    If the provided dimension size is negative, indicating that Fixed dimensions must be positive.\n\nAttributes\n----------\nval : int\n    Stores the fixed dimension size, ensuring that downstream processes utilize this value correctly to define the dimensions of data shapes.\n\nThis method interacts with the Fixed class's __index__ method, enabling instances of Fixed to be treated like native integers in contexts that require integer values. It relies on the operator module for converting various integer types to a standard Python integer format.",
        "signature": "def __init__(self, i):",
        "type": "Method",
        "class_signature": "class Fixed(Unit):"
      },
      "Fixed.__index__": {
        "code": "    def __index__(self):\n        \"\"\"Returns the integer representation of the Fixed dimension.\n\nThis method allows instances of the Fixed class to be used in contexts that require an integer index, such as slicing operations. It retrieves the value stored in the `val` attribute, which represents the fixed size dimension initialized during instance creation.\n\nReturns:\n    int: The integer value of the fixed dimension.\n\nAttributes:\n    val (int): The fixed size dimension, which can be set to a positive integer during initialization of a Fixed instance.\"\"\"\n        return self.val",
        "docstring": "Returns the integer representation of the Fixed dimension.\n\nThis method allows instances of the Fixed class to be used in contexts that require an integer index, such as slicing operations. It retrieves the value stored in the `val` attribute, which represents the fixed size dimension initialized during instance creation.\n\nReturns:\n    int: The integer value of the fixed dimension.\n\nAttributes:\n    val (int): The fixed size dimension, which can be set to a positive integer during initialization of a Fixed instance.",
        "signature": "def __index__(self):",
        "type": "Method",
        "class_signature": "class Fixed(Unit):"
      },
      "Fixed.__eq__": {
        "code": "    def __eq__(self, other):\n        \"\"\"Compares two Fixed instances for equality.\n\nParameters\n----------\nother : object\n    The object to compare with the current Fixed instance. \n\nReturns\n-------\nbool\n    Returns True if `other` is either another Fixed instance with the same value or an integer that matches the Fixed value. Otherwise, returns False.\n\nNotes\n-----\nThis method allows for the comparison of Fixed objects to both their own type and standard integers, providing flexibility in equality checks when dealing with fixed dimensions in data shapes. The 'val' attribute, representing the fixed dimension size, is compared against the integer input to determine equality.\"\"\"\n        return type(other) is Fixed and self.val == other.val or (isinstance(other, int) and self.val == other)",
        "docstring": "Compares two Fixed instances for equality.\n\nParameters\n----------\nother : object\n    The object to compare with the current Fixed instance. \n\nReturns\n-------\nbool\n    Returns True if `other` is either another Fixed instance with the same value or an integer that matches the Fixed value. Otherwise, returns False.\n\nNotes\n-----\nThis method allows for the comparison of Fixed objects to both their own type and standard integers, providing flexibility in equality checks when dealing with fixed dimensions in data shapes. The 'val' attribute, representing the fixed dimension size, is compared against the integer input to determine equality.",
        "signature": "def __eq__(self, other):",
        "type": "Method",
        "class_signature": "class Fixed(Unit):"
      }
    },
    "datashader/datashape/util/__init__.py": {
      "dshape": {
        "code": "def dshape(o):\n    \"\"\"Parse a datashape from various input formats.\n\nParameters:\n- o: Can be a string representing a datashape, an instance of coretypes.DataShape, or other types such as CType, String, Record, JSON, Date, Time, DateTime, Unit, or Mono. It can also be a list or tuple of dimensions.\n\nReturns:\n- An instance of coretypes.DataShape that represents the parsed datashape.\n\nRaises:\n- TypeError: If the input cannot be converted into a datashape.\n- Validation error: If the resulting datashape does not pass the validation checks performed by the `validate` function.\n\nDependencies:\n- Relies on the 'parser' module for parsing strings into datashapes, as well as the 'validate' function from the validation module to ensure the correctness of the datashape.\n- Interacts with various classes from the coretypes module to construct appropriate datashape representations.\"\"\"\n    '\\n    Parse a datashape. For a thorough description see\\n    https://datashape.readthedocs.io/en/latest/\\n\\n    >>> ds = dshape(\\'2 * int32\\')\\n    >>> ds[1]\\n    ctype(\"int32\")\\n    '\n    if isinstance(o, coretypes.DataShape):\n        return o\n    if isinstance(o, str):\n        ds = parser.parse(o, type_symbol_table.sym)\n    elif isinstance(o, (coretypes.CType, coretypes.String, coretypes.Record, coretypes.JSON, coretypes.Date, coretypes.Time, coretypes.DateTime, coretypes.Unit)):\n        ds = coretypes.DataShape(o)\n    elif isinstance(o, coretypes.Mono):\n        ds = o\n    elif isinstance(o, (list, tuple)):\n        ds = coretypes.DataShape(*o)\n    else:\n        raise TypeError('Cannot create dshape from object of type %s' % type(o))\n    validate(ds)\n    return ds",
        "docstring": "Parse a datashape from various input formats.\n\nParameters:\n- o: Can be a string representing a datashape, an instance of coretypes.DataShape, or other types such as CType, String, Record, JSON, Date, Time, DateTime, Unit, or Mono. It can also be a list or tuple of dimensions.\n\nReturns:\n- An instance of coretypes.DataShape that represents the parsed datashape.\n\nRaises:\n- TypeError: If the input cannot be converted into a datashape.\n- Validation error: If the resulting datashape does not pass the validation checks performed by the `validate` function.\n\nDependencies:\n- Relies on the 'parser' module for parsing strings into datashapes, as well as the 'validate' function from the validation module to ensure the correctness of the datashape.\n- Interacts with various classes from the coretypes module to construct appropriate datashape representations.",
        "signature": "def dshape(o):",
        "type": "Function",
        "class_signature": null
      },
      "cat_dshapes": {
        "code": "def cat_dshapes(dslist):\n    \"\"\"Concatenates a list of data shapes (dshapes) along the first axis. The function raises a ValueError if the list is empty, or if there is a mismatch in dimensions other than the first, or if the measures differ. It requires that the first dimension (the leading dimension) of all data shapes is a fixed known size.\n\nParameters:\n- dslist: A list of `coretypes.DataShape` objects to be concatenated. Each dshape should share the same inner shapes after the first dimension.\n\nReturns:\n- A new `coretypes.DataShape` representing the concatenated shape with the first dimension size equal to the sum of the first dimensions of all input dshapes.\n\nRaises:\n- ValueError if `dslist` is empty, or if the inner shapes (after the first dimension) do not match across the provided dshapes.\n\nDependencies:\n- Relies on `coretypes` for data shape representations and ensures that validation of dimensions follows the rules defined there.\n- Utilizes `operator.index` to safely convert dimension sizes to integers.\"\"\"\n    '\\n    Concatenates a list of dshapes together along\\n    the first axis. Raises an error if there is\\n    a mismatch along another axis or the measures\\n    are different.\\n\\n    Requires that the leading dimension be a known\\n    size for all data shapes.\\n    TODO: Relax this restriction to support\\n          streaming dimensions.\\n\\n    >>> cat_dshapes(dshapes(\\'10 * int32\\', \\'5 * int32\\'))\\n    dshape(\"15 * int32\")\\n    '\n    if len(dslist) == 0:\n        raise ValueError('Cannot concatenate an empty list of dshapes')\n    elif len(dslist) == 1:\n        return dslist[0]\n    outer_dim_size = operator.index(dslist[0][0])\n    inner_ds = dslist[0][1:]\n    for ds in dslist[1:]:\n        outer_dim_size += operator.index(ds[0])\n        if ds[1:] != inner_ds:\n            raise ValueError('The datashapes to concatenate much all match after the first dimension (%s vs %s)' % (inner_ds, ds[1:]))\n    return coretypes.DataShape(*[coretypes.Fixed(outer_dim_size)] + list(inner_ds))",
        "docstring": "Concatenates a list of data shapes (dshapes) along the first axis. The function raises a ValueError if the list is empty, or if there is a mismatch in dimensions other than the first, or if the measures differ. It requires that the first dimension (the leading dimension) of all data shapes is a fixed known size.\n\nParameters:\n- dslist: A list of `coretypes.DataShape` objects to be concatenated. Each dshape should share the same inner shapes after the first dimension.\n\nReturns:\n- A new `coretypes.DataShape` representing the concatenated shape with the first dimension size equal to the sum of the first dimensions of all input dshapes.\n\nRaises:\n- ValueError if `dslist` is empty, or if the inner shapes (after the first dimension) do not match across the provided dshapes.\n\nDependencies:\n- Relies on `coretypes` for data shape representations and ensures that validation of dimensions follows the rules defined there.\n- Utilizes `operator.index` to safely convert dimension sizes to integers.",
        "signature": "def cat_dshapes(dslist):",
        "type": "Function",
        "class_signature": null
      },
      "has_var_dim": {
        "code": "def has_var_dim(ds):\n    \"\"\"Returns True if the given datashape (ds) contains a variable dimension. A variable dimension is defined by the presence of either `coretypes.Ellipsis` (representing an unspecified number of dimensions) or `coretypes.Var` (indicating a variable size). Note that this function currently treats variable-length strings as scalars. \n\nParameters:\n- ds (DataShape): The datashape to be evaluated.\n\nReturns:\n- bool: True if ds has a variable dimension; otherwise, False.\n\nDependencies:\n- Utilizes the `has` function to check for the presence of variable dimensions within the datashape, which is defined elsewhere in the code.\n- References `coretypes.Ellipsis` and `coretypes.Var`, which are defined in the `coretypes` module, indicating specific types of variable dimensions within datashapes.\"\"\"\n    \"Returns True if datashape has a variable dimension\\n\\n    Note currently treats variable length string as scalars.\\n\\n    >>> has_var_dim(dshape('2 * int32'))\\n    False\\n    >>> has_var_dim(dshape('var * 2 * int32'))\\n    True\\n    \"\n    return has((coretypes.Ellipsis, coretypes.Var), ds)",
        "docstring": "Returns True if the given datashape (ds) contains a variable dimension. A variable dimension is defined by the presence of either `coretypes.Ellipsis` (representing an unspecified number of dimensions) or `coretypes.Var` (indicating a variable size). Note that this function currently treats variable-length strings as scalars. \n\nParameters:\n- ds (DataShape): The datashape to be evaluated.\n\nReturns:\n- bool: True if ds has a variable dimension; otherwise, False.\n\nDependencies:\n- Utilizes the `has` function to check for the presence of variable dimensions within the datashape, which is defined elsewhere in the code.\n- References `coretypes.Ellipsis` and `coretypes.Var`, which are defined in the `coretypes` module, indicating specific types of variable dimensions within datashapes.",
        "signature": "def has_var_dim(ds):",
        "type": "Function",
        "class_signature": null
      },
      "has": {
        "code": "def has(typ, ds):\n    \"\"\"Checks if a specified type is present within a given datashape or any of its nested components.\n\nParameters:\n- typ: A type (class) to search for within the datashape `ds`.\n- ds: A datashape object, which could be a primitive type, a `coretypes.Record`, a `coretypes.Mono`, or a collection (list or tuple) of datashapes.\n\nReturns:\n- bool: True if `typ` is found in `ds` or any of its subtypes; otherwise, False.\n\nThe function interacts with other parts of the code through the `coretypes` module, which defines various data types used within datashapes. It recursively checks for the presence of `typ` in the structure of `ds`, allowing for complex datashape compositions to be evaluated for the desired type.\"\"\"\n    if isinstance(ds, typ):\n        return True\n    if isinstance(ds, coretypes.Record):\n        return any((has(typ, t) for t in ds.types))\n    if isinstance(ds, coretypes.Mono):\n        return any((has(typ, p) for p in ds.parameters))\n    if isinstance(ds, (list, tuple)):\n        return any((has(typ, item) for item in ds))\n    return False",
        "docstring": "Checks if a specified type is present within a given datashape or any of its nested components.\n\nParameters:\n- typ: A type (class) to search for within the datashape `ds`.\n- ds: A datashape object, which could be a primitive type, a `coretypes.Record`, a `coretypes.Mono`, or a collection (list or tuple) of datashapes.\n\nReturns:\n- bool: True if `typ` is found in `ds` or any of its subtypes; otherwise, False.\n\nThe function interacts with other parts of the code through the `coretypes` module, which defines various data types used within datashapes. It recursively checks for the presence of `typ` in the structure of `ds`, allowing for complex datashape compositions to be evaluated for the desired type.",
        "signature": "def has(typ, ds):",
        "type": "Function",
        "class_signature": null
      },
      "has_ellipsis": {
        "code": "def has_ellipsis(ds):\n    \"\"\"Returns True if the given datashape contains an ellipsis dimension.\n\nParameters:\n- ds: An instance of coretypes.DataShape or a subclass thereof, representing the datashape to be checked.\n\nReturns:\n- bool: True if the datashape has an ellipsis dimension (represented by coretypes.Ellipsis); otherwise, False.\n\nThis function utilizes the helper function `has`, which recursively checks the structure of the datashape. The ellipsis is defined in the coretypes module, specifically as `coretypes.Ellipsis`, where it signifies a variable-length dimension in a datashape.\"\"\"\n    \"Returns True if the datashape has an ellipsis\\n\\n    >>> has_ellipsis(dshape('2 * int'))\\n    False\\n    >>> has_ellipsis(dshape('... * int'))\\n    True\\n    \"\n    return has(coretypes.Ellipsis, ds)",
        "docstring": "Returns True if the given datashape contains an ellipsis dimension.\n\nParameters:\n- ds: An instance of coretypes.DataShape or a subclass thereof, representing the datashape to be checked.\n\nReturns:\n- bool: True if the datashape has an ellipsis dimension (represented by coretypes.Ellipsis); otherwise, False.\n\nThis function utilizes the helper function `has`, which recursively checks the structure of the datashape. The ellipsis is defined in the coretypes module, specifically as `coretypes.Ellipsis`, where it signifies a variable-length dimension in a datashape.",
        "signature": "def has_ellipsis(ds):",
        "type": "Function",
        "class_signature": null
      }
    },
    "datashader/datashape/parser.py": {
      "parse": {
        "code": "def parse(ds_str, sym):\n    \"\"\"Parses a single datashape string into a corresponding DataShape object.\n\nParameters\n----------\nds_str : str\n    The datashape string to parse, which describes the structure of data types.\nsym : TypeSymbolTable\n    The symbol tables that map dimensions, data types, and type constructors used during parsing.\n\nReturns\n-------\nDataShape\n    An object representing the parsed datashape, reflecting the structure defined in `ds_str`.\n\nRaises\n------\nDataShapeSyntaxError\n    If the provided datashape string is invalid or contains unexpected tokens after parsing.\n\nThis function instantiates a `DataShapeParser` object, which handles the parsing logic based on the provided string and symbol tables. It uses the `parse_datashape` method of the parser to convert the input string into a DataShape object, ensuring that no extraneous tokens remain after the datashape. If parsing encounters an issue, it raises an appropriate error via the `raise_error` method.\"\"\"\n    'Parses a single datashape from a string.\\n\\n    Parameters\\n    ----------\\n    ds_str : string\\n        The datashape string to parse.\\n    sym : TypeSymbolTable\\n        The symbol tables of dimensions, dtypes, and type constructors for each.\\n\\n    '\n    dsp = DataShapeParser(ds_str, sym)\n    ds = dsp.parse_datashape()\n    if ds is None:\n        dsp.raise_error('Invalid datashape')\n    if dsp.pos != dsp.end_pos:\n        dsp.raise_error('Unexpected token in datashape')\n    return ds",
        "docstring": "Parses a single datashape string into a corresponding DataShape object.\n\nParameters\n----------\nds_str : str\n    The datashape string to parse, which describes the structure of data types.\nsym : TypeSymbolTable\n    The symbol tables that map dimensions, data types, and type constructors used during parsing.\n\nReturns\n-------\nDataShape\n    An object representing the parsed datashape, reflecting the structure defined in `ds_str`.\n\nRaises\n------\nDataShapeSyntaxError\n    If the provided datashape string is invalid or contains unexpected tokens after parsing.\n\nThis function instantiates a `DataShapeParser` object, which handles the parsing logic based on the provided string and symbol tables. It uses the `parse_datashape` method of the parser to convert the input string into a DataShape object, ensuring that no extraneous tokens remain after the datashape. If parsing encounters an issue, it raises an appropriate error via the `raise_error` method.",
        "signature": "def parse(ds_str, sym):",
        "type": "Function",
        "class_signature": null
      }
    },
    "datashader/datatypes.py": {},
    "datashader/datashape/validation.py": {
      "validate": {
        "code": "def validate(ds):\n    \"\"\"Validate a DataShape to ensure it is well-formed according to predefined rules.\n\nParameters\n----------\nds : DataShape\n    The datashape to validate, which may contain types and parameters defined in the coretypes module.\n\nReturns\n-------\nNone\n    The function does not return a value. Instead, it raises exceptions if the datashape is not valid.\n\nRaises\n------\nTypeError\n    If the datashape contains more than one wildcard (Ellipsis) or if the last parameter is an Ellipsis.\nDataShapeSyntaxError\n    If the datashape syntax is incorrect, such as missing a data type after a wildcard.\n\nNotes\n-----\nThe function interacts with the `traverse` function to apply validation recursively over the parameters of the datashape. The actual validation logic is encapsulated in the `_validate` function, which checks for conditions related to the usage and positioning of wildcards in the datashape structure.\"\"\"\n    '\\n    Validate a datashape to see whether it is well-formed.\\n\\n    Parameters\\n    ----------\\n    ds : DataShape\\n\\n    Examples\\n    --------\\n    >>> from datashader.datashape import dshape\\n    >>> dshape(\\'10 * int32\\')\\n    dshape(\"10 * int32\")\\n    >>> dshape(\\'... * int32\\')\\n    dshape(\"... * int32\")\\n    >>> dshape(\\'... * ... * int32\\') # doctest: +IGNORE_EXCEPTION_DETAIL\\n    Traceback (most recent call last):\\n        ...\\n    TypeError: Can only use a single wildcard\\n    >>> dshape(\\'T * ... * X * ... * X\\') # doctest: +IGNORE_EXCEPTION_DETAIL\\n    Traceback (most recent call last):\\n        ...\\n    TypeError: Can only use a single wildcard\\n    >>> dshape(\\'T * ...\\') # doctest: +IGNORE_EXCEPTION_DETAIL\\n    Traceback (most recent call last):\\n        ...\\n    DataShapeSyntaxError: Expected a dtype\\n    '\n    traverse(_validate, ds)",
        "docstring": "Validate a DataShape to ensure it is well-formed according to predefined rules.\n\nParameters\n----------\nds : DataShape\n    The datashape to validate, which may contain types and parameters defined in the coretypes module.\n\nReturns\n-------\nNone\n    The function does not return a value. Instead, it raises exceptions if the datashape is not valid.\n\nRaises\n------\nTypeError\n    If the datashape contains more than one wildcard (Ellipsis) or if the last parameter is an Ellipsis.\nDataShapeSyntaxError\n    If the datashape syntax is incorrect, such as missing a data type after a wildcard.\n\nNotes\n-----\nThe function interacts with the `traverse` function to apply validation recursively over the parameters of the datashape. The actual validation logic is encapsulated in the `_validate` function, which checks for conditions related to the usage and positioning of wildcards in the datashape structure.",
        "signature": "def validate(ds):",
        "type": "Function",
        "class_signature": null
      }
    }
  },
  "dependency_dict": {
    "datashader/datashape/util/__init__.py:dshape": {},
    "datashader/datashape/parser.py:parse": {
      "datashader/datashape/parser.py": {
        "DataShapeParser.__init__": {
          "code": "    def __init__(self, ds_str, sym):\n        self.ds_str = ds_str\n        self.sym = sym\n        self.lex = lexer.lex(ds_str)\n        self.tokens = []\n        self.pos = -1\n        self.end_pos = None\n        self.advance_tok()",
          "docstring": "",
          "signature": "def __init__(self, ds_str, sym):",
          "type": "Method",
          "class_signature": "class DataShapeParser:"
        },
        "DataShapeParser.parse_datashape": {
          "code": "    def parse_datashape(self):\n        \"\"\"\n        datashape : datashape_nooption\n                  | QUESTIONMARK datashape_nooption\n                  | EXCLAMATIONMARK datashape_nooption\n\n        Returns a datashape object or None.\n        \"\"\"\n        tok = self.tok\n        constructors = {lexer.QUESTIONMARK: 'option'}\n        if tok.id in constructors:\n            self.advance_tok()\n            saved_pos = self.pos\n            ds = self.parse_datashape_nooption()\n            if ds is not None:\n                option = self.syntactic_sugar(self.sym.dtype_constr, constructors[tok.id], '%s dtype construction' % constructors[tok.id], saved_pos - 1)\n                return coretypes.DataShape(option(ds))\n        else:\n            return self.parse_datashape_nooption()",
          "docstring": "datashape : datashape_nooption\n          | QUESTIONMARK datashape_nooption\n          | EXCLAMATIONMARK datashape_nooption\n\nReturns a datashape object or None.",
          "signature": "def parse_datashape(self):",
          "type": "Method",
          "class_signature": "class DataShapeParser:"
        }
      }
    },
    "datashader/datashape/validation.py:validate": {
      "datashader/datashape/validation.py": {
        "traverse": {
          "code": "def traverse(f, t):\n    \"\"\"\n    Map `f` over `t`, calling `f` with type `t` and the map result of the\n    mapping `f` over `t` 's parameters.\n\n    Parameters\n    ----------\n    f : callable\n    t : DataShape\n\n    Returns\n    -------\n    DataShape\n    \"\"\"\n    if isinstance(t, T.Mono) and (not isinstance(t, T.Unit)):\n        return f(t, [traverse(f, p) for p in t.parameters])\n    return t",
          "docstring": "Map `f` over `t`, calling `f` with type `t` and the map result of the\nmapping `f` over `t` 's parameters.\n\nParameters\n----------\nf : callable\nt : DataShape\n\nReturns\n-------\nDataShape",
          "signature": "def traverse(f, t):",
          "type": "Function",
          "class_signature": null
        }
      }
    },
    "datashader/datashape/util/__init__.py:cat_dshapes": {},
    "datashader/datashape/coretypes.py:DataShape:__getitem__": {
      "datashader/datashape/coretypes.py": {
        "Mono.parameters": {
          "code": "    def parameters(self):\n        if self._slotted:\n            return tuple((getattr(self, slot) for slot in self.__slots__))\n        else:\n            return self._parameters",
          "docstring": "",
          "signature": "def parameters(self):",
          "type": "Method",
          "class_signature": "class Mono(metaclass=Type):"
        }
      }
    },
    "datashader/datashape/coretypes.py:Fixed:__index__": {},
    "datashader/datashape/coretypes.py:Fixed:__eq__": {},
    "datashader/datashape/coretypes.py:Fixed:__init__": {},
    "datashader/datashape/coretypes.py:DataShape:__init__": {
      "datashader/datashape/coretypes.py": {
        "_launder": {
          "code": "def _launder(x):\n    \"\"\" Clean up types prior to insertion into DataShape\n\n    >>> from datashader.datashape import dshape\n    >>> _launder(5)         # convert ints to Fixed\n    Fixed(val=5)\n    >>> _launder('int32')   # parse strings\n    ctype(\"int32\")\n    >>> _launder(dshape('int32'))\n    ctype(\"int32\")\n    >>> _launder(Fixed(5))  # No-op on valid parameters\n    Fixed(val=5)\n    \"\"\"\n    if isinstance(x, int):\n        x = Fixed(x)\n    if isinstance(x, str):\n        x = datashape.dshape(x)\n    if isinstance(x, DataShape) and len(x) == 1:\n        return x[0]\n    if isinstance(x, Mono):\n        return x\n    return x",
          "docstring": "Clean up types prior to insertion into DataShape\n\n>>> from datashader.datashape import dshape\n>>> _launder(5)         # convert ints to Fixed\nFixed(val=5)\n>>> _launder('int32')   # parse strings\nctype(\"int32\")\n>>> _launder(dshape('int32'))\nctype(\"int32\")\n>>> _launder(Fixed(5))  # No-op on valid parameters\nFixed(val=5)",
          "signature": "def _launder(x):",
          "type": "Function",
          "class_signature": null
        }
      }
    },
    "datashader/datashape/coretypes.py:Mono:__eq__": {},
    "datashader/datashape/coretypes.py:Mono:Mono": {},
    "datashader/datashape/coretypes.py:DataShape:shape": {
      "datashader/datashape/coretypes.py": {
        "Mono.parameters": {
          "code": "    def parameters(self):\n        if self._slotted:\n            return tuple((getattr(self, slot) for slot in self.__slots__))\n        else:\n            return self._parameters",
          "docstring": "",
          "signature": "def parameters(self):",
          "type": "Method",
          "class_signature": "class Mono(metaclass=Type):"
        }
      }
    },
    "datashader/datashape/coretypes.py:DataShape:measure": {
      "datashader/datashape/coretypes.py": {
        "Mono.parameters": {
          "code": "    def parameters(self):\n        if self._slotted:\n            return tuple((getattr(self, slot) for slot in self.__slots__))\n        else:\n            return self._parameters",
          "docstring": "",
          "signature": "def parameters(self):",
          "type": "Method",
          "class_signature": "class Mono(metaclass=Type):"
        }
      }
    },
    "datashader/datashape/coretypes.py:Mono:info": {
      "datashader/datashape/coretypes.py": {
        "Mono.parameters": {
          "code": "    def parameters(self):\n        if self._slotted:\n            return tuple((getattr(self, slot) for slot in self.__slots__))\n        else:\n            return self._parameters",
          "docstring": "",
          "signature": "def parameters(self):",
          "type": "Method",
          "class_signature": "class Mono(metaclass=Type):"
        }
      }
    },
    "datashader/datashape/coretypes.py:Mono:__repr__": {
      "datashader/datashape/coretypes.py": {
        "Mono._slotted": {
          "code": "    def _slotted(self):\n        return hasattr(self, '__slots__')",
          "docstring": "",
          "signature": "def _slotted(self):",
          "type": "Method",
          "class_signature": "class Mono(metaclass=Type):"
        }
      }
    },
    "datashader/datashape/coretypes.py:CType:__repr__": {
      "datashader/datashape/coretypes.py": {
        "CType.__str__": {
          "code": "    def __str__(self):\n        return self.name",
          "docstring": "",
          "signature": "def __str__(self):",
          "type": "Method",
          "class_signature": "class CType(Unit):"
        }
      }
    },
    "datashader/datashape/util/__init__.py:has_var_dim": {},
    "datashader/datashape/util/__init__.py:has": {
      "datashader/datashape/coretypes.py": {
        "Mono.parameters": {
          "code": "    def parameters(self):\n        if self._slotted:\n            return tuple((getattr(self, slot) for slot in self.__slots__))\n        else:\n            return self._parameters",
          "docstring": "",
          "signature": "def parameters(self):",
          "type": "Method",
          "class_signature": "class Mono(metaclass=Type):"
        }
      }
    },
    "datashader/datashape/util/__init__.py:has_ellipsis": {}
  },
  "call_tree": {
    "datashader/datashape/tests/test_util.py:test_cat_dshapes": {
      "datashader/datashape/util/__init__.py:dshape": {
        "datashader/datashape/parser.py:parse": {
          "datashader/datashape/parser.py:DataShapeParser:__init__": {
            "datashader/datashape/parser.py:DataShapeParser:advance_tok": {
              "datashader/datashape/lexer.py:lex": {}
            }
          },
          "datashader/datashape/parser.py:DataShapeParser:parse_datashape": {
            "datashader/datashape/parser.py:DataShapeParser:tok": {},
            "datashader/datashape/parser.py:DataShapeParser:parse_datashape_nooption": {
              "datashader/datashape/parser.py:DataShapeParser:parse_dim": {
                "datashader/datashape/parser.py:DataShapeParser:tok": {},
                "datashader/datashape/parser.py:DataShapeParser:advance_tok": {
                  "datashader/datashape/lexer.py:lex": {}
                },
                "datashader/datashape/parser.py:DataShapeParser:syntactic_sugar": {},
                "datashader/datashape/coretypes.py:Fixed:__init__": {}
              },
              "datashader/datashape/parser.py:DataShapeParser:tok": {},
              "datashader/datashape/parser.py:DataShapeParser:advance_tok": {
                "datashader/datashape/lexer.py:lex": {}
              },
              "datashader/datashape/parser.py:DataShapeParser:parse_datashape": {
                "[ignored_or_cut_off]": "..."
              },
              "datashader/datashape/coretypes.py:Mono:parameters": {
                "datashader/datashape/coretypes.py:Mono:_slotted": {}
              },
              "datashader/datashape/coretypes.py:DataShape:__init__": {
                "datashader/datashape/coretypes.py:_launder": {}
              }
            }
          }
        },
        "datashader/datashape/validation.py:validate": {
          "datashader/datashape/validation.py:traverse": {
            "datashader/datashape/coretypes.py:Mono:parameters": {
              "datashader/datashape/coretypes.py:Mono:_slotted": {}
            },
            "datashader/datashape/validation.py:traverse": {
              "[ignored_or_cut_off]": "..."
            },
            "datashader/datashape/validation.py:_validate": {
              "datashader/datashape/coretypes.py:Mono:parameters": {
                "datashader/datashape/coretypes.py:Mono:_slotted": {}
              }
            }
          }
        }
      },
      "datashader/datashape/util/__init__.py:cat_dshapes": {
        "datashader/datashape/coretypes.py:DataShape:__getitem__": {
          "datashader/datashape/coretypes.py:Mono:parameters": {
            "datashader/datashape/coretypes.py:Mono:_slotted": {}
          }
        },
        "datashader/datashape/coretypes.py:Fixed:__index__": {},
        "datashader/datashape/coretypes.py:Fixed:__eq__": {},
        "datashader/datashape/coretypes.py:Fixed:__init__": {},
        "datashader/datashape/coretypes.py:DataShape:__init__": {
          "datashader/datashape/coretypes.py:_launder": {}
        }
      },
      "datashader/datashape/coretypes.py:Mono:__eq__": {
        "datashader/datashape/coretypes.py:Mono:Mono": {},
        "datashader/datashape/coretypes.py:DataShape:shape": {
          "datashader/datashape/coretypes.py:Mono:parameters": {
            "datashader/datashape/coretypes.py:Mono:_slotted": {}
          }
        },
        "datashader/datashape/coretypes.py:DataShape:measure": {
          "datashader/datashape/coretypes.py:Mono:parameters": {
            "datashader/datashape/coretypes.py:Mono:_slotted": {}
          }
        },
        "datashader/datashape/coretypes.py:Mono:info": {
          "datashader/datashape/coretypes.py:Mono:parameters": {
            "datashader/datashape/coretypes.py:Mono:_slotted": {}
          }
        },
        "datashader/datashape/coretypes.py:Fixed:__eq__": {}
      }
    },
    "datashader/datashape/tests/test_util.py:test_cat_dshapes_errors": {
      "datashader/datashape/util/__init__.py:cat_dshapes": {
        "datashader/datashape/coretypes.py:DataShape:__getitem__": {
          "datashader/datashape/coretypes.py:Mono:parameters": {
            "datashader/datashape/coretypes.py:Mono:_slotted": {}
          }
        },
        "datashader/datashape/coretypes.py:Fixed:__index__": {},
        "datashader/datashape/coretypes.py:Fixed:__eq__": {},
        "datashader/datashape/coretypes.py:Mono:__repr__": {
          "datashader/datashape/coretypes.py:Mono:_slotted": {}
        },
        "datashader/datashape/coretypes.py:CType:__repr__": {
          "datashader/datashape/coretypes.py:CType:__str__": {}
        }
      },
      "datashader/datashape/util/__init__.py:dshape": {
        "datashader/datashape/parser.py:parse": {
          "datashader/datashape/parser.py:DataShapeParser:__init__": {
            "datashader/datashape/parser.py:DataShapeParser:advance_tok": {
              "datashader/datashape/lexer.py:lex": {}
            }
          },
          "datashader/datashape/parser.py:DataShapeParser:parse_datashape": {
            "datashader/datashape/parser.py:DataShapeParser:tok": {},
            "datashader/datashape/parser.py:DataShapeParser:parse_datashape_nooption": {
              "datashader/datashape/parser.py:DataShapeParser:parse_dim": {
                "datashader/datashape/parser.py:DataShapeParser:tok": {},
                "datashader/datashape/parser.py:DataShapeParser:advance_tok": {
                  "datashader/datashape/lexer.py:lex": {}
                },
                "datashader/datashape/parser.py:DataShapeParser:syntactic_sugar": {},
                "datashader/datashape/coretypes.py:Fixed:__init__": {}
              },
              "datashader/datashape/parser.py:DataShapeParser:tok": {},
              "datashader/datashape/parser.py:DataShapeParser:advance_tok": {
                "datashader/datashape/lexer.py:lex": {}
              },
              "datashader/datashape/parser.py:DataShapeParser:parse_datashape": {
                "[ignored_or_cut_off]": "..."
              },
              "datashader/datashape/coretypes.py:Mono:parameters": {
                "datashader/datashape/coretypes.py:Mono:_slotted": {}
              },
              "datashader/datashape/coretypes.py:DataShape:__init__": {
                "datashader/datashape/coretypes.py:_launder": {}
              }
            }
          }
        },
        "datashader/datashape/validation.py:validate": {
          "datashader/datashape/validation.py:traverse": {
            "datashader/datashape/coretypes.py:Mono:parameters": {
              "datashader/datashape/coretypes.py:Mono:_slotted": {}
            },
            "datashader/datashape/validation.py:traverse": {
              "[ignored_or_cut_off]": "..."
            },
            "datashader/datashape/validation.py:_validate": {
              "datashader/datashape/coretypes.py:Mono:parameters": {
                "datashader/datashape/coretypes.py:Mono:_slotted": {}
              }
            }
          }
        }
      }
    },
    "datashader/datashape/tests/test_util.py:test_has_var_dim": {
      "datashader/datashape/util/__init__.py:dshape": {
        "datashader/datashape/parser.py:parse": {
          "datashader/datashape/parser.py:DataShapeParser:__init__": {
            "datashader/datashape/parser.py:DataShapeParser:advance_tok": {
              "datashader/datashape/lexer.py:lex": {}
            }
          },
          "datashader/datashape/parser.py:DataShapeParser:parse_datashape": {
            "datashader/datashape/parser.py:DataShapeParser:tok": {},
            "datashader/datashape/parser.py:DataShapeParser:parse_datashape_nooption": {
              "datashader/datashape/parser.py:DataShapeParser:parse_dim": {
                "datashader/datashape/parser.py:DataShapeParser:tok": {},
                "datashader/datashape/parser.py:DataShapeParser:advance_tok": {
                  "datashader/datashape/lexer.py:lex": {}
                },
                "datashader/datashape/parser.py:DataShapeParser:syntactic_sugar": {},
                "datashader/datashape/type_symbol_table.py:_ellipsis": {
                  "datashader/datashape/coretypes.py:TypeVar:__init__": {},
                  "datashader/datashape/coretypes.py:Ellipsis:__init__": {}
                },
                "datashader/datashape/coretypes.py:Fixed:__init__": {}
              },
              "datashader/datashape/parser.py:DataShapeParser:tok": {},
              "datashader/datashape/parser.py:DataShapeParser:advance_tok": {
                "datashader/datashape/lexer.py:lex": {}
              },
              "datashader/datashape/parser.py:DataShapeParser:parse_datashape": {
                "[ignored_or_cut_off]": "..."
              },
              "datashader/datashape/coretypes.py:Mono:parameters": {
                "datashader/datashape/coretypes.py:Mono:_slotted": {}
              },
              "datashader/datashape/coretypes.py:DataShape:__init__": {
                "datashader/datashape/coretypes.py:_launder": {}
              },
              "datashader/datashape/parser.py:DataShapeParser:parse_dtype": {
                "datashader/datashape/parser.py:DataShapeParser:tok": {},
                "datashader/datashape/parser.py:DataShapeParser:parse_struct_type": {
                  "datashader/datashape/parser.py:DataShapeParser:tok": {},
                  "datashader/datashape/parser.py:DataShapeParser:advance_tok": {
                    "datashader/datashape/lexer.py:lex": {}
                  },
                  "datashader/datashape/parser.py:DataShapeParser:parse_homogeneous_list": {
                    "datashader/datashape/parser.py:DataShapeParser:parse_struct_field": {
                      "datashader/datashape/parser.py:DataShapeParser:tok": {},
                      "datashader/datashape/parser.py:DataShapeParser:advance_tok": {},
                      "datashader/datashape/parser.py:DataShapeParser:parse_datashape": {
                        "[ignored_or_cut_off]": "..."
                      }
                    },
                    "datashader/datashape/parser.py:DataShapeParser:tok": {},
                    "datashader/datashape/parser.py:DataShapeParser:advance_tok": {
                      "datashader/datashape/lexer.py:lex": {}
                    }
                  },
                  "datashader/datashape/parser.py:DataShapeParser:syntactic_sugar": {},
                  "datashader/datashape/type_symbol_table.py:_struct": {
                    "datashader/datashape/coretypes.py:Record:__init__": {
                      "datashader/datashape/coretypes.py:_launder": {}
                    }
                  }
                }
              },
              "datashader/datashape/coretypes.py:Mono:__len__": {}
            }
          }
        },
        "datashader/datashape/validation.py:validate": {
          "datashader/datashape/validation.py:traverse": {
            "datashader/datashape/coretypes.py:Mono:parameters": {
              "datashader/datashape/coretypes.py:Mono:_slotted": {}
            },
            "datashader/datashape/validation.py:traverse": {
              "[ignored_or_cut_off]": "..."
            },
            "datashader/datashape/validation.py:_validate": {
              "datashader/datashape/coretypes.py:Mono:parameters": {
                "datashader/datashape/coretypes.py:Mono:_slotted": {}
              }
            }
          }
        },
        "datashader/datashape/coretypes.py:DataShape:__init__": {
          "datashader/datashape/coretypes.py:_launder": {
            "datashader/datashape/coretypes.py:DataShape:__len__": {
              "datashader/datashape/coretypes.py:Mono:parameters": {
                "datashader/datashape/coretypes.py:Mono:_slotted": {}
              }
            }
          }
        }
      },
      "datashader/datashape/util/__init__.py:has_var_dim": {
        "datashader/datashape/util/__init__.py:has": {
          "datashader/datashape/coretypes.py:Mono:parameters": {
            "datashader/datashape/coretypes.py:Mono:_slotted": {}
          },
          "datashader/datashape/util/__init__.py:has": {
            "[ignored_or_cut_off]": "..."
          }
        }
      }
    },
    "datashader/datashape/tests/test_util.py:test_not_has_var_dim": {
      "datashader/datashape/util/__init__.py:has_var_dim": {
        "datashader/datashape/util/__init__.py:has": {
          "datashader/datashape/coretypes.py:Mono:parameters": {
            "datashader/datashape/coretypes.py:Mono:_slotted": {}
          },
          "datashader/datashape/util/__init__.py:has": {
            "[ignored_or_cut_off]": "..."
          }
        }
      }
    },
    "datashader/datashape/tests/test_util.py:test_has_ellipsis": {
      "datashader/datashape/util/__init__.py:has_ellipsis": {
        "datashader/datashape/util/__init__.py:has": {
          "datashader/datashape/coretypes.py:Mono:parameters": {
            "datashader/datashape/coretypes.py:Mono:_slotted": {}
          },
          "datashader/datashape/util/__init__.py:has": {
            "[ignored_or_cut_off]": "..."
          }
        }
      }
    },
    "datashader/datashape/tests/test_util.py:test_not_has_ellipsis": {
      "datashader/datashape/util/__init__.py:has_ellipsis": {
        "datashader/datashape/util/__init__.py:has": {
          "datashader/datashape/coretypes.py:Mono:parameters": {
            "datashader/datashape/coretypes.py:Mono:_slotted": {}
          },
          "datashader/datashape/util/__init__.py:has": {
            "[ignored_or_cut_off]": "..."
          }
        }
      }
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_util/datashader-test_util/datashader/tests/test_pandas.py:test_line_manual_range": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_util/datashader-test_util/datashader/tests/test_pandas.py:test_line_autorange": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_util/datashader-test_util/datashader/tests/test_pandas.py:test_area_to_zero_fixedrange": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_util/datashader-test_util/datashader/tests/test_pandas.py:test_area_to_zero_autorange": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_util/datashader-test_util/datashader/tests/test_pandas.py:test_area_to_zero_autorange_gap": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_util/datashader-test_util/datashader/tests/test_pandas.py:test_area_to_line_autorange": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_util/datashader-test_util/datashader/datashape/tests/test_coretypes.py:test_record_parse_optional": {
      "datashader/datashape/coretypes.py:Option:Option": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_util/datashader-test_util/modified_testcases/test_pandas.py:test_line_manual_range": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_util/datashader-test_util/modified_testcases/test_pandas.py:test_line_autorange": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_util/datashader-test_util/modified_testcases/test_pandas.py:test_area_to_zero_fixedrange": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_util/datashader-test_util/modified_testcases/test_pandas.py:test_area_to_zero_autorange": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_util/datashader-test_util/modified_testcases/test_pandas.py:test_area_to_zero_autorange_gap": {
      "datashader/datatypes.py:RaggedDtype": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/datashader-image-test_util/datashader-test_util/modified_testcases/test_pandas.py:test_area_to_line_autorange": {
      "datashader/datatypes.py:RaggedDtype": {}
    }
  },
  "PRD": "# PROJECT NAME: datashader-test_util\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 datashader/\n    \u251c\u2500\u2500 datashape/\n    \u2502   \u251c\u2500\u2500 coretypes.py\n    \u2502   \u2502   \u251c\u2500\u2500 CType.__repr__\n    \u2502   \u2502   \u251c\u2500\u2500 DataShape.__getitem__\n    \u2502   \u2502   \u251c\u2500\u2500 DataShape.__init__\n    \u2502   \u2502   \u251c\u2500\u2500 DataShape.measure\n    \u2502   \u2502   \u251c\u2500\u2500 DataShape.shape\n    \u2502   \u2502   \u251c\u2500\u2500 Fixed.__eq__\n    \u2502   \u2502   \u251c\u2500\u2500 Fixed.__index__\n    \u2502   \u2502   \u251c\u2500\u2500 Fixed.__init__\n    \u2502   \u2502   \u251c\u2500\u2500 Mono.Mono\n    \u2502   \u2502   \u251c\u2500\u2500 Mono.__eq__\n    \u2502   \u2502   \u251c\u2500\u2500 Mono.__repr__\n    \u2502   \u2502   \u251c\u2500\u2500 Mono.info\n    \u2502   \u2502   \u2514\u2500\u2500 Option.Option\n    \u2502   \u251c\u2500\u2500 parser.py\n    \u2502   \u2502   \u2514\u2500\u2500 parse\n    \u2502   \u251c\u2500\u2500 util/\n    \u2502   \u2502   \u2514\u2500\u2500 __init__.py\n    \u2502   \u2502       \u251c\u2500\u2500 cat_dshapes\n    \u2502   \u2502       \u251c\u2500\u2500 dshape\n    \u2502   \u2502       \u251c\u2500\u2500 has\n    \u2502   \u2502       \u251c\u2500\u2500 has_ellipsis\n    \u2502   \u2502       \u2514\u2500\u2500 has_var_dim\n    \u2502   \u2514\u2500\u2500 validation.py\n    \u2502       \u2514\u2500\u2500 validate\n    \u2514\u2500\u2500 datatypes.py\n        \u2514\u2500\u2500 RaggedDtype\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThis module is designed to validate and manipulate Datashape data constructs, which are used to describe the structure and layout of multidimensional data. Its primary purpose is to ensure compatibility and correctness of data shapes in scenarios involving concatenation, variable dimensions, and ellipses, which are essential for handling complex datasets. The module provides capabilities to concatenate compatible data shapes, detect the presence of variable dimensions or ellipses within a data shape, and identify errors in cases where data shape constraints are violated. By ensuring accurate and reliable data shape processing, the module simplifies data schema validation for developers and enhances robustness in data-intensive applications.\n\n## FILE 1: datashader/datashape/coretypes.py\n\n- CLASS METHOD: Mono.__repr__\n  - CLASS SIGNATURE: class Mono(metaclass=Type):\n  - SIGNATURE: def __repr__(self):\n  - DOCSTRING: \n```python\n\"\"\"\nReturns a string representation of the Mono instance, formatted to include the class name and either its slot attributes or parameters.\n\nThe method constructs a string that identifies the class type and details its attributes. If the instance has attributes defined in `__slots__`, it will return these in the form of keyword-value pairs. Otherwise, it will map the parameters of the instance to their string representations.\n\nReturns:\n    str: A string representing the Mono instance including class type and its attributes or parameters.\n\nDependencies:\n- `self.__slots__`: A tuple of attribute names if the class uses slots, which allows for more memory-efficient object representation.\n\nSide Effects:\nNone. The method purely generates a string representation of the instance without modifying any attributes.\n\"\"\"\n```\n\n- CLASS METHOD: Fixed.__eq__\n  - CLASS SIGNATURE: class Fixed(Unit):\n  - SIGNATURE: def __eq__(self, other):\n  - DOCSTRING: \n```python\n\"\"\"\nCompares two Fixed instances for equality.\n\nParameters\n----------\nother : object\n    The object to compare with the current Fixed instance. \n\nReturns\n-------\nbool\n    Returns True if `other` is either another Fixed instance with the same value or an integer that matches the Fixed value. Otherwise, returns False.\n\nNotes\n-----\nThis method allows for the comparison of Fixed objects to both their own type and standard integers, providing flexibility in equality checks when dealing with fixed dimensions in data shapes. The 'val' attribute, representing the fixed dimension size, is compared against the integer input to determine equality.\n\"\"\"\n```\n\n- CLASS METHOD: Fixed.__init__\n  - CLASS SIGNATURE: class Fixed(Unit):\n  - SIGNATURE: def __init__(self, i):\n  - DOCSTRING: \n```python\n\"\"\"\nInitializes a Fixed dimension.\n\nParameters\n----------\ni : int\n    A non-negative integer representing the fixed dimension size. This can be a Python integer or a NumPy integer scalar, as the method utilizes `operator.index()` for compatibility.\n\nRaises\n------\nValueError\n    If the provided dimension size is negative, indicating that Fixed dimensions must be positive.\n\nAttributes\n----------\nval : int\n    Stores the fixed dimension size, ensuring that downstream processes utilize this value correctly to define the dimensions of data shapes.\n\nThis method interacts with the Fixed class's __index__ method, enabling instances of Fixed to be treated like native integers in contexts that require integer values. It relies on the operator module for converting various integer types to a standard Python integer format.\n\"\"\"\n```\n\n- CLASS METHOD: DataShape.__getitem__\n  - CLASS SIGNATURE: class DataShape(Mono):\n  - SIGNATURE: def __getitem__(self, index):\n  - DOCSTRING: \n```python\n\"\"\"\nRetrieve a specific parameter from the `DataShape` instance.\n\nParameters\n----------\nindex : int\n    The index of the parameter to retrieve from the `parameters` tuple of the `DataShape` instance.\n\nReturns\n-------\nThe parameter at the specified index, which can be of various types depending on the structure of the `DataShape`.\n\nNotes\n-----\nThis method allows access to the individual components of the `DataShape`, such as dimensions and measures, which are stored in the `self.parameters` attribute. The `parameters` attribute is a tuple containing the elements of the data shape, and is initialized when the `DataShape` is constructed.\n\"\"\"\n```\n\n- CLASS METHOD: Mono.__eq__\n  - CLASS SIGNATURE: class Mono(metaclass=Type):\n  - SIGNATURE: def __eq__(self, other):\n  - DOCSTRING: \n```python\n\"\"\"\nCheck for equality between two Mono instances.\n\nParameters\n----------\nother : Any\n    The object to compare with the current instance. It is expected to be a Mono or a subclass of Mono.\n\nReturns\n-------\nbool\n    True if the other object is an instance of Mono and both the shape and measure values are equal; False otherwise.\n\nNotes\n-----\n- The equality check relies on the `shape` property, which returns the dimensions of the Mono instance, and the `measure` property, which provides the associated measure.\n- The `measure.info()` method is called to compare the internal details of the measurement types between the two instances.\n\"\"\"\n```\n\n- CLASS METHOD: CType.__repr__\n  - CLASS SIGNATURE: class CType(Unit):\n  - SIGNATURE: def __repr__(self):\n  - DOCSTRING: \n```python\n\"\"\"\nGenerates a string representation of the CType instance.\n\nThis method converts the internal name of the CType into a format suitable for display. It encodes the string to handle any special Unicode characters, ensuring the output is correctly formatted for various character encodings. The result is a string in the format 'ctype(\"name\")', where 'name' is the string representation of the CType instance.\n\nReturns\n-------\nstr\n    A formatted string representing the CType instance, containing its name encoded for ASCII compatibility.\n\nDependencies\n------------\nThis method relies on the __str__ method to retrieve the name of the CType instance. It also uses the `encode` and `decode` methods to ensure proper Unicode handling.\n\"\"\"\n```\n\n- CLASS METHOD: DataShape.__init__\n  - CLASS SIGNATURE: class DataShape(Mono):\n  - SIGNATURE: def __init__(self, *parameters, **kwds):\n  - DOCSTRING: \n```python\n\"\"\"\nInitialize a DataShape object, which serves as a composite container for \ndatashape elements, such as dimensions and measures.\n\nParameters\n----------\n*parameters : Variable length argument list\n    Should contain at least two items: dimensions (instances of Fixed or Var) \n    followed by a measure (an instance of Unit or subclasses thereof). \n\n**kwds : Additional keyword arguments\n    - name (str, optional): If provided, registers the DataShape under this name \n    in the internal type registry.\n\nRaises\n------\nTypeError : If only one parameter is provided or if the last parameter is not a measure, \n    or if any of the preceding parameters are not dimensions.\nValueError : If no parameters are supplied or if only one is given.\n\nAttributes\n----------\n- _parameters : Tuple\n    A collection of cleaned parameters, processed by the _launder function to ensure \n    they are valid datashape types.\n- composite : bool\n    Set to True to indicate that this DataShape is a composite type.\n- name : str or None\n    Optional name under which this DataShape is registered in the type registry.\n\nConstants Used\n--------------\n- MEASURE : Indicates that a type is a measure (defined in the module, used to validate \n  the last parameter).\n- DIMENSION : Indicates that a type is a dimension (defined in the module, used to validate \n  preceding parameters).\n\nThis method relies on the _launder function to clean parameters, ensuring they conform \nto expected types before constructing the DataShape.\n\"\"\"\n```\n\n- CLASS METHOD: DataShape.shape\n  - CLASS SIGNATURE: class DataShape(Mono):\n  - SIGNATURE: def shape(self):\n  - DOCSTRING: \n```python\n\"\"\"\nReturn the shape of the DataShape instance.\n\nThis method retrieves all but the last parameter of the DataShape instance, which represents a collection of dimensions. The last parameter typically denotes the measure (i.e., the data type). This method allows access to the shape of multi-dimensional data structures encapsulated by the DataShape, making it easier to work with data dimensions in a consistent manner.\n\nReturns:\n    tuple: A tuple containing all dimension parameters of the DataShape, excluding the measure. Each dimension is represented by an instance of Fixed or Var classes.\n\nDependencies:\n    This method relies on the `parameters` attribute, which is a tuple holding the components of the DataShape. The last item in this tuple is assumed to be the measure or data type, which is not included in the shape returned by this method.\n\"\"\"\n```\n\n- CLASS METHOD: Mono.info\n  - CLASS SIGNATURE: class Mono(metaclass=Type):\n  - SIGNATURE: def info(self):\n  - DOCSTRING: \n```python\n\"\"\"\nRetrieve information about the Mono type.\n\nThis method returns a tuple containing the type of the current Mono instance \nand its parameters. The parameters are provided during the initialization of \nthe Mono class and represent the characteristics or defining properties of \nthe type.\n\nReturns\n-------\ntuple\n    A tuple where the first element is the type of the current instance \n    (inherited from the built-in type), and the second element is the \n    parameters associated with the instance, stored as an attribute \n    (self.parameters).\n\nNotes\n-----\nThis method is primarily used for introspection and debugging purposes to \ngather type and parameter information about an instance of the Mono class \nand its subclasses.\n\"\"\"\n```\n\n- CLASS METHOD: Fixed.__index__\n  - CLASS SIGNATURE: class Fixed(Unit):\n  - SIGNATURE: def __index__(self):\n  - DOCSTRING: \n```python\n\"\"\"\nReturns the integer representation of the Fixed dimension.\n\nThis method allows instances of the Fixed class to be used in contexts that require an integer index, such as slicing operations. It retrieves the value stored in the `val` attribute, which represents the fixed size dimension initialized during instance creation.\n\nReturns:\n    int: The integer value of the fixed dimension.\n\nAttributes:\n    val (int): The fixed size dimension, which can be set to a positive integer during initialization of a Fixed instance.\n\"\"\"\n```\n\n- CLASS METHOD: DataShape.measure\n  - CLASS SIGNATURE: class DataShape(Mono):\n  - SIGNATURE: def measure(self):\n  - DOCSTRING: \n```python\n\"\"\"\nReturn the measure of the DataShape.\n\nThe measure corresponds to the last element in the parameters of the DataShape instance. This attribute is crucial as it defines the data type of the values contained within the DataShape, such as int32, float64, or user-defined types. The measure is typically a type that follows the conventions established in the datashape module.\n\nReturns:\n    Mono: The measure type of the DataShape, which is the last element in the parameters tuple.\n\nDependencies:\n    This method relies on the `parameters` attribute of the DataShape class, which is populated during initialization and contains the datatypes (measures) used throughout the structure.\n\"\"\"\n```\n\n## FILE 2: datashader/datashape/util/__init__.py\n\n- FUNCTION NAME: has_ellipsis\n  - SIGNATURE: def has_ellipsis(ds):\n  - DOCSTRING: \n```python\n\"\"\"\nReturns True if the given datashape contains an ellipsis dimension.\n\nParameters:\n- ds: An instance of coretypes.DataShape or a subclass thereof, representing the datashape to be checked.\n\nReturns:\n- bool: True if the datashape has an ellipsis dimension (represented by coretypes.Ellipsis); otherwise, False.\n\nThis function utilizes the helper function `has`, which recursively checks the structure of the datashape. The ellipsis is defined in the coretypes module, specifically as `coretypes.Ellipsis`, where it signifies a variable-length dimension in a datashape.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - datashader/datashape/util/__init__.py:has\n\n- FUNCTION NAME: has\n  - SIGNATURE: def has(typ, ds):\n  - DOCSTRING: \n```python\n\"\"\"\nChecks if a specified type is present within a given datashape or any of its nested components.\n\nParameters:\n- typ: A type (class) to search for within the datashape `ds`.\n- ds: A datashape object, which could be a primitive type, a `coretypes.Record`, a `coretypes.Mono`, or a collection (list or tuple) of datashapes.\n\nReturns:\n- bool: True if `typ` is found in `ds` or any of its subtypes; otherwise, False.\n\nThe function interacts with other parts of the code through the `coretypes` module, which defines various data types used within datashapes. It recursively checks for the presence of `typ` in the structure of `ds`, allowing for complex datashape compositions to be evaluated for the desired type.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - datashader/datashape/coretypes.py:Mono:parameters\n    - datashader/datashape/util/__init__.py:has_ellipsis\n    - datashader/datashape/util/__init__.py:has_var_dim\n    - datashader/datashape/util/__init__.py:has\n\n- FUNCTION NAME: cat_dshapes\n  - SIGNATURE: def cat_dshapes(dslist):\n  - DOCSTRING: \n```python\n\"\"\"\nConcatenates a list of data shapes (dshapes) along the first axis. The function raises a ValueError if the list is empty, or if there is a mismatch in dimensions other than the first, or if the measures differ. It requires that the first dimension (the leading dimension) of all data shapes is a fixed known size.\n\nParameters:\n- dslist: A list of `coretypes.DataShape` objects to be concatenated. Each dshape should share the same inner shapes after the first dimension.\n\nReturns:\n- A new `coretypes.DataShape` representing the concatenated shape with the first dimension size equal to the sum of the first dimensions of all input dshapes.\n\nRaises:\n- ValueError if `dslist` is empty, or if the inner shapes (after the first dimension) do not match across the provided dshapes.\n\nDependencies:\n- Relies on `coretypes` for data shape representations and ensures that validation of dimensions follows the rules defined there.\n- Utilizes `operator.index` to safely convert dimension sizes to integers.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - datashader/datashape/coretypes.py:Mono:__repr__\n    - datashader/datashape/coretypes.py:Fixed:__init__\n    - datashader/datashape/coretypes.py:Fixed:__eq__\n    - datashader/datashape/coretypes.py:DataShape:__getitem__\n    - datashader/datashape/coretypes.py:Fixed:__index__\n    - datashader/datashape/coretypes.py:CType:__repr__\n    - datashader/datashape/coretypes.py:DataShape:__init__\n\n- FUNCTION NAME: has_var_dim\n  - SIGNATURE: def has_var_dim(ds):\n  - DOCSTRING: \n```python\n\"\"\"\nReturns True if the given datashape (ds) contains a variable dimension. A variable dimension is defined by the presence of either `coretypes.Ellipsis` (representing an unspecified number of dimensions) or `coretypes.Var` (indicating a variable size). Note that this function currently treats variable-length strings as scalars. \n\nParameters:\n- ds (DataShape): The datashape to be evaluated.\n\nReturns:\n- bool: True if ds has a variable dimension; otherwise, False.\n\nDependencies:\n- Utilizes the `has` function to check for the presence of variable dimensions within the datashape, which is defined elsewhere in the code.\n- References `coretypes.Ellipsis` and `coretypes.Var`, which are defined in the `coretypes` module, indicating specific types of variable dimensions within datashapes.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - datashader/datashape/util/__init__.py:has\n\n- FUNCTION NAME: dshape\n  - SIGNATURE: def dshape(o):\n  - DOCSTRING: \n```python\n\"\"\"\nParse a datashape from various input formats.\n\nParameters:\n- o: Can be a string representing a datashape, an instance of coretypes.DataShape, or other types such as CType, String, Record, JSON, Date, Time, DateTime, Unit, or Mono. It can also be a list or tuple of dimensions.\n\nReturns:\n- An instance of coretypes.DataShape that represents the parsed datashape.\n\nRaises:\n- TypeError: If the input cannot be converted into a datashape.\n- Validation error: If the resulting datashape does not pass the validation checks performed by the `validate` function.\n\nDependencies:\n- Relies on the 'parser' module for parsing strings into datashapes, as well as the 'validate' function from the validation module to ensure the correctness of the datashape.\n- Interacts with various classes from the coretypes module to construct appropriate datashape representations.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - datashader/datashape/validation.py:validate\n    - datashader/datashape/parser.py:parse\n    - datashader/datashape/coretypes.py:DataShape:__init__\n\n## FILE 3: datashader/datashape/parser.py\n\n- FUNCTION NAME: parse\n  - SIGNATURE: def parse(ds_str, sym):\n  - DOCSTRING: \n```python\n\"\"\"\nParses a single datashape string into a corresponding DataShape object.\n\nParameters\n----------\nds_str : str\n    The datashape string to parse, which describes the structure of data types.\nsym : TypeSymbolTable\n    The symbol tables that map dimensions, data types, and type constructors used during parsing.\n\nReturns\n-------\nDataShape\n    An object representing the parsed datashape, reflecting the structure defined in `ds_str`.\n\nRaises\n------\nDataShapeSyntaxError\n    If the provided datashape string is invalid or contains unexpected tokens after parsing.\n\nThis function instantiates a `DataShapeParser` object, which handles the parsing logic based on the provided string and symbol tables. It uses the `parse_datashape` method of the parser to convert the input string into a DataShape object, ensuring that no extraneous tokens remain after the datashape. If parsing encounters an issue, it raises an appropriate error via the `raise_error` method.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - datashader/datashape/util/__init__.py:dshape\n    - datashader/datashape/parser.py:DataShapeParser:__init__\n    - datashader/datashape/parser.py:DataShapeParser:parse_datashape\n\n## FILE 4: datashader/datatypes.py\n\n## FILE 5: datashader/datashape/validation.py\n\n- FUNCTION NAME: validate\n  - SIGNATURE: def validate(ds):\n  - DOCSTRING: \n```python\n\"\"\"\nValidate a DataShape to ensure it is well-formed according to predefined rules.\n\nParameters\n----------\nds : DataShape\n    The datashape to validate, which may contain types and parameters defined in the coretypes module.\n\nReturns\n-------\nNone\n    The function does not return a value. Instead, it raises exceptions if the datashape is not valid.\n\nRaises\n------\nTypeError\n    If the datashape contains more than one wildcard (Ellipsis) or if the last parameter is an Ellipsis.\nDataShapeSyntaxError\n    If the datashape syntax is incorrect, such as missing a data type after a wildcard.\n\nNotes\n-----\nThe function interacts with the `traverse` function to apply validation recursively over the parameters of the datashape. The actual validation logic is encapsulated in the `_validate` function, which checks for conditions related to the usage and positioning of wildcards in the datashape structure.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - datashader/datashape/validation.py:traverse\n    - datashader/datashape/util/__init__.py:dshape\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "datashader/datashape/coretypes.py": "\"\"\"\nThis defines the DataShape type system, with unified\nshape and data type.\n\"\"\"\nimport ctypes\nimport operator\nfrom collections import OrderedDict\nfrom math import ceil\nfrom datashader import datashape\nimport numpy as np\nfrom .internal_utils import IndexCallable, isidentifier\nDIMENSION = 1\nMEASURE = 2\n\nclass Type(type):\n    _registry = {}\n\n    def __new__(meta, name, bases, dct):\n        cls = super(Type, meta).__new__(meta, name, bases, dct)\n        if not dct.get('abstract'):\n            Type._registry[name] = cls\n        return cls\n\n    @classmethod\n    def register(cls, name, type):\n        if name in cls._registry:\n            raise TypeError('There is another type registered with name %s' % name)\n        cls._registry[name] = type\n\n    @classmethod\n    def lookup_type(cls, name):\n        return cls._registry[name]\n\nclass Mono(metaclass=Type):\n    \"\"\"\n    Monotype are unqualified 0 parameters.\n\n    Each type must be reconstructable using its parameters:\n\n        type(datashape_type)(*type.parameters)\n    \"\"\"\n    composite = False\n\n    def __init__(self, *params):\n        self._parameters = params\n\n    @property\n    def _slotted(self):\n        return hasattr(self, '__slots__')\n\n    @property\n    def parameters(self):\n        if self._slotted:\n            return tuple((getattr(self, slot) for slot in self.__slots__))\n        else:\n            return self._parameters\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __hash__(self):\n        try:\n            h = self._hash\n        except AttributeError:\n            h = self._hash = hash(self.shape) ^ hash(self.measure.info())\n        return h\n\n    @property\n    def shape(self):\n        return ()\n\n    def __len__(self):\n        return 1\n\n    def __getitem__(self, key):\n        return [self][key]\n\n    @property\n    def measure(self):\n        return self\n\n    def subarray(self, leading):\n        \"\"\"Returns a data shape object of the subarray with 'leading'\n        dimensions removed. In the case of a measure such as CType,\n        'leading' must be 0, and self is returned.\n        \"\"\"\n        if leading >= 1:\n            raise IndexError('Not enough dimensions in data shape to remove %d leading dimensions.' % leading)\n        else:\n            return self\n\n    def __mul__(self, other):\n        if isinstance(other, str):\n            from datashader import datashape\n            return datashape.dshape(other).__rmul__(self)\n        if isinstance(other, int):\n            other = Fixed(other)\n        if isinstance(other, DataShape):\n            return other.__rmul__(self)\n        return DataShape(self, other)\n\n    def __rmul__(self, other):\n        if isinstance(other, str):\n            from datashader import datashape\n            return self * datashape.dshape(other)\n        if isinstance(other, int):\n            other = Fixed(other)\n        return DataShape(other, self)\n\n    def __getstate__(self):\n        return self.parameters\n\n    def __setstate__(self, state):\n        if self._slotted:\n            for slot, val in zip(self.__slots__, state):\n                setattr(self, slot, val)\n        else:\n            self._parameters = state\n\n    def to_numpy_dtype(self):\n        raise TypeError('DataShape %s is not NumPy-compatible' % self)\n\nclass Unit(Mono):\n    \"\"\"\n    Unit type that does not need to be reconstructed.\n    \"\"\"\n\n    def __str__(self):\n        return type(self).__name__.lower()\n\nclass Ellipsis(Mono):\n    \"\"\"Ellipsis (...). Used to indicate a variable number of dimensions.\n\n    E.g.:\n\n        ... * float32    # float32 array w/ any number of dimensions\n        A... * float32   # float32 array w/ any number of dimensions,\n                        # associated with type variable A\n    \"\"\"\n    __slots__ = ('typevar',)\n\n    def __init__(self, typevar=None):\n        self.typevar = typevar\n\n    def __str__(self):\n        return str(self.typevar) + '...' if self.typevar else '...'\n\n    def __repr__(self):\n        return '%s(%r)' % (type(self).__name__, str(self))\n\nclass Null(Unit):\n    \"\"\"The null datashape.\"\"\"\n    pass\n\nclass Date(Unit):\n    \"\"\" Date type \"\"\"\n    cls = MEASURE\n    __slots__ = ()\n\n    def to_numpy_dtype(self):\n        return np.dtype('datetime64[D]')\n\nclass Time(Unit):\n    \"\"\" Time type \"\"\"\n    cls = MEASURE\n    __slots__ = ('tz',)\n\n    def __init__(self, tz=None):\n        if tz is not None and (not isinstance(tz, str)):\n            raise TypeError('tz parameter to time datashape must be a string')\n        self.tz = tz\n\n    def __str__(self):\n        basename = super().__str__()\n        if self.tz is None:\n            return basename\n        else:\n            return '%s[tz=%r]' % (basename, str(self.tz))\n\nclass DateTime(Unit):\n    \"\"\" DateTime type \"\"\"\n    cls = MEASURE\n    __slots__ = ('tz',)\n\n    def __init__(self, tz=None):\n        if tz is not None and (not isinstance(tz, str)):\n            raise TypeError('tz parameter to datetime datashape must be a string')\n        self.tz = tz\n\n    def __str__(self):\n        basename = super().__str__()\n        if self.tz is None:\n            return basename\n        else:\n            return '%s[tz=%r]' % (basename, str(self.tz))\n\n    def to_numpy_dtype(self):\n        return np.dtype('datetime64[us]')\n_units = ('ns', 'us', 'ms', 's', 'm', 'h', 'D', 'W', 'M', 'Y')\n_unit_aliases = {'year': 'Y', 'week': 'W', 'day': 'D', 'date': 'D', 'hour': 'h', 'second': 's', 'millisecond': 'ms', 'microsecond': 'us', 'nanosecond': 'ns'}\n\ndef normalize_time_unit(s):\n    \"\"\" Normalize time input to one of 'year', 'second', 'millisecond', etc..\n    Example\n    -------\n    >>> normalize_time_unit('milliseconds')\n    'ms'\n    >>> normalize_time_unit('ms')\n    'ms'\n    >>> normalize_time_unit('nanoseconds')\n    'ns'\n    >>> normalize_time_unit('nanosecond')\n    'ns'\n    \"\"\"\n    s = s.strip()\n    if s in _units:\n        return s\n    if s in _unit_aliases:\n        return _unit_aliases[s]\n    if s[-1] == 's' and len(s) > 2:\n        return normalize_time_unit(s.rstrip('s'))\n    raise ValueError('Do not understand time unit %s' % s)\n\nclass TimeDelta(Unit):\n    cls = MEASURE\n    __slots__ = ('unit',)\n\n    def __init__(self, unit='us'):\n        self.unit = normalize_time_unit(str(unit))\n\n    def __str__(self):\n        return 'timedelta[unit=%r]' % self.unit\n\n    def to_numpy_dtype(self):\n        return np.dtype('timedelta64[%s]' % self.unit)\n\nclass Units(Unit):\n    \"\"\" Units type for values with physical units \"\"\"\n    cls = MEASURE\n    __slots__ = ('unit', 'tp')\n\n    def __init__(self, unit, tp=None):\n        if not isinstance(unit, str):\n            raise TypeError('unit parameter to units datashape must be a string')\n        if tp is None:\n            tp = DataShape(float64)\n        elif not isinstance(tp, DataShape):\n            raise TypeError('tp parameter to units datashape must be a datashape type')\n        self.unit = unit\n        self.tp = tp\n\n    def __str__(self):\n        if self.tp == DataShape(float64):\n            return 'units[%r]' % self.unit\n        else:\n            return 'units[%r, %s]' % (self.unit, self.tp)\n\nclass Bytes(Unit):\n    \"\"\" Bytes type \"\"\"\n    cls = MEASURE\n    __slots__ = ()\n_canonical_string_encodings = {'A': 'A', 'ascii': 'A', 'U8': 'U8', 'utf-8': 'U8', 'utf_8': 'U8', 'utf8': 'U8', 'U16': 'U16', 'utf-16': 'U16', 'utf_16': 'U16', 'utf16': 'U16', 'U32': 'U32', 'utf-32': 'U32', 'utf_32': 'U32', 'utf32': 'U32'}\n\nclass String(Unit):\n    \"\"\" String container\n\n    >>> String()\n    ctype(\"string\")\n    >>> String(10, 'ascii')\n    ctype(\"string[10, 'A']\")\n    \"\"\"\n    cls = MEASURE\n    __slots__ = ('fixlen', 'encoding')\n\n    def __init__(self, *args):\n        if len(args) == 0:\n            fixlen, encoding = (None, None)\n        if len(args) == 1:\n            if isinstance(args[0], str):\n                fixlen, encoding = (None, args[0])\n            if isinstance(args[0], int):\n                fixlen, encoding = (args[0], None)\n        elif len(args) == 2:\n            fixlen, encoding = args\n        encoding = encoding or 'U8'\n        if isinstance(encoding, str):\n            encoding = str(encoding)\n        try:\n            encoding = _canonical_string_encodings[encoding]\n        except KeyError:\n            raise ValueError('Unsupported string encoding %s' % repr(encoding))\n        self.encoding = encoding\n        self.fixlen = fixlen\n\n    def __str__(self):\n        if self.fixlen is None and self.encoding == 'U8':\n            return 'string'\n        elif self.fixlen is not None and self.encoding == 'U8':\n            return 'string[%i]' % self.fixlen\n        elif self.fixlen is None and self.encoding != 'U8':\n            return 'string[%s]' % repr(self.encoding).strip('u')\n        else:\n            return 'string[%i, %s]' % (self.fixlen, repr(self.encoding).strip('u'))\n\n    def __repr__(self):\n        s = str(self)\n        return 'ctype(\"%s\")' % s.encode('unicode_escape').decode('ascii')\n\n    def to_numpy_dtype(self):\n        \"\"\"\n        >>> String().to_numpy_dtype()\n        dtype('O')\n        >>> String(30).to_numpy_dtype()\n        dtype('<U30')\n        >>> String(30, 'A').to_numpy_dtype()\n        dtype('S30')\n        \"\"\"\n        if self.fixlen:\n            if self.encoding == 'A':\n                return np.dtype('S%d' % self.fixlen)\n            else:\n                return np.dtype('U%d' % self.fixlen)\n        return np.dtype('O', metadata={'vlen': str})\n\nclass Decimal(Unit):\n    \"\"\"Decimal type corresponding to SQL Decimal/Numeric types.\n\n    The first parameter passed specifies the number of digits of precision that\n    the Decimal contains. If an additional parameter is given, it represents\n    the scale, or number of digits of precision that are after the decimal\n    point.\n\n    The Decimal type makes no requirement of how it is to be stored in memory,\n    therefore, the number of bytes needed to store a Decimal for a given\n    precision will vary based on the platform where it is used.\n\n    Examples\n    --------\n    >>> Decimal(18)\n    Decimal(precision=18, scale=0)\n    >>> Decimal(7, 4)\n    Decimal(precision=7, scale=4)\n    >>> Decimal(precision=11, scale=2)\n    Decimal(precision=11, scale=2)\n    \"\"\"\n    cls = MEASURE\n    __slots__ = ('precision', 'scale')\n\n    def __init__(self, precision, scale=0):\n        self.precision = precision\n        self.scale = scale\n\n    def __str__(self):\n        return 'decimal[precision={precision}, scale={scale}]'.format(precision=self.precision, scale=self.scale)\n\n    def to_numpy_dtype(self):\n        \"\"\"Convert a decimal datashape to a NumPy dtype.\n\n        Note that floating-point (scale > 0) precision will be lost converting\n        to NumPy floats.\n\n        Examples\n        --------\n        >>> Decimal(18).to_numpy_dtype()\n        dtype('int64')\n        >>> Decimal(7,4).to_numpy_dtype()\n        dtype('float64')\n        \"\"\"\n        if self.scale == 0:\n            if self.precision <= 2:\n                return np.dtype(np.int8)\n            elif self.precision <= 4:\n                return np.dtype(np.int16)\n            elif self.precision <= 9:\n                return np.dtype(np.int32)\n            elif self.precision <= 18:\n                return np.dtype(np.int64)\n            else:\n                raise TypeError('Integer Decimal precision > 18 is not NumPy-compatible')\n        else:\n            return np.dtype(np.float64)\n\nclass DataShape(Mono):\n    \"\"\"\n    Composite container for datashape elements.\n\n    Elements of a datashape like ``Fixed(3)``, ``Var()`` or ``int32`` are on,\n    on their own, valid datashapes.  These elements are collected together into\n    a composite ``DataShape`` to be complete.\n\n    This class is not intended to be used directly.  Instead, use the utility\n    ``dshape`` function to create datashapes from strings or datashape\n    elements.\n\n    Examples\n    --------\n\n    >>> from datashader.datashape import Fixed, int32, DataShape, dshape\n\n    >>> DataShape(Fixed(5), int32)  # Rare to DataShape directly\n    dshape(\"5 * int32\")\n\n    >>> dshape('5 * int32')         # Instead use the dshape function\n    dshape(\"5 * int32\")\n\n    >>> dshape([Fixed(5), int32])   # It can even do construction from elements\n    dshape(\"5 * int32\")\n\n    See Also\n    --------\n    datashape.dshape\n    \"\"\"\n    composite = False\n\n    def __len__(self):\n        return len(self.parameters)\n\n    def __str__(self):\n        return self.name or ' * '.join(map(str, self.parameters))\n\n    def __repr__(self):\n        s = pprint(self)\n        if '\\n' in s:\n            return 'dshape(\"\"\"%s\"\"\")' % s\n        else:\n            return 'dshape(\"%s\")' % s\n\n    def subarray(self, leading):\n        \"\"\"Returns a data shape object of the subarray with 'leading'\n        dimensions removed.\n\n        >>> from datashader.datashape import dshape\n        >>> dshape('1 * 2 * 3 * int32').subarray(1)\n        dshape(\"2 * 3 * int32\")\n        >>> dshape('1 * 2 * 3 * int32').subarray(2)\n        dshape(\"3 * int32\")\n        \"\"\"\n        if leading >= len(self.parameters):\n            raise IndexError('Not enough dimensions in data shape to remove %d leading dimensions.' % leading)\n        elif leading in [len(self.parameters) - 1, -1]:\n            return DataShape(self.parameters[-1])\n        else:\n            return DataShape(*self.parameters[leading:])\n\n    def __rmul__(self, other):\n        if isinstance(other, int):\n            other = Fixed(other)\n        return DataShape(other, *self)\n\n    @property\n    def subshape(self):\n        return IndexCallable(self._subshape)\n\n    def _subshape(self, index):\n        \"\"\" The DataShape of an indexed subarray\n\n        >>> from datashader.datashape import dshape\n\n        >>> ds = dshape('var * {name: string, amount: int32}')\n        >>> print(ds.subshape[0])\n        {name: string, amount: int32}\n\n        >>> print(ds.subshape[0:3])\n        3 * {name: string, amount: int32}\n\n        >>> print(ds.subshape[0:7:2, 'amount'])\n        4 * int32\n\n        >>> print(ds.subshape[[1, 10, 15]])\n        3 * {name: string, amount: int32}\n\n        >>> ds = dshape('{x: int, y: int}')\n        >>> print(ds.subshape['x'])\n        int32\n\n        >>> ds = dshape('10 * var * 10 * int32')\n        >>> print(ds.subshape[0:5, 0:3, 5])\n        5 * 3 * int32\n\n        >>> ds = dshape('var * {name: string, amount: int32, id: int32}')\n        >>> print(ds.subshape[:, [0, 2]])\n        var * {name: string, id: int32}\n\n        >>> ds = dshape('var * {name: string, amount: int32, id: int32}')\n        >>> print(ds.subshape[:, ['name', 'id']])\n        var * {name: string, id: int32}\n\n        >>> print(ds.subshape[0, 1:])\n        {amount: int32, id: int32}\n        \"\"\"\n        from .predicates import isdimension\n        if isinstance(index, int) and isdimension(self[0]):\n            return self.subarray(1)\n        if isinstance(self[0], Record) and isinstance(index, str):\n            return self[0][index]\n        if isinstance(self[0], Record) and isinstance(index, int):\n            return self[0].parameters[0][index][1]\n        if isinstance(self[0], Record) and isinstance(index, list):\n            rec = self[0]\n            index = [self[0].names.index(i) if isinstance(i, str) else i for i in index]\n            return DataShape(Record([rec.parameters[0][i] for i in index]))\n        if isinstance(self[0], Record) and isinstance(index, slice):\n            rec = self[0]\n            return DataShape(Record(rec.parameters[0][index]))\n        if isinstance(index, list) and isdimension(self[0]):\n            return len(index) * self.subarray(1)\n        if isinstance(index, slice):\n            if isinstance(self[0], Fixed):\n                n = int(self[0])\n                start = index.start or 0\n                stop = index.stop or n\n                if start < 0:\n                    start = n + start\n                if stop < 0:\n                    stop = n + stop\n                count = stop - start\n            else:\n                start = index.start or 0\n                stop = index.stop\n                if not stop:\n                    count = -start if start < 0 else var\n                if stop is not None and start is not None and (stop >= 0) and (start >= 0):\n                    count = stop - start\n                else:\n                    count = var\n            if count != var and index.step is not None:\n                count = int(ceil(count / index.step))\n            return count * self.subarray(1)\n        if isinstance(index, tuple):\n            if not index:\n                return self\n            elif index[0] is None:\n                return 1 * self._subshape(index[1:])\n            elif len(index) == 1:\n                return self._subshape(index[0])\n            else:\n                ds = self.subarray(1)._subshape(index[1:])\n                return (self[0] * ds)._subshape(index[0])\n        raise TypeError('invalid index value %s of type %r' % (index, type(index).__name__))\n\n    def __setstate__(self, state):\n        self._parameters = state\n        self.composite = True\n        self.name = None\nnumpy_provides_missing = frozenset((Date, DateTime, TimeDelta))\n\nclass Option(Mono):\n    \"\"\"\n    Measure types which may or may not hold data. Makes no\n    indication of how this is implemented in memory.\n    \"\"\"\n    __slots__ = ('ty',)\n\n    def __init__(self, ds):\n        self.ty = _launder(ds)\n\n    @property\n    def shape(self):\n        return self.ty.shape\n\n    @property\n    def itemsize(self):\n        return self.ty.itemsize\n\n    def __str__(self):\n        return '?%s' % self.ty\n\n    def to_numpy_dtype(self):\n        if type(self.ty) in numpy_provides_missing:\n            return self.ty.to_numpy_dtype()\n        raise TypeError('DataShape measure %s is not NumPy-compatible' % self)\n\nclass CType(Unit):\n    \"\"\"\n    Symbol for a sized type mapping uniquely to a native type.\n    \"\"\"\n    cls = MEASURE\n    __slots__ = ('name', '_itemsize', '_alignment')\n\n    def __init__(self, name, itemsize, alignment):\n        self.name = name\n        self._itemsize = itemsize\n        self._alignment = alignment\n        Type.register(name, self)\n\n    @classmethod\n    def from_numpy_dtype(self, dt):\n        \"\"\"\n        From Numpy dtype.\n\n        >>> from datashader.datashape import CType\n        >>> from numpy import dtype\n        >>> CType.from_numpy_dtype(dtype('int32'))\n        ctype(\"int32\")\n        >>> CType.from_numpy_dtype(dtype('i8'))\n        ctype(\"int64\")\n        >>> CType.from_numpy_dtype(dtype('M8'))\n        DateTime(tz=None)\n        >>> CType.from_numpy_dtype(dtype('U30'))  # doctest: +SKIP\n        ctype(\"string[30, 'U32']\")\n        \"\"\"\n        try:\n            return Type.lookup_type(dt.name)\n        except KeyError:\n            pass\n        if np.issubdtype(dt, np.datetime64):\n            unit, _ = np.datetime_data(dt)\n            defaults = {'D': date_, 'Y': date_, 'M': date_, 'W': date_}\n            return defaults.get(unit, datetime_)\n        elif np.issubdtype(dt, np.timedelta64):\n            unit, _ = np.datetime_data(dt)\n            return TimeDelta(unit=unit)\n        elif np.__version__[0] < '2' and np.issubdtype(dt, np.unicode_):\n            return String(dt.itemsize // 4, 'U32')\n        elif np.issubdtype(dt, np.str_) or np.issubdtype(dt, np.bytes_):\n            return String(dt.itemsize, 'ascii')\n        raise NotImplementedError('NumPy datatype %s not supported' % dt)\n\n    @property\n    def itemsize(self):\n        \"\"\"The size of one element of this type.\"\"\"\n        return self._itemsize\n\n    @property\n    def alignment(self):\n        \"\"\"The alignment of one element of this type.\"\"\"\n        return self._alignment\n\n    def to_numpy_dtype(self):\n        \"\"\"\n        To Numpy dtype.\n        \"\"\"\n        name = self.name\n        return np.dtype({'complex[float32]': 'complex64', 'complex[float64]': 'complex128'}.get(name, name))\n\n    def __str__(self):\n        return self.name\n\nclass Fixed(Unit):\n    \"\"\"\n    Fixed dimension.\n    \"\"\"\n    cls = DIMENSION\n    __slots__ = ('val',)\n\n    def __int__(self):\n        return self.val\n    __hash__ = Mono.__hash__\n\n    def __str__(self):\n        return str(self.val)\n\nclass Var(Unit):\n    \"\"\" Variable dimension \"\"\"\n    cls = DIMENSION\n    __slots__ = ()\n\nclass TypeVar(Unit):\n    \"\"\"\n    A free variable in the signature. Not user facing.\n    \"\"\"\n    __slots__ = ('symbol',)\n\n    def __init__(self, symbol):\n        if not symbol[0].isupper():\n            raise ValueError('TypeVar symbol %r does not begin with a capital' % symbol)\n        self.symbol = symbol\n\n    def __str__(self):\n        return str(self.symbol)\n\nclass Function(Mono):\n    \"\"\"Function signature type\n    \"\"\"\n\n    @property\n    def restype(self):\n        return self.parameters[-1]\n\n    @property\n    def argtypes(self):\n        return self.parameters[:-1]\n\n    def __str__(self):\n        return '(%s) -> %s' % (', '.join(map(str, self.argtypes)), self.restype)\n\nclass Map(Mono):\n    __slots__ = ('key', 'value')\n\n    def __init__(self, key, value):\n        self.key = _launder(key)\n        self.value = _launder(value)\n\n    def __str__(self):\n        return '%s[%s, %s]' % (type(self).__name__.lower(), self.key, self.value)\n\n    def to_numpy_dtype(self):\n        return to_numpy_dtype(self)\n\ndef _launder(x):\n    \"\"\" Clean up types prior to insertion into DataShape\n\n    >>> from datashader.datashape import dshape\n    >>> _launder(5)         # convert ints to Fixed\n    Fixed(val=5)\n    >>> _launder('int32')   # parse strings\n    ctype(\"int32\")\n    >>> _launder(dshape('int32'))\n    ctype(\"int32\")\n    >>> _launder(Fixed(5))  # No-op on valid parameters\n    Fixed(val=5)\n    \"\"\"\n    if isinstance(x, int):\n        x = Fixed(x)\n    if isinstance(x, str):\n        x = datashape.dshape(x)\n    if isinstance(x, DataShape) and len(x) == 1:\n        return x[0]\n    if isinstance(x, Mono):\n        return x\n    return x\n\nclass CollectionPrinter:\n\n    def __repr__(self):\n        s = str(self)\n        strs = ('\"\"\"%s\"\"\"' if '\\n' in s else '\"%s\"') % s\n        return 'dshape(%s)' % strs\n\nclass RecordMeta(Type):\n\n    @staticmethod\n    def _unpack_slice(s, idx):\n        if not isinstance(s, slice):\n            raise TypeError('invalid field specification at position %d.\\nfields must be formatted like: {name}:{type}' % idx)\n        name, type_ = packed = (s.start, s.stop)\n        if name is None:\n            raise TypeError('missing field name at position %d' % idx)\n        if not isinstance(name, str):\n            raise TypeError(\"field name at position %d ('%s') was not a string\" % (idx, name))\n        if type_ is None and s.step is None:\n            raise TypeError(\"missing type for field '%s' at position %d\" % (name, idx))\n        if s.step is not None:\n            raise TypeError(\"unexpected slice step for field '%s' at position %d.\\nhint: you might have a second ':'\" % (name, idx))\n        return packed\n\n    def __getitem__(self, types):\n        if not isinstance(types, tuple):\n            types = (types,)\n        return self(list(map(self._unpack_slice, types, range(len(types)))))\n\nclass Record(CollectionPrinter, Mono, metaclass=RecordMeta):\n    \"\"\"\n    A composite data structure of ordered fields mapped to types.\n\n    Properties\n    ----------\n\n    fields: tuple of (name, type) pairs\n        The only stored data, also the input to ``__init__``\n    dict: dict\n        A dictionary view of ``fields``\n    names: list of strings\n        A list of the names\n    types: list of datashapes\n        A list of the datashapes\n\n    Example\n    -------\n\n    >>> Record([['id', 'int'], ['name', 'string'], ['amount', 'real']])\n    dshape(\"{id: int32, name: string, amount: float64}\")\n    \"\"\"\n    cls = MEASURE\n\n    def __init__(self, fields):\n        \"\"\"\n        Parameters\n        ----------\n        fields : list/OrderedDict of (name, type) entries\n            The fields which make up the record.\n        \"\"\"\n        if isinstance(fields, OrderedDict):\n            fields = fields.items()\n        fields = list(fields)\n        names = [str(name) if not isinstance(name, str) else name for name, _ in fields]\n        types = [_launder(v) for _, v in fields]\n        if len(set(names)) != len(names):\n            for name in set(names):\n                names.remove(name)\n            raise ValueError('duplicate field names found: %s' % names)\n        self._parameters = (tuple(zip(names, types)),)\n\n    @property\n    def fields(self):\n        return self._parameters[0]\n\n    @property\n    def dict(self):\n        return dict(self.fields)\n\n    @property\n    def names(self):\n        return [n for n, t in self.fields]\n\n    @property\n    def types(self):\n        return [t for n, t in self.fields]\n\n    def to_numpy_dtype(self):\n        \"\"\"\n        To Numpy record dtype.\n        \"\"\"\n        return np.dtype([(str(name), to_numpy_dtype(typ)) for name, typ in self.fields])\n\n    def __getitem__(self, key):\n        return self.dict[key]\n\n    def __str__(self):\n        return pprint(self)\nR = Record\n\ndef _format_categories(cats, n=10):\n    return '[%s%s]' % (', '.join(map(repr, cats[:n])), ', ...' if len(cats) > n else '')\n\nclass Categorical(Mono):\n    \"\"\"Unordered categorical type.\n    \"\"\"\n    __slots__ = ('categories', 'type', 'ordered')\n    cls = MEASURE\n\n    def __init__(self, categories, type=None, ordered=False):\n        self.categories = tuple(categories)\n        self.type = (type or datashape.discover(self.categories)).measure\n        self.ordered = ordered\n\n    def __str__(self):\n        return '%s[%s, type=%s, ordered=%s]' % (type(self).__name__.lower(), _format_categories(self.categories), self.type, self.ordered)\n\n    def __repr__(self):\n        return '%s(categories=%s, type=%r, ordered=%s)' % (type(self).__name__, _format_categories(self.categories), self.type, self.ordered)\n\nclass Tuple(CollectionPrinter, Mono):\n    \"\"\"\n    A product type.\n    \"\"\"\n    __slots__ = ('dshapes',)\n    cls = MEASURE\n\n    def __init__(self, dshapes):\n        \"\"\"\n        Parameters\n        ----------\n        dshapes : list of dshapes\n            The datashapes which make up the tuple.\n        \"\"\"\n        dshapes = [DataShape(ds) if not isinstance(ds, DataShape) else ds for ds in dshapes]\n        self.dshapes = tuple(dshapes)\n\n    def __str__(self):\n        return '(%s)' % ', '.join(map(str, self.dshapes))\n\n    def to_numpy_dtype(self):\n        \"\"\"\n        To Numpy record dtype.\n        \"\"\"\n        return np.dtype([('f%d' % i, to_numpy_dtype(typ)) for i, typ in enumerate(self.parameters[0])])\n\nclass JSON(Mono):\n    \"\"\" JSON measure \"\"\"\n    cls = MEASURE\n    __slots__ = ()\n\n    def __str__(self):\n        return 'json'\nbool_ = CType('bool', 1, 1)\nchar = CType('char', 1, 1)\nint8 = CType('int8', 1, 1)\nint16 = CType('int16', 2, ctypes.alignment(ctypes.c_int16))\nint32 = CType('int32', 4, ctypes.alignment(ctypes.c_int32))\nint64 = CType('int64', 8, ctypes.alignment(ctypes.c_int64))\nint_ = int32\nType.register('int', int_)\nuint8 = CType('uint8', 1, 1)\nuint16 = CType('uint16', 2, ctypes.alignment(ctypes.c_uint16))\nuint32 = CType('uint32', 4, ctypes.alignment(ctypes.c_uint32))\nuint64 = CType('uint64', 8, ctypes.alignment(ctypes.c_uint64))\nfloat16 = CType('float16', 2, ctypes.alignment(ctypes.c_uint16))\nfloat32 = CType('float32', 4, ctypes.alignment(ctypes.c_float))\nfloat64 = CType('float64', 8, ctypes.alignment(ctypes.c_double))\nreal = float64\nType.register('real', real)\ncomplex_float32 = CType('complex[float32]', 8, ctypes.alignment(ctypes.c_float))\ncomplex_float64 = CType('complex[float64]', 16, ctypes.alignment(ctypes.c_double))\nType.register('complex64', complex_float32)\ncomplex64 = complex_float32\nType.register('complex128', complex_float64)\ncomplex128 = complex_float64\ncomplex_ = complex_float64\ndate_ = Date()\ntime_ = Time()\ndatetime_ = DateTime()\ntimedelta_ = TimeDelta()\nType.register('date', date_)\nType.register('time', time_)\nType.register('datetime', datetime_)\nType.register('timedelta', timedelta_)\nnull = Null()\nType.register('null', null)\nc_byte = int8\nc_short = int16\nc_int = int32\nc_longlong = int64\nc_ubyte = uint8\nc_ushort = uint16\nc_ulonglong = uint64\nif ctypes.sizeof(ctypes.c_long) == 4:\n    c_long = int32\n    c_ulong = uint32\nelse:\n    c_long = int64\n    c_ulong = uint64\nif ctypes.sizeof(ctypes.c_void_p) == 4:\n    intptr = c_ssize_t = int32\n    uintptr = c_size_t = uint32\nelse:\n    intptr = c_ssize_t = int64\n    uintptr = c_size_t = uint64\nType.register('intptr', intptr)\nType.register('uintptr', uintptr)\nc_half = float16\nc_float = float32\nc_double = float64\nhalf = float16\nsingle = float32\ndouble = float64\nvoid = CType('void', 0, 1)\nobject_ = pyobj = CType('object', ctypes.sizeof(ctypes.py_object), ctypes.alignment(ctypes.py_object))\nna = Null\nNullRecord = Record(())\nbytes_ = Bytes()\nstring = String()\njson = JSON()\nType.register('float', c_float)\nType.register('double', c_double)\nType.register('bytes', bytes_)\nType.register('string', String())\nvar = Var()\n\ndef to_numpy_dtype(ds):\n    \"\"\" Throw away the shape information and just return the\n    measure as NumPy dtype instance.\"\"\"\n    if isinstance(ds.measure, datashape.coretypes.Map):\n        ds = ds.measure.key\n    return to_numpy(ds.measure)[1]\n\ndef to_numpy(ds):\n    \"\"\"\n    Downcast a datashape object into a Numpy (shape, dtype) tuple if\n    possible.\n\n    >>> from datashader.datashape import dshape, to_numpy\n    >>> to_numpy(dshape('5 * 5 * int32'))\n    ((5, 5), dtype('int32'))\n    >>> to_numpy(dshape('10 * string[30]'))\n    ((10,), dtype('<U30'))\n    >>> to_numpy(dshape('N * int32'))\n    ((-1,), dtype('int32'))\n    \"\"\"\n    shape = []\n    if isinstance(ds, DataShape):\n        for dim in ds[:-1]:\n            if isinstance(dim, Fixed):\n                shape.append(int(dim))\n            elif isinstance(dim, TypeVar):\n                shape.append(-1)\n            else:\n                raise TypeError('DataShape dimension %s is not NumPy-compatible' % dim)\n        msr = ds[-1]\n    else:\n        msr = ds\n    return (tuple(shape), msr.to_numpy_dtype())\n\ndef from_numpy(shape, dt):\n    \"\"\"\n    Upcast a (shape, dtype) tuple if possible.\n\n    >>> from datashader.datashape import from_numpy\n    >>> from numpy import dtype\n    >>> from_numpy((5, 5), dtype('int32'))\n    dshape(\"5 * 5 * int32\")\n\n    >>> from_numpy((10,), dtype('S10'))\n    dshape(\"10 * string[10, 'A']\")\n    \"\"\"\n    dtype = np.dtype(dt)\n    if dtype.kind == 'S':\n        measure = String(dtype.itemsize, 'A')\n    elif dtype.kind == 'U':\n        measure = String(dtype.itemsize // 4, 'U32')\n    elif dtype.fields:\n        fields = [(name, dtype.fields[name]) for name in dtype.names]\n        rec = [(name, from_numpy(t.shape, t.base)) for name, (t, _) in fields]\n        measure = Record(rec)\n    else:\n        measure = CType.from_numpy_dtype(dtype)\n    if not shape:\n        return measure\n    return DataShape(*tuple(map(Fixed, shape)) + (measure,))\n\ndef print_unicode_string(s):\n    try:\n        return s.decode('unicode_escape').encode('ascii')\n    except AttributeError:\n        return s\n\ndef pprint(ds, width=80):\n    ''' Pretty print a datashape\n\n    >>> from datashader.datashape import dshape, pprint\n    >>> print(pprint(dshape('5 * 3 * int32')))\n    5 * 3 * int32\n\n    >>> ds = dshape(\"\"\"\n    ... 5000000000 * {\n    ...     a: (int, float32, real, string, datetime),\n    ...     b: {c: 5 * int, d: var * 100 * float32}\n    ... }\"\"\")\n    >>> print(pprint(ds))\n    5000000000 * {\n      a: (int32, float32, float64, string, datetime),\n      b: {c: 5 * int32, d: var * 100 * float32}\n      }\n\n    Record measures print like full datashapes\n    >>> print(pprint(ds.measure, width=30))\n    {\n      a: (\n        int32,\n        float32,\n        float64,\n        string,\n        datetime\n        ),\n      b: {\n        c: 5 * int32,\n        d: var * 100 * float32\n        }\n      }\n\n    Control width of the result\n    >>> print(pprint(ds, width=30))\n    5000000000 * {\n      a: (\n        int32,\n        float32,\n        float64,\n        string,\n        datetime\n        ),\n      b: {\n        c: 5 * int32,\n        d: var * 100 * float32\n        }\n      }\n    >>>\n    '''\n    result = ''\n    if isinstance(ds, DataShape):\n        if ds.shape:\n            result += ' * '.join(map(str, ds.shape))\n            result += ' * '\n        ds = ds[-1]\n    if isinstance(ds, Record):\n        pairs = ['%s: %s' % (name if isidentifier(name) else repr(print_unicode_string(name)), pprint(typ, width - len(result) - len(name))) for name, typ in zip(ds.names, ds.types)]\n        short = '{%s}' % ', '.join(pairs)\n        if len(result + short) < width:\n            return result + short\n        else:\n            long = '{\\n%s\\n}' % ',\\n'.join(pairs)\n            return result + long.replace('\\n', '\\n  ')\n    elif isinstance(ds, Tuple):\n        typs = [pprint(typ, width - len(result)) for typ in ds.dshapes]\n        short = '(%s)' % ', '.join(typs)\n        if len(result + short) < width:\n            return result + short\n        else:\n            long = '(\\n%s\\n)' % ',\\n'.join(typs)\n            return result + long.replace('\\n', '\\n  ')\n    else:\n        result += str(ds)\n    return result",
    "datashader/datashape/util/__init__.py": "from itertools import chain\nimport operator\nfrom .. import parser\nfrom .. import type_symbol_table\nfrom ..validation import validate\nfrom .. import coretypes\n__all__ = ('dshape', 'dshapes', 'has_var_dim', 'has_ellipsis', 'cat_dshapes')\nsubclasses = operator.methodcaller('__subclasses__')\n\ndef dshapes(*args):\n    \"\"\"\n    Parse a bunch of datashapes all at once.\n\n    >>> a, b = dshapes('3 * int32', '2 * var * float64')\n    \"\"\"\n    return [dshape(arg) for arg in args]\n\ndef collect(pred, expr):\n    \"\"\" Collect terms in expression that match predicate\n\n    >>> from datashader.datashape import Unit, dshape\n    >>> predicate = lambda term: isinstance(term, Unit)\n    >>> dshape = dshape('var * {value: int64, loc: 2 * int32}')\n    >>> sorted(set(collect(predicate, dshape)), key=str)\n    [Fixed(val=2), ctype(\"int32\"), ctype(\"int64\"), Var()]\n    >>> from datashader.datashape import var, int64\n    >>> sorted(set(collect(predicate, [var, int64])), key=str)\n    [ctype(\"int64\"), Var()]\n    \"\"\"\n    if pred(expr):\n        return [expr]\n    if isinstance(expr, coretypes.Record):\n        return chain.from_iterable((collect(pred, typ) for typ in expr.types))\n    if isinstance(expr, coretypes.Mono):\n        return chain.from_iterable((collect(pred, typ) for typ in expr.parameters))\n    if isinstance(expr, (list, tuple)):\n        return chain.from_iterable((collect(pred, item) for item in expr))",
    "datashader/datashape/parser.py": "\"\"\"\nParser for the datashape grammar.\n\"\"\"\nfrom . import lexer, error\nfrom . import coretypes\n__all__ = ['parse']\n\nclass DataShapeParser:\n    \"\"\"A DataShape parser object.\"\"\"\n\n    def __init__(self, ds_str, sym):\n        self.ds_str = ds_str\n        self.sym = sym\n        self.lex = lexer.lex(ds_str)\n        self.tokens = []\n        self.pos = -1\n        self.end_pos = None\n        self.advance_tok()\n\n    def advance_tok(self):\n        \"\"\"Advances self.pos by one, if it is not already at the end.\"\"\"\n        if self.pos != self.end_pos:\n            self.pos = self.pos + 1\n            try:\n                if self.pos >= len(self.tokens):\n                    self.tokens.append(next(self.lex))\n            except StopIteration:\n                if len(self.tokens) > 0:\n                    span = (self.tokens[self.pos - 1].span[1],) * 2\n                else:\n                    span = (0, 0)\n                self.tokens.append(lexer.Token(None, None, span, None))\n                self.end_pos = self.pos\n\n    @property\n    def tok(self):\n        return self.tokens[self.pos]\n\n    def raise_error(self, errmsg):\n        raise error.DataShapeSyntaxError(self.tok.span[0], '<nofile>', self.ds_str, errmsg)\n\n    def parse_homogeneous_list(self, parse_item, sep_tok_id, errmsg, trailing_sep=False):\n        \"\"\"\n        <item>_list : <item> <SEP> <item>_list\n                    | <item>\n\n        Returns a list of <item>s, or None.\n        \"\"\"\n        saved_pos = self.pos\n        items = []\n        item = True\n        while item is not None:\n            item = parse_item()\n            if item is not None:\n                items.append(item)\n                if self.tok.id == sep_tok_id:\n                    self.advance_tok()\n                else:\n                    return items\n            elif len(items) > 0:\n                if trailing_sep:\n                    return items\n                else:\n                    self.raise_error(errmsg)\n            else:\n                self.pos = saved_pos\n                return None\n\n    def syntactic_sugar(self, symdict, name, dshapemsg, error_pos=None):\n        \"\"\"\n        Looks up a symbol in the provided symbol table dictionary for\n        syntactic sugar, raising a standard error message if the symbol\n        is missing.\n\n        Parameters\n        ----------\n        symdict : symbol table dictionary\n            One of self.sym.dtype, self.sym.dim,\n            self.sym.dtype_constr, or self.sym.dim_constr.\n        name : str\n            The name of the symbol to look up.\n        dshapemsg : str\n            The datashape construct this lookup is for, e.g.\n            '{...} dtype constructor'.\n        error_pos : int, optional\n            The position in the token stream at which to flag the error.\n        \"\"\"\n        entry = symdict.get(name)\n        if entry is not None:\n            return entry\n        else:\n            if error_pos is not None:\n                self.pos = error_pos\n            self.raise_error(('Symbol table missing \"%s\" ' + 'entry for %s') % (name, dshapemsg))\n\n    def parse_datashape(self):\n        \"\"\"\n        datashape : datashape_nooption\n                  | QUESTIONMARK datashape_nooption\n                  | EXCLAMATIONMARK datashape_nooption\n\n        Returns a datashape object or None.\n        \"\"\"\n        tok = self.tok\n        constructors = {lexer.QUESTIONMARK: 'option'}\n        if tok.id in constructors:\n            self.advance_tok()\n            saved_pos = self.pos\n            ds = self.parse_datashape_nooption()\n            if ds is not None:\n                option = self.syntactic_sugar(self.sym.dtype_constr, constructors[tok.id], '%s dtype construction' % constructors[tok.id], saved_pos - 1)\n                return coretypes.DataShape(option(ds))\n        else:\n            return self.parse_datashape_nooption()\n\n    def parse_datashape_nooption(self):\n        \"\"\"\n        datashape_nooption : dim ASTERISK datashape\n                           | dtype\n\n        Returns a datashape object or None.\n        \"\"\"\n        saved_pos = self.pos\n        dim = self.parse_dim()\n        if dim is not None:\n            if self.tok.id == lexer.ASTERISK:\n                self.advance_tok()\n                saved_pos = self.pos\n                dshape = self.parse_datashape()\n                if dshape is None:\n                    self.pos = saved_pos\n                    self.raise_error('Expected a dim or a dtype')\n                return coretypes.DataShape(dim, *dshape.parameters)\n        dtype = self.parse_dtype()\n        if dtype:\n            return coretypes.DataShape(dtype)\n        else:\n            return None\n\n    def parse_dim(self):\n        \"\"\"\n        dim : typevar\n            | ellipsis_typevar\n            | type\n            | type_constr\n            | INTEGER\n            | ELLIPSIS\n        typevar : NAME_UPPER\n        ellipsis_typevar : NAME_UPPER ELLIPSIS\n        type : NAME_LOWER\n        type_constr : NAME_LOWER LBRACKET type_arg_list RBRACKET\n\n        Returns a the dim object, or None.\n        TODO: Support type constructors\n        \"\"\"\n        saved_pos = self.pos\n        tok = self.tok\n        if tok.id == lexer.NAME_UPPER:\n            val = tok.val\n            self.advance_tok()\n            if self.tok.id == lexer.ELLIPSIS:\n                self.advance_tok()\n                tconstr = self.syntactic_sugar(self.sym.dim_constr, 'ellipsis', 'TypeVar... dim constructor', saved_pos)\n                return tconstr(val)\n            elif self.tok.id == lexer.ASTERISK:\n                tconstr = self.syntactic_sugar(self.sym.dim_constr, 'typevar', 'TypeVar dim constructor', saved_pos)\n                return tconstr(val)\n            else:\n                self.pos = saved_pos\n                return None\n        elif tok.id == lexer.NAME_LOWER:\n            name = tok.val\n            self.advance_tok()\n            if self.tok.id == lexer.LBRACKET:\n                dim_constr = self.sym.dim_constr.get(name)\n                if dim_constr is None:\n                    self.pos = saved_pos\n                    return None\n                self.advance_tok()\n                args = self.parse_type_arg_list()\n                if self.tok.id == lexer.RBRACKET:\n                    self.advance_tok()\n                    raise NotImplementedError('dim type constructors not actually supported yet')\n                else:\n                    self.raise_error('Expected a closing \"]\"')\n            else:\n                dim = self.sym.dim.get(name)\n                if dim is not None:\n                    return dim\n                else:\n                    self.pos = saved_pos\n                    return None\n        elif tok.id == lexer.INTEGER:\n            val = tok.val\n            self.advance_tok()\n            if self.tok.id != lexer.ASTERISK:\n                self.pos = saved_pos\n                return None\n            tconstr = self.syntactic_sugar(self.sym.dim_constr, 'fixed', 'integer dimensions')\n            return tconstr(val)\n        elif tok.id == lexer.ELLIPSIS:\n            self.advance_tok()\n            dim = self.syntactic_sugar(self.sym.dim, 'ellipsis', '... dim', saved_pos)\n            return dim\n        else:\n            return None\n\n    def parse_dtype(self):\n        \"\"\"\n        dtype : typevar\n              | type\n              | type_constr\n              | struct_type\n              | funcproto_or_tuple_type\n        typevar : NAME_UPPER\n        ellipsis_typevar : NAME_UPPER ELLIPSIS\n        type : NAME_LOWER\n        type_constr : NAME_LOWER LBRACKET type_arg_list RBRACKET\n        struct_type : LBRACE ...\n        funcproto_or_tuple_type : LPAREN ...\n\n        Returns a the dtype object, or None.\n        \"\"\"\n        saved_pos = self.pos\n        tok = self.tok\n        if tok.id == lexer.NAME_UPPER:\n            val = tok.val\n            self.advance_tok()\n            tconstr = self.syntactic_sugar(self.sym.dtype_constr, 'typevar', 'TypeVar dtype constructor', saved_pos)\n            return tconstr(val)\n        elif tok.id == lexer.NAME_LOWER:\n            name = tok.val\n            self.advance_tok()\n            if self.tok.id == lexer.LBRACKET:\n                dtype_constr = self.sym.dtype_constr.get(name)\n                if dtype_constr is None:\n                    self.pos = saved_pos\n                    return None\n                self.advance_tok()\n                args, kwargs = self.parse_type_arg_list()\n                if self.tok.id == lexer.RBRACKET:\n                    if len(args) == 0 and len(kwargs) == 0:\n                        self.raise_error('Expected at least one type ' + 'constructor argument')\n                    self.advance_tok()\n                    return dtype_constr(*args, **kwargs)\n                else:\n                    self.raise_error('Invalid type constructor argument')\n            else:\n                dtype = self.sym.dtype.get(name)\n                if dtype is not None:\n                    return dtype\n                else:\n                    self.pos = saved_pos\n                    return None\n        elif tok.id == lexer.LBRACE:\n            return self.parse_struct_type()\n        elif tok.id == lexer.LPAREN:\n            return self.parse_funcproto_or_tuple_type()\n        else:\n            return None\n\n    def parse_type_arg_list(self):\n        \"\"\"\n        type_arg_list : type_arg COMMA type_arg_list\n                      | type_kwarg_list\n                      | type_arg\n        type_kwarg_list : type_kwarg COMMA type_kwarg_list\n                        | type_kwarg\n\n        Returns a tuple (args, kwargs), or (None, None).\n        \"\"\"\n        args = []\n        arg = True\n        while arg is not None:\n            arg = self.parse_type_arg()\n            if arg is not None:\n                if self.tok.id == lexer.COMMA:\n                    self.advance_tok()\n                    args.append(arg)\n                else:\n                    args.append(arg)\n                    return (args, {})\n            else:\n                break\n        kwargs = self.parse_homogeneous_list(self.parse_type_kwarg, lexer.COMMA, 'Expected another keyword argument, ' + 'positional arguments cannot follow ' + 'keyword arguments')\n        return (args, dict(kwargs) if kwargs else {})\n\n    def parse_type_arg(self):\n        \"\"\"\n        type_arg : datashape\n                 | INTEGER\n                 | STRING\n                 | BOOLEAN\n                 | list_type_arg\n        list_type_arg : LBRACKET RBRACKET\n                      | LBRACKET datashape_list RBRACKET\n                      | LBRACKET integer_list RBRACKET\n                      | LBRACKET string_list RBRACKET\n\n        Returns a type_arg value, or None.\n        \"\"\"\n        ds = self.parse_datashape()\n        if ds is not None:\n            return ds\n        if self.tok.id in [lexer.INTEGER, lexer.STRING, lexer.BOOLEAN]:\n            val = self.tok.val\n            self.advance_tok()\n            return val\n        elif self.tok.id == lexer.LBRACKET:\n            self.advance_tok()\n            val = self.parse_datashape_list()\n            if val is None:\n                val = self.parse_integer_list()\n            if val is None:\n                val = self.parse_string_list()\n            if val is None:\n                val = self.parse_boolean_list()\n            if self.tok.id == lexer.RBRACKET:\n                self.advance_tok()\n                return [] if val is None else val\n            elif val is None:\n                self.raise_error('Expected a type constructor argument ' + 'or a closing \"]\"')\n            else:\n                self.raise_error('Expected a \",\" or a closing \"]\"')\n        else:\n            return None\n\n    def parse_type_kwarg(self):\n        \"\"\"\n        type_kwarg : NAME_LOWER EQUAL type_arg\n\n        Returns a (name, type_arg) tuple, or None.\n        \"\"\"\n        if self.tok.id != lexer.NAME_LOWER:\n            return None\n        saved_pos = self.pos\n        name = self.tok.val\n        self.advance_tok()\n        if self.tok.id != lexer.EQUAL:\n            self.pos = saved_pos\n            return None\n        self.advance_tok()\n        arg = self.parse_type_arg()\n        if arg is not None:\n            return (name, arg)\n        else:\n            self.raise_error('Expected a type constructor argument')\n\n    def parse_datashape_list(self):\n        \"\"\"\n        datashape_list : datashape COMMA datashape_list\n                       | datashape\n\n        Returns a list of datashape type objects, or None.\n        \"\"\"\n        return self.parse_homogeneous_list(self.parse_datashape, lexer.COMMA, 'Expected another datashape, ' + 'type constructor parameter ' + 'lists must have uniform type')\n\n    def parse_integer(self):\n        \"\"\"\n        integer : INTEGER\n        \"\"\"\n        if self.tok.id == lexer.INTEGER:\n            val = self.tok.val\n            self.advance_tok()\n            return val\n        else:\n            return None\n\n    def parse_integer_list(self):\n        \"\"\"\n        integer_list : INTEGER COMMA integer_list\n                     | INTEGER\n\n        Returns a list of integers, or None.\n        \"\"\"\n        return self.parse_homogeneous_list(self.parse_integer, lexer.COMMA, 'Expected another integer, ' + 'type constructor parameter ' + 'lists must have uniform type')\n\n    def parse_boolean(self):\n        \"\"\"\n        boolean : BOOLEAN\n        \"\"\"\n        if self.tok.id == lexer.BOOLEAN:\n            val = self.tok.val\n            self.advance_tok()\n            return val\n        else:\n            return None\n\n    def parse_boolean_list(self):\n        \"\"\"\n        boolean_list : boolean COMMA boolean_list\n                     | boolean\n\n        Returns a list of booleans, or None.\n        \"\"\"\n        return self.parse_homogeneous_list(self.parse_boolean, lexer.COMMA, 'Expected another boolean, ' + 'type constructor parameter ' + 'lists must have uniform type')\n\n    def parse_string(self):\n        \"\"\"\n        string : STRING\n        \"\"\"\n        if self.tok.id == lexer.STRING:\n            val = self.tok.val\n            self.advance_tok()\n            return val\n        else:\n            return None\n\n    def parse_string_list(self):\n        \"\"\"\n        string_list : STRING COMMA string_list\n                    | STRING\n\n        Returns a list of strings, or None.\n        \"\"\"\n        return self.parse_homogeneous_list(self.parse_string, lexer.COMMA, 'Expected another string, ' + 'type constructor parameter ' + 'lists must have uniform type')\n\n    def parse_struct_type(self):\n        \"\"\"\n        struct_type : LBRACE struct_field_list RBRACE\n                    | LBRACE struct_field_list COMMA RBRACE\n\n        Returns a struct type, or None.\n        \"\"\"\n        if self.tok.id != lexer.LBRACE:\n            return None\n        saved_pos = self.pos\n        self.advance_tok()\n        fields = self.parse_homogeneous_list(self.parse_struct_field, lexer.COMMA, 'Invalid field in struct', trailing_sep=True) or []\n        if self.tok.id != lexer.RBRACE:\n            self.raise_error('Invalid field in struct')\n        self.advance_tok()\n        names = [f[0] for f in fields]\n        types = [f[1] for f in fields]\n        tconstr = self.syntactic_sugar(self.sym.dtype_constr, 'struct', '{...} dtype constructor', saved_pos)\n        return tconstr(names, types)\n\n    def parse_struct_field(self):\n        \"\"\"\n        struct_field : struct_field_name COLON datashape\n        struct_field_name : NAME_LOWER\n                          | NAME_UPPER\n                          | NAME_OTHER\n                          | STRING\n\n        Returns a tuple (name, datashape object) or None\n        \"\"\"\n        if self.tok.id not in [lexer.NAME_LOWER, lexer.NAME_UPPER, lexer.NAME_OTHER, lexer.STRING]:\n            return None\n        name = self.tok.val\n        self.advance_tok()\n        if self.tok.id != lexer.COLON:\n            self.raise_error('Expected a \":\" separating the field ' + 'name from its datashape')\n        self.advance_tok()\n        ds = self.parse_datashape()\n        if ds is None:\n            self.raise_error('Expected the datashape of the field')\n        return (name, ds)\n\n    def parse_funcproto_or_tuple_type(self):\n        \"\"\"\n        funcproto_or_tuple_type : tuple_type RARROW datashape\n                                | tuple_type\n        tuple_type : LPAREN tuple_item_list RPAREN\n                   | LPAREN tuple_item_list COMMA RPAREN\n                   | LPAREN RPAREN\n        tuple_item_list : datashape COMMA tuple_item_list\n                        | datashape\n\n        Returns a tuple type object, a function prototype, or None.\n        \"\"\"\n        if self.tok.id != lexer.LPAREN:\n            return None\n        saved_pos = self.pos\n        self.advance_tok()\n        dshapes = self.parse_homogeneous_list(self.parse_datashape, lexer.COMMA, 'Invalid datashape in tuple', trailing_sep=True) or ()\n        if self.tok.id != lexer.RPAREN:\n            self.raise_error('Invalid datashape in tuple')\n        self.advance_tok()\n        if self.tok.id != lexer.RARROW:\n            tconstr = self.syntactic_sugar(self.sym.dtype_constr, 'tuple', '(...) dtype constructor', saved_pos)\n            return tconstr(dshapes)\n        else:\n            self.advance_tok()\n            ret_dshape = self.parse_datashape()\n            if ret_dshape is None:\n                self.raise_error('Expected function prototype return ' + 'datashape')\n            tconstr = self.syntactic_sugar(self.sym.dtype_constr, 'funcproto', '(...) -> ... dtype constructor', saved_pos)\n            return tconstr(dshapes, ret_dshape)",
    "datashader/datatypes.py": "from __future__ import annotations\nimport re\nfrom functools import total_ordering\nfrom packaging.version import Version\nimport numpy as np\nimport pandas as pd\nfrom numba import jit\nfrom pandas.api.extensions import ExtensionDtype, ExtensionArray, register_extension_dtype\nfrom numbers import Integral\nfrom pandas.api.types import pandas_dtype, is_extension_array_dtype\ntry:\n    from dask.dataframe.extensions import make_array_nonempty\nexcept ImportError:\n    make_array_nonempty = None\n\ndef _validate_ragged_properties(start_indices, flat_array):\n    \"\"\"\n    Validate that start_indices are flat_array arrays that may be used to\n    represent a valid RaggedArray.\n\n    Parameters\n    ----------\n    flat_array: numpy array containing concatenation\n                of all nested arrays to be represented\n                by this ragged array\n    start_indices: unsigned integer numpy array the same\n                   length as the ragged array where values\n                   represent the index into flat_array where\n                   the corresponding ragged array element\n                   begins\n    Raises\n    ------\n    ValueError:\n        if input arguments are invalid or incompatible properties\n    \"\"\"\n    if not isinstance(start_indices, np.ndarray) or start_indices.dtype.kind != 'u' or start_indices.ndim != 1:\n        raise ValueError(\"\\nThe start_indices property of a RaggedArray must be a 1D numpy array of\\nunsigned integers (start_indices.dtype.kind == 'u')\\n    Received value of type {typ}: {v}\".format(typ=type(start_indices), v=repr(start_indices)))\n    if not isinstance(flat_array, np.ndarray) or flat_array.ndim != 1:\n        raise ValueError('\\nThe flat_array property of a RaggedArray must be a 1D numpy array\\n    Received value of type {typ}: {v}'.format(typ=type(flat_array), v=repr(flat_array)))\n    invalid_inds = start_indices > len(flat_array)\n    if invalid_inds.any():\n        some_invalid_vals = start_indices[invalid_inds[:10]]\n        raise ValueError('\\nElements of start_indices must be less than the length of flat_array ({m})\\n    Invalid values include: {vals}'.format(m=len(flat_array), vals=repr(some_invalid_vals)))\n\n@total_ordering\nclass _RaggedElement:\n\n    @staticmethod\n    def ragged_or_nan(a):\n        if np.isscalar(a) and np.isnan(a):\n            return a\n        else:\n            return _RaggedElement(a)\n\n    @staticmethod\n    def array_or_nan(a):\n        if np.isscalar(a) and np.isnan(a):\n            return a\n        else:\n            return a.array\n\n    def __init__(self, array):\n        self.array = array\n\n    def __hash__(self):\n        return hash(self.array.tobytes())\n\n    def __eq__(self, other):\n        if not isinstance(other, _RaggedElement):\n            return False\n        return np.array_equal(self.array, other.array)\n\n    def __lt__(self, other):\n        if not isinstance(other, _RaggedElement):\n            return NotImplemented\n        return _lexograph_lt(self.array, other.array)\n\n    def __repr__(self):\n        array_repr = repr(self.array)\n        return array_repr.replace('array', 'ragged_element')\n\n@register_extension_dtype\nclass RaggedDtype(ExtensionDtype):\n    \"\"\"\n    Pandas ExtensionDtype to represent a ragged array datatype\n\n    Methods not otherwise documented here are inherited from ExtensionDtype;\n    please see the corresponding method on that class for the docstring\n    \"\"\"\n    type = np.ndarray\n    base = np.dtype('O')\n    _subtype_re = re.compile('^ragged\\\\[(?P<subtype>\\\\w+)\\\\]$')\n    _metadata = ('_dtype',)\n\n    @property\n    def name(self):\n        return 'Ragged[{subtype}]'.format(subtype=self.subtype)\n\n    def __repr__(self):\n        return self.name\n\n    @classmethod\n    def construct_array_type(cls):\n        return RaggedArray\n\n    @classmethod\n    def construct_from_string(cls, string):\n        if not isinstance(string, str):\n            raise TypeError(\"'construct_from_string' expects a string, got %s\" % type(string))\n        string = string.lower()\n        msg = \"Cannot construct a 'RaggedDtype' from '{}'\"\n        if string.startswith('ragged'):\n            try:\n                subtype_string = cls._parse_subtype(string)\n                return RaggedDtype(dtype=subtype_string)\n            except Exception:\n                raise TypeError(msg.format(string))\n        else:\n            raise TypeError(msg.format(string))\n\n    def __init__(self, dtype=np.float64):\n        if isinstance(dtype, RaggedDtype):\n            self._dtype = dtype.subtype\n        else:\n            self._dtype = np.dtype(dtype)\n\n    @property\n    def subtype(self):\n        return self._dtype\n\n    @classmethod\n    def _parse_subtype(cls, dtype_string):\n        \"\"\"\n        Parse a datatype string to get the subtype\n\n        Parameters\n        ----------\n        dtype_string: str\n            A string like Ragged[subtype]\n\n        Returns\n        -------\n        subtype: str\n\n        Raises\n        ------\n        ValueError\n            When the subtype cannot be extracted\n        \"\"\"\n        dtype_string = dtype_string.lower()\n        match = cls._subtype_re.match(dtype_string)\n        if match:\n            subtype_string = match.groupdict()['subtype']\n        elif dtype_string == 'ragged':\n            subtype_string = 'float64'\n        else:\n            raise ValueError('Cannot parse {dtype_string}'.format(dtype_string=dtype_string))\n        return subtype_string\n\ndef missing(v):\n    return v is None or (np.isscalar(v) and np.isnan(v))\n\nclass RaggedArray(ExtensionArray):\n    \"\"\"\n    Pandas ExtensionArray to represent ragged arrays\n\n    Methods not otherwise documented here are inherited from ExtensionArray;\n    please see the corresponding method on that class for the docstring\n    \"\"\"\n\n    def __init__(self, data, dtype=None, copy=False):\n        \"\"\"\n        Construct a RaggedArray\n\n        Parameters\n        ----------\n        data: list or array or dict or RaggedArray\n            * list or 1D-array: A List or 1D array of lists or 1D arrays that\n                                should be represented by the RaggedArray\n\n            * dict: A dict containing 'start_indices' and 'flat_array' keys\n                    with numpy array values where:\n                    - flat_array:  numpy array containing concatenation\n                                   of all nested arrays to be represented\n                                   by this ragged array\n                    - start_indices: unsigned integer numpy array the same\n                                     length as the ragged array where values\n                                     represent the index into flat_array where\n                                     the corresponding ragged array element\n                                     begins\n            * RaggedArray: A RaggedArray instance to copy\n\n        dtype: RaggedDtype or np.dtype or str or None (default None)\n            Datatype to use to store underlying values from data.\n            If none (the default) then dtype will be determined using the\n            numpy.result_type function.\n        copy : bool (default False)\n            Whether to deep copy the input arrays. Only relevant when `data`\n            has type `dict` or `RaggedArray`. When data is a `list` or\n            `array`, input arrays are always copied.\n        \"\"\"\n        if isinstance(data, dict) and all((k in data for k in ['start_indices', 'flat_array'])):\n            _validate_ragged_properties(start_indices=data['start_indices'], flat_array=data['flat_array'])\n            self._start_indices = data['start_indices']\n            self._flat_array = data['flat_array']\n            dtype = self._flat_array.dtype\n            if copy:\n                self._start_indices = self._start_indices.copy()\n                self._flat_array = self._flat_array.copy()\n        elif isinstance(data, RaggedArray):\n            self._flat_array = data.flat_array\n            self._start_indices = data.start_indices\n            dtype = self._flat_array.dtype\n            if copy:\n                self._start_indices = self._start_indices.copy()\n                self._flat_array = self._flat_array.copy()\n        else:\n            index_len = len(data)\n            buffer_len = sum((len(datum) if not missing(datum) else 0 for datum in data))\n            for nbits in [8, 16, 32, 64]:\n                start_indices_dtype = 'uint' + str(nbits)\n                max_supported = np.iinfo(start_indices_dtype).max\n                if buffer_len <= max_supported:\n                    break\n            if dtype is None:\n                non_missing = [np.atleast_1d(v) for v in data if not missing(v)]\n                if non_missing:\n                    dtype = np.result_type(*non_missing)\n                else:\n                    dtype = 'float64'\n            elif isinstance(dtype, RaggedDtype):\n                dtype = dtype.subtype\n            self._start_indices = np.zeros(index_len, dtype=start_indices_dtype)\n            self._flat_array = np.zeros(buffer_len, dtype=dtype)\n            next_start_ind = 0\n            for i, array_el in enumerate(data):\n                n = len(array_el) if not missing(array_el) else 0\n                self._start_indices[i] = next_start_ind\n                if not n:\n                    continue\n                self._flat_array[next_start_ind:next_start_ind + n] = array_el\n                next_start_ind += n\n        self._dtype = RaggedDtype(dtype=dtype)\n\n    def __eq__(self, other):\n        if isinstance(other, RaggedArray):\n            if len(other) != len(self):\n                raise ValueError('\\nCannot check equality of RaggedArray values of unequal length\\n    len(ra1) == {len_ra1}\\n    len(ra2) == {len_ra2}'.format(len_ra1=len(self), len_ra2=len(other)))\n            result = _eq_ragged_ragged(self.start_indices, self.flat_array, other.start_indices, other.flat_array)\n        else:\n            if not isinstance(other, np.ndarray):\n                other_array = np.asarray(other)\n            else:\n                other_array = other\n            if other_array.ndim == 1 and other_array.dtype.kind != 'O':\n                result = _eq_ragged_scalar(self.start_indices, self.flat_array, other_array)\n            elif other_array.ndim == 1 and other_array.dtype.kind == 'O' and (len(other_array) == len(self)):\n                result = _eq_ragged_ndarray1d(self.start_indices, self.flat_array, other_array)\n            elif other_array.ndim == 2 and other_array.dtype.kind != 'O' and (other_array.shape[0] == len(self)):\n                result = _eq_ragged_ndarray2d(self.start_indices, self.flat_array, other_array)\n            else:\n                raise ValueError('\\nCannot check equality of RaggedArray of length {ra_len} with:\\n    {other}'.format(ra_len=len(self), other=repr(other)))\n        return result\n\n    def __ne__(self, other):\n        return np.logical_not(self == other)\n\n    @property\n    def flat_array(self):\n        \"\"\"\n        numpy array containing concatenation of all nested arrays\n\n        Returns\n        -------\n        np.ndarray\n        \"\"\"\n        return self._flat_array\n\n    @property\n    def start_indices(self):\n        \"\"\"\n        unsigned integer numpy array the same length as the ragged array where\n        values represent the index into flat_array where the corresponding\n        ragged array element begins\n\n        Returns\n        -------\n        np.ndarray\n        \"\"\"\n        return self._start_indices\n\n    def __len__(self):\n        return len(self._start_indices)\n\n    def __getitem__(self, item):\n        err_msg = 'Only integers, slices and integer or booleanarrays are valid indices.'\n        if isinstance(item, Integral):\n            if item < -len(self) or item >= len(self):\n                raise IndexError('{item} is out of bounds'.format(item=item))\n            else:\n                if item < 0:\n                    item += len(self)\n                slice_start = self.start_indices[item]\n                slice_end = self.start_indices[item + 1] if item + 1 <= len(self) - 1 else len(self.flat_array)\n                return self.flat_array[slice_start:slice_end] if slice_end != slice_start else np.nan\n        elif type(item) is slice:\n            data = []\n            selected_indices = np.arange(len(self))[item]\n            for selected_index in selected_indices:\n                data.append(self[selected_index])\n            return RaggedArray(data, dtype=self.flat_array.dtype)\n        elif isinstance(item, (np.ndarray, ExtensionArray, list, tuple)):\n            if isinstance(item, (np.ndarray, ExtensionArray)):\n                kind = item.dtype.kind\n            else:\n                item = pd.array(item)\n                kind = item.dtype.kind\n            if len(item) == 0:\n                return self.take([], allow_fill=False)\n            elif kind == 'b':\n                if len(item) != len(self):\n                    raise IndexError('Boolean index has wrong length: {} instead of {}'.format(len(item), len(self)))\n                isna = pd.isna(item)\n                if isna.any():\n                    if Version(pd.__version__) > Version('1.0.1'):\n                        item[isna] = False\n                    else:\n                        raise ValueError('Cannot mask with a boolean indexer containing NA values')\n                data = []\n                for i, m in enumerate(item):\n                    if m:\n                        data.append(self[i])\n                return RaggedArray(data, dtype=self.flat_array.dtype)\n            elif kind in ('i', 'u'):\n                if any(pd.isna(item)):\n                    raise ValueError('Cannot index with an integer indexer containing NA values')\n                return self.take(item, allow_fill=False)\n            else:\n                raise IndexError(err_msg)\n        else:\n            raise IndexError(err_msg)\n\n    @classmethod\n    def _from_sequence(cls, scalars, dtype=None, copy=False):\n        return RaggedArray(scalars, dtype=dtype)\n\n    @classmethod\n    def _from_factorized(cls, values, original):\n        return RaggedArray([_RaggedElement.array_or_nan(v) for v in values], dtype=original.flat_array.dtype)\n\n    def _as_ragged_element_array(self):\n        return np.array([_RaggedElement.ragged_or_nan(self[i]) for i in range(len(self))])\n\n    def _values_for_factorize(self):\n        return (self._as_ragged_element_array(), np.nan)\n\n    def _values_for_argsort(self):\n        return self._as_ragged_element_array()\n\n    def unique(self):\n        from pandas import unique\n        uniques = unique(self._as_ragged_element_array())\n        return self._from_sequence([_RaggedElement.array_or_nan(v) for v in uniques], dtype=self.dtype)\n\n    def fillna(self, value=None, method=None, limit=None):\n        from pandas.util._validators import validate_fillna_kwargs\n        from pandas.core.missing import get_fill_func\n        value, method = validate_fillna_kwargs(value, method)\n        mask = self.isna()\n        if isinstance(value, RaggedArray):\n            if len(value) != len(self):\n                raise ValueError(\"Length of 'value' does not match. Got ({})  expected {}\".format(len(value), len(self)))\n            value = value[mask]\n        if mask.any():\n            if method is not None:\n                func = get_fill_func(method)\n                new_values = func(self.astype(object), limit=limit, mask=mask)\n                new_values = self._from_sequence(new_values, dtype=self.dtype)\n            else:\n                new_values = list(self)\n                mask_indices, = np.where(mask)\n                for ind in mask_indices:\n                    new_values[ind] = value\n                new_values = self._from_sequence(new_values, dtype=self.dtype)\n        else:\n            new_values = self.copy()\n        return new_values\n\n    def shift(self, periods=1, fill_value=None):\n        if not len(self) or periods == 0:\n            return self.copy()\n        if fill_value is None:\n            fill_value = np.nan\n        empty = self._from_sequence([fill_value] * min(abs(periods), len(self)), dtype=self.dtype)\n        if periods > 0:\n            a = empty\n            b = self[:-periods]\n        else:\n            a = self[abs(periods):]\n            b = empty\n        return self._concat_same_type([a, b])\n\n    def searchsorted(self, value, side='left', sorter=None):\n        arr = self._as_ragged_element_array()\n        if isinstance(value, RaggedArray):\n            search_value = value._as_ragged_element_array()\n        else:\n            search_value = _RaggedElement(value)\n        return arr.searchsorted(search_value, side=side, sorter=sorter)\n\n    def isna(self):\n        stop_indices = np.hstack([self.start_indices[1:], [len(self.flat_array)]])\n        element_lengths = stop_indices - self.start_indices\n        return element_lengths == 0\n\n    def take(self, indices, allow_fill=False, fill_value=None):\n        if allow_fill:\n            invalid_inds = [i for i in indices if i < -1]\n            if invalid_inds:\n                raise ValueError('\\nInvalid indices for take with allow_fill True: {inds}'.format(inds=invalid_inds[:9]))\n            sequence = [self[i] if i >= 0 else fill_value for i in indices]\n        else:\n            if len(self) == 0 and len(indices) > 0:\n                raise IndexError('cannot do a non-empty take from an empty axis|out of bounds')\n            sequence = [self[i] for i in indices]\n        return RaggedArray(sequence, dtype=self.flat_array.dtype)\n\n    def copy(self, deep=False):\n        data = dict(flat_array=self.flat_array, start_indices=self.start_indices)\n        return RaggedArray(data, copy=deep)\n\n    @classmethod\n    def _concat_same_type(cls, to_concat):\n        flat_array = np.hstack([ra.flat_array for ra in to_concat])\n        offsets = np.hstack([[0], np.cumsum([len(ra.flat_array) for ra in to_concat[:-1]])]).astype('uint64')\n        start_indices = np.hstack([ra.start_indices + offset for offset, ra in zip(offsets, to_concat)])\n        return RaggedArray(dict(flat_array=flat_array, start_indices=start_indices), copy=False)\n\n    @property\n    def dtype(self):\n        return self._dtype\n\n    @property\n    def nbytes(self):\n        return self._flat_array.nbytes + self._start_indices.nbytes\n\n    def astype(self, dtype, copy=True):\n        dtype = pandas_dtype(dtype)\n        if isinstance(dtype, RaggedDtype):\n            if copy:\n                return self.copy()\n            return self\n        elif is_extension_array_dtype(dtype):\n            return dtype.construct_array_type()._from_sequence(np.asarray(self))\n        return np.array([v for v in self], dtype=dtype)\n\n    def tolist(self):\n        if self.ndim > 1:\n            return [item.tolist() for item in self]\n        else:\n            return list(self)\n\n    def __array__(self, dtype=None, copy=True):\n        dtype = np.dtype(object) if dtype is None else np.dtype(dtype)\n        if copy:\n            return np.array(self.tolist(), dtype=dtype)\n        else:\n            return np.array(self, dtype=dtype)\n\n    def duplicated(self, *args, **kwargs):\n        msg = 'duplicated is not implemented for RaggedArray'\n        raise NotImplementedError(msg)\n\n@jit(nopython=True, nogil=True)\ndef _eq_ragged_ragged(start_indices1, flat_array1, start_indices2, flat_array2):\n    \"\"\"\n    Compare elements of two ragged arrays of the same length\n\n    Parameters\n    ----------\n    start_indices1: ndarray\n        start indices of a RaggedArray 1\n    flat_array1: ndarray\n        flat_array property of a RaggedArray 1\n    start_indices2: ndarray\n        start indices of a RaggedArray 2\n    flat_array2: ndarray\n        flat_array property of a RaggedArray 2\n\n    Returns\n    -------\n    mask: ndarray\n        1D bool array of same length as inputs with elements True when\n        corresponding elements are equal, False otherwise\n    \"\"\"\n    n = len(start_indices1)\n    m1 = len(flat_array1)\n    m2 = len(flat_array2)\n    result = np.zeros(n, dtype=np.bool_)\n    for i in range(n):\n        start_index1 = start_indices1[i]\n        stop_index1 = start_indices1[i + 1] if i < n - 1 else m1\n        len_1 = stop_index1 - start_index1\n        start_index2 = start_indices2[i]\n        stop_index2 = start_indices2[i + 1] if i < n - 1 else m2\n        len_2 = stop_index2 - start_index2\n        if len_1 != len_2:\n            el_equal = False\n        else:\n            el_equal = True\n            for flat_index1, flat_index2 in zip(range(start_index1, stop_index1), range(start_index2, stop_index2)):\n                el_1 = flat_array1[flat_index1]\n                el_2 = flat_array2[flat_index2]\n                el_equal &= el_1 == el_2\n        result[i] = el_equal\n    return result\n\n@jit(nopython=True, nogil=True)\ndef _eq_ragged_scalar(start_indices, flat_array, val):\n    \"\"\"\n    Compare elements of a RaggedArray with a scalar array\n\n    Parameters\n    ----------\n    start_indices: ndarray\n        start indices of a RaggedArray\n    flat_array: ndarray\n        flat_array property of a RaggedArray\n    val: ndarray\n\n    Returns\n    -------\n    mask: ndarray\n        1D bool array of same length as inputs with elements True when\n        ragged element equals scalar val, False otherwise.\n    \"\"\"\n    n = len(start_indices)\n    m = len(flat_array)\n    cols = len(val)\n    result = np.zeros(n, dtype=np.bool_)\n    for i in range(n):\n        start_index = start_indices[i]\n        stop_index = start_indices[i + 1] if i < n - 1 else m\n        if stop_index - start_index != cols:\n            el_equal = False\n        else:\n            el_equal = True\n            for val_index, flat_index in enumerate(range(start_index, stop_index)):\n                el_equal &= flat_array[flat_index] == val[val_index]\n        result[i] = el_equal\n    return result\n\ndef _eq_ragged_ndarray1d(start_indices, flat_array, a):\n    \"\"\"\n    Compare a RaggedArray with a 1D numpy object array of the same length\n\n    Parameters\n    ----------\n    start_indices: ndarray\n        start indices of a RaggedArray\n    flat_array: ndarray\n        flat_array property of a RaggedArray\n    a: ndarray\n        1D numpy array of same length as ra\n\n    Returns\n    -------\n    mask: ndarray\n        1D bool array of same length as input with elements True when\n        corresponding elements are equal, False otherwise\n\n    Notes\n    -----\n    This function is not numba accelerated because it, by design, inputs\n    a numpy object array\n    \"\"\"\n    n = len(start_indices)\n    m = len(flat_array)\n    result = np.zeros(n, dtype=np.bool_)\n    for i in range(n):\n        start_index = start_indices[i]\n        stop_index = start_indices[i + 1] if i < n - 1 else m\n        a_val = a[i]\n        if a_val is None or (np.isscalar(a_val) and np.isnan(a_val)) or len(a_val) == 0:\n            result[i] = start_index == stop_index\n        else:\n            result[i] = np.array_equal(flat_array[start_index:stop_index], a_val)\n    return result\n\n@jit(nopython=True, nogil=True)\ndef _eq_ragged_ndarray2d(start_indices, flat_array, a):\n    \"\"\"\n    Compare a RaggedArray with rows of a 2D numpy object array\n\n    Parameters\n    ----------\n    start_indices: ndarray\n        start indices of a RaggedArray\n    flat_array: ndarray\n        flat_array property of a RaggedArray\n    a: ndarray\n        A 2D numpy array where the length of the first dimension matches the\n        length of the RaggedArray\n\n    Returns\n    -------\n    mask: ndarray\n        1D bool array of same length as input RaggedArray with elements True\n        when corresponding elements of ra equal corresponding row of `a`\n    \"\"\"\n    n = len(start_indices)\n    m = len(flat_array)\n    cols = a.shape[1]\n    result = np.zeros(n, dtype=np.bool_)\n    for row in range(n):\n        start_index = start_indices[row]\n        stop_index = start_indices[row + 1] if row < n - 1 else m\n        if stop_index - start_index != cols:\n            el_equal = False\n        else:\n            el_equal = True\n            for col, flat_index in enumerate(range(start_index, stop_index)):\n                el_equal &= flat_array[flat_index] == a[row, col]\n        result[row] = el_equal\n    return result\n\n@jit(nopython=True, nogil=True)\ndef _lexograph_lt(a1, a2):\n    \"\"\"\n    Compare two 1D numpy arrays lexographically\n    Parameters\n    ----------\n    a1: ndarray\n        1D numpy array\n    a2: ndarray\n        1D numpy array\n\n    Returns\n    -------\n    comparison:\n        True if a1 < a2, False otherwise\n    \"\"\"\n    for e1, e2 in zip(a1, a2):\n        if e1 < e2:\n            return True\n        elif e1 > e2:\n            return False\n    return len(a1) < len(a2)\n\ndef ragged_array_non_empty(dtype):\n    return RaggedArray([[1], [1, 2]], dtype=dtype)\nif make_array_nonempty:\n    make_array_nonempty.register(RaggedDtype)(ragged_array_non_empty)",
    "datashader/datashape/validation.py": "\"\"\"\nDatashape validation.\n\"\"\"\nfrom . import coretypes as T\n\ndef traverse(f, t):\n    \"\"\"\n    Map `f` over `t`, calling `f` with type `t` and the map result of the\n    mapping `f` over `t` 's parameters.\n\n    Parameters\n    ----------\n    f : callable\n    t : DataShape\n\n    Returns\n    -------\n    DataShape\n    \"\"\"\n    if isinstance(t, T.Mono) and (not isinstance(t, T.Unit)):\n        return f(t, [traverse(f, p) for p in t.parameters])\n    return t\n\ndef _validate(ds, params):\n    if isinstance(ds, T.DataShape):\n        ellipses = [x for x in ds.parameters if isinstance(x, T.Ellipsis)]\n        if len(ellipses) > 1:\n            raise TypeError('Can only use a single wildcard')\n        elif isinstance(ds.parameters[-1], T.Ellipsis):\n            raise TypeError('Measure may not be an Ellipsis (...)')"
  }
}