{
  "dir_path": "/app/skforecast",
  "package_name": "skforecast",
  "sample_name": "skforecast-test_binning_in_sample_residuals",
  "src_dir": "skforecast/",
  "test_dir": "tests/",
  "test_file": "skforecast/recursive/tests/tests_forecaster_recursive/test_binning_in_sample_residuals.py",
  "test_code": "# Unit test _binning_in_sample_residuals ForecasterRecursive\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom skforecast.recursive import ForecasterRecursive\n\n\ndef test_binning_in_sample_residuals_output():\n    \"\"\"\n    Test that _binning_in_sample_residuals returns the expected output.\n    \"\"\"\n\n    forecaster = ForecasterRecursive(\n        regressor=object(),\n        lags = 5,\n        binner_kwargs={'n_bins': 3}\n    )\n\n    rng = np.random.default_rng(12345)\n    y_pred = rng.normal(100, 15, 20)\n    y_true = rng.normal(100, 10, 20)\n\n    forecaster._binning_in_sample_residuals(\n        y_pred=y_pred,\n        y_true=y_true,\n    )\n\n    expected_1 = np.array([\n                     34.58035615,  22.08911948,  15.6081091 ,   7.0808798 ,\n                     55.47454021,   1.80092458,  12.2624188 , -12.32822882,\n                     -0.451743  ,  11.83152763,  -7.11862253,   6.32581108,\n                     -20.92461257, -21.95291202, -10.55026794, -27.43753138,\n                     -6.24112163, -25.62685698,  -4.31327121, -18.40910724\n                 ])\n\n    expected_2 = {\n        0: np.array([\n               34.58035615, 22.08911948, 15.6081091,  7.0808798, 55.47454021,\n               1.80092458, 12.2624188\n           ]),\n        1: np.array([\n               -12.32822882,  -0.45174300,  11.83152763,  -7.11862253,\n               6.32581108, -20.92461257\n           ]),\n        2: np.array([\n               -21.95291202, -10.55026794, -27.43753138,  -6.24112163,\n               -25.62685698,  -4.31327121, -18.40910724\n           ])\n    }\n\n    expected_3 = {\n        0: (70.70705405481715, 90.25638761254116),\n        1: (90.25638761254116, 109.36821559391004),\n        2: (109.36821559391004, 135.2111448156828)\n    }\n\n    np.testing.assert_almost_equal(forecaster.in_sample_residuals_, expected_1)\n    for k in expected_2.keys():\n        np.testing.assert_almost_equal(forecaster.in_sample_residuals_by_bin_[k], expected_2[k])\n    assert forecaster.binner_intervals_ == expected_3\n\n\ndef test_binning_in_sample_residuals_stores_maximum_10000_residuals():\n    \"\"\"\n    Test that maximum 10000 residuals are stored.\n    \"\"\"\n\n    n = 15000\n    y = pd.Series(\n            data = np.random.normal(loc=10, scale=1, size=n),\n            index = pd.date_range(start='01-01-2000', periods=n, freq='h')\n        )\n    forecaster = ForecasterRecursive(\n                    regressor=LinearRegression(),\n                    lags = 5,\n                    binner_kwargs={'n_bins': 2}\n                )\n    forecaster.fit(y)\n    max_residuals_per_bin = int(10_000 // forecaster.binner.n_bins_)\n\n    for v in forecaster.in_sample_residuals_by_bin_.values():\n        assert len(v) == max_residuals_per_bin\n        assert len(v) > 0\n\n    np.testing.assert_array_almost_equal(\n        forecaster.in_sample_residuals_,\n        np.concatenate(list(forecaster.in_sample_residuals_by_bin_.values()))\n    )\n\n    assert len(forecaster.in_sample_residuals_) == 10_000\n",
  "GT_file_code": {
    "skforecast/recursive/_forecaster_recursive.py": "################################################################################\n#                           ForecasterRecursive                                #\n#                                                                              #\n# This work by skforecast team is licensed under the BSD 3-Clause License.     #\n################################################################################\n# coding=utf-8\n\nfrom typing import Union, Tuple, Optional, Callable\nimport warnings\nimport sys\nimport numpy as np\nimport pandas as pd\nimport inspect\nfrom copy import copy\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import clone\n\nimport skforecast\nfrom ..base import ForecasterBase\nfrom ..exceptions import DataTransformationWarning\nfrom ..utils import (\n    initialize_lags,\n    initialize_window_features,\n    initialize_weights,\n    check_select_fit_kwargs,\n    check_y,\n    check_exog,\n    get_exog_dtypes,\n    check_exog_dtypes,\n    check_predict_input,\n    check_interval,\n    preprocess_y,\n    preprocess_last_window,\n    preprocess_exog,\n    input_to_frame,\n    date_to_index_position,\n    expand_index,\n    transform_numpy,\n    transform_dataframe,\n)\nfrom ..preprocessing import TimeSeriesDifferentiator\nfrom ..preprocessing import QuantileBinner\n\n\nclass ForecasterRecursive(ForecasterBase):\n    \"\"\"\n    This class turns any regressor compatible with the scikit-learn API into a\n    recursive autoregressive (multi-step) forecaster.\n    \n    Parameters\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n    lags : int, list, numpy ndarray, range, default `None`\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n    \n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n        - `None`: no lags are included as predictors. \n    window_features : object, list, default `None`\n        Instance or list of instances used to create window features. Window features\n        are created from the original time series and are included as predictors.\n    transformer_y : object transformer (preprocessor), default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and inverse_transform.\n        ColumnTransformers are not allowed since they do not have inverse_transform method.\n        The transformation is applied to `y` before training the forecaster. \n    transformer_exog : object transformer (preprocessor), default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable, default `None`\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\n        method. The resulting `sample_weight` cannot have negative values.\n    differentiation : int, default `None`\n        Order of differencing applied to the time series before training the forecaster.\n        If `None`, no differencing is applied. The order of differentiation is the number\n        of times the differencing operation is applied to a time series. Differencing\n        involves computing the differences between consecutive data points in the series.\n        Differentiation is reversed in the output of `predict()` and `predict_interval()`.\n        **WARNING: This argument is newly introduced and requires special attention. It\n        is still experimental and may undergo changes.**\n    fit_kwargs : dict, default `None`\n        Additional arguments to be passed to the `fit` method of the regressor.\n    binner_kwargs : dict, default `None`\n        Additional arguments to pass to the `QuantileBinner` used to discretize \n        the residuals into k bins according to the predicted values associated \n        with each residual. Available arguments are: `n_bins`, `method`, `subsample`,\n        `random_state` and `dtype`. Argument `method` is passed internally to the\n        fucntion `numpy.percentile`.\n        **New in version 0.14.0**\n    forecaster_id : str, int, default `None`\n        Name used as an identifier of the forecaster.\n    \n    Attributes\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n    lags : numpy ndarray\n        Lags used as predictors.\n    lags_names : list\n        Names of the lags used as predictors.\n    max_lag : int\n        Maximum lag included in `lags`.\n    window_features : list\n        Class or list of classes used to create window features.\n    window_features_names : list\n        Names of the window features to be included in the `X_train` matrix.\n    window_features_class_names : list\n        Names of the classes used to create the window features.\n    max_size_window_features : int\n        Maximum window size required by the window features.\n    window_size : int\n        The window size needed to create the predictors. It is calculated as the \n        maximum value between `max_lag` and `max_size_window_features`. If \n        differentiation is used, `window_size` is increased by n units equal to \n        the order of differentiation so that predictors can be generated correctly.\n    transformer_y : object transformer (preprocessor)\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and inverse_transform.\n        ColumnTransformers are not allowed since they do not have inverse_transform method.\n        The transformation is applied to `y` before training the forecaster.\n    transformer_exog : object transformer (preprocessor)\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\n        method. The resulting `sample_weight` cannot have negative values.\n    differentiation : int\n        Order of differencing applied to the time series before training the forecaster.\n        If `None`, no differencing is applied. The order of differentiation is the number\n        of times the differencing operation is applied to a time series. Differencing\n        involves computing the differences between consecutive data points in the series.\n        Differentiation is reversed in the output of `predict()` and `predict_interval()`.\n        **WARNING: This argument is newly introduced and requires special attention. It\n        is still experimental and may undergo changes.**\n        **New in version 0.10.0**\n    binner : sklearn.preprocessing.KBinsDiscretizer\n        `KBinsDiscretizer` used to discretize residuals into k bins according \n        to the predicted values associated with each residual.\n        **New in version 0.12.0**\n    binner_intervals_ : dict\n        Intervals used to discretize residuals into k bins according to the predicted\n        values associated with each residual.\n        **New in version 0.12.0**\n    binner_kwargs : dict\n        Additional arguments to pass to the `QuantileBinner` used to discretize \n        the residuals into k bins according to the predicted values associated \n        with each residual. Available arguments are: `n_bins`, `method`, `subsample`,\n        `random_state` and `dtype`. Argument `method` is passed internally to the\n        fucntion `numpy.percentile`.\n        **New in version 0.14.0**\n    source_code_weight_func : str\n        Source code of the custom function used to create weights.\n    differentiation : int\n        Order of differencing applied to the time series before training the \n        forecaster.\n    differentiator : TimeSeriesDifferentiator\n        Skforecast object used to differentiate the time series.\n    last_window_ : pandas DataFrame\n        This window represents the most recent data observed by the predictor\n        during its training phase. It contains the values needed to predict the\n        next step immediately after the training data. These values are stored\n        in the original scale of the time series before undergoing any transformations\n        or differentiation. When `differentiation` parameter is specified, the\n        dimensions of the `last_window_` are expanded as many values as the order\n        of differentiation. For example, if `lags` = 7 and `differentiation` = 1,\n        `last_window_` will have 8 values.\n    index_type_ : type\n        Type of index of the input used in training.\n    index_freq_ : str\n        Frequency of Index of the input used in training.\n    training_range_ : pandas Index\n        First and last values of index of the data used during training.\n    exog_in_ : bool\n        If the forecaster has been trained using exogenous variable/s.\n    exog_names_in_ : list\n        Names of the exogenous variables used during training.\n    exog_type_in_ : type\n        Type of exogenous data (pandas Series or DataFrame) used in training.\n    exog_dtypes_in_ : dict\n        Type of each exogenous variable/s used in training. If `transformer_exog` \n        is used, the dtypes are calculated before the transformation.\n    X_train_window_features_names_out_ : list\n        Names of the window features included in the matrix `X_train` created\n        internally for training.\n    X_train_exog_names_out_ : list\n        Names of the exogenous variables included in the matrix `X_train` created\n        internally for training. It can be different from `exog_names_in_` if\n        some exogenous variables are transformed during the training process.\n    X_train_features_names_out_ : list\n        Names of columns of the matrix created internally for training.\n    fit_kwargs : dict\n        Additional arguments to be passed to the `fit` method of the regressor.\n    in_sample_residuals_ : numpy ndarray\n        Residuals of the model when predicting training data. Only stored up to\n        10_000 values. If `transformer_y` is not `None`, residuals are stored in\n        the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation.\n    in_sample_residuals_by_bin_ : dict\n        In sample residuals binned according to the predicted value each residual\n        is associated with. If `transformer_y` is not `None`, residuals are stored\n        in the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n        **New in version 0.14.0**\n    out_sample_residuals_ : numpy ndarray\n        Residuals of the model when predicting non training data. Only stored up to\n        10_000 values. If `transformer_y` is not `None`, residuals are stored in\n        the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation.\n    out_sample_residuals_by_bin_ : dict\n        Out of sample residuals binned according to the predicted value each residual\n        is associated with. If `transformer_y` is not `None`, residuals are stored\n        in the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n        **New in version 0.12.0**\n    creation_date : str\n        Date of creation.\n    is_fitted : bool\n        Tag to identify if the regressor has been fitted (trained).\n    fit_date : str\n        Date of last fit.\n    skforecast_version : str\n        Version of skforecast library used to create the forecaster.\n    python_version : str\n        Version of python used to create the forecaster.\n    forecaster_id : str, int\n        Name used as an identifier of the forecaster.\n    \n    \"\"\"\n\n    def __init__(\n        self,\n        regressor: object,\n        lags: Optional[Union[int, list, np.ndarray, range]] = None,\n        window_features: Optional[Union[object, list]] = None,\n        transformer_y: Optional[object] = None,\n        transformer_exog: Optional[object] = None,\n        weight_func: Optional[Callable] = None,\n        differentiation: Optional[int] = None,\n        fit_kwargs: Optional[dict] = None,\n        binner_kwargs: Optional[dict] = None,\n        forecaster_id: Optional[Union[str, int]] = None\n    ) -> None:\n        \n        self.regressor                          = copy(regressor)\n        self.transformer_y                      = transformer_y\n        self.transformer_exog                   = transformer_exog\n        self.weight_func                        = weight_func\n        self.source_code_weight_func            = None\n        self.differentiation                    = differentiation\n        self.differentiator                     = None\n        self.last_window_                       = None\n        self.index_type_                        = None\n        self.index_freq_                        = None\n        self.training_range_                    = None\n        self.exog_in_                           = False\n        self.exog_names_in_                     = None\n        self.exog_type_in_                      = None\n        self.exog_dtypes_in_                    = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_            = None\n        self.X_train_features_names_out_        = None\n        self.in_sample_residuals_               = None\n        self.out_sample_residuals_              = None\n        self.in_sample_residuals_by_bin_        = None\n        self.out_sample_residuals_by_bin_       = None\n        self.creation_date                      = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.is_fitted                          = False\n        self.fit_date                           = None\n        self.skforecast_version                 = skforecast.__version__\n        self.python_version                     = sys.version.split(\" \")[0]\n        self.forecaster_id                      = forecaster_id\n\n        self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\n        self.window_features, self.window_features_names, self.max_size_window_features = (\n            initialize_window_features(window_features)\n        )\n        if self.window_features is None and self.lags is None:\n            raise ValueError(\n                \"At least one of the arguments `lags` or `window_features` \"\n                \"must be different from None. This is required to create the \"\n                \"predictors used in training the forecaster.\"\n            )\n        \n        self.window_size = max(\n            [ws for ws in [self.max_lag, self.max_size_window_features] \n             if ws is not None]\n        )\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [\n                type(wf).__name__ for wf in self.window_features\n            ] \n\n        self.binner_kwargs = binner_kwargs\n        if binner_kwargs is None:\n            self.binner_kwargs = {\n                'n_bins': 10, 'method': 'linear', 'subsample': 200000,\n                'random_state': 789654, 'dtype': np.float64\n            }\n        self.binner = QuantileBinner(**self.binner_kwargs)\n        self.binner_intervals_ = None\n\n        if self.differentiation is not None:\n            if not isinstance(differentiation, int) or differentiation < 1:\n                raise ValueError(\n                    f\"Argument `differentiation` must be an integer equal to or \"\n                    f\"greater than 1. Got {differentiation}.\"\n                )\n            self.window_size += self.differentiation\n            self.differentiator = TimeSeriesDifferentiator(\n                order=self.differentiation, window_size=self.window_size\n            )\n\n        self.weight_func, self.source_code_weight_func, _ = initialize_weights(\n            forecaster_name = type(self).__name__, \n            regressor       = regressor, \n            weight_func     = weight_func, \n            series_weights  = None\n        )\n\n        self.fit_kwargs = check_select_fit_kwargs(\n                              regressor  = regressor,\n                              fit_kwargs = fit_kwargs\n                          )\n\n    def __repr__(\n        self\n    ) -> str:\n        \"\"\"\n        Information displayed when a ForecasterRecursive object is printed.\n        \"\"\"\n\n        (\n            params,\n            _,\n            _,\n            exog_names_in_,\n            _,\n        ) = self._preprocess_repr(\n                regressor      = self.regressor,\n                exog_names_in_ = self.exog_names_in_\n            )\n        \n        params = self._format_text_repr(params)\n        exog_names_in_ = self._format_text_repr(exog_names_in_)\n\n        info = (\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"{type(self).__name__} \\n\"\n            f\"{'=' * len(type(self).__name__)} \\n\"\n            f\"Regressor: {type(self.regressor).__name__} \\n\"\n            f\"Lags: {self.lags} \\n\"\n            f\"Window features: {self.window_features_names} \\n\"\n            f\"Window size: {self.window_size} \\n\"\n            f\"Exogenous included: {self.exog_in_} \\n\"\n            f\"Exogenous names: {exog_names_in_} \\n\"\n            f\"Transformer for y: {self.transformer_y} \\n\"\n            f\"Transformer for exog: {self.transformer_exog} \\n\"\n            f\"Weight function included: {True if self.weight_func is not None else False} \\n\"\n            f\"Differentiation order: {self.differentiation} \\n\"\n            f\"Training range: {self.training_range_.to_list() if self.is_fitted else None} \\n\"\n            f\"Training index type: {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None} \\n\"\n            f\"Training index frequency: {self.index_freq_ if self.is_fitted else None} \\n\"\n            f\"Regressor parameters: {params} \\n\"\n            f\"fit_kwargs: {self.fit_kwargs} \\n\"\n            f\"Creation date: {self.creation_date} \\n\"\n            f\"Last fit date: {self.fit_date} \\n\"\n            f\"Skforecast version: {self.skforecast_version} \\n\"\n            f\"Python version: {self.python_version} \\n\"\n            f\"Forecaster id: {self.forecaster_id} \\n\"\n        )\n\n        return info\n\n\n    def _repr_html_(self):\n        \"\"\"\n        HTML representation of the object.\n        The \"General Information\" section is expanded by default.\n        \"\"\"\n\n        (\n            params,\n            _,\n            _,\n            exog_names_in_,\n            _,\n        ) = self._preprocess_repr(\n                regressor      = self.regressor,\n                exog_names_in_ = self.exog_names_in_\n            )\n\n        style, unique_id = self._get_style_repr_html(self.is_fitted)\n        \n        content = f\"\"\"\n        <div class=\"container-{unique_id}\">\n            <h2>{type(self).__name__}</h2>\n            <details open>\n                <summary>General Information</summary>\n                <ul>\n                    <li><strong>Regressor:</strong> {type(self.regressor).__name__}</li>\n                    <li><strong>Lags:</strong> {self.lags}</li>\n                    <li><strong>Window features:</strong> {self.window_features_names}</li>\n                    <li><strong>Window size:</strong> {self.window_size}</li>\n                    <li><strong>Exogenous included:</strong> {self.exog_in_}</li>\n                    <li><strong>Weight function included:</strong> {self.weight_func is not None}</li>\n                    <li><strong>Differentiation order:</strong> {self.differentiation}</li>\n                    <li><strong>Creation date:</strong> {self.creation_date}</li>\n                    <li><strong>Last fit date:</strong> {self.fit_date}</li>\n                    <li><strong>Skforecast version:</strong> {self.skforecast_version}</li>\n                    <li><strong>Python version:</strong> {self.python_version}</li>\n                    <li><strong>Forecaster id:</strong> {self.forecaster_id}</li>\n                </ul>\n            </details>\n            <details>\n                <summary>Exogenous Variables</summary>\n                <ul>\n                    {exog_names_in_}\n                </ul>\n            </details>\n            <details>\n                <summary>Data Transformations</summary>\n                <ul>\n                    <li><strong>Transformer for y:</strong> {self.transformer_y}</li>\n                    <li><strong>Transformer for exog:</strong> {self.transformer_exog}</li>\n                </ul>\n            </details>\n            <details>\n                <summary>Training Information</summary>\n                <ul>\n                    <li><strong>Training range:</strong> {self.training_range_.to_list() if self.is_fitted else 'Not fitted'}</li>\n                    <li><strong>Training index type:</strong> {str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted'}</li>\n                    <li><strong>Training index frequency:</strong> {self.index_freq_ if self.is_fitted else 'Not fitted'}</li>\n                </ul>\n            </details>\n            <details>\n                <summary>Regressor Parameters</summary>\n                <ul>\n                    {params}\n                </ul>\n            </details>\n            <details>\n                <summary>Fit Kwargs</summary>\n                <ul>\n                    {self.fit_kwargs}\n                </ul>\n            </details>\n            <p>\n                <a href=\"https://skforecast.org/{skforecast.__version__}/api/forecasterrecursive.html\">&#128712 <strong>API Reference</strong></a>\n                &nbsp;&nbsp;\n                <a href=\"https://skforecast.org/{skforecast.__version__}/user_guides/autoregresive-forecaster.html\">&#128462 <strong>User Guide</strong></a>\n            </p>\n        </div>\n        \"\"\"\n\n        # Return the combined style and content\n        return style + content\n\n\n    def _create_lags(\n        self,\n        y: np.ndarray,\n        X_as_pandas: bool = False,\n        train_index: Optional[pd.Index] = None\n    ) -> Tuple[Optional[Union[np.ndarray, pd.DataFrame]], np.ndarray]:\n        \"\"\"\n        Create the lagged values and their target variable from a time series.\n        \n        Note that the returned matrix `X_data` contains the lag 1 in the first \n        column, the lag 2 in the in the second column and so on.\n        \n        Parameters\n        ----------\n        y : numpy ndarray\n            Training time series values.\n        X_as_pandas : bool, default `False`\n            If `True`, the returned matrix `X_data` is a pandas DataFrame.\n        train_index : pandas Index, default `None`\n            Index of the training data. It is used to create the pandas DataFrame\n            `X_data` when `X_as_pandas` is `True`.\n\n        Returns\n        -------\n        X_data : numpy ndarray, pandas DataFrame, None\n            Lagged values (predictors).\n        y_data : numpy ndarray\n            Values of the time series related to each row of `X_data`.\n        \n        \"\"\"\n\n        X_data = None\n        if self.lags is not None:\n            n_rows = len(y) - self.window_size\n            X_data = np.full(\n                shape=(n_rows, len(self.lags)), fill_value=np.nan, order='F', dtype=float\n            )\n            for i, lag in enumerate(self.lags):\n                X_data[:, i] = y[self.window_size - lag: -lag]\n\n            if X_as_pandas:\n                X_data = pd.DataFrame(\n                             data    = X_data,\n                             columns = self.lags_names,\n                             index   = train_index\n                         )\n\n        y_data = y[self.window_size:]\n\n        return X_data, y_data\n\n\n    def _create_window_features(\n        self, \n        y: pd.Series,\n        train_index: pd.Index,\n        X_as_pandas: bool = False,\n    ) -> Tuple[list, list]:\n        \"\"\"\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        train_index : pandas Index\n            Index of the training data. It is used to create the pandas DataFrame\n            `X_train_window_features` when `X_as_pandas` is `True`.\n        X_as_pandas : bool, default `False`\n            If `True`, the returned matrix `X_train_window_features` is a \n            pandas DataFrame.\n\n        Returns\n        -------\n        X_train_window_features : list\n            List of numpy ndarrays or pandas DataFrames with the window features.\n        X_train_window_features_names_out_ : list\n            Names of the window features.\n        \n        \"\"\"\n\n        len_train_index = len(train_index)\n        X_train_window_features = []\n        X_train_window_features_names_out_ = []\n        for wf in self.window_features:\n            X_train_wf = wf.transform_batch(y)\n            if not isinstance(X_train_wf, pd.DataFrame):\n                raise TypeError(\n                    (f\"The method `transform_batch` of {type(wf).__name__} \"\n                     f\"must return a pandas DataFrame.\")\n                )\n            X_train_wf = X_train_wf.iloc[-len_train_index:]\n            if not len(X_train_wf) == len_train_index:\n                raise ValueError(\n                    (f\"The method `transform_batch` of {type(wf).__name__} \"\n                     f\"must return a DataFrame with the same number of rows as \"\n                     f\"the input time series - `window_size`: {len_train_index}.\")\n                )\n            if not (X_train_wf.index == train_index).all():\n                raise ValueError(\n                    (f\"The method `transform_batch` of {type(wf).__name__} \"\n                     f\"must return a DataFrame with the same index as \"\n                     f\"the input time series - `window_size`.\")\n                )\n            \n            X_train_window_features_names_out_.extend(X_train_wf.columns)\n            if not X_as_pandas:\n                X_train_wf = X_train_wf.to_numpy()     \n            X_train_window_features.append(X_train_wf)\n\n        return X_train_window_features, X_train_window_features_names_out_\n\n\n    def _create_train_X_y(\n        self,\n        y: pd.Series,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n    ) -> Tuple[pd.DataFrame, pd.Series, list, list, list, list, dict]:\n        \"\"\"\n        Create training matrices from univariate time series and exogenous\n        variables.\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors).\n        y_train : pandas Series\n            Values of the time series related to each row of `X_train`.\n        exog_names_in_ : list\n            Names of the exogenous variables used during training.\n        X_train_window_features_names_out_ : list\n            Names of the window features included in the matrix `X_train` created\n            internally for training.\n        X_train_exog_names_out_ : list\n            Names of the exogenous variables included in the matrix `X_train` created\n            internally for training. It can be different from `exog_names_in_` if\n            some exogenous variables are transformed during the training process.\n        X_train_features_names_out_ : list\n            Names of the columns of the matrix created internally for training.\n        exog_dtypes_in_ : dict\n            Type of each exogenous variable/s used in training. If `transformer_exog` \n            is used, the dtypes are calculated before the transformation.\n        \n        \"\"\"\n\n        check_y(y=y)\n        y = input_to_frame(data=y, input_name='y')\n\n        if len(y) <= self.window_size:\n            raise ValueError(\n                (f\"Length of `y` must be greater than the maximum window size \"\n                 f\"needed by the forecaster.\\n\"\n                 f\"    Length `y`: {len(y)}.\\n\"\n                 f\"    Max window size: {self.window_size}.\\n\"\n                 f\"    Lags window size: {self.max_lag}.\\n\"\n                 f\"    Window features window size: {self.max_size_window_features}.\")\n            )\n\n        fit_transformer = False if self.is_fitted else True\n        y = transform_dataframe(\n                df                = y, \n                transformer       = self.transformer_y,\n                fit               = fit_transformer,\n                inverse_transform = False,\n            )\n        y_values, y_index = preprocess_y(y=y)\n        train_index = y_index[self.window_size:]\n\n        if self.differentiation is not None:\n            if not self.is_fitted:\n                y_values = self.differentiator.fit_transform(y_values)\n            else:\n                differentiator = copy(self.differentiator)\n                y_values = differentiator.fit_transform(y_values)\n\n        exog_names_in_ = None\n        exog_dtypes_in_ = None\n        categorical_features = False\n        if exog is not None:\n            check_exog(exog=exog, allow_nan=True)\n            exog = input_to_frame(data=exog, input_name='exog')\n\n            len_y = len(y_values)\n            len_train_index = len(train_index)\n            len_exog = len(exog)\n            if not len_exog == len_y and not len_exog == len_train_index:\n                raise ValueError(\n                    f\"Length of `exog` must be equal to the length of `y` (if index is \"\n                    f\"fully aligned) or length of `y` - `window_size` (if `exog` \"\n                    f\"starts after the first `window_size` values).\\n\"\n                    f\"    `exog`              : ({exog.index[0]} -- {exog.index[-1]})  (n={len_exog})\\n\"\n                    f\"    `y`                 : ({y.index[0]} -- {y.index[-1]})  (n={len_y})\\n\"\n                    f\"    `y` - `window_size` : ({train_index[0]} -- {train_index[-1]})  (n={len_train_index})\"\n                )\n\n            exog_names_in_ = exog.columns.to_list()\n            exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = fit_transformer,\n                       inverse_transform = False\n                   )\n\n            check_exog_dtypes(exog, call_check_exog=True)\n            categorical_features = (\n                exog.select_dtypes(include=np.number).shape[1] != exog.shape[1]\n            )\n\n            _, exog_index = preprocess_exog(exog=exog, return_values=False)\n            if len_exog == len_y:\n                if not (exog_index == y_index).all():\n                    raise ValueError(\n                        \"When `exog` has the same length as `y`, the index of \"\n                        \"`exog` must be aligned with the index of `y` \"\n                        \"to ensure the correct alignment of values.\"\n                    )\n                # The first `self.window_size` positions have to be removed from \n                # exog since they are not in X_train.\n                exog = exog.iloc[self.window_size:, ]\n            else:\n                if not (exog_index == train_index).all():\n                    raise ValueError(\n                        \"When `exog` doesn't contain the first `window_size` observations, \"\n                        \"the index of `exog` must be aligned with the index of `y` minus \"\n                        \"the first `window_size` observations to ensure the correct \"\n                        \"alignment of values.\"\n                    )\n            \n        X_train = []\n        X_train_features_names_out_ = []\n        X_as_pandas = True if categorical_features else False\n\n        X_train_lags, y_train = self._create_lags(\n            y=y_values, X_as_pandas=X_as_pandas, train_index=train_index\n        )\n        if X_train_lags is not None:\n            X_train.append(X_train_lags)\n            X_train_features_names_out_.extend(self.lags_names)\n        \n        X_train_window_features_names_out_ = None\n        if self.window_features is not None:\n            n_diff = 0 if self.differentiation is None else self.differentiation\n            y_window_features = pd.Series(y_values[n_diff:], index=y_index[n_diff:])\n            X_train_window_features, X_train_window_features_names_out_ = (\n                self._create_window_features(\n                    y=y_window_features, X_as_pandas=X_as_pandas, train_index=train_index\n                )\n            )\n            X_train.extend(X_train_window_features)\n            X_train_features_names_out_.extend(X_train_window_features_names_out_)\n\n        X_train_exog_names_out_ = None\n        if exog is not None:\n            X_train_exog_names_out_ = exog.columns.to_list()  \n            if not X_as_pandas:\n                exog = exog.to_numpy()     \n            X_train_features_names_out_.extend(X_train_exog_names_out_)\n            X_train.append(exog)\n        \n        if len(X_train) == 1:\n            X_train = X_train[0]\n        else:\n            if X_as_pandas:\n                X_train = pd.concat(X_train, axis=1)\n            else:\n                X_train = np.concatenate(X_train, axis=1)\n                \n        if X_as_pandas:\n            X_train.index = train_index\n        else:\n            X_train = pd.DataFrame(\n                          data    = X_train,\n                          index   = train_index,\n                          columns = X_train_features_names_out_\n                      )\n        \n        y_train = pd.Series(\n                      data  = y_train,\n                      index = train_index,\n                      name  = 'y'\n                  )\n\n        return (\n            X_train,\n            y_train,\n            exog_names_in_,\n            X_train_window_features_names_out_,\n            X_train_exog_names_out_,\n            X_train_features_names_out_,\n            exog_dtypes_in_\n        )\n\n\n    def create_train_X_y(\n        self,\n        y: pd.Series,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n    ) -> Tuple[pd.DataFrame, pd.Series]:\n        \"\"\"\n        Create training matrices from univariate time series and exogenous\n        variables.\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors).\n        y_train : pandas Series\n            Values of the time series related to each row of `X_data`.\n        \n        \"\"\"\n\n        output = self._create_train_X_y(y=y, exog=exog)\n\n        X_train = output[0]\n        y_train = output[1]\n\n        return X_train, y_train\n\n\n    def _train_test_split_one_step_ahead(\n        self,\n        y: pd.Series,\n        initial_train_size: int,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n    ) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n        \"\"\"\n        Create matrices needed to train and test the forecaster for one-step-ahead\n        predictions.\n\n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        initial_train_size : int\n            Initial size of the training set. It is the number of observations used\n            to train the forecaster before making the first prediction.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n        \n        Returns\n        -------\n        X_train : pandas DataFrame\n            Predictor values used to train the model.\n        y_train : pandas Series\n            Target values related to each row of `X_train`.\n        X_test : pandas DataFrame\n            Predictor values used to test the model.\n        y_test : pandas Series\n            Target values related to each row of `X_test`.\n        \n        \"\"\"\n\n        is_fitted = self.is_fitted\n        self.is_fitted = False\n        X_train, y_train, *_ = self._create_train_X_y(\n            y    = y.iloc[: initial_train_size],\n            exog = exog.iloc[: initial_train_size] if exog is not None else None\n        )\n\n        test_init = initial_train_size - self.window_size\n        self.is_fitted = True\n        X_test, y_test, *_ = self._create_train_X_y(\n            y    = y.iloc[test_init:],\n            exog = exog.iloc[test_init:] if exog is not None else None\n        )\n\n        self.is_fitted = is_fitted\n\n        return X_train, y_train, X_test, y_test\n\n\n    def create_sample_weights(\n        self,\n        X_train: pd.DataFrame,\n    ) -> np.ndarray:\n        \"\"\"\n        Crate weights for each observation according to the forecaster's attribute\n        `weight_func`.\n\n        Parameters\n        ----------\n        X_train : pandas DataFrame\n            Dataframe created with the `create_train_X_y` method, first return.\n\n        Returns\n        -------\n        sample_weight : numpy ndarray\n            Weights to use in `fit` method.\n\n        \"\"\"\n\n        sample_weight = None\n\n        if self.weight_func is not None:\n            sample_weight = self.weight_func(X_train.index)\n\n        if sample_weight is not None:\n            if np.isnan(sample_weight).any():\n                raise ValueError(\n                    \"The resulting `sample_weight` cannot have NaN values.\"\n                )\n            if np.any(sample_weight < 0):\n                raise ValueError(\n                    \"The resulting `sample_weight` cannot have negative values.\"\n                )\n            if np.sum(sample_weight) == 0:\n                raise ValueError(\n                    (\"The resulting `sample_weight` cannot be normalized because \"\n                     \"the sum of the weights is zero.\")\n                )\n\n        return sample_weight\n\n\n    def fit(\n        self,\n        y: pd.Series,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        store_last_window: bool = True,\n        store_in_sample_residuals: bool = True,\n        random_state: int = 123\n    ) -> None:\n        \"\"\"\n        Training Forecaster.\n\n        Additional arguments to be passed to the `fit` method of the regressor \n        can be added with the `fit_kwargs` argument when initializing the forecaster.\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned so\n            that y[i] is regressed on exog[i].\n        store_last_window : bool, default `True`\n            Whether or not to store the last window (`last_window_`) of training data.\n        store_in_sample_residuals : bool, default `True`\n            If `True`, in-sample residuals will be stored in the forecaster object\n            after fitting (`in_sample_residuals_` attribute).\n        random_state : int, default `123`\n            Set a seed for the random generator so that the stored sample \n            residuals are always deterministic.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        # Reset values in case the forecaster has already been fitted.\n        self.last_window_                       = None\n        self.index_type_                        = None\n        self.index_freq_                        = None\n        self.training_range_                    = None\n        self.exog_in_                           = False\n        self.exog_names_in_                     = None\n        self.exog_type_in_                      = None\n        self.exog_dtypes_in_                    = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_            = None\n        self.X_train_features_names_out_        = None\n        self.in_sample_residuals_               = None\n        self.is_fitted                          = False\n        self.fit_date                           = None\n\n        (\n            X_train,\n            y_train,\n            exog_names_in_,\n            X_train_window_features_names_out_,\n            X_train_exog_names_out_,\n            X_train_features_names_out_,\n            exog_dtypes_in_\n        ) = self._create_train_X_y(y=y, exog=exog)\n        sample_weight = self.create_sample_weights(X_train=X_train)\n\n        if sample_weight is not None:\n            self.regressor.fit(\n                X             = X_train,\n                y             = y_train,\n                sample_weight = sample_weight,\n                **self.fit_kwargs\n            )\n        else:\n            self.regressor.fit(X=X_train, y=y_train, **self.fit_kwargs)\n\n        self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n        self.X_train_features_names_out_ = X_train_features_names_out_\n\n        self.is_fitted = True\n        self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.training_range_ = preprocess_y(y=y, return_values=False)[1][[0, -1]]\n        self.index_type_ = type(X_train.index)\n        if isinstance(X_train.index, pd.DatetimeIndex):\n            self.index_freq_ = X_train.index.freqstr\n        else: \n            self.index_freq_ = X_train.index.step\n\n        if exog is not None:\n            self.exog_in_ = True\n            self.exog_type_in_ = type(exog)\n            self.exog_names_in_ = exog_names_in_\n            self.exog_dtypes_in_ = exog_dtypes_in_\n            self.X_train_exog_names_out_ = X_train_exog_names_out_\n\n        # This is done to save time during fit in functions such as backtesting()\n        if store_in_sample_residuals:\n            self._binning_in_sample_residuals(\n                y_true       = y_train.to_numpy(),\n                y_pred       = self.regressor.predict(X_train).ravel(),\n                random_state = random_state\n            )\n\n        # The last time window of training data is stored so that lags needed as\n        # predictors in the first iteration of `predict()` can be calculated. It\n        # also includes the values need to calculate the diferenctiation.\n        if store_last_window:\n            self.last_window_ = (\n                y.iloc[-self.window_size:]\n                .copy()\n                .to_frame(name=y.name if y.name is not None else 'y')\n            )\n\n\n    def _binning_in_sample_residuals(\n        self,\n        y_true: np.ndarray,\n        y_pred: np.ndarray,\n        random_state: int = 123\n    ) -> None:\n        \"\"\"\n        Binning residuals according to the predicted value each residual is\n        associated with. First a skforecast.preprocessing.QuantileBinner object\n        is fitted to the predicted values. Then, residuals are binned according\n        to the predicted value each residual is associated with. Residuals are\n        stored in the forecaster object as `in_sample_residuals_` and\n        `in_sample_residuals_by_bin_`.\n        If `transformer_y` is not `None`, `y_true` and `y_pred` are transformed\n        before calculating residuals. If `differentiation` is not `None`, `y_true`\n        and `y_pred` are differentiated before calculating residuals. If both,\n        `transformer_y` and `differentiation` are not `None`, transformation is\n        done before differentiation. The number of residuals stored per bin is\n        limited to  `10_000 // self.binner.n_bins_`. The total number of residuals\n        stored is `10_000`.\n        **New in version 0.14.0**\n\n        Parameters\n        ----------\n        y_true : numpy ndarray\n            True values of the time series.\n        y_pred : numpy ndarray\n            Predicted values of the time series.\n        random_state : int, default `123`\n            Set a seed for the random generator so that the stored sample \n            residuals are always deterministic.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        data = pd.DataFrame({'prediction': y_pred, 'residuals': (y_true - y_pred)})\n        data['bin'] = self.binner.fit_transform(y_pred).astype(int)\n        self.in_sample_residuals_by_bin_ = (\n            data.groupby('bin')['residuals'].apply(np.array).to_dict()\n        )\n\n        rng = np.random.default_rng(seed=random_state)\n        max_sample = 10_000 // self.binner.n_bins_\n        for k, v in self.in_sample_residuals_by_bin_.items():\n            \n            if len(v) > max_sample:\n                sample = v[rng.integers(low=0, high=len(v), size=max_sample)]\n                self.in_sample_residuals_by_bin_[k] = sample\n\n        self.in_sample_residuals_ = np.concatenate(list(\n            self.in_sample_residuals_by_bin_.values()\n        ))\n\n        self.binner_intervals_ = self.binner.intervals_\n\n\n    def _create_predict_inputs(\n        self,\n        steps: Union[int, str, pd.Timestamp], \n        last_window: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        predict_boot: bool = False,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = False,\n        check_inputs: bool = True,\n    ) -> Tuple[np.ndarray, Optional[np.ndarray], pd.Index, int]:\n        \"\"\"\n        Create the inputs needed for the first iteration of the prediction \n        process. As this is a recursive process, the last window is updated at \n        each iteration of the prediction process.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        predict_boot : bool, default `False`\n            If `True`, residuals are returned to generate bootstrapping predictions.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n\n        Returns\n        -------\n        last_window_values : numpy ndarray\n            Series values used to create the predictors needed in the first \n            iteration of the prediction (t + 1).\n        exog_values : numpy ndarray, None\n            Exogenous variable/s included as predictor/s.\n        prediction_index : pandas Index\n            Index of the predictions.\n        steps: int\n            Number of future steps predicted.\n        \n        \"\"\"\n\n        if last_window is None:\n            last_window = self.last_window_\n\n        if self.is_fitted:\n            steps = date_to_index_position(\n                        index        = last_window.index,\n                        date_input   = steps,\n                        date_literal = 'steps'\n                    )\n\n        if check_inputs:\n            check_predict_input(\n                forecaster_name  = type(self).__name__,\n                steps            = steps,\n                is_fitted        = self.is_fitted,\n                exog_in_         = self.exog_in_,\n                index_type_      = self.index_type_,\n                index_freq_      = self.index_freq_,\n                window_size      = self.window_size,\n                last_window      = last_window,\n                exog             = exog,\n                exog_type_in_    = self.exog_type_in_,\n                exog_names_in_   = self.exog_names_in_,\n                interval         = None\n            )\n        \n            if predict_boot and not use_in_sample_residuals:\n                if not use_binned_residuals and self.out_sample_residuals_ is None:\n                    raise ValueError(\n                        \"`forecaster.out_sample_residuals_` is `None`. Use \"\n                        \"`use_in_sample_residuals=True` or the \"\n                        \"`set_out_sample_residuals()` method before predicting.\"\n                    )\n                if use_binned_residuals and self.out_sample_residuals_by_bin_ is None:\n                    raise ValueError(\n                        \"`forecaster.out_sample_residuals_by_bin_` is `None`. Use \"\n                        \"`use_in_sample_residuals=True` or the \"\n                        \"`set_out_sample_residuals()` method before predicting.\"\n                    )\n\n        last_window = last_window.iloc[-self.window_size:].copy()\n        last_window_values, last_window_index = preprocess_last_window(\n                                                    last_window = last_window\n                                                )\n\n        last_window_values = transform_numpy(\n                                 array             = last_window_values,\n                                 transformer       = self.transformer_y,\n                                 fit               = False,\n                                 inverse_transform = False\n                             )\n        if self.differentiation is not None:\n            last_window_values = self.differentiator.fit_transform(last_window_values)\n\n        if exog is not None:\n            exog = input_to_frame(data=exog, input_name='exog')\n            exog = exog.loc[:, self.exog_names_in_]\n            exog = transform_dataframe(\n                       df                = exog,\n                       transformer       = self.transformer_exog,\n                       fit               = False,\n                       inverse_transform = False\n                   )\n            check_exog_dtypes(exog=exog)\n            exog_values = exog.to_numpy()[:steps]\n        else:\n            exog_values = None\n\n        prediction_index = expand_index(\n                               index = last_window_index,\n                               steps = steps,\n                           )\n\n        return last_window_values, exog_values, prediction_index, steps\n\n\n    def _recursive_predict(\n        self,\n        steps: int,\n        last_window_values: np.ndarray,\n        exog_values: Optional[np.ndarray] = None,\n        residuals: Optional[Union[np.ndarray, dict]] = None,\n        use_binned_residuals: bool = False,\n    ) -> np.ndarray:\n        \"\"\"\n        Predict n steps ahead. It is an iterative process in which, each prediction,\n        is used as a predictor for the next step.\n        \n        Parameters\n        ----------\n        steps : int\n            Number of future steps predicted.\n        last_window_values : numpy ndarray\n            Series values used to create the predictors needed in the first \n            iteration of the prediction (t + 1).\n        exog_values : numpy ndarray, default `None`\n            Exogenous variable/s included as predictor/s.\n        residuals : numpy ndarray, dict, default `None`\n            Residuals used to generate bootstrapping predictions.\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : numpy ndarray\n            Predicted values.\n        \n        \"\"\"\n\n        n_lags = len(self.lags) if self.lags is not None else 0\n        n_window_features = (\n            len(self.X_train_window_features_names_out_)\n            if self.window_features is not None\n            else 0\n        )\n        n_exog = exog_values.shape[1] if exog_values is not None else 0\n\n        X = np.full(\n            shape=(n_lags + n_window_features + n_exog), fill_value=np.nan, dtype=float\n        )\n        predictions = np.full(shape=steps, fill_value=np.nan, dtype=float)\n        last_window = np.concatenate((last_window_values, predictions))\n\n        for i in range(steps):\n\n            if self.lags is not None:\n                X[:n_lags] = last_window[-self.lags - (steps - i)]\n            if self.window_features is not None:\n                X[n_lags : n_lags + n_window_features] = np.concatenate(\n                    [\n                        wf.transform(last_window[i : -(steps - i)])\n                        for wf in self.window_features\n                    ]\n                )\n            if exog_values is not None:\n                X[n_lags + n_window_features:] = exog_values[i]\n        \n            pred = self.regressor.predict(X.reshape(1, -1)).ravel()\n            \n            if residuals is not None:\n                if use_binned_residuals:\n                    predicted_bin = (\n                        self.binner.transform(pred).item()\n                    )\n                    step_residual = residuals[predicted_bin][i]\n                else:\n                    step_residual = residuals[i]\n                \n                pred += step_residual\n            \n            predictions[i] = pred[0]\n\n            # Update `last_window` values. The first position is discarded and \n            # the new prediction is added at the end.\n            last_window[-(steps - i)] = pred[0]\n\n        return predictions\n\n\n    def create_predict_X(\n        self,\n        steps: int,\n        last_window: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        Create the predictors needed to predict `steps` ahead. As it is a recursive\n        process, the predictors are created at each iteration of the prediction \n        process.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n\n        Returns\n        -------\n        X_predict : pandas DataFrame\n            Pandas DataFrame with the predictors for each step. The index \n            is the same as the prediction index.\n        \n        \"\"\"\n\n        last_window_values, exog_values, prediction_index, steps = (\n            self._create_predict_inputs(steps=steps, last_window=last_window, exog=exog)\n        )\n        \n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\", \n                message=\"X does not have valid feature names\", \n                category=UserWarning\n            )\n            predictions = self._recursive_predict(\n                              steps              = steps,\n                              last_window_values = last_window_values,\n                              exog_values        = exog_values\n                          )\n\n        X_predict = []\n        full_predictors = np.concatenate((last_window_values, predictions))\n\n        if self.lags is not None:\n            idx = np.arange(-steps, 0)[:, None] - self.lags\n            X_lags = full_predictors[idx + len(full_predictors)]\n            X_predict.append(X_lags)\n\n        if self.window_features is not None:\n            X_window_features = np.full(\n                shape      = (steps, len(self.X_train_window_features_names_out_)), \n                fill_value = np.nan, \n                order      = 'C',\n                dtype      = float\n            )\n            for i in range(steps):\n                X_window_features[i, :] = np.concatenate(\n                    [wf.transform(full_predictors[i:-(steps - i)]) \n                     for wf in self.window_features]\n                )\n            X_predict.append(X_window_features)\n\n        if exog is not None:\n            X_predict.append(exog_values)\n\n        X_predict = pd.DataFrame(\n                        data    = np.concatenate(X_predict, axis=1),\n                        columns = self.X_train_features_names_out_,\n                        index   = prediction_index\n                    )\n        \n        if self.transformer_y is not None or self.differentiation is not None:\n            warnings.warn(\n                \"The output matrix is in the transformed scale due to the \"\n                \"inclusion of transformations or differentiation in the Forecaster. \"\n                \"As a result, any predictions generated using this matrix will also \"\n                \"be in the transformed scale. Please refer to the documentation \"\n                \"for more details: \"\n                \"https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html\",\n                DataTransformationWarning\n            )\n\n        return X_predict\n\n\n    def predict(\n        self,\n        steps: Union[int, str, pd.Timestamp],\n        last_window: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        check_inputs: bool = True\n    ) -> pd.Series:\n        \"\"\"\n        Predict n steps ahead. It is an recursive process in which, each prediction,\n        is used as a predictor for the next step.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n\n        Returns\n        -------\n        predictions : pandas Series\n            Predicted values.\n        \n        \"\"\"\n\n        last_window_values, exog_values, prediction_index, steps = (\n            self._create_predict_inputs(\n                steps=steps,\n                last_window=last_window,\n                exog=exog,\n                check_inputs=check_inputs,\n            )\n        )\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\", \n                message=\"X does not have valid feature names\", \n                category=UserWarning\n            )\n            predictions = self._recursive_predict(\n                              steps              = steps,\n                              last_window_values = last_window_values,\n                              exog_values        = exog_values\n                          )\n\n        if self.differentiation is not None:\n            predictions = self.differentiator.inverse_transform_next_window(predictions)\n\n        predictions = transform_numpy(\n                          array             = predictions,\n                          transformer       = self.transformer_y,\n                          fit               = False,\n                          inverse_transform = True\n                      )\n\n        predictions = pd.Series(\n                          data  = predictions,\n                          index = prediction_index,\n                          name  = 'pred'\n                      )\n\n        return predictions\n\n\n    def predict_bootstrapping(\n        self,\n        steps: Union[int, str, pd.Timestamp],\n        last_window: Optional[pd.Series] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = False\n    ) -> pd.DataFrame:\n        \"\"\"\n        Generate multiple forecasting predictions using a bootstrapping process. \n        By sampling from a collection of past observed errors (the residuals),\n        each iteration of bootstrapping generates a different set of predictions. \n        See the Notes section for more information. \n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        boot_predictions : pandas DataFrame\n            Predictions generated by bootstrapping.\n            Shape: (steps, n_boot)\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n        Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n        \"\"\"\n\n        (\n            last_window_values,\n            exog_values,\n            prediction_index,\n            steps\n        ) = self._create_predict_inputs(\n            steps                   = steps, \n            last_window             = last_window, \n            exog                    = exog,\n            predict_boot            = True, \n            use_in_sample_residuals = use_in_sample_residuals,\n            use_binned_residuals    = use_binned_residuals\n        )\n\n        if use_in_sample_residuals:\n            residuals = self.in_sample_residuals_\n            residuals_by_bin = self.in_sample_residuals_by_bin_\n        else:\n            residuals = self.out_sample_residuals_\n            residuals_by_bin = self.out_sample_residuals_by_bin_\n\n        rng = np.random.default_rng(seed=random_state)\n        if use_binned_residuals:\n            sampled_residuals = {\n                k: v[rng.integers(low=0, high=len(v), size=(steps, n_boot))]\n                for k, v in residuals_by_bin.items()\n            }\n        else:\n            sampled_residuals = residuals[\n                rng.integers(low=0, high=len(residuals), size=(steps, n_boot))\n            ]\n        \n        boot_columns = []\n        boot_predictions = np.full(\n                               shape      = (steps, n_boot),\n                               fill_value = np.nan,\n                               order      = 'F',\n                               dtype      = float\n                           )\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\", \n                message=\"X does not have valid feature names\", \n                category=UserWarning\n            )\n            for i in range(n_boot):\n\n                if use_binned_residuals:\n                    boot_sampled_residuals = {\n                        k: v[:, i]\n                        for k, v in sampled_residuals.items()\n                    }\n                else:\n                    boot_sampled_residuals = sampled_residuals[:, i]\n\n                boot_columns.append(f\"pred_boot_{i}\")\n                boot_predictions[:, i] = self._recursive_predict(\n                    steps                = steps,\n                    last_window_values   = last_window_values,\n                    exog_values          = exog_values,\n                    residuals            = boot_sampled_residuals,\n                    use_binned_residuals = use_binned_residuals,\n                )\n\n        if self.differentiation is not None:\n            boot_predictions = (\n                self.differentiator.inverse_transform_next_window(boot_predictions)\n            )\n        \n        if self.transformer_y:\n            boot_predictions = np.apply_along_axis(\n                                   func1d            = transform_numpy,\n                                   axis              = 0,\n                                   arr               = boot_predictions,\n                                   transformer       = self.transformer_y,\n                                   fit               = False,\n                                   inverse_transform = True\n                               )\n\n        boot_predictions = pd.DataFrame(\n                               data    = boot_predictions,\n                               index   = prediction_index,\n                               columns = boot_columns\n                           )\n\n        return boot_predictions\n\n\n    def predict_interval(\n        self,\n        steps: Union[int, str, pd.Timestamp],\n        last_window: Optional[pd.Series] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        interval: list = [5, 95],\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = False\n    ) -> pd.DataFrame:\n        \"\"\"\n        Iterative process in which each prediction is used as a predictor\n        for the next step, and bootstrapping is used to estimate prediction\n        intervals. Both predictions and intervals are returned.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        interval : list, default `[5, 95]`\n            Confidence of the prediction interval estimated. Sequence of \n            percentiles to compute, which must be between 0 and 100 inclusive. \n            For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Values predicted by the forecaster and their estimated interval.\n\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n\n        check_interval(interval=interval)\n\n        boot_predictions = self.predict_bootstrapping(\n                               steps                   = steps,\n                               last_window             = last_window,\n                               exog                    = exog,\n                               n_boot                  = n_boot,\n                               random_state            = random_state,\n                               use_in_sample_residuals = use_in_sample_residuals,\n                               use_binned_residuals    = use_binned_residuals\n                           )\n\n        predictions = self.predict(\n                          steps        = steps,\n                          last_window  = last_window,\n                          exog         = exog,\n                          check_inputs = False\n                      )\n\n        interval = np.array(interval) / 100\n        predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n        predictions_interval.columns = ['lower_bound', 'upper_bound']\n        predictions = pd.concat((predictions, predictions_interval), axis=1)\n\n        return predictions\n\n\n    def predict_quantiles(\n        self,\n        steps: Union[int, str, pd.Timestamp],\n        last_window: Optional[pd.Series] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        quantiles: list = [0.05, 0.5, 0.95],\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = False\n    ) -> pd.DataFrame:\n        \"\"\"\n        Calculate the specified quantiles for each step. After generating \n        multiple forecasting predictions through a bootstrapping process, each \n        quantile is calculated for each step.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        quantiles : list, default `[0.05, 0.5, 0.95]`\n            Sequence of quantiles to compute, which must be between 0 and 1 \n            inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n            `quantiles = [0.05, 0.5, 0.95]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate quantiles.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot quantiles are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create prediction quantiles. If `False`, out of\n            sample residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Quantiles predicted by the forecaster.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n\n        check_interval(quantiles=quantiles)\n\n        boot_predictions = self.predict_bootstrapping(\n                               steps                   = steps,\n                               last_window             = last_window,\n                               exog                    = exog,\n                               n_boot                  = n_boot,\n                               random_state            = random_state,\n                               use_in_sample_residuals = use_in_sample_residuals,\n                               use_binned_residuals    = use_binned_residuals\n                           )\n\n        predictions = boot_predictions.quantile(q=quantiles, axis=1).transpose()\n        predictions.columns = [f'q_{q}' for q in quantiles]\n\n        return predictions\n\n\n    def predict_dist(\n        self,\n        steps: Union[int, str, pd.Timestamp],\n        distribution: object,\n        last_window: Optional[pd.Series] = None,\n        exog: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        n_boot: int = 250,\n        random_state: int = 123,\n        use_in_sample_residuals: bool = True,\n        use_binned_residuals: bool = False\n    ) -> pd.DataFrame:\n        \"\"\"\n        Fit a given probability distribution for each step. After generating \n        multiple forecasting predictions through a bootstrapping process, each \n        step is fitted to the given distribution.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        distribution : Object\n            A distribution object from scipy.stats.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).  \n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Distribution parameters estimated for each step.\n\n        \"\"\"\n\n        boot_samples = self.predict_bootstrapping(\n                           steps                   = steps,\n                           last_window             = last_window,\n                           exog                    = exog,\n                           n_boot                  = n_boot,\n                           random_state            = random_state,\n                           use_in_sample_residuals = use_in_sample_residuals,\n                           use_binned_residuals    = use_binned_residuals\n                       )       \n\n        param_names = [p for p in inspect.signature(distribution._pdf).parameters\n                       if not p == 'x'] + [\"loc\", \"scale\"]\n        param_values = np.apply_along_axis(\n                           lambda x: distribution.fit(x),\n                           axis = 1,\n                           arr  = boot_samples\n                       )\n        predictions = pd.DataFrame(\n                          data    = param_values,\n                          columns = param_names,\n                          index   = boot_samples.index\n                      )\n\n        return predictions\n\n    def set_params(\n        self, \n        params: dict\n    ) -> None:\n        \"\"\"\n        Set new values to the parameters of the scikit learn model stored in the\n        forecaster.\n        \n        Parameters\n        ----------\n        params : dict\n            Parameters values.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        self.regressor = clone(self.regressor)\n        self.regressor.set_params(**params)\n\n    def set_fit_kwargs(\n        self, \n        fit_kwargs: dict\n    ) -> None:\n        \"\"\"\n        Set new values for the additional keyword arguments passed to the `fit` \n        method of the regressor.\n        \n        Parameters\n        ----------\n        fit_kwargs : dict\n            Dict of the form {\"argument\": new_value}.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n\n    def set_lags(\n        self, \n        lags: Optional[Union[int, list, np.ndarray, range]] = None\n    ) -> None:\n        \"\"\"\n        Set new value to the attribute `lags`. Attributes `lags_names`, \n        `max_lag` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        lags : int, list, numpy ndarray, range, default `None`\n            Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. \n        \n            - `int`: include lags from 1 to `lags` (included).\n            - `list`, `1d numpy ndarray` or `range`: include only lags present in \n            `lags`, all elements must be int.\n            - `None`: no lags are included as predictors. \n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        if self.window_features is None and lags is None:\n            raise ValueError(\n                \"At least one of the arguments `lags` or `window_features` \"\n                \"must be different from None. This is required to create the \"\n                \"predictors used in training the forecaster.\"\n            )\n        \n        self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\n        self.window_size = max(\n            [ws for ws in [self.max_lag, self.max_size_window_features] \n             if ws is not None]\n        )\n        if self.differentiation is not None:\n            self.window_size += self.differentiation\n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_window_features(\n        self, \n        window_features: Optional[Union[object, list]] = None\n    ) -> None:\n        \"\"\"\n        Set new value to the attribute `window_features`. Attributes \n        `max_size_window_features`, `window_features_names`, \n        `window_features_class_names` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        window_features : object, list, default `None`\n            Instance or list of instances used to create window features. Window features\n            are created from the original time series and are included as predictors.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        if window_features is None and self.lags is None:\n            raise ValueError(\n                \"At least one of the arguments `lags` or `window_features` \"\n                \"must be different from None. This is required to create the \"\n                \"predictors used in training the forecaster.\"\n            )\n        \n        self.window_features, self.window_features_names, self.max_size_window_features = (\n            initialize_window_features(window_features)\n        )\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [\n                type(wf).__name__ for wf in self.window_features\n            ] \n        self.window_size = max(\n            [ws for ws in [self.max_lag, self.max_size_window_features] \n             if ws is not None]\n        )\n        if self.differentiation is not None:\n            self.window_size += self.differentiation\n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_out_sample_residuals(\n        self,\n        y_true: Union[pd.Series, np.ndarray],\n        y_pred: Union[pd.Series, np.ndarray],\n        append: bool = False,\n        random_state: int = 123\n    ) -> None:\n        \"\"\"\n        Set new values to the attribute `out_sample_residuals_`. Out of sample\n        residuals are meant to be calculated using observations that did not\n        participate in the training process. `y_true` and `y_pred` are expected\n        to be in the original scale of the time series. Residuals are calculated\n        as `y_true` - `y_pred`, after applying the necessary transformations and\n        differentiations if the forecaster includes them (`self.transformer_y`\n        and `self.differentiation`). Two internal attributes are updated:\n\n        + `out_sample_residuals_`: residuals stored in a numpy ndarray.\n        + `out_sample_residuals_by_bin_`: residuals are binned according to the\n        predicted value they are associated with and stored in a dictionary, where\n        the keys are the  intervals of the predicted values and the values are\n        the residuals associated with that range. If a bin binning is empty, it\n        is filled with a random sample of residuals from other bins. This is done\n        to ensure that all bins have at least one residual and can be used in the\n        prediction process.\n\n        A total of 10_000 residuals are stored in the attribute `out_sample_residuals_`.\n        If the number of residuals is greater than 10_000, a random sample of\n        10_000 residuals is stored. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n        \n        Parameters\n        ----------\n        y_true : pandas Series, numpy ndarray, default `None`\n            True values of the time series from which the residuals have been\n            calculated.\n        y_pred : pandas Series, numpy ndarray, default `None`\n            Predicted values of the time series.\n        append : bool, default `False`\n            If `True`, new residuals are added to the once already stored in the\n            forecaster. If after appending the new residuals, the limit of\n            `10_000 // self.binner.n_bins_` values per bin is reached, a random\n            sample of residuals is stored.\n        random_state : int, default `123`\n            Sets a seed to the random sampling for reproducible output.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n\n        if not self.is_fitted:\n            raise NotFittedError(\n                \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n                \"arguments before using `set_out_sample_residuals()`.\"\n            )\n\n        if not isinstance(y_true, (np.ndarray, pd.Series)):\n            raise TypeError(\n                f\"`y_true` argument must be `numpy ndarray` or `pandas Series`. \"\n                f\"Got {type(y_true)}.\"\n            )\n        \n        if not isinstance(y_pred, (np.ndarray, pd.Series)):\n            raise TypeError(\n                f\"`y_pred` argument must be `numpy ndarray` or `pandas Series`. \"\n                f\"Got {type(y_pred)}.\"\n            )\n        \n        if len(y_true) != len(y_pred):\n            raise ValueError(\n                f\"`y_true` and `y_pred` must have the same length. \"\n                f\"Got {len(y_true)} and {len(y_pred)}.\"\n            )\n        \n        if isinstance(y_true, pd.Series) and isinstance(y_pred, pd.Series):\n            if not y_true.index.equals(y_pred.index):\n                raise ValueError(\n                    \"`y_true` and `y_pred` must have the same index.\"\n                )\n\n        if not isinstance(y_pred, np.ndarray):\n            y_pred = y_pred.to_numpy()\n\n        if not isinstance(y_true, np.ndarray):\n            y_true = y_true.to_numpy()\n\n        if self.transformer_y:\n            y_true = transform_numpy(\n                         array             = y_true,\n                         transformer       = self.transformer_y,\n                         fit               = False,\n                         inverse_transform = False\n                     )\n            y_pred = transform_numpy(\n                         array             = y_pred,\n                         transformer       = self.transformer_y,\n                         fit               = False,\n                         inverse_transform = False\n                     )\n        \n        if self.differentiation is not None:\n            differentiator = copy(self.differentiator)\n            differentiator.set_params(window_size=None)\n            y_true = differentiator.fit_transform(y_true)[self.differentiation:]\n            y_pred = differentiator.fit_transform(y_pred)[self.differentiation:]\n        \n        residuals = y_true - y_pred\n        data = pd.DataFrame({'prediction': y_pred, 'residuals': residuals})\n        data['bin'] = self.binner.transform(y_pred).astype(int)\n        residuals_by_bin = data.groupby('bin')['residuals'].apply(np.array).to_dict()\n\n        if append and self.out_sample_residuals_by_bin_ is not None:\n            for k, v in residuals_by_bin.items():\n                if k in self.out_sample_residuals_by_bin_:\n                    self.out_sample_residuals_by_bin_[k] = np.concatenate((\n                        self.out_sample_residuals_by_bin_[k], v)\n                    )\n                else:\n                    self.out_sample_residuals_by_bin_[k] = v\n        else:\n            self.out_sample_residuals_by_bin_ = residuals_by_bin\n\n        max_samples = 10_000 // self.binner.n_bins_\n        rng = np.random.default_rng(seed=random_state)\n        for k, v in self.out_sample_residuals_by_bin_.items():\n            if len(v) > max_samples:\n                sample = rng.choice(a=v, size=max_samples, replace=False)\n                self.out_sample_residuals_by_bin_[k] = sample\n\n        for k in self.in_sample_residuals_by_bin_.keys():\n            if k not in self.out_sample_residuals_by_bin_:\n                self.out_sample_residuals_by_bin_[k] = np.array([])\n\n        empty_bins = [\n            k for k, v in self.out_sample_residuals_by_bin_.items() \n            if len(v) == 0\n        ]\n        if empty_bins:\n            warnings.warn(\n                f\"The following bins have no out of sample residuals: {empty_bins}. \"\n                f\"No predicted values fall in the interval \"\n                f\"{[self.binner_intervals_[bin] for bin in empty_bins]}. \"\n                f\"Empty bins will be filled with a random sample of residuals.\"\n            )\n            for k in empty_bins:\n                self.out_sample_residuals_by_bin_[k] = rng.choice(\n                    a       = residuals,\n                    size    = max_samples,\n                    replace = True\n                )\n\n        self.out_sample_residuals_ = np.concatenate(list(\n                                         self.out_sample_residuals_by_bin_.values()\n                                     ))\n\n    def get_feature_importances(\n        self,\n        sort_importance: bool = True\n    ) -> pd.DataFrame:\n        \"\"\"\n        Return feature importances of the regressor stored in the forecaster.\n        Only valid when regressor stores internally the feature importances in the\n        attribute `feature_importances_` or `coef_`. Otherwise, returns `None`.\n\n        Parameters\n        ----------\n        sort_importance: bool, default `True`\n            If `True`, sorts the feature importances in descending order.\n\n        Returns\n        -------\n        feature_importances : pandas DataFrame\n            Feature importances associated with each predictor.\n\n        \"\"\"\n\n        if not self.is_fitted:\n            raise NotFittedError(\n                \"This forecaster is not fitted yet. Call `fit` with appropriate \"\n                \"arguments before using `get_feature_importances()`.\"\n            )\n\n        if isinstance(self.regressor, Pipeline):\n            estimator = self.regressor[-1]\n        else:\n            estimator = self.regressor\n\n        if hasattr(estimator, 'feature_importances_'):\n            feature_importances = estimator.feature_importances_\n        elif hasattr(estimator, 'coef_'):\n            feature_importances = estimator.coef_\n        else:\n            warnings.warn(\n                f\"Impossible to access feature importances for regressor of type \"\n                f\"{type(estimator)}. This method is only valid when the \"\n                f\"regressor stores internally the feature importances in the \"\n                f\"attribute `feature_importances_` or `coef_`.\"\n            )\n            feature_importances = None\n\n        if feature_importances is not None:\n            feature_importances = pd.DataFrame({\n                                      'feature': self.X_train_features_names_out_,\n                                      'importance': feature_importances\n                                  })\n            if sort_importance:\n                feature_importances = feature_importances.sort_values(\n                                          by='importance', ascending=False\n                                      )\n\n        return feature_importances\n"
  },
  "GT_src_dict": {
    "skforecast/recursive/_forecaster_recursive.py": {
      "ForecasterRecursive.__init__": {
        "code": "    def __init__(self, regressor: object, lags: Optional[Union[int, list, np.ndarray, range]]=None, window_features: Optional[Union[object, list]]=None, transformer_y: Optional[object]=None, transformer_exog: Optional[object]=None, weight_func: Optional[Callable]=None, differentiation: Optional[int]=None, fit_kwargs: Optional[dict]=None, binner_kwargs: Optional[dict]=None, forecaster_id: Optional[Union[str, int]]=None) -> None:\n        \"\"\"Initializes the ForecasterRecursive class, which implements a recursive autoregressive \n    multi-step forecasting method using a scikit-learn compatible regressor. It manages \n    lagged values, window features, and any data transformations applied to the target \n    and exogenous variables.\n\n    Parameters\n    ----------\n    regressor : object\n        A scikit-learn compatible regressor or pipeline used for generating forecasts.\n    lags : Optional[Union[int, list, np.ndarray, range]], default=None\n        Defines the lagged values to be used as predictors. Can be an integer specifying \n        the maximum lag, a list/array of specific lags, or None.\n    window_features : Optional[Union[object, list]], default=None\n        Instantiates or a list of instances to create additional predictors from the \n        time series.\n    transformer_y : Optional[object], default=None\n        A scikit-learn compatible transformer for the target variable `y` applied before \n        training.\n    transformer_exog : Optional[object], default=None\n        A scikit-learn compatible transformer for exogenous variables applied before training.\n    weight_func : Optional[Callable], default=None\n        A callable function that assigns weights to each sample based on the index.\n    differentiation : Optional[int], default=None\n        The order of differencing to be applied to the target variable before training.\n    fit_kwargs : Optional[dict], default=None\n        Additional keyword arguments to pass to the regressor's `fit` method.\n    binner_kwargs : Optional[dict], default=None\n        Parameters for the QuantileBinner used for binning residuals.\n    forecaster_id : Optional[Union[str, int]], default=None\n        An identifier for instances of the forecaster.\n\n    Attributes\n    ----------\n    creation_date : str\n        The timestamp of when the forecaster instance was created.\n    is_fitted : bool\n        A flag indicating whether the regressor has been fitted.\n    ...\n\n    Raises\n    ------\n    ValueError\n        If neither `lags` nor `window_features` are provided.\n\n    Notes\n    -----\n    This class relies on several helper functions from the utils module, such as \n    `initialize_lags`, `initialize_window_features`, and `initialize_weights`, \n    to set up lags, window features, and weights accordingly.\"\"\"\n        self.regressor = copy(regressor)\n        self.transformer_y = transformer_y\n        self.transformer_exog = transformer_exog\n        self.weight_func = weight_func\n        self.source_code_weight_func = None\n        self.differentiation = differentiation\n        self.differentiator = None\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.exog_in_ = False\n        self.exog_names_in_ = None\n        self.exog_type_in_ = None\n        self.exog_dtypes_in_ = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_ = None\n        self.X_train_features_names_out_ = None\n        self.in_sample_residuals_ = None\n        self.out_sample_residuals_ = None\n        self.in_sample_residuals_by_bin_ = None\n        self.out_sample_residuals_by_bin_ = None\n        self.creation_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.is_fitted = False\n        self.fit_date = None\n        self.skforecast_version = skforecast.__version__\n        self.python_version = sys.version.split(' ')[0]\n        self.forecaster_id = forecaster_id\n        self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\n        self.window_features, self.window_features_names, self.max_size_window_features = initialize_window_features(window_features)\n        if self.window_features is None and self.lags is None:\n            raise ValueError('At least one of the arguments `lags` or `window_features` must be different from None. This is required to create the predictors used in training the forecaster.')\n        self.window_size = max([ws for ws in [self.max_lag, self.max_size_window_features] if ws is not None])\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [type(wf).__name__ for wf in self.window_features]\n        self.binner_kwargs = binner_kwargs\n        if binner_kwargs is None:\n            self.binner_kwargs = {'n_bins': 10, 'method': 'linear', 'subsample': 200000, 'random_state': 789654, 'dtype': np.float64}\n        self.binner = QuantileBinner(**self.binner_kwargs)\n        self.binner_intervals_ = None\n        if self.differentiation is not None:\n            if not isinstance(differentiation, int) or differentiation < 1:\n                raise ValueError(f'Argument `differentiation` must be an integer equal to or greater than 1. Got {differentiation}.')\n            self.window_size += self.differentiation\n            self.differentiator = TimeSeriesDifferentiator(order=self.differentiation, window_size=self.window_size)\n        self.weight_func, self.source_code_weight_func, _ = initialize_weights(forecaster_name=type(self).__name__, regressor=regressor, weight_func=weight_func, series_weights=None)\n        self.fit_kwargs = check_select_fit_kwargs(regressor=regressor, fit_kwargs=fit_kwargs)",
        "docstring": "Initializes the ForecasterRecursive class, which implements a recursive autoregressive \nmulti-step forecasting method using a scikit-learn compatible regressor. It manages \nlagged values, window features, and any data transformations applied to the target \nand exogenous variables.\n\nParameters\n----------\nregressor : object\n    A scikit-learn compatible regressor or pipeline used for generating forecasts.\nlags : Optional[Union[int, list, np.ndarray, range]], default=None\n    Defines the lagged values to be used as predictors. Can be an integer specifying \n    the maximum lag, a list/array of specific lags, or None.\nwindow_features : Optional[Union[object, list]], default=None\n    Instantiates or a list of instances to create additional predictors from the \n    time series.\ntransformer_y : Optional[object], default=None\n    A scikit-learn compatible transformer for the target variable `y` applied before \n    training.\ntransformer_exog : Optional[object], default=None\n    A scikit-learn compatible transformer for exogenous variables applied before training.\nweight_func : Optional[Callable], default=None\n    A callable function that assigns weights to each sample based on the index.\ndifferentiation : Optional[int], default=None\n    The order of differencing to be applied to the target variable before training.\nfit_kwargs : Optional[dict], default=None\n    Additional keyword arguments to pass to the regressor's `fit` method.\nbinner_kwargs : Optional[dict], default=None\n    Parameters for the QuantileBinner used for binning residuals.\nforecaster_id : Optional[Union[str, int]], default=None\n    An identifier for instances of the forecaster.\n\nAttributes\n----------\ncreation_date : str\n    The timestamp of when the forecaster instance was created.\nis_fitted : bool\n    A flag indicating whether the regressor has been fitted.\n...\n\nRaises\n------\nValueError\n    If neither `lags` nor `window_features` are provided.\n\nNotes\n-----\nThis class relies on several helper functions from the utils module, such as \n`initialize_lags`, `initialize_window_features`, and `initialize_weights`, \nto set up lags, window features, and weights accordingly.",
        "signature": "def __init__(self, regressor: object, lags: Optional[Union[int, list, np.ndarray, range]]=None, window_features: Optional[Union[object, list]]=None, transformer_y: Optional[object]=None, transformer_exog: Optional[object]=None, weight_func: Optional[Callable]=None, differentiation: Optional[int]=None, fit_kwargs: Optional[dict]=None, binner_kwargs: Optional[dict]=None, forecaster_id: Optional[Union[str, int]]=None) -> None:",
        "type": "Method",
        "class_signature": "class ForecasterRecursive(ForecasterBase):"
      },
      "ForecasterRecursive.fit": {
        "code": "    def fit(self, y: pd.Series, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, store_last_window: bool=True, store_in_sample_residuals: bool=True, random_state: int=123) -> None:\n        \"\"\"Fit the forecaster to the training data, learning the relationship between the time series and any exogenous variables, if provided. This method prepares the forecaster for making predictions by creating training matrices and fitting the specified regressor.\n\nParameters\n----------\ny : pd.Series\n    The training time series data, which the forecaster will learn to predict.\nexog : Optional[Union[pd.Series, pd.DataFrame]], default `None`\n    Exogenous variables to be included as additional predictors. Must align with the index of `y`.\nstore_last_window : bool, default `True`\n    If `True`, saves the last observed values in the training data for future predictions.\nstore_in_sample_residuals : bool, default `True`\n    If `True`, in-sample residuals will be calculated and stored for later use.\nrandom_state : int, default `123`\n    Seed for random number generation to ensure reproducibility of results.\n\nReturns\n-------\nNone\n\nNotes\n-----\n- This method interacts with the `create_train_X_y` method to prepare training inputs and is essential for fitting the regressor.\n- It utilizes `create_sample_weights` to generate observation weights based on the index, which can influence the fitting process.\n- The method updates various internal states, including `is_fitted`, to indicate the model's readiness for making predictions, and handles the storage of important attributes like `last_window_`, which is crucial for the prediction cycle.\n- If `exog` data is provided, it updates the attributes related to exogenous inputs, allowing for more complex forecasting scenarios.\n- Internal calls to methods like `_binning_in_sample_residuals` allow the class to maintain residual distributions, which can be important for making robust predictions later.\"\"\"\n        '\\n        Training Forecaster.\\n\\n        Additional arguments to be passed to the `fit` method of the regressor \\n        can be added with the `fit_kwargs` argument when initializing the forecaster.\\n        \\n        Parameters\\n        ----------\\n        y : pandas Series\\n            Training time series.\\n        exog : pandas Series, pandas DataFrame, default `None`\\n            Exogenous variable/s included as predictor/s. Must have the same\\n            number of observations as `y` and their indexes must be aligned so\\n            that y[i] is regressed on exog[i].\\n        store_last_window : bool, default `True`\\n            Whether or not to store the last window (`last_window_`) of training data.\\n        store_in_sample_residuals : bool, default `True`\\n            If `True`, in-sample residuals will be stored in the forecaster object\\n            after fitting (`in_sample_residuals_` attribute).\\n        random_state : int, default `123`\\n            Set a seed for the random generator so that the stored sample \\n            residuals are always deterministic.\\n\\n        Returns\\n        -------\\n        None\\n        \\n        '\n        self.last_window_ = None\n        self.index_type_ = None\n        self.index_freq_ = None\n        self.training_range_ = None\n        self.exog_in_ = False\n        self.exog_names_in_ = None\n        self.exog_type_in_ = None\n        self.exog_dtypes_in_ = None\n        self.X_train_window_features_names_out_ = None\n        self.X_train_exog_names_out_ = None\n        self.X_train_features_names_out_ = None\n        self.in_sample_residuals_ = None\n        self.is_fitted = False\n        self.fit_date = None\n        X_train, y_train, exog_names_in_, X_train_window_features_names_out_, X_train_exog_names_out_, X_train_features_names_out_, exog_dtypes_in_ = self._create_train_X_y(y=y, exog=exog)\n        sample_weight = self.create_sample_weights(X_train=X_train)\n        if sample_weight is not None:\n            self.regressor.fit(X=X_train, y=y_train, sample_weight=sample_weight, **self.fit_kwargs)\n        else:\n            self.regressor.fit(X=X_train, y=y_train, **self.fit_kwargs)\n        self.X_train_window_features_names_out_ = X_train_window_features_names_out_\n        self.X_train_features_names_out_ = X_train_features_names_out_\n        self.is_fitted = True\n        self.fit_date = pd.Timestamp.today().strftime('%Y-%m-%d %H:%M:%S')\n        self.training_range_ = preprocess_y(y=y, return_values=False)[1][[0, -1]]\n        self.index_type_ = type(X_train.index)\n        if isinstance(X_train.index, pd.DatetimeIndex):\n            self.index_freq_ = X_train.index.freqstr\n        else:\n            self.index_freq_ = X_train.index.step\n        if exog is not None:\n            self.exog_in_ = True\n            self.exog_type_in_ = type(exog)\n            self.exog_names_in_ = exog_names_in_\n            self.exog_dtypes_in_ = exog_dtypes_in_\n            self.X_train_exog_names_out_ = X_train_exog_names_out_\n        if store_in_sample_residuals:\n            self._binning_in_sample_residuals(y_true=y_train.to_numpy(), y_pred=self.regressor.predict(X_train).ravel(), random_state=random_state)\n        if store_last_window:\n            self.last_window_ = y.iloc[-self.window_size:].copy().to_frame(name=y.name if y.name is not None else 'y')",
        "docstring": "Fit the forecaster to the training data, learning the relationship between the time series and any exogenous variables, if provided. This method prepares the forecaster for making predictions by creating training matrices and fitting the specified regressor.\n\nParameters\n----------\ny : pd.Series\n    The training time series data, which the forecaster will learn to predict.\nexog : Optional[Union[pd.Series, pd.DataFrame]], default `None`\n    Exogenous variables to be included as additional predictors. Must align with the index of `y`.\nstore_last_window : bool, default `True`\n    If `True`, saves the last observed values in the training data for future predictions.\nstore_in_sample_residuals : bool, default `True`\n    If `True`, in-sample residuals will be calculated and stored for later use.\nrandom_state : int, default `123`\n    Seed for random number generation to ensure reproducibility of results.\n\nReturns\n-------\nNone\n\nNotes\n-----\n- This method interacts with the `create_train_X_y` method to prepare training inputs and is essential for fitting the regressor.\n- It utilizes `create_sample_weights` to generate observation weights based on the index, which can influence the fitting process.\n- The method updates various internal states, including `is_fitted`, to indicate the model's readiness for making predictions, and handles the storage of important attributes like `last_window_`, which is crucial for the prediction cycle.\n- If `exog` data is provided, it updates the attributes related to exogenous inputs, allowing for more complex forecasting scenarios.\n- Internal calls to methods like `_binning_in_sample_residuals` allow the class to maintain residual distributions, which can be important for making robust predictions later.",
        "signature": "def fit(self, y: pd.Series, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, store_last_window: bool=True, store_in_sample_residuals: bool=True, random_state: int=123) -> None:",
        "type": "Method",
        "class_signature": "class ForecasterRecursive(ForecasterBase):"
      },
      "ForecasterRecursive._binning_in_sample_residuals": {
        "code": "    def _binning_in_sample_residuals(self, y_true: np.ndarray, y_pred: np.ndarray, random_state: int=123) -> None:\n        \"\"\"Binning in-sample residuals according to the predicted values associated with each residual. The method fits a `QuantileBinner` object from the `skforecast.preprocessing` module to the predicted values, bins the residuals, and stores them in the forecaster object. If a `transformer_y` is applied, it transforms both true and predicted values before calculating residuals. The method also handles any necessary differentiation.\n\nParameters\n----------\ny_true : np.ndarray\n    The true values of the time series, used to compute residuals.\ny_pred : np.ndarray\n    The predicted values of the time series.\nrandom_state : int, default `123`\n    A seed for the random number generator, ensuring reproducibility.\n\nReturns\n-------\nNone\n\nSide Effects\n-------------\n1. Updates the attribute `in_sample_residuals_` with concatenated residuals.\n2. Constructs and updates `in_sample_residuals_by_bin_`, a dictionary categorizing residuals by predicted value intervals.\n3. The method limits the number of stored residuals per bin to `10,000 // self.binner.n_bins_`.\n\nDependencies\n------------\nThe method interacts with the `QuantileBinner` class, which is responsible for dividing residuals into bins based on the predicted values. It also utilizes the `transformer_y` for preprocessing and `self.binner.intervals_` for accessing the defined intervals of the bins post-fitting.\"\"\"\n        '\\n        Binning residuals according to the predicted value each residual is\\n        associated with. First a skforecast.preprocessing.QuantileBinner object\\n        is fitted to the predicted values. Then, residuals are binned according\\n        to the predicted value each residual is associated with. Residuals are\\n        stored in the forecaster object as `in_sample_residuals_` and\\n        `in_sample_residuals_by_bin_`.\\n        If `transformer_y` is not `None`, `y_true` and `y_pred` are transformed\\n        before calculating residuals. If `differentiation` is not `None`, `y_true`\\n        and `y_pred` are differentiated before calculating residuals. If both,\\n        `transformer_y` and `differentiation` are not `None`, transformation is\\n        done before differentiation. The number of residuals stored per bin is\\n        limited to  `10_000 // self.binner.n_bins_`. The total number of residuals\\n        stored is `10_000`.\\n        **New in version 0.14.0**\\n\\n        Parameters\\n        ----------\\n        y_true : numpy ndarray\\n            True values of the time series.\\n        y_pred : numpy ndarray\\n            Predicted values of the time series.\\n        random_state : int, default `123`\\n            Set a seed for the random generator so that the stored sample \\n            residuals are always deterministic.\\n\\n        Returns\\n        -------\\n        None\\n        \\n        '\n        data = pd.DataFrame({'prediction': y_pred, 'residuals': y_true - y_pred})\n        data['bin'] = self.binner.fit_transform(y_pred).astype(int)\n        self.in_sample_residuals_by_bin_ = data.groupby('bin')['residuals'].apply(np.array).to_dict()\n        rng = np.random.default_rng(seed=random_state)\n        max_sample = 10000 // self.binner.n_bins_\n        for k, v in self.in_sample_residuals_by_bin_.items():\n            if len(v) > max_sample:\n                sample = v[rng.integers(low=0, high=len(v), size=max_sample)]\n                self.in_sample_residuals_by_bin_[k] = sample\n        self.in_sample_residuals_ = np.concatenate(list(self.in_sample_residuals_by_bin_.values()))\n        self.binner_intervals_ = self.binner.intervals_",
        "docstring": "Binning in-sample residuals according to the predicted values associated with each residual. The method fits a `QuantileBinner` object from the `skforecast.preprocessing` module to the predicted values, bins the residuals, and stores them in the forecaster object. If a `transformer_y` is applied, it transforms both true and predicted values before calculating residuals. The method also handles any necessary differentiation.\n\nParameters\n----------\ny_true : np.ndarray\n    The true values of the time series, used to compute residuals.\ny_pred : np.ndarray\n    The predicted values of the time series.\nrandom_state : int, default `123`\n    A seed for the random number generator, ensuring reproducibility.\n\nReturns\n-------\nNone\n\nSide Effects\n-------------\n1. Updates the attribute `in_sample_residuals_` with concatenated residuals.\n2. Constructs and updates `in_sample_residuals_by_bin_`, a dictionary categorizing residuals by predicted value intervals.\n3. The method limits the number of stored residuals per bin to `10,000 // self.binner.n_bins_`.\n\nDependencies\n------------\nThe method interacts with the `QuantileBinner` class, which is responsible for dividing residuals into bins based on the predicted values. It also utilizes the `transformer_y` for preprocessing and `self.binner.intervals_` for accessing the defined intervals of the bins post-fitting.",
        "signature": "def _binning_in_sample_residuals(self, y_true: np.ndarray, y_pred: np.ndarray, random_state: int=123) -> None:",
        "type": "Method",
        "class_signature": "class ForecasterRecursive(ForecasterBase):"
      }
    }
  },
  "dependency_dict": {
    "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:__init__": {
      "skforecast/utils/utils.py": {
        "initialize_lags": {
          "code": "def initialize_lags(\n    forecaster_name: str,\n    lags: Any\n) -> Union[Optional[np.ndarray], Optional[list], Optional[int]]:\n    \"\"\"\n    Check lags argument input and generate the corresponding numpy ndarray.\n\n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    lags : Any\n        Lags used as predictors.\n\n    Returns\n    -------\n    lags : numpy ndarray, None\n        Lags used as predictors.\n    lags_names : list, None\n        Names of the lags used as predictors.\n    max_lag : int, None\n        Maximum value of the lags.\n    \n    \"\"\"\n\n    lags_names = None\n    max_lag = None\n    if lags is not None:\n        if isinstance(lags, int):\n            if lags < 1:\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n            lags = np.arange(1, lags + 1)\n\n        if isinstance(lags, (list, tuple, range)):\n            lags = np.array(lags)\n        \n        if isinstance(lags, np.ndarray):\n            if lags.size == 0:\n                return None, None, None\n            if lags.ndim != 1:\n                raise ValueError(\"`lags` must be a 1-dimensional array.\")\n            if not np.issubdtype(lags.dtype, np.integer):\n                raise TypeError(\"All values in `lags` must be integers.\")\n            if np.any(lags < 1):\n                raise ValueError(\"Minimum value of lags allowed is 1.\")\n        else:\n            if forecaster_name != 'ForecasterDirectMultiVariate':\n                raise TypeError(\n                    (f\"`lags` argument must be an int, 1d numpy ndarray, range, \"\n                     f\"tuple or list. Got {type(lags)}.\")\n                )\n            else:\n                raise TypeError(\n                    (f\"`lags` argument must be a dict, int, 1d numpy ndarray, range, \"\n                     f\"tuple or list. Got {type(lags)}.\")\n                )\n        \n        lags_names = [f'lag_{i}' for i in lags]\n        max_lag = max(lags)\n\n    return lags, lags_names, max_lag",
          "docstring": "Check lags argument input and generate the corresponding numpy ndarray.\n\nParameters\n----------\nforecaster_name : str\n    Forecaster name.\nlags : Any\n    Lags used as predictors.\n\nReturns\n-------\nlags : numpy ndarray, None\n    Lags used as predictors.\nlags_names : list, None\n    Names of the lags used as predictors.\nmax_lag : int, None\n    Maximum value of the lags.",
          "signature": "def initialize_lags(forecaster_name: str, lags: Any) -> Union[Optional[np.ndarray], Optional[list], Optional[int]]:",
          "type": "Function",
          "class_signature": null
        },
        "initialize_window_features": {
          "code": "def initialize_window_features(\n    window_features: Any\n) -> Union[Optional[list], Optional[list], Optional[int]]:\n    \"\"\"\n    Check window_features argument input and generate the corresponding list.\n\n    Parameters\n    ----------\n    window_features : Any\n        Classes used to create window features.\n\n    Returns\n    -------\n    window_features : list, None\n        List of classes used to create window features.\n    window_features_names : list, None\n        List with all the features names of the window features.\n    max_size_window_features : int, None\n        Maximum value of the `window_sizes` attribute of all classes.\n    \n    \"\"\"\n\n    needed_atts = ['window_sizes', 'features_names']\n    needed_methods = ['transform_batch', 'transform']\n\n    max_window_sizes = None\n    window_features_names = None\n    max_size_window_features = None\n    if window_features is not None:\n        if isinstance(window_features, list) and len(window_features) < 1:\n            raise ValueError(\n                \"Argument `window_features` must contain at least one element.\"\n            )\n        if not isinstance(window_features, list):\n            window_features = [window_features]\n\n        link_to_docs = (\n            \"\\nVisit the documentation for more information about how to create \"\n            \"custom window features:\\n\"\n            \"https://skforecast.org/latest/user_guides/window-features-and-custom-features.html#create-your-custom-window-features\"\n        )\n        \n        max_window_sizes = []\n        window_features_names = []\n        for wf in window_features:\n            wf_name = type(wf).__name__\n            atts_methods = set([a for a in dir(wf)])\n            if not set(needed_atts).issubset(atts_methods):\n                raise ValueError(\n                    f\"{wf_name} must have the attributes: {needed_atts}.\" + link_to_docs\n                )\n            if not set(needed_methods).issubset(atts_methods):\n                raise ValueError(\n                    f\"{wf_name} must have the methods: {needed_methods}.\" + link_to_docs\n                )\n            \n            window_sizes = wf.window_sizes\n            if not isinstance(window_sizes, (int, list)):\n                raise TypeError(\n                    f\"Attribute `window_sizes` of {wf_name} must be an int or a list \"\n                    f\"of ints. Got {type(window_sizes)}.\" + link_to_docs\n                )\n            \n            if isinstance(window_sizes, int):\n                if window_sizes < 1:\n                    raise ValueError(\n                        f\"If argument `window_sizes` is an integer, it must be equal to or \"\n                        f\"greater than 1. Got {window_sizes} from {wf_name}.\" + link_to_docs\n                    )\n                max_window_sizes.append(window_sizes)\n            else:\n                if not all(isinstance(ws, int) for ws in window_sizes) or not all(\n                    ws >= 1 for ws in window_sizes\n                ):                    \n                    raise ValueError(\n                        f\"If argument `window_sizes` is a list, all elements must be integers \"\n                        f\"equal to or greater than 1. Got {window_sizes} from {wf_name}.\" + link_to_docs\n                    )\n                max_window_sizes.append(max(window_sizes))\n\n            features_names = wf.features_names\n            if not isinstance(features_names, (str, list)):\n                raise TypeError(\n                    f\"Attribute `features_names` of {wf_name} must be a str or \"\n                    f\"a list of strings. Got {type(features_names)}.\" + link_to_docs\n                )\n            if isinstance(features_names, str):\n                window_features_names.append(features_names)\n            else:\n                if not all(isinstance(fn, str) for fn in features_names):\n                    raise TypeError(\n                        f\"If argument `features_names` is a list, all elements \"\n                        f\"must be strings. Got {features_names} from {wf_name}.\" + link_to_docs\n                    )\n                window_features_names.extend(features_names)\n\n        max_size_window_features = max(max_window_sizes)\n        if len(set(window_features_names)) != len(window_features_names):\n            raise ValueError(\n                f\"All window features names must be unique. Got {window_features_names}.\"\n            )\n\n    return window_features, window_features_names, max_size_window_features",
          "docstring": "Check window_features argument input and generate the corresponding list.\n\nParameters\n----------\nwindow_features : Any\n    Classes used to create window features.\n\nReturns\n-------\nwindow_features : list, None\n    List of classes used to create window features.\nwindow_features_names : list, None\n    List with all the features names of the window features.\nmax_size_window_features : int, None\n    Maximum value of the `window_sizes` attribute of all classes.",
          "signature": "def initialize_window_features(window_features: Any) -> Union[Optional[list], Optional[list], Optional[int]]:",
          "type": "Function",
          "class_signature": null
        },
        "initialize_weights": {
          "code": "def initialize_weights(\n    forecaster_name: str,\n    regressor: object,\n    weight_func: Union[Callable, dict],\n    series_weights: dict\n) -> Tuple[Union[Callable, dict], Union[str, dict], dict]:\n    \"\"\"\n    Check weights arguments, `weight_func` and `series_weights` for the different \n    forecasters. Create `source_code_weight_func`, source code of the custom \n    function(s) used to create weights.\n    \n    Parameters\n    ----------\n    forecaster_name : str\n        Forecaster name.\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        Regressor of the forecaster.\n    weight_func : Callable, dict\n        Argument `weight_func` of the forecaster.\n    series_weights : dict\n        Argument `series_weights` of the forecaster.\n\n    Returns\n    -------\n    weight_func : Callable, dict\n        Argument `weight_func` of the forecaster.\n    source_code_weight_func : str, dict\n        Argument `source_code_weight_func` of the forecaster.\n    series_weights : dict\n        Argument `series_weights` of the forecaster.\n    \n    \"\"\"\n\n    source_code_weight_func = None\n\n    if weight_func is not None:\n\n        if forecaster_name in ['ForecasterRecursiveMultiSeries']:\n            if not isinstance(weight_func, (Callable, dict)):\n                raise TypeError(\n                    (f\"Argument `weight_func` must be a Callable or a dict of \"\n                     f\"Callables. Got {type(weight_func)}.\")\n                )\n        elif not isinstance(weight_func, Callable):\n            raise TypeError(\n                f\"Argument `weight_func` must be a Callable. Got {type(weight_func)}.\"\n            )\n        \n        if isinstance(weight_func, dict):\n            source_code_weight_func = {}\n            for key in weight_func:\n                source_code_weight_func[key] = inspect.getsource(weight_func[key])\n        else:\n            source_code_weight_func = inspect.getsource(weight_func)\n\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(\n                (f\"Argument `weight_func` is ignored since regressor {regressor} \"\n                 f\"does not accept `sample_weight` in its `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n            weight_func = None\n            source_code_weight_func = None\n\n    if series_weights is not None:\n        if not isinstance(series_weights, dict):\n            raise TypeError(\n                (f\"Argument `series_weights` must be a dict of floats or ints.\"\n                 f\"Got {type(series_weights)}.\")\n            )\n        if 'sample_weight' not in inspect.signature(regressor.fit).parameters:\n            warnings.warn(\n                (f\"Argument `series_weights` is ignored since regressor {regressor} \"\n                 f\"does not accept `sample_weight` in its `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n            series_weights = None\n\n    return weight_func, source_code_weight_func, series_weights",
          "docstring": "Check weights arguments, `weight_func` and `series_weights` for the different \nforecasters. Create `source_code_weight_func`, source code of the custom \nfunction(s) used to create weights.\n\nParameters\n----------\nforecaster_name : str\n    Forecaster name.\nregressor : regressor or pipeline compatible with the scikit-learn API\n    Regressor of the forecaster.\nweight_func : Callable, dict\n    Argument `weight_func` of the forecaster.\nseries_weights : dict\n    Argument `series_weights` of the forecaster.\n\nReturns\n-------\nweight_func : Callable, dict\n    Argument `weight_func` of the forecaster.\nsource_code_weight_func : str, dict\n    Argument `source_code_weight_func` of the forecaster.\nseries_weights : dict\n    Argument `series_weights` of the forecaster.",
          "signature": "def initialize_weights(forecaster_name: str, regressor: object, weight_func: Union[Callable, dict], series_weights: dict) -> Tuple[Union[Callable, dict], Union[str, dict], dict]:",
          "type": "Function",
          "class_signature": null
        },
        "check_select_fit_kwargs": {
          "code": "def check_select_fit_kwargs(\n    regressor: object,\n    fit_kwargs: Optional[dict] = None\n) -> dict:\n    \"\"\"\n    Check if `fit_kwargs` is a dict and select only the keys that are used by\n    the `fit` method of the regressor.\n\n    Parameters\n    ----------\n    regressor : object\n        Regressor object.\n    fit_kwargs : dict, default `None`\n        Dictionary with the arguments to pass to the `fit' method of the forecaster.\n\n    Returns\n    -------\n    fit_kwargs : dict\n        Dictionary with the arguments to be passed to the `fit` method of the \n        regressor after removing the unused keys.\n    \n    \"\"\"\n\n    if fit_kwargs is None:\n        fit_kwargs = {}\n    else:\n        if not isinstance(fit_kwargs, dict):\n            raise TypeError(\n                f\"Argument `fit_kwargs` must be a dict. Got {type(fit_kwargs)}.\"\n            )\n\n        # Non used keys\n        non_used_keys = [k for k in fit_kwargs.keys()\n                         if k not in inspect.signature(regressor.fit).parameters]\n        if non_used_keys:\n            warnings.warn(\n                (f\"Argument/s {non_used_keys} ignored since they are not used by the \"\n                 f\"regressor's `fit` method.\"),\n                 IgnoredArgumentWarning\n            )\n\n        if 'sample_weight' in fit_kwargs.keys():\n            warnings.warn(\n                (\"The `sample_weight` argument is ignored. Use `weight_func` to pass \"\n                 \"a function that defines the individual weights for each sample \"\n                 \"based on its index.\"),\n                 IgnoredArgumentWarning\n            )\n            del fit_kwargs['sample_weight']\n\n        # Select only the keyword arguments allowed by the regressor's `fit` method.\n        fit_kwargs = {k: v for k, v in fit_kwargs.items()\n                      if k in inspect.signature(regressor.fit).parameters}\n\n    return fit_kwargs",
          "docstring": "Check if `fit_kwargs` is a dict and select only the keys that are used by\nthe `fit` method of the regressor.\n\nParameters\n----------\nregressor : object\n    Regressor object.\nfit_kwargs : dict, default `None`\n    Dictionary with the arguments to pass to the `fit' method of the forecaster.\n\nReturns\n-------\nfit_kwargs : dict\n    Dictionary with the arguments to be passed to the `fit` method of the \n    regressor after removing the unused keys.",
          "signature": "def check_select_fit_kwargs(regressor: object, fit_kwargs: Optional[dict]=None) -> dict:",
          "type": "Function",
          "class_signature": null
        }
      },
      "skforecast/preprocessing/preprocessing.py": {
        "QuantileBinner.__init__": {
          "code": "    def __init__(\n        self,\n        n_bins: int,\n        method: Optional[str] = \"linear\",\n        subsample: int = 200000,\n        dtype: Optional[type] = np.float64,\n        random_state: Optional[int] = 789654\n    ):\n        \n        self._validate_params(\n            n_bins,\n            method,\n            subsample,\n            dtype,\n            random_state\n        )\n\n        self.n_bins       = n_bins\n        self.method       = method\n        self.subsample    = subsample\n        self.random_state = random_state\n        self.dtype        = dtype\n        self.n_bins_      = None\n        self.bin_edges_   = None\n        self.intervals_   = None",
          "docstring": "",
          "signature": "def __init__(self, n_bins: int, method: Optional[str]='linear', subsample: int=200000, dtype: Optional[type]=np.float64, random_state: Optional[int]=789654):",
          "type": "Method",
          "class_signature": "class QuantileBinner:"
        }
      }
    },
    "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:_binning_in_sample_residuals": {
      "skforecast/preprocessing/preprocessing.py": {
        "QuantileBinner.fit_transform": {
          "code": "    def fit_transform(self, X):\n        \"\"\"\n        Fit the model to the data and return the bin indices for the same data.\n        \n        Parameters\n        ----------\n        X : numpy.ndarray\n            The data to fit and transform.\n        \n        Returns\n        -------\n        bin_indices : numpy.ndarray\n            The indices of the bins each value belongs to.\n            Values less than the smallest bin edge are assigned to the first bin,\n            and values greater than the largest bin edge are assigned to the last bin.\n        \n        \"\"\"\n\n        self.fit(X)\n\n        return self.transform(X)",
          "docstring": "Fit the model to the data and return the bin indices for the same data.\n\nParameters\n----------\nX : numpy.ndarray\n    The data to fit and transform.\n\nReturns\n-------\nbin_indices : numpy.ndarray\n    The indices of the bins each value belongs to.\n    Values less than the smallest bin edge are assigned to the first bin,\n    and values greater than the largest bin edge are assigned to the last bin.",
          "signature": "def fit_transform(self, X):",
          "type": "Method",
          "class_signature": "class QuantileBinner:"
        }
      }
    },
    "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:fit": {
      "skforecast/utils/utils.py": {
        "preprocess_y": {
          "code": "def preprocess_y(\n    y: Union[pd.Series, pd.DataFrame],\n    return_values: bool = True\n) -> Tuple[Union[None, np.ndarray], pd.Index]:\n    \"\"\"\n    Return values and index of series separately. Index is overwritten \n    according to the next rules:\n    \n    - If index is of type `DatetimeIndex` and has frequency, nothing is \n    changed.\n    - If index is of type `RangeIndex`, nothing is changed.\n    - If index is of type `DatetimeIndex` but has no frequency, a \n    `RangeIndex` is created.\n    - If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\n    \n    Parameters\n    ----------\n    y : pandas Series, pandas DataFrame\n        Time series.\n    return_values : bool, default `True`\n        If `True` return the values of `y` as numpy ndarray. This option is \n        intended to avoid copying data when it is not necessary.\n\n    Returns\n    -------\n    y_values : None, numpy ndarray\n        Numpy array with values of `y`.\n    y_index : pandas Index\n        Index of `y` modified according to the rules.\n    \n    \"\"\"\n    \n    if isinstance(y.index, pd.DatetimeIndex) and y.index.freq is not None:\n        y_index = y.index\n    elif isinstance(y.index, pd.RangeIndex):\n        y_index = y.index\n    elif isinstance(y.index, pd.DatetimeIndex) and y.index.freq is None:\n        warnings.warn(\n            (\"Series has DatetimeIndex index but no frequency. \"\n             \"Index is overwritten with a RangeIndex of step 1.\")\n        )\n        y_index = pd.RangeIndex(\n                      start = 0,\n                      stop  = len(y),\n                      step  = 1\n                  )\n    else:\n        warnings.warn(\n            (\"Series has no DatetimeIndex nor RangeIndex index. \"\n             \"Index is overwritten with a RangeIndex.\")\n        )\n        y_index = pd.RangeIndex(\n                      start = 0,\n                      stop  = len(y),\n                      step  = 1\n                  )\n\n    y_values = y.to_numpy(copy=True).ravel() if return_values else None\n\n    return y_values, y_index",
          "docstring": "Return values and index of series separately. Index is overwritten \naccording to the next rules:\n\n- If index is of type `DatetimeIndex` and has frequency, nothing is \nchanged.\n- If index is of type `RangeIndex`, nothing is changed.\n- If index is of type `DatetimeIndex` but has no frequency, a \n`RangeIndex` is created.\n- If index is not of type `DatetimeIndex`, a `RangeIndex` is created.\n\nParameters\n----------\ny : pandas Series, pandas DataFrame\n    Time series.\nreturn_values : bool, default `True`\n    If `True` return the values of `y` as numpy ndarray. This option is \n    intended to avoid copying data when it is not necessary.\n\nReturns\n-------\ny_values : None, numpy ndarray\n    Numpy array with values of `y`.\ny_index : pandas Index\n    Index of `y` modified according to the rules.",
          "signature": "def preprocess_y(y: Union[pd.Series, pd.DataFrame], return_values: bool=True) -> Tuple[Union[None, np.ndarray], pd.Index]:",
          "type": "Function",
          "class_signature": null
        }
      },
      "skforecast/recursive/_forecaster_recursive.py": {
        "ForecasterRecursive._create_train_X_y": {
          "code": "    def _create_train_X_y(self, y: pd.Series, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, pd.Series, list, list, list, list, dict]:\n        \"\"\"\n        Create training matrices from univariate time series and exogenous\n        variables.\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors).\n        y_train : pandas Series\n            Values of the time series related to each row of `X_train`.\n        exog_names_in_ : list\n            Names of the exogenous variables used during training.\n        X_train_window_features_names_out_ : list\n            Names of the window features included in the matrix `X_train` created\n            internally for training.\n        X_train_exog_names_out_ : list\n            Names of the exogenous variables included in the matrix `X_train` created\n            internally for training. It can be different from `exog_names_in_` if\n            some exogenous variables are transformed during the training process.\n        X_train_features_names_out_ : list\n            Names of the columns of the matrix created internally for training.\n        exog_dtypes_in_ : dict\n            Type of each exogenous variable/s used in training. If `transformer_exog` \n            is used, the dtypes are calculated before the transformation.\n        \n        \"\"\"\n        check_y(y=y)\n        y = input_to_frame(data=y, input_name='y')\n        if len(y) <= self.window_size:\n            raise ValueError(f'Length of `y` must be greater than the maximum window size needed by the forecaster.\\n    Length `y`: {len(y)}.\\n    Max window size: {self.window_size}.\\n    Lags window size: {self.max_lag}.\\n    Window features window size: {self.max_size_window_features}.')\n        fit_transformer = False if self.is_fitted else True\n        y = transform_dataframe(df=y, transformer=self.transformer_y, fit=fit_transformer, inverse_transform=False)\n        y_values, y_index = preprocess_y(y=y)\n        train_index = y_index[self.window_size:]\n        if self.differentiation is not None:\n            if not self.is_fitted:\n                y_values = self.differentiator.fit_transform(y_values)\n            else:\n                differentiator = copy(self.differentiator)\n                y_values = differentiator.fit_transform(y_values)\n        exog_names_in_ = None\n        exog_dtypes_in_ = None\n        categorical_features = False\n        if exog is not None:\n            check_exog(exog=exog, allow_nan=True)\n            exog = input_to_frame(data=exog, input_name='exog')\n            len_y = len(y_values)\n            len_train_index = len(train_index)\n            len_exog = len(exog)\n            if not len_exog == len_y and (not len_exog == len_train_index):\n                raise ValueError(f'Length of `exog` must be equal to the length of `y` (if index is fully aligned) or length of `y` - `window_size` (if `exog` starts after the first `window_size` values).\\n    `exog`              : ({exog.index[0]} -- {exog.index[-1]})  (n={len_exog})\\n    `y`                 : ({y.index[0]} -- {y.index[-1]})  (n={len_y})\\n    `y` - `window_size` : ({train_index[0]} -- {train_index[-1]})  (n={len_train_index})')\n            exog_names_in_ = exog.columns.to_list()\n            exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n            exog = transform_dataframe(df=exog, transformer=self.transformer_exog, fit=fit_transformer, inverse_transform=False)\n            check_exog_dtypes(exog, call_check_exog=True)\n            categorical_features = exog.select_dtypes(include=np.number).shape[1] != exog.shape[1]\n            _, exog_index = preprocess_exog(exog=exog, return_values=False)\n            if len_exog == len_y:\n                if not (exog_index == y_index).all():\n                    raise ValueError('When `exog` has the same length as `y`, the index of `exog` must be aligned with the index of `y` to ensure the correct alignment of values.')\n                exog = exog.iloc[self.window_size:,]\n            elif not (exog_index == train_index).all():\n                raise ValueError(\"When `exog` doesn't contain the first `window_size` observations, the index of `exog` must be aligned with the index of `y` minus the first `window_size` observations to ensure the correct alignment of values.\")\n        X_train = []\n        X_train_features_names_out_ = []\n        X_as_pandas = True if categorical_features else False\n        X_train_lags, y_train = self._create_lags(y=y_values, X_as_pandas=X_as_pandas, train_index=train_index)\n        if X_train_lags is not None:\n            X_train.append(X_train_lags)\n            X_train_features_names_out_.extend(self.lags_names)\n        X_train_window_features_names_out_ = None\n        if self.window_features is not None:\n            n_diff = 0 if self.differentiation is None else self.differentiation\n            y_window_features = pd.Series(y_values[n_diff:], index=y_index[n_diff:])\n            X_train_window_features, X_train_window_features_names_out_ = self._create_window_features(y=y_window_features, X_as_pandas=X_as_pandas, train_index=train_index)\n            X_train.extend(X_train_window_features)\n            X_train_features_names_out_.extend(X_train_window_features_names_out_)\n        X_train_exog_names_out_ = None\n        if exog is not None:\n            X_train_exog_names_out_ = exog.columns.to_list()\n            if not X_as_pandas:\n                exog = exog.to_numpy()\n            X_train_features_names_out_.extend(X_train_exog_names_out_)\n            X_train.append(exog)\n        if len(X_train) == 1:\n            X_train = X_train[0]\n        elif X_as_pandas:\n            X_train = pd.concat(X_train, axis=1)\n        else:\n            X_train = np.concatenate(X_train, axis=1)\n        if X_as_pandas:\n            X_train.index = train_index\n        else:\n            X_train = pd.DataFrame(data=X_train, index=train_index, columns=X_train_features_names_out_)\n        y_train = pd.Series(data=y_train, index=train_index, name='y')\n        return (X_train, y_train, exog_names_in_, X_train_window_features_names_out_, X_train_exog_names_out_, X_train_features_names_out_, exog_dtypes_in_)",
          "docstring": "Create training matrices from univariate time series and exogenous\nvariables.\n\nParameters\n----------\ny : pandas Series\n    Training time series.\nexog : pandas Series, pandas DataFrame, default `None`\n    Exogenous variable/s included as predictor/s. Must have the same\n    number of observations as `y` and their indexes must be aligned.\n\nReturns\n-------\nX_train : pandas DataFrame\n    Training values (predictors).\ny_train : pandas Series\n    Values of the time series related to each row of `X_train`.\nexog_names_in_ : list\n    Names of the exogenous variables used during training.\nX_train_window_features_names_out_ : list\n    Names of the window features included in the matrix `X_train` created\n    internally for training.\nX_train_exog_names_out_ : list\n    Names of the exogenous variables included in the matrix `X_train` created\n    internally for training. It can be different from `exog_names_in_` if\n    some exogenous variables are transformed during the training process.\nX_train_features_names_out_ : list\n    Names of the columns of the matrix created internally for training.\nexog_dtypes_in_ : dict\n    Type of each exogenous variable/s used in training. If `transformer_exog` \n    is used, the dtypes are calculated before the transformation.",
          "signature": "def _create_train_X_y(self, y: pd.Series, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, pd.Series, list, list, list, list, dict]:",
          "type": "Method",
          "class_signature": "class ForecasterRecursive(ForecasterBase):"
        },
        "ForecasterRecursive.create_sample_weights": {
          "code": "    def create_sample_weights(self, X_train: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Crate weights for each observation according to the forecaster's attribute\n        `weight_func`.\n\n        Parameters\n        ----------\n        X_train : pandas DataFrame\n            Dataframe created with the `create_train_X_y` method, first return.\n\n        Returns\n        -------\n        sample_weight : numpy ndarray\n            Weights to use in `fit` method.\n\n        \"\"\"\n        sample_weight = None\n        if self.weight_func is not None:\n            sample_weight = self.weight_func(X_train.index)\n        if sample_weight is not None:\n            if np.isnan(sample_weight).any():\n                raise ValueError('The resulting `sample_weight` cannot have NaN values.')\n            if np.any(sample_weight < 0):\n                raise ValueError('The resulting `sample_weight` cannot have negative values.')\n            if np.sum(sample_weight) == 0:\n                raise ValueError('The resulting `sample_weight` cannot be normalized because the sum of the weights is zero.')\n        return sample_weight",
          "docstring": "Crate weights for each observation according to the forecaster's attribute\n`weight_func`.\n\nParameters\n----------\nX_train : pandas DataFrame\n    Dataframe created with the `create_train_X_y` method, first return.\n\nReturns\n-------\nsample_weight : numpy ndarray\n    Weights to use in `fit` method.",
          "signature": "def create_sample_weights(self, X_train: pd.DataFrame) -> np.ndarray:",
          "type": "Method",
          "class_signature": "class ForecasterRecursive(ForecasterBase):"
        }
      }
    }
  },
  "PRD": "# PROJECT NAME: skforecast-test_binning_in_sample_residuals\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 skforecast/\n    \u2514\u2500\u2500 recursive/\n        \u2514\u2500\u2500 _forecaster_recursive.py\n            \u251c\u2500\u2500 ForecasterRecursive.__init__\n            \u251c\u2500\u2500 ForecasterRecursive._binning_in_sample_residuals\n            \u2514\u2500\u2500 ForecasterRecursive.fit\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThe module facilitates the binning and management of in-sample residuals for the `ForecasterRecursive` model, enhancing its error analysis and interpretability. It provides functionality to systematically group residuals into discrete bins based on predictive outputs, allowing developers to analyze error distributions and identify patterns or anomalies across predefined ranges. Additionally, the module implements controlled storage of residuals, capping their maximum count for efficient memory usage and performance. By offering these capabilities, it addresses the challenge of managing and analyzing large volumes of residuals in time series forecasting, enabling more scalable and actionable insights for model evaluation and refinement.\n\n## FILE 1: skforecast/recursive/_forecaster_recursive.py\n\n- CLASS METHOD: ForecasterRecursive._binning_in_sample_residuals\n  - CLASS SIGNATURE: class ForecasterRecursive(ForecasterBase):\n  - SIGNATURE: def _binning_in_sample_residuals(self, y_true: np.ndarray, y_pred: np.ndarray, random_state: int=123) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nBinning in-sample residuals according to the predicted values associated with each residual. The method fits a `QuantileBinner` object from the `skforecast.preprocessing` module to the predicted values, bins the residuals, and stores them in the forecaster object. If a `transformer_y` is applied, it transforms both true and predicted values before calculating residuals. The method also handles any necessary differentiation.\n\nParameters\n----------\ny_true : np.ndarray\n    The true values of the time series, used to compute residuals.\ny_pred : np.ndarray\n    The predicted values of the time series.\nrandom_state : int, default `123`\n    A seed for the random number generator, ensuring reproducibility.\n\nReturns\n-------\nNone\n\nSide Effects\n-------------\n1. Updates the attribute `in_sample_residuals_` with concatenated residuals.\n2. Constructs and updates `in_sample_residuals_by_bin_`, a dictionary categorizing residuals by predicted value intervals.\n3. The method limits the number of stored residuals per bin to `10,000 // self.binner.n_bins_`.\n\nDependencies\n------------\nThe method interacts with the `QuantileBinner` class, which is responsible for dividing residuals into bins based on the predicted values. It also utilizes the `transformer_y` for preprocessing and `self.binner.intervals_` for accessing the defined intervals of the bins post-fitting.\n\"\"\"\n```\n\n- CLASS METHOD: ForecasterRecursive.fit\n  - CLASS SIGNATURE: class ForecasterRecursive(ForecasterBase):\n  - SIGNATURE: def fit(self, y: pd.Series, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, store_last_window: bool=True, store_in_sample_residuals: bool=True, random_state: int=123) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nFit the forecaster to the training data, learning the relationship between the time series and any exogenous variables, if provided. This method prepares the forecaster for making predictions by creating training matrices and fitting the specified regressor.\n\nParameters\n----------\ny : pd.Series\n    The training time series data, which the forecaster will learn to predict.\nexog : Optional[Union[pd.Series, pd.DataFrame]], default `None`\n    Exogenous variables to be included as additional predictors. Must align with the index of `y`.\nstore_last_window : bool, default `True`\n    If `True`, saves the last observed values in the training data for future predictions.\nstore_in_sample_residuals : bool, default `True`\n    If `True`, in-sample residuals will be calculated and stored for later use.\nrandom_state : int, default `123`\n    Seed for random number generation to ensure reproducibility of results.\n\nReturns\n-------\nNone\n\nNotes\n-----\n- This method interacts with the `create_train_X_y` method to prepare training inputs and is essential for fitting the regressor.\n- It utilizes `create_sample_weights` to generate observation weights based on the index, which can influence the fitting process.\n- The method updates various internal states, including `is_fitted`, to indicate the model's readiness for making predictions, and handles the storage of important attributes like `last_window_`, which is crucial for the prediction cycle.\n- If `exog` data is provided, it updates the attributes related to exogenous inputs, allowing for more complex forecasting scenarios.\n- Internal calls to methods like `_binning_in_sample_residuals` allow the class to maintain residual distributions, which can be important for making robust predictions later.\n\"\"\"\n```\n\n- CLASS METHOD: ForecasterRecursive.__init__\n  - CLASS SIGNATURE: class ForecasterRecursive(ForecasterBase):\n  - SIGNATURE: def __init__(self, regressor: object, lags: Optional[Union[int, list, np.ndarray, range]]=None, window_features: Optional[Union[object, list]]=None, transformer_y: Optional[object]=None, transformer_exog: Optional[object]=None, weight_func: Optional[Callable]=None, differentiation: Optional[int]=None, fit_kwargs: Optional[dict]=None, binner_kwargs: Optional[dict]=None, forecaster_id: Optional[Union[str, int]]=None) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nInitializes the ForecasterRecursive class, which implements a recursive autoregressive \nmulti-step forecasting method using a scikit-learn compatible regressor. It manages \nlagged values, window features, and any data transformations applied to the target \nand exogenous variables.\n\nParameters\n----------\nregressor : object\n    A scikit-learn compatible regressor or pipeline used for generating forecasts.\nlags : Optional[Union[int, list, np.ndarray, range]], default=None\n    Defines the lagged values to be used as predictors. Can be an integer specifying \n    the maximum lag, a list/array of specific lags, or None.\nwindow_features : Optional[Union[object, list]], default=None\n    Instantiates or a list of instances to create additional predictors from the \n    time series.\ntransformer_y : Optional[object], default=None\n    A scikit-learn compatible transformer for the target variable `y` applied before \n    training.\ntransformer_exog : Optional[object], default=None\n    A scikit-learn compatible transformer for exogenous variables applied before training.\nweight_func : Optional[Callable], default=None\n    A callable function that assigns weights to each sample based on the index.\ndifferentiation : Optional[int], default=None\n    The order of differencing to be applied to the target variable before training.\nfit_kwargs : Optional[dict], default=None\n    Additional keyword arguments to pass to the regressor's `fit` method.\nbinner_kwargs : Optional[dict], default=None\n    Parameters for the QuantileBinner used for binning residuals.\nforecaster_id : Optional[Union[str, int]], default=None\n    An identifier for instances of the forecaster.\n\nAttributes\n----------\ncreation_date : str\n    The timestamp of when the forecaster instance was created.\nis_fitted : bool\n    A flag indicating whether the regressor has been fitted.\n...\n\nRaises\n------\nValueError\n    If neither `lags` nor `window_features` are provided.\n\nNotes\n-----\nThis class relies on several helper functions from the utils module, such as \n`initialize_lags`, `initialize_window_features`, and `initialize_weights`, \nto set up lags, window features, and weights accordingly.\n\"\"\"\n```\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "skforecast/recursive/_forecaster_recursive.py": "from typing import Union, Tuple, Optional, Callable\nimport warnings\nimport sys\nimport numpy as np\nimport pandas as pd\nimport inspect\nfrom copy import copy\nfrom sklearn.exceptions import NotFittedError\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import clone\nimport skforecast\nfrom ..base import ForecasterBase\nfrom ..exceptions import DataTransformationWarning\nfrom ..utils import initialize_lags, initialize_window_features, initialize_weights, check_select_fit_kwargs, check_y, check_exog, get_exog_dtypes, check_exog_dtypes, check_predict_input, check_interval, preprocess_y, preprocess_last_window, preprocess_exog, input_to_frame, date_to_index_position, expand_index, transform_numpy, transform_dataframe\nfrom ..preprocessing import TimeSeriesDifferentiator\nfrom ..preprocessing import QuantileBinner\n\nclass ForecasterRecursive(ForecasterBase):\n    \"\"\"\n    This class turns any regressor compatible with the scikit-learn API into a\n    recursive autoregressive (multi-step) forecaster.\n    \n    Parameters\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n    lags : int, list, numpy ndarray, range, default `None`\n        Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1.\n    \n        - `int`: include lags from 1 to `lags` (included).\n        - `list`, `1d numpy ndarray` or `range`: include only lags present in \n        `lags`, all elements must be int.\n        - `None`: no lags are included as predictors. \n    window_features : object, list, default `None`\n        Instance or list of instances used to create window features. Window features\n        are created from the original time series and are included as predictors.\n    transformer_y : object transformer (preprocessor), default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and inverse_transform.\n        ColumnTransformers are not allowed since they do not have inverse_transform method.\n        The transformation is applied to `y` before training the forecaster. \n    transformer_exog : object transformer (preprocessor), default `None`\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable, default `None`\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\n        method. The resulting `sample_weight` cannot have negative values.\n    differentiation : int, default `None`\n        Order of differencing applied to the time series before training the forecaster.\n        If `None`, no differencing is applied. The order of differentiation is the number\n        of times the differencing operation is applied to a time series. Differencing\n        involves computing the differences between consecutive data points in the series.\n        Differentiation is reversed in the output of `predict()` and `predict_interval()`.\n        **WARNING: This argument is newly introduced and requires special attention. It\n        is still experimental and may undergo changes.**\n    fit_kwargs : dict, default `None`\n        Additional arguments to be passed to the `fit` method of the regressor.\n    binner_kwargs : dict, default `None`\n        Additional arguments to pass to the `QuantileBinner` used to discretize \n        the residuals into k bins according to the predicted values associated \n        with each residual. Available arguments are: `n_bins`, `method`, `subsample`,\n        `random_state` and `dtype`. Argument `method` is passed internally to the\n        fucntion `numpy.percentile`.\n        **New in version 0.14.0**\n    forecaster_id : str, int, default `None`\n        Name used as an identifier of the forecaster.\n    \n    Attributes\n    ----------\n    regressor : regressor or pipeline compatible with the scikit-learn API\n        An instance of a regressor or pipeline compatible with the scikit-learn API.\n    lags : numpy ndarray\n        Lags used as predictors.\n    lags_names : list\n        Names of the lags used as predictors.\n    max_lag : int\n        Maximum lag included in `lags`.\n    window_features : list\n        Class or list of classes used to create window features.\n    window_features_names : list\n        Names of the window features to be included in the `X_train` matrix.\n    window_features_class_names : list\n        Names of the classes used to create the window features.\n    max_size_window_features : int\n        Maximum window size required by the window features.\n    window_size : int\n        The window size needed to create the predictors. It is calculated as the \n        maximum value between `max_lag` and `max_size_window_features`. If \n        differentiation is used, `window_size` is increased by n units equal to \n        the order of differentiation so that predictors can be generated correctly.\n    transformer_y : object transformer (preprocessor)\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API with methods: fit, transform, fit_transform and inverse_transform.\n        ColumnTransformers are not allowed since they do not have inverse_transform method.\n        The transformation is applied to `y` before training the forecaster.\n    transformer_exog : object transformer (preprocessor)\n        An instance of a transformer (preprocessor) compatible with the scikit-learn\n        preprocessing API. The transformation is applied to `exog` before training the\n        forecaster. `inverse_transform` is not available when using ColumnTransformers.\n    weight_func : Callable\n        Function that defines the individual weights for each sample based on the\n        index. For example, a function that assigns a lower weight to certain dates.\n        Ignored if `regressor` does not have the argument `sample_weight` in its `fit`\n        method. The resulting `sample_weight` cannot have negative values.\n    differentiation : int\n        Order of differencing applied to the time series before training the forecaster.\n        If `None`, no differencing is applied. The order of differentiation is the number\n        of times the differencing operation is applied to a time series. Differencing\n        involves computing the differences between consecutive data points in the series.\n        Differentiation is reversed in the output of `predict()` and `predict_interval()`.\n        **WARNING: This argument is newly introduced and requires special attention. It\n        is still experimental and may undergo changes.**\n        **New in version 0.10.0**\n    binner : sklearn.preprocessing.KBinsDiscretizer\n        `KBinsDiscretizer` used to discretize residuals into k bins according \n        to the predicted values associated with each residual.\n        **New in version 0.12.0**\n    binner_intervals_ : dict\n        Intervals used to discretize residuals into k bins according to the predicted\n        values associated with each residual.\n        **New in version 0.12.0**\n    binner_kwargs : dict\n        Additional arguments to pass to the `QuantileBinner` used to discretize \n        the residuals into k bins according to the predicted values associated \n        with each residual. Available arguments are: `n_bins`, `method`, `subsample`,\n        `random_state` and `dtype`. Argument `method` is passed internally to the\n        fucntion `numpy.percentile`.\n        **New in version 0.14.0**\n    source_code_weight_func : str\n        Source code of the custom function used to create weights.\n    differentiation : int\n        Order of differencing applied to the time series before training the \n        forecaster.\n    differentiator : TimeSeriesDifferentiator\n        Skforecast object used to differentiate the time series.\n    last_window_ : pandas DataFrame\n        This window represents the most recent data observed by the predictor\n        during its training phase. It contains the values needed to predict the\n        next step immediately after the training data. These values are stored\n        in the original scale of the time series before undergoing any transformations\n        or differentiation. When `differentiation` parameter is specified, the\n        dimensions of the `last_window_` are expanded as many values as the order\n        of differentiation. For example, if `lags` = 7 and `differentiation` = 1,\n        `last_window_` will have 8 values.\n    index_type_ : type\n        Type of index of the input used in training.\n    index_freq_ : str\n        Frequency of Index of the input used in training.\n    training_range_ : pandas Index\n        First and last values of index of the data used during training.\n    exog_in_ : bool\n        If the forecaster has been trained using exogenous variable/s.\n    exog_names_in_ : list\n        Names of the exogenous variables used during training.\n    exog_type_in_ : type\n        Type of exogenous data (pandas Series or DataFrame) used in training.\n    exog_dtypes_in_ : dict\n        Type of each exogenous variable/s used in training. If `transformer_exog` \n        is used, the dtypes are calculated before the transformation.\n    X_train_window_features_names_out_ : list\n        Names of the window features included in the matrix `X_train` created\n        internally for training.\n    X_train_exog_names_out_ : list\n        Names of the exogenous variables included in the matrix `X_train` created\n        internally for training. It can be different from `exog_names_in_` if\n        some exogenous variables are transformed during the training process.\n    X_train_features_names_out_ : list\n        Names of columns of the matrix created internally for training.\n    fit_kwargs : dict\n        Additional arguments to be passed to the `fit` method of the regressor.\n    in_sample_residuals_ : numpy ndarray\n        Residuals of the model when predicting training data. Only stored up to\n        10_000 values. If `transformer_y` is not `None`, residuals are stored in\n        the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation.\n    in_sample_residuals_by_bin_ : dict\n        In sample residuals binned according to the predicted value each residual\n        is associated with. If `transformer_y` is not `None`, residuals are stored\n        in the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n        **New in version 0.14.0**\n    out_sample_residuals_ : numpy ndarray\n        Residuals of the model when predicting non training data. Only stored up to\n        10_000 values. If `transformer_y` is not `None`, residuals are stored in\n        the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation.\n    out_sample_residuals_by_bin_ : dict\n        Out of sample residuals binned according to the predicted value each residual\n        is associated with. If `transformer_y` is not `None`, residuals are stored\n        in the transformed scale. If `differentiation` is not `None`, residuals are\n        stored after differentiation. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n        **New in version 0.12.0**\n    creation_date : str\n        Date of creation.\n    is_fitted : bool\n        Tag to identify if the regressor has been fitted (trained).\n    fit_date : str\n        Date of last fit.\n    skforecast_version : str\n        Version of skforecast library used to create the forecaster.\n    python_version : str\n        Version of python used to create the forecaster.\n    forecaster_id : str, int\n        Name used as an identifier of the forecaster.\n    \n    \"\"\"\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Information displayed when a ForecasterRecursive object is printed.\n        \"\"\"\n        params, _, _, exog_names_in_, _ = self._preprocess_repr(regressor=self.regressor, exog_names_in_=self.exog_names_in_)\n        params = self._format_text_repr(params)\n        exog_names_in_ = self._format_text_repr(exog_names_in_)\n        info = f'{'=' * len(type(self).__name__)} \\n{type(self).__name__} \\n{'=' * len(type(self).__name__)} \\nRegressor: {type(self.regressor).__name__} \\nLags: {self.lags} \\nWindow features: {self.window_features_names} \\nWindow size: {self.window_size} \\nExogenous included: {self.exog_in_} \\nExogenous names: {exog_names_in_} \\nTransformer for y: {self.transformer_y} \\nTransformer for exog: {self.transformer_exog} \\nWeight function included: {(True if self.weight_func is not None else False)} \\nDifferentiation order: {self.differentiation} \\nTraining range: {(self.training_range_.to_list() if self.is_fitted else None)} \\nTraining index type: {(str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else None)} \\nTraining index frequency: {(self.index_freq_ if self.is_fitted else None)} \\nRegressor parameters: {params} \\nfit_kwargs: {self.fit_kwargs} \\nCreation date: {self.creation_date} \\nLast fit date: {self.fit_date} \\nSkforecast version: {self.skforecast_version} \\nPython version: {self.python_version} \\nForecaster id: {self.forecaster_id} \\n'\n        return info\n\n    def _repr_html_(self):\n        \"\"\"\n        HTML representation of the object.\n        The \"General Information\" section is expanded by default.\n        \"\"\"\n        params, _, _, exog_names_in_, _ = self._preprocess_repr(regressor=self.regressor, exog_names_in_=self.exog_names_in_)\n        style, unique_id = self._get_style_repr_html(self.is_fitted)\n        content = f'\\n        <div class=\"container-{unique_id}\">\\n            <h2>{type(self).__name__}</h2>\\n            <details open>\\n                <summary>General Information</summary>\\n                <ul>\\n                    <li><strong>Regressor:</strong> {type(self.regressor).__name__}</li>\\n                    <li><strong>Lags:</strong> {self.lags}</li>\\n                    <li><strong>Window features:</strong> {self.window_features_names}</li>\\n                    <li><strong>Window size:</strong> {self.window_size}</li>\\n                    <li><strong>Exogenous included:</strong> {self.exog_in_}</li>\\n                    <li><strong>Weight function included:</strong> {self.weight_func is not None}</li>\\n                    <li><strong>Differentiation order:</strong> {self.differentiation}</li>\\n                    <li><strong>Creation date:</strong> {self.creation_date}</li>\\n                    <li><strong>Last fit date:</strong> {self.fit_date}</li>\\n                    <li><strong>Skforecast version:</strong> {self.skforecast_version}</li>\\n                    <li><strong>Python version:</strong> {self.python_version}</li>\\n                    <li><strong>Forecaster id:</strong> {self.forecaster_id}</li>\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Exogenous Variables</summary>\\n                <ul>\\n                    {exog_names_in_}\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Data Transformations</summary>\\n                <ul>\\n                    <li><strong>Transformer for y:</strong> {self.transformer_y}</li>\\n                    <li><strong>Transformer for exog:</strong> {self.transformer_exog}</li>\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Training Information</summary>\\n                <ul>\\n                    <li><strong>Training range:</strong> {(self.training_range_.to_list() if self.is_fitted else 'Not fitted')}</li>\\n                    <li><strong>Training index type:</strong> {(str(self.index_type_).split('.')[-1][:-2] if self.is_fitted else 'Not fitted')}</li>\\n                    <li><strong>Training index frequency:</strong> {(self.index_freq_ if self.is_fitted else 'Not fitted')}</li>\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Regressor Parameters</summary>\\n                <ul>\\n                    {params}\\n                </ul>\\n            </details>\\n            <details>\\n                <summary>Fit Kwargs</summary>\\n                <ul>\\n                    {self.fit_kwargs}\\n                </ul>\\n            </details>\\n            <p>\\n                <a href=\"https://skforecast.org/{skforecast.__version__}/api/forecasterrecursive.html\">&#128712 <strong>API Reference</strong></a>\\n                &nbsp;&nbsp;\\n                <a href=\"https://skforecast.org/{skforecast.__version__}/user_guides/autoregresive-forecaster.html\">&#128462 <strong>User Guide</strong></a>\\n            </p>\\n        </div>\\n        '\n        return style + content\n\n    def _create_lags(self, y: np.ndarray, X_as_pandas: bool=False, train_index: Optional[pd.Index]=None) -> Tuple[Optional[Union[np.ndarray, pd.DataFrame]], np.ndarray]:\n        \"\"\"\n        Create the lagged values and their target variable from a time series.\n        \n        Note that the returned matrix `X_data` contains the lag 1 in the first \n        column, the lag 2 in the in the second column and so on.\n        \n        Parameters\n        ----------\n        y : numpy ndarray\n            Training time series values.\n        X_as_pandas : bool, default `False`\n            If `True`, the returned matrix `X_data` is a pandas DataFrame.\n        train_index : pandas Index, default `None`\n            Index of the training data. It is used to create the pandas DataFrame\n            `X_data` when `X_as_pandas` is `True`.\n\n        Returns\n        -------\n        X_data : numpy ndarray, pandas DataFrame, None\n            Lagged values (predictors).\n        y_data : numpy ndarray\n            Values of the time series related to each row of `X_data`.\n        \n        \"\"\"\n        X_data = None\n        if self.lags is not None:\n            n_rows = len(y) - self.window_size\n            X_data = np.full(shape=(n_rows, len(self.lags)), fill_value=np.nan, order='F', dtype=float)\n            for i, lag in enumerate(self.lags):\n                X_data[:, i] = y[self.window_size - lag:-lag]\n            if X_as_pandas:\n                X_data = pd.DataFrame(data=X_data, columns=self.lags_names, index=train_index)\n        y_data = y[self.window_size:]\n        return (X_data, y_data)\n\n    def _create_window_features(self, y: pd.Series, train_index: pd.Index, X_as_pandas: bool=False) -> Tuple[list, list]:\n        \"\"\"\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        train_index : pandas Index\n            Index of the training data. It is used to create the pandas DataFrame\n            `X_train_window_features` when `X_as_pandas` is `True`.\n        X_as_pandas : bool, default `False`\n            If `True`, the returned matrix `X_train_window_features` is a \n            pandas DataFrame.\n\n        Returns\n        -------\n        X_train_window_features : list\n            List of numpy ndarrays or pandas DataFrames with the window features.\n        X_train_window_features_names_out_ : list\n            Names of the window features.\n        \n        \"\"\"\n        len_train_index = len(train_index)\n        X_train_window_features = []\n        X_train_window_features_names_out_ = []\n        for wf in self.window_features:\n            X_train_wf = wf.transform_batch(y)\n            if not isinstance(X_train_wf, pd.DataFrame):\n                raise TypeError(f'The method `transform_batch` of {type(wf).__name__} must return a pandas DataFrame.')\n            X_train_wf = X_train_wf.iloc[-len_train_index:]\n            if not len(X_train_wf) == len_train_index:\n                raise ValueError(f'The method `transform_batch` of {type(wf).__name__} must return a DataFrame with the same number of rows as the input time series - `window_size`: {len_train_index}.')\n            if not (X_train_wf.index == train_index).all():\n                raise ValueError(f'The method `transform_batch` of {type(wf).__name__} must return a DataFrame with the same index as the input time series - `window_size`.')\n            X_train_window_features_names_out_.extend(X_train_wf.columns)\n            if not X_as_pandas:\n                X_train_wf = X_train_wf.to_numpy()\n            X_train_window_features.append(X_train_wf)\n        return (X_train_window_features, X_train_window_features_names_out_)\n\n    def _create_train_X_y(self, y: pd.Series, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, pd.Series, list, list, list, list, dict]:\n        \"\"\"\n        Create training matrices from univariate time series and exogenous\n        variables.\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors).\n        y_train : pandas Series\n            Values of the time series related to each row of `X_train`.\n        exog_names_in_ : list\n            Names of the exogenous variables used during training.\n        X_train_window_features_names_out_ : list\n            Names of the window features included in the matrix `X_train` created\n            internally for training.\n        X_train_exog_names_out_ : list\n            Names of the exogenous variables included in the matrix `X_train` created\n            internally for training. It can be different from `exog_names_in_` if\n            some exogenous variables are transformed during the training process.\n        X_train_features_names_out_ : list\n            Names of the columns of the matrix created internally for training.\n        exog_dtypes_in_ : dict\n            Type of each exogenous variable/s used in training. If `transformer_exog` \n            is used, the dtypes are calculated before the transformation.\n        \n        \"\"\"\n        check_y(y=y)\n        y = input_to_frame(data=y, input_name='y')\n        if len(y) <= self.window_size:\n            raise ValueError(f'Length of `y` must be greater than the maximum window size needed by the forecaster.\\n    Length `y`: {len(y)}.\\n    Max window size: {self.window_size}.\\n    Lags window size: {self.max_lag}.\\n    Window features window size: {self.max_size_window_features}.')\n        fit_transformer = False if self.is_fitted else True\n        y = transform_dataframe(df=y, transformer=self.transformer_y, fit=fit_transformer, inverse_transform=False)\n        y_values, y_index = preprocess_y(y=y)\n        train_index = y_index[self.window_size:]\n        if self.differentiation is not None:\n            if not self.is_fitted:\n                y_values = self.differentiator.fit_transform(y_values)\n            else:\n                differentiator = copy(self.differentiator)\n                y_values = differentiator.fit_transform(y_values)\n        exog_names_in_ = None\n        exog_dtypes_in_ = None\n        categorical_features = False\n        if exog is not None:\n            check_exog(exog=exog, allow_nan=True)\n            exog = input_to_frame(data=exog, input_name='exog')\n            len_y = len(y_values)\n            len_train_index = len(train_index)\n            len_exog = len(exog)\n            if not len_exog == len_y and (not len_exog == len_train_index):\n                raise ValueError(f'Length of `exog` must be equal to the length of `y` (if index is fully aligned) or length of `y` - `window_size` (if `exog` starts after the first `window_size` values).\\n    `exog`              : ({exog.index[0]} -- {exog.index[-1]})  (n={len_exog})\\n    `y`                 : ({y.index[0]} -- {y.index[-1]})  (n={len_y})\\n    `y` - `window_size` : ({train_index[0]} -- {train_index[-1]})  (n={len_train_index})')\n            exog_names_in_ = exog.columns.to_list()\n            exog_dtypes_in_ = get_exog_dtypes(exog=exog)\n            exog = transform_dataframe(df=exog, transformer=self.transformer_exog, fit=fit_transformer, inverse_transform=False)\n            check_exog_dtypes(exog, call_check_exog=True)\n            categorical_features = exog.select_dtypes(include=np.number).shape[1] != exog.shape[1]\n            _, exog_index = preprocess_exog(exog=exog, return_values=False)\n            if len_exog == len_y:\n                if not (exog_index == y_index).all():\n                    raise ValueError('When `exog` has the same length as `y`, the index of `exog` must be aligned with the index of `y` to ensure the correct alignment of values.')\n                exog = exog.iloc[self.window_size:,]\n            elif not (exog_index == train_index).all():\n                raise ValueError(\"When `exog` doesn't contain the first `window_size` observations, the index of `exog` must be aligned with the index of `y` minus the first `window_size` observations to ensure the correct alignment of values.\")\n        X_train = []\n        X_train_features_names_out_ = []\n        X_as_pandas = True if categorical_features else False\n        X_train_lags, y_train = self._create_lags(y=y_values, X_as_pandas=X_as_pandas, train_index=train_index)\n        if X_train_lags is not None:\n            X_train.append(X_train_lags)\n            X_train_features_names_out_.extend(self.lags_names)\n        X_train_window_features_names_out_ = None\n        if self.window_features is not None:\n            n_diff = 0 if self.differentiation is None else self.differentiation\n            y_window_features = pd.Series(y_values[n_diff:], index=y_index[n_diff:])\n            X_train_window_features, X_train_window_features_names_out_ = self._create_window_features(y=y_window_features, X_as_pandas=X_as_pandas, train_index=train_index)\n            X_train.extend(X_train_window_features)\n            X_train_features_names_out_.extend(X_train_window_features_names_out_)\n        X_train_exog_names_out_ = None\n        if exog is not None:\n            X_train_exog_names_out_ = exog.columns.to_list()\n            if not X_as_pandas:\n                exog = exog.to_numpy()\n            X_train_features_names_out_.extend(X_train_exog_names_out_)\n            X_train.append(exog)\n        if len(X_train) == 1:\n            X_train = X_train[0]\n        elif X_as_pandas:\n            X_train = pd.concat(X_train, axis=1)\n        else:\n            X_train = np.concatenate(X_train, axis=1)\n        if X_as_pandas:\n            X_train.index = train_index\n        else:\n            X_train = pd.DataFrame(data=X_train, index=train_index, columns=X_train_features_names_out_)\n        y_train = pd.Series(data=y_train, index=train_index, name='y')\n        return (X_train, y_train, exog_names_in_, X_train_window_features_names_out_, X_train_exog_names_out_, X_train_features_names_out_, exog_dtypes_in_)\n\n    def create_train_X_y(self, y: pd.Series, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, pd.Series]:\n        \"\"\"\n        Create training matrices from univariate time series and exogenous\n        variables.\n        \n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n\n        Returns\n        -------\n        X_train : pandas DataFrame\n            Training values (predictors).\n        y_train : pandas Series\n            Values of the time series related to each row of `X_data`.\n        \n        \"\"\"\n        output = self._create_train_X_y(y=y, exog=exog)\n        X_train = output[0]\n        y_train = output[1]\n        return (X_train, y_train)\n\n    def _train_test_split_one_step_ahead(self, y: pd.Series, initial_train_size: int, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n        \"\"\"\n        Create matrices needed to train and test the forecaster for one-step-ahead\n        predictions.\n\n        Parameters\n        ----------\n        y : pandas Series\n            Training time series.\n        initial_train_size : int\n            Initial size of the training set. It is the number of observations used\n            to train the forecaster before making the first prediction.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s. Must have the same\n            number of observations as `y` and their indexes must be aligned.\n        \n        Returns\n        -------\n        X_train : pandas DataFrame\n            Predictor values used to train the model.\n        y_train : pandas Series\n            Target values related to each row of `X_train`.\n        X_test : pandas DataFrame\n            Predictor values used to test the model.\n        y_test : pandas Series\n            Target values related to each row of `X_test`.\n        \n        \"\"\"\n        is_fitted = self.is_fitted\n        self.is_fitted = False\n        X_train, y_train, *_ = self._create_train_X_y(y=y.iloc[:initial_train_size], exog=exog.iloc[:initial_train_size] if exog is not None else None)\n        test_init = initial_train_size - self.window_size\n        self.is_fitted = True\n        X_test, y_test, *_ = self._create_train_X_y(y=y.iloc[test_init:], exog=exog.iloc[test_init:] if exog is not None else None)\n        self.is_fitted = is_fitted\n        return (X_train, y_train, X_test, y_test)\n\n    def create_sample_weights(self, X_train: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Crate weights for each observation according to the forecaster's attribute\n        `weight_func`.\n\n        Parameters\n        ----------\n        X_train : pandas DataFrame\n            Dataframe created with the `create_train_X_y` method, first return.\n\n        Returns\n        -------\n        sample_weight : numpy ndarray\n            Weights to use in `fit` method.\n\n        \"\"\"\n        sample_weight = None\n        if self.weight_func is not None:\n            sample_weight = self.weight_func(X_train.index)\n        if sample_weight is not None:\n            if np.isnan(sample_weight).any():\n                raise ValueError('The resulting `sample_weight` cannot have NaN values.')\n            if np.any(sample_weight < 0):\n                raise ValueError('The resulting `sample_weight` cannot have negative values.')\n            if np.sum(sample_weight) == 0:\n                raise ValueError('The resulting `sample_weight` cannot be normalized because the sum of the weights is zero.')\n        return sample_weight\n\n    def _create_predict_inputs(self, steps: Union[int, str, pd.Timestamp], last_window: Optional[Union[pd.Series, pd.DataFrame]]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, predict_boot: bool=False, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False, check_inputs: bool=True) -> Tuple[np.ndarray, Optional[np.ndarray], pd.Index, int]:\n        \"\"\"\n        Create the inputs needed for the first iteration of the prediction \n        process. As this is a recursive process, the last window is updated at \n        each iteration of the prediction process.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        predict_boot : bool, default `False`\n            If `True`, residuals are returned to generate bootstrapping predictions.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n\n        Returns\n        -------\n        last_window_values : numpy ndarray\n            Series values used to create the predictors needed in the first \n            iteration of the prediction (t + 1).\n        exog_values : numpy ndarray, None\n            Exogenous variable/s included as predictor/s.\n        prediction_index : pandas Index\n            Index of the predictions.\n        steps: int\n            Number of future steps predicted.\n        \n        \"\"\"\n        if last_window is None:\n            last_window = self.last_window_\n        if self.is_fitted:\n            steps = date_to_index_position(index=last_window.index, date_input=steps, date_literal='steps')\n        if check_inputs:\n            check_predict_input(forecaster_name=type(self).__name__, steps=steps, is_fitted=self.is_fitted, exog_in_=self.exog_in_, index_type_=self.index_type_, index_freq_=self.index_freq_, window_size=self.window_size, last_window=last_window, exog=exog, exog_type_in_=self.exog_type_in_, exog_names_in_=self.exog_names_in_, interval=None)\n            if predict_boot and (not use_in_sample_residuals):\n                if not use_binned_residuals and self.out_sample_residuals_ is None:\n                    raise ValueError('`forecaster.out_sample_residuals_` is `None`. Use `use_in_sample_residuals=True` or the `set_out_sample_residuals()` method before predicting.')\n                if use_binned_residuals and self.out_sample_residuals_by_bin_ is None:\n                    raise ValueError('`forecaster.out_sample_residuals_by_bin_` is `None`. Use `use_in_sample_residuals=True` or the `set_out_sample_residuals()` method before predicting.')\n        last_window = last_window.iloc[-self.window_size:].copy()\n        last_window_values, last_window_index = preprocess_last_window(last_window=last_window)\n        last_window_values = transform_numpy(array=last_window_values, transformer=self.transformer_y, fit=False, inverse_transform=False)\n        if self.differentiation is not None:\n            last_window_values = self.differentiator.fit_transform(last_window_values)\n        if exog is not None:\n            exog = input_to_frame(data=exog, input_name='exog')\n            exog = exog.loc[:, self.exog_names_in_]\n            exog = transform_dataframe(df=exog, transformer=self.transformer_exog, fit=False, inverse_transform=False)\n            check_exog_dtypes(exog=exog)\n            exog_values = exog.to_numpy()[:steps]\n        else:\n            exog_values = None\n        prediction_index = expand_index(index=last_window_index, steps=steps)\n        return (last_window_values, exog_values, prediction_index, steps)\n\n    def _recursive_predict(self, steps: int, last_window_values: np.ndarray, exog_values: Optional[np.ndarray]=None, residuals: Optional[Union[np.ndarray, dict]]=None, use_binned_residuals: bool=False) -> np.ndarray:\n        \"\"\"\n        Predict n steps ahead. It is an iterative process in which, each prediction,\n        is used as a predictor for the next step.\n        \n        Parameters\n        ----------\n        steps : int\n            Number of future steps predicted.\n        last_window_values : numpy ndarray\n            Series values used to create the predictors needed in the first \n            iteration of the prediction (t + 1).\n        exog_values : numpy ndarray, default `None`\n            Exogenous variable/s included as predictor/s.\n        residuals : numpy ndarray, dict, default `None`\n            Residuals used to generate bootstrapping predictions.\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : numpy ndarray\n            Predicted values.\n        \n        \"\"\"\n        n_lags = len(self.lags) if self.lags is not None else 0\n        n_window_features = len(self.X_train_window_features_names_out_) if self.window_features is not None else 0\n        n_exog = exog_values.shape[1] if exog_values is not None else 0\n        X = np.full(shape=n_lags + n_window_features + n_exog, fill_value=np.nan, dtype=float)\n        predictions = np.full(shape=steps, fill_value=np.nan, dtype=float)\n        last_window = np.concatenate((last_window_values, predictions))\n        for i in range(steps):\n            if self.lags is not None:\n                X[:n_lags] = last_window[-self.lags - (steps - i)]\n            if self.window_features is not None:\n                X[n_lags:n_lags + n_window_features] = np.concatenate([wf.transform(last_window[i:-(steps - i)]) for wf in self.window_features])\n            if exog_values is not None:\n                X[n_lags + n_window_features:] = exog_values[i]\n            pred = self.regressor.predict(X.reshape(1, -1)).ravel()\n            if residuals is not None:\n                if use_binned_residuals:\n                    predicted_bin = self.binner.transform(pred).item()\n                    step_residual = residuals[predicted_bin][i]\n                else:\n                    step_residual = residuals[i]\n                pred += step_residual\n            predictions[i] = pred[0]\n            last_window[-(steps - i)] = pred[0]\n        return predictions\n\n    def create_predict_X(self, steps: int, last_window: Optional[Union[pd.Series, pd.DataFrame]]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None) -> pd.DataFrame:\n        \"\"\"\n        Create the predictors needed to predict `steps` ahead. As it is a recursive\n        process, the predictors are created at each iteration of the prediction \n        process.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n\n        Returns\n        -------\n        X_predict : pandas DataFrame\n            Pandas DataFrame with the predictors for each step. The index \n            is the same as the prediction index.\n        \n        \"\"\"\n        last_window_values, exog_values, prediction_index, steps = self._create_predict_inputs(steps=steps, last_window=last_window, exog=exog)\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', message='X does not have valid feature names', category=UserWarning)\n            predictions = self._recursive_predict(steps=steps, last_window_values=last_window_values, exog_values=exog_values)\n        X_predict = []\n        full_predictors = np.concatenate((last_window_values, predictions))\n        if self.lags is not None:\n            idx = np.arange(-steps, 0)[:, None] - self.lags\n            X_lags = full_predictors[idx + len(full_predictors)]\n            X_predict.append(X_lags)\n        if self.window_features is not None:\n            X_window_features = np.full(shape=(steps, len(self.X_train_window_features_names_out_)), fill_value=np.nan, order='C', dtype=float)\n            for i in range(steps):\n                X_window_features[i, :] = np.concatenate([wf.transform(full_predictors[i:-(steps - i)]) for wf in self.window_features])\n            X_predict.append(X_window_features)\n        if exog is not None:\n            X_predict.append(exog_values)\n        X_predict = pd.DataFrame(data=np.concatenate(X_predict, axis=1), columns=self.X_train_features_names_out_, index=prediction_index)\n        if self.transformer_y is not None or self.differentiation is not None:\n            warnings.warn('The output matrix is in the transformed scale due to the inclusion of transformations or differentiation in the Forecaster. As a result, any predictions generated using this matrix will also be in the transformed scale. Please refer to the documentation for more details: https://skforecast.org/latest/user_guides/training-and-prediction-matrices.html', DataTransformationWarning)\n        return X_predict\n\n    def predict(self, steps: Union[int, str, pd.Timestamp], last_window: Optional[Union[pd.Series, pd.DataFrame]]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, check_inputs: bool=True) -> pd.Series:\n        \"\"\"\n        Predict n steps ahead. It is an recursive process in which, each prediction,\n        is used as a predictor for the next step.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, pandas DataFrame, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        check_inputs : bool, default `True`\n            If `True`, the input is checked for possible warnings and errors \n            with the `check_predict_input` function. This argument is created \n            for internal use and is not recommended to be changed.\n\n        Returns\n        -------\n        predictions : pandas Series\n            Predicted values.\n        \n        \"\"\"\n        last_window_values, exog_values, prediction_index, steps = self._create_predict_inputs(steps=steps, last_window=last_window, exog=exog, check_inputs=check_inputs)\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', message='X does not have valid feature names', category=UserWarning)\n            predictions = self._recursive_predict(steps=steps, last_window_values=last_window_values, exog_values=exog_values)\n        if self.differentiation is not None:\n            predictions = self.differentiator.inverse_transform_next_window(predictions)\n        predictions = transform_numpy(array=predictions, transformer=self.transformer_y, fit=False, inverse_transform=True)\n        predictions = pd.Series(data=predictions, index=prediction_index, name='pred')\n        return predictions\n\n    def predict_bootstrapping(self, steps: Union[int, str, pd.Timestamp], last_window: Optional[pd.Series]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False) -> pd.DataFrame:\n        \"\"\"\n        Generate multiple forecasting predictions using a bootstrapping process. \n        By sampling from a collection of past observed errors (the residuals),\n        each iteration of bootstrapping generates a different set of predictions. \n        See the Notes section for more information. \n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in `self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        boot_predictions : pandas DataFrame\n            Predictions generated by bootstrapping.\n            Shape: (steps, n_boot)\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp3/prediction-intervals.html#prediction-intervals-from-bootstrapped-residuals\n        Forecasting: Principles and Practice (3nd ed) Rob J Hyndman and George Athanasopoulos.\n\n        \"\"\"\n        last_window_values, exog_values, prediction_index, steps = self._create_predict_inputs(steps=steps, last_window=last_window, exog=exog, predict_boot=True, use_in_sample_residuals=use_in_sample_residuals, use_binned_residuals=use_binned_residuals)\n        if use_in_sample_residuals:\n            residuals = self.in_sample_residuals_\n            residuals_by_bin = self.in_sample_residuals_by_bin_\n        else:\n            residuals = self.out_sample_residuals_\n            residuals_by_bin = self.out_sample_residuals_by_bin_\n        rng = np.random.default_rng(seed=random_state)\n        if use_binned_residuals:\n            sampled_residuals = {k: v[rng.integers(low=0, high=len(v), size=(steps, n_boot))] for k, v in residuals_by_bin.items()}\n        else:\n            sampled_residuals = residuals[rng.integers(low=0, high=len(residuals), size=(steps, n_boot))]\n        boot_columns = []\n        boot_predictions = np.full(shape=(steps, n_boot), fill_value=np.nan, order='F', dtype=float)\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', message='X does not have valid feature names', category=UserWarning)\n            for i in range(n_boot):\n                if use_binned_residuals:\n                    boot_sampled_residuals = {k: v[:, i] for k, v in sampled_residuals.items()}\n                else:\n                    boot_sampled_residuals = sampled_residuals[:, i]\n                boot_columns.append(f'pred_boot_{i}')\n                boot_predictions[:, i] = self._recursive_predict(steps=steps, last_window_values=last_window_values, exog_values=exog_values, residuals=boot_sampled_residuals, use_binned_residuals=use_binned_residuals)\n        if self.differentiation is not None:\n            boot_predictions = self.differentiator.inverse_transform_next_window(boot_predictions)\n        if self.transformer_y:\n            boot_predictions = np.apply_along_axis(func1d=transform_numpy, axis=0, arr=boot_predictions, transformer=self.transformer_y, fit=False, inverse_transform=True)\n        boot_predictions = pd.DataFrame(data=boot_predictions, index=prediction_index, columns=boot_columns)\n        return boot_predictions\n\n    def predict_interval(self, steps: Union[int, str, pd.Timestamp], last_window: Optional[pd.Series]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, interval: list=[5, 95], n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False) -> pd.DataFrame:\n        \"\"\"\n        Iterative process in which each prediction is used as a predictor\n        for the next step, and bootstrapping is used to estimate prediction\n        intervals. Both predictions and intervals are returned.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        interval : list, default `[5, 95]`\n            Confidence of the prediction interval estimated. Sequence of \n            percentiles to compute, which must be between 0 and 100 inclusive. \n            For example, interval of 95% should be as `interval = [2.5, 97.5]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Values predicted by the forecaster and their estimated interval.\n\n            - pred: predictions.\n            - lower_bound: lower bound of the interval.\n            - upper_bound: upper bound of the interval.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n        check_interval(interval=interval)\n        boot_predictions = self.predict_bootstrapping(steps=steps, last_window=last_window, exog=exog, n_boot=n_boot, random_state=random_state, use_in_sample_residuals=use_in_sample_residuals, use_binned_residuals=use_binned_residuals)\n        predictions = self.predict(steps=steps, last_window=last_window, exog=exog, check_inputs=False)\n        interval = np.array(interval) / 100\n        predictions_interval = boot_predictions.quantile(q=interval, axis=1).transpose()\n        predictions_interval.columns = ['lower_bound', 'upper_bound']\n        predictions = pd.concat((predictions, predictions_interval), axis=1)\n        return predictions\n\n    def predict_quantiles(self, steps: Union[int, str, pd.Timestamp], last_window: Optional[pd.Series]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, quantiles: list=[0.05, 0.5, 0.95], n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False) -> pd.DataFrame:\n        \"\"\"\n        Calculate the specified quantiles for each step. After generating \n        multiple forecasting predictions through a bootstrapping process, each \n        quantile is calculated for each step.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).\n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        quantiles : list, default `[0.05, 0.5, 0.95]`\n            Sequence of quantiles to compute, which must be between 0 and 1 \n            inclusive. For example, quantiles of 0.05, 0.5 and 0.95 should be as \n            `quantiles = [0.05, 0.5, 0.95]`.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate quantiles.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot quantiles are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create prediction quantiles. If `False`, out of\n            sample residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Quantiles predicted by the forecaster.\n\n        Notes\n        -----\n        More information about prediction intervals in forecasting:\n        https://otexts.com/fpp2/prediction-intervals.html\n        Forecasting: Principles and Practice (2nd ed) Rob J Hyndman and\n        George Athanasopoulos.\n        \n        \"\"\"\n        check_interval(quantiles=quantiles)\n        boot_predictions = self.predict_bootstrapping(steps=steps, last_window=last_window, exog=exog, n_boot=n_boot, random_state=random_state, use_in_sample_residuals=use_in_sample_residuals, use_binned_residuals=use_binned_residuals)\n        predictions = boot_predictions.quantile(q=quantiles, axis=1).transpose()\n        predictions.columns = [f'q_{q}' for q in quantiles]\n        return predictions\n\n    def predict_dist(self, steps: Union[int, str, pd.Timestamp], distribution: object, last_window: Optional[pd.Series]=None, exog: Optional[Union[pd.Series, pd.DataFrame]]=None, n_boot: int=250, random_state: int=123, use_in_sample_residuals: bool=True, use_binned_residuals: bool=False) -> pd.DataFrame:\n        \"\"\"\n        Fit a given probability distribution for each step. After generating \n        multiple forecasting predictions through a bootstrapping process, each \n        step is fitted to the given distribution.\n        \n        Parameters\n        ----------\n        steps : int, str, pandas Timestamp\n            Number of steps to predict. \n            \n            + If steps is int, number of steps to predict. \n            + If str or pandas Datetime, the prediction will be up to that date.\n        distribution : Object\n            A distribution object from scipy.stats.\n        last_window : pandas Series, default `None`\n            Series values used to create the predictors (lags) needed in the \n            first iteration of the prediction (t + 1).  \n            If `last_window = None`, the values stored in` self.last_window_` are\n            used to calculate the initial predictors, and the predictions start\n            right after training data.\n        exog : pandas Series, pandas DataFrame, default `None`\n            Exogenous variable/s included as predictor/s.\n        n_boot : int, default `250`\n            Number of bootstrapping iterations used to estimate predictions.\n        random_state : int, default `123`\n            Sets a seed to the random generator, so that boot predictions are always \n            deterministic.\n        use_in_sample_residuals : bool, default `True`\n            If `True`, residuals from the training data are used as proxy of\n            prediction error to create predictions. If `False`, out of sample \n            residuals are used. In the latter case, the user should have\n            calculated and stored the residuals within the forecaster (see\n            `set_out_sample_residuals()`).\n        use_binned_residuals : bool, default `False`\n            If `True`, residuals used in each bootstrapping iteration are selected\n            conditioning on the predicted values. If `False`, residuals are selected\n            randomly without conditioning on the predicted values.\n            **WARNING: This argument is newly introduced and requires special attention.\n            It is still experimental and may undergo changes.\n            **New in version 0.12.0**\n\n        Returns\n        -------\n        predictions : pandas DataFrame\n            Distribution parameters estimated for each step.\n\n        \"\"\"\n        boot_samples = self.predict_bootstrapping(steps=steps, last_window=last_window, exog=exog, n_boot=n_boot, random_state=random_state, use_in_sample_residuals=use_in_sample_residuals, use_binned_residuals=use_binned_residuals)\n        param_names = [p for p in inspect.signature(distribution._pdf).parameters if not p == 'x'] + ['loc', 'scale']\n        param_values = np.apply_along_axis(lambda x: distribution.fit(x), axis=1, arr=boot_samples)\n        predictions = pd.DataFrame(data=param_values, columns=param_names, index=boot_samples.index)\n        return predictions\n\n    def set_params(self, params: dict) -> None:\n        \"\"\"\n        Set new values to the parameters of the scikit learn model stored in the\n        forecaster.\n        \n        Parameters\n        ----------\n        params : dict\n            Parameters values.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        self.regressor = clone(self.regressor)\n        self.regressor.set_params(**params)\n\n    def set_fit_kwargs(self, fit_kwargs: dict) -> None:\n        \"\"\"\n        Set new values for the additional keyword arguments passed to the `fit` \n        method of the regressor.\n        \n        Parameters\n        ----------\n        fit_kwargs : dict\n            Dict of the form {\"argument\": new_value}.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        self.fit_kwargs = check_select_fit_kwargs(self.regressor, fit_kwargs=fit_kwargs)\n\n    def set_lags(self, lags: Optional[Union[int, list, np.ndarray, range]]=None) -> None:\n        \"\"\"\n        Set new value to the attribute `lags`. Attributes `lags_names`, \n        `max_lag` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        lags : int, list, numpy ndarray, range, default `None`\n            Lags used as predictors. Index starts at 1, so lag 1 is equal to t-1. \n        \n            - `int`: include lags from 1 to `lags` (included).\n            - `list`, `1d numpy ndarray` or `range`: include only lags present in \n            `lags`, all elements must be int.\n            - `None`: no lags are included as predictors. \n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        if self.window_features is None and lags is None:\n            raise ValueError('At least one of the arguments `lags` or `window_features` must be different from None. This is required to create the predictors used in training the forecaster.')\n        self.lags, self.lags_names, self.max_lag = initialize_lags(type(self).__name__, lags)\n        self.window_size = max([ws for ws in [self.max_lag, self.max_size_window_features] if ws is not None])\n        if self.differentiation is not None:\n            self.window_size += self.differentiation\n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_window_features(self, window_features: Optional[Union[object, list]]=None) -> None:\n        \"\"\"\n        Set new value to the attribute `window_features`. Attributes \n        `max_size_window_features`, `window_features_names`, \n        `window_features_class_names` and `window_size` are also updated.\n        \n        Parameters\n        ----------\n        window_features : object, list, default `None`\n            Instance or list of instances used to create window features. Window features\n            are created from the original time series and are included as predictors.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        if window_features is None and self.lags is None:\n            raise ValueError('At least one of the arguments `lags` or `window_features` must be different from None. This is required to create the predictors used in training the forecaster.')\n        self.window_features, self.window_features_names, self.max_size_window_features = initialize_window_features(window_features)\n        self.window_features_class_names = None\n        if window_features is not None:\n            self.window_features_class_names = [type(wf).__name__ for wf in self.window_features]\n        self.window_size = max([ws for ws in [self.max_lag, self.max_size_window_features] if ws is not None])\n        if self.differentiation is not None:\n            self.window_size += self.differentiation\n            self.differentiator.set_params(window_size=self.window_size)\n\n    def set_out_sample_residuals(self, y_true: Union[pd.Series, np.ndarray], y_pred: Union[pd.Series, np.ndarray], append: bool=False, random_state: int=123) -> None:\n        \"\"\"\n        Set new values to the attribute `out_sample_residuals_`. Out of sample\n        residuals are meant to be calculated using observations that did not\n        participate in the training process. `y_true` and `y_pred` are expected\n        to be in the original scale of the time series. Residuals are calculated\n        as `y_true` - `y_pred`, after applying the necessary transformations and\n        differentiations if the forecaster includes them (`self.transformer_y`\n        and `self.differentiation`). Two internal attributes are updated:\n\n        + `out_sample_residuals_`: residuals stored in a numpy ndarray.\n        + `out_sample_residuals_by_bin_`: residuals are binned according to the\n        predicted value they are associated with and stored in a dictionary, where\n        the keys are the  intervals of the predicted values and the values are\n        the residuals associated with that range. If a bin binning is empty, it\n        is filled with a random sample of residuals from other bins. This is done\n        to ensure that all bins have at least one residual and can be used in the\n        prediction process.\n\n        A total of 10_000 residuals are stored in the attribute `out_sample_residuals_`.\n        If the number of residuals is greater than 10_000, a random sample of\n        10_000 residuals is stored. The number of residuals stored per bin is\n        limited to `10_000 // self.binner.n_bins_`.\n        \n        Parameters\n        ----------\n        y_true : pandas Series, numpy ndarray, default `None`\n            True values of the time series from which the residuals have been\n            calculated.\n        y_pred : pandas Series, numpy ndarray, default `None`\n            Predicted values of the time series.\n        append : bool, default `False`\n            If `True`, new residuals are added to the once already stored in the\n            forecaster. If after appending the new residuals, the limit of\n            `10_000 // self.binner.n_bins_` values per bin is reached, a random\n            sample of residuals is stored.\n        random_state : int, default `123`\n            Sets a seed to the random sampling for reproducible output.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n        if not self.is_fitted:\n            raise NotFittedError('This forecaster is not fitted yet. Call `fit` with appropriate arguments before using `set_out_sample_residuals()`.')\n        if not isinstance(y_true, (np.ndarray, pd.Series)):\n            raise TypeError(f'`y_true` argument must be `numpy ndarray` or `pandas Series`. Got {type(y_true)}.')\n        if not isinstance(y_pred, (np.ndarray, pd.Series)):\n            raise TypeError(f'`y_pred` argument must be `numpy ndarray` or `pandas Series`. Got {type(y_pred)}.')\n        if len(y_true) != len(y_pred):\n            raise ValueError(f'`y_true` and `y_pred` must have the same length. Got {len(y_true)} and {len(y_pred)}.')\n        if isinstance(y_true, pd.Series) and isinstance(y_pred, pd.Series):\n            if not y_true.index.equals(y_pred.index):\n                raise ValueError('`y_true` and `y_pred` must have the same index.')\n        if not isinstance(y_pred, np.ndarray):\n            y_pred = y_pred.to_numpy()\n        if not isinstance(y_true, np.ndarray):\n            y_true = y_true.to_numpy()\n        if self.transformer_y:\n            y_true = transform_numpy(array=y_true, transformer=self.transformer_y, fit=False, inverse_transform=False)\n            y_pred = transform_numpy(array=y_pred, transformer=self.transformer_y, fit=False, inverse_transform=False)\n        if self.differentiation is not None:\n            differentiator = copy(self.differentiator)\n            differentiator.set_params(window_size=None)\n            y_true = differentiator.fit_transform(y_true)[self.differentiation:]\n            y_pred = differentiator.fit_transform(y_pred)[self.differentiation:]\n        residuals = y_true - y_pred\n        data = pd.DataFrame({'prediction': y_pred, 'residuals': residuals})\n        data['bin'] = self.binner.transform(y_pred).astype(int)\n        residuals_by_bin = data.groupby('bin')['residuals'].apply(np.array).to_dict()\n        if append and self.out_sample_residuals_by_bin_ is not None:\n            for k, v in residuals_by_bin.items():\n                if k in self.out_sample_residuals_by_bin_:\n                    self.out_sample_residuals_by_bin_[k] = np.concatenate((self.out_sample_residuals_by_bin_[k], v))\n                else:\n                    self.out_sample_residuals_by_bin_[k] = v\n        else:\n            self.out_sample_residuals_by_bin_ = residuals_by_bin\n        max_samples = 10000 // self.binner.n_bins_\n        rng = np.random.default_rng(seed=random_state)\n        for k, v in self.out_sample_residuals_by_bin_.items():\n            if len(v) > max_samples:\n                sample = rng.choice(a=v, size=max_samples, replace=False)\n                self.out_sample_residuals_by_bin_[k] = sample\n        for k in self.in_sample_residuals_by_bin_.keys():\n            if k not in self.out_sample_residuals_by_bin_:\n                self.out_sample_residuals_by_bin_[k] = np.array([])\n        empty_bins = [k for k, v in self.out_sample_residuals_by_bin_.items() if len(v) == 0]\n        if empty_bins:\n            warnings.warn(f'The following bins have no out of sample residuals: {empty_bins}. No predicted values fall in the interval {[self.binner_intervals_[bin] for bin in empty_bins]}. Empty bins will be filled with a random sample of residuals.')\n            for k in empty_bins:\n                self.out_sample_residuals_by_bin_[k] = rng.choice(a=residuals, size=max_samples, replace=True)\n        self.out_sample_residuals_ = np.concatenate(list(self.out_sample_residuals_by_bin_.values()))\n\n    def get_feature_importances(self, sort_importance: bool=True) -> pd.DataFrame:\n        \"\"\"\n        Return feature importances of the regressor stored in the forecaster.\n        Only valid when regressor stores internally the feature importances in the\n        attribute `feature_importances_` or `coef_`. Otherwise, returns `None`.\n\n        Parameters\n        ----------\n        sort_importance: bool, default `True`\n            If `True`, sorts the feature importances in descending order.\n\n        Returns\n        -------\n        feature_importances : pandas DataFrame\n            Feature importances associated with each predictor.\n\n        \"\"\"\n        if not self.is_fitted:\n            raise NotFittedError('This forecaster is not fitted yet. Call `fit` with appropriate arguments before using `get_feature_importances()`.')\n        if isinstance(self.regressor, Pipeline):\n            estimator = self.regressor[-1]\n        else:\n            estimator = self.regressor\n        if hasattr(estimator, 'feature_importances_'):\n            feature_importances = estimator.feature_importances_\n        elif hasattr(estimator, 'coef_'):\n            feature_importances = estimator.coef_\n        else:\n            warnings.warn(f'Impossible to access feature importances for regressor of type {type(estimator)}. This method is only valid when the regressor stores internally the feature importances in the attribute `feature_importances_` or `coef_`.')\n            feature_importances = None\n        if feature_importances is not None:\n            feature_importances = pd.DataFrame({'feature': self.X_train_features_names_out_, 'importance': feature_importances})\n            if sort_importance:\n                feature_importances = feature_importances.sort_values(by='importance', ascending=False)\n        return feature_importances"
  },
  "call_tree": {
    "skforecast/recursive/tests/tests_forecaster_recursive/test_binning_in_sample_residuals.py:test_binning_in_sample_residuals_output": {
      "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:__init__": {
        "skforecast/utils/utils.py:initialize_lags": {},
        "skforecast/utils/utils.py:initialize_window_features": {},
        "skforecast/preprocessing/preprocessing.py:QuantileBinner:__init__": {
          "skforecast/preprocessing/preprocessing.py:QuantileBinner:_validate_params": {}
        },
        "skforecast/utils/utils.py:initialize_weights": {},
        "skforecast/utils/utils.py:check_select_fit_kwargs": {}
      },
      "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:_binning_in_sample_residuals": {
        "skforecast/preprocessing/preprocessing.py:QuantileBinner:fit_transform": {
          "skforecast/preprocessing/preprocessing.py:QuantileBinner:fit": {},
          "skforecast/preprocessing/preprocessing.py:QuantileBinner:transform": {}
        }
      }
    },
    "skforecast/recursive/tests/tests_forecaster_recursive/test_binning_in_sample_residuals.py:test_binning_in_sample_residuals_stores_maximum_10000_residuals": {
      "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:__init__": {
        "skforecast/utils/utils.py:initialize_lags": {},
        "skforecast/utils/utils.py:initialize_window_features": {},
        "skforecast/preprocessing/preprocessing.py:QuantileBinner:__init__": {
          "skforecast/preprocessing/preprocessing.py:QuantileBinner:_validate_params": {}
        },
        "skforecast/utils/utils.py:initialize_weights": {},
        "skforecast/utils/utils.py:check_select_fit_kwargs": {}
      },
      "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:fit": {
        "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:_create_train_X_y": {
          "skforecast/utils/utils.py:check_y": {},
          "skforecast/utils/utils.py:input_to_frame": {},
          "skforecast/utils/utils.py:transform_dataframe": {},
          "skforecast/utils/utils.py:preprocess_y": {},
          "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:_create_lags": {}
        },
        "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:create_sample_weights": {},
        "skforecast/utils/utils.py:preprocess_y": {},
        "skforecast/recursive/_forecaster_recursive.py:ForecasterRecursive:_binning_in_sample_residuals": {
          "skforecast/preprocessing/preprocessing.py:QuantileBinner:fit_transform": {
            "skforecast/preprocessing/preprocessing.py:QuantileBinner:fit": {},
            "skforecast/preprocessing/preprocessing.py:QuantileBinner:transform": {}
          }
        }
      }
    }
  }
}