{
  "dir_path": "/app/amazon_textract_textractor",
  "package_name": "amazon_textract_textractor",
  "sample_name": "amazon_textract_textractor-test_layout",
  "src_dir": "textractor/",
  "test_dir": "tests/",
  "test_file": "tests/test_layout.py",
  "test_code": "import json\nimport os\nimport unittest\nimport PIL\nfrom tests.utils import get_fixture_path\nfrom textractor import Textractor\nfrom textractor.entities.document import Document\nfrom textractor.entities.word import Word\nfrom textractor.entities.line import Line\nfrom textractor.entities.page import Page\nfrom textractor.entities.table import Table\nfrom textractor.entities.value import Value\nfrom textractor.data.constants import TableFormat\nfrom textractor.entities.key_value import KeyValue\nfrom textractor.visualizers.entitylist import EntityList\nfrom textractor.exceptions import InvalidProfileNameError\nfrom textractor.entities.selection_element import SelectionElement\nfrom textractor.data.constants import TextTypes, SimilarityMetric, TextractFeatures, Direction, DirectionalFinderType\n\nfrom .utils import save_document_to_fixture_path\n\nclass TestLayout(unittest.TestCase):\n    def test_layout(self):\n        profile_name = \"default\"\n        current_directory = os.path.abspath(os.path.dirname(__file__))\n\n        if profile_name is None:\n            raise InvalidProfileNameError(\n                \"Textractor could not be initialized. Populate profile_name with a valid input in tests/test_table.py.\"\n            )\n\n        if os.environ.get(\"CALL_TEXTRACT\"):\n            extractor = Textractor(profile_name=profile_name, kms_key_id=\"\")\n            document = extractor.analyze_document(\n                file_source=os.path.join(current_directory, \"fixtures/paystub.jpg\"),\n                features=[TextractFeatures.LAYOUT, TextractFeatures.TABLES, TextractFeatures.FORMS],\n            )\n            with open(get_fixture_path(), \"w\") as f:\n                json.dump(document.response, f)\n        else:\n            document = Document.open(get_fixture_path())\n\n        print(document.text)\n",
  "GT_file_code": {
    "textractor/entities/document.py": "\"\"\"The Document class is defined to host all the various DocumentEntity objects within it. :class:`DocumentEntity` objects can be \naccessed, searched and exported the functions given below.\"\"\"\n\nimport boto3\nimport json\nimport os\nimport string\nimport logging\nimport xlsxwriter\nimport io\nfrom pathlib import Path\nfrom typing import List, IO, Union, AnyStr, Tuple\nfrom copy import deepcopy\nfrom collections import defaultdict\nfrom PIL import Image\n\nfrom textractor.entities.expense_document import ExpenseDocument\nfrom textractor.entities.identity_document import IdentityDocument\nfrom textractor.entities.word import Word\nfrom textractor.entities.line import Line\nfrom textractor.entities.page import Page\nfrom textractor.entities.table import Table\nfrom textractor.entities.query import Query\nfrom textractor.entities.signature import Signature\nfrom textractor.entities.layout import Layout\nfrom textractor.exceptions import InputError\nfrom textractor.entities.key_value import KeyValue\nfrom textractor.entities.bbox import SpatialObject\nfrom textractor.utils.s3_utils import download_from_s3\nfrom textractor.visualizers.entitylist import EntityList\nfrom textractor.data.constants import (\n    TextTypes,\n    SimilarityMetric,\n    Direction,\n    DirectionalFinderType,\n)\nfrom textractor.utils.search_utils import SearchUtils\nfrom textractor.data.text_linearization_config import TextLinearizationConfig\nfrom textractor.data.html_linearization_config import HTMLLinearizationConfig\nfrom textractor.entities.linearizable import Linearizable\n\n\nclass Document(SpatialObject, Linearizable):\n    \"\"\"\n    Represents the description of a single document, as it would appear in the input to the Textract API.\n    Document serves as the root node of the object model hierarchy,\n    which should be used as an intermediate form for most analytic purposes.\n    The Document node also contains the metadata of the document.\n    \"\"\"\n\n    @classmethod\n    def open(cls, fp: Union[dict, str, Path, IO[AnyStr]]):\n        \"\"\"Create a Document object from a JSON file path, file handle or response dictionary\n\n        :param fp: _description_\n        :type fp: Union[dict, str, Path, IO[AnyStr]]\n        :raises InputError: Raised on input not being of type Union[dict, str, Path, IO[AnyStr]]\n        :return: Document object\n        :rtype: Document\n        \"\"\"\n        from textractor.parsers import response_parser\n\n        if isinstance(fp, dict):\n            return response_parser.parse(fp)\n        elif isinstance(fp, str):\n            if fp.startswith(\"s3://\"):\n                # FIXME: Opening s3 clients for everything should be avoided\n                client = boto3.client(\"s3\")\n                return response_parser.parse(json.load(download_from_s3(client, fp)))\n            with open(fp, \"r\") as f:\n                return response_parser.parse(json.load(f))\n        elif isinstance(fp, Path):\n            with open(fp, \"r\") as f:\n                return response_parser.parse(json.load(f))\n        elif isinstance(fp, io.IOBase):\n            return response_parser.parse(json.load(fp))\n        else:\n            raise InputError(\n                f\"Document.open() input must be of type dict, str, Path or a file handle, not {type(fp)}\"\n            )\n\n    def __init__(self, num_pages: int = 1):\n        \"\"\"\n        Creates a new document, ideally containing entity objects pertaining to each page.\n\n        :param num_pages: Number of pages in the input Document.\n        \"\"\"\n        super().__init__(width=0, height=0)\n        self.num_pages: int = num_pages\n        self._pages: List[Page] = []\n        self._identity_documents: List[IdentityDocument] = []\n        self._trp2_document = None\n        self.response = None\n\n    @property\n    def words(self) -> EntityList[Word]:\n        \"\"\"\n        Returns all the :class:`Word` objects present in the Document.\n\n        :return: List of Word objects, each representing a word within the Document.\n        :rtype: EntityList[Word]\n        \"\"\"\n        return EntityList(sum([page.words for page in self.pages], []))\n\n    @property\n    def text(self) -> str:\n        \"\"\"Returns the document text as one string\n\n        :return: Page text seperated by line return\n        :rtype: str\n        \"\"\"\n        return os.linesep.join([page.text for page in self.pages])\n\n    @property\n    def identity_documents(self) -> EntityList[IdentityDocument]:\n        \"\"\"\n        Returns all the :class:`IdentityDocument` objects present in the Document.\n\n        :return: List of IdentityDocument objects, each representing an identity document within the Document.\n        :rtype: EntityList[IdentityDocument]\n        \"\"\"\n        return EntityList(self._identity_documents)\n\n    @identity_documents.setter\n    def identity_documents(self, identity_documents: List[IdentityDocument]):\n        \"\"\"\n        Set all the identity documents detected inside the Document\n        \"\"\"\n        self._identity_documents = identity_documents\n\n    @property\n    def expense_documents(self) -> EntityList[ExpenseDocument]:\n        \"\"\"\n        Returns all the :class:`ExpenseDocument` objects present in the Document.\n\n        :return: List of ExpenseDocument objects, each representing an expense document within the Document.\n        :rtype: EntityList[ExpenseDocument]\n        \"\"\"\n        return EntityList(sum([page.expense_documents for page in self.pages], []))\n\n    @property\n    def lines(self) -> EntityList[Line]:\n        \"\"\"\n        Returns all the :class:`Line` objects present in the Document.\n\n        :return: List of Line objects, each representing a line within the Document.\n        :rtype: EntityList[Line]\n        \"\"\"\n        return EntityList(sum([page.lines for page in self.pages], []))\n\n    @property\n    def key_values(self) -> EntityList[KeyValue]:\n        \"\"\"\n        Returns all the :class:`KeyValue` objects present in the Document.\n\n        :return: List of KeyValue objects, each representing a key-value pair within the Document.\n        :rtype: EntityList[KeyValue]\n        \"\"\"\n        return EntityList(sum([page.key_values for page in self.pages], []))\n\n    @property\n    def checkboxes(self) -> EntityList[KeyValue]:\n        \"\"\"\n        Returns all the :class:`KeyValue` objects with SelectionElements present in the Document.\n\n        :return: List of KeyValue objects, each representing a checkbox within the Document.\n        :rtype: EntityList[KeyValue]\n        \"\"\"\n        return EntityList(sum([page.checkboxes for page in self.pages], []))\n\n    @property\n    def tables(self) -> EntityList[Table]:\n        \"\"\"\n        Returns all the :class:`Table` objects present in the Document.\n\n        :return: List of Table objects, each representing a table within the Document.\n        :rtype: EntityList[Table]\n        \"\"\"\n        return EntityList(sum([page.tables for page in self.pages], []))\n\n    @property\n    def queries(self) -> EntityList[Query]:\n        \"\"\"\n        Returns all the :class:`Query` objects present in the Document.\n\n        :return: List of Query objects.\n        :rtype: EntityList[Query]\n        \"\"\"\n        return EntityList(sum([page.queries for page in self.pages], []))\n\n    @property\n    def signatures(self) -> EntityList[Signature]:\n        \"\"\"\n        Returns all the :class:`Signature` objects present in the Document.\n\n        :return: List of Signature objects.\n        :rtype: EntityList[Signature]\n        \"\"\"\n        return EntityList(sum([page.signatures for page in self.pages], []))\n\n    @property\n    def layouts(self) -> EntityList[Layout]:\n        \"\"\"\n        Returns all the :class:`Layout` objects present in the Document\n\n        :return: List of Layout objects\n        :rtype: EntityList[Layout]\n        \"\"\"\n        return EntityList(sum([page.layouts for page in self.pages], []))\n\n    @property\n    def identity_document(self) -> EntityList[IdentityDocument]:\n        \"\"\"\n        Returns all the :class:`IdentityDocument` objects present in the Page.\n\n        :return: List of IdentityDocument objects.\n        :rtype: EntityList\n        \"\"\"\n        return EntityList(self._identity_documents)\n\n    @identity_document.setter\n    def identity_document(self, identity_documents: List[IdentityDocument]):\n        \"\"\"\n        Add IdentityDocument objects to the Page.\n\n        :param tables: List of IdentityDocument objects.\n        :type identity_documents: list\n        \"\"\"\n        self._identity_document = identity_documents\n\n    @property\n    def images(self) -> List[Image.Image]:\n        \"\"\"\n        Returns all the page images in the Document.\n\n        :return: List of PIL Image objects.\n        :rtype: PIL.Image\n        \"\"\"\n        return [page.image for page in self._pages]\n\n    @property\n    def pages(self) -> List[Page]:\n        \"\"\"\n        Returns all the :class:`Page` objects present in the Document.\n\n        :return: List of Page objects, each representing a Page within the Document.\n        :rtype: List\n        \"\"\"\n        return self._pages\n\n    @pages.setter\n    def pages(self, pages: List[Page]):\n        \"\"\"\n        Add Page objects to the Document.\n\n        :param pages: List of Page objects, each representing a page within the document.\n        No specific ordering is assumed with input.\n        :type pages: List[Page]\n        \"\"\"\n        self._pages = sorted(pages, key=lambda x: x.page_num)\n\n    def get_text_and_words(\n        self, config: TextLinearizationConfig = TextLinearizationConfig()\n    ) -> Tuple[str, List]:\n        text, words_lists = zip(*[p.get_text_and_words(config) for p in self.pages])\n        flattened_words = []\n        for words in words_lists:\n            flattened_words.extend(words)\n        return config.layout_element_separator.join(text), flattened_words\n\n    def page(self, page_no: int = 0):\n        \"\"\"\n        Returns :class:`Page` object/s depending on the input page_no. Follows zero-indexing.\n\n        :param page_no: if int, returns single Page Object, else if list, it returns a list of\n                        Page objects.\n        :type page_no: int if single page, list of int if multiple pages\n\n        :return: Filters and returns Page objects depending on the input page_no\n        :rtype: Page or List[Page]\n        \"\"\"\n        if isinstance(page_no, int):\n            return self.pages[page_no]\n        elif isinstance(page_no, list):\n            return [self.pages[num] for num in page_no]\n        else:\n            raise InputError(\"page_no parameter doesn't match required data type.\")\n\n    def to_html(self, config: HTMLLinearizationConfig = HTMLLinearizationConfig()):\n        \"\"\"\n        Returns the HTML representation of the document, effectively calls Linearizable.to_html()\n        but add <html><body></body></html> around the result and put each page in a <div>. \n\n        :return: HTML text of the entity\n        :rtype: str\n        \"\"\"\n        \n        html = \"<html><body>\"\n        for page in self.pages:\n            html += f\"<div>{page.to_html(config=config)}</div>\"\n        html += \"</body></html>\"\n        \n        return html\n\n    def __repr__(self):\n        return os.linesep.join(\n            [\n                \"This document holds the following data:\",\n                f\"Pages - {len(self.pages)}\",\n                f\"Words - {len(self.words)}\",\n                f\"Lines - {len(self.lines)}\",\n                f\"Key-values - {len(self.key_values)}\",\n                f\"Checkboxes - {len(self.checkboxes)}\",\n                f\"Tables - {len(self.tables)}\",\n                f\"Queries - {len(self.queries)}\",\n                f\"Signatures - {len(self.signatures)}\",\n                f\"Identity Documents - {len(self.identity_documents)}\",\n                f\"Expense Documents - {len(self.expense_documents)}\",\n            ]\n        )\n\n    def to_trp2(self):\n        \"\"\"\n        Parses the response to the trp2 format for backward compatibility\n\n        :return: TDocument object that can be used with the older Textractor libraries\n        :rtype: TDocument\n        \"\"\"\n        from trp.trp2 import TDocument, TDocumentSchema\n        \n        if not self._trp2_document:\n            self._trp2_document = TDocumentSchema().load(self.response)\n        return self._trp2_document\n\n    def visualize(self, *args, **kwargs):\n        \"\"\"\n        Returns the object's children in a visualization EntityList object\n\n        :return: Returns an EntityList object\n        :rtype: EntityList\n        \"\"\"\n        return EntityList(self.pages).visualize(*args, **kwargs)\n\n    def keys(self, include_checkboxes: bool = True) -> List[str]:\n        \"\"\"\n        Prints all keys for key-value pairs and checkboxes if the document contains them.\n\n        :param include_checkboxes: True/False. Set False if checkboxes need to be excluded.\n        :type include_checkboxes: bool\n\n        :return: List of strings containing key names in the Document\n        :rtype: List[str]\n        \"\"\"\n        keys = []\n        keys = [keyvalue.key for keyvalue in self.key_values]\n        if include_checkboxes:\n            keys += [keyvalue.key for keyvalue in self.checkboxes]\n        return keys\n\n    def filter_checkboxes(\n        self, selected: bool = True, not_selected: bool = True\n    ) -> List[KeyValue]:\n        \"\"\"\n        Return a list of :class:`KeyValue` objects containing checkboxes if the document contains them.\n\n        :param selected: True/False Return SELECTED checkboxes\n        :type selected: bool\n        :param not_selected: True/False Return NOT_SELECTED checkboxes\n        :type not_selected: bool\n\n        :return: Returns checkboxes that match the conditions set by the flags.\n        :rtype: EntityList[KeyValue]\n        \"\"\"\n\n        checkboxes = EntityList([])\n        for page in self.pages:\n            checkboxes.extend(\n                page.filter_checkboxes(selected=selected, not_selected=not_selected)\n            )\n        return checkboxes\n\n    def get_words_by_type(self, text_type: TextTypes = TextTypes.PRINTED) -> List[Word]:\n        \"\"\"\n        Returns list of :class:`Word` entities that match the input text type.\n\n        :param text_type: TextTypes.PRINTED or TextTypes.HANDWRITING\n        :type text_type: TextTypes\n        :return: Returns list of Word entities that match the input text type.\n        :rtype: EntityList[Word]\n        \"\"\"\n        if not self.words:\n            logging.warn(\"Document contains no word entities.\")\n            return []\n\n        filtered_words = EntityList()\n        for page in self.pages:\n            filtered_words.extend(page.get_words_by_type(text_type=text_type))\n        return filtered_words\n\n    def search_words(\n        self,\n        keyword: str,\n        top_k: int = 1,\n        similarity_metric: SimilarityMetric = SimilarityMetric.LEVENSHTEIN,\n        similarity_threshold: float = 0.6,\n    ) -> List[Word]:\n        \"\"\"\n        Return a list of top_k words that match the keyword.\n\n        :param keyword: Keyword that is used to query the document.\n        :type keyword: str\n        :param top_k: Number of closest word objects to be returned\n        :type top_k: int\n        :param similarity_metric: SimilarityMetric.COSINE, SimilarityMetric.EUCLIDEAN or SimilarityMetric.LEVENSHTEIN. SimilarityMetric.COSINE is chosen as default.\n        :type similarity_metric: SimilarityMetric\n        :param similarity_threshold: Measure of how similar document key is to queried key. default=0.6\n        :type similarity_threshold: float\n\n        :return: Returns a list of words that match the queried key sorted from highest\n                 to lowest similarity.\n        :rtype: EntityList[Word]\n        \"\"\"\n\n        top_n_words = []\n        for page in self.pages:\n            top_n_words.extend(\n                page._search_words_with_similarity(\n                    keyword=keyword,\n                    top_k=top_k,\n                    similarity_metric=similarity_metric,\n                    similarity_threshold=similarity_threshold,\n                )\n            )\n\n        top_n_words = sorted(top_n_words, key=lambda x: x[0], reverse=True)[:top_k]\n        top_n_words = EntityList([ent[1] for ent in top_n_words])\n\n        return top_n_words\n\n    def search_lines(\n        self,\n        keyword: str,\n        top_k: int = 1,\n        similarity_metric: SimilarityMetric = SimilarityMetric.LEVENSHTEIN,\n        similarity_threshold: float = 0.6,\n    ) -> List[Line]:\n        \"\"\"\n        Return a list of top_k lines that contain the queried keyword.\n\n        :param keyword: Keyword that is used to query the document.\n        :type keyword: str\n        :param top_k: Number of closest line objects to be returned\n        :type top_k: int\n        :param similarity_metric: SimilarityMetric.COSINE, SimilarityMetric.EUCLIDEAN or SimilarityMetric.LEVENSHTEIN. SimilarityMetric.COSINE is chosen as default.\n        :type similarity_metric: SimilarityMetric\n        :param similarity_threshold: Measure of how similar document key is to queried key. default=0.6\n        :type similarity_threshold: float\n\n        :return: Returns a list of lines that contain the queried key sorted from highest\n                 to lowest similarity.\n        :rtype: EntityList[Line]\n        \"\"\"\n        if not isinstance(similarity_metric, SimilarityMetric):\n            raise InputError(\n                \"similarity_metric parameter should be of SimilarityMetric type. Find input choices from textractor.data.constants\"\n            )\n\n        top_n_lines = []\n        for page in self.pages:\n            top_n_lines.extend(\n                page._search_lines_with_similarity(\n                    keyword=keyword,\n                    top_k=top_k,\n                    similarity_metric=similarity_metric,\n                    similarity_threshold=similarity_threshold,\n                )\n            )\n\n        top_n_lines = EntityList([ent[1] for ent in top_n_lines][:top_k])\n\n        return top_n_lines\n\n    # KeyValue entity related functions\n    def get(\n        self,\n        key: str,\n        top_k_matches: int = 1,\n        similarity_metric: SimilarityMetric = SimilarityMetric.LEVENSHTEIN,\n        similarity_threshold: float = 0.6,\n    ):\n        \"\"\"\n        Return upto top_k_matches of key-value pairs for the key that is queried from the document.\n\n        :param key: Query key to match\n        :type key: str\n        :param top_k_matches: Maximum number of matches to return\n        :type top_k_matches: int\n        :param similarity_metric: SimilarityMetric.COSINE, SimilarityMetric.EUCLIDEAN or SimilarityMetric.LEVENSHTEIN. SimilarityMetric.COSINE is chosen as default.\n        :type similarity_metric: SimilarityMetric\n        :param similarity_threshold: Measure of how similar document key is to queried key. default=0.6\n        :type similarity_threshold: float\n\n        :return: Returns a list of key-value pairs that match the queried key sorted from highest to lowest similarity.\n        :rtype: EntityList[KeyValue]\n        \"\"\"\n        if not isinstance(similarity_metric, SimilarityMetric):\n            raise InputError(\n                \"similarity_metric parameter should be of SimilarityMetric type. Find input choices from textractor.data.constants\"\n            )\n\n        top_n = []\n        similarity_threshold = (\n            -similarity_threshold\n            if similarity_metric == SimilarityMetric.EUCLIDEAN\n            else similarity_threshold\n        )\n        lowest_similarity = similarity_threshold\n\n        for kv in self.key_values + self.checkboxes:\n            try:\n                edited_document_key = \"\".join(\n                    [\n                        char\n                        for char in kv.key.__repr__()\n                        if char not in string.punctuation\n                    ]\n                )\n            except:\n                pass\n            key = \"\".join([char for char in key if char not in string.punctuation])\n\n            similarity = [\n                SearchUtils.get_word_similarity(key, word, similarity_metric)\n                for word in edited_document_key.split(\" \")\n            ]\n            similarity.append(\n                SearchUtils.get_word_similarity(\n                    key, edited_document_key, similarity_metric\n                )\n            )\n\n            similarity = (\n                min(similarity)\n                if similarity_metric == SimilarityMetric.EUCLIDEAN\n                else max(similarity)\n            )\n\n            if similarity > similarity_threshold:\n                if len(top_n) < top_k_matches:\n                    top_n.append((kv, similarity))\n                elif similarity > lowest_similarity:\n                    top_n[-1] = (kv, similarity)\n                top_n = sorted(top_n, key=lambda x: x[1], reverse=True)\n                lowest_similarity = top_n[-1][1]\n\n        if not top_n:\n            logging.warning(\n                f\"Query key does not match any existing keys in the document.{os.linesep}{self.keys()}\"\n            )\n            return EntityList([])\n\n        logging.info(f\"Query key matched {len(top_n)} key-values in the document.\")\n\n        return EntityList([value[0] for value in top_n])\n\n    # Export document entities into supported formats\n    def export_kv_to_csv(\n        self,\n        include_kv: bool = True,\n        include_checkboxes: bool = True,\n        filepath: str = \"Key-Values.csv\",\n        sep: str = \";\",\n    ):\n        \"\"\"\n        Export key-value entities and checkboxes in csv format.\n\n        :param include_kv: True if KVs are to be exported. Else False.\n        :type include_kv: bool\n        :param include_checkboxes: True if checkboxes are to be exported. Else False.\n        :type include_checkboxes: bool\n        :param filepath: Path to where file is to be stored.\n        :type filepath: str\n        :param sep: Separator to be used in the csv file.\n        :type sep: str\n        \"\"\"\n        keys = []\n        values = []\n        if include_kv and not self.key_values:\n            logging.warning(\"Document does not contain key-values.\")\n        elif include_kv:\n            for kv in self.key_values:\n                keys.append(\" \".join([w.text for w in kv.key]))\n                values.append(kv.value.get_text())\n\n        if include_checkboxes and not self.checkboxes:\n            logging.warning(\"Document does not contain checkbox elements.\")\n        elif include_checkboxes:\n            for kv in self.checkboxes:\n                keys.append(\" \".join([w.text for w in kv.key]))\n                values.append(kv.value.children[0].status.name)\n\n        with open(filepath, \"w\") as f:\n            f.write(f\"Key{sep}Value{os.linesep}\")\n            for k, v in zip(keys, values):\n                f.write(f\"{k}{sep}{v}{os.linesep}\")\n\n        logging.info(\n            f\"csv file stored at location {os.path.join(os.getcwd(),filepath)}\"\n        )\n\n    def export_kv_to_txt(\n        self,\n        include_kv: bool = True,\n        include_checkboxes: bool = True,\n        filepath: str = \"Key-Values.txt\",\n    ):\n        \"\"\"\n        Export key-value entities and checkboxes in txt format.\n\n        :param include_kv: True if KVs are to be exported. Else False.\n        :type include_kv: bool\n        :param include_checkboxes: True if checkboxes are to be exported. Else False.\n        :type include_checkboxes: bool\n        :param filepath: Path to where file is to be stored.\n        :type filepath: str\n        \"\"\"\n        export_str = \"\"\n        index = 1\n        if include_kv and not self.key_values:\n            logging.warning(\"Document does not contain key-values.\")\n        elif include_kv:\n            for kv in self.key_values:\n                export_str += (\n                    f\"{index}. {kv.key.__repr__()} : {kv.value.__repr__()}{os.linesep}\"\n                )\n                index += 1\n\n        if include_checkboxes and not self.checkboxes:\n            logging.warning(\"Document does not contain checkbox elements.\")\n        elif include_checkboxes:\n            for kv in self.checkboxes:\n                export_str += f\"{index}. {kv.key.__repr__()} : {kv.value.children[0].status.name}{os.linesep}\"\n                index += 1\n\n        with open(filepath, \"w\") as text_file:\n            text_file.write(export_str)\n        logging.info(\n            f\"txt file stored at location {os.path.join(os.getcwd(),filepath)}\"\n        )\n\n    def export_tables_to_excel(self, filepath):\n        \"\"\"\n        Creates an excel file and writes each table on a separate worksheet within the workbook.\n        This is stored on the filepath passed by the user.\n\n        :param filepath: Path to store the exported Excel file.\n        :type filepath: str, required\n        \"\"\"\n        if not filepath:\n            logging.error(\"Filepath required to store excel file.\")\n        workbook = xlsxwriter.Workbook(filepath)\n        for table in self.tables:\n            workbook = table.to_excel(\n                filepath=None, workbook=workbook, save_workbook=False\n            )\n        workbook.close()\n\n    def independent_words(self):\n        \"\"\"\n        :return: Return all words in the document, outside of tables, checkboxes, key-values.\n        :rtype: EntityList[Word]\n        \"\"\"\n        if not self.words:\n            logging.warning(\"Words have not been assigned to this Document object.\")\n            return []\n\n        else:\n            table_words = sum([table.words for table in self.tables], [])\n            kv_words = sum([kv.words for kv in self.key_values], [])\n            checkbox_words = sum([kv.words for kv in self.checkboxes], [])\n            dependent_words = table_words + checkbox_words + kv_words\n            dependent_word_ids = set([word.id for word in dependent_words])\n            independent_words = [\n                word for word in self.words if word.id not in dependent_word_ids\n            ]\n            return EntityList(independent_words)\n\n    def return_duplicates(self):\n        \"\"\"\n        Returns a dictionary containing page numbers as keys and list of :class:`EntityList` objects as values.\n        Each :class:`EntityList` instance contains the key-values and the last item is the table which contains duplicate information.\n        This function is intended to let the Textract user know of duplicate objects extracted by the various Textract models.\n\n        :return: Dictionary containing page numbers as keys and list of EntityList objects as values.\n        :rtype: Dict[page_num, List[EntityList[DocumentEntity]]]\n        \"\"\"\n        document_duplicates = defaultdict(list)\n\n        for page in self.pages:\n            document_duplicates[page.page_num].extend(page.return_duplicates())\n\n        return document_duplicates\n\n    def directional_finder(\n        self,\n        word_1: str = \"\",\n        word_2: str = \"\",\n        page: int = -1,\n        prefix: str = \"\",\n        direction=Direction.BELOW,\n        entities=[],\n    ):\n        \"\"\"\n        The function returns entity types present in entities by prepending the prefix provided by te user. This helps in cases of repeating\n        key-values and checkboxes. The user can manipulate original data or produce a copy. The main advantage of this function is to be able to define direction.\n\n        :param word_1: The reference word from where x1, y1 coordinates are derived\n        :type word_1: str, required\n        :param word_2: The second word preferably in the direction indicated by the parameter direction. When it isn't given the end of page coordinates are used in the given direction.\n        :type word_2: str, optional\n        :param page: page number of the page in the document to search the entities in.\n        :type page: int, required\n        :param prefix: User provided prefix to prepend to the key . Without prefix, the method acts as a search by geometry function\n        :type prefix: str, optional\n        :param entities: List of DirectionalFinderType inputs.\n        :type entities: List[DirectionalFinderType]\n\n        :return: Returns the EntityList of modified key-value and/or checkboxes\n        :rtype: EntityList\n        \"\"\"\n\n        if not word_1 or page == -1:\n            return EntityList([])\n\n        x1, x2, y1, y2 = self._get_coords(word_1, word_2, direction, page)\n\n        if x1 == -1:\n            return EntityList([])\n\n        page_obj = self.pages[page - 1]\n        entity_dict = {\n            DirectionalFinderType.KEY_VALUE_SET: self.key_values,\n            DirectionalFinderType.SELECTION_ELEMENT: self.checkboxes,\n        }\n\n        entitylist = []\n        for entity_type in entities:\n            entitylist.extend(list(entity_dict[entity_type]))\n\n        new_key_values = self._get_kv_with_direction(\n            direction, entitylist, (x1, x2, y1, y2)\n        )\n\n        final_kv = []\n        for kv in new_key_values:\n            if kv.key:\n                key_words = [deepcopy(word) for word in kv.key]\n                key_words[0].text = prefix + key_words[0].text\n                new_kv = deepcopy(kv)\n                new_kv.key = key_words\n                final_kv.append(new_kv)\n            else:\n                final_kv.append(kv)\n\n        return EntityList(final_kv)\n\n    def _get_kv_with_direction(self, direction, entitylist, coords):\n        \"\"\"Return key-values and checkboxes in entitylist present in the direction given with respect to the coordinates.\"\"\"\n        if direction == Direction.ABOVE:\n            new_key_values = [\n                kv\n                for kv in entitylist\n                if kv.bbox.y <= coords[2] and kv.bbox.y >= coords[-1]\n            ]\n\n        elif direction == Direction.BELOW:\n            new_key_values = [\n                kv\n                for kv in entitylist\n                if kv.bbox.y >= coords[2] and kv.bbox.y <= coords[-1]\n            ]\n\n        elif direction == Direction.RIGHT:\n            new_key_values = [\n                kv\n                for kv in entitylist\n                if kv.bbox.x >= coords[0] and kv.bbox.x <= coords[1]\n            ]\n            new_key_values = [\n                kv\n                for kv in new_key_values\n                if kv.bbox.y >= coords[2] - kv.bbox.height\n                and kv.bbox.y <= coords[-1] + 3 * kv.bbox.height\n            ]\n\n        elif direction == Direction.LEFT:\n            new_key_values = [\n                kv\n                for kv in entitylist\n                if kv.bbox.x <= coords[0] and kv.bbox.x >= coords[1]\n            ]\n            new_key_values = [\n                kv\n                for kv in new_key_values\n                if kv.bbox.y >= coords[2] - kv.bbox.height\n                and kv.bbox.y <= coords[-1] + 3 * kv.bbox.height\n            ]\n\n        return new_key_values\n\n    def _get_coords(self, word_1, word_2, direction, page):\n        \"\"\"\n        Returns coordinates for the area within which to search for key-values with the directional_finder by retrieving coordinates of word_1 \\\n        and word_2 if it exists else end of page.\n        \"\"\"\n        word_1_objects = self.search_lines(\n            keyword=word_1,\n            top_k=5,\n            similarity_metric=SimilarityMetric.COSINE,\n            similarity_threshold=0.5,\n        )\n        word_1_objects = (\n            [word for word in word_1_objects if word.page == page] if page != -1 else []\n        )\n\n        if not word_1_objects:\n            logging.warning(f\"{word_1} not found in page {page}\")\n            return -1, -1, -1, -1\n        else:\n            word_1_obj = word_1_objects[0]\n            x1, y1 = word_1_obj.bbox.x, word_1_obj.bbox.y\n\n        if word_2:\n            word_2_objects = self.search_lines(\n                keyword=word_2,\n                top_k=5,\n                similarity_metric=SimilarityMetric.COSINE,\n                similarity_threshold=0.5,\n            )\n            word_2_objects = [word for word in word_2_objects if word.page == page]\n            if not word_2_objects:\n                logging.warning(f\"{word_2} not found in page {page}\")\n                return -1, -1, -1, -1\n            else:\n                word_2_obj = word_2_objects[0]\n                x2, y2 = word_2_obj.bbox.x, word_2_obj.bbox.y\n        else:\n            x2, y2 = x1, y1\n\n        if direction == Direction.ABOVE:\n            x1, x2, y1, y2 = (x1, x2, y1, y2) if y2 < y1 else (x1, 0, y1, 0)\n\n        elif direction == Direction.BELOW:\n            x1, x2, y1, y2 = (x1, x2, y1, y2) if y2 > y1 else (x1, 1, y1, 1)\n\n        elif direction == Direction.RIGHT:\n            x1, x2, y1, y2 = (x1, x2, y1, y2) if x2 > x1 else (x1, 1, y1, y1)\n\n        elif direction == Direction.LEFT:\n            x1, x2, y1, y2 = (x1, x2, y1, y2) if x2 < x1 else (x1, 0, y1, y1)\n\n        else:\n            return -1, -1, -1, -1\n\n        return x1, x2, y1, y2\n",
    "textractor/parsers/response_parser.py": "\"\"\"\nConsumes Textract JSON response and converts them to a Document object format.\nThis class contains all the necessary utilities to create entity objects from JSON blocks within the response.\nUse ResponseParser's parse function to handle API response and convert them to Document objects.\n\"\"\"\n\nimport logging\nimport uuid\nfrom copy import deepcopy\nfrom typing import Any, List, Dict, Tuple\nfrom collections import defaultdict\nfrom textractor.entities.identity_document import IdentityDocument\nfrom textractor.entities.expense_document import ExpenseDocument\nfrom textractor.entities.expense_field import (\n    Expense,\n    ExpenseField,\n    ExpenseType,\n    ExpenseGroupProperty,\n    LineItemGroup,\n    LineItemRow,\n)\n\nfrom textractor.entities.page import Page\nfrom textractor.entities.query_result import QueryResult\nfrom textractor.entities.signature import Signature\nfrom textractor.entities.word import Word\nfrom textractor.entities.line import Line\nfrom textractor.entities.value import Value\nfrom textractor.entities.table import Table\nfrom textractor.entities.bbox import BoundingBox\nfrom textractor.entities.document import Document\nfrom textractor.entities.key_value import KeyValue\nfrom textractor.entities.table_cell import TableCell\nfrom textractor.entities.table_title import TableTitle\nfrom textractor.entities.table_footer import TableFooter\nfrom textractor.entities.query import Query\nfrom textractor.entities.selection_element import SelectionElement\nfrom textractor.entities.layout import Layout\nfrom textractor.data.constants import (\n    LAYOUT_ENTITY,\n    LAYOUT_FIGURE,\n    TABLE_FOOTER,\n    TABLE_TITLE,\n    COLUMN_HEADER,\n    TABLE_SUMMARY,\n    TABLE_SECTION_TITLE,\n    TABLE_STRUCTURED,\n    TABLE_SEMI_STRUCTURED,\n    SelectionStatus,\n    TextTypes,\n    TableTypes,\n    HANDWRITING,\n    PRINTED,\n    WORD,\n    LINE,\n    KEY_VALUE_SET,\n    CELL,\n    TABLE,\n    SELECTION_ELEMENT,\n    PAGE,\n    MERGED_CELL,\n    QUERY,\n    SIGNATURE,\n    LAYOUT,\n    LAYOUT_LIST,\n    LAYOUT_TABLE,\n    LAYOUT_KEY_VALUE,\n)\nfrom textractor.utils.legacy_utils import converter\n\nTHRESHOLD = 0.95\n\n\ndef _create_document_object(response: dict) -> Document:\n    \"\"\"\n    Consumes API Response in JSON format and creates a Document object.\n\n    :param response: json response from Textract API\n    :type response: dict\n\n    :return: Returns a Document object populated with metadata on number of pages.\n    :rtype: Document\n    \"\"\"\n    doc = Document(num_pages=response[\"DocumentMetadata\"][\"Pages\"])\n    return doc\n\n\ndef _filter_block_type(response: dict, entity: str) -> List[Dict[str, Any]]:\n    \"\"\"\n    Consumes entire JSON response, filters and returns list of blocks corresponding to the entity\n    parameter from API response JSON.\n\n    :param response: JSON response from Textract API\n    :type response: dict\n    :param entity: Entity to be extracted from the JSON response\n    :type entity: str\n\n    :return: Returns a list of JSON blocks that match entity parameter.\n    :rtype: List\n    \"\"\"\n    return [block for block in response[\"Blocks\"] if block[\"BlockType\"] == entity]\n\n\ndef _filter_by_entity(\n    block_json: List[Dict[str, Any]], entity_type: str\n) -> Dict[str, Any]:\n    \"\"\"\n    Filters and returns dictionary of blocks corresponding to the entity_type from API response JSON.\n\n    :param block_json: list of blocks belonging to a specific entity\n    :type block_json: List[Dict[str, Any]]\n    :param entity_type: EntityType used to select/filter from list of blocks\n    :type entity_type: str\n\n    :return: Dictionary mapping of block ID with JSON block for entity type.\n    :rtype: Dict[str, Any]\n    \"\"\"\n    return {\n        block[\"Id\"]: block\n        for block in block_json\n        if (\n            \"EntityTypes\" in block and\n            len(block[\"EntityTypes\"]) and\n            block[\"EntityTypes\"][0] == entity_type\n        )\n    }\n\n\ndef _get_relationship_ids(block_json: Dict[str, Any], relationship: str) -> List[str]:\n    \"\"\"\n    Takes the JSON block corresponding to an entity and returns the Ids of the chosen Relationship if the Relationship exists.\n\n    :param block_json: JSON block corresponding to an entity\n    :type block_json: List[Dict[str, Any]]\n    :relationship: CHILD or VALUE as input\n    :type relationship: str\n\n    :return: List of IDs with type Relationship to entity\n    :rtype: List\n    \"\"\"\n    ids = []\n    try:\n        ids = [\n            rel[\"Ids\"]\n            for rel in block_json[\"Relationships\"]\n            if rel[\"Type\"] == relationship\n        ][0]\n    except:\n        logging.info(\n            f\"{block_json['BlockType']} - {block_json['Id']} does not have ids with {relationship} relationship.\"\n        )\n    return ids\n\n\ndef _create_page_objects(\n    response: dict,\n) -> Tuple[Dict[str, Page], List[Dict[str, Any]]]:\n    \"\"\"\n    Consumes API Response in JSON format and returns Page objects for the Document.\n\n    :param response: JSON response from Textract API\n    :type response: dict\n\n    :return: Returns dictionary with page ID - Page object mapping,  list of JSON blocks belonging to PAGE blocks.\n    :rtype: Dict[str, Page], List[str]\n    \"\"\"\n    pages = []\n    page_elements = _filter_block_type(response, entity=PAGE)\n\n    for page_json in page_elements:\n        asset_id = page_json[\"Id\"]\n        width = page_json[\"Geometry\"][\"BoundingBox\"][\"Width\"]\n        height = page_json[\"Geometry\"][\"BoundingBox\"][\"Height\"]\n        page_num = page_json[\"Page\"] if len(page_elements) > 1 else 1\n        page_children = _get_relationship_ids(page_json, relationship=\"CHILD\")\n        page = Page(\n            id=asset_id,\n            width=width,\n            height=height,\n            page_num=page_num,\n            child_ids=page_children,\n        )\n        pages.append(page)\n    pages = {page.id: page for page in pages}\n    return pages, page_elements\n\n\ndef _create_word_objects(\n    word_ids: List[str],\n    id_json_map: Dict[str, str],\n    existing_words: Dict[str, Word],\n    page: Page,\n) -> List[Word]:\n    \"\"\"\n    Creates list of Word objects for all word_ids passed to the function.\n\n    :param word_ids: List of ids corresponding to the words present within Page.\n    :type word_ids: list\n    :param id_json_map: Dictionary containing entity_id: JSON block mapping.\n    :type id_json_map: dict\n    :param page: Instance of parent Page object.\n    :type page: Page\n\n    :return: Returns a list of Word objects for the IDs passed in word_ids.\n    :rtype: list\n    \"\"\"\n    words = []\n    text_type = {PRINTED: TextTypes.PRINTED, HANDWRITING: TextTypes.HANDWRITING}\n\n    for word_id in word_ids:\n        if word_id in existing_words:\n            words.append(existing_words[word_id])\n        else:\n            # FIXME: This could be gated\n            if not word_id in id_json_map:\n                continue\n            elem = id_json_map[word_id]\n            word = Word(\n                entity_id=elem[\"Id\"],\n                bbox=BoundingBox.from_normalized_dict(\n                    elem[\"Geometry\"][\"BoundingBox\"], spatial_object=page\n                ),\n                text=elem.get(\"Text\"),\n                text_type=text_type[elem.get(\"TextType\")],\n                confidence=elem[\"Confidence\"],\n            )\n            word.raw_object = elem\n            words.append(word)\n            existing_words[word_id] = word\n\n    for word in words:\n        word.page = page.page_num\n        word.page_id = page.id\n\n    return words\n\n\ndef _create_line_objects(\n    line_ids: List[str],\n    id_json_map: Dict[str, str],\n    existing_words: Dict[str, Word],\n    page: Page,\n) -> Tuple[List[Line], List[Word]]:\n    \"\"\"\n    Creates list of Line objects for all lines in the Page derived from the API JSON response.\n\n    :param line_ids: List of IDs corresponding to the lines present within Page.\n    :type line_ids: list\n    :param id_json_map: Dictionary containing entity_id: JSON block mapping.\n    :type id_json_map: dict\n    :param page: Instance of parent Page object.\n    :type page: Page\n\n    :return: Returns a list of Line objects for the IDs passed in line_ids and list of Word objects\n             belonging to the corresponding Line objects.\n    :rtype: List[Line], List[Word]\n    \"\"\"\n    page_lines = []\n\n    for line_id in line_ids:\n        if line_id in page.child_ids:\n            page_lines.append(id_json_map[line_id])\n\n    lines = []\n    page_words = []\n    for line in page_lines:\n        if _get_relationship_ids(line, relationship=\"CHILD\"):\n            line_words = _create_word_objects(\n                _get_relationship_ids(line, relationship=\"CHILD\"),\n                id_json_map,\n                existing_words,\n                page,\n            )\n            page_words.extend(line_words)\n            lines.append(\n                Line(\n                    entity_id=line[\"Id\"],\n                    bbox=BoundingBox.from_normalized_dict(\n                        line[\"Geometry\"][\"BoundingBox\"], spatial_object=page\n                    ),\n                    words=line_words,\n                    confidence=line[\"Confidence\"],\n                )\n            )\n            for word in line_words:\n                word.line = lines[-1]\n                word.line_id = lines[-1].id\n                word.line_bbox = lines[-1].bbox\n            lines[-1]._children = line_words\n            lines[-1].raw_object = line\n\n    for line in lines:\n        line.page = page.page_num\n        line.page_id = page.id\n\n    return lines, page_words\n\n\ndef _create_selection_objects(\n    selection_ids: List[str], id_json_map: Dict[str, Any], page: Page\n) -> Dict[str, SelectionElement]:\n    \"\"\"\n    Creates dictionary mapping of SelectionElement ID with SelectionElement objects for all ids passed in selection_ids.\n\n    :param selection_ids: List of ids corresponding to the SelectionElements.\n    :type selection_ids: list\n    :param id_json_map: Dictionary containing entity_id: JSON block mapping.\n    :type id_json_map: dict\n    :param page: Instance of parent Page object.\n    :type page: Page\n\n    :return: Returns a dictionary mapping of SelectionElement IDs with SelectionElement objects for the IDs present in\n             selection_ids.\n    :rtype: Dict[str, SelectionElement]\n    \"\"\"\n    checkbox_elements = [id_json_map[selection_id] for selection_id in selection_ids]\n\n    status = {\n        \"SELECTED\": SelectionStatus.SELECTED,\n        \"NOT_SELECTED\": SelectionStatus.NOT_SELECTED,\n    }\n\n    checkboxes = {}\n    for block in checkbox_elements:\n        checkboxes[block[\"Id\"]] = SelectionElement(\n            entity_id=block[\"Id\"],\n            bbox=BoundingBox.from_normalized_dict(\n                block[\"Geometry\"][\"BoundingBox\"], spatial_object=page\n            ),\n            status=status[block[\"SelectionStatus\"]],\n            confidence=block[\"Confidence\"],\n        )\n        checkboxes[block[\"Id\"]].raw_object = block\n\n    for c in checkboxes.values():\n        c.page = page.page_num\n        c.page_id = page.id\n\n    return checkboxes\n\n\ndef _create_value_objects(\n    value_ids: List[str],\n    id_json_map: Dict[str, Any],\n    entity_id_map: Dict[str, list],\n    existing_words: Dict[str, Word],\n    page: Page,\n) -> Tuple[Dict[str, Value], Dict[str, SelectionElement]]:\n    \"\"\"\n    Creates dictionary containing Value objects for all value_ids in the Page derived from the API response JSON.\n\n    :param value_ids: List of ids corresponding to the Values in the page.\n    :type value_ids: list\n    :param id_json_map: Dictionary containing entity_id:JSON block mapping.\n    :type id_json_map: dict\n    :param entity_id_map: Dictionary containing entity_type:List[entity_id] mapping.\n    :type entity_id_map: dict\n    :param page: Instance of parent Page object.\n    :type page: Page\n\n    :return: Dictionary mapping value_ids to Value objects.\n    :rtype: Dict[str, Value]\n    \"\"\"\n    # FIXME: This could be gated\n    values_info = {value_id: id_json_map.get(value_id, None) for value_id in value_ids}\n\n    values = {}\n    for block_id, block in values_info.items():\n        # FIXME: This should be gated\n        if block is None:\n            continue\n        values[block_id] = Value(\n            entity_id=block_id,\n            bbox=BoundingBox.from_normalized_dict(\n                block[\"Geometry\"][\"BoundingBox\"], spatial_object=page\n            ),\n            confidence=block[\"Confidence\"],\n        )\n        values[block_id].raw_object = block\n\n    checkboxes = _create_selection_objects(\n        entity_id_map[SELECTION_ELEMENT], id_json_map, page\n    )\n\n    # Add children to Value object\n    for val_id in values.keys():\n        val_child_ids = _get_relationship_ids(values_info[val_id], relationship=\"CHILD\")\n        for child_id in val_child_ids:\n            # FIXME: This should be gated\n            if child_id not in id_json_map:\n                continue\n            if id_json_map[child_id][\"BlockType\"] == WORD:\n                words = _create_word_objects(\n                    [child_id], id_json_map, existing_words, page\n                )\n                values[val_id].words += words\n                values[val_id].add_children(words)\n\n            elif id_json_map[child_id][\"BlockType\"] == SIGNATURE:\n                continue\n            else:\n                checkbox = checkboxes[child_id]\n                checkbox.value_id = val_id\n                values[val_id].add_children([checkbox])\n                values[val_id].contains_checkbox = True\n\n            values[val_id].page = page.page_num\n            values[val_id].page_id = page.id\n\n    return values, checkboxes\n\n\ndef _create_query_objects(\n    query_ids: List[str],\n    id_json_map: Dict[str, str],\n    entity_id_map: Dict[str, list],\n    page: Page,\n) -> List[Query]:\n    page_queries = []\n    for query_id in query_ids:\n        if query_id in page.child_ids:\n            page_queries.append(id_json_map[query_id])\n\n    query_result_id_map = {}\n    for block in page_queries:\n        answer = _get_relationship_ids(block, relationship=\"ANSWER\")\n        query_result_id_map[block[\"Id\"]] = answer[0] if answer else None\n\n    query_results = _create_query_result_objects(\n        list(query_result_id_map.values()), id_json_map, entity_id_map, page\n    )\n\n    queries = []\n    for query in page_queries:\n        query_result = query_results.get(query_result_id_map[query[\"Id\"]])\n        query_obj = Query(\n            query[\"Id\"],\n            query[\"Query\"][\"Text\"],\n            query[\"Query\"].get(\"Alias\"),\n            query_result,\n            query_result.bbox if query_result is not None else None,\n        )\n        query_obj.raw_object = query\n        queries.append(query_obj)\n\n    return queries\n\n\ndef _create_query_result_objects(\n    query_result_ids: List[str],\n    id_json_map: Dict[str, str],\n    entity_id_map: Dict[str, list],\n    page: Page,\n) -> Dict[str, QueryResult]:\n    page_query_results = []\n    for query_result_id in query_result_ids:\n        if query_result_id in page.child_ids and query_result_id in id_json_map:\n            page_query_results.append(id_json_map[query_result_id])\n\n    query_results = {}\n    for block in page_query_results:\n        query_results[block[\"Id\"]] = QueryResult(\n            entity_id=block[\"Id\"],\n            confidence=block[\"Confidence\"],\n            result_bbox=BoundingBox.from_normalized_dict(\n                (\n                    block.get(\"Geometry\") or\n                    {\n                        \"BoundingBox\": {\n                            \"Width\": 1.0,\n                            \"Height\": 1.0,\n                            \"Left\": 0.0,\n                            \"Top\": 0.0,\n                        }\n                    }\n                )[\"BoundingBox\"],\n                spatial_object=page,\n            ),\n            answer=block[\"Text\"],\n        )\n        query_results[block[\"Id\"]].raw_object = block\n\n    for query_result_id, query_result in query_results.items():\n        query_result.page = page.page_num\n        query_result.page_id = page.id\n\n    return query_results\n\n\ndef _create_signature_objects(\n    signature_ids: List[str],\n    id_json_map: Dict[str, str],\n    entity_id_map: Dict[str, list],\n    page: Page,\n) -> Dict[str, Signature]:\n    page_signatures = []\n    for signature_id in signature_ids:\n        if signature_id in page.child_ids:\n            page_signatures.append(id_json_map[signature_id])\n\n    signatures = {}\n    for block in page_signatures:\n        signatures[block[\"Id\"]] = Signature(\n            entity_id=block[\"Id\"],\n            confidence=block[\"Confidence\"],\n            bbox=BoundingBox.from_normalized_dict(\n                block[\"Geometry\"][\"BoundingBox\"], spatial_object=page\n            ),\n        )\n        signatures[block[\"Id\"]].raw_object = block\n\n    for signature_id, signature in signatures.items():\n        signature.page = page.page_num\n        signature.page_id = page.id\n\n    signatures_added = set()\n    for layout in sorted(page.leaf_layouts, key=lambda x: x.bbox.y):\n        if layout.layout_type == LAYOUT_ENTITY:\n            continue\n        for signature in sorted(signatures.values(), key=lambda x: x.bbox.y):\n            if (\n                layout.bbox.get_intersection(signature.bbox).area\n                > THRESHOLD * signature.bbox.area\n                and signature not in signatures_added\n            ):\n                layout.children.append(signature)\n                signatures_added.add(signature)\n                del signatures[signature.id]\n\n    signature_layouts = []\n    for signature in signatures.values():\n        if signature not in signatures_added:\n            signatures_added.add(signature)\n            layout = Layout(\n                entity_id=str(uuid.uuid4()),\n                bbox=signature.bbox,\n                label=LAYOUT_ENTITY,\n                reading_order=-1,\n            )\n            layout.children.append(signature)\n            layout.page = page.page_num\n            layout.page_id = page.id\n            signature_layouts.append(layout)\n\n    layouts_to_remove = []\n    for layout in page.leaf_layouts:\n        layouts_that_intersect = []\n        for signature_layout in signature_layouts:\n            intersection = layout.bbox.get_intersection(signature_layout.bbox).area\n            if intersection:\n                layouts_that_intersect.append(signature_layout)\n        words_in_sub_layouts = set()\n        for i, intersect_layout in enumerate(\n            sorted(layouts_that_intersect, key=lambda l: (l.bbox.y, l.bbox.x))\n        ):\n            intersect_layout.reading_order = (\n                (layout.reading_order + (i + 1) * 0.1)\n                if intersect_layout.reading_order == -1\n                else min(\n                    intersect_layout.reading_order, layout.reading_order + (i + 1) * 0.1\n                )\n            )\n            for w in intersect_layout.children[0].words:\n                words_in_sub_layouts.add(w.id)\n        if words_in_sub_layouts:\n            remaining_words = []\n            for w in layout.words:\n                if w.id not in words_in_sub_layouts:\n                    remaining_words.append(w)\n            if remaining_words:\n                layout.bbox = BoundingBox.enclosing_bbox(\n                    [w.bbox for w in remaining_words]\n                )\n                layout._children = list(set([w.line for w in remaining_words]))\n            else:\n                layouts_to_remove.append(layout)\n\n    for layout in layouts_to_remove:\n        page.leaf_layouts.remove(layout)\n\n    for layout in signature_layouts:\n        page.leaf_layouts.append(layout)\n\n    return list(signatures_added)\n\n\n\ndef _create_keyvalue_objects(\n    key_value_ids: List[str],\n    id_json_map: Dict[str, Any],\n    id_entity_map: Dict[str, str],\n    entity_id_map: Dict[str, list],\n    existing_words: Dict[str, Word],\n    page: Page,\n) -> Tuple[List[KeyValue], List[Word], Dict[str, SelectionElement]]:\n    \"\"\"\n    Creates list of KeyValue objects for all key-value pairs in the Page derived from the API response JSON.\n\n    :param key_value_ids: List of ids corresponding to the KeyValues in the page.\n    :type key_value_ids: list\n    :param id_json_map: Dictionary containing entity_id:JSON block mapping.\n    :type id_json_map: dict\n    :param entity_id_map: Dictionary containing entity_type:List[entity_id] mapping.\n    :type entity_id_map: dict\n    :param id_entity_map: Dictionary containing entity_id:entity_type mapping.\n    :type id_entity_map: dict\n    :param page: Instance of parent Page object.\n    :type page: Page\n\n    :return: Returns a list of KeyValue objects and list of Word objects with CHILD relationship\n             to the KeyValue objects.\n    :rtype: List[KeyValue], List[Word]\n    \"\"\"\n    page_kv = []\n    for kv_id in key_value_ids:\n        if kv_id in page.child_ids:\n            page_kv.append(id_json_map[kv_id])\n\n    keys_info = _filter_by_entity(page_kv, entity_type=\"KEY\")\n\n    key_value_id_map = {\n        block[\"Id\"]: _get_relationship_ids(block, relationship=\"VALUE\")[0]\n        for block in keys_info.values()\n    }\n\n    values, selection_elements = _create_value_objects(\n        list(key_value_id_map.values()),\n        id_json_map,\n        entity_id_map,\n        existing_words,\n        page,\n    )\n\n    keys = {}\n    for block in keys_info.values():\n        keys[block[\"Id\"]] = KeyValue(\n            entity_id=block[\"Id\"],\n            bbox=BoundingBox.from_normalized_dict(\n                block[\"Geometry\"][\"BoundingBox\"], spatial_object=page\n            ),\n            # FIXME: Should be gated\n            contains_checkbox=(\n                key_value_id_map[block[\"Id\"]] in values and\n                values[key_value_id_map[block[\"Id\"]]].contains_checkbox\n            ),\n            value=values.get(key_value_id_map[block[\"Id\"]]),\n            confidence=block[\"Confidence\"],\n        )\n        keys[block[\"Id\"]].raw_object = block\n\n    # Add words and children (Value Object) to KeyValue object\n    kv_words = []\n    for key_id in keys.keys():\n        if keys[key_id].value is None:\n            continue\n        keys[key_id].value.key_id = key_id\n        if keys[key_id].contains_checkbox:\n            keys[key_id].value.children[0].key_id = key_id\n            keys[key_id].selection_status = [\n                c\n                for c in keys[key_id].value.children\n                if c.__class__.__name__ == \"SelectionElement\"\n            ][0].status\n        else:\n            kv_words.extend(values[key_value_id_map[key_id]].words)\n\n        key_child_ids = _get_relationship_ids(keys_info[key_id], relationship=\"CHILD\")\n        key_word_ids = [\n            child_id\n            for child_id in key_child_ids\n            # FIXME: This should be gated\n            if child_id in id_json_map and id_json_map[child_id][\"BlockType\"] == WORD\n        ]\n        key_words = _create_word_objects(\n            key_word_ids, id_json_map, existing_words, page\n        )\n\n        key_child_ids = [\n            child_id for child_id in key_child_ids if child_id not in key_word_ids\n        ]\n        # find a place to add selection elements for keys\n\n        keys[key_id].key = key_words\n        keys[key_id].add_children(key_words)\n        keys[key_id].add_children([keys[key_id].value])\n        kv_words.extend(key_words)\n\n    key_values = list(keys.values())\n    for kv in key_values:\n        kv.bbox = BoundingBox.enclosing_bbox([kv.bbox] + ([kv.value.bbox] if kv.value is not None else []))\n        kv.page = page.page_num\n        kv.page_id = page.id\n\n    #kv_added = set()\n    #for layout in sorted(page.leaf_layouts, key=lambda x: x.bbox.y):\n    #    if layout.layout_type == LAYOUT_ENTITY:\n    #        continue\n    #    for kv in sorted(key_values, key=lambda x: x.bbox.y):\n    #        if (\n    #            layout.bbox.get_intersection(kv.bbox).area > THRESHOLD * kv.bbox.area\n    #            and kv not in kv_added\n    #        ):\n    #            layout.children.append(kv)\n    #            kv_added.add(kv)\n    #            key_values.remove(kv)\n\n    return key_values, kv_words, selection_elements\n\n\ndef _create_layout_objects(\n    layout_ids: List[Any],\n    id_json_map: Dict[str, str],\n    id_entity_map: Dict[str, List[str]],\n    line_by_id: Dict[str, Line],\n    page: Page,\n) -> Tuple[List[Layout], List[Layout]]:\n    \"\"\"\n    Creates Layout objects.\n\n    :param page_layouts: Reading-ordered list containing JSON structure of tables within the page.\n    :type page_layouts: list\n    :param id_json_map: Dictionary containing entity_id:JSON block mapping.\n    :type id_json_map: dict\n    :param id_entity_map: Dictionary containing entity_id:entity_type mapping.\n    :type id_entity_map: dict\n    :param page: Instance of parent Page object.\n    :type page: Page\n\n    :return: Returns a list containing Layout objects.\n    :rtype: List[Layout]\n    \"\"\"\n\n    page_layouts = []\n    for layout_id in layout_ids:\n        if layout_id in page.child_ids:\n            page_layouts.append(id_json_map[layout_id])\n\n    leaf_layouts = []\n    container_layouts = []\n    parsed_blocks = set()\n    for i, block in enumerate(page_layouts):\n        if block[\"Id\"] in parsed_blocks:\n            continue\n        if block[\"BlockType\"] in (LAYOUT_LIST,):\n            container_layouts.append(\n                Layout(\n                    entity_id=block[\"Id\"],\n                    confidence=block[\"Confidence\"],\n                    reading_order=i,\n                    label=block[\"BlockType\"],\n                    bbox=BoundingBox.from_normalized_dict(\n                        block[\"Geometry\"][\"BoundingBox\"], spatial_object=page\n                    ),\n                )\n            )\n            parsed_blocks.add(block[\"Id\"])\n            for relationship in (block.get(\"Relationships\", []) or []):\n                if relationship[\"Type\"] != \"CHILD\":\n                    continue\n                for leaf_id in relationship[\"Ids\"]:\n                    block = id_json_map[leaf_id]\n                    parsed_blocks.add(leaf_id)\n                    container_layouts[-1].children.append(\n                        Layout(\n                            entity_id=block[\"Id\"],\n                            confidence=block[\"Confidence\"],\n                            reading_order=i,\n                            label=block[\"BlockType\"],\n                            bbox=BoundingBox.from_normalized_dict(\n                                block[\"Geometry\"][\"BoundingBox\"], spatial_object=page\n                            ),\n                        )\n                    )\n                    container_layouts[-1].children[-1].raw_object = block\n                    for relationship in (block.get(\"Relationships\", []) or []):\n                        if relationship[\"Type\"] != \"CHILD\":\n                            continue\n                        container_layouts[-1].children[-1].add_children(\n                            [line_by_id[line_id] for line_id in relationship[\"Ids\"] if line_id in line_by_id]\n                        )\n        else:\n            leaf_layouts.append(\n                Layout(\n                    entity_id=block[\"Id\"],\n                    confidence=block[\"Confidence\"],\n                    reading_order=i,\n                    label=block[\"BlockType\"],\n                    bbox=BoundingBox.from_normalized_dict(\n                        block[\"Geometry\"][\"BoundingBox\"], spatial_object=page\n                    ),\n                )\n            )\n            leaf_layouts[-1].raw_object = block\n            for relationship in (block.get(\"Relationships\", []) or []):\n                if relationship[\"Type\"] != \"CHILD\":\n                    continue\n                leaf_layouts[-1].add_children(\n                    [line_by_id[line_id] for line_id in relationship[\"Ids\"] if line_id in line_by_id]\n                )\n\n    for layout in leaf_layouts + container_layouts:\n        layout.page = page.page_num\n        layout.page_id = page.id\n\n    return container_layouts, leaf_layouts\n\n\ndef _create_table_cell_objects(\n    page_tables: List[Any],\n    id_entity_map: Dict[str, List[str]],\n    id_json_map: Dict[str, str],\n    page: Page,\n) -> Tuple[Dict[str, TableCell], Dict[str, Any]]:\n    \"\"\"\n    Creates TableCell objects for all page_tables passed as input present on a single Page of the Document.\n\n    :param page_tables: List containing JSON structure of tables within the page.\n    :type page_tables: list\n    :param id_entity_map: Dictionary containing entity_id:entity_type mapping.\n    :type id_entity_map: dict\n    :param id_json_map: Dictionary containing entity_id:JSON block mapping.\n    :type id_json_map: dict\n    :param page: Instance of parent Page object.\n    :type page: Page\n\n    :return: Returns a dictionary containing TableCells mapped with their IDs and dictionary containing ID: CELL JSON mapping.\n    :rtype: Dict[str, TableCell], Dict[str, Any]\n    \"\"\"\n    all_table_cells_info = {}\n    for table in page_tables:\n        for cell_id in _get_relationship_ids(table, relationship=\"CHILD\"):\n            # FIXME: This should be gated\n            if cell_id in id_entity_map and id_entity_map[cell_id] == CELL:\n                all_table_cells_info[cell_id] = id_json_map[cell_id]\n\n    table_cells = {}\n    for elem_id, elem in all_table_cells_info.items():\n        entity_types = elem.get(\"EntityTypes\", []) or []\n        table_cells[elem_id] = TableCell(\n            entity_id=elem_id,\n            bbox=BoundingBox.from_normalized_dict(\n                elem[\"Geometry\"][\"BoundingBox\"], spatial_object=page\n            ),\n            row_index=elem[\"RowIndex\"],\n            col_index=elem[\"ColumnIndex\"],\n            row_span=elem[\"RowSpan\"],\n            col_span=elem[\"ColumnSpan\"],\n            confidence=elem[\"Confidence\"],\n            is_column_header=COLUMN_HEADER in entity_types,\n            is_title=TABLE_TITLE in entity_types,\n            is_footer=TABLE_FOOTER in entity_types,\n            is_summary=TABLE_SUMMARY in entity_types,\n            is_section_title=TABLE_SECTION_TITLE in entity_types,\n        )\n        table_cells[elem_id].raw_object = elem\n\n    for cell in table_cells.values():\n        cell.page = page.page_num\n        cell.page_id = page.id\n    return table_cells, all_table_cells_info\n\n\ndef _create_table_objects(\n    table_ids: List[str],\n    id_json_map: Dict[str, Any],\n    id_entity_map: Dict[str, List[str]],\n    entity_id_map: Dict[str, List[str]],\n    existing_words: Dict[str, Word],\n    key_values: Dict[str, KeyValue],\n    checkboxes: Dict[str, SelectionElement],\n    page: Page,\n) -> Tuple[List[Table], List[Word]]:\n    \"\"\"\n    Creates list of Table objects for all tables in the Page derived from the API response JSON.\n    This includes creating TableCell objects and updating metadata for each cell. The TableCell objects are assigned as children\n    of the table.\n\n    :param table_ids: List of ids corresponding to the Tables in the page.\n    :type table_ids: list\n    :param id_json_map: Dictionary containing entity_id:JSON block mapping.\n    :type id_json_map: dict\n    :param id_entity_map: Dictionary containing entity_id:entity_type mapping.\n    :type id_entity_map: dict\n    :param entity_id_map: Dictionary containing entity_type:List[entity_id] mapping.\n    :type entity_id_map: dict\n    :param page: Instance of parent Page object.\n    :type page: Page\n\n    :return: Returns a list of table objects and list of words present in tables.\n    :rtype: List[Table], List[Word]\n    \"\"\"\n    # Create Tables\n    page_tables = []\n    for table_id in table_ids:\n        if table_id in page.child_ids:\n            page_tables.append(id_json_map[table_id])\n\n    tables = {}\n    for val in page_tables:\n        tables[val[\"Id\"]] = Table(\n            entity_id=val[\"Id\"],\n            bbox=BoundingBox.from_normalized_dict(\n                val[\"Geometry\"][\"BoundingBox\"], spatial_object=page\n            ),\n        )\n        # Setting table type based on the entity types present in the table\n        if TABLE_STRUCTURED in (val.get(\"EntityTypes\", []) or []):\n            tables[val[\"Id\"]].table_type = TableTypes.STRUCTURED\n        elif TABLE_SEMI_STRUCTURED in (val.get(\"EntityTypes\", []) or []):\n            tables[val[\"Id\"]].table_type = TableTypes.SEMI_STRUCTURED\n        else:\n            tables[val[\"Id\"]].table_type = TableTypes.UNKNOWN\n        # Setting raw JSON in the resulting object\n        tables[val[\"Id\"]].raw_object = val\n\n    # Create Table Cells\n    table_cells, all_table_cells_info = _create_table_cell_objects(\n        page_tables, id_entity_map, id_json_map, page\n    )\n\n    merged_table_cells = [\n        id_json_map[merge_id] for merge_id in entity_id_map[MERGED_CELL]\n    ]\n\n    # Add children to cells\n    merged_child_map = {\n        merged_cell[\"Id\"]: _get_relationship_ids(merged_cell, relationship=\"CHILD\")\n        for merged_cell in merged_table_cells\n    }\n    merged_child_ids = sum([ids for ids in merged_child_map.values()], [])\n\n    table_words = []\n    added_key_values = set()\n    for cell_id, cell in all_table_cells_info.items():\n        children = _get_relationship_ids(cell, relationship=\"CHILD\")\n        # FIXME: This should be gated\n        cell_word_ids = [\n            child_id for child_id in children if (child_id in id_entity_map and id_entity_map[child_id] == WORD)\n        ]\n        # FIXME: This should be gated\n        selection_ids = [\n            child_id\n            for child_id in children\n            if child_id in id_entity_map and id_entity_map[child_id] == SELECTION_ELEMENT\n        ]\n\n        cell_words = _create_word_objects(\n            cell_word_ids, id_json_map, existing_words, page\n        )\n        for w in cell_words:\n            w.cell_id = table_cells[cell_id].id\n            w.cell_bbox = table_cells[cell_id].bbox\n            w.row_span = table_cells[cell_id].row_span\n            w.col_span = table_cells[cell_id].col_span\n            w.row_index = table_cells[cell_id].row_index\n            w.col_index = table_cells[cell_id].col_index\n        table_words.extend(cell_words)\n\n        table_cells[cell_id].add_children(cell_words)\n        \n        # There are two types of selection elements, one comes from Tables, the other for KVs\n        # This tries to reconcile both and to insert the selection element in the right place\n        for child_id in selection_ids:\n            if checkboxes[child_id].key_id in added_key_values:\n                continue\n            # This is a KeyValue\n            if checkboxes[child_id].key_id is not None:\n                kv = key_values[checkboxes[child_id].key_id]\n                try:\n                    if not kv.words:\n                        added_key_values.add(kv.id)\n                        continue\n                    i = table_cells[cell_id]._children.index(kv.words[0])\n                    table_cells[cell_id]._children.insert(i, kv)\n                    for w in kv.words:\n                        try:\n                            table_cells[cell_id]._children.remove(w)\n                        except ValueError:\n                            continue\n                    added_key_values.add(checkboxes[child_id].key_id)\n                except ValueError:\n                    # Word is not in the table cells words\n                    continue\n            # This is just a checkbox\n            else:\n                table_cells[cell_id]._children.append(checkboxes[child_id])\n\n        # update metadata\n        meta_info = cell.get(\"EntityTypes\", []) or []\n        merged_info = [MERGED_CELL] if cell_id in merged_child_ids else []\n        table_cells[cell_id]._update_response_metadata(meta_info + merged_info)\n\n    # This is problematic because a KV will usually not be within a single cell, but instead\n    # cover multiple cells (usually one for key, one for value)\n    for kv_id, kv in key_values.items():\n        if kv_id in added_key_values:\n            continue\n        for table in page_tables:\n            table = tables[table[\"Id\"]]\n            # If the kv is in the table bbox, we just drop it entirely\n            if all([w in table_words for w in kv.words]):\n                added_key_values.add(kv_id)\n\n    # optimize code\n    for merge_id, child_cells in merged_child_map.items():\n        for child_id in child_cells:\n            if child_id in table_cells.keys():\n                table_cells[child_id].parent_cell_id = merge_id\n                # FIXME: This should be gated\n                table_cells[child_id].siblings = [\n                    table_cells[cid] for cid in child_cells if cid in table_cells\n                ]  # CHECK IF IDS ARE BETTER THAN INSTANCES\n\n    # Create table title (if exists)\n    for table in page_tables:\n        children = _get_relationship_ids(table, relationship=\"TABLE_TITLE\")\n        for child_id in children:\n            if child_id not in id_json_map:\n                continue\n            tables[table[\"Id\"]].title = TableTitle(\n                entity_id=child_id,\n                bbox=BoundingBox.from_normalized_dict(\n                    id_json_map[child_id][\"Geometry\"][\"BoundingBox\"],\n                    spatial_object=page,\n                ),\n            )\n            children = _get_relationship_ids(\n                id_json_map[child_id], relationship=\"CHILD\"\n            )\n            tables[table[\"Id\"]].title.words = _create_word_objects(\n                # FIXME: This should be gated\n                [child_id for child_id in children if (child_id in id_entity_map and id_entity_map[child_id] == WORD)],\n                id_json_map,\n                existing_words,\n                page,\n            )\n\n    # Create table footer (if exists)\n    for table in page_tables:\n        children = _get_relationship_ids(table, relationship=\"TABLE_FOOTER\")\n        for child_id in children:\n            if child_id not in id_json_map:\n                continue\n            tables[table[\"Id\"]].footers.append(\n                TableFooter(\n                    entity_id=child_id,\n                    bbox=BoundingBox.from_normalized_dict(\n                        id_json_map[child_id][\"Geometry\"][\"BoundingBox\"],\n                        spatial_object=page,\n                    ),\n                )\n            )\n            children = _get_relationship_ids(\n                id_json_map[child_id], relationship=\"CHILD\"\n            )\n            tables[table[\"Id\"]].footers[-1].words = _create_word_objects(\n                [child_id for child_id in children if child_id in id_entity_map and id_entity_map[child_id] == WORD],\n                id_json_map,\n                existing_words,\n                page,\n            )\n\n    # Associate Children with Tables\n    for table in page_tables:\n        children = _get_relationship_ids(table, relationship=\"CHILD\")\n        children_cells = []\n        for child_id in children:\n            # FIXME: This should be gated\n            if child_id not in table_cells:\n                continue\n            children_cells.append(table_cells[child_id])\n            if table_cells[child_id].is_title and tables[table[\"Id\"]].title is not None:\n                tables[table[\"Id\"]].title.is_floating = False\n        # FIXME: This will be slow and there should be a better way to do it.\n        words = set()\n        for child_id in children:\n            if child_id not in table_cells:\n                continue\n            for w in table_cells[child_id].words:\n                words.add(w.id)\n        for footer in tables[table[\"Id\"]].footers:\n            for w in footer.words:\n                if w.id in words:\n                    footer.is_floating = False\n                    break\n\n        tables[table[\"Id\"]].add_cells(children_cells)\n        tables[table[\"Id\"]].add_children(children_cells)\n\n    # Assign tables to layout elements\n    table_added = set()\n    for layout in sorted(page.leaf_layouts, key=lambda x: x.bbox.y):\n        if layout.layout_type == LAYOUT_TABLE:\n            for table in sorted(list(tables.values()), key=lambda x: x.bbox.y):\n                if layout.bbox.get_intersection(table.bbox).area > THRESHOLD*table.bbox.area and table not in table_added:\n                    for w in table.words:\n                        layout.remove(w)\n                    layout.children.append(table)\n                    layout.bbox = BoundingBox.enclosing_bbox(layout.children)\n                    table_added.add(table)\n\n    tables_layout = []\n    for table in tables.values():\n        if table not in table_added:\n            table_added.add(table)\n            layout = Layout(\n                entity_id=str(uuid.uuid4()),\n                bbox=table.bbox,\n                label=LAYOUT_TABLE,\n                reading_order=-1,\n            )\n            layout.children.append(table)\n            layout.page = page.page_num\n            layout.page_id = page.id\n            tables_layout.append(layout)\n\n    layouts_to_remove = []\n    for layout in page.leaf_layouts:\n        layouts_that_intersect = []\n        for table_layout in tables_layout:\n            intersection = layout.bbox.get_intersection(table_layout.bbox).area\n            if intersection:\n                layouts_that_intersect.append(table_layout)\n        words_in_sub_layouts = set()\n        for i, intersect_layout in enumerate(\n            sorted(layouts_that_intersect, key=lambda l: (l.bbox.y, l.bbox.x))\n        ):\n            intersect_layout.reading_order = (\n                (layout.reading_order + (i + 1) * 0.1)\n                if intersect_layout.reading_order == -1\n                else min(\n                    intersect_layout.reading_order, layout.reading_order + (i + 1) * 0.1\n                )\n            )\n            for w in intersect_layout.children[0].words:\n                words_in_sub_layouts.add(w)\n        if words_in_sub_layouts:\n            for word in words_in_sub_layouts:\n                layout.remove(word)\n            if layout._children:\n                layout.bbox = BoundingBox.enclosing_bbox(layout._children)\n            else:\n                layouts_to_remove.append(layout)\n\n    for layout in layouts_to_remove:\n        page.leaf_layouts.remove(layout)\n\n    for layout in tables_layout:\n        page.leaf_layouts.append(layout)\n\n    tables = list(tables.values())\n    for table in tables:\n        table.page = page.page_num\n        table.page_id = page.id\n    return tables, table_words, added_key_values\n\n\ndef parse_document_api_response(response: dict) -> Document:\n    \"\"\"\n    Parses Textract JSON response and converts them into Document object containing Page objects.\n    A valid Page object must contain at least a unique name and physical dimensions.\n\n    :param response: JSON response data in a format readable by the ResponseParser\n    :type response: dict\n\n    :return: Document object containing the hierarchy of DocumentEntity descendants.\n    :rtype: Document\n    \"\"\"\n    document = _create_document_object(response)\n\n    id_entity_map, id_json_map, entity_id_map, existing_words = (\n        {},\n        {},\n        defaultdict(list),\n        {},\n    )\n    # Create de entity id map for faster lookup\n    for block in response[\"Blocks\"]:\n        id_entity_map[block[\"Id\"]] = block[\"BlockType\"]\n        id_json_map[block[\"Id\"]] = block\n        if block[\"BlockType\"].startswith(\"LAYOUT\"):\n            entity_id_map[\"LAYOUT\"].append(block[\"Id\"])\n        else:\n            entity_id_map[block[\"BlockType\"]].append(block[\"Id\"])\n\n    # Create the empty pages\n    pages, page_elements = _create_page_objects(response)\n    assert len(pages) == response[\"DocumentMetadata\"][\"Pages\"]\n\n    # Fill the page with the detected entities\n    for page_json in page_elements:\n        page = pages[page_json[\"Id\"]]\n\n        # Creating lines\n        lines, line_words = _create_line_objects(\n            entity_id_map[LINE], id_json_map, existing_words, page\n        )\n        page.lines = deepcopy(lines)\n\n        line_by_id = {l.id: l for l in lines}\n\n        # Creating layouts\n        container_layouts, leaf_layouts = _create_layout_objects(\n            entity_id_map[LAYOUT],\n            id_json_map,\n            entity_id_map,\n            line_by_id,\n            page,\n        )\n\n        # If no layouts were created, we create fake layouts\n        if not container_layouts and not leaf_layouts:\n            # We are in a scenario where the LAYOUT API was not called. We will fake wrap\n            # all the lines to get a good linearized output regardless.\n            for i, line in enumerate(lines):\n                layout = Layout(\n                    entity_id=line.id,\n                    bbox=line.bbox,\n                    label=LAYOUT_ENTITY,\n                    reading_order=i,\n                )\n                layout._children = [line]\n                layout.page = page.page_num\n                layout.page_id = page.id\n                leaf_layouts.append(layout)\n\n        page._container_layouts.extend(container_layouts)\n        page._leaf_layouts.extend(leaf_layouts)\n\n        # Create key value objects\n        key_values, kv_words, selection_elements = _create_keyvalue_objects(\n            entity_id_map[KEY_VALUE_SET],\n            id_json_map,\n            id_entity_map,\n            entity_id_map,\n            existing_words,\n            page,\n        )\n        kvs = [kv for kv in key_values if not kv.contains_checkbox]\n        checkboxes = [kv for kv in key_values if kv.contains_checkbox]\n\n        # For backward compatibility reason we split kvs and checkboxes under two attributes\n        page.key_values = kvs\n        page.checkboxes = checkboxes\n\n        for checkbox in checkboxes:\n            id_entity_map[checkbox.id] = SELECTION_ELEMENT\n\n        # Create the table objects\n        tables, table_words, kv_added = _create_table_objects(\n            entity_id_map[TABLE],\n            id_json_map,\n            id_entity_map,\n            entity_id_map,\n            existing_words,\n            # We pass the KeyValue objects because there will be overlap and we need to make\n            # sure that the layout children are unique.\n            {kv.id:kv for kv in key_values},\n            selection_elements,\n            page,\n        )\n        page.tables = tables\n\n        # Using the kv_added returned by _create_table_objects, we try to match the remaining KVs\n        # to existing layout elements.\n        for layout in sorted(page.leaf_layouts, key=lambda x: x.bbox.y):\n            if layout.layout_type == LAYOUT_ENTITY:\n                continue\n            for kv in sorted(key_values, key=lambda x: x.bbox.y):\n                if (\n                    layout.bbox.get_intersection(kv.bbox).area > THRESHOLD * kv.bbox.area\n                    and kv.id not in kv_added\n                ):\n                    # Ignore if the KV is already overlapping with a table\n                    if any([w.cell_id for w in kv.words]):\n                        kv_added.add(kv.id)\n                        continue\n                    # Removing the duplicate words\n                    for w in kv.words:\n                        layout.remove(w)\n                    # Adding the KV to the layout children (order is not relevant)\n                    layout.children.append(kv)\n                    kv_added.add(kv.id)\n                    key_values.remove(kv)\n\n        \n        page.leaf_layouts = [l for l in page.leaf_layouts if l.children or l.layout_type == LAYOUT_FIGURE]\n\n        # We create layout elements for the KeyValues that did not match to a layout element in the\n        # previous step\n        kv_layouts = []\n        for kv in key_values:\n            if kv.id not in kv_added:\n                kv_added.add(kv.id)\n                layout = Layout(\n                    entity_id=str(uuid.uuid4()),\n                    bbox=kv.bbox,\n                    label=LAYOUT_KEY_VALUE,\n                    reading_order=-1,\n                )\n                layout.children.append(kv)\n                layout.page = page.page_num\n                layout.page_id = page.id\n                kv_layouts.append(layout)\n\n        # We update the existing layout elements to avoid overlap, this should only happen to\n        # a few KV layouts as the previous step will have caught most overlap.\n        layouts_to_remove = []\n        kv_layouts_to_ignore = []\n        layouts_that_intersect = defaultdict(list)\n        for layout in page.leaf_layouts:\n            for kv_layout in kv_layouts:\n                intersection = layout.bbox.get_intersection(kv_layout.bbox).area\n                if intersection:\n                    layouts_that_intersect[layout].append(kv_layout)\n        for layout, intersections in layouts_that_intersect.items():\n            words_in_sub_layouts = set()\n            for i, intersect_layout in enumerate(\n                sorted(intersections, key=lambda l: (l.bbox.y, l.bbox.x))\n            ):\n                # If a new KV layout intersected with more than one layout, we ignore it\n                if sum([intersect_layout in intsects for intsects in layouts_that_intersect.values()]) > 1:\n                    kv_layouts_to_ignore.append(intersect_layout)\n                    continue\n                # We assign a slightly higher reading order to the intersected layout\n                intersect_layout.reading_order = (\n                    (layout.reading_order + (i + 1) * 0.1)\n                    if intersect_layout.reading_order == -1\n                    else min(\n                        intersect_layout.reading_order, layout.reading_order + (i + 1) * 0.1\n                    )\n                )\n                # We take only the first child as the intersected layout will only have the KV as\n                # its child. \n                for w in intersect_layout.children[0].words:\n                    words_in_sub_layouts.add(w)\n            for word in words_in_sub_layouts:\n                layout.remove(word)\n            if not layout.children and layout.layout_type != LAYOUT_FIGURE:\n                layouts_to_remove.append(layout)\n\n        # Clean up layouts that became empty due to the previous step. \n        for layout in layouts_to_remove:\n            page.leaf_layouts.remove(layout)\n\n        # Add the new KV layouts to the page\n        for layout in kv_layouts:\n            if layout not in kv_layouts_to_ignore:\n                page.leaf_layouts.append(layout)\n\n        # Set the page word, create lines for orphaned words\n        all_words = table_words + kv_words + line_words\n        for word in all_words:\n            if word.line is None:\n                line = Line(\n                    str(uuid.uuid4()),\n                    word.bbox,\n                    words=[word],\n                    confidence=word.confidence,\n                )\n                line.page = page.page_num\n                line.page_id = page.id\n                word.line = line\n                page.lines.append(line)\n        all_words = {word.id: word for word in all_words}\n\n        page.words = list(all_words.values())\n\n        # Create query objects\n        queries = _create_query_objects(\n            entity_id_map[QUERY], id_json_map, entity_id_map, page\n        )\n        page.queries = queries\n\n        # Create signature objects\n        signatures = _create_signature_objects(\n            entity_id_map[SIGNATURE], id_json_map, entity_id_map, page\n        )\n        page.signatures = signatures\n\n        # Final clean up of the layout objects\n        word_set = set()\n        for layout in sorted(page.layouts, key=lambda l: l.reading_order):\n            layout.visit(word_set)\n            if not layout.children and layout.layout_type != LAYOUT_FIGURE:\n                try:\n                    page.leaf_layouts.remove(layout)\n                except:\n                    page.container_layouts.remove(layout)\n\n    document.pages = sorted(list(pages.values()), key=lambda x: x.page_num)\n    document.response = response\n    return document\n\ndef parse_analyze_id_response(response):\n    id_documents = []\n    response[\"Blocks\"] = []\n    for doc in response[\"IdentityDocuments\"]:\n        fields = {}\n        for field in doc[\"IdentityDocumentFields\"]:\n            fields[field[\"Type\"][\"Text\"]] = {\n                \"key\": field[\"Type\"][\"Text\"],\n                \"value\": field[\"ValueDetection\"][\"Text\"],\n                \"confidence\": field[\"ValueDetection\"][\"Confidence\"],\n            }\n        id_documents.append(IdentityDocument(fields))\n        id_documents[-1].raw_object = doc\n        response[\"Blocks\"].extend(doc.get(\"Blocks\", []))\n    # FIXME: Quick fix, we need something more robust\n    document = parse_document_api_response(response)\n    del response[\"Blocks\"]\n    document.identity_documents = id_documents\n    document.response = response\n    return document\n\n\ndef create_expense_from_field(field: Dict, page: Page) -> ExpenseField:\n    if \"Type\" in field:\n        type_expense = ExpenseType(\n            field[\"Type\"][\"Text\"], field[\"Type\"][\"Confidence\"], field[\"Type\"]\n        )\n    else:\n        type_expense = None\n    if \"ValueDetection\" in field:\n        value_expense = Expense(\n            bbox=(\n                None\n                if not \"Geometry\" in field[\"ValueDetection\"]\n                else BoundingBox.from_normalized_dict(\n                    field[\"ValueDetection\"][\"Geometry\"][\"BoundingBox\"],\n                    spatial_object=page,\n                )\n            ),\n            text=field[\"ValueDetection\"][\"Text\"],\n            confidence=field[\"ValueDetection\"][\"Confidence\"],\n            page=page.page_num,\n        )\n        value_expense.raw_object = field[\"ValueDetection\"]\n    else:\n        value_expense = None\n    if \"LabelDetection\" in field:\n        label_expense = Expense(\n            bbox=BoundingBox.from_normalized_dict(\n                field[\"LabelDetection\"][\"Geometry\"][\"BoundingBox\"], spatial_object=page\n            ),\n            text=field[\"LabelDetection\"][\"Text\"],\n            confidence=field[\"LabelDetection\"][\"Confidence\"],\n            page=page.page_num,\n        )\n        label_expense.raw_object = field[\"LabelDetection\"]\n    else:\n        label_expense = None\n    group_properties = []\n    if \"GroupProperties\" in field:\n        for group_property in field[\"GroupProperties\"]:\n            group_properties.append(\n                ExpenseGroupProperty(\n                    id=group_property[\"Id\"], types=group_property[\"Types\"]\n                )\n            )\n    if \"Currency\" in field:\n        currency = field[\"Currency\"][\"Code\"]\n    else:\n        currency = None\n    return ExpenseField(\n        type_expense,\n        value_expense,\n        group_properties=group_properties,\n        label=label_expense,\n        currency=currency,\n        page=page.page_num,\n    )\n\n\ndef parser_analyze_expense_response(response):\n    response[\"Blocks\"] = [\n        b for doc in response[\"ExpenseDocuments\"] for b in doc.get(\"Blocks\", [])\n    ]\n    document = parse_document_api_response(response)\n    for doc in response[\"ExpenseDocuments\"]:\n        page_number = None\n        if len(doc[\"SummaryFields\"]):\n            page_number = doc[\"SummaryFields\"][0].get(\"PageNumber\")\n        elif len(doc[\"LineItemGroups\"]):\n            # A group must have at least one LineItem, and a LI must have at least one field:\n            first_field = doc[\"LineItemGroups\"][0][\"LineItems\"][0][\"LineItemExpenseFields\"][0]\n            page_number = first_field.get(\"PageNumber\")\n        if page_number is None:\n            logging.warning(\n                \"Skipping parsing ExpenseDocument %s as its page number could not be determined\"\n                % (doc[\"ExpenseIndex\"],)\n            )\n            continue\n\n        page = document.pages[page_number - 1]\n        summary_fields = []\n        for summary_field in doc[\"SummaryFields\"]:\n            summary_fields.append(create_expense_from_field(summary_field, page))\n            summary_fields[-1].raw_object = summary_field\n\n        line_items_groups = []\n        for line_items_group in doc[\"LineItemGroups\"]:\n            line_item_rows = []\n            for i, line_item in enumerate(line_items_group[\"LineItems\"]):\n                row_expenses = []\n                for line_item_field in line_item[\"LineItemExpenseFields\"]:\n                    row_expenses.append(\n                        create_expense_from_field(line_item_field, page)\n                    )\n                    row_expenses[-1].raw_object = line_item_field\n                line_item_rows.append(\n                    LineItemRow(\n                        index=i,\n                        line_item_expense_fields=row_expenses,\n                        page=page.page_num,\n                    )\n                )\n            if not line_item_rows:\n                continue\n            line_items_groups.append(\n                LineItemGroup(\n                    index=line_items_group[\"LineItemGroupIndex\"],\n                    line_item_rows=line_item_rows,\n                    page=page.page_num,\n                )\n            )\n\n        bbox = BoundingBox.enclosing_bbox(\n            bboxes=[s.bbox for s in summary_fields]\n            + [g.bbox for g in line_items_groups],\n            spatial_object=page,\n        )\n        expense_document = ExpenseDocument(\n            summary_fields=summary_fields,\n            line_items_groups=line_items_groups,\n            bounding_box=bbox,\n            page=page.page_num,\n        )\n        expense_document.raw_object = doc\n        document.pages[page_number - 1].expense_documents.append(\n            expense_document\n        )\n    document.response = response\n    return document\n\ndef parse(response: dict) -> Document:\n    \"\"\"\n    Ingests response data and API Call Mode and calls the appropriate function for it.\n    Presently supports only SYNC and ASYNC API calls. Will be extended to Analyze ID and Expense in the future.\n\n    :param response: JSON response data in a format readable by the ResponseParser.\n    :type response: dict\n\n    :return: Document object returned after making respective parse function calls.\n    :rtype: Document\n    \"\"\"\n    if \"IdentityDocuments\" in response:\n        return parse_analyze_id_response(response)\n    if \"ExpenseDocuments\" in response:\n        return parser_analyze_expense_response(response)\n    else:\n        return parse_document_api_response(converter(response))\n",
    "textractor/entities/page.py": "\"\"\"\nRepresents a single :class:`Document` page, as it would appear in the Textract API output.\nThe :class:`Page` object also contains the metadata such as the physical dimensions of the page (width, height, in pixels), child_ids etc.\n\"\"\"\n\nimport os\nimport string\nimport logging\nimport xlsxwriter\nfrom typing import List, Tuple\nfrom copy import deepcopy\nfrom collections import defaultdict\nfrom textractor.entities.expense_document import ExpenseDocument\n\nfrom textractor.entities.word import Word\nfrom textractor.entities.line import Line\nfrom textractor.entities.table import Table\nfrom textractor.entities.signature import Signature\nfrom textractor.entities.layout import Layout\nfrom textractor.entities.page_layout import PageLayout\nfrom textractor.exceptions import InputError\nfrom textractor.entities.key_value import KeyValue\nfrom textractor.entities.query import Query\nfrom textractor.entities.identity_document import IdentityDocument\nfrom textractor.entities.bbox import SpatialObject\nfrom textractor.data.constants import SelectionStatus, Direction, DirectionalFinderType\nfrom textractor.data.constants import TextTypes, SimilarityMetric\nfrom textractor.data.constants import (\n    LAYOUT_TEXT,\n    LAYOUT_TITLE,\n    LAYOUT_HEADER,\n    LAYOUT_FOOTER,\n    LAYOUT_SECTION_HEADER,\n    LAYOUT_PAGE_NUMBER,\n    LAYOUT_LIST,\n    LAYOUT_FIGURE,\n    LAYOUT_TABLE,\n    LAYOUT_KEY_VALUE,\n)\nfrom textractor.data.text_linearization_config import TextLinearizationConfig\nfrom textractor.entities.selection_element import SelectionElement\nfrom textractor.utils.geometry_util import sort_by_position\nfrom textractor.utils.search_utils import SearchUtils, jaccard_similarity\nfrom textractor.visualizers.entitylist import EntityList\nfrom textractor.entities.linearizable import Linearizable\n\n\nclass Page(SpatialObject, Linearizable):\n    \"\"\"\n    Creates a new document, ideally representing a single item in the dataset.\n\n    :param id: Unique id of the Page\n    :type id: str\n    :param width: Width of page, in pixels\n    :type width: float\n    :param height: Height of page, in pixels\n    :type height: float\n    :param page_num: Page number in the document linked to this Page object\n    :type page_num: int\n    :param child_ids: IDs of child entities in the Page as determined by Textract\n    :type child_ids: List\n    \"\"\"\n\n    def __init__(\n        self, id: str, width: int, height: int, page_num: int = -1, child_ids=None\n    ):\n        super().__init__(width=width, height=height)\n        self.id = id\n        self._words: EntityList[Word] = EntityList([])\n        self._lines: EntityList[Line] = EntityList([])\n        self._key_values: EntityList[KeyValue] = EntityList([])\n        self._checkboxes: EntityList[KeyValue] = EntityList([])\n        self._tables: EntityList[Table] = EntityList([])\n        self._queries: EntityList[Query] = EntityList([])\n        self._expense_documents: EntityList[ExpenseDocument] = EntityList([])\n        self._leaf_layouts: EntityList[Layout] = EntityList([])\n        self._container_layouts: EntityList[Layout] = EntityList([])\n        self.kv_cache = defaultdict(list)\n        self.metadata = {}\n        self.page_num = page_num\n        self.child_ids: List[str] = child_ids\n        self.image = None\n\n    # functions to add entities to the Page object\n    @property\n    def words(self) -> EntityList[Word]:\n        \"\"\"\n        Returns all the :class:`Word` objects present in the Page.\n\n        :return: List of Word objects, each representing a word within the Page.\n        :rtype: EntityList[Word]\n        \"\"\"\n        return self._words\n\n    @words.setter\n    def words(self, words: List[Word]):\n        \"\"\"\n        Add Word objects to the Page.\n\n        :param words: List of Word objects, each representing a word within the page. No specific ordering is assumed.\n        :type words: List[Word]\n        \"\"\"\n        self._words = words\n        self._words = EntityList(sort_by_position(list(set(self._words))))\n\n    @property\n    def lines(self) -> EntityList[Line]:\n        \"\"\"\n        Returns all the :class:`Line` objects present in the Page.\n\n        :return: List of Line objects, each representing a line within the Page.\n        :rtype: EntityList[Line]\n        \"\"\"\n        return self._lines\n\n    @lines.setter\n    def lines(self, lines: List[Line]):\n        \"\"\"\n        Add Line objects to the Document.\n\n        :param lines: List of Line objects, each representing a line within the Page.\n        :type lines: List[Line]\n        \"\"\"\n        self._lines = EntityList(sort_by_position(lines))\n\n    @property\n    def text(self) -> str:\n        \"\"\"\n        Returns the page text\n\n        :return: Linearized page text\n        :rtype: str\n        \"\"\"\n        return self.get_text()\n\n    def get_text_and_words(\n        self, config: TextLinearizationConfig = TextLinearizationConfig()\n    ) -> Tuple[str, List[Word]]:\n        \"\"\"\n        Returns the page text and words sorted in reading order\n\n        :param config: Text linearization configuration object, defaults to TextLinearizationConfig()\n        :type config: TextLinearizationConfig, optional\n        :return: Tuple of page text and words\n        :rtype: Tuple[str, List[Word]]\n        \"\"\"\n        unsorted_layouts = [l for l in self.layouts if l.reading_order < 0]\n        sorted_layouts = [l for l in self.layouts if l.reading_order >= 0]\n        if unsorted_layouts:\n            for unsorted_layout in sorted(\n                unsorted_layouts, key=lambda x: (x.bbox.y, x.bbox.x)\n            ):\n                closest_layout = None\n                closest_reading_order_distance = None\n                for layout in sorted(sorted_layouts, key=lambda x: x.reading_order):\n                    dist = layout.bbox.get_distance(unsorted_layout)\n                    if (\n                        closest_reading_order_distance is None\n                        or dist < closest_reading_order_distance\n                    ):\n                        closest_layout = layout\n                if closest_layout:\n                    sorted_layouts.insert(\n                        sorted_layouts.index(closest_layout) + 1, unsorted_layout\n                    )\n                else:\n                    sorted_layouts.append(unsorted_layout)\n\n        page_texts_and_words = [l.get_text_and_words(config) for l in sorted_layouts]\n\n        if not page_texts_and_words:\n            return \"\", []\n\n        text, words = zip(*page_texts_and_words)\n        combined_words = []\n        for w in words:\n            combined_words += w\n        return config.layout_element_separator.join(text), combined_words\n\n    @property\n    def page_layout(self) -> PageLayout:\n        return PageLayout(\n            titles=EntityList(\n                [l for l in self.layouts if l.layout_type == LAYOUT_TITLE]\n            ),\n            headers=EntityList(\n                [l for l in self.layouts if l.layout_type == LAYOUT_HEADER]\n            ),\n            footers=EntityList(\n                [l for l in self.layouts if l.layout_type == LAYOUT_FOOTER]\n            ),\n            section_headers=EntityList(\n                [l for l in self.layouts if l.layout_type == LAYOUT_SECTION_HEADER]\n            ),\n            page_numbers=EntityList(\n                [l for l in self.layouts if l.layout_type == LAYOUT_PAGE_NUMBER]\n            ),\n            lists=EntityList([l for l in self.layouts if l.layout_type == LAYOUT_LIST]),\n            figures=EntityList(\n                [l for l in self.layouts if l.layout_type == LAYOUT_FIGURE]\n            ),\n            tables=EntityList(\n                [l for l in self.layouts if l.layout_type == LAYOUT_TABLE]\n            ),\n            key_values=EntityList(\n                [l for l in self.layouts if l.layout_type == LAYOUT_KEY_VALUE]\n            ),\n        )\n\n    @property\n    def key_values(self) -> EntityList[KeyValue]:\n        \"\"\"\n        Returns all the :class:`KeyValue` objects present in the Page.\n\n        :return: List of KeyValue objects, each representing a key-value pair within the Page.\n        :rtype: EntityList[KeyValue]\n        \"\"\"\n        return self._key_values\n\n    @key_values.setter\n    def key_values(self, kv: List[KeyValue]):\n        \"\"\"\n        Add KeyValue objects to the Page.\n\n        :param kv: List of KeyValue objects, each representing a KV area within the document page.\n        :type kv: List[KeyValue]\n        \"\"\"\n        self._key_values = EntityList(sort_by_position(kv))\n\n    @property\n    def checkboxes(self) -> EntityList[KeyValue]:\n        \"\"\"\n        Returns all the :class:`KeyValue` objects with :class:`SelectionElement` present in the Page.\n\n        :return: List of KeyValue objects, each representing a checkbox within the Page.\n        :rtype: EntityList[KeyValue]\n        \"\"\"\n        return self._checkboxes\n\n    @checkboxes.setter\n    def checkboxes(self, checkbox: List[KeyValue]):\n        \"\"\"\n        Add KeyValue objects containing SelectionElement children to the Page.\n\n        :param checkbox: List of KeyValue objects, each representing a checkbox area within the document page.\n        :type checkbox: List[KeyValue]\n        \"\"\"\n        self._checkboxes = EntityList(sort_by_position(checkbox))\n\n    @property\n    def tables(self) -> EntityList[Table]:\n        \"\"\"\n        Returns all the :class:`Table` objects present in the Page.\n\n        :return: List of Table objects, each representing a table within the Page.\n        :rtype: EntityList\n        \"\"\"\n        return self._tables\n\n    @tables.setter\n    def tables(self, tables: List[Table]):\n        \"\"\"\n        Add Table objects to the Page.\n\n        :param tables: List of Table objects, each representing a Table area within the document page.\n        :type tables: list\n        \"\"\"\n        self._tables = EntityList(tables)\n\n    @property\n    def queries(self) -> EntityList[Query]:\n        \"\"\"\n        Returns all the :class:`Query` objects present in the Page.\n\n        :return: List of Query objects.\n        :rtype: EntityList\n        \"\"\"\n        return self._queries\n\n    @queries.setter\n    def queries(self, queries: List[Query]):\n        \"\"\"\n        Add Signature objects to the Page.\n\n        :param signatures: List of Signature objects.\n        :type signatures: list\n        \"\"\"\n        self._queries = EntityList(queries)\n\n    @property\n    def signatures(self) -> EntityList[Signature]:\n        \"\"\"\n        Returns all the :class:`Signature` objects present in the Page.\n\n        :return: List of Signature objects.\n        :rtype: EntityList\n        \"\"\"\n        return self._signatures\n\n    @signatures.setter\n    def signatures(self, signatures: List[Signature]):\n        \"\"\"\n        Add Signature objects to the Page.\n\n        :param signatures: List of Signature objects.\n        :type signatures: list\n        \"\"\"\n        self._signatures = EntityList(signatures)\n\n    @property\n    def layouts(self) -> EntityList[Layout]:\n        \"\"\"\n        Returns all the :class:`Layout` objects present in the Page.\n\n        :return: List of Layout objects.\n        :rtype: EntityList\n        \"\"\"\n        return EntityList(\n            sorted(\n                self._leaf_layouts + self._container_layouts,\n                key=lambda c: c.reading_order,\n            )\n        )\n\n    @property\n    def leaf_layouts(self) -> EntityList[Layout]:\n        \"\"\"\n        Returns all the leaf :class:`Layout` objects present in the Page.\n\n        :return: List of Layout objects.\n        :rtype: EntityList\n        \"\"\"\n        return self._leaf_layouts\n\n    @leaf_layouts.setter\n    def leaf_layouts(self, leaf_layouts: List[Layout]):\n        \"\"\"\n        Add leaf layout objects to the Page.\n\n        :param layouts: List of Layout objects.\n        :type layouts: list\n        \"\"\"\n        self._leaf_layouts = EntityList(leaf_layouts)\n\n    @property\n    def container_layouts(self) -> EntityList[Layout]:\n        \"\"\"\n        Returns all the container :class:`Layout` objects present in the Page.\n\n        :return: List of Layout objects.\n        :rtype: EntityList\n        \"\"\"\n        return self._container_layouts\n\n    @container_layouts.setter\n    def container_layouts(self, container_layouts: List[Layout]):\n        \"\"\"\n        Add Layout objects to the Page.\n\n        :param layouts: List of Layout objects.\n        :type layouts: list\n        \"\"\"\n        self._container_layouts = EntityList(container_layouts)\n\n    @property\n    def expense_documents(self) -> EntityList[ExpenseDocument]:\n        \"\"\"\n        Returns all the :class:`ExpenseDocument` objects present in the Page.\n\n        :return: List of ExpenseDocument objects.\n        :rtype: EntityList\n        \"\"\"\n        return self._expense_documents\n\n    @expense_documents.setter\n    def expense_documents(self, expense_documents: List[ExpenseDocument]):\n        \"\"\"\n        Add ExpenseDocument objects to the Page.\n\n        :param tables: List of ExpenseDocument objects.\n        :type expense_documents: list\n        \"\"\"\n        self._expense_documents = EntityList(expense_documents)\n\n    def __repr__(self):\n        return os.linesep.join(\n            [\n                f\"This Page ({self.page_num}) holds the following data:\",\n                f\"Words - {len(self.words)}\",\n                f\"Lines - {len(self.lines)}\",\n                f\"Key-values - {len(self.key_values)}\",\n                f\"Checkboxes - {len(self.checkboxes)}\",\n                f\"Tables - {len(self.tables)}\",\n                f\"Queries - {len(self.queries)}\",\n                f\"Signatures - {len(self.signatures)}\",\n                f\"Expense documents - {len(self.expense_documents)}\",\n                f\"Layout elements - {len(self.layouts)}\",\n            ]\n        )\n\n    def __getitem__(self, key):\n        output = self.get(key)\n        if output:\n            return output\n        raise KeyError(f\"{key} was not found in Document\")\n\n    def keys(self, include_checkboxes: bool = True) -> List[str]:\n        \"\"\"\n        Prints all keys for key-value pairs and checkboxes if the page contains them.\n\n        :param include_checkboxes: True/False. Set False if checkboxes need to be excluded.\n        :type include_checkboxes: bool\n\n        :return: List of strings containing key names in the Page\n        :rtype: List[str]\n        \"\"\"\n        keys = []\n        keys = [keyvalue.key for keyvalue in self.key_values]\n        if include_checkboxes:\n            keys += [keyvalue.key for keyvalue in self.checkboxes]\n        return keys\n\n    def filter_checkboxes(\n        self, selected: bool = True, not_selected: bool = True\n    ) -> EntityList[KeyValue]:\n        \"\"\"\n        Return a list of :class:`KeyValue` objects containing checkboxes if the page contains them.\n\n        :param selected: True/False Return SELECTED checkboxes\n        :type selected: bool\n        :param not_selected: True/False Return NOT_SELECTED checkboxes\n        :type not_selected: bool\n\n        :return: Returns checkboxes that match the conditions set by the flags.\n        :rtype: EntityList[KeyValue]\n        \"\"\"\n        if not self.checkboxes:\n            logging.warning(f\"This document does not contain checkboxes\")\n            return []\n        else:\n            if selected and not_selected:\n                checkboxes = self.checkboxes\n                return EntityList(checkboxes)\n\n            checkboxes = []\n            if selected:\n                checkboxes = [\n                    kv\n                    for kv in self.checkboxes\n                    if kv.selection_status == SelectionStatus.SELECTED\n                ]\n            if not_selected:\n                checkboxes = [\n                    kv\n                    for kv in self.checkboxes\n                    if kv.selection_status == SelectionStatus.NOT_SELECTED\n                ]\n\n            return EntityList(checkboxes)\n\n    def get_words_by_type(\n        self, text_type: TextTypes = TextTypes.PRINTED\n    ) -> EntityList[Word]:\n        \"\"\"\n        Returns list of :class:`Word` entities that match the input text type.\n\n        :param text_type: TextTypes.PRINTED or TextTypes.HANDWRITING\n        :type text_type: TextTypes\n\n        :return: Returns list of Word entities that match the input text type.\n        :rtype: EntityList[Word]\n        \"\"\"\n        if not isinstance(text_type, TextTypes):\n            raise InputError(\n                \"text_type parameter should be of TextTypes type. Find input choices from textractor.data.constants\"\n            )\n\n        if not self.words:\n            logging.warn(\"Document contains no word entities.\")\n            return []\n\n        filtered_words = [word for word in self.words if word.text_type == text_type]\n        return EntityList(filtered_words)\n\n    def _search_words_with_similarity(\n        self,\n        keyword: str,\n        top_k: int = 1,\n        similarity_metric: SimilarityMetric = SimilarityMetric.LEVENSHTEIN,\n        similarity_threshold: float = 0.6,\n    ) -> List[Tuple[Word, float]]:\n        \"\"\"\n        Returns a list of top_k words with their similarity to the keyword.\n\n        :param keyword: Keyword that is used to query the document.\n        :type keyword: str, required\n        :param top_k: Number of closest word objects to be returned. default=1\n        :type top_k: int, optional\n        :param similarity_metric: SimilarityMetric.COSINE, SimilarityMetric.EUCLIDEAN or SimilarityMetric.LEVENSHTEIN. SimilarityMetric.COSINE is chosen as default.\n        :type similarity_metric: SimilarityMetric\n        :param similarity_threshold: Measure of how similar document key is to queried key. default=0.6\n        :type similarity_threshold: float\n\n        :return: Returns a list of tuples containing similarity and Word.\n        :rtype: List[Tuple(float, Word)]]\n        \"\"\"\n        if not isinstance(similarity_metric, SimilarityMetric):\n            raise InputError(\n                \"similarity_metric parameter should be of SimilarityMetric type. Find input choices from textractor.data.constants\"\n            )\n        top_n_words = []\n        similarity_threshold = (\n            similarity_threshold\n            if similarity_metric == SimilarityMetric.COSINE\n            else -(similarity_threshold)\n        )\n        lowest_similarity = similarity_threshold\n\n        for word in self.words:\n            similarity = SearchUtils.get_word_similarity(\n                keyword, word.text, similarity_metric\n            )\n            similarity = (\n                similarity\n                if similarity_metric == SimilarityMetric.COSINE\n                else -(similarity)\n            )\n\n            if len(top_n_words) < top_k and similarity > similarity_threshold:\n                top_n_words.append((similarity, word))\n            elif similarity > lowest_similarity:\n                top_n_words[-1] = (similarity, word)\n            else:\n                continue\n            top_n_words = sorted(top_n_words, key=lambda x: x[0], reverse=True)\n            lowest_similarity = top_n_words[-1][0]\n\n        return top_n_words\n\n    def search_words(\n        self,\n        keyword: str,\n        top_k: int = 1,\n        similarity_metric: SimilarityMetric = SimilarityMetric.LEVENSHTEIN,\n        similarity_threshold: float = 0.6,\n    ) -> EntityList[Word]:\n        \"\"\"\n        Return a list of top_k words that match the keyword.\n\n        :param keyword: Keyword that is used to query the document.\n        :type keyword: str, required\n        :param top_k: Number of closest word objects to be returned. default=1\n        :type top_k: int, optional\n        :param similarity_metric: SimilarityMetric.COSINE, SimilarityMetric.EUCLIDEAN or SimilarityMetric.LEVENSHTEIN. SimilarityMetric.COSINE is chosen as default.\n        :type similarity_metric: SimilarityMetric\n        :param similarity_threshold: Measure of how similar document key is to queried key. default=0.6\n        :type similarity_threshold: float\n\n        :return: Returns a list of words that match the queried key sorted from highest to lowest similarity.\n        :rtype: EntityList[Word]\n        \"\"\"\n\n        top_n_words = EntityList(\n            [\n                ent[1]\n                for ent in self._search_words_with_similarity(\n                    keyword=keyword,\n                    top_k=top_k,\n                    similarity_metric=similarity_metric,\n                    similarity_threshold=similarity_threshold,\n                )\n            ]\n        )\n\n        return top_n_words\n\n    def _search_lines_with_similarity(\n        self,\n        keyword: str,\n        top_k: int = 1,\n        similarity_metric: SimilarityMetric = SimilarityMetric.LEVENSHTEIN,\n        similarity_threshold: int = 0.6,\n    ) -> List[Tuple[Line, float]]:\n        \"\"\"\n        Return a list of top_k lines that contain the queried keyword.\n\n        :param keyword: Keyword that is used to query the page.\n        :type keyword: str\n        :param top_k: Number of closest line objects to be returned\n        :type top_k: int\n        :param similarity_metric: SimilarityMetric.COSINE, SimilarityMetric.EUCLIDEAN or SimilarityMetric.LEVENSHTEIN. SimilarityMetric.COSINE is chosen as default.\n        :type similarity_metric: SimilarityMetric\n        :param similarity_threshold: Measure of how similar page key is to queried key. default=0.6\n        :type similarity_threshold: float\n\n        :return: Returns a list of tuples of lines and their similarity to the keyword that contain the queried key sorted\n                 from highest to lowest similarity.\n        :rtype: List[Tuple[Line, float]]\n        \"\"\"\n        if not isinstance(similarity_metric, SimilarityMetric):\n            raise InputError(\n                \"similarity_metric parameter should be of SimilarityMetric type. Find input choices from textractor.data.constants\"\n            )\n\n        top_n_lines = []\n        similarity_threshold = (\n            similarity_threshold\n            if similarity_metric == SimilarityMetric.COSINE\n            else -(similarity_threshold)\n        )\n        lowest_similarity = similarity_threshold\n\n        for line in self.lines:\n            similarity = [\n                SearchUtils.get_word_similarity(keyword, word, similarity_metric)\n                for word in line.__repr__().split(\" \")\n            ]\n            similarity.append(\n                SearchUtils.get_word_similarity(\n                    keyword, line.__repr__(), similarity_metric\n                )\n            )\n            similarity = (\n                max(similarity)\n                if similarity_metric == SimilarityMetric.COSINE\n                else -min(similarity)\n            )\n\n            if len(top_n_lines) < top_k and similarity > similarity_threshold:\n                top_n_lines.append((similarity, line))\n            elif similarity > lowest_similarity:\n                top_n_lines[-1] = (similarity, line)\n            else:\n                continue\n            top_n_lines = sorted(top_n_lines, key=lambda x: x[0], reverse=True)\n            lowest_similarity = top_n_lines[-1][0]\n\n        return top_n_lines\n\n    def search_lines(\n        self,\n        keyword: str,\n        top_k: int = 1,\n        similarity_metric: SimilarityMetric = SimilarityMetric.LEVENSHTEIN,\n        similarity_threshold: int = 0.6,\n    ) -> EntityList[Line]:\n        \"\"\"\n        Return a list of top_k lines that contain the queried keyword.\n\n        :param keyword: Keyword that is used to query the page.\n        :type keyword: str\n        :param top_k: Number of closest line objects to be returned\n        :type top_k: int\n        :param similarity_metric: SimilarityMetric.COSINE, SimilarityMetric.EUCLIDEAN or SimilarityMetric.LEVENSHTEIN. SimilarityMetric.COSINE is chosen as default.\n        :type similarity_metric: SimilarityMetric\n        :param similarity_threshold: Measure of how similar page key is to queried key. default=0.6\n        :type similarity_threshold: float\n\n        :return: Returns a list of lines that contain the queried key sorted\n                 from highest to lowest similarity.\n        :rtype: EntityList[Line]\n        \"\"\"\n\n        top_n_lines = EntityList(\n            [\n                ent[1]\n                for ent in self._search_lines_with_similarity(\n                    keyword=keyword,\n                    top_k=top_k,\n                    similarity_metric=similarity_metric,\n                    similarity_threshold=similarity_threshold,\n                )\n            ]\n        )\n\n        return top_n_lines\n\n    # KeyValue entity related functions\n    def get(\n        self,\n        key: str,\n        top_k_matches: int = 1,\n        similarity_metric: SimilarityMetric = SimilarityMetric.LEVENSHTEIN,\n        similarity_threshold: float = 0.6,\n    ) -> EntityList[KeyValue]:\n        \"\"\"\n        Return upto top_k_matches of key-value pairs for the key that is queried from the page.\n\n        :param key: Query key to match\n        :type key: str\n        :param top_k_matches: Maximum number of matches to return\n        :type top_k_matches: int\n        :param similarity_metric: SimilarityMetric.COSINE, SimilarityMetric.EUCLIDEAN or SimilarityMetric.LEVENSHTEIN. SimilarityMetric.COSINE is chosen as default.\n        :type similarity_metric: SimilarityMetric\n        :param similarity_threshold: Measure of how similar page key is to queried key. default=0.6\n        :type similarity_threshold: float\n\n        :return: Returns a list of key-value pairs that match the queried key sorted from highest\n                 to lowest similarity.\n        :rtype: EntityList[KeyValue]\n        \"\"\"\n        if not isinstance(similarity_metric, SimilarityMetric):\n            raise InputError(\n                \"similarity_metric parameter should be of SimilarityMetric type. Find input choices from textractor.data.constants\"\n            )\n\n        top_n = []\n        similarity_threshold = (\n            similarity_threshold\n            if similarity_metric == SimilarityMetric.COSINE\n            else -(similarity_threshold)\n        )\n        lowest_similarity = similarity_threshold\n\n        for kv in self.key_values + self.checkboxes:\n            try:\n                edited_document_key = \"\".join(\n                    [\n                        char\n                        for char in kv.key.__repr__()\n                        if char not in string.punctuation\n                    ]\n                )\n            except:\n                pass\n            key = \"\".join([char for char in key if char not in string.punctuation])\n\n            similarity = [\n                SearchUtils.get_word_similarity(key, word, similarity_metric)\n                for word in edited_document_key.split(\" \")\n            ]\n            similarity.append(\n                SearchUtils.get_word_similarity(\n                    key, edited_document_key, similarity_metric\n                )\n            )\n\n            similarity = (\n                max(similarity)\n                if similarity_metric == SimilarityMetric.COSINE\n                else -min(similarity)\n            )\n\n            if similarity > similarity_threshold:\n                if len(top_n) < top_k_matches:\n                    top_n.append((kv, similarity))\n                elif similarity > lowest_similarity:\n                    top_n[-1] = (kv, similarity)\n                top_n = sorted(top_n, key=lambda x: x[1], reverse=True)\n                lowest_similarity = top_n[-1][1]\n\n        if not top_n:\n            logging.warning(\n                f\"Query key does not match any existing keys in the document.{os.linesep}{self.keys()}\"\n            )\n\n        logging.info(f\"Query key matched {len(top_n)} key-values in the document.\")\n\n        return EntityList([value[0] for value in top_n])\n\n    # Export document entities into supported formats\n    def export_kv_to_csv(\n        self,\n        include_kv: bool = True,\n        include_checkboxes: bool = True,\n        filepath: str = \"Key-Values.csv\",\n    ):\n        \"\"\"\n        Export key-value entities and checkboxes in csv format.\n\n        :param include_kv: True if KVs are to be exported. Else False.\n        :type include_kv: bool\n        :param include_checkboxes: True if checkboxes are to be exported. Else False.\n        :type include_checkboxes: bool\n        :param filepath: Path to where file is to be stored.\n        :type filepath: str\n        \"\"\"\n        keys = []\n        values = []\n        if include_kv and not self.key_values:\n            logging.warning(\"Document does not contain key-values.\")\n        elif include_kv:\n            for kv in self.key_values:\n                keys.append(kv.key.__repr__())\n                values.append(kv.value.__repr__())\n\n        if include_checkboxes and not self.checkboxes:\n            logging.warning(\"Document does not contain checkbox elements.\")\n        elif include_checkboxes:\n            for kv in self.checkboxes:\n                keys.append(kv.key.__repr__())\n                values.append(kv.value.children[0].status.name)\n\n        with open(filepath, \"w\") as f:\n            f.write(f\"Key,Value{os.linesep}\")\n            for k, v in zip(keys, values):\n                f.write(f\"{k},{v}{os.linesep}\")\n\n        logging.info(\n            f\"csv file stored at location {os.path.join(os.getcwd(), filepath)}\"\n        )\n\n    def export_kv_to_txt(\n        self,\n        include_kv: bool = True,\n        include_checkboxes: bool = True,\n        filepath: str = \"Key-Values.txt\",\n    ):\n        \"\"\"\n        Export key-value entities and checkboxes in txt format.\n\n        :param include_kv: True if KVs are to be exported. Else False.\n        :type include_kv: bool\n        :param include_checkboxes: True if checkboxes are to be exported. Else False.\n        :type include_checkboxes: bool\n        :param filepath: Path to where file is to be stored.\n        :type filepath: str\n        \"\"\"\n        export_str = []\n        index = 1\n        if include_kv and not self.key_values:\n            logging.warning(\"Document does not contain key-values.\")\n        elif include_kv:\n            for kv in self.key_values:\n                export_str.append(\n                    f\"{index}. {kv.key.__repr__()} : {kv.value.__repr__()}{os.linesep}\"\n                )\n                index += 1\n\n        if include_checkboxes and not self.checkboxes:\n            logging.warning(\"Document does not contain checkbox elements.\")\n        elif include_checkboxes:\n            for kv in self.checkboxes:\n                export_str.append(\n                    f\"{index}. {kv.key.__repr__()} : {kv.value.__repr__()}{os.linesep}\"\n                )\n                index += 1\n\n        with open(filepath, \"w\") as text_file:\n            text_file.write(\"\".join(export_str))\n        logging.info(\n            f\"txt file stored at location {os.path.join(os.getcwd(),filepath)}\"\n        )\n\n    def independent_words(self) -> EntityList[Word]:\n        \"\"\"\n        :return: Return all words in the document, outside of tables, checkboxes, key-values.\n        :rtype: EntityList[Word]\n        \"\"\"\n        if not self.words:\n            logging.warning(\"Words have not been assigned to this Document object.\")\n            return []\n\n        else:\n            table_words = sum([table.words for table in self.tables], [])\n            kv_words = sum([kv.words for kv in self.key_values], [])\n            checkbox_words = sum([kv.words for kv in self.checkboxes], [])\n            dependent_words = table_words + checkbox_words + kv_words\n            dependent_word_ids = set([word.id for word in dependent_words])\n            independent_words = [\n                word for word in self.words if word.id not in dependent_word_ids\n            ]\n            return EntityList(independent_words)\n\n    def export_tables_to_excel(self, filepath):\n        \"\"\"\n        Creates an excel file and writes each table on a separate worksheet within the workbook.\n        This is stored on the filepath passed by the user.\n\n        :param filepath: Path to store the exported Excel file.\n        :type filepath: str, required\n        \"\"\"\n        if not filepath:\n            logging.error(\"Filepath required to store excel file.\")\n        workbook = xlsxwriter.Workbook(filepath)\n        for table in self.tables:\n            workbook = table.to_excel(\n                filepath=None, workbook=workbook, save_workbook=False\n            )\n        workbook.close()\n\n    def _update_entity_page_num(self):\n        \"\"\"Updates page number if Textractor API call was given a list of images.\"\"\"\n        entities = (\n            self.words + self.lines + self.key_values + self.checkboxes + self.tables\n        )\n        for entity in entities:\n            entity.page = self.page_num\n\n    def return_duplicates(self):\n        \"\"\"\n        Returns a list containing :class:`EntityList` objects.\n        Each :class:`EntityList` instance contains the key-values and the last item is the table which contains duplicate information.\n        This function is intended to let the Textract user know of duplicate objects extracted by the various Textract models.\n\n        :return: List of EntityList objects each containing the intersection of KeyValue and Table entities on the page.\n        :rtype: List[EntityList]\n        \"\"\"\n        tables = self.tables\n        key_values = self.key_values\n\n        document_duplicates = []\n\n        for table in tables:\n            table_duplicates = EntityList([])\n            table_x1, table_x2, table_y1, table_y2 = (\n                table.bbox.x,\n                table.bbox.x + table.bbox.width,\n                table.bbox.y,\n                table.bbox.y + table.bbox.height,\n            )\n            for kv in key_values:\n                if (\n                    kv.bbox.x >= table_x1\n                    and kv.bbox.x <= table_x2\n                    and kv.bbox.y >= table_y1\n                    and kv.bbox.y <= table_y2\n                ):\n                    table_duplicates.append(kv)\n\n            if table_duplicates:\n                table_duplicates.append(table)\n\n            document_duplicates.append(table_duplicates)\n\n        return document_duplicates\n\n    def directional_finder(\n        self,\n        word_1: str = \"\",\n        word_2: str = \"\",\n        prefix: str = \"\",\n        direction=Direction.BELOW,\n        entities=[],\n    ):\n        \"\"\"\n        The function returns entity types present in entities by prepending the prefix provided by te user. This helps in cases of repeating\n        key-values and checkboxes. The user can manipulate original data or produce a copy. The main advantage of this function is to be able to define direction.\n\n        :param word_1: The reference word from where x1, y1 coordinates are derived\n        :type word_1: str, required\n        :param word_2: The second word preferably in the direction indicated by the parameter direction. When it isn't given the end of page coordinates are used in the given direction.\n        :type word_2: str, optional\n        :param prefix: User provided prefix to prepend to the key . Without prefix, the method acts as a search by geometry function\n        :type prefix: str, optional\n        :param entities: List of DirectionalFinderType inputs.\n        :type entities: List[DirectionalFinderType]\n\n        :return: Returns the EntityList of modified key-value and/or checkboxes\n        :rtype: EntityList\n        \"\"\"\n\n        if not word_1:\n            return EntityList([])\n\n        x1, x2, y1, y2 = self._get_coords(word_1, word_2, direction)\n\n        if x1 == -1:\n            return EntityList([])\n\n        entity_dict = {\n            DirectionalFinderType.KEY_VALUE_SET: self.key_values,\n            DirectionalFinderType.SELECTION_ELEMENT: self.checkboxes,\n        }\n\n        entitylist = []\n        for entity_type in entities:\n            entitylist.extend(list(entity_dict[entity_type]))\n\n        new_key_values = self._get_kv_with_direction(\n            direction, entitylist, (x1, x2, y1, y2)\n        )\n\n        final_kv = []\n        for kv in new_key_values:\n            if kv.key:\n                key_words = [deepcopy(word) for word in kv.key]\n                key_words[0].text = prefix + key_words[0].text\n                new_kv = deepcopy(kv)\n                new_kv.key = key_words\n                final_kv.append(new_kv)\n            else:\n                final_kv.append(kv)\n\n        return EntityList(final_kv)\n\n    def _get_kv_with_direction(self, direction, entitylist, coords):\n        \"\"\"Return key-values and checkboxes in entitiylist present in the direction given with respect to the coordinates.\"\"\"\n        if direction == Direction.ABOVE:\n            new_key_values = [\n                kv\n                for kv in entitylist\n                if kv.bbox.y <= coords[2] and kv.bbox.y >= coords[-1]\n            ]\n\n        elif direction == Direction.BELOW:\n            new_key_values = [\n                kv\n                for kv in entitylist\n                if kv.bbox.y >= coords[2] and kv.bbox.y <= coords[-1]\n            ]\n\n        elif direction == Direction.RIGHT:\n            new_key_values = [\n                kv\n                for kv in entitylist\n                if kv.bbox.x >= coords[0] and kv.bbox.x <= coords[1]\n            ]\n            new_key_values = [\n                kv\n                for kv in new_key_values\n                if kv.bbox.y >= coords[2] - kv.bbox.height\n                and kv.bbox.y <= coords[-1] + 3 * kv.bbox.height\n            ]\n\n        elif direction == Direction.LEFT:\n            new_key_values = [\n                kv\n                for kv in entitylist\n                if kv.bbox.x <= coords[0] and kv.bbox.x >= coords[1]\n            ]\n            new_key_values = [\n                kv\n                for kv in new_key_values\n                if kv.bbox.y >= coords[2] - kv.bbox.height\n                and kv.bbox.y <= coords[-1] + 3 * kv.bbox.height\n            ]\n\n        return new_key_values\n\n    def _get_coords(self, word_1, word_2, direction):\n        \"\"\"\n        Returns coordinates for the area within which to search for key-values with the directional_finder by retrieving coordinates of word_1 \\\n        and word_2 if it exists else end of page.\n        \"\"\"\n        word_1_objects = self.search_lines(\n            keyword=word_1,\n            top_k=5,\n            similarity_metric=SimilarityMetric.COSINE,\n            similarity_threshold=0.5,\n        )\n\n        if not word_1_objects:\n            logging.warning(f\"{word_1} not found in page\")\n            return -1, -1, -1, -1\n        else:\n            word_1_obj = word_1_objects[0]\n            x1, y1 = word_1_obj.bbox.x, word_1_obj.bbox.y\n\n        if word_2:\n            word_2_objects = self.search_lines(\n                keyword=word_2,\n                top_k=5,\n                similarity_metric=SimilarityMetric.COSINE,\n                similarity_threshold=0.5,\n            )\n\n            if not word_2_objects:\n                logging.warning(f\"{word_2} not found in page\")\n                return -1, -1, -1, -1\n            else:\n                word_2_obj = word_2_objects[0]\n                x2, y2 = word_2_obj.bbox.x, word_2_obj.bbox.y\n        else:\n            x2, y2 = 1, 1\n\n        if direction == Direction.ABOVE:\n            x1, x2, y1, y2 = (x1, x2, y1, y2) if y2 < y1 else (x1, 0, y1, 0)\n\n        elif direction == Direction.BELOW:\n            x1, x2, y1, y2 = (x1, x2, y1, y2) if y2 > y1 else (x1, 1, y1, 1)\n\n        elif direction == Direction.RIGHT:\n            x1, x2, y1, y2 = (x1, x2, y1, y2) if x2 > x1 else (x1, 1, y1, y1)\n\n        elif direction == Direction.LEFT:\n            x1, x2, y1, y2 = (x1, x2, y1, y2) if x2 < x1 else (x1, 0, y1, y1)\n\n        else:\n            return -1, -1, -1, -1\n\n        return x1, x2, y1, y2\n\n    def visualize(self, *args, **kwargs):\n        \"\"\"\n        Returns the object's children in a visualization EntityList object\n\n        :return: Returns an EntityList object\n        :rtype: EntityList\n        \"\"\"\n        return EntityList(self).visualize(*args, **kwargs)\n"
  },
  "GT_src_dict": {
    "textractor/entities/document.py": {
      "Document.open": {
        "code": "    def open(cls, fp: Union[dict, str, Path, IO[AnyStr]]):\n        \"\"\"Creates a Document object from various input formats such as a JSON file path, file handle, or response dictionary. \n\nParameters:\n- fp (Union[dict, str, Path, IO[AnyStr]]): The input representation of the document, which can be a dictionary, a string (for file path), a Path object, or a file-like object. This parameter is crucial for instantiating the Document with the relevant content.\n\nReturns:\n- Document: An instance of the Document class populated with the data parsed from the input.\n\nRaises:\n- InputError: If the input type does not match one of the expected types.\n\nThis method utilizes the `response_parser` from the `textractor.parsers` module to handle the parsing of inputs. If the input is a string that starts with \"s3://\", it retrieves the JSON content from an S3 bucket using the `boto3` library. It also reads from local files or file-like objects, transforming the input into a structured Document object.\"\"\"\n        'Create a Document object from a JSON file path, file handle or response dictionary\\n\\n        :param fp: _description_\\n        :type fp: Union[dict, str, Path, IO[AnyStr]]\\n        :raises InputError: Raised on input not being of type Union[dict, str, Path, IO[AnyStr]]\\n        :return: Document object\\n        :rtype: Document\\n        '\n        from textractor.parsers import response_parser\n        if isinstance(fp, dict):\n            return response_parser.parse(fp)\n        elif isinstance(fp, str):\n            if fp.startswith('s3://'):\n                client = boto3.client('s3')\n                return response_parser.parse(json.load(download_from_s3(client, fp)))\n            with open(fp, 'r') as f:\n                return response_parser.parse(json.load(f))\n        elif isinstance(fp, Path):\n            with open(fp, 'r') as f:\n                return response_parser.parse(json.load(f))\n        elif isinstance(fp, io.IOBase):\n            return response_parser.parse(json.load(fp))\n        else:\n            raise InputError(f'Document.open() input must be of type dict, str, Path or a file handle, not {type(fp)}')",
        "docstring": "Creates a Document object from various input formats such as a JSON file path, file handle, or response dictionary. \n\nParameters:\n- fp (Union[dict, str, Path, IO[AnyStr]]): The input representation of the document, which can be a dictionary, a string (for file path), a Path object, or a file-like object. This parameter is crucial for instantiating the Document with the relevant content.\n\nReturns:\n- Document: An instance of the Document class populated with the data parsed from the input.\n\nRaises:\n- InputError: If the input type does not match one of the expected types.\n\nThis method utilizes the `response_parser` from the `textractor.parsers` module to handle the parsing of inputs. If the input is a string that starts with \"s3://\", it retrieves the JSON content from an S3 bucket using the `boto3` library. It also reads from local files or file-like objects, transforming the input into a structured Document object.",
        "signature": "def open(cls, fp: Union[dict, str, Path, IO[AnyStr]]):",
        "type": "Method",
        "class_signature": "class Document(SpatialObject, Linearizable):"
      },
      "Document.text": {
        "code": "    def text(self) -> str:\n        \"\"\"Returns the concatenated text of all pages in the Document as a single string. Each page's text is separated by a line return (newline character). This method aggregates text from the underlying :class:`Page` objects contained within the Document, which are accessed via the `self.pages` attribute. The main dependency for this method is the `Page` class's `text` property, which retrieves the text from an individual page.\n\nThe `os.linesep` constant is used to ensure platform-specific line endings, facilitating compatibility across different operating systems. This method does not take any parameters and does not have side effects beyond returning the formatted text representation of the Document's contents.\"\"\"\n        'Returns the document text as one string\\n\\n        :return: Page text seperated by line return\\n        :rtype: str\\n        '\n        return os.linesep.join([page.text for page in self.pages])",
        "docstring": "Returns the concatenated text of all pages in the Document as a single string. Each page's text is separated by a line return (newline character). This method aggregates text from the underlying :class:`Page` objects contained within the Document, which are accessed via the `self.pages` attribute. The main dependency for this method is the `Page` class's `text` property, which retrieves the text from an individual page.\n\nThe `os.linesep` constant is used to ensure platform-specific line endings, facilitating compatibility across different operating systems. This method does not take any parameters and does not have side effects beyond returning the formatted text representation of the Document's contents.",
        "signature": "def text(self) -> str:",
        "type": "Method",
        "class_signature": "class Document(SpatialObject, Linearizable):"
      },
      "Document.pages": {
        "code": "    def pages(self, pages: List[Page]):\n        \"\"\"Sets the list of Page objects in the Document, representing the individual pages of the document. This method takes a list of Page instances, sorts them based on their `page_num` attribute to ensure correct ordering, and updates the internal `_pages` attribute. The `page_num` attribute is expected to be defined in the Page class, which allows for the correct arrangement of pages in the document. Note that no specific ordering is assumed from the input list, thus the sorting is a crucial step to maintain the logical sequence of the pages.\"\"\"\n        '\\n        Add Page objects to the Document.\\n\\n        :param pages: List of Page objects, each representing a page within the document.\\n        No specific ordering is assumed with input.\\n        :type pages: List[Page]\\n        '\n        self._pages = sorted(pages, key=lambda x: x.page_num)",
        "docstring": "Sets the list of Page objects in the Document, representing the individual pages of the document. This method takes a list of Page instances, sorts them based on their `page_num` attribute to ensure correct ordering, and updates the internal `_pages` attribute. The `page_num` attribute is expected to be defined in the Page class, which allows for the correct arrangement of pages in the document. Note that no specific ordering is assumed from the input list, thus the sorting is a crucial step to maintain the logical sequence of the pages.",
        "signature": "def pages(self, pages: List[Page]):",
        "type": "Method",
        "class_signature": "class Document(SpatialObject, Linearizable):"
      }
    },
    "textractor/parsers/response_parser.py": {
      "parse": {
        "code": "def parse(response: dict) -> Document:\n    \"\"\"Parses a JSON response from the Textract API and routes the data to the appropriate parsing function based on the type of document present in the response. Currently supports analysis for identity documents and expense documents.\n\n:param response: JSON response data from the Textract API, which must include either \"IdentityDocuments\" or \"ExpenseDocuments\" to determine the parsing logic.\n:type response: dict\n\n:return: Returns a Document object that encapsulates the hierarchy of entities extracted from the response, including pages, key-value pairs, tables, and layouts.\n:rtype: Document\n\nDependencies include `parse_analyze_id_response`, `parser_analyze_expense_response`, and `parse_document_api_response`, which handle the parsing of specific document types. The function also uses `converter(response)` to transform the response format if necessary, ensuring compatibility across different API call modes. This function is pivotal for the integration and processing of Texas API outputs into structured data formats usable within the application.\"\"\"\n    '\\n    Ingests response data and API Call Mode and calls the appropriate function for it.\\n    Presently supports only SYNC and ASYNC API calls. Will be extended to Analyze ID and Expense in the future.\\n\\n    :param response: JSON response data in a format readable by the ResponseParser.\\n    :type response: dict\\n\\n    :return: Document object returned after making respective parse function calls.\\n    :rtype: Document\\n    '\n    if 'IdentityDocuments' in response:\n        return parse_analyze_id_response(response)\n    if 'ExpenseDocuments' in response:\n        return parser_analyze_expense_response(response)\n    else:\n        return parse_document_api_response(converter(response))",
        "docstring": "Parses a JSON response from the Textract API and routes the data to the appropriate parsing function based on the type of document present in the response. Currently supports analysis for identity documents and expense documents.\n\n:param response: JSON response data from the Textract API, which must include either \"IdentityDocuments\" or \"ExpenseDocuments\" to determine the parsing logic.\n:type response: dict\n\n:return: Returns a Document object that encapsulates the hierarchy of entities extracted from the response, including pages, key-value pairs, tables, and layouts.\n:rtype: Document\n\nDependencies include `parse_analyze_id_response`, `parser_analyze_expense_response`, and `parse_document_api_response`, which handle the parsing of specific document types. The function also uses `converter(response)` to transform the response format if necessary, ensuring compatibility across different API call modes. This function is pivotal for the integration and processing of Texas API outputs into structured data formats usable within the application.",
        "signature": "def parse(response: dict) -> Document:",
        "type": "Function",
        "class_signature": null
      }
    },
    "textractor/entities/page.py": {
      "Page.text": {
        "code": "    def text(self) -> str:\n        \"\"\"Returns the linearized text of the page, combining all relevant textual elements from the page's layout. This method utilizes the `get_text()` function, which aggregates text from different layout types while ensuring the order reflects typical reading patterns.\n\n:returns: A string representing the entirety of the page text in a linear format.\n:rtype: str\n:dependencies: This method interacts with the `get_text()` function, which retrieves and concatenates text based on the layouts present in the page. The layouts are managed within the attributes of the Page class, which include various layout types defined in `textractor.data.constants`, specifically for layout classification (titles, headers, sections, etc.).\"\"\"\n        '\\n        Returns the page text\\n\\n        :return: Linearized page text\\n        :rtype: str\\n        '\n        return self.get_text()",
        "docstring": "Returns the linearized text of the page, combining all relevant textual elements from the page's layout. This method utilizes the `get_text()` function, which aggregates text from different layout types while ensuring the order reflects typical reading patterns.\n\n:returns: A string representing the entirety of the page text in a linear format.\n:rtype: str\n:dependencies: This method interacts with the `get_text()` function, which retrieves and concatenates text based on the layouts present in the page. The layouts are managed within the attributes of the Page class, which include various layout types defined in `textractor.data.constants`, specifically for layout classification (titles, headers, sections, etc.).",
        "signature": "def text(self) -> str:",
        "type": "Method",
        "class_signature": "class Page(SpatialObject, Linearizable):"
      }
    }
  },
  "dependency_dict": {
    "textractor/entities/document.py:Document:open": {},
    "textractor/parsers/response_parser.py:parse": {
      "textractor/utils/legacy_utils.py": {
        "converter": {
          "code": "def converter(response):\n    blocks_to_delete = []\n    page_blocks = []\n    try:\n        for i, block in enumerate(response[\"Blocks\"]):\n            if block.get(\"BlockType\") == \"PAGE\":\n                page_blocks.append(block)\n            elif block.get(\"BlockType\", \"\").startswith(\"LAYOUT_FIGURE_\"):\n                block[\"BlockType\"] = LAYOUT_TEXT\n            elif (\n                block.get(\"BlockType\", \"\").startswith(\"LAYOUT_\") and\n                block.get(\"BlockType\") not in [\n                    LAYOUT_TEXT,\n                    LAYOUT_TITLE,\n                    LAYOUT_HEADER,\n                    LAYOUT_FOOTER,\n                    LAYOUT_SECTION_HEADER,\n                    LAYOUT_PAGE_NUMBER,\n                    LAYOUT_LIST,\n                    LAYOUT_FIGURE,\n                    LAYOUT_TABLE,\n                    LAYOUT_KEY_VALUE,\n                ]\n            ):\n                block[\"BlockType\"] = LAYOUT_FIGURE\n            elif block.get(\"BlockType\") == LAYOUT_FIGURE and \"CONTAINER\" in block.get(\"EntityTypes\", []):\n                blocks_to_delete.append((i, block))\n        \n        blocks_to_delete_id_set = set([b[\"Id\"] for _, b in blocks_to_delete])\n        for page_block in page_blocks:\n            for relationship in page_block.get(\"Relationships\", []):\n                if relationship[\"Type\"] == \"CHILD\":\n                    relationship[\"Ids\"] = [\n                        id\n                        for id in relationship[\"Ids\"]\n                        if id not in blocks_to_delete_id_set\n                    ]\n                    break\n            \n        for i, block in blocks_to_delete[::-1]:\n            del response[\"Blocks\"][i]\n    except Exception as ex:\n        logging.warning(f\"Failed to convert the response for backward compatibility. {str(ex)}\")\n    \n    return response",
          "docstring": "",
          "signature": "def converter(response):",
          "type": "Function",
          "class_signature": null
        }
      },
      "textractor/parsers/response_parser.py": {
        "parse_document_api_response": {
          "code": "def parse_document_api_response(response: dict) -> Document:\n    \"\"\"\n    Parses Textract JSON response and converts them into Document object containing Page objects.\n    A valid Page object must contain at least a unique name and physical dimensions.\n\n    :param response: JSON response data in a format readable by the ResponseParser\n    :type response: dict\n\n    :return: Document object containing the hierarchy of DocumentEntity descendants.\n    :rtype: Document\n    \"\"\"\n    document = _create_document_object(response)\n    id_entity_map, id_json_map, entity_id_map, existing_words = ({}, {}, defaultdict(list), {})\n    for block in response['Blocks']:\n        id_entity_map[block['Id']] = block['BlockType']\n        id_json_map[block['Id']] = block\n        if block['BlockType'].startswith('LAYOUT'):\n            entity_id_map['LAYOUT'].append(block['Id'])\n        else:\n            entity_id_map[block['BlockType']].append(block['Id'])\n    pages, page_elements = _create_page_objects(response)\n    assert len(pages) == response['DocumentMetadata']['Pages']\n    for page_json in page_elements:\n        page = pages[page_json['Id']]\n        lines, line_words = _create_line_objects(entity_id_map[LINE], id_json_map, existing_words, page)\n        page.lines = deepcopy(lines)\n        line_by_id = {l.id: l for l in lines}\n        container_layouts, leaf_layouts = _create_layout_objects(entity_id_map[LAYOUT], id_json_map, entity_id_map, line_by_id, page)\n        if not container_layouts and (not leaf_layouts):\n            for i, line in enumerate(lines):\n                layout = Layout(entity_id=line.id, bbox=line.bbox, label=LAYOUT_ENTITY, reading_order=i)\n                layout._children = [line]\n                layout.page = page.page_num\n                layout.page_id = page.id\n                leaf_layouts.append(layout)\n        page._container_layouts.extend(container_layouts)\n        page._leaf_layouts.extend(leaf_layouts)\n        key_values, kv_words, selection_elements = _create_keyvalue_objects(entity_id_map[KEY_VALUE_SET], id_json_map, id_entity_map, entity_id_map, existing_words, page)\n        kvs = [kv for kv in key_values if not kv.contains_checkbox]\n        checkboxes = [kv for kv in key_values if kv.contains_checkbox]\n        page.key_values = kvs\n        page.checkboxes = checkboxes\n        for checkbox in checkboxes:\n            id_entity_map[checkbox.id] = SELECTION_ELEMENT\n        tables, table_words, kv_added = _create_table_objects(entity_id_map[TABLE], id_json_map, id_entity_map, entity_id_map, existing_words, {kv.id: kv for kv in key_values}, selection_elements, page)\n        page.tables = tables\n        for layout in sorted(page.leaf_layouts, key=lambda x: x.bbox.y):\n            if layout.layout_type == LAYOUT_ENTITY:\n                continue\n            for kv in sorted(key_values, key=lambda x: x.bbox.y):\n                if layout.bbox.get_intersection(kv.bbox).area > THRESHOLD * kv.bbox.area and kv.id not in kv_added:\n                    if any([w.cell_id for w in kv.words]):\n                        kv_added.add(kv.id)\n                        continue\n                    for w in kv.words:\n                        layout.remove(w)\n                    layout.children.append(kv)\n                    kv_added.add(kv.id)\n                    key_values.remove(kv)\n        page.leaf_layouts = [l for l in page.leaf_layouts if l.children or l.layout_type == LAYOUT_FIGURE]\n        kv_layouts = []\n        for kv in key_values:\n            if kv.id not in kv_added:\n                kv_added.add(kv.id)\n                layout = Layout(entity_id=str(uuid.uuid4()), bbox=kv.bbox, label=LAYOUT_KEY_VALUE, reading_order=-1)\n                layout.children.append(kv)\n                layout.page = page.page_num\n                layout.page_id = page.id\n                kv_layouts.append(layout)\n        layouts_to_remove = []\n        kv_layouts_to_ignore = []\n        layouts_that_intersect = defaultdict(list)\n        for layout in page.leaf_layouts:\n            for kv_layout in kv_layouts:\n                intersection = layout.bbox.get_intersection(kv_layout.bbox).area\n                if intersection:\n                    layouts_that_intersect[layout].append(kv_layout)\n        for layout, intersections in layouts_that_intersect.items():\n            words_in_sub_layouts = set()\n            for i, intersect_layout in enumerate(sorted(intersections, key=lambda l: (l.bbox.y, l.bbox.x))):\n                if sum([intersect_layout in intsects for intsects in layouts_that_intersect.values()]) > 1:\n                    kv_layouts_to_ignore.append(intersect_layout)\n                    continue\n                intersect_layout.reading_order = layout.reading_order + (i + 1) * 0.1 if intersect_layout.reading_order == -1 else min(intersect_layout.reading_order, layout.reading_order + (i + 1) * 0.1)\n                for w in intersect_layout.children[0].words:\n                    words_in_sub_layouts.add(w)\n            for word in words_in_sub_layouts:\n                layout.remove(word)\n            if not layout.children and layout.layout_type != LAYOUT_FIGURE:\n                layouts_to_remove.append(layout)\n        for layout in layouts_to_remove:\n            page.leaf_layouts.remove(layout)\n        for layout in kv_layouts:\n            if layout not in kv_layouts_to_ignore:\n                page.leaf_layouts.append(layout)\n        all_words = table_words + kv_words + line_words\n        for word in all_words:\n            if word.line is None:\n                line = Line(str(uuid.uuid4()), word.bbox, words=[word], confidence=word.confidence)\n                line.page = page.page_num\n                line.page_id = page.id\n                word.line = line\n                page.lines.append(line)\n        all_words = {word.id: word for word in all_words}\n        page.words = list(all_words.values())\n        queries = _create_query_objects(entity_id_map[QUERY], id_json_map, entity_id_map, page)\n        page.queries = queries\n        signatures = _create_signature_objects(entity_id_map[SIGNATURE], id_json_map, entity_id_map, page)\n        page.signatures = signatures\n        word_set = set()\n        for layout in sorted(page.layouts, key=lambda l: l.reading_order):\n            layout.visit(word_set)\n            if not layout.children and layout.layout_type != LAYOUT_FIGURE:\n                try:\n                    page.leaf_layouts.remove(layout)\n                except:\n                    page.container_layouts.remove(layout)\n    document.pages = sorted(list(pages.values()), key=lambda x: x.page_num)\n    document.response = response\n    return document",
          "docstring": "Parses Textract JSON response and converts them into Document object containing Page objects.\nA valid Page object must contain at least a unique name and physical dimensions.\n\n:param response: JSON response data in a format readable by the ResponseParser\n:type response: dict\n\n:return: Document object containing the hierarchy of DocumentEntity descendants.\n:rtype: Document",
          "signature": "def parse_document_api_response(response: dict) -> Document:",
          "type": "Function",
          "class_signature": null
        }
      }
    },
    "textractor/entities/document.py:Document:text": {},
    "textractor/entities/document.py:Document:pages": {},
    "textractor/entities/page.py:Page:text": {
      "textractor/entities/linearizable.py": {
        "Linearizable.get_text": {
          "code": "    def get_text(\n        self, config: TextLinearizationConfig = TextLinearizationConfig()\n    ) -> str:\n        \"\"\"\n        Returns the linearized text of the entity\n\n        :param config: Text linearization confi \n        :type config:   \n        :return: Linearized text of the entity\n        :rtype: str\n        \"\"\"\n        text, _ = self.get_text_and_words(config=config)\n        return text",
          "docstring": "Returns the linearized text of the entity\n\n:param config: Text linearization confi \n:type config:   \n:return: Linearized text of the entity\n:rtype: str",
          "signature": "def get_text(self, config: TextLinearizationConfig=TextLinearizationConfig()) -> str:",
          "type": "Method",
          "class_signature": "class Linearizable(ABC):"
        }
      }
    }
  },
  "call_tree": {
    "tests/test_layout.py:TestLayout:test_layout": {
      "tests/utils.py:get_fixture_path": {},
      "textractor/entities/document.py:Document:open": {
        "textractor/parsers/response_parser.py:parse": {
          "textractor/utils/legacy_utils.py:converter": {},
          "textractor/parsers/response_parser.py:parse_document_api_response": {
            "textractor/parsers/response_parser.py:_create_document_object": {
              "textractor/entities/document.py:Document:__init__": {
                "textractor/entities/bbox.py:SpatialObject:__init__": {}
              }
            },
            "textractor/parsers/response_parser.py:_create_page_objects": {
              "textractor/parsers/response_parser.py:_filter_block_type": {},
              "textractor/parsers/response_parser.py:_get_relationship_ids": {},
              "textractor/entities/page.py:Page:__init__": {
                "textractor/entities/bbox.py:SpatialObject:__init__": {},
                "textractor/visualizers/entitylist.py:EntityList:__init__": {}
              }
            },
            "textractor/parsers/response_parser.py:_create_line_objects": {
              "textractor/parsers/response_parser.py:_get_relationship_ids": {},
              "textractor/parsers/response_parser.py:_create_word_objects": {
                "textractor/entities/bbox.py:BoundingBox:from_normalized_dict": {
                  "textractor/entities/bbox.py:BoundingBox:_from_dict": {
                    "textractor/entities/bbox.py:BoundingBox:__init__": {
                      "textractor/entities/bbox.py:SpatialObject:__init__": {}
                    }
                  }
                },
                "textractor/entities/word.py:Word:__init__": {
                  "textractor/entities/document_entity.py:DocumentEntity:__init__": {}
                },
                "textractor/entities/document_entity.py:DocumentEntity:raw_object": {},
                "textractor/entities/word.py:Word:page": {},
                "textractor/entities/word.py:Word:page_id": {}
              },
              "textractor/entities/bbox.py:BoundingBox:from_normalized_dict": {
                "textractor/entities/bbox.py:BoundingBox:_from_dict": {
                  "textractor/entities/bbox.py:BoundingBox:__init__": {
                    "textractor/entities/bbox.py:SpatialObject:__init__": {}
                  }
                }
              },
              "textractor/entities/line.py:Line:__init__": {
                "textractor/entities/document_entity.py:DocumentEntity:__init__": {},
                "textractor/entities/document_entity.py:DocumentEntity:bbox": {}
              },
              "textractor/entities/document_entity.py:DocumentEntity:bbox": {},
              "textractor/entities/document_entity.py:DocumentEntity:raw_object": {},
              "textractor/entities/line.py:Line:page": {},
              "textractor/entities/line.py:Line:page_id": {}
            },
            "textractor/entities/page.py:Page:lines": {
              "textractor/utils/geometry_util.py:sort_by_position": {
                "textractor/entities/document_entity.py:DocumentEntity:bbox": {}
              },
              "textractor/visualizers/entitylist.py:EntityList:__init__": {}
            },
            "textractor/parsers/response_parser.py:_create_layout_objects": {
              "textractor/entities/bbox.py:BoundingBox:from_normalized_dict": {
                "textractor/entities/bbox.py:BoundingBox:_from_dict": {
                  "textractor/entities/bbox.py:BoundingBox:__init__": {
                    "textractor/entities/bbox.py:SpatialObject:__init__": {}
                  }
                }
              },
              "textractor/entities/layout.py:Layout:__init__": {
                "textractor/entities/document_entity.py:DocumentEntity:__init__": {}
              },
              "textractor/entities/document_entity.py:DocumentEntity:raw_object": {},
              "textractor/entities/document_entity.py:DocumentEntity:add_children": {},
              "textractor/entities/layout.py:Layout:page": {},
              "textractor/entities/layout.py:Layout:page_id": {}
            },
            "textractor/parsers/response_parser.py:_create_keyvalue_objects": {
              "textractor/parsers/response_parser.py:_filter_by_entity": {},
              "textractor/parsers/response_parser.py:_get_relationship_ids": {},
              "textractor/parsers/response_parser.py:_create_value_objects": {
                "textractor/entities/bbox.py:BoundingBox:from_normalized_dict": {
                  "textractor/entities/bbox.py:BoundingBox:_from_dict": {
                    "textractor/entities/bbox.py:BoundingBox:__init__": {
                      "textractor/entities/bbox.py:SpatialObject:__init__": {}
                    }
                  }
                },
                "textractor/entities/value.py:Value:__init__": {
                  "textractor/entities/document_entity.py:DocumentEntity:__init__": {}
                },
                "textractor/entities/document_entity.py:DocumentEntity:raw_object": {},
                "textractor/parsers/response_parser.py:_create_selection_objects": {},
                "textractor/parsers/response_parser.py:_get_relationship_ids": {},
                "textractor/parsers/response_parser.py:_create_word_objects": {
                  "textractor/entities/word.py:Word:page": {},
                  "textractor/entities/word.py:Word:page_id": {}
                },
                "textractor/entities/value.py:Value:words": {
                  "textractor/entities/value.py:Value:contains_checkbox": {},
                  "textractor/visualizers/entitylist.py:EntityList:__init__": {},
                  "textractor/entities/document_entity.py:DocumentEntity:bbox": {}
                },
                "textractor/entities/document_entity.py:DocumentEntity:add_children": {},
                "textractor/entities/value.py:Value:page": {},
                "textractor/entities/value.py:Value:page_id": {}
              },
              "textractor/entities/bbox.py:BoundingBox:from_normalized_dict": {
                "textractor/entities/bbox.py:BoundingBox:_from_dict": {
                  "textractor/entities/bbox.py:BoundingBox:__init__": {
                    "textractor/entities/bbox.py:SpatialObject:__init__": {}
                  }
                }
              },
              "textractor/entities/value.py:Value:contains_checkbox": {},
              "textractor/entities/key_value.py:KeyValue:__init__": {
                "textractor/entities/document_entity.py:DocumentEntity:__init__": {}
              },
              "textractor/entities/document_entity.py:DocumentEntity:raw_object": {},
              "textractor/entities/key_value.py:KeyValue:value": {},
              "textractor/entities/value.py:Value:key_id": {},
              "textractor/entities/value.py:Value:words": {
                "textractor/entities/value.py:Value:contains_checkbox": {},
                "textractor/visualizers/entitylist.py:EntityList:__init__": {}
              },
              "textractor/parsers/response_parser.py:_create_word_objects": {
                "textractor/entities/word.py:Word:page": {},
                "textractor/entities/word.py:Word:page_id": {}
              },
              "textractor/entities/key_value.py:KeyValue:key": {
                "textractor/visualizers/entitylist.py:EntityList:__init__": {}
              },
              "textractor/entities/document_entity.py:DocumentEntity:add_children": {},
              "textractor/entities/document_entity.py:DocumentEntity:bbox": {},
              "textractor/entities/bbox.py:BoundingBox:enclosing_bbox": {
                "textractor/entities/bbox.py:BoundingBox:__init__": {
                  "textractor/entities/bbox.py:SpatialObject:__init__": {}
                }
              },
              "textractor/entities/key_value.py:KeyValue:page": {},
              "textractor/entities/key_value.py:KeyValue:page_id": {}
            },
            "textractor/entities/page.py:Page:key_values": {
              "textractor/utils/geometry_util.py:sort_by_position": {
                "textractor/entities/document_entity.py:DocumentEntity:bbox": {}
              },
              "textractor/visualizers/entitylist.py:EntityList:__init__": {}
            },
            "textractor/entities/page.py:Page:checkboxes": {
              "textractor/utils/geometry_util.py:sort_by_position": {},
              "textractor/visualizers/entitylist.py:EntityList:__init__": {}
            },
            "textractor/parsers/response_parser.py:_create_table_objects": {
              "textractor/entities/bbox.py:BoundingBox:from_normalized_dict": {
                "textractor/entities/bbox.py:BoundingBox:_from_dict": {
                  "textractor/entities/bbox.py:BoundingBox:__init__": {
                    "textractor/entities/bbox.py:SpatialObject:__init__": {}
                  }
                }
              },
              "textractor/entities/table.py:Table:__init__": {
                "textractor/entities/document_entity.py:DocumentEntity:__init__": {}
              },
              "textractor/entities/table.py:Table:table_type": {},
              "textractor/entities/document_entity.py:DocumentEntity:raw_object": {},
              "textractor/parsers/response_parser.py:_create_table_cell_objects": {
                "textractor/parsers/response_parser.py:_get_relationship_ids": {},
                "textractor/entities/bbox.py:BoundingBox:from_normalized_dict": {
                  "textractor/entities/bbox.py:BoundingBox:_from_dict": {
                    "textractor/entities/bbox.py:BoundingBox:__init__": {
                      "textractor/entities/bbox.py:SpatialObject:__init__": {}
                    }
                  }
                },
                "textractor/entities/table_cell.py:TableCell:__init__": {
                  "textractor/entities/document_entity.py:DocumentEntity:__init__": {}
                },
                "textractor/entities/document_entity.py:DocumentEntity:raw_object": {},
                "textractor/entities/table_cell.py:TableCell:page": {},
                "textractor/entities/table_cell.py:TableCell:page_id": {}
              },
              "textractor/parsers/response_parser.py:_get_relationship_ids": {},
              "textractor/parsers/response_parser.py:_create_word_objects": {
                "textractor/entities/word.py:Word:page": {},
                "textractor/entities/word.py:Word:page_id": {}
              },
              "textractor/entities/document_entity.py:DocumentEntity:bbox": {},
              "textractor/entities/table_cell.py:TableCell:row_span": {},
              "textractor/entities/table_cell.py:TableCell:col_span": {},
              "textractor/entities/table_cell.py:TableCell:row_index": {},
              "textractor/entities/table_cell.py:TableCell:col_index": {},
              "textractor/entities/document_entity.py:DocumentEntity:add_children": {},
              "textractor/entities/table_cell.py:TableCell:_update_response_metadata": {},
              "textractor/entities/key_value.py:KeyValue:words": {
                "textractor/entities/key_value.py:KeyValue:value": {},
                "textractor/entities/value.py:Value:words": {
                  "textractor/entities/value.py:Value:contains_checkbox": {},
                  "textractor/visualizers/entitylist.py:EntityList:__init__": {}
                },
                "textractor/visualizers/entitylist.py:EntityList:__add__": {
                  "textractor/visualizers/entitylist.py:EntityList:__init__": {}
                },
                "textractor/entities/document_entity.py:DocumentEntity:bbox": {},
                "textractor/visualizers/entitylist.py:EntityList:__init__": {}
              },
              "textractor/entities/table.py:Table:footers": {},
              "textractor/entities/table_footer.py:TableFooter:__init__": {
                "textractor/entities/document_entity.py:DocumentEntity:__init__": {}
              },
              "textractor/entities/table_footer.py:TableFooter:words": {
                "textractor/visualizers/entitylist.py:EntityList:__init__": {}
              },
              "textractor/entities/table_cell.py:TableCell:is_title": {},
              "textractor/entities/table_cell.py:TableCell:words": {
                "textractor/entities/table_cell.py:TableCell:get_text_and_words": {
                  "textractor/entities/document_entity.py:DocumentEntity:children": {},
                  "textractor/utils/text_utils.py:linearize_children": {
                    "textractor/entities/word.py:Word:Word": {},
                    "textractor/entities/document_entity.py:DocumentEntity:bbox": {},
                    "textractor/entities/line.py:Line:__init__": {
                      "textractor/entities/document_entity.py:DocumentEntity:__init__": {}
                    },
                    "textractor/utils/text_utils.py:group_elements_horizontally": {
                      "textractor/utils/text_utils.py:compare_bounding_box": {},
                      "textractor/utils/text_utils.py:should_group": {}
                    },
                    "textractor/entities/line.py:Line:get_text_and_words": {
                      "textractor/entities/document_entity.py:DocumentEntity:bbox": {},
                      "textractor/entities/line.py:Line:words": {},
                      "textractor/entities/line.py:Line:text": {},
                      "textractor/utils/html_utils.py:escape_text": {
                        "textractor/data/html_linearization_config.py:HTMLLinearizationConfig": {}
                      }
                    },
                    "textractor/entities/bbox.py:BoundingBox:enclosing_bbox": {
                      "textractor/entities/bbox.py:BoundingBox:__init__": {}
                    },
                    "textractor/utils/text_utils.py:part_of_same_paragraph": {
                      "textractor/entities/document_entity.py:DocumentEntity:bbox": {}
                    }
                  }
                },
                "textractor/visualizers/entitylist.py:EntityList:__init__": {}
              },
              "textractor/entities/table.py:Table:add_cells": {
                "textractor/entities/table_cell.py:TableCell:table_id": {},
                "textractor/entities/table_cell.py:TableCell:row_index": {},
                "textractor/entities/table_cell.py:TableCell:col_index": {}
              },
              "textractor/entities/page.py:Page:leaf_layouts": {},
              "textractor/entities/bbox.py:BoundingBox:get_intersection": {
                "textractor/entities/bbox.py:BoundingBox": {},
                "textractor/entities/bbox.py:BoundingBox:from_denormalized_corners": {
                  "textractor/entities/bbox.py:BoundingBox:__init__": {
                    "textractor/entities/bbox.py:SpatialObject:__init__": {}
                  }
                }
              },
              "textractor/entities/bbox.py:BoundingBox:area": {},
              "textractor/entities/table.py:Table:words": {
                "textractor/entities/table_cell.py:TableCell:words": {
                  "textractor/entities/table_cell.py:TableCell:get_text_and_words": {
                    "textractor/entities/document_entity.py:DocumentEntity:children": {},
                    "textractor/utils/text_utils.py:linearize_children": {
                      "textractor/entities/document_entity.py:DocumentEntity:bbox": {},
                      "textractor/entities/line.py:Line:__init__": {},
                      "textractor/utils/text_utils.py:group_elements_horizontally": {},
                      "textractor/entities/line.py:Line:get_text_and_words": {},
                      "textractor/entities/bbox.py:BoundingBox:enclosing_bbox": {},
                      "textractor/utils/text_utils.py:part_of_same_paragraph": {}
                    }
                  },
                  "textractor/visualizers/entitylist.py:EntityList:__init__": {}
                },
                "textractor/visualizers/entitylist.py:EntityList:__init__": {}
              },
              "textractor/entities/document_entity.py:DocumentEntity:remove": {
                "textractor/entities/document_entity.py:DocumentEntity:remove": {
                  "[ignored_or_cut_off]": "..."
                },
                "textractor/entities/document_entity.py:DocumentEntity:children": {}
              },
              "textractor/entities/document_entity.py:DocumentEntity:children": {},
              "textractor/entities/bbox.py:BoundingBox:enclosing_bbox": {
                "textractor/entities/document_entity.py:DocumentEntity:bbox": {},
                "textractor/entities/bbox.py:BoundingBox:__init__": {
                  "textractor/entities/bbox.py:SpatialObject:__init__": {}
                }
              },
              "textractor/entities/layout.py:Layout:__init__": {
                "textractor/entities/document_entity.py:DocumentEntity:__init__": {}
              },
              "textractor/entities/layout.py:Layout:page": {},
              "textractor/entities/layout.py:Layout:page_id": {},
              "textractor/entities/table.py:Table:page": {},
              "textractor/entities/table.py:Table:page_id": {}
            },
            "textractor/entities/page.py:Page:tables": {
              "textractor/visualizers/entitylist.py:EntityList:__init__": {}
            },
            "textractor/entities/page.py:Page:leaf_layouts": {
              "textractor/visualizers/entitylist.py:EntityList:__init__": {}
            },
            "textractor/entities/document_entity.py:DocumentEntity:bbox": {},
            "textractor/entities/bbox.py:BoundingBox:get_intersection": {
              "textractor/entities/bbox.py:BoundingBox:from_denormalized_corners": {
                "textractor/entities/bbox.py:BoundingBox:__init__": {
                  "textractor/entities/bbox.py:SpatialObject:__init__": {}
                }
              }
            },
            "textractor/entities/bbox.py:BoundingBox:area": {},
            "textractor/entities/key_value.py:KeyValue:words": {
              "textractor/entities/key_value.py:KeyValue:value": {},
              "textractor/entities/value.py:Value:words": {
                "textractor/entities/value.py:Value:contains_checkbox": {},
                "textractor/visualizers/entitylist.py:EntityList:__init__": {}
              },
              "textractor/visualizers/entitylist.py:EntityList:__add__": {
                "textractor/visualizers/entitylist.py:EntityList:__init__": {}
              },
              "textractor/entities/document_entity.py:DocumentEntity:bbox": {},
              "textractor/visualizers/entitylist.py:EntityList:__init__": {}
            },
            "textractor/entities/document_entity.py:DocumentEntity:remove": {
              "textractor/entities/document_entity.py:DocumentEntity:remove": {
                "[ignored_or_cut_off]": "..."
              },
              "textractor/entities/document_entity.py:DocumentEntity:children": {}
            },
            "textractor/entities/document_entity.py:DocumentEntity:children": {},
            "textractor/entities/layout.py:Layout:__init__": {
              "textractor/entities/document_entity.py:DocumentEntity:__init__": {}
            },
            "textractor/entities/layout.py:Layout:page": {},
            "textractor/entities/layout.py:Layout:page_id": {},
            "textractor/entities/page.py:Page:words": {
              "textractor/utils/geometry_util.py:sort_by_position": {
                "textractor/entities/document_entity.py:DocumentEntity:bbox": {}
              },
              "textractor/visualizers/entitylist.py:EntityList:__init__": {}
            },
            "textractor/parsers/response_parser.py:_create_query_objects": {
              "textractor/parsers/response_parser.py:_create_query_result_objects": {}
            },
            "textractor/entities/page.py:Page:queries": {
              "textractor/visualizers/entitylist.py:EntityList:__init__": {}
            },
            "textractor/parsers/response_parser.py:_create_signature_objects": {
              "textractor/entities/page.py:Page:leaf_layouts": {},
              "textractor/entities/document_entity.py:DocumentEntity:bbox": {}
            },
            "textractor/entities/page.py:Page:signatures": {
              "textractor/visualizers/entitylist.py:EntityList:__init__": {}
            },
            "textractor/entities/page.py:Page:layouts": {
              "textractor/visualizers/entitylist.py:EntityList:__add__": {
                "textractor/visualizers/entitylist.py:EntityList:__init__": {}
              },
              "textractor/visualizers/entitylist.py:EntityList:__init__": {}
            },
            "textractor/entities/document_entity.py:DocumentEntity:visit": {
              "textractor/entities/document_entity.py:DocumentEntity:visit": {
                "[ignored_or_cut_off]": "..."
              }
            },
            "textractor/entities/document.py:Document:pages": {}
          }
        }
      },
      "textractor/entities/document.py:Document:text": {
        "textractor/entities/document.py:Document:pages": {},
        "textractor/entities/page.py:Page:text": {
          "textractor/entities/linearizable.py:Linearizable:get_text": {
            "textractor/entities/page.py:Page:get_text_and_words": {
              "textractor/entities/page.py:Page:layouts": {
                "textractor/visualizers/entitylist.py:EntityList:__add__": {
                  "textractor/visualizers/entitylist.py:EntityList:__init__": {}
                },
                "textractor/visualizers/entitylist.py:EntityList:__init__": {}
              },
              "textractor/entities/layout.py:Layout:get_text_and_words": {
                "textractor/entities/document_entity.py:DocumentEntity:children": {},
                "textractor/utils/text_utils.py:linearize_children": {
                  "textractor/utils/text_utils.py:group_elements_horizontally": {
                    "textractor/utils/text_utils.py:compare_bounding_box": {
                      "textractor/entities/document_entity.py:DocumentEntity:bbox": {}
                    },
                    "textractor/utils/text_utils.py:should_group": {}
                  },
                  "textractor/entities/document_entity.py:DocumentEntity:bbox": {},
                  "textractor/entities/line.py:Line:get_text_and_words": {
                    "textractor/entities/document_entity.py:DocumentEntity:bbox": {},
                    "textractor/entities/line.py:Line:words": {},
                    "textractor/entities/line.py:Line:text": {
                      "textractor/entities/line.py:Line:words": {},
                      "textractor/entities/word.py:Word:text": {}
                    },
                    "textractor/utils/html_utils.py:escape_text": {}
                  },
                  "textractor/entities/bbox.py:BoundingBox:enclosing_bbox": {
                    "textractor/entities/bbox.py:BoundingBox:__init__": {
                      "textractor/entities/bbox.py:SpatialObject:__init__": {}
                    }
                  },
                  "textractor/entities/line.py:Line:__init__": {
                    "textractor/entities/document_entity.py:DocumentEntity:__init__": {}
                  },
                  "textractor/entities/table.py:Table:get_text_and_words": {
                    "textractor/entities/table.py:Table:words": {
                      "textractor/entities/table_cell.py:TableCell:words": {},
                      "textractor/visualizers/entitylist.py:EntityList:__init__": {}
                    },
                    "textractor/entities/table_cell.py:TableCell:row_index": {},
                    "textractor/entities/table_cell.py:TableCell:col_index": {},
                    "textractor/entities/document_entity.py:DocumentEntity:bbox": {},
                    "textractor/entities/table_cell.py:TableCell:col_span": {},
                    "textractor/entities/table_cell.py:TableCell:row_span": {},
                    "textractor/entities/table_cell.py:TableCell:get_text_and_words": {
                      "textractor/entities/document_entity.py:DocumentEntity:children": {},
                      "textractor/utils/text_utils.py:linearize_children": {
                        "[ignored_or_cut_off]": "..."
                      }
                    },
                    "textractor/utils/html_utils.py:add_id_to_html_tag": {},
                    "textractor/entities/table_cell.py:TableCell:_get_merged_cell_range": {
                      "textractor/entities/table_cell.py:TableCell:row_index": {},
                      "textractor/entities/table_cell.py:TableCell:col_index": {}
                    },
                    "textractor/entities/bbox.py:BoundingBox:enclosing_bbox": {
                      "textractor/entities/document_entity.py:DocumentEntity:bbox": {},
                      "textractor/entities/bbox.py:BoundingBox:__init__": {}
                    },
                    "textractor/entities/document_entity.py:DocumentEntity:children": {},
                    "textractor/utils/text_utils.py:linearize_children": {
                      "[ignored_or_cut_off]": "..."
                    }
                  },
                  "textractor/utils/text_utils.py:part_of_same_paragraph": {
                    "textractor/entities/document_entity.py:DocumentEntity:bbox": {}
                  },
                  "textractor/entities/key_value.py:KeyValue:get_text_and_words": {
                    "textractor/entities/key_value.py:KeyValue:key": {},
                    "textractor/entities/word.py:Word:text": {},
                    "textractor/entities/key_value.py:KeyValue:value": {},
                    "textractor/entities/value.py:Value:get_text_and_words": {
                      "textractor/entities/value.py:Value:contains_checkbox": {},
                      "textractor/entities/value.py:Value:words": {},
                      "textractor/utils/text_utils.py:linearize_children": {
                        "[ignored_or_cut_off]": "..."
                      },
                      "textractor/utils/html_utils.py:add_id_to_html_tag": {},
                      "textractor/entities/document_entity.py:DocumentEntity:bbox": {}
                    },
                    "textractor/utils/html_utils.py:add_id_to_html_tag": {},
                    "textractor/visualizers/entitylist.py:EntityList:__add__": {
                      "textractor/visualizers/entitylist.py:EntityList:__init__": {}
                    },
                    "textractor/entities/document_entity.py:DocumentEntity:bbox": {}
                  }
                },
                "textractor/utils/html_utils.py:add_id_to_html_tag": {
                  "textractor/data/html_linearization_config.py:HTMLLinearizationConfig": {}
                }
              }
            }
          }
        }
      }
    }
  },
  "PRD": "# PROJECT NAME: amazon_textract_textractor-test_layout\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 textractor/\n    \u251c\u2500\u2500 entities/\n    \u2502   \u251c\u2500\u2500 document.py\n    \u2502   \u2502   \u251c\u2500\u2500 Document.open\n    \u2502   \u2502   \u251c\u2500\u2500 Document.pages\n    \u2502   \u2502   \u2514\u2500\u2500 Document.text\n    \u2502   \u2514\u2500\u2500 page.py\n    \u2502       \u2514\u2500\u2500 Page.text\n    \u2514\u2500\u2500 parsers/\n        \u2514\u2500\u2500 response_parser.py\n            \u2514\u2500\u2500 parse\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThe module is designed to process, analyze, and extract structured and unstructured data from text-rich documents, leveraging Amazon Textract's OCR and document analysis capabilities. It enables the classification and extraction of various document elements, including text, tables, forms, and key-value pairs, while also offering support for different levels of text hierarchy, such as words, lines, pages, and selection elements. By automating the parsing and processing of document content, the module streamlines workflows for developers working with complex document types, reducing manual effort and improving accuracy when handling scanned images or PDFs. This functionality empowers users to efficiently transform diverse document formats into structured, actionable data.\n\n## FILE 1: textractor/entities/document.py\n\n- CLASS METHOD: Document.pages\n  - CLASS SIGNATURE: class Document(SpatialObject, Linearizable):\n  - SIGNATURE: def pages(self, pages: List[Page]):\n  - DOCSTRING: \n```python\n\"\"\"\nSets the list of Page objects in the Document, representing the individual pages of the document. This method takes a list of Page instances, sorts them based on their `page_num` attribute to ensure correct ordering, and updates the internal `_pages` attribute. The `page_num` attribute is expected to be defined in the Page class, which allows for the correct arrangement of pages in the document. Note that no specific ordering is assumed from the input list, thus the sorting is a crucial step to maintain the logical sequence of the pages.\n\"\"\"\n```\n\n- CLASS METHOD: Document.open\n  - CLASS SIGNATURE: class Document(SpatialObject, Linearizable):\n  - SIGNATURE: def open(cls, fp: Union[dict, str, Path, IO[AnyStr]]):\n  - DOCSTRING: \n```python\n\"\"\"\nCreates a Document object from various input formats such as a JSON file path, file handle, or response dictionary. \n\nParameters:\n- fp (Union[dict, str, Path, IO[AnyStr]]): The input representation of the document, which can be a dictionary, a string (for file path), a Path object, or a file-like object. This parameter is crucial for instantiating the Document with the relevant content.\n\nReturns:\n- Document: An instance of the Document class populated with the data parsed from the input.\n\nRaises:\n- InputError: If the input type does not match one of the expected types.\n\nThis method utilizes the `response_parser` from the `textractor.parsers` module to handle the parsing of inputs. If the input is a string that starts with \"s3://\", it retrieves the JSON content from an S3 bucket using the `boto3` library. It also reads from local files or file-like objects, transforming the input into a structured Document object.\n\"\"\"\n```\n\n- CLASS METHOD: Document.text\n  - CLASS SIGNATURE: class Document(SpatialObject, Linearizable):\n  - SIGNATURE: def text(self) -> str:\n  - DOCSTRING: \n```python\n\"\"\"\nReturns the concatenated text of all pages in the Document as a single string. Each page's text is separated by a line return (newline character). This method aggregates text from the underlying :class:`Page` objects contained within the Document, which are accessed via the `self.pages` attribute. The main dependency for this method is the `Page` class's `text` property, which retrieves the text from an individual page.\n\nThe `os.linesep` constant is used to ensure platform-specific line endings, facilitating compatibility across different operating systems. This method does not take any parameters and does not have side effects beyond returning the formatted text representation of the Document's contents.\n\"\"\"\n```\n\n## FILE 2: textractor/parsers/response_parser.py\n\n- FUNCTION NAME: parse\n  - SIGNATURE: def parse(response: dict) -> Document:\n  - DOCSTRING: \n```python\n\"\"\"\nParses a JSON response from the Textract API and routes the data to the appropriate parsing function based on the type of document present in the response. Currently supports analysis for identity documents and expense documents.\n\n:param response: JSON response data from the Textract API, which must include either \"IdentityDocuments\" or \"ExpenseDocuments\" to determine the parsing logic.\n:type response: dict\n\n:return: Returns a Document object that encapsulates the hierarchy of entities extracted from the response, including pages, key-value pairs, tables, and layouts.\n:rtype: Document\n\nDependencies include `parse_analyze_id_response`, `parser_analyze_expense_response`, and `parse_document_api_response`, which handle the parsing of specific document types. The function also uses `converter(response)` to transform the response format if necessary, ensuring compatibility across different API call modes. This function is pivotal for the integration and processing of Texas API outputs into structured data formats usable within the application.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - textractor/entities/document.py:Document:open\n    - textractor/utils/legacy_utils.py:converter\n    - textractor/parsers/response_parser.py:parse_document_api_response\n\n## FILE 3: textractor/entities/page.py\n\n- CLASS METHOD: Page.text\n  - CLASS SIGNATURE: class Page(SpatialObject, Linearizable):\n  - SIGNATURE: def text(self) -> str:\n  - DOCSTRING: \n```python\n\"\"\"\nReturns the linearized text of the page, combining all relevant textual elements from the page's layout. This method utilizes the `get_text()` function, which aggregates text from different layout types while ensuring the order reflects typical reading patterns.\n\n:returns: A string representing the entirety of the page text in a linear format.\n:rtype: str\n:dependencies: This method interacts with the `get_text()` function, which retrieves and concatenates text based on the layouts present in the page. The layouts are managed within the attributes of the Page class, which include various layout types defined in `textractor.data.constants`, specifically for layout classification (titles, headers, sections, etc.).\n\"\"\"\n```\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "textractor/entities/document.py": "\"\"\"The Document class is defined to host all the various DocumentEntity objects within it. :class:`DocumentEntity` objects can be \naccessed, searched and exported the functions given below.\"\"\"\nimport boto3\nimport json\nimport os\nimport string\nimport logging\nimport xlsxwriter\nimport io\nfrom pathlib import Path\nfrom typing import List, IO, Union, AnyStr, Tuple\nfrom copy import deepcopy\nfrom collections import defaultdict\nfrom PIL import Image\nfrom textractor.entities.expense_document import ExpenseDocument\nfrom textractor.entities.identity_document import IdentityDocument\nfrom textractor.entities.word import Word\nfrom textractor.entities.line import Line\nfrom textractor.entities.page import Page\nfrom textractor.entities.table import Table\nfrom textractor.entities.query import Query\nfrom textractor.entities.signature import Signature\nfrom textractor.entities.layout import Layout\nfrom textractor.exceptions import InputError\nfrom textractor.entities.key_value import KeyValue\nfrom textractor.entities.bbox import SpatialObject\nfrom textractor.utils.s3_utils import download_from_s3\nfrom textractor.visualizers.entitylist import EntityList\nfrom textractor.data.constants import TextTypes, SimilarityMetric, Direction, DirectionalFinderType\nfrom textractor.utils.search_utils import SearchUtils\nfrom textractor.data.text_linearization_config import TextLinearizationConfig\nfrom textractor.data.html_linearization_config import HTMLLinearizationConfig\nfrom textractor.entities.linearizable import Linearizable\n\nclass Document(SpatialObject, Linearizable):\n    \"\"\"\n    Represents the description of a single document, as it would appear in the input to the Textract API.\n    Document serves as the root node of the object model hierarchy,\n    which should be used as an intermediate form for most analytic purposes.\n    The Document node also contains the metadata of the document.\n    \"\"\"\n\n    def __init__(self, num_pages: int=1):\n        \"\"\"\n        Creates a new document, ideally containing entity objects pertaining to each page.\n\n        :param num_pages: Number of pages in the input Document.\n        \"\"\"\n        super().__init__(width=0, height=0)\n        self.num_pages: int = num_pages\n        self._pages: List[Page] = []\n        self._identity_documents: List[IdentityDocument] = []\n        self._trp2_document = None\n        self.response = None\n\n    @property\n    def words(self) -> EntityList[Word]:\n        \"\"\"\n        Returns all the :class:`Word` objects present in the Document.\n\n        :return: List of Word objects, each representing a word within the Document.\n        :rtype: EntityList[Word]\n        \"\"\"\n        return EntityList(sum([page.words for page in self.pages], []))\n\n    @property\n    def identity_documents(self) -> EntityList[IdentityDocument]:\n        \"\"\"\n        Returns all the :class:`IdentityDocument` objects present in the Document.\n\n        :return: List of IdentityDocument objects, each representing an identity document within the Document.\n        :rtype: EntityList[IdentityDocument]\n        \"\"\"\n        return EntityList(self._identity_documents)\n\n    @identity_documents.setter\n    def identity_documents(self, identity_documents: List[IdentityDocument]):\n        \"\"\"\n        Set all the identity documents detected inside the Document\n        \"\"\"\n        self._identity_documents = identity_documents\n\n    @property\n    def expense_documents(self) -> EntityList[ExpenseDocument]:\n        \"\"\"\n        Returns all the :class:`ExpenseDocument` objects present in the Document.\n\n        :return: List of ExpenseDocument objects, each representing an expense document within the Document.\n        :rtype: EntityList[ExpenseDocument]\n        \"\"\"\n        return EntityList(sum([page.expense_documents for page in self.pages], []))\n\n    @property\n    def lines(self) -> EntityList[Line]:\n        \"\"\"\n        Returns all the :class:`Line` objects present in the Document.\n\n        :return: List of Line objects, each representing a line within the Document.\n        :rtype: EntityList[Line]\n        \"\"\"\n        return EntityList(sum([page.lines for page in self.pages], []))\n\n    @property\n    def key_values(self) -> EntityList[KeyValue]:\n        \"\"\"\n        Returns all the :class:`KeyValue` objects present in the Document.\n\n        :return: List of KeyValue objects, each representing a key-value pair within the Document.\n        :rtype: EntityList[KeyValue]\n        \"\"\"\n        return EntityList(sum([page.key_values for page in self.pages], []))\n\n    @property\n    def checkboxes(self) -> EntityList[KeyValue]:\n        \"\"\"\n        Returns all the :class:`KeyValue` objects with SelectionElements present in the Document.\n\n        :return: List of KeyValue objects, each representing a checkbox within the Document.\n        :rtype: EntityList[KeyValue]\n        \"\"\"\n        return EntityList(sum([page.checkboxes for page in self.pages], []))\n\n    @property\n    def tables(self) -> EntityList[Table]:\n        \"\"\"\n        Returns all the :class:`Table` objects present in the Document.\n\n        :return: List of Table objects, each representing a table within the Document.\n        :rtype: EntityList[Table]\n        \"\"\"\n        return EntityList(sum([page.tables for page in self.pages], []))\n\n    @property\n    def queries(self) -> EntityList[Query]:\n        \"\"\"\n        Returns all the :class:`Query` objects present in the Document.\n\n        :return: List of Query objects.\n        :rtype: EntityList[Query]\n        \"\"\"\n        return EntityList(sum([page.queries for page in self.pages], []))\n\n    @property\n    def signatures(self) -> EntityList[Signature]:\n        \"\"\"\n        Returns all the :class:`Signature` objects present in the Document.\n\n        :return: List of Signature objects.\n        :rtype: EntityList[Signature]\n        \"\"\"\n        return EntityList(sum([page.signatures for page in self.pages], []))\n\n    @property\n    def layouts(self) -> EntityList[Layout]:\n        \"\"\"\n        Returns all the :class:`Layout` objects present in the Document\n\n        :return: List of Layout objects\n        :rtype: EntityList[Layout]\n        \"\"\"\n        return EntityList(sum([page.layouts for page in self.pages], []))\n\n    @property\n    def identity_document(self) -> EntityList[IdentityDocument]:\n        \"\"\"\n        Returns all the :class:`IdentityDocument` objects present in the Page.\n\n        :return: List of IdentityDocument objects.\n        :rtype: EntityList\n        \"\"\"\n        return EntityList(self._identity_documents)\n\n    @identity_document.setter\n    def identity_document(self, identity_documents: List[IdentityDocument]):\n        \"\"\"\n        Add IdentityDocument objects to the Page.\n\n        :param tables: List of IdentityDocument objects.\n        :type identity_documents: list\n        \"\"\"\n        self._identity_document = identity_documents\n\n    @property\n    def images(self) -> List[Image.Image]:\n        \"\"\"\n        Returns all the page images in the Document.\n\n        :return: List of PIL Image objects.\n        :rtype: PIL.Image\n        \"\"\"\n        return [page.image for page in self._pages]\n\n    def get_text_and_words(self, config: TextLinearizationConfig=TextLinearizationConfig()) -> Tuple[str, List]:\n        text, words_lists = zip(*[p.get_text_and_words(config) for p in self.pages])\n        flattened_words = []\n        for words in words_lists:\n            flattened_words.extend(words)\n        return (config.layout_element_separator.join(text), flattened_words)\n\n    def page(self, page_no: int=0):\n        \"\"\"\n        Returns :class:`Page` object/s depending on the input page_no. Follows zero-indexing.\n\n        :param page_no: if int, returns single Page Object, else if list, it returns a list of\n                        Page objects.\n        :type page_no: int if single page, list of int if multiple pages\n\n        :return: Filters and returns Page objects depending on the input page_no\n        :rtype: Page or List[Page]\n        \"\"\"\n        if isinstance(page_no, int):\n            return self.pages[page_no]\n        elif isinstance(page_no, list):\n            return [self.pages[num] for num in page_no]\n        else:\n            raise InputError(\"page_no parameter doesn't match required data type.\")\n\n    def to_html(self, config: HTMLLinearizationConfig=HTMLLinearizationConfig()):\n        \"\"\"\n        Returns the HTML representation of the document, effectively calls Linearizable.to_html()\n        but add <html><body></body></html> around the result and put each page in a <div>. \n\n        :return: HTML text of the entity\n        :rtype: str\n        \"\"\"\n        html = '<html><body>'\n        for page in self.pages:\n            html += f'<div>{page.to_html(config=config)}</div>'\n        html += '</body></html>'\n        return html\n\n    def __repr__(self):\n        return os.linesep.join(['This document holds the following data:', f'Pages - {len(self.pages)}', f'Words - {len(self.words)}', f'Lines - {len(self.lines)}', f'Key-values - {len(self.key_values)}', f'Checkboxes - {len(self.checkboxes)}', f'Tables - {len(self.tables)}', f'Queries - {len(self.queries)}', f'Signatures - {len(self.signatures)}', f'Identity Documents - {len(self.identity_documents)}', f'Expense Documents - {len(self.expense_documents)}'])\n\n    def to_trp2(self):\n        \"\"\"\n        Parses the response to the trp2 format for backward compatibility\n\n        :return: TDocument object that can be used with the older Textractor libraries\n        :rtype: TDocument\n        \"\"\"\n        from trp.trp2 import TDocument, TDocumentSchema\n        if not self._trp2_document:\n            self._trp2_document = TDocumentSchema().load(self.response)\n        return self._trp2_document\n\n    def visualize(self, *args, **kwargs):\n        \"\"\"\n        Returns the object's children in a visualization EntityList object\n\n        :return: Returns an EntityList object\n        :rtype: EntityList\n        \"\"\"\n        return EntityList(self.pages).visualize(*args, **kwargs)\n\n    def keys(self, include_checkboxes: bool=True) -> List[str]:\n        \"\"\"\n        Prints all keys for key-value pairs and checkboxes if the document contains them.\n\n        :param include_checkboxes: True/False. Set False if checkboxes need to be excluded.\n        :type include_checkboxes: bool\n\n        :return: List of strings containing key names in the Document\n        :rtype: List[str]\n        \"\"\"\n        keys = []\n        keys = [keyvalue.key for keyvalue in self.key_values]\n        if include_checkboxes:\n            keys += [keyvalue.key for keyvalue in self.checkboxes]\n        return keys\n\n    def filter_checkboxes(self, selected: bool=True, not_selected: bool=True) -> List[KeyValue]:\n        \"\"\"\n        Return a list of :class:`KeyValue` objects containing checkboxes if the document contains them.\n\n        :param selected: True/False Return SELECTED checkboxes\n        :type selected: bool\n        :param not_selected: True/False Return NOT_SELECTED checkboxes\n        :type not_selected: bool\n\n        :return: Returns checkboxes that match the conditions set by the flags.\n        :rtype: EntityList[KeyValue]\n        \"\"\"\n        checkboxes = EntityList([])\n        for page in self.pages:\n            checkboxes.extend(page.filter_checkboxes(selected=selected, not_selected=not_selected))\n        return checkboxes\n\n    def get_words_by_type(self, text_type: TextTypes=TextTypes.PRINTED) -> List[Word]:\n        \"\"\"\n        Returns list of :class:`Word` entities that match the input text type.\n\n        :param text_type: TextTypes.PRINTED or TextTypes.HANDWRITING\n        :type text_type: TextTypes\n        :return: Returns list of Word entities that match the input text type.\n        :rtype: EntityList[Word]\n        \"\"\"\n        if not self.words:\n            logging.warn('Document contains no word entities.')\n            return []\n        filtered_words = EntityList()\n        for page in self.pages:\n            filtered_words.extend(page.get_words_by_type(text_type=text_type))\n        return filtered_words\n\n    def search_words(self, keyword: str, top_k: int=1, similarity_metric: SimilarityMetric=SimilarityMetric.LEVENSHTEIN, similarity_threshold: float=0.6) -> List[Word]:\n        \"\"\"\n        Return a list of top_k words that match the keyword.\n\n        :param keyword: Keyword that is used to query the document.\n        :type keyword: str\n        :param top_k: Number of closest word objects to be returned\n        :type top_k: int\n        :param similarity_metric: SimilarityMetric.COSINE, SimilarityMetric.EUCLIDEAN or SimilarityMetric.LEVENSHTEIN. SimilarityMetric.COSINE is chosen as default.\n        :type similarity_metric: SimilarityMetric\n        :param similarity_threshold: Measure of how similar document key is to queried key. default=0.6\n        :type similarity_threshold: float\n\n        :return: Returns a list of words that match the queried key sorted from highest\n                 to lowest similarity.\n        :rtype: EntityList[Word]\n        \"\"\"\n        top_n_words = []\n        for page in self.pages:\n            top_n_words.extend(page._search_words_with_similarity(keyword=keyword, top_k=top_k, similarity_metric=similarity_metric, similarity_threshold=similarity_threshold))\n        top_n_words = sorted(top_n_words, key=lambda x: x[0], reverse=True)[:top_k]\n        top_n_words = EntityList([ent[1] for ent in top_n_words])\n        return top_n_words\n\n    def search_lines(self, keyword: str, top_k: int=1, similarity_metric: SimilarityMetric=SimilarityMetric.LEVENSHTEIN, similarity_threshold: float=0.6) -> List[Line]:\n        \"\"\"\n        Return a list of top_k lines that contain the queried keyword.\n\n        :param keyword: Keyword that is used to query the document.\n        :type keyword: str\n        :param top_k: Number of closest line objects to be returned\n        :type top_k: int\n        :param similarity_metric: SimilarityMetric.COSINE, SimilarityMetric.EUCLIDEAN or SimilarityMetric.LEVENSHTEIN. SimilarityMetric.COSINE is chosen as default.\n        :type similarity_metric: SimilarityMetric\n        :param similarity_threshold: Measure of how similar document key is to queried key. default=0.6\n        :type similarity_threshold: float\n\n        :return: Returns a list of lines that contain the queried key sorted from highest\n                 to lowest similarity.\n        :rtype: EntityList[Line]\n        \"\"\"\n        if not isinstance(similarity_metric, SimilarityMetric):\n            raise InputError('similarity_metric parameter should be of SimilarityMetric type. Find input choices from textractor.data.constants')\n        top_n_lines = []\n        for page in self.pages:\n            top_n_lines.extend(page._search_lines_with_similarity(keyword=keyword, top_k=top_k, similarity_metric=similarity_metric, similarity_threshold=similarity_threshold))\n        top_n_lines = EntityList([ent[1] for ent in top_n_lines][:top_k])\n        return top_n_lines\n\n    def get(self, key: str, top_k_matches: int=1, similarity_metric: SimilarityMetric=SimilarityMetric.LEVENSHTEIN, similarity_threshold: float=0.6):\n        \"\"\"\n        Return upto top_k_matches of key-value pairs for the key that is queried from the document.\n\n        :param key: Query key to match\n        :type key: str\n        :param top_k_matches: Maximum number of matches to return\n        :type top_k_matches: int\n        :param similarity_metric: SimilarityMetric.COSINE, SimilarityMetric.EUCLIDEAN or SimilarityMetric.LEVENSHTEIN. SimilarityMetric.COSINE is chosen as default.\n        :type similarity_metric: SimilarityMetric\n        :param similarity_threshold: Measure of how similar document key is to queried key. default=0.6\n        :type similarity_threshold: float\n\n        :return: Returns a list of key-value pairs that match the queried key sorted from highest to lowest similarity.\n        :rtype: EntityList[KeyValue]\n        \"\"\"\n        if not isinstance(similarity_metric, SimilarityMetric):\n            raise InputError('similarity_metric parameter should be of SimilarityMetric type. Find input choices from textractor.data.constants')\n        top_n = []\n        similarity_threshold = -similarity_threshold if similarity_metric == SimilarityMetric.EUCLIDEAN else similarity_threshold\n        lowest_similarity = similarity_threshold\n        for kv in self.key_values + self.checkboxes:\n            try:\n                edited_document_key = ''.join([char for char in kv.key.__repr__() if char not in string.punctuation])\n            except:\n                pass\n            key = ''.join([char for char in key if char not in string.punctuation])\n            similarity = [SearchUtils.get_word_similarity(key, word, similarity_metric) for word in edited_document_key.split(' ')]\n            similarity.append(SearchUtils.get_word_similarity(key, edited_document_key, similarity_metric))\n            similarity = min(similarity) if similarity_metric == SimilarityMetric.EUCLIDEAN else max(similarity)\n            if similarity > similarity_threshold:\n                if len(top_n) < top_k_matches:\n                    top_n.append((kv, similarity))\n                elif similarity > lowest_similarity:\n                    top_n[-1] = (kv, similarity)\n                top_n = sorted(top_n, key=lambda x: x[1], reverse=True)\n                lowest_similarity = top_n[-1][1]\n        if not top_n:\n            logging.warning(f'Query key does not match any existing keys in the document.{os.linesep}{self.keys()}')\n            return EntityList([])\n        logging.info(f'Query key matched {len(top_n)} key-values in the document.')\n        return EntityList([value[0] for value in top_n])\n\n    def export_kv_to_csv(self, include_kv: bool=True, include_checkboxes: bool=True, filepath: str='Key-Values.csv', sep: str=';'):\n        \"\"\"\n        Export key-value entities and checkboxes in csv format.\n\n        :param include_kv: True if KVs are to be exported. Else False.\n        :type include_kv: bool\n        :param include_checkboxes: True if checkboxes are to be exported. Else False.\n        :type include_checkboxes: bool\n        :param filepath: Path to where file is to be stored.\n        :type filepath: str\n        :param sep: Separator to be used in the csv file.\n        :type sep: str\n        \"\"\"\n        keys = []\n        values = []\n        if include_kv and (not self.key_values):\n            logging.warning('Document does not contain key-values.')\n        elif include_kv:\n            for kv in self.key_values:\n                keys.append(' '.join([w.text for w in kv.key]))\n                values.append(kv.value.get_text())\n        if include_checkboxes and (not self.checkboxes):\n            logging.warning('Document does not contain checkbox elements.')\n        elif include_checkboxes:\n            for kv in self.checkboxes:\n                keys.append(' '.join([w.text for w in kv.key]))\n                values.append(kv.value.children[0].status.name)\n        with open(filepath, 'w') as f:\n            f.write(f'Key{sep}Value{os.linesep}')\n            for k, v in zip(keys, values):\n                f.write(f'{k}{sep}{v}{os.linesep}')\n        logging.info(f'csv file stored at location {os.path.join(os.getcwd(), filepath)}')\n\n    def export_kv_to_txt(self, include_kv: bool=True, include_checkboxes: bool=True, filepath: str='Key-Values.txt'):\n        \"\"\"\n        Export key-value entities and checkboxes in txt format.\n\n        :param include_kv: True if KVs are to be exported. Else False.\n        :type include_kv: bool\n        :param include_checkboxes: True if checkboxes are to be exported. Else False.\n        :type include_checkboxes: bool\n        :param filepath: Path to where file is to be stored.\n        :type filepath: str\n        \"\"\"\n        export_str = ''\n        index = 1\n        if include_kv and (not self.key_values):\n            logging.warning('Document does not contain key-values.')\n        elif include_kv:\n            for kv in self.key_values:\n                export_str += f'{index}. {kv.key.__repr__()} : {kv.value.__repr__()}{os.linesep}'\n                index += 1\n        if include_checkboxes and (not self.checkboxes):\n            logging.warning('Document does not contain checkbox elements.')\n        elif include_checkboxes:\n            for kv in self.checkboxes:\n                export_str += f'{index}. {kv.key.__repr__()} : {kv.value.children[0].status.name}{os.linesep}'\n                index += 1\n        with open(filepath, 'w') as text_file:\n            text_file.write(export_str)\n        logging.info(f'txt file stored at location {os.path.join(os.getcwd(), filepath)}')\n\n    def export_tables_to_excel(self, filepath):\n        \"\"\"\n        Creates an excel file and writes each table on a separate worksheet within the workbook.\n        This is stored on the filepath passed by the user.\n\n        :param filepath: Path to store the exported Excel file.\n        :type filepath: str, required\n        \"\"\"\n        if not filepath:\n            logging.error('Filepath required to store excel file.')\n        workbook = xlsxwriter.Workbook(filepath)\n        for table in self.tables:\n            workbook = table.to_excel(filepath=None, workbook=workbook, save_workbook=False)\n        workbook.close()\n\n    def independent_words(self):\n        \"\"\"\n        :return: Return all words in the document, outside of tables, checkboxes, key-values.\n        :rtype: EntityList[Word]\n        \"\"\"\n        if not self.words:\n            logging.warning('Words have not been assigned to this Document object.')\n            return []\n        else:\n            table_words = sum([table.words for table in self.tables], [])\n            kv_words = sum([kv.words for kv in self.key_values], [])\n            checkbox_words = sum([kv.words for kv in self.checkboxes], [])\n            dependent_words = table_words + checkbox_words + kv_words\n            dependent_word_ids = set([word.id for word in dependent_words])\n            independent_words = [word for word in self.words if word.id not in dependent_word_ids]\n            return EntityList(independent_words)\n\n    def return_duplicates(self):\n        \"\"\"\n        Returns a dictionary containing page numbers as keys and list of :class:`EntityList` objects as values.\n        Each :class:`EntityList` instance contains the key-values and the last item is the table which contains duplicate information.\n        This function is intended to let the Textract user know of duplicate objects extracted by the various Textract models.\n\n        :return: Dictionary containing page numbers as keys and list of EntityList objects as values.\n        :rtype: Dict[page_num, List[EntityList[DocumentEntity]]]\n        \"\"\"\n        document_duplicates = defaultdict(list)\n        for page in self.pages:\n            document_duplicates[page.page_num].extend(page.return_duplicates())\n        return document_duplicates\n\n    def directional_finder(self, word_1: str='', word_2: str='', page: int=-1, prefix: str='', direction=Direction.BELOW, entities=[]):\n        \"\"\"\n        The function returns entity types present in entities by prepending the prefix provided by te user. This helps in cases of repeating\n        key-values and checkboxes. The user can manipulate original data or produce a copy. The main advantage of this function is to be able to define direction.\n\n        :param word_1: The reference word from where x1, y1 coordinates are derived\n        :type word_1: str, required\n        :param word_2: The second word preferably in the direction indicated by the parameter direction. When it isn't given the end of page coordinates are used in the given direction.\n        :type word_2: str, optional\n        :param page: page number of the page in the document to search the entities in.\n        :type page: int, required\n        :param prefix: User provided prefix to prepend to the key . Without prefix, the method acts as a search by geometry function\n        :type prefix: str, optional\n        :param entities: List of DirectionalFinderType inputs.\n        :type entities: List[DirectionalFinderType]\n\n        :return: Returns the EntityList of modified key-value and/or checkboxes\n        :rtype: EntityList\n        \"\"\"\n        if not word_1 or page == -1:\n            return EntityList([])\n        x1, x2, y1, y2 = self._get_coords(word_1, word_2, direction, page)\n        if x1 == -1:\n            return EntityList([])\n        page_obj = self.pages[page - 1]\n        entity_dict = {DirectionalFinderType.KEY_VALUE_SET: self.key_values, DirectionalFinderType.SELECTION_ELEMENT: self.checkboxes}\n        entitylist = []\n        for entity_type in entities:\n            entitylist.extend(list(entity_dict[entity_type]))\n        new_key_values = self._get_kv_with_direction(direction, entitylist, (x1, x2, y1, y2))\n        final_kv = []\n        for kv in new_key_values:\n            if kv.key:\n                key_words = [deepcopy(word) for word in kv.key]\n                key_words[0].text = prefix + key_words[0].text\n                new_kv = deepcopy(kv)\n                new_kv.key = key_words\n                final_kv.append(new_kv)\n            else:\n                final_kv.append(kv)\n        return EntityList(final_kv)\n\n    def _get_kv_with_direction(self, direction, entitylist, coords):\n        \"\"\"Return key-values and checkboxes in entitylist present in the direction given with respect to the coordinates.\"\"\"\n        if direction == Direction.ABOVE:\n            new_key_values = [kv for kv in entitylist if kv.bbox.y <= coords[2] and kv.bbox.y >= coords[-1]]\n        elif direction == Direction.BELOW:\n            new_key_values = [kv for kv in entitylist if kv.bbox.y >= coords[2] and kv.bbox.y <= coords[-1]]\n        elif direction == Direction.RIGHT:\n            new_key_values = [kv for kv in entitylist if kv.bbox.x >= coords[0] and kv.bbox.x <= coords[1]]\n            new_key_values = [kv for kv in new_key_values if kv.bbox.y >= coords[2] - kv.bbox.height and kv.bbox.y <= coords[-1] + 3 * kv.bbox.height]\n        elif direction == Direction.LEFT:\n            new_key_values = [kv for kv in entitylist if kv.bbox.x <= coords[0] and kv.bbox.x >= coords[1]]\n            new_key_values = [kv for kv in new_key_values if kv.bbox.y >= coords[2] - kv.bbox.height and kv.bbox.y <= coords[-1] + 3 * kv.bbox.height]\n        return new_key_values\n\n    def _get_coords(self, word_1, word_2, direction, page):\n        \"\"\"\n        Returns coordinates for the area within which to search for key-values with the directional_finder by retrieving coordinates of word_1         and word_2 if it exists else end of page.\n        \"\"\"\n        word_1_objects = self.search_lines(keyword=word_1, top_k=5, similarity_metric=SimilarityMetric.COSINE, similarity_threshold=0.5)\n        word_1_objects = [word for word in word_1_objects if word.page == page] if page != -1 else []\n        if not word_1_objects:\n            logging.warning(f'{word_1} not found in page {page}')\n            return (-1, -1, -1, -1)\n        else:\n            word_1_obj = word_1_objects[0]\n            x1, y1 = (word_1_obj.bbox.x, word_1_obj.bbox.y)\n        if word_2:\n            word_2_objects = self.search_lines(keyword=word_2, top_k=5, similarity_metric=SimilarityMetric.COSINE, similarity_threshold=0.5)\n            word_2_objects = [word for word in word_2_objects if word.page == page]\n            if not word_2_objects:\n                logging.warning(f'{word_2} not found in page {page}')\n                return (-1, -1, -1, -1)\n            else:\n                word_2_obj = word_2_objects[0]\n                x2, y2 = (word_2_obj.bbox.x, word_2_obj.bbox.y)\n        else:\n            x2, y2 = (x1, y1)\n        if direction == Direction.ABOVE:\n            x1, x2, y1, y2 = (x1, x2, y1, y2) if y2 < y1 else (x1, 0, y1, 0)\n        elif direction == Direction.BELOW:\n            x1, x2, y1, y2 = (x1, x2, y1, y2) if y2 > y1 else (x1, 1, y1, 1)\n        elif direction == Direction.RIGHT:\n            x1, x2, y1, y2 = (x1, x2, y1, y2) if x2 > x1 else (x1, 1, y1, y1)\n        elif direction == Direction.LEFT:\n            x1, x2, y1, y2 = (x1, x2, y1, y2) if x2 < x1 else (x1, 0, y1, y1)\n        else:\n            return (-1, -1, -1, -1)\n        return (x1, x2, y1, y2)",
    "textractor/parsers/response_parser.py": "\"\"\"\nConsumes Textract JSON response and converts them to a Document object format.\nThis class contains all the necessary utilities to create entity objects from JSON blocks within the response.\nUse ResponseParser's parse function to handle API response and convert them to Document objects.\n\"\"\"\nimport logging\nimport uuid\nfrom copy import deepcopy\nfrom typing import Any, List, Dict, Tuple\nfrom collections import defaultdict\nfrom textractor.entities.identity_document import IdentityDocument\nfrom textractor.entities.expense_document import ExpenseDocument\nfrom textractor.entities.expense_field import Expense, ExpenseField, ExpenseType, ExpenseGroupProperty, LineItemGroup, LineItemRow\nfrom textractor.entities.page import Page\nfrom textractor.entities.query_result import QueryResult\nfrom textractor.entities.signature import Signature\nfrom textractor.entities.word import Word\nfrom textractor.entities.line import Line\nfrom textractor.entities.value import Value\nfrom textractor.entities.table import Table\nfrom textractor.entities.bbox import BoundingBox\nfrom textractor.entities.document import Document\nfrom textractor.entities.key_value import KeyValue\nfrom textractor.entities.table_cell import TableCell\nfrom textractor.entities.table_title import TableTitle\nfrom textractor.entities.table_footer import TableFooter\nfrom textractor.entities.query import Query\nfrom textractor.entities.selection_element import SelectionElement\nfrom textractor.entities.layout import Layout\nfrom textractor.data.constants import LAYOUT_ENTITY, LAYOUT_FIGURE, TABLE_FOOTER, TABLE_TITLE, COLUMN_HEADER, TABLE_SUMMARY, TABLE_SECTION_TITLE, TABLE_STRUCTURED, TABLE_SEMI_STRUCTURED, SelectionStatus, TextTypes, TableTypes, HANDWRITING, PRINTED, WORD, LINE, KEY_VALUE_SET, CELL, TABLE, SELECTION_ELEMENT, PAGE, MERGED_CELL, QUERY, SIGNATURE, LAYOUT, LAYOUT_LIST, LAYOUT_TABLE, LAYOUT_KEY_VALUE\nfrom textractor.utils.legacy_utils import converter\nTHRESHOLD = 0.95\n\ndef _create_document_object(response: dict) -> Document:\n    \"\"\"\n    Consumes API Response in JSON format and creates a Document object.\n\n    :param response: json response from Textract API\n    :type response: dict\n\n    :return: Returns a Document object populated with metadata on number of pages.\n    :rtype: Document\n    \"\"\"\n    doc = Document(num_pages=response['DocumentMetadata']['Pages'])\n    return doc\n\ndef _filter_block_type(response: dict, entity: str) -> List[Dict[str, Any]]:\n    \"\"\"\n    Consumes entire JSON response, filters and returns list of blocks corresponding to the entity\n    parameter from API response JSON.\n\n    :param response: JSON response from Textract API\n    :type response: dict\n    :param entity: Entity to be extracted from the JSON response\n    :type entity: str\n\n    :return: Returns a list of JSON blocks that match entity parameter.\n    :rtype: List\n    \"\"\"\n    return [block for block in response['Blocks'] if block['BlockType'] == entity]\n\ndef _filter_by_entity(block_json: List[Dict[str, Any]], entity_type: str) -> Dict[str, Any]:\n    \"\"\"\n    Filters and returns dictionary of blocks corresponding to the entity_type from API response JSON.\n\n    :param block_json: list of blocks belonging to a specific entity\n    :type block_json: List[Dict[str, Any]]\n    :param entity_type: EntityType used to select/filter from list of blocks\n    :type entity_type: str\n\n    :return: Dictionary mapping of block ID with JSON block for entity type.\n    :rtype: Dict[str, Any]\n    \"\"\"\n    return {block['Id']: block for block in block_json if 'EntityTypes' in block and len(block['EntityTypes']) and (block['EntityTypes'][0] == entity_type)}\n\ndef _get_relationship_ids(block_json: Dict[str, Any], relationship: str) -> List[str]:\n    \"\"\"\n    Takes the JSON block corresponding to an entity and returns the Ids of the chosen Relationship if the Relationship exists.\n\n    :param block_json: JSON block corresponding to an entity\n    :type block_json: List[Dict[str, Any]]\n    :relationship: CHILD or VALUE as input\n    :type relationship: str\n\n    :return: List of IDs with type Relationship to entity\n    :rtype: List\n    \"\"\"\n    ids = []\n    try:\n        ids = [rel['Ids'] for rel in block_json['Relationships'] if rel['Type'] == relationship][0]\n    except:\n        logging.info(f'{block_json['BlockType']} - {block_json['Id']} does not have ids with {relationship} relationship.')\n    return ids\n\ndef _create_page_objects(response: dict) -> Tuple[Dict[str, Page], List[Dict[str, Any]]]:\n    \"\"\"\n    Consumes API Response in JSON format and returns Page objects for the Document.\n\n    :param response: JSON response from Textract API\n    :type response: dict\n\n    :return: Returns dictionary with page ID - Page object mapping,  list of JSON blocks belonging to PAGE blocks.\n    :rtype: Dict[str, Page], List[str]\n    \"\"\"\n    pages = []\n    page_elements = _filter_block_type(response, entity=PAGE)\n    for page_json in page_elements:\n        asset_id = page_json['Id']\n        width = page_json['Geometry']['BoundingBox']['Width']\n        height = page_json['Geometry']['BoundingBox']['Height']\n        page_num = page_json['Page'] if len(page_elements) > 1 else 1\n        page_children = _get_relationship_ids(page_json, relationship='CHILD')\n        page = Page(id=asset_id, width=width, height=height, page_num=page_num, child_ids=page_children)\n        pages.append(page)\n    pages = {page.id: page for page in pages}\n    return (pages, page_elements)\n\ndef _create_word_objects(word_ids: List[str], id_json_map: Dict[str, str], existing_words: Dict[str, Word], page: Page) -> List[Word]:\n    \"\"\"\n    Creates list of Word objects for all word_ids passed to the function.\n\n    :param word_ids: List of ids corresponding to the words present within Page.\n    :type word_ids: list\n    :param id_json_map: Dictionary containing entity_id: JSON block mapping.\n    :type id_json_map: dict\n    :param page: Instance of parent Page object.\n    :type page: Page\n\n    :return: Returns a list of Word objects for the IDs passed in word_ids.\n    :rtype: list\n    \"\"\"\n    words = []\n    text_type = {PRINTED: TextTypes.PRINTED, HANDWRITING: TextTypes.HANDWRITING}\n    for word_id in word_ids:\n        if word_id in existing_words:\n            words.append(existing_words[word_id])\n        else:\n            if not word_id in id_json_map:\n                continue\n            elem = id_json_map[word_id]\n            word = Word(entity_id=elem['Id'], bbox=BoundingBox.from_normalized_dict(elem['Geometry']['BoundingBox'], spatial_object=page), text=elem.get('Text'), text_type=text_type[elem.get('TextType')], confidence=elem['Confidence'])\n            word.raw_object = elem\n            words.append(word)\n            existing_words[word_id] = word\n    for word in words:\n        word.page = page.page_num\n        word.page_id = page.id\n    return words\n\ndef _create_line_objects(line_ids: List[str], id_json_map: Dict[str, str], existing_words: Dict[str, Word], page: Page) -> Tuple[List[Line], List[Word]]:\n    \"\"\"\n    Creates list of Line objects for all lines in the Page derived from the API JSON response.\n\n    :param line_ids: List of IDs corresponding to the lines present within Page.\n    :type line_ids: list\n    :param id_json_map: Dictionary containing entity_id: JSON block mapping.\n    :type id_json_map: dict\n    :param page: Instance of parent Page object.\n    :type page: Page\n\n    :return: Returns a list of Line objects for the IDs passed in line_ids and list of Word objects\n             belonging to the corresponding Line objects.\n    :rtype: List[Line], List[Word]\n    \"\"\"\n    page_lines = []\n    for line_id in line_ids:\n        if line_id in page.child_ids:\n            page_lines.append(id_json_map[line_id])\n    lines = []\n    page_words = []\n    for line in page_lines:\n        if _get_relationship_ids(line, relationship='CHILD'):\n            line_words = _create_word_objects(_get_relationship_ids(line, relationship='CHILD'), id_json_map, existing_words, page)\n            page_words.extend(line_words)\n            lines.append(Line(entity_id=line['Id'], bbox=BoundingBox.from_normalized_dict(line['Geometry']['BoundingBox'], spatial_object=page), words=line_words, confidence=line['Confidence']))\n            for word in line_words:\n                word.line = lines[-1]\n                word.line_id = lines[-1].id\n                word.line_bbox = lines[-1].bbox\n            lines[-1]._children = line_words\n            lines[-1].raw_object = line\n    for line in lines:\n        line.page = page.page_num\n        line.page_id = page.id\n    return (lines, page_words)\n\ndef _create_selection_objects(selection_ids: List[str], id_json_map: Dict[str, Any], page: Page) -> Dict[str, SelectionElement]:\n    \"\"\"\n    Creates dictionary mapping of SelectionElement ID with SelectionElement objects for all ids passed in selection_ids.\n\n    :param selection_ids: List of ids corresponding to the SelectionElements.\n    :type selection_ids: list\n    :param id_json_map: Dictionary containing entity_id: JSON block mapping.\n    :type id_json_map: dict\n    :param page: Instance of parent Page object.\n    :type page: Page\n\n    :return: Returns a dictionary mapping of SelectionElement IDs with SelectionElement objects for the IDs present in\n             selection_ids.\n    :rtype: Dict[str, SelectionElement]\n    \"\"\"\n    checkbox_elements = [id_json_map[selection_id] for selection_id in selection_ids]\n    status = {'SELECTED': SelectionStatus.SELECTED, 'NOT_SELECTED': SelectionStatus.NOT_SELECTED}\n    checkboxes = {}\n    for block in checkbox_elements:\n        checkboxes[block['Id']] = SelectionElement(entity_id=block['Id'], bbox=BoundingBox.from_normalized_dict(block['Geometry']['BoundingBox'], spatial_object=page), status=status[block['SelectionStatus']], confidence=block['Confidence'])\n        checkboxes[block['Id']].raw_object = block\n    for c in checkboxes.values():\n        c.page = page.page_num\n        c.page_id = page.id\n    return checkboxes\n\ndef _create_value_objects(value_ids: List[str], id_json_map: Dict[str, Any], entity_id_map: Dict[str, list], existing_words: Dict[str, Word], page: Page) -> Tuple[Dict[str, Value], Dict[str, SelectionElement]]:\n    \"\"\"\n    Creates dictionary containing Value objects for all value_ids in the Page derived from the API response JSON.\n\n    :param value_ids: List of ids corresponding to the Values in the page.\n    :type value_ids: list\n    :param id_json_map: Dictionary containing entity_id:JSON block mapping.\n    :type id_json_map: dict\n    :param entity_id_map: Dictionary containing entity_type:List[entity_id] mapping.\n    :type entity_id_map: dict\n    :param page: Instance of parent Page object.\n    :type page: Page\n\n    :return: Dictionary mapping value_ids to Value objects.\n    :rtype: Dict[str, Value]\n    \"\"\"\n    values_info = {value_id: id_json_map.get(value_id, None) for value_id in value_ids}\n    values = {}\n    for block_id, block in values_info.items():\n        if block is None:\n            continue\n        values[block_id] = Value(entity_id=block_id, bbox=BoundingBox.from_normalized_dict(block['Geometry']['BoundingBox'], spatial_object=page), confidence=block['Confidence'])\n        values[block_id].raw_object = block\n    checkboxes = _create_selection_objects(entity_id_map[SELECTION_ELEMENT], id_json_map, page)\n    for val_id in values.keys():\n        val_child_ids = _get_relationship_ids(values_info[val_id], relationship='CHILD')\n        for child_id in val_child_ids:\n            if child_id not in id_json_map:\n                continue\n            if id_json_map[child_id]['BlockType'] == WORD:\n                words = _create_word_objects([child_id], id_json_map, existing_words, page)\n                values[val_id].words += words\n                values[val_id].add_children(words)\n            elif id_json_map[child_id]['BlockType'] == SIGNATURE:\n                continue\n            else:\n                checkbox = checkboxes[child_id]\n                checkbox.value_id = val_id\n                values[val_id].add_children([checkbox])\n                values[val_id].contains_checkbox = True\n            values[val_id].page = page.page_num\n            values[val_id].page_id = page.id\n    return (values, checkboxes)\n\ndef _create_query_objects(query_ids: List[str], id_json_map: Dict[str, str], entity_id_map: Dict[str, list], page: Page) -> List[Query]:\n    page_queries = []\n    for query_id in query_ids:\n        if query_id in page.child_ids:\n            page_queries.append(id_json_map[query_id])\n    query_result_id_map = {}\n    for block in page_queries:\n        answer = _get_relationship_ids(block, relationship='ANSWER')\n        query_result_id_map[block['Id']] = answer[0] if answer else None\n    query_results = _create_query_result_objects(list(query_result_id_map.values()), id_json_map, entity_id_map, page)\n    queries = []\n    for query in page_queries:\n        query_result = query_results.get(query_result_id_map[query['Id']])\n        query_obj = Query(query['Id'], query['Query']['Text'], query['Query'].get('Alias'), query_result, query_result.bbox if query_result is not None else None)\n        query_obj.raw_object = query\n        queries.append(query_obj)\n    return queries\n\ndef _create_query_result_objects(query_result_ids: List[str], id_json_map: Dict[str, str], entity_id_map: Dict[str, list], page: Page) -> Dict[str, QueryResult]:\n    page_query_results = []\n    for query_result_id in query_result_ids:\n        if query_result_id in page.child_ids and query_result_id in id_json_map:\n            page_query_results.append(id_json_map[query_result_id])\n    query_results = {}\n    for block in page_query_results:\n        query_results[block['Id']] = QueryResult(entity_id=block['Id'], confidence=block['Confidence'], result_bbox=BoundingBox.from_normalized_dict((block.get('Geometry') or {'BoundingBox': {'Width': 1.0, 'Height': 1.0, 'Left': 0.0, 'Top': 0.0}})['BoundingBox'], spatial_object=page), answer=block['Text'])\n        query_results[block['Id']].raw_object = block\n    for query_result_id, query_result in query_results.items():\n        query_result.page = page.page_num\n        query_result.page_id = page.id\n    return query_results\n\ndef _create_signature_objects(signature_ids: List[str], id_json_map: Dict[str, str], entity_id_map: Dict[str, list], page: Page) -> Dict[str, Signature]:\n    page_signatures = []\n    for signature_id in signature_ids:\n        if signature_id in page.child_ids:\n            page_signatures.append(id_json_map[signature_id])\n    signatures = {}\n    for block in page_signatures:\n        signatures[block['Id']] = Signature(entity_id=block['Id'], confidence=block['Confidence'], bbox=BoundingBox.from_normalized_dict(block['Geometry']['BoundingBox'], spatial_object=page))\n        signatures[block['Id']].raw_object = block\n    for signature_id, signature in signatures.items():\n        signature.page = page.page_num\n        signature.page_id = page.id\n    signatures_added = set()\n    for layout in sorted(page.leaf_layouts, key=lambda x: x.bbox.y):\n        if layout.layout_type == LAYOUT_ENTITY:\n            continue\n        for signature in sorted(signatures.values(), key=lambda x: x.bbox.y):\n            if layout.bbox.get_intersection(signature.bbox).area > THRESHOLD * signature.bbox.area and signature not in signatures_added:\n                layout.children.append(signature)\n                signatures_added.add(signature)\n                del signatures[signature.id]\n    signature_layouts = []\n    for signature in signatures.values():\n        if signature not in signatures_added:\n            signatures_added.add(signature)\n            layout = Layout(entity_id=str(uuid.uuid4()), bbox=signature.bbox, label=LAYOUT_ENTITY, reading_order=-1)\n            layout.children.append(signature)\n            layout.page = page.page_num\n            layout.page_id = page.id\n            signature_layouts.append(layout)\n    layouts_to_remove = []\n    for layout in page.leaf_layouts:\n        layouts_that_intersect = []\n        for signature_layout in signature_layouts:\n            intersection = layout.bbox.get_intersection(signature_layout.bbox).area\n            if intersection:\n                layouts_that_intersect.append(signature_layout)\n        words_in_sub_layouts = set()\n        for i, intersect_layout in enumerate(sorted(layouts_that_intersect, key=lambda l: (l.bbox.y, l.bbox.x))):\n            intersect_layout.reading_order = layout.reading_order + (i + 1) * 0.1 if intersect_layout.reading_order == -1 else min(intersect_layout.reading_order, layout.reading_order + (i + 1) * 0.1)\n            for w in intersect_layout.children[0].words:\n                words_in_sub_layouts.add(w.id)\n        if words_in_sub_layouts:\n            remaining_words = []\n            for w in layout.words:\n                if w.id not in words_in_sub_layouts:\n                    remaining_words.append(w)\n            if remaining_words:\n                layout.bbox = BoundingBox.enclosing_bbox([w.bbox for w in remaining_words])\n                layout._children = list(set([w.line for w in remaining_words]))\n            else:\n                layouts_to_remove.append(layout)\n    for layout in layouts_to_remove:\n        page.leaf_layouts.remove(layout)\n    for layout in signature_layouts:\n        page.leaf_layouts.append(layout)\n    return list(signatures_added)\n\ndef _create_keyvalue_objects(key_value_ids: List[str], id_json_map: Dict[str, Any], id_entity_map: Dict[str, str], entity_id_map: Dict[str, list], existing_words: Dict[str, Word], page: Page) -> Tuple[List[KeyValue], List[Word], Dict[str, SelectionElement]]:\n    \"\"\"\n    Creates list of KeyValue objects for all key-value pairs in the Page derived from the API response JSON.\n\n    :param key_value_ids: List of ids corresponding to the KeyValues in the page.\n    :type key_value_ids: list\n    :param id_json_map: Dictionary containing entity_id:JSON block mapping.\n    :type id_json_map: dict\n    :param entity_id_map: Dictionary containing entity_type:List[entity_id] mapping.\n    :type entity_id_map: dict\n    :param id_entity_map: Dictionary containing entity_id:entity_type mapping.\n    :type id_entity_map: dict\n    :param page: Instance of parent Page object.\n    :type page: Page\n\n    :return: Returns a list of KeyValue objects and list of Word objects with CHILD relationship\n             to the KeyValue objects.\n    :rtype: List[KeyValue], List[Word]\n    \"\"\"\n    page_kv = []\n    for kv_id in key_value_ids:\n        if kv_id in page.child_ids:\n            page_kv.append(id_json_map[kv_id])\n    keys_info = _filter_by_entity(page_kv, entity_type='KEY')\n    key_value_id_map = {block['Id']: _get_relationship_ids(block, relationship='VALUE')[0] for block in keys_info.values()}\n    values, selection_elements = _create_value_objects(list(key_value_id_map.values()), id_json_map, entity_id_map, existing_words, page)\n    keys = {}\n    for block in keys_info.values():\n        keys[block['Id']] = KeyValue(entity_id=block['Id'], bbox=BoundingBox.from_normalized_dict(block['Geometry']['BoundingBox'], spatial_object=page), contains_checkbox=key_value_id_map[block['Id']] in values and values[key_value_id_map[block['Id']]].contains_checkbox, value=values.get(key_value_id_map[block['Id']]), confidence=block['Confidence'])\n        keys[block['Id']].raw_object = block\n    kv_words = []\n    for key_id in keys.keys():\n        if keys[key_id].value is None:\n            continue\n        keys[key_id].value.key_id = key_id\n        if keys[key_id].contains_checkbox:\n            keys[key_id].value.children[0].key_id = key_id\n            keys[key_id].selection_status = [c for c in keys[key_id].value.children if c.__class__.__name__ == 'SelectionElement'][0].status\n        else:\n            kv_words.extend(values[key_value_id_map[key_id]].words)\n        key_child_ids = _get_relationship_ids(keys_info[key_id], relationship='CHILD')\n        key_word_ids = [child_id for child_id in key_child_ids if child_id in id_json_map and id_json_map[child_id]['BlockType'] == WORD]\n        key_words = _create_word_objects(key_word_ids, id_json_map, existing_words, page)\n        key_child_ids = [child_id for child_id in key_child_ids if child_id not in key_word_ids]\n        keys[key_id].key = key_words\n        keys[key_id].add_children(key_words)\n        keys[key_id].add_children([keys[key_id].value])\n        kv_words.extend(key_words)\n    key_values = list(keys.values())\n    for kv in key_values:\n        kv.bbox = BoundingBox.enclosing_bbox([kv.bbox] + ([kv.value.bbox] if kv.value is not None else []))\n        kv.page = page.page_num\n        kv.page_id = page.id\n    return (key_values, kv_words, selection_elements)\n\ndef _create_layout_objects(layout_ids: List[Any], id_json_map: Dict[str, str], id_entity_map: Dict[str, List[str]], line_by_id: Dict[str, Line], page: Page) -> Tuple[List[Layout], List[Layout]]:\n    \"\"\"\n    Creates Layout objects.\n\n    :param page_layouts: Reading-ordered list containing JSON structure of tables within the page.\n    :type page_layouts: list\n    :param id_json_map: Dictionary containing entity_id:JSON block mapping.\n    :type id_json_map: dict\n    :param id_entity_map: Dictionary containing entity_id:entity_type mapping.\n    :type id_entity_map: dict\n    :param page: Instance of parent Page object.\n    :type page: Page\n\n    :return: Returns a list containing Layout objects.\n    :rtype: List[Layout]\n    \"\"\"\n    page_layouts = []\n    for layout_id in layout_ids:\n        if layout_id in page.child_ids:\n            page_layouts.append(id_json_map[layout_id])\n    leaf_layouts = []\n    container_layouts = []\n    parsed_blocks = set()\n    for i, block in enumerate(page_layouts):\n        if block['Id'] in parsed_blocks:\n            continue\n        if block['BlockType'] in (LAYOUT_LIST,):\n            container_layouts.append(Layout(entity_id=block['Id'], confidence=block['Confidence'], reading_order=i, label=block['BlockType'], bbox=BoundingBox.from_normalized_dict(block['Geometry']['BoundingBox'], spatial_object=page)))\n            parsed_blocks.add(block['Id'])\n            for relationship in block.get('Relationships', []) or []:\n                if relationship['Type'] != 'CHILD':\n                    continue\n                for leaf_id in relationship['Ids']:\n                    block = id_json_map[leaf_id]\n                    parsed_blocks.add(leaf_id)\n                    container_layouts[-1].children.append(Layout(entity_id=block['Id'], confidence=block['Confidence'], reading_order=i, label=block['BlockType'], bbox=BoundingBox.from_normalized_dict(block['Geometry']['BoundingBox'], spatial_object=page)))\n                    container_layouts[-1].children[-1].raw_object = block\n                    for relationship in block.get('Relationships', []) or []:\n                        if relationship['Type'] != 'CHILD':\n                            continue\n                        container_layouts[-1].children[-1].add_children([line_by_id[line_id] for line_id in relationship['Ids'] if line_id in line_by_id])\n        else:\n            leaf_layouts.append(Layout(entity_id=block['Id'], confidence=block['Confidence'], reading_order=i, label=block['BlockType'], bbox=BoundingBox.from_normalized_dict(block['Geometry']['BoundingBox'], spatial_object=page)))\n            leaf_layouts[-1].raw_object = block\n            for relationship in block.get('Relationships', []) or []:\n                if relationship['Type'] != 'CHILD':\n                    continue\n                leaf_layouts[-1].add_children([line_by_id[line_id] for line_id in relationship['Ids'] if line_id in line_by_id])\n    for layout in leaf_layouts + container_layouts:\n        layout.page = page.page_num\n        layout.page_id = page.id\n    return (container_layouts, leaf_layouts)\n\ndef _create_table_cell_objects(page_tables: List[Any], id_entity_map: Dict[str, List[str]], id_json_map: Dict[str, str], page: Page) -> Tuple[Dict[str, TableCell], Dict[str, Any]]:\n    \"\"\"\n    Creates TableCell objects for all page_tables passed as input present on a single Page of the Document.\n\n    :param page_tables: List containing JSON structure of tables within the page.\n    :type page_tables: list\n    :param id_entity_map: Dictionary containing entity_id:entity_type mapping.\n    :type id_entity_map: dict\n    :param id_json_map: Dictionary containing entity_id:JSON block mapping.\n    :type id_json_map: dict\n    :param page: Instance of parent Page object.\n    :type page: Page\n\n    :return: Returns a dictionary containing TableCells mapped with their IDs and dictionary containing ID: CELL JSON mapping.\n    :rtype: Dict[str, TableCell], Dict[str, Any]\n    \"\"\"\n    all_table_cells_info = {}\n    for table in page_tables:\n        for cell_id in _get_relationship_ids(table, relationship='CHILD'):\n            if cell_id in id_entity_map and id_entity_map[cell_id] == CELL:\n                all_table_cells_info[cell_id] = id_json_map[cell_id]\n    table_cells = {}\n    for elem_id, elem in all_table_cells_info.items():\n        entity_types = elem.get('EntityTypes', []) or []\n        table_cells[elem_id] = TableCell(entity_id=elem_id, bbox=BoundingBox.from_normalized_dict(elem['Geometry']['BoundingBox'], spatial_object=page), row_index=elem['RowIndex'], col_index=elem['ColumnIndex'], row_span=elem['RowSpan'], col_span=elem['ColumnSpan'], confidence=elem['Confidence'], is_column_header=COLUMN_HEADER in entity_types, is_title=TABLE_TITLE in entity_types, is_footer=TABLE_FOOTER in entity_types, is_summary=TABLE_SUMMARY in entity_types, is_section_title=TABLE_SECTION_TITLE in entity_types)\n        table_cells[elem_id].raw_object = elem\n    for cell in table_cells.values():\n        cell.page = page.page_num\n        cell.page_id = page.id\n    return (table_cells, all_table_cells_info)\n\ndef _create_table_objects(table_ids: List[str], id_json_map: Dict[str, Any], id_entity_map: Dict[str, List[str]], entity_id_map: Dict[str, List[str]], existing_words: Dict[str, Word], key_values: Dict[str, KeyValue], checkboxes: Dict[str, SelectionElement], page: Page) -> Tuple[List[Table], List[Word]]:\n    \"\"\"\n    Creates list of Table objects for all tables in the Page derived from the API response JSON.\n    This includes creating TableCell objects and updating metadata for each cell. The TableCell objects are assigned as children\n    of the table.\n\n    :param table_ids: List of ids corresponding to the Tables in the page.\n    :type table_ids: list\n    :param id_json_map: Dictionary containing entity_id:JSON block mapping.\n    :type id_json_map: dict\n    :param id_entity_map: Dictionary containing entity_id:entity_type mapping.\n    :type id_entity_map: dict\n    :param entity_id_map: Dictionary containing entity_type:List[entity_id] mapping.\n    :type entity_id_map: dict\n    :param page: Instance of parent Page object.\n    :type page: Page\n\n    :return: Returns a list of table objects and list of words present in tables.\n    :rtype: List[Table], List[Word]\n    \"\"\"\n    page_tables = []\n    for table_id in table_ids:\n        if table_id in page.child_ids:\n            page_tables.append(id_json_map[table_id])\n    tables = {}\n    for val in page_tables:\n        tables[val['Id']] = Table(entity_id=val['Id'], bbox=BoundingBox.from_normalized_dict(val['Geometry']['BoundingBox'], spatial_object=page))\n        if TABLE_STRUCTURED in (val.get('EntityTypes', []) or []):\n            tables[val['Id']].table_type = TableTypes.STRUCTURED\n        elif TABLE_SEMI_STRUCTURED in (val.get('EntityTypes', []) or []):\n            tables[val['Id']].table_type = TableTypes.SEMI_STRUCTURED\n        else:\n            tables[val['Id']].table_type = TableTypes.UNKNOWN\n        tables[val['Id']].raw_object = val\n    table_cells, all_table_cells_info = _create_table_cell_objects(page_tables, id_entity_map, id_json_map, page)\n    merged_table_cells = [id_json_map[merge_id] for merge_id in entity_id_map[MERGED_CELL]]\n    merged_child_map = {merged_cell['Id']: _get_relationship_ids(merged_cell, relationship='CHILD') for merged_cell in merged_table_cells}\n    merged_child_ids = sum([ids for ids in merged_child_map.values()], [])\n    table_words = []\n    added_key_values = set()\n    for cell_id, cell in all_table_cells_info.items():\n        children = _get_relationship_ids(cell, relationship='CHILD')\n        cell_word_ids = [child_id for child_id in children if child_id in id_entity_map and id_entity_map[child_id] == WORD]\n        selection_ids = [child_id for child_id in children if child_id in id_entity_map and id_entity_map[child_id] == SELECTION_ELEMENT]\n        cell_words = _create_word_objects(cell_word_ids, id_json_map, existing_words, page)\n        for w in cell_words:\n            w.cell_id = table_cells[cell_id].id\n            w.cell_bbox = table_cells[cell_id].bbox\n            w.row_span = table_cells[cell_id].row_span\n            w.col_span = table_cells[cell_id].col_span\n            w.row_index = table_cells[cell_id].row_index\n            w.col_index = table_cells[cell_id].col_index\n        table_words.extend(cell_words)\n        table_cells[cell_id].add_children(cell_words)\n        for child_id in selection_ids:\n            if checkboxes[child_id].key_id in added_key_values:\n                continue\n            if checkboxes[child_id].key_id is not None:\n                kv = key_values[checkboxes[child_id].key_id]\n                try:\n                    if not kv.words:\n                        added_key_values.add(kv.id)\n                        continue\n                    i = table_cells[cell_id]._children.index(kv.words[0])\n                    table_cells[cell_id]._children.insert(i, kv)\n                    for w in kv.words:\n                        try:\n                            table_cells[cell_id]._children.remove(w)\n                        except ValueError:\n                            continue\n                    added_key_values.add(checkboxes[child_id].key_id)\n                except ValueError:\n                    continue\n            else:\n                table_cells[cell_id]._children.append(checkboxes[child_id])\n        meta_info = cell.get('EntityTypes', []) or []\n        merged_info = [MERGED_CELL] if cell_id in merged_child_ids else []\n        table_cells[cell_id]._update_response_metadata(meta_info + merged_info)\n    for kv_id, kv in key_values.items():\n        if kv_id in added_key_values:\n            continue\n        for table in page_tables:\n            table = tables[table['Id']]\n            if all([w in table_words for w in kv.words]):\n                added_key_values.add(kv_id)\n    for merge_id, child_cells in merged_child_map.items():\n        for child_id in child_cells:\n            if child_id in table_cells.keys():\n                table_cells[child_id].parent_cell_id = merge_id\n                table_cells[child_id].siblings = [table_cells[cid] for cid in child_cells if cid in table_cells]\n    for table in page_tables:\n        children = _get_relationship_ids(table, relationship='TABLE_TITLE')\n        for child_id in children:\n            if child_id not in id_json_map:\n                continue\n            tables[table['Id']].title = TableTitle(entity_id=child_id, bbox=BoundingBox.from_normalized_dict(id_json_map[child_id]['Geometry']['BoundingBox'], spatial_object=page))\n            children = _get_relationship_ids(id_json_map[child_id], relationship='CHILD')\n            tables[table['Id']].title.words = _create_word_objects([child_id for child_id in children if child_id in id_entity_map and id_entity_map[child_id] == WORD], id_json_map, existing_words, page)\n    for table in page_tables:\n        children = _get_relationship_ids(table, relationship='TABLE_FOOTER')\n        for child_id in children:\n            if child_id not in id_json_map:\n                continue\n            tables[table['Id']].footers.append(TableFooter(entity_id=child_id, bbox=BoundingBox.from_normalized_dict(id_json_map[child_id]['Geometry']['BoundingBox'], spatial_object=page)))\n            children = _get_relationship_ids(id_json_map[child_id], relationship='CHILD')\n            tables[table['Id']].footers[-1].words = _create_word_objects([child_id for child_id in children if child_id in id_entity_map and id_entity_map[child_id] == WORD], id_json_map, existing_words, page)\n    for table in page_tables:\n        children = _get_relationship_ids(table, relationship='CHILD')\n        children_cells = []\n        for child_id in children:\n            if child_id not in table_cells:\n                continue\n            children_cells.append(table_cells[child_id])\n            if table_cells[child_id].is_title and tables[table['Id']].title is not None:\n                tables[table['Id']].title.is_floating = False\n        words = set()\n        for child_id in children:\n            if child_id not in table_cells:\n                continue\n            for w in table_cells[child_id].words:\n                words.add(w.id)\n        for footer in tables[table['Id']].footers:\n            for w in footer.words:\n                if w.id in words:\n                    footer.is_floating = False\n                    break\n        tables[table['Id']].add_cells(children_cells)\n        tables[table['Id']].add_children(children_cells)\n    table_added = set()\n    for layout in sorted(page.leaf_layouts, key=lambda x: x.bbox.y):\n        if layout.layout_type == LAYOUT_TABLE:\n            for table in sorted(list(tables.values()), key=lambda x: x.bbox.y):\n                if layout.bbox.get_intersection(table.bbox).area > THRESHOLD * table.bbox.area and table not in table_added:\n                    for w in table.words:\n                        layout.remove(w)\n                    layout.children.append(table)\n                    layout.bbox = BoundingBox.enclosing_bbox(layout.children)\n                    table_added.add(table)\n    tables_layout = []\n    for table in tables.values():\n        if table not in table_added:\n            table_added.add(table)\n            layout = Layout(entity_id=str(uuid.uuid4()), bbox=table.bbox, label=LAYOUT_TABLE, reading_order=-1)\n            layout.children.append(table)\n            layout.page = page.page_num\n            layout.page_id = page.id\n            tables_layout.append(layout)\n    layouts_to_remove = []\n    for layout in page.leaf_layouts:\n        layouts_that_intersect = []\n        for table_layout in tables_layout:\n            intersection = layout.bbox.get_intersection(table_layout.bbox).area\n            if intersection:\n                layouts_that_intersect.append(table_layout)\n        words_in_sub_layouts = set()\n        for i, intersect_layout in enumerate(sorted(layouts_that_intersect, key=lambda l: (l.bbox.y, l.bbox.x))):\n            intersect_layout.reading_order = layout.reading_order + (i + 1) * 0.1 if intersect_layout.reading_order == -1 else min(intersect_layout.reading_order, layout.reading_order + (i + 1) * 0.1)\n            for w in intersect_layout.children[0].words:\n                words_in_sub_layouts.add(w)\n        if words_in_sub_layouts:\n            for word in words_in_sub_layouts:\n                layout.remove(word)\n            if layout._children:\n                layout.bbox = BoundingBox.enclosing_bbox(layout._children)\n            else:\n                layouts_to_remove.append(layout)\n    for layout in layouts_to_remove:\n        page.leaf_layouts.remove(layout)\n    for layout in tables_layout:\n        page.leaf_layouts.append(layout)\n    tables = list(tables.values())\n    for table in tables:\n        table.page = page.page_num\n        table.page_id = page.id\n    return (tables, table_words, added_key_values)\n\ndef parse_document_api_response(response: dict) -> Document:\n    \"\"\"\n    Parses Textract JSON response and converts them into Document object containing Page objects.\n    A valid Page object must contain at least a unique name and physical dimensions.\n\n    :param response: JSON response data in a format readable by the ResponseParser\n    :type response: dict\n\n    :return: Document object containing the hierarchy of DocumentEntity descendants.\n    :rtype: Document\n    \"\"\"\n    document = _create_document_object(response)\n    id_entity_map, id_json_map, entity_id_map, existing_words = ({}, {}, defaultdict(list), {})\n    for block in response['Blocks']:\n        id_entity_map[block['Id']] = block['BlockType']\n        id_json_map[block['Id']] = block\n        if block['BlockType'].startswith('LAYOUT'):\n            entity_id_map['LAYOUT'].append(block['Id'])\n        else:\n            entity_id_map[block['BlockType']].append(block['Id'])\n    pages, page_elements = _create_page_objects(response)\n    assert len(pages) == response['DocumentMetadata']['Pages']\n    for page_json in page_elements:\n        page = pages[page_json['Id']]\n        lines, line_words = _create_line_objects(entity_id_map[LINE], id_json_map, existing_words, page)\n        page.lines = deepcopy(lines)\n        line_by_id = {l.id: l for l in lines}\n        container_layouts, leaf_layouts = _create_layout_objects(entity_id_map[LAYOUT], id_json_map, entity_id_map, line_by_id, page)\n        if not container_layouts and (not leaf_layouts):\n            for i, line in enumerate(lines):\n                layout = Layout(entity_id=line.id, bbox=line.bbox, label=LAYOUT_ENTITY, reading_order=i)\n                layout._children = [line]\n                layout.page = page.page_num\n                layout.page_id = page.id\n                leaf_layouts.append(layout)\n        page._container_layouts.extend(container_layouts)\n        page._leaf_layouts.extend(leaf_layouts)\n        key_values, kv_words, selection_elements = _create_keyvalue_objects(entity_id_map[KEY_VALUE_SET], id_json_map, id_entity_map, entity_id_map, existing_words, page)\n        kvs = [kv for kv in key_values if not kv.contains_checkbox]\n        checkboxes = [kv for kv in key_values if kv.contains_checkbox]\n        page.key_values = kvs\n        page.checkboxes = checkboxes\n        for checkbox in checkboxes:\n            id_entity_map[checkbox.id] = SELECTION_ELEMENT\n        tables, table_words, kv_added = _create_table_objects(entity_id_map[TABLE], id_json_map, id_entity_map, entity_id_map, existing_words, {kv.id: kv for kv in key_values}, selection_elements, page)\n        page.tables = tables\n        for layout in sorted(page.leaf_layouts, key=lambda x: x.bbox.y):\n            if layout.layout_type == LAYOUT_ENTITY:\n                continue\n            for kv in sorted(key_values, key=lambda x: x.bbox.y):\n                if layout.bbox.get_intersection(kv.bbox).area > THRESHOLD * kv.bbox.area and kv.id not in kv_added:\n                    if any([w.cell_id for w in kv.words]):\n                        kv_added.add(kv.id)\n                        continue\n                    for w in kv.words:\n                        layout.remove(w)\n                    layout.children.append(kv)\n                    kv_added.add(kv.id)\n                    key_values.remove(kv)\n        page.leaf_layouts = [l for l in page.leaf_layouts if l.children or l.layout_type == LAYOUT_FIGURE]\n        kv_layouts = []\n        for kv in key_values:\n            if kv.id not in kv_added:\n                kv_added.add(kv.id)\n                layout = Layout(entity_id=str(uuid.uuid4()), bbox=kv.bbox, label=LAYOUT_KEY_VALUE, reading_order=-1)\n                layout.children.append(kv)\n                layout.page = page.page_num\n                layout.page_id = page.id\n                kv_layouts.append(layout)\n        layouts_to_remove = []\n        kv_layouts_to_ignore = []\n        layouts_that_intersect = defaultdict(list)\n        for layout in page.leaf_layouts:\n            for kv_layout in kv_layouts:\n                intersection = layout.bbox.get_intersection(kv_layout.bbox).area\n                if intersection:\n                    layouts_that_intersect[layout].append(kv_layout)\n        for layout, intersections in layouts_that_intersect.items():\n            words_in_sub_layouts = set()\n            for i, intersect_layout in enumerate(sorted(intersections, key=lambda l: (l.bbox.y, l.bbox.x))):\n                if sum([intersect_layout in intsects for intsects in layouts_that_intersect.values()]) > 1:\n                    kv_layouts_to_ignore.append(intersect_layout)\n                    continue\n                intersect_layout.reading_order = layout.reading_order + (i + 1) * 0.1 if intersect_layout.reading_order == -1 else min(intersect_layout.reading_order, layout.reading_order + (i + 1) * 0.1)\n                for w in intersect_layout.children[0].words:\n                    words_in_sub_layouts.add(w)\n            for word in words_in_sub_layouts:\n                layout.remove(word)\n            if not layout.children and layout.layout_type != LAYOUT_FIGURE:\n                layouts_to_remove.append(layout)\n        for layout in layouts_to_remove:\n            page.leaf_layouts.remove(layout)\n        for layout in kv_layouts:\n            if layout not in kv_layouts_to_ignore:\n                page.leaf_layouts.append(layout)\n        all_words = table_words + kv_words + line_words\n        for word in all_words:\n            if word.line is None:\n                line = Line(str(uuid.uuid4()), word.bbox, words=[word], confidence=word.confidence)\n                line.page = page.page_num\n                line.page_id = page.id\n                word.line = line\n                page.lines.append(line)\n        all_words = {word.id: word for word in all_words}\n        page.words = list(all_words.values())\n        queries = _create_query_objects(entity_id_map[QUERY], id_json_map, entity_id_map, page)\n        page.queries = queries\n        signatures = _create_signature_objects(entity_id_map[SIGNATURE], id_json_map, entity_id_map, page)\n        page.signatures = signatures\n        word_set = set()\n        for layout in sorted(page.layouts, key=lambda l: l.reading_order):\n            layout.visit(word_set)\n            if not layout.children and layout.layout_type != LAYOUT_FIGURE:\n                try:\n                    page.leaf_layouts.remove(layout)\n                except:\n                    page.container_layouts.remove(layout)\n    document.pages = sorted(list(pages.values()), key=lambda x: x.page_num)\n    document.response = response\n    return document\n\ndef parse_analyze_id_response(response):\n    id_documents = []\n    response['Blocks'] = []\n    for doc in response['IdentityDocuments']:\n        fields = {}\n        for field in doc['IdentityDocumentFields']:\n            fields[field['Type']['Text']] = {'key': field['Type']['Text'], 'value': field['ValueDetection']['Text'], 'confidence': field['ValueDetection']['Confidence']}\n        id_documents.append(IdentityDocument(fields))\n        id_documents[-1].raw_object = doc\n        response['Blocks'].extend(doc.get('Blocks', []))\n    document = parse_document_api_response(response)\n    del response['Blocks']\n    document.identity_documents = id_documents\n    document.response = response\n    return document\n\ndef create_expense_from_field(field: Dict, page: Page) -> ExpenseField:\n    if 'Type' in field:\n        type_expense = ExpenseType(field['Type']['Text'], field['Type']['Confidence'], field['Type'])\n    else:\n        type_expense = None\n    if 'ValueDetection' in field:\n        value_expense = Expense(bbox=None if not 'Geometry' in field['ValueDetection'] else BoundingBox.from_normalized_dict(field['ValueDetection']['Geometry']['BoundingBox'], spatial_object=page), text=field['ValueDetection']['Text'], confidence=field['ValueDetection']['Confidence'], page=page.page_num)\n        value_expense.raw_object = field['ValueDetection']\n    else:\n        value_expense = None\n    if 'LabelDetection' in field:\n        label_expense = Expense(bbox=BoundingBox.from_normalized_dict(field['LabelDetection']['Geometry']['BoundingBox'], spatial_object=page), text=field['LabelDetection']['Text'], confidence=field['LabelDetection']['Confidence'], page=page.page_num)\n        label_expense.raw_object = field['LabelDetection']\n    else:\n        label_expense = None\n    group_properties = []\n    if 'GroupProperties' in field:\n        for group_property in field['GroupProperties']:\n            group_properties.append(ExpenseGroupProperty(id=group_property['Id'], types=group_property['Types']))\n    if 'Currency' in field:\n        currency = field['Currency']['Code']\n    else:\n        currency = None\n    return ExpenseField(type_expense, value_expense, group_properties=group_properties, label=label_expense, currency=currency, page=page.page_num)\n\ndef parser_analyze_expense_response(response):\n    response['Blocks'] = [b for doc in response['ExpenseDocuments'] for b in doc.get('Blocks', [])]\n    document = parse_document_api_response(response)\n    for doc in response['ExpenseDocuments']:\n        page_number = None\n        if len(doc['SummaryFields']):\n            page_number = doc['SummaryFields'][0].get('PageNumber')\n        elif len(doc['LineItemGroups']):\n            first_field = doc['LineItemGroups'][0]['LineItems'][0]['LineItemExpenseFields'][0]\n            page_number = first_field.get('PageNumber')\n        if page_number is None:\n            logging.warning('Skipping parsing ExpenseDocument %s as its page number could not be determined' % (doc['ExpenseIndex'],))\n            continue\n        page = document.pages[page_number - 1]\n        summary_fields = []\n        for summary_field in doc['SummaryFields']:\n            summary_fields.append(create_expense_from_field(summary_field, page))\n            summary_fields[-1].raw_object = summary_field\n        line_items_groups = []\n        for line_items_group in doc['LineItemGroups']:\n            line_item_rows = []\n            for i, line_item in enumerate(line_items_group['LineItems']):\n                row_expenses = []\n                for line_item_field in line_item['LineItemExpenseFields']:\n                    row_expenses.append(create_expense_from_field(line_item_field, page))\n                    row_expenses[-1].raw_object = line_item_field\n                line_item_rows.append(LineItemRow(index=i, line_item_expense_fields=row_expenses, page=page.page_num))\n            if not line_item_rows:\n                continue\n            line_items_groups.append(LineItemGroup(index=line_items_group['LineItemGroupIndex'], line_item_rows=line_item_rows, page=page.page_num))\n        bbox = BoundingBox.enclosing_bbox(bboxes=[s.bbox for s in summary_fields] + [g.bbox for g in line_items_groups], spatial_object=page)\n        expense_document = ExpenseDocument(summary_fields=summary_fields, line_items_groups=line_items_groups, bounding_box=bbox, page=page.page_num)\n        expense_document.raw_object = doc\n        document.pages[page_number - 1].expense_documents.append(expense_document)\n    document.response = response\n    return document",
    "textractor/entities/page.py": "\"\"\"\nRepresents a single :class:`Document` page, as it would appear in the Textract API output.\nThe :class:`Page` object also contains the metadata such as the physical dimensions of the page (width, height, in pixels), child_ids etc.\n\"\"\"\nimport os\nimport string\nimport logging\nimport xlsxwriter\nfrom typing import List, Tuple\nfrom copy import deepcopy\nfrom collections import defaultdict\nfrom textractor.entities.expense_document import ExpenseDocument\nfrom textractor.entities.word import Word\nfrom textractor.entities.line import Line\nfrom textractor.entities.table import Table\nfrom textractor.entities.signature import Signature\nfrom textractor.entities.layout import Layout\nfrom textractor.entities.page_layout import PageLayout\nfrom textractor.exceptions import InputError\nfrom textractor.entities.key_value import KeyValue\nfrom textractor.entities.query import Query\nfrom textractor.entities.identity_document import IdentityDocument\nfrom textractor.entities.bbox import SpatialObject\nfrom textractor.data.constants import SelectionStatus, Direction, DirectionalFinderType\nfrom textractor.data.constants import TextTypes, SimilarityMetric\nfrom textractor.data.constants import LAYOUT_TEXT, LAYOUT_TITLE, LAYOUT_HEADER, LAYOUT_FOOTER, LAYOUT_SECTION_HEADER, LAYOUT_PAGE_NUMBER, LAYOUT_LIST, LAYOUT_FIGURE, LAYOUT_TABLE, LAYOUT_KEY_VALUE\nfrom textractor.data.text_linearization_config import TextLinearizationConfig\nfrom textractor.entities.selection_element import SelectionElement\nfrom textractor.utils.geometry_util import sort_by_position\nfrom textractor.utils.search_utils import SearchUtils, jaccard_similarity\nfrom textractor.visualizers.entitylist import EntityList\nfrom textractor.entities.linearizable import Linearizable\n\nclass Page(SpatialObject, Linearizable):\n    \"\"\"\n    Creates a new document, ideally representing a single item in the dataset.\n\n    :param id: Unique id of the Page\n    :type id: str\n    :param width: Width of page, in pixels\n    :type width: float\n    :param height: Height of page, in pixels\n    :type height: float\n    :param page_num: Page number in the document linked to this Page object\n    :type page_num: int\n    :param child_ids: IDs of child entities in the Page as determined by Textract\n    :type child_ids: List\n    \"\"\"\n\n    def __init__(self, id: str, width: int, height: int, page_num: int=-1, child_ids=None):\n        super().__init__(width=width, height=height)\n        self.id = id\n        self._words: EntityList[Word] = EntityList([])\n        self._lines: EntityList[Line] = EntityList([])\n        self._key_values: EntityList[KeyValue] = EntityList([])\n        self._checkboxes: EntityList[KeyValue] = EntityList([])\n        self._tables: EntityList[Table] = EntityList([])\n        self._queries: EntityList[Query] = EntityList([])\n        self._expense_documents: EntityList[ExpenseDocument] = EntityList([])\n        self._leaf_layouts: EntityList[Layout] = EntityList([])\n        self._container_layouts: EntityList[Layout] = EntityList([])\n        self.kv_cache = defaultdict(list)\n        self.metadata = {}\n        self.page_num = page_num\n        self.child_ids: List[str] = child_ids\n        self.image = None\n\n    @property\n    def words(self) -> EntityList[Word]:\n        \"\"\"\n        Returns all the :class:`Word` objects present in the Page.\n\n        :return: List of Word objects, each representing a word within the Page.\n        :rtype: EntityList[Word]\n        \"\"\"\n        return self._words\n\n    @words.setter\n    def words(self, words: List[Word]):\n        \"\"\"\n        Add Word objects to the Page.\n\n        :param words: List of Word objects, each representing a word within the page. No specific ordering is assumed.\n        :type words: List[Word]\n        \"\"\"\n        self._words = words\n        self._words = EntityList(sort_by_position(list(set(self._words))))\n\n    @property\n    def lines(self) -> EntityList[Line]:\n        \"\"\"\n        Returns all the :class:`Line` objects present in the Page.\n\n        :return: List of Line objects, each representing a line within the Page.\n        :rtype: EntityList[Line]\n        \"\"\"\n        return self._lines\n\n    @lines.setter\n    def lines(self, lines: List[Line]):\n        \"\"\"\n        Add Line objects to the Document.\n\n        :param lines: List of Line objects, each representing a line within the Page.\n        :type lines: List[Line]\n        \"\"\"\n        self._lines = EntityList(sort_by_position(lines))\n\n    def get_text_and_words(self, config: TextLinearizationConfig=TextLinearizationConfig()) -> Tuple[str, List[Word]]:\n        \"\"\"\n        Returns the page text and words sorted in reading order\n\n        :param config: Text linearization configuration object, defaults to TextLinearizationConfig()\n        :type config: TextLinearizationConfig, optional\n        :return: Tuple of page text and words\n        :rtype: Tuple[str, List[Word]]\n        \"\"\"\n        unsorted_layouts = [l for l in self.layouts if l.reading_order < 0]\n        sorted_layouts = [l for l in self.layouts if l.reading_order >= 0]\n        if unsorted_layouts:\n            for unsorted_layout in sorted(unsorted_layouts, key=lambda x: (x.bbox.y, x.bbox.x)):\n                closest_layout = None\n                closest_reading_order_distance = None\n                for layout in sorted(sorted_layouts, key=lambda x: x.reading_order):\n                    dist = layout.bbox.get_distance(unsorted_layout)\n                    if closest_reading_order_distance is None or dist < closest_reading_order_distance:\n                        closest_layout = layout\n                if closest_layout:\n                    sorted_layouts.insert(sorted_layouts.index(closest_layout) + 1, unsorted_layout)\n                else:\n                    sorted_layouts.append(unsorted_layout)\n        page_texts_and_words = [l.get_text_and_words(config) for l in sorted_layouts]\n        if not page_texts_and_words:\n            return ('', [])\n        text, words = zip(*page_texts_and_words)\n        combined_words = []\n        for w in words:\n            combined_words += w\n        return (config.layout_element_separator.join(text), combined_words)\n\n    @property\n    def page_layout(self) -> PageLayout:\n        return PageLayout(titles=EntityList([l for l in self.layouts if l.layout_type == LAYOUT_TITLE]), headers=EntityList([l for l in self.layouts if l.layout_type == LAYOUT_HEADER]), footers=EntityList([l for l in self.layouts if l.layout_type == LAYOUT_FOOTER]), section_headers=EntityList([l for l in self.layouts if l.layout_type == LAYOUT_SECTION_HEADER]), page_numbers=EntityList([l for l in self.layouts if l.layout_type == LAYOUT_PAGE_NUMBER]), lists=EntityList([l for l in self.layouts if l.layout_type == LAYOUT_LIST]), figures=EntityList([l for l in self.layouts if l.layout_type == LAYOUT_FIGURE]), tables=EntityList([l for l in self.layouts if l.layout_type == LAYOUT_TABLE]), key_values=EntityList([l for l in self.layouts if l.layout_type == LAYOUT_KEY_VALUE]))\n\n    @property\n    def key_values(self) -> EntityList[KeyValue]:\n        \"\"\"\n        Returns all the :class:`KeyValue` objects present in the Page.\n\n        :return: List of KeyValue objects, each representing a key-value pair within the Page.\n        :rtype: EntityList[KeyValue]\n        \"\"\"\n        return self._key_values\n\n    @key_values.setter\n    def key_values(self, kv: List[KeyValue]):\n        \"\"\"\n        Add KeyValue objects to the Page.\n\n        :param kv: List of KeyValue objects, each representing a KV area within the document page.\n        :type kv: List[KeyValue]\n        \"\"\"\n        self._key_values = EntityList(sort_by_position(kv))\n\n    @property\n    def checkboxes(self) -> EntityList[KeyValue]:\n        \"\"\"\n        Returns all the :class:`KeyValue` objects with :class:`SelectionElement` present in the Page.\n\n        :return: List of KeyValue objects, each representing a checkbox within the Page.\n        :rtype: EntityList[KeyValue]\n        \"\"\"\n        return self._checkboxes\n\n    @checkboxes.setter\n    def checkboxes(self, checkbox: List[KeyValue]):\n        \"\"\"\n        Add KeyValue objects containing SelectionElement children to the Page.\n\n        :param checkbox: List of KeyValue objects, each representing a checkbox area within the document page.\n        :type checkbox: List[KeyValue]\n        \"\"\"\n        self._checkboxes = EntityList(sort_by_position(checkbox))\n\n    @property\n    def tables(self) -> EntityList[Table]:\n        \"\"\"\n        Returns all the :class:`Table` objects present in the Page.\n\n        :return: List of Table objects, each representing a table within the Page.\n        :rtype: EntityList\n        \"\"\"\n        return self._tables\n\n    @tables.setter\n    def tables(self, tables: List[Table]):\n        \"\"\"\n        Add Table objects to the Page.\n\n        :param tables: List of Table objects, each representing a Table area within the document page.\n        :type tables: list\n        \"\"\"\n        self._tables = EntityList(tables)\n\n    @property\n    def queries(self) -> EntityList[Query]:\n        \"\"\"\n        Returns all the :class:`Query` objects present in the Page.\n\n        :return: List of Query objects.\n        :rtype: EntityList\n        \"\"\"\n        return self._queries\n\n    @queries.setter\n    def queries(self, queries: List[Query]):\n        \"\"\"\n        Add Signature objects to the Page.\n\n        :param signatures: List of Signature objects.\n        :type signatures: list\n        \"\"\"\n        self._queries = EntityList(queries)\n\n    @property\n    def signatures(self) -> EntityList[Signature]:\n        \"\"\"\n        Returns all the :class:`Signature` objects present in the Page.\n\n        :return: List of Signature objects.\n        :rtype: EntityList\n        \"\"\"\n        return self._signatures\n\n    @signatures.setter\n    def signatures(self, signatures: List[Signature]):\n        \"\"\"\n        Add Signature objects to the Page.\n\n        :param signatures: List of Signature objects.\n        :type signatures: list\n        \"\"\"\n        self._signatures = EntityList(signatures)\n\n    @property\n    def layouts(self) -> EntityList[Layout]:\n        \"\"\"\n        Returns all the :class:`Layout` objects present in the Page.\n\n        :return: List of Layout objects.\n        :rtype: EntityList\n        \"\"\"\n        return EntityList(sorted(self._leaf_layouts + self._container_layouts, key=lambda c: c.reading_order))\n\n    @property\n    def leaf_layouts(self) -> EntityList[Layout]:\n        \"\"\"\n        Returns all the leaf :class:`Layout` objects present in the Page.\n\n        :return: List of Layout objects.\n        :rtype: EntityList\n        \"\"\"\n        return self._leaf_layouts\n\n    @leaf_layouts.setter\n    def leaf_layouts(self, leaf_layouts: List[Layout]):\n        \"\"\"\n        Add leaf layout objects to the Page.\n\n        :param layouts: List of Layout objects.\n        :type layouts: list\n        \"\"\"\n        self._leaf_layouts = EntityList(leaf_layouts)\n\n    @property\n    def container_layouts(self) -> EntityList[Layout]:\n        \"\"\"\n        Returns all the container :class:`Layout` objects present in the Page.\n\n        :return: List of Layout objects.\n        :rtype: EntityList\n        \"\"\"\n        return self._container_layouts\n\n    @container_layouts.setter\n    def container_layouts(self, container_layouts: List[Layout]):\n        \"\"\"\n        Add Layout objects to the Page.\n\n        :param layouts: List of Layout objects.\n        :type layouts: list\n        \"\"\"\n        self._container_layouts = EntityList(container_layouts)\n\n    @property\n    def expense_documents(self) -> EntityList[ExpenseDocument]:\n        \"\"\"\n        Returns all the :class:`ExpenseDocument` objects present in the Page.\n\n        :return: List of ExpenseDocument objects.\n        :rtype: EntityList\n        \"\"\"\n        return self._expense_documents\n\n    @expense_documents.setter\n    def expense_documents(self, expense_documents: List[ExpenseDocument]):\n        \"\"\"\n        Add ExpenseDocument objects to the Page.\n\n        :param tables: List of ExpenseDocument objects.\n        :type expense_documents: list\n        \"\"\"\n        self._expense_documents = EntityList(expense_documents)\n\n    def __repr__(self):\n        return os.linesep.join([f'This Page ({self.page_num}) holds the following data:', f'Words - {len(self.words)}', f'Lines - {len(self.lines)}', f'Key-values - {len(self.key_values)}', f'Checkboxes - {len(self.checkboxes)}', f'Tables - {len(self.tables)}', f'Queries - {len(self.queries)}', f'Signatures - {len(self.signatures)}', f'Expense documents - {len(self.expense_documents)}', f'Layout elements - {len(self.layouts)}'])\n\n    def __getitem__(self, key):\n        output = self.get(key)\n        if output:\n            return output\n        raise KeyError(f'{key} was not found in Document')\n\n    def keys(self, include_checkboxes: bool=True) -> List[str]:\n        \"\"\"\n        Prints all keys for key-value pairs and checkboxes if the page contains them.\n\n        :param include_checkboxes: True/False. Set False if checkboxes need to be excluded.\n        :type include_checkboxes: bool\n\n        :return: List of strings containing key names in the Page\n        :rtype: List[str]\n        \"\"\"\n        keys = []\n        keys = [keyvalue.key for keyvalue in self.key_values]\n        if include_checkboxes:\n            keys += [keyvalue.key for keyvalue in self.checkboxes]\n        return keys\n\n    def filter_checkboxes(self, selected: bool=True, not_selected: bool=True) -> EntityList[KeyValue]:\n        \"\"\"\n        Return a list of :class:`KeyValue` objects containing checkboxes if the page contains them.\n\n        :param selected: True/False Return SELECTED checkboxes\n        :type selected: bool\n        :param not_selected: True/False Return NOT_SELECTED checkboxes\n        :type not_selected: bool\n\n        :return: Returns checkboxes that match the conditions set by the flags.\n        :rtype: EntityList[KeyValue]\n        \"\"\"\n        if not self.checkboxes:\n            logging.warning(f'This document does not contain checkboxes')\n            return []\n        else:\n            if selected and not_selected:\n                checkboxes = self.checkboxes\n                return EntityList(checkboxes)\n            checkboxes = []\n            if selected:\n                checkboxes = [kv for kv in self.checkboxes if kv.selection_status == SelectionStatus.SELECTED]\n            if not_selected:\n                checkboxes = [kv for kv in self.checkboxes if kv.selection_status == SelectionStatus.NOT_SELECTED]\n            return EntityList(checkboxes)\n\n    def get_words_by_type(self, text_type: TextTypes=TextTypes.PRINTED) -> EntityList[Word]:\n        \"\"\"\n        Returns list of :class:`Word` entities that match the input text type.\n\n        :param text_type: TextTypes.PRINTED or TextTypes.HANDWRITING\n        :type text_type: TextTypes\n\n        :return: Returns list of Word entities that match the input text type.\n        :rtype: EntityList[Word]\n        \"\"\"\n        if not isinstance(text_type, TextTypes):\n            raise InputError('text_type parameter should be of TextTypes type. Find input choices from textractor.data.constants')\n        if not self.words:\n            logging.warn('Document contains no word entities.')\n            return []\n        filtered_words = [word for word in self.words if word.text_type == text_type]\n        return EntityList(filtered_words)\n\n    def _search_words_with_similarity(self, keyword: str, top_k: int=1, similarity_metric: SimilarityMetric=SimilarityMetric.LEVENSHTEIN, similarity_threshold: float=0.6) -> List[Tuple[Word, float]]:\n        \"\"\"\n        Returns a list of top_k words with their similarity to the keyword.\n\n        :param keyword: Keyword that is used to query the document.\n        :type keyword: str, required\n        :param top_k: Number of closest word objects to be returned. default=1\n        :type top_k: int, optional\n        :param similarity_metric: SimilarityMetric.COSINE, SimilarityMetric.EUCLIDEAN or SimilarityMetric.LEVENSHTEIN. SimilarityMetric.COSINE is chosen as default.\n        :type similarity_metric: SimilarityMetric\n        :param similarity_threshold: Measure of how similar document key is to queried key. default=0.6\n        :type similarity_threshold: float\n\n        :return: Returns a list of tuples containing similarity and Word.\n        :rtype: List[Tuple(float, Word)]]\n        \"\"\"\n        if not isinstance(similarity_metric, SimilarityMetric):\n            raise InputError('similarity_metric parameter should be of SimilarityMetric type. Find input choices from textractor.data.constants')\n        top_n_words = []\n        similarity_threshold = similarity_threshold if similarity_metric == SimilarityMetric.COSINE else -similarity_threshold\n        lowest_similarity = similarity_threshold\n        for word in self.words:\n            similarity = SearchUtils.get_word_similarity(keyword, word.text, similarity_metric)\n            similarity = similarity if similarity_metric == SimilarityMetric.COSINE else -similarity\n            if len(top_n_words) < top_k and similarity > similarity_threshold:\n                top_n_words.append((similarity, word))\n            elif similarity > lowest_similarity:\n                top_n_words[-1] = (similarity, word)\n            else:\n                continue\n            top_n_words = sorted(top_n_words, key=lambda x: x[0], reverse=True)\n            lowest_similarity = top_n_words[-1][0]\n        return top_n_words\n\n    def search_words(self, keyword: str, top_k: int=1, similarity_metric: SimilarityMetric=SimilarityMetric.LEVENSHTEIN, similarity_threshold: float=0.6) -> EntityList[Word]:\n        \"\"\"\n        Return a list of top_k words that match the keyword.\n\n        :param keyword: Keyword that is used to query the document.\n        :type keyword: str, required\n        :param top_k: Number of closest word objects to be returned. default=1\n        :type top_k: int, optional\n        :param similarity_metric: SimilarityMetric.COSINE, SimilarityMetric.EUCLIDEAN or SimilarityMetric.LEVENSHTEIN. SimilarityMetric.COSINE is chosen as default.\n        :type similarity_metric: SimilarityMetric\n        :param similarity_threshold: Measure of how similar document key is to queried key. default=0.6\n        :type similarity_threshold: float\n\n        :return: Returns a list of words that match the queried key sorted from highest to lowest similarity.\n        :rtype: EntityList[Word]\n        \"\"\"\n        top_n_words = EntityList([ent[1] for ent in self._search_words_with_similarity(keyword=keyword, top_k=top_k, similarity_metric=similarity_metric, similarity_threshold=similarity_threshold)])\n        return top_n_words\n\n    def _search_lines_with_similarity(self, keyword: str, top_k: int=1, similarity_metric: SimilarityMetric=SimilarityMetric.LEVENSHTEIN, similarity_threshold: int=0.6) -> List[Tuple[Line, float]]:\n        \"\"\"\n        Return a list of top_k lines that contain the queried keyword.\n\n        :param keyword: Keyword that is used to query the page.\n        :type keyword: str\n        :param top_k: Number of closest line objects to be returned\n        :type top_k: int\n        :param similarity_metric: SimilarityMetric.COSINE, SimilarityMetric.EUCLIDEAN or SimilarityMetric.LEVENSHTEIN. SimilarityMetric.COSINE is chosen as default.\n        :type similarity_metric: SimilarityMetric\n        :param similarity_threshold: Measure of how similar page key is to queried key. default=0.6\n        :type similarity_threshold: float\n\n        :return: Returns a list of tuples of lines and their similarity to the keyword that contain the queried key sorted\n                 from highest to lowest similarity.\n        :rtype: List[Tuple[Line, float]]\n        \"\"\"\n        if not isinstance(similarity_metric, SimilarityMetric):\n            raise InputError('similarity_metric parameter should be of SimilarityMetric type. Find input choices from textractor.data.constants')\n        top_n_lines = []\n        similarity_threshold = similarity_threshold if similarity_metric == SimilarityMetric.COSINE else -similarity_threshold\n        lowest_similarity = similarity_threshold\n        for line in self.lines:\n            similarity = [SearchUtils.get_word_similarity(keyword, word, similarity_metric) for word in line.__repr__().split(' ')]\n            similarity.append(SearchUtils.get_word_similarity(keyword, line.__repr__(), similarity_metric))\n            similarity = max(similarity) if similarity_metric == SimilarityMetric.COSINE else -min(similarity)\n            if len(top_n_lines) < top_k and similarity > similarity_threshold:\n                top_n_lines.append((similarity, line))\n            elif similarity > lowest_similarity:\n                top_n_lines[-1] = (similarity, line)\n            else:\n                continue\n            top_n_lines = sorted(top_n_lines, key=lambda x: x[0], reverse=True)\n            lowest_similarity = top_n_lines[-1][0]\n        return top_n_lines\n\n    def search_lines(self, keyword: str, top_k: int=1, similarity_metric: SimilarityMetric=SimilarityMetric.LEVENSHTEIN, similarity_threshold: int=0.6) -> EntityList[Line]:\n        \"\"\"\n        Return a list of top_k lines that contain the queried keyword.\n\n        :param keyword: Keyword that is used to query the page.\n        :type keyword: str\n        :param top_k: Number of closest line objects to be returned\n        :type top_k: int\n        :param similarity_metric: SimilarityMetric.COSINE, SimilarityMetric.EUCLIDEAN or SimilarityMetric.LEVENSHTEIN. SimilarityMetric.COSINE is chosen as default.\n        :type similarity_metric: SimilarityMetric\n        :param similarity_threshold: Measure of how similar page key is to queried key. default=0.6\n        :type similarity_threshold: float\n\n        :return: Returns a list of lines that contain the queried key sorted\n                 from highest to lowest similarity.\n        :rtype: EntityList[Line]\n        \"\"\"\n        top_n_lines = EntityList([ent[1] for ent in self._search_lines_with_similarity(keyword=keyword, top_k=top_k, similarity_metric=similarity_metric, similarity_threshold=similarity_threshold)])\n        return top_n_lines\n\n    def get(self, key: str, top_k_matches: int=1, similarity_metric: SimilarityMetric=SimilarityMetric.LEVENSHTEIN, similarity_threshold: float=0.6) -> EntityList[KeyValue]:\n        \"\"\"\n        Return upto top_k_matches of key-value pairs for the key that is queried from the page.\n\n        :param key: Query key to match\n        :type key: str\n        :param top_k_matches: Maximum number of matches to return\n        :type top_k_matches: int\n        :param similarity_metric: SimilarityMetric.COSINE, SimilarityMetric.EUCLIDEAN or SimilarityMetric.LEVENSHTEIN. SimilarityMetric.COSINE is chosen as default.\n        :type similarity_metric: SimilarityMetric\n        :param similarity_threshold: Measure of how similar page key is to queried key. default=0.6\n        :type similarity_threshold: float\n\n        :return: Returns a list of key-value pairs that match the queried key sorted from highest\n                 to lowest similarity.\n        :rtype: EntityList[KeyValue]\n        \"\"\"\n        if not isinstance(similarity_metric, SimilarityMetric):\n            raise InputError('similarity_metric parameter should be of SimilarityMetric type. Find input choices from textractor.data.constants')\n        top_n = []\n        similarity_threshold = similarity_threshold if similarity_metric == SimilarityMetric.COSINE else -similarity_threshold\n        lowest_similarity = similarity_threshold\n        for kv in self.key_values + self.checkboxes:\n            try:\n                edited_document_key = ''.join([char for char in kv.key.__repr__() if char not in string.punctuation])\n            except:\n                pass\n            key = ''.join([char for char in key if char not in string.punctuation])\n            similarity = [SearchUtils.get_word_similarity(key, word, similarity_metric) for word in edited_document_key.split(' ')]\n            similarity.append(SearchUtils.get_word_similarity(key, edited_document_key, similarity_metric))\n            similarity = max(similarity) if similarity_metric == SimilarityMetric.COSINE else -min(similarity)\n            if similarity > similarity_threshold:\n                if len(top_n) < top_k_matches:\n                    top_n.append((kv, similarity))\n                elif similarity > lowest_similarity:\n                    top_n[-1] = (kv, similarity)\n                top_n = sorted(top_n, key=lambda x: x[1], reverse=True)\n                lowest_similarity = top_n[-1][1]\n        if not top_n:\n            logging.warning(f'Query key does not match any existing keys in the document.{os.linesep}{self.keys()}')\n        logging.info(f'Query key matched {len(top_n)} key-values in the document.')\n        return EntityList([value[0] for value in top_n])\n\n    def export_kv_to_csv(self, include_kv: bool=True, include_checkboxes: bool=True, filepath: str='Key-Values.csv'):\n        \"\"\"\n        Export key-value entities and checkboxes in csv format.\n\n        :param include_kv: True if KVs are to be exported. Else False.\n        :type include_kv: bool\n        :param include_checkboxes: True if checkboxes are to be exported. Else False.\n        :type include_checkboxes: bool\n        :param filepath: Path to where file is to be stored.\n        :type filepath: str\n        \"\"\"\n        keys = []\n        values = []\n        if include_kv and (not self.key_values):\n            logging.warning('Document does not contain key-values.')\n        elif include_kv:\n            for kv in self.key_values:\n                keys.append(kv.key.__repr__())\n                values.append(kv.value.__repr__())\n        if include_checkboxes and (not self.checkboxes):\n            logging.warning('Document does not contain checkbox elements.')\n        elif include_checkboxes:\n            for kv in self.checkboxes:\n                keys.append(kv.key.__repr__())\n                values.append(kv.value.children[0].status.name)\n        with open(filepath, 'w') as f:\n            f.write(f'Key,Value{os.linesep}')\n            for k, v in zip(keys, values):\n                f.write(f'{k},{v}{os.linesep}')\n        logging.info(f'csv file stored at location {os.path.join(os.getcwd(), filepath)}')\n\n    def export_kv_to_txt(self, include_kv: bool=True, include_checkboxes: bool=True, filepath: str='Key-Values.txt'):\n        \"\"\"\n        Export key-value entities and checkboxes in txt format.\n\n        :param include_kv: True if KVs are to be exported. Else False.\n        :type include_kv: bool\n        :param include_checkboxes: True if checkboxes are to be exported. Else False.\n        :type include_checkboxes: bool\n        :param filepath: Path to where file is to be stored.\n        :type filepath: str\n        \"\"\"\n        export_str = []\n        index = 1\n        if include_kv and (not self.key_values):\n            logging.warning('Document does not contain key-values.')\n        elif include_kv:\n            for kv in self.key_values:\n                export_str.append(f'{index}. {kv.key.__repr__()} : {kv.value.__repr__()}{os.linesep}')\n                index += 1\n        if include_checkboxes and (not self.checkboxes):\n            logging.warning('Document does not contain checkbox elements.')\n        elif include_checkboxes:\n            for kv in self.checkboxes:\n                export_str.append(f'{index}. {kv.key.__repr__()} : {kv.value.__repr__()}{os.linesep}')\n                index += 1\n        with open(filepath, 'w') as text_file:\n            text_file.write(''.join(export_str))\n        logging.info(f'txt file stored at location {os.path.join(os.getcwd(), filepath)}')\n\n    def independent_words(self) -> EntityList[Word]:\n        \"\"\"\n        :return: Return all words in the document, outside of tables, checkboxes, key-values.\n        :rtype: EntityList[Word]\n        \"\"\"\n        if not self.words:\n            logging.warning('Words have not been assigned to this Document object.')\n            return []\n        else:\n            table_words = sum([table.words for table in self.tables], [])\n            kv_words = sum([kv.words for kv in self.key_values], [])\n            checkbox_words = sum([kv.words for kv in self.checkboxes], [])\n            dependent_words = table_words + checkbox_words + kv_words\n            dependent_word_ids = set([word.id for word in dependent_words])\n            independent_words = [word for word in self.words if word.id not in dependent_word_ids]\n            return EntityList(independent_words)\n\n    def export_tables_to_excel(self, filepath):\n        \"\"\"\n        Creates an excel file and writes each table on a separate worksheet within the workbook.\n        This is stored on the filepath passed by the user.\n\n        :param filepath: Path to store the exported Excel file.\n        :type filepath: str, required\n        \"\"\"\n        if not filepath:\n            logging.error('Filepath required to store excel file.')\n        workbook = xlsxwriter.Workbook(filepath)\n        for table in self.tables:\n            workbook = table.to_excel(filepath=None, workbook=workbook, save_workbook=False)\n        workbook.close()\n\n    def _update_entity_page_num(self):\n        \"\"\"Updates page number if Textractor API call was given a list of images.\"\"\"\n        entities = self.words + self.lines + self.key_values + self.checkboxes + self.tables\n        for entity in entities:\n            entity.page = self.page_num\n\n    def return_duplicates(self):\n        \"\"\"\n        Returns a list containing :class:`EntityList` objects.\n        Each :class:`EntityList` instance contains the key-values and the last item is the table which contains duplicate information.\n        This function is intended to let the Textract user know of duplicate objects extracted by the various Textract models.\n\n        :return: List of EntityList objects each containing the intersection of KeyValue and Table entities on the page.\n        :rtype: List[EntityList]\n        \"\"\"\n        tables = self.tables\n        key_values = self.key_values\n        document_duplicates = []\n        for table in tables:\n            table_duplicates = EntityList([])\n            table_x1, table_x2, table_y1, table_y2 = (table.bbox.x, table.bbox.x + table.bbox.width, table.bbox.y, table.bbox.y + table.bbox.height)\n            for kv in key_values:\n                if kv.bbox.x >= table_x1 and kv.bbox.x <= table_x2 and (kv.bbox.y >= table_y1) and (kv.bbox.y <= table_y2):\n                    table_duplicates.append(kv)\n            if table_duplicates:\n                table_duplicates.append(table)\n            document_duplicates.append(table_duplicates)\n        return document_duplicates\n\n    def directional_finder(self, word_1: str='', word_2: str='', prefix: str='', direction=Direction.BELOW, entities=[]):\n        \"\"\"\n        The function returns entity types present in entities by prepending the prefix provided by te user. This helps in cases of repeating\n        key-values and checkboxes. The user can manipulate original data or produce a copy. The main advantage of this function is to be able to define direction.\n\n        :param word_1: The reference word from where x1, y1 coordinates are derived\n        :type word_1: str, required\n        :param word_2: The second word preferably in the direction indicated by the parameter direction. When it isn't given the end of page coordinates are used in the given direction.\n        :type word_2: str, optional\n        :param prefix: User provided prefix to prepend to the key . Without prefix, the method acts as a search by geometry function\n        :type prefix: str, optional\n        :param entities: List of DirectionalFinderType inputs.\n        :type entities: List[DirectionalFinderType]\n\n        :return: Returns the EntityList of modified key-value and/or checkboxes\n        :rtype: EntityList\n        \"\"\"\n        if not word_1:\n            return EntityList([])\n        x1, x2, y1, y2 = self._get_coords(word_1, word_2, direction)\n        if x1 == -1:\n            return EntityList([])\n        entity_dict = {DirectionalFinderType.KEY_VALUE_SET: self.key_values, DirectionalFinderType.SELECTION_ELEMENT: self.checkboxes}\n        entitylist = []\n        for entity_type in entities:\n            entitylist.extend(list(entity_dict[entity_type]))\n        new_key_values = self._get_kv_with_direction(direction, entitylist, (x1, x2, y1, y2))\n        final_kv = []\n        for kv in new_key_values:\n            if kv.key:\n                key_words = [deepcopy(word) for word in kv.key]\n                key_words[0].text = prefix + key_words[0].text\n                new_kv = deepcopy(kv)\n                new_kv.key = key_words\n                final_kv.append(new_kv)\n            else:\n                final_kv.append(kv)\n        return EntityList(final_kv)\n\n    def _get_kv_with_direction(self, direction, entitylist, coords):\n        \"\"\"Return key-values and checkboxes in entitiylist present in the direction given with respect to the coordinates.\"\"\"\n        if direction == Direction.ABOVE:\n            new_key_values = [kv for kv in entitylist if kv.bbox.y <= coords[2] and kv.bbox.y >= coords[-1]]\n        elif direction == Direction.BELOW:\n            new_key_values = [kv for kv in entitylist if kv.bbox.y >= coords[2] and kv.bbox.y <= coords[-1]]\n        elif direction == Direction.RIGHT:\n            new_key_values = [kv for kv in entitylist if kv.bbox.x >= coords[0] and kv.bbox.x <= coords[1]]\n            new_key_values = [kv for kv in new_key_values if kv.bbox.y >= coords[2] - kv.bbox.height and kv.bbox.y <= coords[-1] + 3 * kv.bbox.height]\n        elif direction == Direction.LEFT:\n            new_key_values = [kv for kv in entitylist if kv.bbox.x <= coords[0] and kv.bbox.x >= coords[1]]\n            new_key_values = [kv for kv in new_key_values if kv.bbox.y >= coords[2] - kv.bbox.height and kv.bbox.y <= coords[-1] + 3 * kv.bbox.height]\n        return new_key_values\n\n    def _get_coords(self, word_1, word_2, direction):\n        \"\"\"\n        Returns coordinates for the area within which to search for key-values with the directional_finder by retrieving coordinates of word_1         and word_2 if it exists else end of page.\n        \"\"\"\n        word_1_objects = self.search_lines(keyword=word_1, top_k=5, similarity_metric=SimilarityMetric.COSINE, similarity_threshold=0.5)\n        if not word_1_objects:\n            logging.warning(f'{word_1} not found in page')\n            return (-1, -1, -1, -1)\n        else:\n            word_1_obj = word_1_objects[0]\n            x1, y1 = (word_1_obj.bbox.x, word_1_obj.bbox.y)\n        if word_2:\n            word_2_objects = self.search_lines(keyword=word_2, top_k=5, similarity_metric=SimilarityMetric.COSINE, similarity_threshold=0.5)\n            if not word_2_objects:\n                logging.warning(f'{word_2} not found in page')\n                return (-1, -1, -1, -1)\n            else:\n                word_2_obj = word_2_objects[0]\n                x2, y2 = (word_2_obj.bbox.x, word_2_obj.bbox.y)\n        else:\n            x2, y2 = (1, 1)\n        if direction == Direction.ABOVE:\n            x1, x2, y1, y2 = (x1, x2, y1, y2) if y2 < y1 else (x1, 0, y1, 0)\n        elif direction == Direction.BELOW:\n            x1, x2, y1, y2 = (x1, x2, y1, y2) if y2 > y1 else (x1, 1, y1, 1)\n        elif direction == Direction.RIGHT:\n            x1, x2, y1, y2 = (x1, x2, y1, y2) if x2 > x1 else (x1, 1, y1, y1)\n        elif direction == Direction.LEFT:\n            x1, x2, y1, y2 = (x1, x2, y1, y2) if x2 < x1 else (x1, 0, y1, y1)\n        else:\n            return (-1, -1, -1, -1)\n        return (x1, x2, y1, y2)\n\n    def visualize(self, *args, **kwargs):\n        \"\"\"\n        Returns the object's children in a visualization EntityList object\n\n        :return: Returns an EntityList object\n        :rtype: EntityList\n        \"\"\"\n        return EntityList(self).visualize(*args, **kwargs)"
  }
}