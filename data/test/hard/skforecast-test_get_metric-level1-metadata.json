{
  "dir_path": "/app/skforecast",
  "package_name": "skforecast",
  "sample_name": "skforecast-test_get_metric",
  "src_dir": "skforecast/",
  "test_dir": "tests/",
  "test_file": "skforecast/metrics/tests/tests_metrics/test_get_metric.py",
  "test_code": "# Unit test _get_metric\n# ==============================================================================\nimport re\nimport pytest\nimport numpy as np\nfrom skforecast.metrics import _get_metric\nfrom skforecast.metrics import mean_absolute_scaled_error\nfrom skforecast.metrics import root_mean_squared_scaled_error\nfrom sklearn.metrics import mean_squared_error \nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.metrics import median_absolute_error\n\n\ndef test_get_metric_ValueError_when_metric_not_in_metrics_allowed():\n    \"\"\"\n    Test ValueError is raised when metric is not in metrics allowed.\n    \"\"\"\n    metric = 'not_a_metric'\n    allowed_metrics = [\n        \"mean_squared_error\",\n        \"mean_absolute_error\",\n        \"mean_absolute_percentage_error\",\n        \"mean_squared_log_error\",\n        \"mean_absolute_scaled_error\",\n        \"root_mean_squared_scaled_error\",\n        \"median_absolute_error\"\n    ]\n    \n    err_msg = re.escape(f\"Allowed metrics are: {allowed_metrics}. Got {metric}.\")\n    with pytest.raises(ValueError, match = err_msg):\n        _get_metric(metric)\n\n\n@pytest.mark.parametrize(\"metric_str, metric_callable\", \n                         [('mean_squared_error', mean_squared_error),\n                          ('mean_absolute_error', mean_absolute_error),\n                          ('mean_absolute_percentage_error', mean_absolute_percentage_error),\n                          ('mean_squared_log_error', mean_squared_log_error),\n                          ('median_absolute_error', median_absolute_error)], \n                         ids = lambda dt: f'mertic_str, metric_callable: {dt}')\ndef test_get_metric_output_for_all_metrics(metric_str, metric_callable):\n    \"\"\"\n    Test output for all metrics allowed.\n    \"\"\"\n    y_true = np.array([3, 1, 2, 7])\n    y_pred = np.array([2.5, 0.0, 2, 8])\n\n    metric = _get_metric(metric_str)\n    expected = metric_callable(y_true=y_true, y_pred=y_pred)\n    \n    assert metric(y_true=y_true, y_pred=y_pred) == expected\n\n\n@pytest.mark.parametrize(\"metric_str, metric_callable\", \n                         [('mean_absolute_scaled_error', mean_absolute_scaled_error),\n                          ('root_mean_squared_scaled_error', root_mean_squared_scaled_error)], \n                         ids = lambda dt : f'mertic_str, metric_callable: {dt}')\ndef test_get_metric_output_for_all_metrics_y_train(metric_str, metric_callable):\n    \"\"\"\n    Test output for all metrics allowed with y_train argument.\n    \"\"\"\n    y_true = np.array([3, -0.5, 2, 7])\n    y_pred = np.array([2.5, 0.0, 2, 8])\n    y_train = np.array([2, 3, 4, 5])\n\n    metric = _get_metric(metric_str)\n    expected = metric_callable(y_true=y_true, y_pred=y_pred, y_train=y_train)\n    \n    assert metric(y_true=y_true, y_pred=y_pred, y_train=y_train) == expected",
  "GT_file_code": {
    "skforecast/metrics/metrics.py": "################################################################################\n#                                metrics                                       #\n#                                                                              #\n# This work by skforecast team is licensed under the BSD 3-Clause License.     #\n################################################################################\n# coding=utf-8\n\nfrom typing import Union, Callable\nimport numpy as np\nimport pandas as pd\nimport inspect\nfrom functools import wraps\nfrom sklearn.metrics import (\n    mean_squared_error,\n    mean_absolute_error,\n    mean_absolute_percentage_error,\n    mean_squared_log_error,\n    median_absolute_error,\n)\n\n\ndef _get_metric(metric: str) -> Callable:\n    \"\"\"\n    Get the corresponding scikit-learn function to calculate the metric.\n\n    Parameters\n    ----------\n    metric : str\n        Metric used to quantify the goodness of fit of the model.\n\n    Returns\n    -------\n    metric : Callable\n        scikit-learn function to calculate the desired metric.\n\n    \"\"\"\n\n    allowed_metrics = [\n        \"mean_squared_error\",\n        \"mean_absolute_error\",\n        \"mean_absolute_percentage_error\",\n        \"mean_squared_log_error\",\n        \"mean_absolute_scaled_error\",\n        \"root_mean_squared_scaled_error\",\n        \"median_absolute_error\",\n    ]\n\n    if metric not in allowed_metrics:\n        raise ValueError((f\"Allowed metrics are: {allowed_metrics}. Got {metric}.\"))\n\n    metrics = {\n        \"mean_squared_error\": mean_squared_error,\n        \"mean_absolute_error\": mean_absolute_error,\n        \"mean_absolute_percentage_error\": mean_absolute_percentage_error,\n        \"mean_squared_log_error\": mean_squared_log_error,\n        \"mean_absolute_scaled_error\": mean_absolute_scaled_error,\n        \"root_mean_squared_scaled_error\": root_mean_squared_scaled_error,\n        \"median_absolute_error\": median_absolute_error,\n    }\n\n    metric = add_y_train_argument(metrics[metric])\n\n    return metric\n\n\ndef add_y_train_argument(func: Callable) -> Callable:\n    \"\"\"\n    Add `y_train` argument to a function if it is not already present.\n\n    Parameters\n    ----------\n    func : callable\n        Function to which the argument is added.\n\n    Returns\n    -------\n    wrapper : callable\n        Function with `y_train` argument added.\n    \n    \"\"\"\n\n    sig = inspect.signature(func)\n    \n    if \"y_train\" in sig.parameters:\n        return func\n\n    new_params = list(sig.parameters.values()) + [\n        inspect.Parameter(\"y_train\", inspect.Parameter.KEYWORD_ONLY, default=None)\n    ]\n    new_sig = sig.replace(parameters=new_params)\n\n    @wraps(func)\n    def wrapper(*args, y_train=None, **kwargs):\n        return func(*args, **kwargs)\n    \n    wrapper.__signature__ = new_sig\n    \n    return wrapper\n\n\ndef mean_absolute_scaled_error(\n    y_true: Union[pd.Series, np.ndarray],\n    y_pred: Union[pd.Series, np.ndarray],\n    y_train: Union[list, pd.Series, np.ndarray],\n) -> float:\n    \"\"\"\n    Mean Absolute Scaled Error (MASE)\n\n    MASE is a scale-independent error metric that measures the accuracy of\n    a forecast. It is the mean absolute error of the forecast divided by the\n    mean absolute error of a naive forecast in the training set. The naive\n    forecast is the one obtained by shifting the time series by one period.\n    If y_train is a list of numpy arrays or pandas Series, it is considered\n    that each element is the true value of the target variable in the training\n    set for each time series. In this case, the naive forecast is calculated\n    for each time series separately.\n\n    Parameters\n    ----------\n    y_true : pandas Series, numpy ndarray\n        True values of the target variable.\n    y_pred : pandas Series, numpy ndarray\n        Predicted values of the target variable.\n    y_train : list, pandas Series, numpy ndarray\n        True values of the target variable in the training set. If `list`, it\n        is consider that each element is the true value of the target variable\n        in the training set for each time series.\n\n    Returns\n    -------\n    mase : float\n        MASE value.\n    \n    \"\"\"\n\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_true` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_pred` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_train, (list, pd.Series, np.ndarray)):\n        raise TypeError(\"`y_train` must be a list, pandas Series or numpy ndarray.\")\n    if isinstance(y_train, list):\n        for x in y_train:\n            if not isinstance(x, (pd.Series, np.ndarray)):\n                raise TypeError(\n                    (\"When `y_train` is a list, each element must be a pandas Series \"\n                     \"or numpy ndarray.\")\n                )\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"`y_true` and `y_pred` must have the same length.\")\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError(\"`y_true` and `y_pred` must have at least one element.\")\n\n    if isinstance(y_train, list):\n        naive_forecast = np.concatenate([np.diff(x) for x in y_train])\n    else:\n        naive_forecast = np.diff(y_train)\n\n    mase = np.mean(np.abs(y_true - y_pred)) / np.nanmean(np.abs(naive_forecast))\n\n    return mase\n\n\ndef root_mean_squared_scaled_error(\n    y_true: Union[pd.Series, np.ndarray],\n    y_pred: Union[pd.Series, np.ndarray],\n    y_train: Union[list, pd.Series, np.ndarray],\n) -> float:\n    \"\"\"\n    Root Mean Squared Scaled Error (RMSSE)\n\n    RMSSE is a scale-independent error metric that measures the accuracy of\n    a forecast. It is the root mean squared error of the forecast divided by\n    the root mean squared error of a naive forecast in the training set. The\n    naive forecast is the one obtained by shifting the time series by one period.\n    If y_train is a list of numpy arrays or pandas Series, it is considered\n    that each element is the true value of the target variable in the training\n    set for each time series. In this case, the naive forecast is calculated\n    for each time series separately.\n\n    Parameters\n    ----------\n    y_true : pandas Series, numpy ndarray\n        True values of the target variable.\n    y_pred : pandas Series, numpy ndarray\n        Predicted values of the target variable.\n    y_train : list, pandas Series, numpy ndarray\n        True values of the target variable in the training set. If list, it\n        is consider that each element is the true value of the target variable\n        in the training set for each time series.\n\n    Returns\n    -------\n    rmsse : float\n        RMSSE value.\n    \n    \"\"\"\n\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_true` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError(\"`y_pred` must be a pandas Series or numpy ndarray.\")\n    if not isinstance(y_train, (list, pd.Series, np.ndarray)):\n        raise TypeError(\"`y_train` must be a list, pandas Series or numpy ndarray.\")\n    if isinstance(y_train, list):\n        for x in y_train:\n            if not isinstance(x, (pd.Series, np.ndarray)):\n                raise TypeError(\n                    (\"When `y_train` is a list, each element must be a pandas Series \"\n                     \"or numpy ndarray.\")\n                )\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"`y_true` and `y_pred` must have the same length.\")\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError(\"`y_true` and `y_pred` must have at least one element.\")\n\n    if isinstance(y_train, list):\n        naive_forecast = np.concatenate([np.diff(x) for x in y_train])\n    else:\n        naive_forecast = np.diff(y_train)\n    \n    rmsse = np.sqrt(np.mean((y_true - y_pred) ** 2)) / np.sqrt(np.nanmean(naive_forecast ** 2))\n    \n    return rmsse\n"
  },
  "GT_src_dict": {
    "skforecast/metrics/metrics.py": {
      "_get_metric": {
        "code": "def _get_metric(metric: str) -> Callable:\n    \"\"\"Get the corresponding scikit-learn metric function based on the specified metric name.\n\nParameters\n----------\nmetric : str\n    The name of the metric to quantify the goodness of fit of the model. It should be one of the following: \n    \"mean_squared_error\", \"mean_absolute_error\", \"mean_absolute_percentage_error\", \n    \"mean_squared_log_error\", \"mean_absolute_scaled_error\", \"root_mean_squared_scaled_error\", \n    or \"median_absolute_error\".\n\nReturns\n-------\nCallable\n    A callable metric function from scikit-learn that computes the desired metric. The function returned \n    will have an additional `y_train` argument added if it is not already part of the function signature.\n\nRaises\n------\nValueError\n    If the provided metric name is not one of the allowed metrics.\n\nNotes\n-----\nThis function interacts with the `add_y_train_argument` function to modify the metric function's signature.\nIt utilizes a dictionary `metrics`, which maps the string names of metrics to their corresponding functions \nimported from `sklearn.metrics`. The list `allowed_metrics` is defined within the function to enforce valid metric names.\"\"\"\n    '\\n    Get the corresponding scikit-learn function to calculate the metric.\\n\\n    Parameters\\n    ----------\\n    metric : str\\n        Metric used to quantify the goodness of fit of the model.\\n\\n    Returns\\n    -------\\n    metric : Callable\\n        scikit-learn function to calculate the desired metric.\\n\\n    '\n    allowed_metrics = ['mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_log_error', 'mean_absolute_scaled_error', 'root_mean_squared_scaled_error', 'median_absolute_error']\n    if metric not in allowed_metrics:\n        raise ValueError(f'Allowed metrics are: {allowed_metrics}. Got {metric}.')\n    metrics = {'mean_squared_error': mean_squared_error, 'mean_absolute_error': mean_absolute_error, 'mean_absolute_percentage_error': mean_absolute_percentage_error, 'mean_squared_log_error': mean_squared_log_error, 'mean_absolute_scaled_error': mean_absolute_scaled_error, 'root_mean_squared_scaled_error': root_mean_squared_scaled_error, 'median_absolute_error': median_absolute_error}\n    metric = add_y_train_argument(metrics[metric])\n    return metric",
        "docstring": "Get the corresponding scikit-learn metric function based on the specified metric name.\n\nParameters\n----------\nmetric : str\n    The name of the metric to quantify the goodness of fit of the model. It should be one of the following: \n    \"mean_squared_error\", \"mean_absolute_error\", \"mean_absolute_percentage_error\", \n    \"mean_squared_log_error\", \"mean_absolute_scaled_error\", \"root_mean_squared_scaled_error\", \n    or \"median_absolute_error\".\n\nReturns\n-------\nCallable\n    A callable metric function from scikit-learn that computes the desired metric. The function returned \n    will have an additional `y_train` argument added if it is not already part of the function signature.\n\nRaises\n------\nValueError\n    If the provided metric name is not one of the allowed metrics.\n\nNotes\n-----\nThis function interacts with the `add_y_train_argument` function to modify the metric function's signature.\nIt utilizes a dictionary `metrics`, which maps the string names of metrics to their corresponding functions \nimported from `sklearn.metrics`. The list `allowed_metrics` is defined within the function to enforce valid metric names.",
        "signature": "def _get_metric(metric: str) -> Callable:",
        "type": "Function",
        "class_signature": null
      },
      "mean_absolute_scaled_error": {
        "code": "def mean_absolute_scaled_error(y_true: Union[pd.Series, np.ndarray], y_pred: Union[pd.Series, np.ndarray], y_train: Union[list, pd.Series, np.ndarray]) -> float:\n    \"\"\"Calculate the Mean Absolute Scaled Error (MASE) between the true values and the predicted values of a forecasted model. MASE is a scale-independent metric that evaluates forecast accuracy by comparing the mean absolute error of the predictions to the mean absolute error of a naive forecast derived from the training data.\n\nParameters\n----------\ny_true : Union[pd.Series, np.ndarray]\n    True values of the target variable.\ny_pred : Union[pd.Series, np.ndarray]\n    Predicted values of the target variable.\ny_train : Union[list, pd.Series, np.ndarray]\n    True values of the target variable in the training set. If a list, each element is considered a separate time series.\n\nReturns\n-------\nfloat\n    The MASE value, representing the accuracy of the forecast.\n\nRaises\n------\nTypeError\n    If `y_true`, `y_pred`, or `y_train` is not of the expected types.\nValueError\n    If the lengths of `y_true` and `y_pred` do not match, or if they are empty.\n\nThe function makes use of NumPy and Pandas libraries for numerical operations and data handling, ensuring efficient calculation of metrics over potentially large datasets. It also expects that the naive forecast is computed by applying a simple differencing operation (`np.diff`) on the training data.\"\"\"\n    '\\n    Mean Absolute Scaled Error (MASE)\\n\\n    MASE is a scale-independent error metric that measures the accuracy of\\n    a forecast. It is the mean absolute error of the forecast divided by the\\n    mean absolute error of a naive forecast in the training set. The naive\\n    forecast is the one obtained by shifting the time series by one period.\\n    If y_train is a list of numpy arrays or pandas Series, it is considered\\n    that each element is the true value of the target variable in the training\\n    set for each time series. In this case, the naive forecast is calculated\\n    for each time series separately.\\n\\n    Parameters\\n    ----------\\n    y_true : pandas Series, numpy ndarray\\n        True values of the target variable.\\n    y_pred : pandas Series, numpy ndarray\\n        Predicted values of the target variable.\\n    y_train : list, pandas Series, numpy ndarray\\n        True values of the target variable in the training set. If `list`, it\\n        is consider that each element is the true value of the target variable\\n        in the training set for each time series.\\n\\n    Returns\\n    -------\\n    mase : float\\n        MASE value.\\n    \\n    '\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError('`y_true` must be a pandas Series or numpy ndarray.')\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError('`y_pred` must be a pandas Series or numpy ndarray.')\n    if not isinstance(y_train, (list, pd.Series, np.ndarray)):\n        raise TypeError('`y_train` must be a list, pandas Series or numpy ndarray.')\n    if isinstance(y_train, list):\n        for x in y_train:\n            if not isinstance(x, (pd.Series, np.ndarray)):\n                raise TypeError('When `y_train` is a list, each element must be a pandas Series or numpy ndarray.')\n    if len(y_true) != len(y_pred):\n        raise ValueError('`y_true` and `y_pred` must have the same length.')\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError('`y_true` and `y_pred` must have at least one element.')\n    if isinstance(y_train, list):\n        naive_forecast = np.concatenate([np.diff(x) for x in y_train])\n    else:\n        naive_forecast = np.diff(y_train)\n    mase = np.mean(np.abs(y_true - y_pred)) / np.nanmean(np.abs(naive_forecast))\n    return mase",
        "docstring": "Calculate the Mean Absolute Scaled Error (MASE) between the true values and the predicted values of a forecasted model. MASE is a scale-independent metric that evaluates forecast accuracy by comparing the mean absolute error of the predictions to the mean absolute error of a naive forecast derived from the training data.\n\nParameters\n----------\ny_true : Union[pd.Series, np.ndarray]\n    True values of the target variable.\ny_pred : Union[pd.Series, np.ndarray]\n    Predicted values of the target variable.\ny_train : Union[list, pd.Series, np.ndarray]\n    True values of the target variable in the training set. If a list, each element is considered a separate time series.\n\nReturns\n-------\nfloat\n    The MASE value, representing the accuracy of the forecast.\n\nRaises\n------\nTypeError\n    If `y_true`, `y_pred`, or `y_train` is not of the expected types.\nValueError\n    If the lengths of `y_true` and `y_pred` do not match, or if they are empty.\n\nThe function makes use of NumPy and Pandas libraries for numerical operations and data handling, ensuring efficient calculation of metrics over potentially large datasets. It also expects that the naive forecast is computed by applying a simple differencing operation (`np.diff`) on the training data.",
        "signature": "def mean_absolute_scaled_error(y_true: Union[pd.Series, np.ndarray], y_pred: Union[pd.Series, np.ndarray], y_train: Union[list, pd.Series, np.ndarray]) -> float:",
        "type": "Function",
        "class_signature": null
      },
      "root_mean_squared_scaled_error": {
        "code": "def root_mean_squared_scaled_error(y_true: Union[pd.Series, np.ndarray], y_pred: Union[pd.Series, np.ndarray], y_train: Union[list, pd.Series, np.ndarray]) -> float:\n    \"\"\"Root Mean Squared Scaled Error (RMSSE)\n\nRMSSE is a scale-independent error metric used to assess forecast accuracy. It calculates the root mean squared error (RMSE) of the predicted values compared to the true values, normalizing it by the RMSE of a naive forecast based on historical data. The naive forecast is generated by shifting the time series by one period. This metric is particularly useful when working with time series data across different scales.\n\nParameters\n----------\ny_true : Union[pd.Series, np.ndarray]\n    The true values of the target variable.\ny_pred : Union[pd.Series, np.ndarray]\n    The predicted values of the target variable.\ny_train : Union[list, pd.Series, np.ndarray]\n    True values of the target variable in the training set. If a list, it is assumed each element represents the target variable for a separate time series.\n\nReturns\n-------\nfloat\n    The RMSSE value, indicating the accuracy of the forecast relative to the naive forecast.\n\nRaises\n------\nTypeError\n    If any of the inputs are not of the expected types (pandas Series, numpy ndarray, list).\nValueError\n    If `y_true` and `y_pred` do not have the same length or if they are empty.\n\nDependencies\n------------\nThis function utilizes `numpy` for numerical operations, particularly for calculating the mean and standard deviation, and requires `pandas` for handling time series data. The `np.diff` function is employed to compute naive forecasts, while the return value is derived from the computed RMSE values.\"\"\"\n    '\\n    Root Mean Squared Scaled Error (RMSSE)\\n\\n    RMSSE is a scale-independent error metric that measures the accuracy of\\n    a forecast. It is the root mean squared error of the forecast divided by\\n    the root mean squared error of a naive forecast in the training set. The\\n    naive forecast is the one obtained by shifting the time series by one period.\\n    If y_train is a list of numpy arrays or pandas Series, it is considered\\n    that each element is the true value of the target variable in the training\\n    set for each time series. In this case, the naive forecast is calculated\\n    for each time series separately.\\n\\n    Parameters\\n    ----------\\n    y_true : pandas Series, numpy ndarray\\n        True values of the target variable.\\n    y_pred : pandas Series, numpy ndarray\\n        Predicted values of the target variable.\\n    y_train : list, pandas Series, numpy ndarray\\n        True values of the target variable in the training set. If list, it\\n        is consider that each element is the true value of the target variable\\n        in the training set for each time series.\\n\\n    Returns\\n    -------\\n    rmsse : float\\n        RMSSE value.\\n    \\n    '\n    if not isinstance(y_true, (pd.Series, np.ndarray)):\n        raise TypeError('`y_true` must be a pandas Series or numpy ndarray.')\n    if not isinstance(y_pred, (pd.Series, np.ndarray)):\n        raise TypeError('`y_pred` must be a pandas Series or numpy ndarray.')\n    if not isinstance(y_train, (list, pd.Series, np.ndarray)):\n        raise TypeError('`y_train` must be a list, pandas Series or numpy ndarray.')\n    if isinstance(y_train, list):\n        for x in y_train:\n            if not isinstance(x, (pd.Series, np.ndarray)):\n                raise TypeError('When `y_train` is a list, each element must be a pandas Series or numpy ndarray.')\n    if len(y_true) != len(y_pred):\n        raise ValueError('`y_true` and `y_pred` must have the same length.')\n    if len(y_true) == 0 or len(y_pred) == 0:\n        raise ValueError('`y_true` and `y_pred` must have at least one element.')\n    if isinstance(y_train, list):\n        naive_forecast = np.concatenate([np.diff(x) for x in y_train])\n    else:\n        naive_forecast = np.diff(y_train)\n    rmsse = np.sqrt(np.mean((y_true - y_pred) ** 2)) / np.sqrt(np.nanmean(naive_forecast ** 2))\n    return rmsse",
        "docstring": "Root Mean Squared Scaled Error (RMSSE)\n\nRMSSE is a scale-independent error metric used to assess forecast accuracy. It calculates the root mean squared error (RMSE) of the predicted values compared to the true values, normalizing it by the RMSE of a naive forecast based on historical data. The naive forecast is generated by shifting the time series by one period. This metric is particularly useful when working with time series data across different scales.\n\nParameters\n----------\ny_true : Union[pd.Series, np.ndarray]\n    The true values of the target variable.\ny_pred : Union[pd.Series, np.ndarray]\n    The predicted values of the target variable.\ny_train : Union[list, pd.Series, np.ndarray]\n    True values of the target variable in the training set. If a list, it is assumed each element represents the target variable for a separate time series.\n\nReturns\n-------\nfloat\n    The RMSSE value, indicating the accuracy of the forecast relative to the naive forecast.\n\nRaises\n------\nTypeError\n    If any of the inputs are not of the expected types (pandas Series, numpy ndarray, list).\nValueError\n    If `y_true` and `y_pred` do not have the same length or if they are empty.\n\nDependencies\n------------\nThis function utilizes `numpy` for numerical operations, particularly for calculating the mean and standard deviation, and requires `pandas` for handling time series data. The `np.diff` function is employed to compute naive forecasts, while the return value is derived from the computed RMSE values.",
        "signature": "def root_mean_squared_scaled_error(y_true: Union[pd.Series, np.ndarray], y_pred: Union[pd.Series, np.ndarray], y_train: Union[list, pd.Series, np.ndarray]) -> float:",
        "type": "Function",
        "class_signature": null
      },
      "wrapper": {
        "code": "    def wrapper(*args, y_train=None, **kwargs):\n        \"\"\"Wrapper function that adds a `y_train` keyword-only argument to the metric functions if it is not already present. This allows the metric functions to optionally utilize the training set's true values for improved metric calculations. \n\nParameters\n----------\n*args : tuple\n    Positional arguments to be passed to the original metric function.\ny_train : optional\n    True values of the target variable in the training set, passed as a keyword-only argument.\n**kwargs : dict\n    Additional keyword arguments to be passed to the original metric function.\n\nReturns\n-------\ncallable\n    The result of the metric function with the provided arguments, excluding `y_train`, which is added as needed. \n\nDependencies\n------------\nThis wrapper depends on Python's `inspect` module to manipulate the function signature and add parameters dynamically. It interacts directly with the metric function it's wrapping to facilitate additional functionality without changing the original function's structure.\"\"\"\n        return func(*args, **kwargs)",
        "docstring": "Wrapper function that adds a `y_train` keyword-only argument to the metric functions if it is not already present. This allows the metric functions to optionally utilize the training set's true values for improved metric calculations. \n\nParameters\n----------\n*args : tuple\n    Positional arguments to be passed to the original metric function.\ny_train : optional\n    True values of the target variable in the training set, passed as a keyword-only argument.\n**kwargs : dict\n    Additional keyword arguments to be passed to the original metric function.\n\nReturns\n-------\ncallable\n    The result of the metric function with the provided arguments, excluding `y_train`, which is added as needed. \n\nDependencies\n------------\nThis wrapper depends on Python's `inspect` module to manipulate the function signature and add parameters dynamically. It interacts directly with the metric function it's wrapping to facilitate additional functionality without changing the original function's structure.",
        "signature": "def wrapper(*args, y_train=None, **kwargs):",
        "type": "Function",
        "class_signature": null
      }
    }
  },
  "dependency_dict": {
    "skforecast/metrics/metrics.py:_get_metric": {
      "skforecast/metrics/metrics.py": {
        "add_y_train_argument": {
          "code": "def add_y_train_argument(func: Callable) -> Callable:\n    \"\"\"\n    Add `y_train` argument to a function if it is not already present.\n\n    Parameters\n    ----------\n    func : callable\n        Function to which the argument is added.\n\n    Returns\n    -------\n    wrapper : callable\n        Function with `y_train` argument added.\n    \n    \"\"\"\n    sig = inspect.signature(func)\n    if 'y_train' in sig.parameters:\n        return func\n    new_params = list(sig.parameters.values()) + [inspect.Parameter('y_train', inspect.Parameter.KEYWORD_ONLY, default=None)]\n    new_sig = sig.replace(parameters=new_params)\n    wrapper.__signature__ = new_sig\n    return wrapper",
          "docstring": "Add `y_train` argument to a function if it is not already present.\n\nParameters\n----------\nfunc : callable\n    Function to which the argument is added.\n\nReturns\n-------\nwrapper : callable\n    Function with `y_train` argument added.",
          "signature": "def add_y_train_argument(func: Callable) -> Callable:",
          "type": "Function",
          "class_signature": null
        }
      }
    }
  },
  "PRD": "# PROJECT NAME: skforecast-test_get_metric\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 skforecast/\n    \u2514\u2500\u2500 metrics/\n        \u2514\u2500\u2500 metrics.py\n            \u251c\u2500\u2500 _get_metric\n            \u251c\u2500\u2500 mean_absolute_scaled_error\n            \u251c\u2500\u2500 root_mean_squared_scaled_error\n            \u2514\u2500\u2500 wrapper\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThe module provides functionality for validating and retrieving metric functions used in evaluating model performance. It ensures that only a predefined set of allowed metrics can be selected and maps string inputs representing metric names to their corresponding callable implementations. The module supports compatibility with both standard metrics from libraries such as scikit-learn and custom metrics with additional arguments, such as training data. By standardizing metric retrieval and enforcing validity, it eliminates potential errors related to unsupported metrics, streamlining the evaluation process for developers building forecasting or machine learning models.\n\n## FILE 1: skforecast/metrics/metrics.py\n\n- FUNCTION NAME: wrapper\n  - SIGNATURE: def wrapper(*args, y_train=None, **kwargs):\n  - DOCSTRING: \n```python\n\"\"\"\nWrapper function that adds a `y_train` keyword-only argument to the metric functions if it is not already present. This allows the metric functions to optionally utilize the training set's true values for improved metric calculations. \n\nParameters\n----------\n*args : tuple\n    Positional arguments to be passed to the original metric function.\ny_train : optional\n    True values of the target variable in the training set, passed as a keyword-only argument.\n**kwargs : dict\n    Additional keyword arguments to be passed to the original metric function.\n\nReturns\n-------\ncallable\n    The result of the metric function with the provided arguments, excluding `y_train`, which is added as needed. \n\nDependencies\n------------\nThis wrapper depends on Python's `inspect` module to manipulate the function signature and add parameters dynamically. It interacts directly with the metric function it's wrapping to facilitate additional functionality without changing the original function's structure.\n\"\"\"\n```\n\n- FUNCTION NAME: mean_absolute_scaled_error\n  - SIGNATURE: def mean_absolute_scaled_error(y_true: Union[pd.Series, np.ndarray], y_pred: Union[pd.Series, np.ndarray], y_train: Union[list, pd.Series, np.ndarray]) -> float:\n  - DOCSTRING: \n```python\n\"\"\"\nCalculate the Mean Absolute Scaled Error (MASE) between the true values and the predicted values of a forecasted model. MASE is a scale-independent metric that evaluates forecast accuracy by comparing the mean absolute error of the predictions to the mean absolute error of a naive forecast derived from the training data.\n\nParameters\n----------\ny_true : Union[pd.Series, np.ndarray]\n    True values of the target variable.\ny_pred : Union[pd.Series, np.ndarray]\n    Predicted values of the target variable.\ny_train : Union[list, pd.Series, np.ndarray]\n    True values of the target variable in the training set. If a list, each element is considered a separate time series.\n\nReturns\n-------\nfloat\n    The MASE value, representing the accuracy of the forecast.\n\nRaises\n------\nTypeError\n    If `y_true`, `y_pred`, or `y_train` is not of the expected types.\nValueError\n    If the lengths of `y_true` and `y_pred` do not match, or if they are empty.\n\nThe function makes use of NumPy and Pandas libraries for numerical operations and data handling, ensuring efficient calculation of metrics over potentially large datasets. It also expects that the naive forecast is computed by applying a simple differencing operation (`np.diff`) on the training data.\n\"\"\"\n```\n\n- FUNCTION NAME: _get_metric\n  - SIGNATURE: def _get_metric(metric: str) -> Callable:\n  - DOCSTRING: \n```python\n\"\"\"\nGet the corresponding scikit-learn metric function based on the specified metric name.\n\nParameters\n----------\nmetric : str\n    The name of the metric to quantify the goodness of fit of the model. It should be one of the following: \n    \"mean_squared_error\", \"mean_absolute_error\", \"mean_absolute_percentage_error\", \n    \"mean_squared_log_error\", \"mean_absolute_scaled_error\", \"root_mean_squared_scaled_error\", \n    or \"median_absolute_error\".\n\nReturns\n-------\nCallable\n    A callable metric function from scikit-learn that computes the desired metric. The function returned \n    will have an additional `y_train` argument added if it is not already part of the function signature.\n\nRaises\n------\nValueError\n    If the provided metric name is not one of the allowed metrics.\n\nNotes\n-----\nThis function interacts with the `add_y_train_argument` function to modify the metric function's signature.\nIt utilizes a dictionary `metrics`, which maps the string names of metrics to their corresponding functions \nimported from `sklearn.metrics`. The list `allowed_metrics` is defined within the function to enforce valid metric names.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - skforecast/metrics/metrics.py:add_y_train_argument\n\n- FUNCTION NAME: root_mean_squared_scaled_error\n  - SIGNATURE: def root_mean_squared_scaled_error(y_true: Union[pd.Series, np.ndarray], y_pred: Union[pd.Series, np.ndarray], y_train: Union[list, pd.Series, np.ndarray]) -> float:\n  - DOCSTRING: \n```python\n\"\"\"\nRoot Mean Squared Scaled Error (RMSSE)\n\nRMSSE is a scale-independent error metric used to assess forecast accuracy. It calculates the root mean squared error (RMSE) of the predicted values compared to the true values, normalizing it by the RMSE of a naive forecast based on historical data. The naive forecast is generated by shifting the time series by one period. This metric is particularly useful when working with time series data across different scales.\n\nParameters\n----------\ny_true : Union[pd.Series, np.ndarray]\n    The true values of the target variable.\ny_pred : Union[pd.Series, np.ndarray]\n    The predicted values of the target variable.\ny_train : Union[list, pd.Series, np.ndarray]\n    True values of the target variable in the training set. If a list, it is assumed each element represents the target variable for a separate time series.\n\nReturns\n-------\nfloat\n    The RMSSE value, indicating the accuracy of the forecast relative to the naive forecast.\n\nRaises\n------\nTypeError\n    If any of the inputs are not of the expected types (pandas Series, numpy ndarray, list).\nValueError\n    If `y_true` and `y_pred` do not have the same length or if they are empty.\n\nDependencies\n------------\nThis function utilizes `numpy` for numerical operations, particularly for calculating the mean and standard deviation, and requires `pandas` for handling time series data. The `np.diff` function is employed to compute naive forecasts, while the return value is derived from the computed RMSE values.\n\"\"\"\n```\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "skforecast/metrics/metrics.py": "from typing import Union, Callable\nimport numpy as np\nimport pandas as pd\nimport inspect\nfrom functools import wraps\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, mean_squared_log_error, median_absolute_error\n\ndef add_y_train_argument(func: Callable) -> Callable:\n    \"\"\"\n    Add `y_train` argument to a function if it is not already present.\n\n    Parameters\n    ----------\n    func : callable\n        Function to which the argument is added.\n\n    Returns\n    -------\n    wrapper : callable\n        Function with `y_train` argument added.\n    \n    \"\"\"\n    sig = inspect.signature(func)\n    if 'y_train' in sig.parameters:\n        return func\n    new_params = list(sig.parameters.values()) + [inspect.Parameter('y_train', inspect.Parameter.KEYWORD_ONLY, default=None)]\n    new_sig = sig.replace(parameters=new_params)\n    wrapper.__signature__ = new_sig\n    return wrapper"
  },
  "call_tree": {
    "skforecast/metrics/tests/tests_metrics/test_get_metric.py:test_get_metric_ValueError_when_metric_not_in_metrics_allowed": {
      "skforecast/metrics/metrics.py:_get_metric": {}
    },
    "skforecast/metrics/tests/tests_metrics/test_get_metric.py:test_get_metric_output_for_all_metrics": {
      "skforecast/metrics/metrics.py:_get_metric": {
        "skforecast/metrics/metrics.py:add_y_train_argument": {}
      },
      "skforecast/metrics/metrics.py:wrapper": {}
    },
    "skforecast/metrics/tests/tests_metrics/test_get_metric.py:test_get_metric_output_for_all_metrics_y_train": {
      "skforecast/metrics/metrics.py:_get_metric": {
        "skforecast/metrics/metrics.py:add_y_train_argument": {}
      },
      "skforecast/metrics/metrics.py:mean_absolute_scaled_error": {},
      "skforecast/metrics/metrics.py:root_mean_squared_scaled_error": {}
    }
  }
}