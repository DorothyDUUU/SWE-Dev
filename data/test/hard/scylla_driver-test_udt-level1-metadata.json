{
  "dir_path": "/app/scylla_driver",
  "package_name": "scylla_driver",
  "sample_name": "scylla_driver-test_udt",
  "src_dir": "cassandra/",
  "test_dir": "tests/",
  "test_file": "tests/unit/cqlengine/test_udt.py",
  "test_code": "# Copyright DataStax, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nfrom cassandra.cqlengine import columns\nfrom cassandra.cqlengine.models import Model\nfrom cassandra.cqlengine.usertype import UserType\n\n\nclass UDTTest(unittest.TestCase):\n\n    def test_initialization_without_existing_connection(self):\n        \"\"\"\n        Test that users can define models with UDTs without initializing\n        connections.\n\n        Written to reproduce PYTHON-649.\n        \"\"\"\n\n        class Value(UserType):\n            t = columns.Text()\n\n        class DummyUDT(Model):\n            __keyspace__ = 'ks'\n            primary_key = columns.Integer(primary_key=True)\n            value = columns.UserDefinedType(Value)\n",
  "GT_file_code": {
    "cassandra/policies.py": "# Copyright DataStax, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport random\n\nfrom collections import namedtuple\nfrom functools import lru_cache\nfrom itertools import islice, cycle, groupby, repeat\nimport logging\nfrom random import randint, shuffle\nfrom threading import Lock\nimport socket\nimport warnings\n\nlog = logging.getLogger(__name__)\n\nfrom cassandra import WriteType as WT\n\n\n# This is done this way because WriteType was originally\n# defined here and in order not to break the API.\n# It may removed in the next mayor.\nWriteType = WT\n\nfrom cassandra import ConsistencyLevel, OperationTimedOut\n\nclass HostDistance(object):\n    \"\"\"\n    A measure of how \"distant\" a node is from the client, which\n    may influence how the load balancer distributes requests\n    and how many connections are opened to the node.\n    \"\"\"\n\n    IGNORED = -1\n    \"\"\"\n    A node with this distance should never be queried or have\n    connections opened to it.\n    \"\"\"\n\n    LOCAL_RACK = 0\n    \"\"\"\n    Nodes with ``LOCAL_RACK`` distance will be preferred for operations\n    under some load balancing policies (such as :class:`.RackAwareRoundRobinPolicy`)\n    and will have a greater number of connections opened against\n    them by default.\n\n    This distance is typically used for nodes within the same\n    datacenter and the same rack as the client.\n    \"\"\"\n\n    LOCAL = 1\n    \"\"\"\n    Nodes with ``LOCAL`` distance will be preferred for operations\n    under some load balancing policies (such as :class:`.DCAwareRoundRobinPolicy`)\n    and will have a greater number of connections opened against\n    them by default.\n\n    This distance is typically used for nodes within the same\n    datacenter as the client.\n    \"\"\"\n\n    REMOTE = 2\n    \"\"\"\n    Nodes with ``REMOTE`` distance will be treated as a last resort\n    by some load balancing policies (such as :class:`.DCAwareRoundRobinPolicy`\n    and :class:`.RackAwareRoundRobinPolicy`)and will have a smaller number of\n    connections opened against them by default.\n\n    This distance is typically used for nodes outside of the\n    datacenter that the client is running in.\n    \"\"\"\n\n\nclass HostStateListener(object):\n\n    def on_up(self, host):\n        \"\"\" Called when a node is marked up. \"\"\"\n        raise NotImplementedError()\n\n    def on_down(self, host):\n        \"\"\" Called when a node is marked down. \"\"\"\n        raise NotImplementedError()\n\n    def on_add(self, host):\n        \"\"\"\n        Called when a node is added to the cluster.  The newly added node\n        should be considered up.\n        \"\"\"\n        raise NotImplementedError()\n\n    def on_remove(self, host):\n        \"\"\" Called when a node is removed from the cluster. \"\"\"\n        raise NotImplementedError()\n\n\nclass LoadBalancingPolicy(HostStateListener):\n    \"\"\"\n    Load balancing policies are used to decide how to distribute\n    requests among all possible coordinator nodes in the cluster.\n\n    In particular, they may focus on querying \"near\" nodes (those\n    in a local datacenter) or on querying nodes who happen to\n    be replicas for the requested data.\n\n    You may also use subclasses of :class:`.LoadBalancingPolicy` for\n    custom behavior.\n\n    You should always use immutable collections (e.g., tuples or\n    frozensets) to store information about hosts to prevent accidental\n    modification. When there are changes to the hosts (e.g., a host is\n    down or up), the old collection should be replaced with a new one.\n    \"\"\"\n\n    _hosts_lock = None\n\n    def __init__(self):\n        self._hosts_lock = Lock()\n\n    def distance(self, host):\n        \"\"\"\n        Returns a measure of how remote a :class:`~.pool.Host` is in\n        terms of the :class:`.HostDistance` enums.\n        \"\"\"\n        raise NotImplementedError()\n\n    def populate(self, cluster, hosts):\n        \"\"\"\n        This method is called to initialize the load balancing\n        policy with a set of :class:`.Host` instances before its\n        first use.  The `cluster` parameter is an instance of\n        :class:`.Cluster`.\n        \"\"\"\n        raise NotImplementedError()\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        \"\"\"\n        Given a :class:`~.query.Statement` instance, return a iterable\n        of :class:`.Host` instances which should be queried in that\n        order.  A generator may work well for custom implementations\n        of this method.\n\n        Note that the `query` argument may be :const:`None` when preparing\n        statements.\n\n        `working_keyspace` should be the string name of the current keyspace,\n        as set through :meth:`.Session.set_keyspace()` or with a ``USE``\n        statement.\n        \"\"\"\n        raise NotImplementedError()\n\n    def check_supported(self):\n        \"\"\"\n        This will be called after the cluster Metadata has been initialized.\n        If the load balancing policy implementation cannot be supported for\n        some reason (such as a missing C extension), this is the point at\n        which it should raise an exception.\n        \"\"\"\n        pass\n\n\nclass RoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    A subclass of :class:`.LoadBalancingPolicy` which evenly\n    distributes queries across all nodes in the cluster,\n    regardless of what datacenter the nodes may be in.\n    \"\"\"\n    _live_hosts = frozenset(())\n    _position = 0\n\n    def populate(self, cluster, hosts):\n        self._live_hosts = frozenset(hosts)\n        if len(hosts) > 1:\n            self._position = randint(0, len(hosts) - 1)\n\n    def distance(self, host):\n        return HostDistance.LOCAL\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        # not thread-safe, but we don't care much about lost increments\n        # for the purposes of load balancing\n        pos = self._position\n        self._position += 1\n\n        hosts = self._live_hosts\n        length = len(hosts)\n        if length:\n            pos %= length\n            return islice(cycle(hosts), pos, pos + length)\n        else:\n            return []\n\n    def on_up(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.union((host, ))\n\n    def on_down(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.difference((host, ))\n\n    def on_add(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.union((host, ))\n\n    def on_remove(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.difference((host, ))\n\n\nclass DCAwareRoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    Similar to :class:`.RoundRobinPolicy`, but prefers hosts\n    in the local datacenter and only uses nodes in remote\n    datacenters as a last resort.\n    \"\"\"\n\n    local_dc = None\n    used_hosts_per_remote_dc = 0\n\n    def __init__(self, local_dc='', used_hosts_per_remote_dc=0):\n        \"\"\"\n        The `local_dc` parameter should be the name of the datacenter\n        (such as is reported by ``nodetool ring``) that should\n        be considered local. If not specified, the driver will choose\n        a local_dc based on the first host among :attr:`.Cluster.contact_points`\n        having a valid DC. If relying on this mechanism, all specified\n        contact points should be nodes in a single, local DC.\n\n        `used_hosts_per_remote_dc` controls how many nodes in\n        each remote datacenter will have connections opened\n        against them. In other words, `used_hosts_per_remote_dc` hosts\n        will be considered :attr:`~.HostDistance.REMOTE` and the\n        rest will be considered :attr:`~.HostDistance.IGNORED`.\n        By default, all remote hosts are ignored.\n        \"\"\"\n        self.local_dc = local_dc\n        self.used_hosts_per_remote_dc = used_hosts_per_remote_dc\n        self._dc_live_hosts = {}\n        self._position = 0\n        self._endpoints = []\n        LoadBalancingPolicy.__init__(self)\n\n    def _dc(self, host):\n        return host.datacenter or self.local_dc\n\n    def populate(self, cluster, hosts):\n        for dc, dc_hosts in groupby(hosts, lambda h: self._dc(h)):\n            self._dc_live_hosts[dc] = tuple(set(dc_hosts))\n\n        if not self.local_dc:\n            self._endpoints = [\n                endpoint\n                for endpoint in cluster.endpoints_resolved]\n\n        self._position = randint(0, len(hosts) - 1) if hosts else 0\n\n    def distance(self, host):\n        dc = self._dc(host)\n        if dc == self.local_dc:\n            return HostDistance.LOCAL\n\n        if not self.used_hosts_per_remote_dc:\n            return HostDistance.IGNORED\n        else:\n            dc_hosts = self._dc_live_hosts.get(dc)\n            if not dc_hosts:\n                return HostDistance.IGNORED\n\n            if host in list(dc_hosts)[:self.used_hosts_per_remote_dc]:\n                return HostDistance.REMOTE\n            else:\n                return HostDistance.IGNORED\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        # not thread-safe, but we don't care much about lost increments\n        # for the purposes of load balancing\n        pos = self._position\n        self._position += 1\n\n        local_live = self._dc_live_hosts.get(self.local_dc, ())\n        pos = (pos % len(local_live)) if local_live else 0\n        for host in islice(cycle(local_live), pos, pos + len(local_live)):\n            yield host\n\n        # the dict can change, so get candidate DCs iterating over keys of a copy\n        other_dcs = [dc for dc in self._dc_live_hosts.copy().keys() if dc != self.local_dc]\n        for dc in other_dcs:\n            remote_live = self._dc_live_hosts.get(dc, ())\n            for host in remote_live[:self.used_hosts_per_remote_dc]:\n                yield host\n\n    def on_up(self, host):\n        # not worrying about threads because this will happen during\n        # control connection startup/refresh\n        if not self.local_dc and host.datacenter:\n            if host.endpoint in self._endpoints:\n                self.local_dc = host.datacenter\n                log.info(\"Using datacenter '%s' for DCAwareRoundRobinPolicy (via host '%s'); \"\n                         \"if incorrect, please specify a local_dc to the constructor, \"\n                         \"or limit contact points to local cluster nodes\" %\n                         (self.local_dc, host.endpoint))\n                del self._endpoints\n\n        dc = self._dc(host)\n        with self._hosts_lock:\n            current_hosts = self._dc_live_hosts.get(dc, ())\n            if host not in current_hosts:\n                self._dc_live_hosts[dc] = current_hosts + (host, )\n\n    def on_down(self, host):\n        dc = self._dc(host)\n        with self._hosts_lock:\n            current_hosts = self._dc_live_hosts.get(dc, ())\n            if host in current_hosts:\n                hosts = tuple(h for h in current_hosts if h != host)\n                if hosts:\n                    self._dc_live_hosts[dc] = hosts\n                else:\n                    del self._dc_live_hosts[dc]\n\n    def on_add(self, host):\n        self.on_up(host)\n\n    def on_remove(self, host):\n        self.on_down(host)\n\nclass RackAwareRoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    Similar to :class:`.DCAwareRoundRobinPolicy`, but prefers hosts\n    in the local rack, before hosts in the local datacenter but a\n    different rack, before hosts in all other datercentres\n    \"\"\"\n\n    local_dc = None\n    local_rack = None\n    used_hosts_per_remote_dc = 0\n\n    def __init__(self, local_dc, local_rack, used_hosts_per_remote_dc=0):\n        \"\"\"\n        The `local_dc` and `local_rack` parameters should be the name of the\n        datacenter and rack (such as is reported by ``nodetool ring``) that\n        should be considered local.\n\n        `used_hosts_per_remote_dc` controls how many nodes in\n        each remote datacenter will have connections opened\n        against them. In other words, `used_hosts_per_remote_dc` hosts\n        will be considered :attr:`~.HostDistance.REMOTE` and the\n        rest will be considered :attr:`~.HostDistance.IGNORED`.\n        By default, all remote hosts are ignored.\n        \"\"\"\n        self.local_rack = local_rack\n        self.local_dc = local_dc\n        self.used_hosts_per_remote_dc = used_hosts_per_remote_dc\n        self._live_hosts = {}\n        self._dc_live_hosts = {}\n        self._endpoints = []\n        self._position = 0\n        LoadBalancingPolicy.__init__(self)\n\n    def _rack(self, host):\n        return host.rack or self.local_rack\n\n    def _dc(self, host):\n        return host.datacenter or self.local_dc\n\n    def populate(self, cluster, hosts):\n        for (dc, rack), rack_hosts in groupby(hosts, lambda host: (self._dc(host), self._rack(host))):\n            self._live_hosts[(dc, rack)] = tuple(set(rack_hosts))\n        for dc, dc_hosts in groupby(hosts, lambda host: self._dc(host)):\n            self._dc_live_hosts[dc] = tuple(set(dc_hosts))\n\n        self._position = randint(0, len(hosts) - 1) if hosts else 0\n\n    def distance(self, host):\n        rack = self._rack(host)\n        dc = self._dc(host)\n        if rack == self.local_rack and dc == self.local_dc:\n            return HostDistance.LOCAL_RACK\n\n        if dc == self.local_dc:\n            return HostDistance.LOCAL\n\n        if not self.used_hosts_per_remote_dc:\n            return HostDistance.IGNORED\n\n        dc_hosts = self._dc_live_hosts.get(dc, ())\n        if not dc_hosts:\n            return HostDistance.IGNORED\n        if host in dc_hosts and dc_hosts.index(host) < self.used_hosts_per_remote_dc:\n            return HostDistance.REMOTE\n        else:\n            return HostDistance.IGNORED\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        pos = self._position\n        self._position += 1\n\n        local_rack_live = self._live_hosts.get((self.local_dc, self.local_rack), ())\n        pos = (pos % len(local_rack_live)) if local_rack_live else 0\n        # Slice the cyclic iterator to start from pos and include the next len(local_live) elements\n        # This ensures we get exactly one full cycle starting from pos\n        for host in islice(cycle(local_rack_live), pos, pos + len(local_rack_live)):\n            yield host\n\n        local_live = [host for host in self._dc_live_hosts.get(self.local_dc, ()) if host.rack != self.local_rack]\n        pos = (pos % len(local_live)) if local_live else 0\n        for host in islice(cycle(local_live), pos, pos + len(local_live)):\n            yield host\n\n        # the dict can change, so get candidate DCs iterating over keys of a copy\n        for dc, remote_live in self._dc_live_hosts.copy().items():\n            if dc != self.local_dc:\n                for host in remote_live[:self.used_hosts_per_remote_dc]:\n                    yield host\n\n    def on_up(self, host):\n        dc = self._dc(host)\n        rack = self._rack(host)\n        with self._hosts_lock:\n            current_rack_hosts = self._live_hosts.get((dc, rack), ())\n            if host not in current_rack_hosts:\n                self._live_hosts[(dc, rack)] = current_rack_hosts + (host, )\n            current_dc_hosts = self._dc_live_hosts.get(dc, ())\n            if host not in current_dc_hosts:\n                self._dc_live_hosts[dc] = current_dc_hosts + (host, )\n\n    def on_down(self, host):\n        dc = self._dc(host)\n        rack = self._rack(host)\n        with self._hosts_lock:\n            current_rack_hosts = self._live_hosts.get((dc, rack), ())\n            if host in current_rack_hosts:\n                hosts = tuple(h for h in current_rack_hosts if h != host)\n                if hosts:\n                    self._live_hosts[(dc, rack)] = hosts\n                else:\n                    del self._live_hosts[(dc, rack)]\n            current_dc_hosts = self._dc_live_hosts.get(dc, ())\n            if host in current_dc_hosts:\n                hosts = tuple(h for h in current_dc_hosts if h != host)\n                if hosts:\n                    self._dc_live_hosts[dc] = hosts\n                else:\n                    del self._dc_live_hosts[dc]\n\n    def on_add(self, host):\n        self.on_up(host)\n\n    def on_remove(self, host):\n        self.on_down(host)\n\nclass TokenAwarePolicy(LoadBalancingPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` wrapper that adds token awareness to\n    a child policy.\n\n    This alters the child policy's behavior so that it first attempts to\n    send queries to :attr:`~.HostDistance.LOCAL` replicas (as determined\n    by the child policy) based on the :class:`.Statement`'s\n    :attr:`~.Statement.routing_key`. If :attr:`.shuffle_replicas` is\n    truthy, these replicas will be yielded in a random order. Once those\n    hosts are exhausted, the remaining hosts in the child policy's query\n    plan will be used in the order provided by the child policy.\n\n    If no :attr:`~.Statement.routing_key` is set on the query, the child\n    policy's query plan will be used as is.\n    \"\"\"\n\n    _child_policy = None\n    _cluster_metadata = None\n    _tablets_routing_v1 = False\n    shuffle_replicas = False\n    \"\"\"\n    Yield local replicas in a random order.\n    \"\"\"\n\n    def __init__(self, child_policy, shuffle_replicas=False):\n        self._child_policy = child_policy\n        self.shuffle_replicas = shuffle_replicas\n\n    def populate(self, cluster, hosts):\n        self._cluster_metadata = cluster.metadata\n        self._tablets_routing_v1 = cluster.control_connection._tablets_routing_v1\n        self._child_policy.populate(cluster, hosts)\n\n    def check_supported(self):\n        if not self._cluster_metadata.can_support_partitioner():\n            raise RuntimeError(\n                '%s cannot be used with the cluster partitioner (%s) because '\n                'the relevant C extension for this driver was not compiled. '\n                'See the installation instructions for details on building '\n                'and installing the C extensions.' %\n                (self.__class__.__name__, self._cluster_metadata.partitioner))\n\n    def distance(self, *args, **kwargs):\n        return self._child_policy.distance(*args, **kwargs)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        keyspace = query.keyspace if query and query.keyspace else working_keyspace\n\n        child = self._child_policy\n        if query is None or query.routing_key is None or keyspace is None:\n            for host in child.make_query_plan(keyspace, query):\n                yield host\n            return\n\n        replicas = []\n        if self._tablets_routing_v1:\n            tablet = self._cluster_metadata._tablets.get_tablet_for_key(\n                keyspace, query.table, self._cluster_metadata.token_map.token_class.from_key(query.routing_key))\n\n            if tablet is not None:\n                replicas_mapped = set(map(lambda r: r[0], tablet.replicas))\n                child_plan = child.make_query_plan(keyspace, query)\n\n                replicas = [host for host in child_plan if host.host_id in replicas_mapped]\n\n        if not replicas:\n            replicas = self._cluster_metadata.get_replicas(keyspace, query.routing_key)\n\n        if self.shuffle_replicas:\n            shuffle(replicas)\n\n        for replica in replicas:\n            if replica.is_up and child.distance(replica) in [HostDistance.LOCAL, HostDistance.LOCAL_RACK]:\n                yield replica\n\n        for host in child.make_query_plan(keyspace, query):\n            # skip if we've already listed this host\n            if host not in replicas or child.distance(host) == HostDistance.REMOTE:\n                yield host\n\n    def on_up(self, *args, **kwargs):\n        return self._child_policy.on_up(*args, **kwargs)\n\n    def on_down(self, *args, **kwargs):\n        return self._child_policy.on_down(*args, **kwargs)\n\n    def on_add(self, *args, **kwargs):\n        return self._child_policy.on_add(*args, **kwargs)\n\n    def on_remove(self, *args, **kwargs):\n        return self._child_policy.on_remove(*args, **kwargs)\n\n\nclass WhiteListRoundRobinPolicy(RoundRobinPolicy):\n    \"\"\"\n    A subclass of :class:`.RoundRobinPolicy` which evenly\n    distributes queries across all nodes in the cluster,\n    regardless of what datacenter the nodes may be in, but\n    only if that node exists in the list of allowed nodes\n\n    This policy is addresses the issue described in\n    https://datastax-oss.atlassian.net/browse/JAVA-145\n    Where connection errors occur when connection\n    attempts are made to private IP addresses remotely\n    \"\"\"\n\n    def __init__(self, hosts):\n        \"\"\"\n        The `hosts` parameter should be a sequence of hosts to permit\n        connections to.\n        \"\"\"\n        self._allowed_hosts = tuple(hosts)\n        self._allowed_hosts_resolved = []\n        for h in self._allowed_hosts:\n            unix_socket_path = getattr(h, \"_unix_socket_path\", None)\n            if unix_socket_path:\n                self._allowed_hosts_resolved.append(unix_socket_path)\n            else:\n                self._allowed_hosts_resolved.extend([endpoint[4][0]\n                                        for endpoint in socket.getaddrinfo(h, None, socket.AF_UNSPEC, socket.SOCK_STREAM)])\n\n        RoundRobinPolicy.__init__(self)\n\n    def populate(self, cluster, hosts):\n        self._live_hosts = frozenset(h for h in hosts if h.address in self._allowed_hosts_resolved)\n\n        if len(hosts) <= 1:\n            self._position = 0\n        else:\n            self._position = randint(0, len(hosts) - 1)\n\n    def distance(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            return HostDistance.LOCAL\n        else:\n            return HostDistance.IGNORED\n\n    def on_up(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            RoundRobinPolicy.on_up(self, host)\n\n    def on_add(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            RoundRobinPolicy.on_add(self, host)\n\n\nclass HostFilterPolicy(LoadBalancingPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` subclass configured with a child policy,\n    and a single-argument predicate. This policy defers to the child policy for\n    hosts where ``predicate(host)`` is truthy. Hosts for which\n    ``predicate(host)`` is falsy will be considered :attr:`.IGNORED`, and will\n    not be used in a query plan.\n\n    This can be used in the cases where you need a whitelist or blacklist\n    policy, e.g. to prepare for decommissioning nodes or for testing:\n\n    .. code-block:: python\n\n        def address_is_ignored(host):\n            return host.address in [ignored_address0, ignored_address1]\n\n        blacklist_filter_policy = HostFilterPolicy(\n            child_policy=RoundRobinPolicy(),\n            predicate=address_is_ignored\n        )\n\n        cluster = Cluster(\n            primary_host,\n            load_balancing_policy=blacklist_filter_policy,\n        )\n\n    See the note in the :meth:`.make_query_plan` documentation for a caveat on\n    how wrapping ordering polices (e.g. :class:`.RoundRobinPolicy`) may break\n    desirable properties of the wrapped policy.\n\n    Please note that whitelist and blacklist policies are not recommended for\n    general, day-to-day use. You probably want something like\n    :class:`.DCAwareRoundRobinPolicy`, which prefers a local DC but has\n    fallbacks, over a brute-force method like whitelisting or blacklisting.\n    \"\"\"\n\n    def __init__(self, child_policy, predicate):\n        \"\"\"\n        :param child_policy: an instantiated :class:`.LoadBalancingPolicy`\n                             that this one will defer to.\n        :param predicate: a one-parameter function that takes a :class:`.Host`.\n                          If it returns a falsy value, the :class:`.Host` will\n                          be :attr:`.IGNORED` and not returned in query plans.\n        \"\"\"\n        super(HostFilterPolicy, self).__init__()\n        self._child_policy = child_policy\n        self._predicate = predicate\n\n    def on_up(self, host, *args, **kwargs):\n        return self._child_policy.on_up(host, *args, **kwargs)\n\n    def on_down(self, host, *args, **kwargs):\n        return self._child_policy.on_down(host, *args, **kwargs)\n\n    def on_add(self, host, *args, **kwargs):\n        return self._child_policy.on_add(host, *args, **kwargs)\n\n    def on_remove(self, host, *args, **kwargs):\n        return self._child_policy.on_remove(host, *args, **kwargs)\n\n    @property\n    def predicate(self):\n        \"\"\"\n        A predicate, set on object initialization, that takes a :class:`.Host`\n        and returns a value. If the value is falsy, the :class:`.Host` is\n        :class:`~HostDistance.IGNORED`. If the value is truthy,\n        :class:`.HostFilterPolicy` defers to the child policy to determine the\n        host's distance.\n\n        This is a read-only value set in ``__init__``, implemented as a\n        ``property``.\n        \"\"\"\n        return self._predicate\n\n    def distance(self, host):\n        \"\"\"\n        Checks if ``predicate(host)``, then returns\n        :attr:`~HostDistance.IGNORED` if falsy, and defers to the child policy\n        otherwise.\n        \"\"\"\n        if self.predicate(host):\n            return self._child_policy.distance(host)\n        else:\n            return HostDistance.IGNORED\n\n    def populate(self, cluster, hosts):\n        self._child_policy.populate(cluster=cluster, hosts=hosts)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        \"\"\"\n        Defers to the child policy's\n        :meth:`.LoadBalancingPolicy.make_query_plan` and filters the results.\n\n        Note that this filtering may break desirable properties of the wrapped\n        policy in some cases. For instance, imagine if you configure this\n        policy to filter out ``host2``, and to wrap a round-robin policy that\n        rotates through three hosts in the order ``host1, host2, host3``,\n        ``host2, host3, host1``, ``host3, host1, host2``, repeating. This\n        policy will yield ``host1, host3``, ``host3, host1``, ``host3, host1``,\n        disproportionately favoring ``host3``.\n        \"\"\"\n        child_qp = self._child_policy.make_query_plan(\n            working_keyspace=working_keyspace, query=query\n        )\n        for host in child_qp:\n            if self.predicate(host):\n                yield host\n\n    def check_supported(self):\n        return self._child_policy.check_supported()\n\n\nclass ConvictionPolicy(object):\n    \"\"\"\n    A policy which decides when hosts should be considered down\n    based on the types of failures and the number of failures.\n\n    If custom behavior is needed, this class may be subclassed.\n    \"\"\"\n\n    def __init__(self, host):\n        \"\"\"\n        `host` is an instance of :class:`.Host`.\n        \"\"\"\n        self.host = host\n\n    def add_failure(self, connection_exc):\n        \"\"\"\n        Implementations should return :const:`True` if the host should be\n        convicted, :const:`False` otherwise.\n        \"\"\"\n        raise NotImplementedError()\n\n    def reset(self):\n        \"\"\"\n        Implementations should clear out any convictions or state regarding\n        the host.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass SimpleConvictionPolicy(ConvictionPolicy):\n    \"\"\"\n    The default implementation of :class:`ConvictionPolicy`,\n    which simply marks a host as down after the first failure\n    of any kind.\n    \"\"\"\n\n    def add_failure(self, connection_exc):\n        return not isinstance(connection_exc, OperationTimedOut)\n\n    def reset(self):\n        pass\n\n\nclass ReconnectionPolicy(object):\n    \"\"\"\n    This class and its subclasses govern how frequently an attempt is made\n    to reconnect to nodes that are marked as dead.\n\n    If custom behavior is needed, this class may be subclassed.\n    \"\"\"\n\n    def new_schedule(self):\n        \"\"\"\n        This should return a finite or infinite iterable of delays (each as a\n        floating point number of seconds) in-between each failed reconnection\n        attempt.  Note that if the iterable is finite, reconnection attempts\n        will cease once the iterable is exhausted.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass ConstantReconnectionPolicy(ReconnectionPolicy):\n    \"\"\"\n    A :class:`.ReconnectionPolicy` subclass which sleeps for a fixed delay\n    in-between each reconnection attempt.\n    \"\"\"\n\n    def __init__(self, delay, max_attempts=64):\n        \"\"\"\n        `delay` should be a floating point number of seconds to wait in-between\n        each attempt.\n\n        `max_attempts` should be a total number of attempts to be made before\n        giving up, or :const:`None` to continue reconnection attempts forever.\n        The default is 64.\n        \"\"\"\n        if delay < 0:\n            raise ValueError(\"delay must not be negative\")\n        if max_attempts is not None and max_attempts < 0:\n            raise ValueError(\"max_attempts must not be negative\")\n\n        self.delay = delay\n        self.max_attempts = max_attempts\n\n    def new_schedule(self):\n        if self.max_attempts:\n            return repeat(self.delay, self.max_attempts)\n        return repeat(self.delay)\n\n\nclass ExponentialReconnectionPolicy(ReconnectionPolicy):\n    \"\"\"\n    A :class:`.ReconnectionPolicy` subclass which exponentially increases\n    the length of the delay in-between each reconnection attempt up to\n    a set maximum delay.\n\n    A random amount of jitter (+/- 15%) will be added to the pure exponential\n    delay value to avoid the situations where many reconnection handlers are\n    trying to reconnect at exactly the same time.\n    \"\"\"\n\n    # TODO: max_attempts is 64 to preserve legacy default behavior\n    # consider changing to None in major release to prevent the policy\n    # giving up forever\n    def __init__(self, base_delay, max_delay, max_attempts=64):\n        \"\"\"\n        `base_delay` and `max_delay` should be in floating point units of\n        seconds.\n\n        `max_attempts` should be a total number of attempts to be made before\n        giving up, or :const:`None` to continue reconnection attempts forever.\n        The default is 64.\n        \"\"\"\n        if base_delay < 0 or max_delay < 0:\n            raise ValueError(\"Delays may not be negative\")\n\n        if max_delay < base_delay:\n            raise ValueError(\"Max delay must be greater than base delay\")\n\n        if max_attempts is not None and max_attempts < 0:\n            raise ValueError(\"max_attempts must not be negative\")\n\n        self.base_delay = base_delay\n        self.max_delay = max_delay\n        self.max_attempts = max_attempts\n\n    def new_schedule(self):\n        i, overflowed = 0, False\n        while self.max_attempts is None or i < self.max_attempts:\n            if overflowed:\n                yield self.max_delay\n            else:\n                try:\n                    yield self._add_jitter(min(self.base_delay * (2 ** i), self.max_delay))\n                except OverflowError:\n                    overflowed = True\n                    yield self.max_delay\n\n            i += 1\n\n    # Adds -+ 15% to the delay provided\n    def _add_jitter(self, value):\n        jitter = randint(85, 115)\n        delay = (jitter * value) / 100\n        return min(max(self.base_delay, delay), self.max_delay)\n\n\nclass RetryPolicy(object):\n    \"\"\"\n    A policy that describes whether to retry, rethrow, or ignore coordinator\n    timeout and unavailable failures. These are failures reported from the\n    server side. Timeouts are configured by\n    `settings in cassandra.yaml <https://github.com/apache/cassandra/blob/cassandra-2.1.4/conf/cassandra.yaml#L568-L584>`_.\n    Unavailable failures occur when the coordinator cannot achieve the consistency\n    level for a request. For further information see the method descriptions\n    below.\n\n    To specify a default retry policy, set the\n    :attr:`.Cluster.default_retry_policy` attribute to an instance of this\n    class or one of its subclasses.\n\n    To specify a retry policy per query, set the :attr:`.Statement.retry_policy`\n    attribute to an instance of this class or one of its subclasses.\n\n    If custom behavior is needed for retrying certain operations,\n    this class may be subclassed.\n    \"\"\"\n\n    RETRY = 0\n    \"\"\"\n    This should be returned from the below methods if the operation\n    should be retried on the same connection.\n    \"\"\"\n\n    RETHROW = 1\n    \"\"\"\n    This should be returned from the below methods if the failure\n    should be propagated and no more retries attempted.\n    \"\"\"\n\n    IGNORE = 2\n    \"\"\"\n    This should be returned from the below methods if the failure\n    should be ignored but no more retries should be attempted.\n    \"\"\"\n\n    RETRY_NEXT_HOST = 3\n    \"\"\"\n    This should be returned from the below methods if the operation\n    should be retried on another connection.\n    \"\"\"\n\n    def on_read_timeout(self, query, consistency, required_responses,\n                        received_responses, data_retrieved, retry_num):\n        \"\"\"\n        This is called when a read operation times out from the coordinator's\n        perspective (i.e. a replica did not respond to the coordinator in time).\n        It should return a tuple with two items: one of the class enums (such\n        as :attr:`.RETRY`) and a :class:`.ConsistencyLevel` to retry the\n        operation at or :const:`None` to keep the same consistency level.\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        The `required_responses` and `received_responses` parameters describe\n        how many replicas needed to respond to meet the requested consistency\n        level and how many actually did respond before the coordinator timed\n        out the request. `data_retrieved` is a boolean indicating whether\n        any of those responses contained data (as opposed to just a digest).\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, operations will be retried at most once, and only if\n        a sufficient number of replicas responded (with data digests).\n        \"\"\"\n        if retry_num != 0:\n            return self.RETHROW, None\n        elif received_responses >= required_responses and not data_retrieved:\n            return self.RETRY, consistency\n        else:\n            return self.RETHROW, None\n\n    def on_write_timeout(self, query, consistency, write_type,\n                         required_responses, received_responses, retry_num):\n        \"\"\"\n        This is called when a write operation times out from the coordinator's\n        perspective (i.e. a replica did not respond to the coordinator in time).\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `write_type` is one of the :class:`.WriteType` enums describing the\n        type of write operation.\n\n        The `required_responses` and `received_responses` parameters describe\n        how many replicas needed to acknowledge the write to meet the requested\n        consistency level and how many replicas actually did acknowledge the\n        write before the coordinator timed out the request.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, failed write operations will retried at most once, and\n        they will only be retried if the `write_type` was\n        :attr:`~.WriteType.BATCH_LOG`.\n        \"\"\"\n        if retry_num != 0:\n            return self.RETHROW, None\n        elif write_type == WriteType.BATCH_LOG:\n            return self.RETRY, consistency\n        else:\n            return self.RETHROW, None\n\n    def on_unavailable(self, query, consistency, required_replicas, alive_replicas, retry_num):\n        \"\"\"\n        This is called when the coordinator node determines that a read or\n        write operation cannot be successful because the number of live\n        replicas are too low to meet the requested :class:`.ConsistencyLevel`.\n        This means that the read or write operation was never forwarded to\n        any replicas.\n\n        `query` is the :class:`.Statement` that failed.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `required_replicas` is the number of replicas that would have needed to\n        acknowledge the operation to meet the requested consistency level.\n        `alive_replicas` is the number of replicas that the coordinator\n        considered alive at the time of the request.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, if this is the first retry, it triggers a retry on the next\n        host in the query plan with the same consistency level. If this is not the\n        first retry, no retries will be attempted and the error will be re-raised.\n        \"\"\"\n        return (self.RETRY_NEXT_HOST, None) if retry_num == 0 else (self.RETHROW, None)\n\n    def on_request_error(self, query, consistency, error, retry_num):\n        \"\"\"\n        This is called when an unexpected error happens. This can be in the\n        following situations:\n\n        * On a connection error\n        * On server errors: overloaded, isBootstrapping, serverError, etc.\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `error` the instance of the exception.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, it triggers a retry on the next host in the query plan\n        with the same consistency level.\n        \"\"\"\n        # TODO revisit this for the next major\n        # To preserve the same behavior than before, we don't take retry_num into account\n        return self.RETRY_NEXT_HOST, None\n\n\nclass FallthroughRetryPolicy(RetryPolicy):\n    \"\"\"\n    A retry policy that never retries and always propagates failures to\n    the application.\n    \"\"\"\n\n    def on_read_timeout(self, *args, **kwargs):\n        return self.RETHROW, None\n\n    def on_write_timeout(self, *args, **kwargs):\n        return self.RETHROW, None\n\n    def on_unavailable(self, *args, **kwargs):\n        return self.RETHROW, None\n\n    def on_request_error(self, *args, **kwargs):\n        return self.RETHROW, None\n\n\nclass DowngradingConsistencyRetryPolicy(RetryPolicy):\n    \"\"\"\n    *Deprecated:* This retry policy will be removed in the next major release.\n\n    A retry policy that sometimes retries with a lower consistency level than\n    the one initially requested.\n\n    **BEWARE**: This policy may retry queries using a lower consistency\n    level than the one initially requested. By doing so, it may break\n    consistency guarantees. In other words, if you use this retry policy,\n    there are cases (documented below) where a read at :attr:`~.QUORUM`\n    *may not* see a preceding write at :attr:`~.QUORUM`. Do not use this\n    policy unless you have understood the cases where this can happen and\n    are ok with that. It is also recommended to subclass this class so\n    that queries that required a consistency level downgrade can be\n    recorded (so that repairs can be made later, etc).\n\n    This policy implements the same retries as :class:`.RetryPolicy`,\n    but on top of that, it also retries in the following cases:\n\n    * On a read timeout: if the number of replicas that responded is\n      greater than one but lower than is required by the requested\n      consistency level, the operation is retried at a lower consistency\n      level.\n    * On a write timeout: if the operation is an :attr:`~.UNLOGGED_BATCH`\n      and at least one replica acknowledged the write, the operation is\n      retried at a lower consistency level.  Furthermore, for other\n      write types, if at least one replica acknowledged the write, the\n      timeout is ignored.\n    * On an unavailable exception: if at least one replica is alive, the\n      operation is retried at a lower consistency level.\n\n    The reasoning behind this retry policy is as follows: if, based\n    on the information the Cassandra coordinator node returns, retrying the\n    operation with the initially requested consistency has a chance to\n    succeed, do it. Otherwise, if based on that information we know the\n    initially requested consistency level cannot be achieved currently, then:\n\n    * For writes, ignore the exception (thus silently failing the\n      consistency requirement) if we know the write has been persisted on at\n      least one replica.\n    * For reads, try reading at a lower consistency level (thus silently\n      failing the consistency requirement).\n\n    In other words, this policy implements the idea that if the requested\n    consistency level cannot be achieved, the next best thing for writes is\n    to make sure the data is persisted, and that reading something is better\n    than reading nothing, even if there is a risk of reading stale data.\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(DowngradingConsistencyRetryPolicy, self).__init__(*args, **kwargs)\n        warnings.warn('DowngradingConsistencyRetryPolicy is deprecated '\n                      'and will be removed in the next major release.',\n                      DeprecationWarning)\n\n    def _pick_consistency(self, num_responses):\n        if num_responses >= 3:\n            return self.RETRY, ConsistencyLevel.THREE\n        elif num_responses >= 2:\n            return self.RETRY, ConsistencyLevel.TWO\n        elif num_responses >= 1:\n            return self.RETRY, ConsistencyLevel.ONE\n        else:\n            return self.RETHROW, None\n\n    def on_read_timeout(self, query, consistency, required_responses,\n                        received_responses, data_retrieved, retry_num):\n        if retry_num != 0:\n            return self.RETHROW, None\n        elif ConsistencyLevel.is_serial(consistency):\n            # Downgrading does not make sense for a CAS read query\n            return self.RETHROW, None\n        elif received_responses < required_responses:\n            return self._pick_consistency(received_responses)\n        elif not data_retrieved:\n            return self.RETRY, consistency\n        else:\n            return self.RETHROW, None\n\n    def on_write_timeout(self, query, consistency, write_type,\n                         required_responses, received_responses, retry_num):\n        if retry_num != 0:\n            return self.RETHROW, None\n\n        if write_type in (WriteType.SIMPLE, WriteType.BATCH, WriteType.COUNTER):\n            if received_responses > 0:\n                # persisted on at least one replica\n                return self.IGNORE, None\n            else:\n                return self.RETHROW, None\n        elif write_type == WriteType.UNLOGGED_BATCH:\n            return self._pick_consistency(received_responses)\n        elif write_type == WriteType.BATCH_LOG:\n            return self.RETRY, consistency\n\n        return self.RETHROW, None\n\n    def on_unavailable(self, query, consistency, required_replicas, alive_replicas, retry_num):\n        if retry_num != 0:\n            return self.RETHROW, None\n        elif ConsistencyLevel.is_serial(consistency):\n            # failed at the paxos phase of a LWT, retry on the next host\n            return self.RETRY_NEXT_HOST, None\n        else:\n            return self._pick_consistency(alive_replicas)\n\n\nclass ExponentialBackoffRetryPolicy(RetryPolicy):\n    \"\"\"\n    A policy that do retries with exponential backoff\n    \"\"\"\n\n    def __init__(self, max_num_retries: float, min_interval: float = 0.1, max_interval: float = 10.0,\n                 *args, **kwargs):\n        \"\"\"\n        `max_num_retries` counts how many times the operation would be retried,\n        `min_interval` is the initial time in seconds to wait before first retry\n        `max_interval` is the maximum time to wait between retries\n        \"\"\"\n        self.min_interval = min_interval\n        self.max_num_retries = max_num_retries\n        self.max_interval = max_interval\n        super(ExponentialBackoffRetryPolicy).__init__(*args, **kwargs)\n\n    def _calculate_backoff(self, attempt: int):\n        delay = min(self.max_interval, self.min_interval * 2 ** attempt)\n        # add some jitter\n        delay += random.random() * self.min_interval - (self.min_interval / 2)\n        return delay\n\n    def on_read_timeout(self, query, consistency, required_responses,\n                        received_responses, data_retrieved, retry_num):\n        if retry_num < self.max_num_retries and received_responses >= required_responses and not data_retrieved:\n            return self.RETRY, consistency, self._calculate_backoff(retry_num)\n        else:\n            return self.RETHROW, None, None\n\n    def on_write_timeout(self, query, consistency, write_type,\n                         required_responses, received_responses, retry_num):\n        if retry_num < self.max_num_retries and write_type == WriteType.BATCH_LOG:\n            return self.RETRY, consistency, self._calculate_backoff(retry_num)\n        else:\n            return self.RETHROW, None, None\n\n    def on_unavailable(self, query, consistency, required_replicas,\n                       alive_replicas, retry_num):\n        if retry_num < self.max_num_retries:\n            return self.RETRY_NEXT_HOST, None, self._calculate_backoff(retry_num)\n        else:\n            return self.RETHROW, None, None\n\n    def on_request_error(self, query, consistency, error, retry_num):\n        if retry_num < self.max_num_retries:\n            return self.RETRY_NEXT_HOST, None, self._calculate_backoff(retry_num)\n        else:\n            return self.RETHROW, None, None\n\n\nclass AddressTranslator(object):\n    \"\"\"\n    Interface for translating cluster-defined endpoints.\n\n    The driver discovers nodes using server metadata and topology change events. Normally,\n    the endpoint defined by the server is the right way to connect to a node. In some environments,\n    these addresses may not be reachable, or not preferred (public vs. private IPs in cloud environments,\n    suboptimal routing, etc). This interface allows for translating from server defined endpoints to\n    preferred addresses for driver connections.\n\n    *Note:* :attr:`~Cluster.contact_points` provided while creating the :class:`~.Cluster` instance are not\n    translated using this mechanism -- only addresses received from Cassandra nodes are.\n    \"\"\"\n    def translate(self, addr):\n        \"\"\"\n        Accepts the node ip address, and returns a translated address to be used connecting to this node.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass IdentityTranslator(AddressTranslator):\n    \"\"\"\n    Returns the endpoint with no translation\n    \"\"\"\n    def translate(self, addr):\n        return addr\n\n\nclass EC2MultiRegionTranslator(AddressTranslator):\n    \"\"\"\n    Resolves private ips of the hosts in the same datacenter as the client, and public ips of hosts in other datacenters.\n    \"\"\"\n    def translate(self, addr):\n        \"\"\"\n        Reverse DNS the public broadcast_address, then lookup that hostname to get the AWS-resolved IP, which\n        will point to the private IP address within the same datacenter.\n        \"\"\"\n        # get family of this address so we translate to the same\n        family = socket.getaddrinfo(addr, 0, socket.AF_UNSPEC, socket.SOCK_STREAM)[0][0]\n        host = socket.getfqdn(addr)\n        for a in socket.getaddrinfo(host, 0, family, socket.SOCK_STREAM):\n            try:\n                return a[4][0]\n            except Exception:\n                pass\n        return addr\n\n\nclass SpeculativeExecutionPolicy(object):\n    \"\"\"\n    Interface for specifying speculative execution plans\n    \"\"\"\n\n    def new_plan(self, keyspace, statement):\n        \"\"\"\n        Returns\n\n        :param keyspace:\n        :param statement:\n        :return:\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass SpeculativeExecutionPlan(object):\n    def next_execution(self, host):\n        raise NotImplementedError()\n\n\nclass NoSpeculativeExecutionPlan(SpeculativeExecutionPlan):\n    def next_execution(self, host):\n        return -1\n\n\nclass NoSpeculativeExecutionPolicy(SpeculativeExecutionPolicy):\n\n    def new_plan(self, keyspace, statement):\n        return NoSpeculativeExecutionPlan()\n\n\nclass ConstantSpeculativeExecutionPolicy(SpeculativeExecutionPolicy):\n    \"\"\"\n    A speculative execution policy that sends a new query every X seconds (**delay**) for a maximum of Y attempts (**max_attempts**).\n    \"\"\"\n\n    def __init__(self, delay, max_attempts):\n        self.delay = delay\n        self.max_attempts = max_attempts\n\n    class ConstantSpeculativeExecutionPlan(SpeculativeExecutionPlan):\n        def __init__(self, delay, max_attempts):\n            self.delay = delay\n            self.remaining = max_attempts\n\n        def next_execution(self, host):\n            if self.remaining > 0:\n                self.remaining -= 1\n                return self.delay\n            else:\n                return -1\n\n    def new_plan(self, keyspace, statement):\n        return self.ConstantSpeculativeExecutionPlan(self.delay, self.max_attempts)\n\n\nclass WrapperPolicy(LoadBalancingPolicy):\n\n    def __init__(self, child_policy):\n        self._child_policy = child_policy\n\n    def distance(self, *args, **kwargs):\n        return self._child_policy.distance(*args, **kwargs)\n\n    def populate(self, cluster, hosts):\n        self._child_policy.populate(cluster, hosts)\n\n    def on_up(self, *args, **kwargs):\n        return self._child_policy.on_up(*args, **kwargs)\n\n    def on_down(self, *args, **kwargs):\n        return self._child_policy.on_down(*args, **kwargs)\n\n    def on_add(self, *args, **kwargs):\n        return self._child_policy.on_add(*args, **kwargs)\n\n    def on_remove(self, *args, **kwargs):\n        return self._child_policy.on_remove(*args, **kwargs)\n\n\nclass DefaultLoadBalancingPolicy(WrapperPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` wrapper that adds the ability to target a specific host first.\n\n    If no host is set on the query, the child policy's query plan will be used as is.\n    \"\"\"\n\n    _cluster_metadata = None\n\n    def populate(self, cluster, hosts):\n        self._cluster_metadata = cluster.metadata\n        self._child_policy.populate(cluster, hosts)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        if query and query.keyspace:\n            keyspace = query.keyspace\n        else:\n            keyspace = working_keyspace\n\n        # TODO remove next major since execute(..., host=XXX) is now available\n        addr = getattr(query, 'target_host', None) if query else None\n        target_host = self._cluster_metadata.get_host(addr)\n\n        child = self._child_policy\n        if target_host and target_host.is_up:\n            yield target_host\n            for h in child.make_query_plan(keyspace, query):\n                if h != target_host:\n                    yield h\n        else:\n            for h in child.make_query_plan(keyspace, query):\n                yield h\n\n\n# TODO for backward compatibility, remove in next major\nclass DSELoadBalancingPolicy(DefaultLoadBalancingPolicy):\n    \"\"\"\n    *Deprecated:* This will be removed in the next major release,\n    consider using :class:`.DefaultLoadBalancingPolicy`.\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(DSELoadBalancingPolicy, self).__init__(*args, **kwargs)\n        warnings.warn(\"DSELoadBalancingPolicy will be removed in 4.0. Consider using \"\n                      \"DefaultLoadBalancingPolicy.\", DeprecationWarning)\n\n\nclass NeverRetryPolicy(RetryPolicy):\n    def _rethrow(self, *args, **kwargs):\n        return self.RETHROW, None\n\n    on_read_timeout = _rethrow\n    on_write_timeout = _rethrow\n    on_unavailable = _rethrow\n\n\nColDesc = namedtuple('ColDesc', ['ks', 'table', 'col'])\n\nclass ColumnEncryptionPolicy(object):\n    \"\"\"\n    A policy enabling (mostly) transparent encryption and decryption of data before it is\n    sent to the cluster.\n\n    Key materials and other configurations are specified on a per-column basis.  This policy can\n    then be used by driver structures which are aware of the underlying columns involved in their\n    work.  In practice this includes the following cases:\n\n    * Prepared statements - data for columns specified by the cluster's policy will be transparently\n      encrypted before they are sent\n    * Rows returned from any query - data for columns specified by the cluster's policy will be\n      transparently decrypted before they are returned to the user\n\n    To enable this functionality, create an instance of this class (or more likely a subclass)\n    before creating a cluster.  This policy should then be configured and supplied to the Cluster\n    at creation time via the :attr:`.Cluster.column_encryption_policy` attribute.\n    \"\"\"\n\n    def encrypt(self, coldesc, obj_bytes):\n        \"\"\"\n        Encrypt the specified bytes using the cryptography materials for the specified column.\n        Largely used internally, although this could also be used to encrypt values supplied\n        to non-prepared statements in a way that is consistent with this policy.\n        \"\"\"\n        raise NotImplementedError()\n\n    def decrypt(self, coldesc, encrypted_bytes):\n        \"\"\"\n        Decrypt the specified (encrypted) bytes using the cryptography materials for the\n        specified column.  Used internally; could be used externally as well but there's\n        not currently an obvious use case.\n        \"\"\"\n        raise NotImplementedError()\n\n    def add_column(self, coldesc, key):\n        \"\"\"\n        Provide cryptography materials to be used when encrypted and/or decrypting data\n        for the specified column.\n        \"\"\"\n        raise NotImplementedError()\n\n    def contains_column(self, coldesc):\n        \"\"\"\n        Predicate to determine if a specific column is supported by this policy.\n        Currently only used internally.\n        \"\"\"\n        raise NotImplementedError()\n\n    def encode_and_encrypt(self, coldesc, obj):\n        \"\"\"\n        Helper function to enable use of this policy on simple (i.e. non-prepared)\n        statements.\n        \"\"\"\n        raise NotImplementedError()\n",
    "cassandra/cqlengine/columns.py": "# Copyright DataStax, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom copy import deepcopy, copy\nfrom datetime import date, datetime, timedelta\nimport logging\nfrom uuid import UUID as _UUID\n\nfrom cassandra import util\nfrom cassandra.cqltypes import SimpleDateType, _cqltypes, UserType\nfrom cassandra.cqlengine import ValidationError\nfrom cassandra.cqlengine.functions import get_total_seconds\nfrom cassandra.util import Duration as _Duration\n\nlog = logging.getLogger(__name__)\n\n\nclass BaseValueManager(object):\n\n    def __init__(self, instance, column, value):\n        self.instance = instance\n        self.column = column\n        self.value = value\n        self.previous_value = None\n        self.explicit = False\n\n    @property\n    def deleted(self):\n        return self.column._val_is_null(self.value) and (self.explicit or not self.column._val_is_null(self.previous_value))\n\n    @property\n    def changed(self):\n        \"\"\"\n        Indicates whether or not this value has changed.\n\n        :rtype: boolean\n\n        \"\"\"\n        if self.explicit:\n            return self.value != self.previous_value\n\n        if isinstance(self.column, BaseContainerColumn):\n            default_value = self.column.get_default()\n            if self.column._val_is_null(default_value):\n                return not self.column._val_is_null(self.value) and self.value != self.previous_value\n            elif self.previous_value is None:\n                return self.value != default_value\n\n            return self.value != self.previous_value\n\n        return False\n\n    def reset_previous_value(self):\n        self.previous_value = deepcopy(self.value)\n\n    def getval(self):\n        return self.value\n\n    def setval(self, val):\n        self.value = val\n        self.explicit = True\n\n    def delval(self):\n        self.value = None\n\n    def get_property(self):\n        _get = lambda slf: self.getval()\n        _set = lambda slf, val: self.setval(val)\n        _del = lambda slf: self.delval()\n\n        if self.column.can_delete:\n            return property(_get, _set, _del)\n        else:\n            return property(_get, _set)\n\n\nclass Column(object):\n\n    # the cassandra type this column maps to\n    db_type = None\n    value_manager = BaseValueManager\n\n    instance_counter = 0\n\n    _python_type_hashable = True\n\n    primary_key = False\n    \"\"\"\n    bool flag, indicates this column is a primary key. The first primary key defined\n    on a model is the partition key (unless partition keys are set), all others are cluster keys\n    \"\"\"\n\n    partition_key = False\n\n    \"\"\"\n    indicates that this column should be the partition key, defining\n    more than one partition key column creates a compound partition key\n    \"\"\"\n\n    index = False\n    \"\"\"\n    bool flag, indicates an index should be created for this column\n    \"\"\"\n\n    custom_index = False\n    \"\"\"\n    bool flag, indicates an index is managed outside of cqlengine. This is\n    useful if you want to do filter queries on fields that have custom\n    indexes.\n    \"\"\"\n\n    db_field = None\n    \"\"\"\n    the fieldname this field will map to in the database\n    \"\"\"\n\n    default = None\n    \"\"\"\n    the default value, can be a value or a callable (no args)\n    \"\"\"\n\n    required = False\n    \"\"\"\n    boolean, is the field required? Model validation will raise and\n    exception if required is set to True and there is a None value assigned\n    \"\"\"\n\n    clustering_order = None\n    \"\"\"\n    only applicable on clustering keys (primary keys that are not partition keys)\n    determines the order that the clustering keys are sorted on disk\n    \"\"\"\n\n    discriminator_column = False\n    \"\"\"\n    boolean, if set to True, this column will be used for discriminating records\n    of inherited models.\n\n    Should only be set on a column of an abstract model being used for inheritance.\n\n    There may only be one discriminator column per model. See :attr:`~.__discriminator_value__`\n    for how to specify the value of this column on specialized models.\n    \"\"\"\n\n    static = False\n    \"\"\"\n    boolean, if set to True, this is a static column, with a single value per partition\n    \"\"\"\n\n    def __init__(self,\n                 primary_key=False,\n                 partition_key=False,\n                 index=False,\n                 db_field=None,\n                 default=None,\n                 required=False,\n                 clustering_order=None,\n                 discriminator_column=False,\n                 static=False,\n                 custom_index=False):\n        self.partition_key = partition_key\n        self.primary_key = partition_key or primary_key\n        self.index = index\n        self.custom_index = custom_index\n        self.db_field = db_field\n        self.default = default\n        self.required = required\n        self.clustering_order = clustering_order\n        self.discriminator_column = discriminator_column\n\n        # the column name in the model definition\n        self.column_name = None\n        self._partition_key_index = None\n        self.static = static\n\n        self.value = None\n\n        # keep track of instantiation order\n        self.position = Column.instance_counter\n        Column.instance_counter += 1\n\n    def __ne__(self, other):\n        if isinstance(other, Column):\n            return self.position != other.position\n        return NotImplemented\n\n    def __eq__(self, other):\n        if isinstance(other, Column):\n            return self.position == other.position\n        return NotImplemented\n\n    def __lt__(self, other):\n        if isinstance(other, Column):\n            return self.position < other.position\n        return NotImplemented\n\n    def __le__(self, other):\n        if isinstance(other, Column):\n            return self.position <= other.position\n        return NotImplemented\n\n    def __gt__(self, other):\n        if isinstance(other, Column):\n            return self.position > other.position\n        return NotImplemented\n\n    def __ge__(self, other):\n        if isinstance(other, Column):\n            return self.position >= other.position\n        return NotImplemented\n\n    def __hash__(self):\n        return id(self)\n\n    def validate(self, value):\n        \"\"\"\n        Returns a cleaned and validated value. Raises a ValidationError\n        if there's a problem\n        \"\"\"\n        if value is None:\n            if self.required:\n                raise ValidationError('{0} - None values are not allowed'.format(self.column_name or self.db_field))\n        return value\n\n    def to_python(self, value):\n        \"\"\"\n        Converts data from the database into python values\n        raises a ValidationError if the value can't be converted\n        \"\"\"\n        return value\n\n    def to_database(self, value):\n        \"\"\"\n        Converts python value into database value\n        \"\"\"\n        return value\n\n    @property\n    def has_default(self):\n        return self.default is not None\n\n    @property\n    def is_primary_key(self):\n        return self.primary_key\n\n    @property\n    def can_delete(self):\n        return not self.primary_key\n\n    def get_default(self):\n        if self.has_default:\n            if callable(self.default):\n                return self.default()\n            else:\n                return self.default\n\n    def get_column_def(self):\n        \"\"\"\n        Returns a column definition for CQL table definition\n        \"\"\"\n        static = \"static\" if self.static else \"\"\n        return '{0} {1} {2}'.format(self.cql, self.db_type, static)\n\n    # TODO: make columns use cqltypes under the hood\n    # until then, this bridges the gap in using types along with cassandra.metadata for CQL generation\n    def cql_parameterized_type(self):\n        return self.db_type\n\n    def set_column_name(self, name):\n        \"\"\"\n        Sets the column name during document class construction\n        This value will be ignored if db_field is set in __init__\n        \"\"\"\n        self.column_name = name\n\n    @property\n    def db_field_name(self):\n        \"\"\" Returns the name of the cql name of this column \"\"\"\n        return self.db_field if self.db_field is not None else self.column_name\n\n    @property\n    def db_index_name(self):\n        \"\"\" Returns the name of the cql index \"\"\"\n        return 'index_{0}'.format(self.db_field_name)\n\n    @property\n    def has_index(self):\n        return self.index or self.custom_index\n\n    @property\n    def cql(self):\n        return self.get_cql()\n\n    def get_cql(self):\n        return '\"{0}\"'.format(self.db_field_name)\n\n    def _val_is_null(self, val):\n        \"\"\" determines if the given value equates to a null value for the given column type \"\"\"\n        return val is None\n\n    @property\n    def sub_types(self):\n        return []\n\n    @property\n    def cql_type(self):\n        return _cqltypes[self.db_type]\n\n\nclass Blob(Column):\n    \"\"\"\n    Stores a raw binary value\n    \"\"\"\n    db_type = 'blob'\n\n    def to_database(self, value):\n\n        if not isinstance(value, (bytes, bytearray)):\n            raise Exception(\"expecting a binary, got a %s\" % type(value))\n\n        val = super(Bytes, self).to_database(value)\n        return bytearray(val)\n\n\nBytes = Blob\n\n\nclass Inet(Column):\n    \"\"\"\n    Stores an IP address in IPv4 or IPv6 format\n    \"\"\"\n    db_type = 'inet'\n\n\nclass Text(Column):\n    \"\"\"\n    Stores a UTF-8 encoded string\n    \"\"\"\n    db_type = 'text'\n\n    def __init__(self, min_length=None, max_length=None, **kwargs):\n        \"\"\"\n        :param int min_length: Sets the minimum length of this string, for validation purposes.\n            Defaults to 1 if this is a ``required`` column. Otherwise, None.\n        :param int max_length: Sets the maximum length of this string, for validation purposes.\n        \"\"\"\n        self.min_length = (\n            1 if min_length is None and kwargs.get('required', False)\n            else min_length)\n        self.max_length = max_length\n\n        if self.min_length is not None:\n            if self.min_length < 0:\n                raise ValueError(\n                    'Minimum length is not allowed to be negative.')\n\n        if self.max_length is not None:\n            if self.max_length < 0:\n                raise ValueError(\n                    'Maximum length is not allowed to be negative.')\n\n        if self.min_length is not None and self.max_length is not None:\n            if self.max_length < self.min_length:\n                raise ValueError(\n                    'Maximum length must be greater or equal '\n                    'to minimum length.')\n\n        super(Text, self).__init__(**kwargs)\n\n    def validate(self, value):\n        value = super(Text, self).validate(value)\n        if not isinstance(value, (str, bytearray)) and value is not None:\n            raise ValidationError('{0} {1} is not a string'.format(self.column_name, type(value)))\n        if self.max_length is not None:\n            if value and len(value) > self.max_length:\n                raise ValidationError('{0} is longer than {1} characters'.format(self.column_name, self.max_length))\n        if self.min_length:\n            if (self.min_length and not value) or len(value) < self.min_length:\n                raise ValidationError('{0} is shorter than {1} characters'.format(self.column_name, self.min_length))\n        return value\n\n\nclass Ascii(Text):\n    \"\"\"\n    Stores a US-ASCII character string\n    \"\"\"\n    db_type = 'ascii'\n\n    def validate(self, value):\n        \"\"\" Only allow ASCII and None values.\n\n        Check against US-ASCII, a.k.a. 7-bit ASCII, a.k.a. ISO646-US, a.k.a.\n        the Basic Latin block of the Unicode character set.\n\n        Source: https://github.com/apache/cassandra/blob\n        /3dcbe90e02440e6ee534f643c7603d50ca08482b/src/java/org/apache/cassandra\n        /serializers/AsciiSerializer.java#L29\n        \"\"\"\n        value = super(Ascii, self).validate(value)\n        if value:\n            charset = value if isinstance(\n                value, (bytearray, )) else map(ord, value)\n            if not set(range(128)).issuperset(charset):\n                raise ValidationError(\n                    '{!r} is not an ASCII string.'.format(value))\n        return value\n\n\nclass Integer(Column):\n    \"\"\"\n    Stores a 32-bit signed integer value\n    \"\"\"\n\n    db_type = 'int'\n\n    def validate(self, value):\n        val = super(Integer, self).validate(value)\n        if val is None:\n            return\n        try:\n            return int(val)\n        except (TypeError, ValueError):\n            raise ValidationError(\"{0} {1} can't be converted to integral value\".format(self.column_name, value))\n\n    def to_python(self, value):\n        return self.validate(value)\n\n    def to_database(self, value):\n        return self.validate(value)\n\n\nclass TinyInt(Integer):\n    \"\"\"\n    Stores an 8-bit signed integer value\n\n    .. versionadded:: 2.6.0\n\n    requires C* 2.2+ and protocol v4+\n    \"\"\"\n    db_type = 'tinyint'\n\n\nclass SmallInt(Integer):\n    \"\"\"\n    Stores a 16-bit signed integer value\n\n    .. versionadded:: 2.6.0\n\n    requires C* 2.2+ and protocol v4+\n    \"\"\"\n    db_type = 'smallint'\n\n\nclass BigInt(Integer):\n    \"\"\"\n    Stores a 64-bit signed integer value\n    \"\"\"\n    db_type = 'bigint'\n\n\nclass VarInt(Column):\n    \"\"\"\n    Stores an arbitrary-precision integer\n    \"\"\"\n    db_type = 'varint'\n\n    def validate(self, value):\n        val = super(VarInt, self).validate(value)\n        if val is None:\n            return\n        try:\n            return int(val)\n        except (TypeError, ValueError):\n            raise ValidationError(\n                \"{0} {1} can't be converted to integral value\".format(self.column_name, value))\n\n    def to_python(self, value):\n        return self.validate(value)\n\n    def to_database(self, value):\n        return self.validate(value)\n\n\nclass CounterValueManager(BaseValueManager):\n    def __init__(self, instance, column, value):\n        super(CounterValueManager, self).__init__(instance, column, value)\n        self.value = self.value or 0\n        self.previous_value = self.previous_value or 0\n\n\nclass Counter(Integer):\n    \"\"\"\n    Stores a counter that can be incremented and decremented\n    \"\"\"\n    db_type = 'counter'\n\n    value_manager = CounterValueManager\n\n    def __init__(self,\n                 index=False,\n                 db_field=None,\n                 required=False):\n        super(Counter, self).__init__(\n            primary_key=False,\n            partition_key=False,\n            index=index,\n            db_field=db_field,\n            default=0,\n            required=required,\n        )\n\n\nclass DateTime(Column):\n    \"\"\"\n    Stores a datetime value\n    \"\"\"\n    db_type = 'timestamp'\n\n    truncate_microseconds = False\n    \"\"\"\n    Set this ``True`` to have model instances truncate the date, quantizing it in the same way it will be in the database.\n    This allows equality comparison between assigned values and values read back from the database::\n\n        DateTime.truncate_microseconds = True\n        assert Model.create(id=0, d=datetime.utcnow()) == Model.objects(id=0).first()\n\n    Defaults to ``False`` to preserve legacy behavior. May change in the future.\n    \"\"\"\n\n    def to_python(self, value):\n        if value is None:\n            return\n        if isinstance(value, datetime):\n            if DateTime.truncate_microseconds:\n                us = value.microsecond\n                truncated_us = us // 1000 * 1000\n                return value - timedelta(microseconds=us - truncated_us)\n            else:\n                return value\n        elif isinstance(value, date):\n            return datetime(*(value.timetuple()[:6]))\n\n        return datetime.utcfromtimestamp(value)\n\n    def to_database(self, value):\n        value = super(DateTime, self).to_database(value)\n        if value is None:\n            return\n        if not isinstance(value, datetime):\n            if isinstance(value, date):\n                value = datetime(value.year, value.month, value.day)\n            else:\n                raise ValidationError(\"{0} '{1}' is not a datetime object\".format(self.column_name, value))\n        epoch = datetime(1970, 1, 1, tzinfo=value.tzinfo)\n        offset = get_total_seconds(epoch.tzinfo.utcoffset(epoch)) if epoch.tzinfo else 0\n\n        return int((get_total_seconds(value - epoch) - offset) * 1000)\n\n\nclass Date(Column):\n    \"\"\"\n    Stores a simple date, with no time-of-day\n\n    .. versionchanged:: 2.6.0\n\n        removed overload of Date and DateTime. DateTime is a drop-in replacement for legacy models\n\n    requires C* 2.2+ and protocol v4+\n    \"\"\"\n    db_type = 'date'\n\n    def to_database(self, value):\n        if value is None:\n            return\n\n        # need to translate to int version because some dates are not representable in\n        # string form (datetime limitation)\n        d = value if isinstance(value, util.Date) else util.Date(value)\n        return d.days_from_epoch + SimpleDateType.EPOCH_OFFSET_DAYS\n\n    def to_python(self, value):\n        if value is None:\n            return\n        if isinstance(value, util.Date):\n            return value\n        if isinstance(value, datetime):\n            value = value.date()\n        return util.Date(value)\n\nclass Time(Column):\n    \"\"\"\n    Stores a timezone-naive time-of-day, with nanosecond precision\n\n    .. versionadded:: 2.6.0\n\n    requires C* 2.2+ and protocol v4+\n    \"\"\"\n    db_type = 'time'\n\n    def to_database(self, value):\n        value = super(Time, self).to_database(value)\n        if value is None:\n            return\n        # str(util.Time) yields desired CQL encoding\n        return value if isinstance(value, util.Time) else util.Time(value)\n\n    def to_python(self, value):\n        value = super(Time, self).to_database(value)\n        if value is None:\n            return\n        if isinstance(value, util.Time):\n            return value\n        return util.Time(value)\n\nclass Duration(Column):\n    \"\"\"\n    Stores a duration (months, days, nanoseconds)\n\n    .. versionadded:: 3.10.0\n\n    requires C* 3.10+ and protocol v4+\n    \"\"\"\n    db_type = 'duration'\n\n    def validate(self, value):\n        val = super(Duration, self).validate(value)\n        if val is None:\n            return\n        if not isinstance(val, _Duration):\n            raise TypeError('{0} {1} is not a valid Duration.'.format(self.column_name, value))\n        return val\n\n\nclass UUID(Column):\n    \"\"\"\n    Stores a type 1 or 4 UUID\n    \"\"\"\n    db_type = 'uuid'\n\n    def validate(self, value):\n        val = super(UUID, self).validate(value)\n        if val is None:\n            return\n        if isinstance(val, _UUID):\n            return val\n        if isinstance(val, str):\n            try:\n                return _UUID(val)\n            except ValueError:\n                # fall-through to error\n                pass\n        raise ValidationError(\"{0} {1} is not a valid uuid\".format(\n            self.column_name, value))\n\n    def to_python(self, value):\n        return self.validate(value)\n\n    def to_database(self, value):\n        return self.validate(value)\n\n\nclass TimeUUID(UUID):\n    \"\"\"\n    UUID containing timestamp\n    \"\"\"\n\n    db_type = 'timeuuid'\n\n\nclass Boolean(Column):\n    \"\"\"\n    Stores a boolean True or False value\n    \"\"\"\n    db_type = 'boolean'\n\n    def validate(self, value):\n        \"\"\" Always returns a Python boolean. \"\"\"\n        value = super(Boolean, self).validate(value)\n\n        if value is not None:\n            value = bool(value)\n\n        return value\n\n    def to_python(self, value):\n        return self.validate(value)\n\n\nclass BaseFloat(Column):\n    def validate(self, value):\n        value = super(BaseFloat, self).validate(value)\n        if value is None:\n            return\n        try:\n            return float(value)\n        except (TypeError, ValueError):\n            raise ValidationError(\"{0} {1} is not a valid float\".format(self.column_name, value))\n\n    def to_python(self, value):\n        return self.validate(value)\n\n    def to_database(self, value):\n        return self.validate(value)\n\n\nclass Float(BaseFloat):\n    \"\"\"\n    Stores a single-precision floating-point value\n    \"\"\"\n    db_type = 'float'\n\n\nclass Double(BaseFloat):\n    \"\"\"\n    Stores a double-precision floating-point value\n    \"\"\"\n    db_type = 'double'\n\n\nclass Decimal(Column):\n    \"\"\"\n    Stores a variable precision decimal value\n    \"\"\"\n    db_type = 'decimal'\n\n    def validate(self, value):\n        from decimal import Decimal as _Decimal\n        from decimal import InvalidOperation\n        val = super(Decimal, self).validate(value)\n        if val is None:\n            return\n        try:\n            return _Decimal(repr(val)) if isinstance(val, float) else _Decimal(val)\n        except InvalidOperation:\n            raise ValidationError(\"{0} '{1}' can't be coerced to decimal\".format(self.column_name, val))\n\n    def to_python(self, value):\n        return self.validate(value)\n\n    def to_database(self, value):\n        return self.validate(value)\n\n\nclass BaseCollectionColumn(Column):\n    \"\"\"\n    Base Container type for collection-like columns.\n\n    http://cassandra.apache.org/doc/cql3/CQL-3.0.html#collections\n    \"\"\"\n    def __init__(self, types, **kwargs):\n        \"\"\"\n        :param types: a sequence of sub types in this collection\n        \"\"\"\n        instances = []\n        for t in types:\n            inheritance_comparator = issubclass if isinstance(t, type) else isinstance\n            if not inheritance_comparator(t, Column):\n                raise ValidationError(\"%s is not a column class\" % (t,))\n            if t.db_type is None:\n                raise ValidationError(\"%s is an abstract type\" % (t,))\n            inst = t() if isinstance(t, type) else t\n            if isinstance(t, BaseCollectionColumn):\n                inst._freeze_db_type()\n            instances.append(inst)\n\n        self.types = instances\n        super(BaseCollectionColumn, self).__init__(**kwargs)\n\n    def validate(self, value):\n        value = super(BaseCollectionColumn, self).validate(value)\n        # It is dangerous to let collections have more than 65535.\n        # See: https://issues.apache.org/jira/browse/CASSANDRA-5428\n        if value is not None and len(value) > 65535:\n            raise ValidationError(\"{0} Collection can't have more than 65535 elements.\".format(self.column_name))\n        return value\n\n    def _val_is_null(self, val):\n        return not val\n\n    def _freeze_db_type(self):\n        if not self.db_type.startswith('frozen'):\n            self.db_type = \"frozen<%s>\" % (self.db_type,)\n\n    @property\n    def sub_types(self):\n        return self.types\n\n    @property\n    def cql_type(self):\n        return _cqltypes[self.__class__.__name__.lower()].apply_parameters([c.cql_type for c in self.types])\n\n\nclass Tuple(BaseCollectionColumn):\n    \"\"\"\n    Stores a fixed-length set of positional values\n\n    http://docs.datastax.com/en/cql/3.1/cql/cql_reference/tupleType.html\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        :param args: column types representing tuple composition\n        \"\"\"\n        if not args:\n            raise ValueError(\"Tuple must specify at least one inner type\")\n        super(Tuple, self).__init__(args, **kwargs)\n        self.db_type = 'tuple<{0}>'.format(', '.join(typ.db_type for typ in self.types))\n\n    def validate(self, value):\n        val = super(Tuple, self).validate(value)\n        if val is None:\n            return\n        if len(val) > len(self.types):\n            raise ValidationError(\"Value %r has more fields than tuple definition (%s)\" %\n                                  (val, ', '.join(t for t in self.types)))\n        return tuple(t.validate(v) for t, v in zip(self.types, val))\n\n    def to_python(self, value):\n        if value is None:\n            return tuple()\n        return tuple(t.to_python(v) for t, v in zip(self.types, value))\n\n    def to_database(self, value):\n        if value is None:\n            return\n        return tuple(t.to_database(v) for t, v in zip(self.types, value))\n\n\nclass BaseContainerColumn(BaseCollectionColumn):\n    pass\n\n\nclass Set(BaseContainerColumn):\n    \"\"\"\n    Stores a set of unordered, unique values\n\n    http://www.datastax.com/documentation/cql/3.1/cql/cql_using/use_set_t.html\n    \"\"\"\n\n    _python_type_hashable = False\n\n    def __init__(self, value_type, strict=True, default=set, **kwargs):\n        \"\"\"\n        :param value_type: a column class indicating the types of the value\n        :param strict: sets whether non set values will be coerced to set\n            type on validation, or raise a validation error, defaults to True\n        \"\"\"\n        self.strict = strict\n        super(Set, self).__init__((value_type,), default=default, **kwargs)\n        self.value_col = self.types[0]\n        if not self.value_col._python_type_hashable:\n            raise ValidationError(\"Cannot create a Set with unhashable value type (see PYTHON-494)\")\n        self.db_type = 'set<{0}>'.format(self.value_col.db_type)\n\n    def validate(self, value):\n        val = super(Set, self).validate(value)\n        if val is None:\n            return\n        types = (set, util.SortedSet) if self.strict else (set, util.SortedSet, list, tuple)\n        if not isinstance(val, types):\n            if self.strict:\n                raise ValidationError('{0} {1} is not a set object'.format(self.column_name, val))\n            else:\n                raise ValidationError('{0} {1} cannot be coerced to a set object'.format(self.column_name, val))\n\n        if None in val:\n            raise ValidationError(\"{0} None not allowed in a set\".format(self.column_name))\n        # TODO: stop doing this conversion because it doesn't support non-hashable collections as keys (cassandra does)\n        # will need to start using the cassandra.util types in the next major rev (PYTHON-494)\n        return set(self.value_col.validate(v) for v in val)\n\n    def to_python(self, value):\n        if value is None:\n            return set()\n        return set(self.value_col.to_python(v) for v in value)\n\n    def to_database(self, value):\n        if value is None:\n            return None\n        return set(self.value_col.to_database(v) for v in value)\n\n\nclass List(BaseContainerColumn):\n    \"\"\"\n    Stores a list of ordered values\n\n    http://www.datastax.com/documentation/cql/3.1/cql/cql_using/use_list_t.html\n    \"\"\"\n\n    _python_type_hashable = False\n\n    def __init__(self, value_type, default=list, **kwargs):\n        \"\"\"\n        :param value_type: a column class indicating the types of the value\n        \"\"\"\n        super(List, self).__init__((value_type,), default=default, **kwargs)\n        self.value_col = self.types[0]\n        self.db_type = 'list<{0}>'.format(self.value_col.db_type)\n\n    def validate(self, value):\n        val = super(List, self).validate(value)\n        if val is None:\n            return\n        if not isinstance(val, (set, list, tuple)):\n            raise ValidationError('{0} {1} is not a list object'.format(self.column_name, val))\n        if None in val:\n            raise ValidationError(\"{0} None is not allowed in a list\".format(self.column_name))\n        return [self.value_col.validate(v) for v in val]\n\n    def to_python(self, value):\n        if value is None:\n            return []\n        return [self.value_col.to_python(v) for v in value]\n\n    def to_database(self, value):\n        if value is None:\n            return None\n        return [self.value_col.to_database(v) for v in value]\n\n\nclass Map(BaseContainerColumn):\n    \"\"\"\n    Stores a key -> value map (dictionary)\n\n    https://docs.datastax.com/en/dse/6.7/cql/cql/cql_using/useMap.html\n    \"\"\"\n\n    _python_type_hashable = False\n\n    def __init__(self, key_type, value_type, default=dict, **kwargs):\n        \"\"\"\n        :param key_type: a column class indicating the types of the key\n        :param value_type: a column class indicating the types of the value\n        \"\"\"\n        super(Map, self).__init__((key_type, value_type), default=default, **kwargs)\n        self.key_col = self.types[0]\n        self.value_col = self.types[1]\n\n        if not self.key_col._python_type_hashable:\n            raise ValidationError(\"Cannot create a Map with unhashable key type (see PYTHON-494)\")\n\n        self.db_type = 'map<{0}, {1}>'.format(self.key_col.db_type, self.value_col.db_type)\n\n    def validate(self, value):\n        val = super(Map, self).validate(value)\n        if val is None:\n            return\n        if not isinstance(val, (dict, util.OrderedMap)):\n            raise ValidationError('{0} {1} is not a dict object'.format(self.column_name, val))\n        if None in val:\n            raise ValidationError(\"{0} None is not allowed in a map\".format(self.column_name))\n        # TODO: stop doing this conversion because it doesn't support non-hashable collections as keys (cassandra does)\n        # will need to start using the cassandra.util types in the next major rev (PYTHON-494)\n        return dict((self.key_col.validate(k), self.value_col.validate(v)) for k, v in val.items())\n\n    def to_python(self, value):\n        if value is None:\n            return {}\n        if value is not None:\n            return dict((self.key_col.to_python(k), self.value_col.to_python(v)) for k, v in value.items())\n\n    def to_database(self, value):\n        if value is None:\n            return None\n        return dict((self.key_col.to_database(k), self.value_col.to_database(v)) for k, v in value.items())\n\n\nclass UDTValueManager(BaseValueManager):\n    @property\n    def changed(self):\n        if self.explicit:\n            return self.value != self.previous_value\n\n        default_value = self.column.get_default()\n        if not self.column._val_is_null(default_value):\n            return self.value != default_value\n        elif self.previous_value is None:\n            return not self.column._val_is_null(self.value) and self.value.has_changed_fields()\n\n        return False\n\n    def reset_previous_value(self):\n        if self.value is not None:\n            self.value.reset_changed_fields()\n        self.previous_value = copy(self.value)\n\n\nclass UserDefinedType(Column):\n    \"\"\"\n    User Defined Type column\n\n    http://www.datastax.com/documentation/cql/3.1/cql/cql_using/cqlUseUDT.html\n\n    These columns are represented by a specialization of :class:`cassandra.cqlengine.usertype.UserType`.\n\n    Please see :ref:`user_types` for examples and discussion.\n    \"\"\"\n\n    value_manager = UDTValueManager\n\n    def __init__(self, user_type, **kwargs):\n        \"\"\"\n        :param type user_type: specifies the :class:`~.cqlengine.usertype.UserType` model of the column\n        \"\"\"\n        self.user_type = user_type\n        self.db_type = \"frozen<%s>\" % user_type.type_name()\n        super(UserDefinedType, self).__init__(**kwargs)\n\n    @property\n    def sub_types(self):\n        return list(self.user_type._fields.values())\n\n    @property\n    def cql_type(self):\n        return UserType.make_udt_class(keyspace='', udt_name=self.user_type.type_name(),\n                                       field_names=[c.db_field_name for c in self.user_type._fields.values()],\n                                       field_types=[c.cql_type for c in self.user_type._fields.values()])\n\n    def validate(self, value):\n        val = super(UserDefinedType, self).validate(value)\n        if val is None:\n            return\n        val.validate()\n        return val\n\n    def to_python(self, value):\n        if value is None:\n            return\n\n        for name, field in self.user_type._fields.items():\n            if value[name] is not None or isinstance(field, BaseContainerColumn):\n                value[name] = field.to_python(value[name])\n\n        return value\n\n    def to_database(self, value):\n        if value is None:\n            return\n\n        copied_value = deepcopy(value)\n        for name, field in self.user_type._fields.items():\n            if copied_value[name] is not None or isinstance(field, BaseContainerColumn):\n                copied_value[name] = field.to_database(copied_value[name])\n\n        return copied_value\n\n\ndef resolve_udts(col_def, out_list):\n    for col in col_def.sub_types:\n        resolve_udts(col, out_list)\n    if isinstance(col_def, UserDefinedType):\n        out_list.append(col_def.user_type)\n\n\nclass _PartitionKeysToken(Column):\n    \"\"\"\n    virtual column representing token of partition columns.\n    Used by filter(pk__token=Token(...)) filters\n    \"\"\"\n\n    def __init__(self, model):\n        self.partition_columns = list(model._partition_keys.values())\n        super(_PartitionKeysToken, self).__init__(partition_key=True)\n\n    @property\n    def db_field_name(self):\n        return 'token({0})'.format(', '.join(['\"{0}\"'.format(c.db_field_name) for c in self.partition_columns]))\n",
    "cassandra/cqlengine/query.py": "# Copyright DataStax, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport copy\nfrom datetime import datetime, timedelta\nfrom functools import partial\nimport time\nfrom warnings import warn\n\nfrom cassandra.query import SimpleStatement, BatchType as CBatchType, BatchStatement\nfrom cassandra.cqlengine import columns, CQLEngineException, ValidationError, UnicodeMixin\nfrom cassandra.cqlengine import connection as conn\nfrom cassandra.cqlengine.functions import Token, BaseQueryFunction, QueryValue\nfrom cassandra.cqlengine.operators import (InOperator, EqualsOperator, GreaterThanOperator,\n                                           GreaterThanOrEqualOperator, LessThanOperator,\n                                           LessThanOrEqualOperator, ContainsOperator, BaseWhereOperator)\nfrom cassandra.cqlengine.statements import (WhereClause, SelectStatement, DeleteStatement,\n                                            UpdateStatement, InsertStatement,\n                                            BaseCQLStatement, MapDeleteClause, ConditionalClause)\n\n\nclass QueryException(CQLEngineException):\n    pass\n\n\nclass IfNotExistsWithCounterColumn(CQLEngineException):\n    pass\n\n\nclass IfExistsWithCounterColumn(CQLEngineException):\n    pass\n\n\nclass LWTException(CQLEngineException):\n    \"\"\"Lightweight conditional exception.\n\n    This exception will be raised when a write using an `IF` clause could not be\n    applied due to existing data violating the condition. The existing data is\n    available through the ``existing`` attribute.\n\n    :param existing: The current state of the data which prevented the write.\n    \"\"\"\n    def __init__(self, existing):\n        super(LWTException, self).__init__(\"LWT Query was not applied\")\n        self.existing = existing\n\n\nclass DoesNotExist(QueryException):\n    pass\n\n\nclass MultipleObjectsReturned(QueryException):\n    pass\n\n\ndef check_applied(result):\n    \"\"\"\n    Raises LWTException if it looks like a failed LWT request. A LWTException\n    won't be raised in the special case in which there are several failed LWT\n    in a  :class:`~cqlengine.query.BatchQuery`.\n    \"\"\"\n    try:\n        applied = result.was_applied\n    except Exception:\n        applied = True  # result was not LWT form\n    if not applied:\n        raise LWTException(result.one())\n\n\nclass AbstractQueryableColumn(UnicodeMixin):\n    \"\"\"\n    exposes cql query operators through pythons\n    builtin comparator symbols\n    \"\"\"\n\n    def _get_column(self):\n        raise NotImplementedError\n\n    def __unicode__(self):\n        raise NotImplementedError\n\n    def _to_database(self, val):\n        if isinstance(val, QueryValue):\n            return val\n        else:\n            return self._get_column().to_database(val)\n\n    def in_(self, item):\n        \"\"\"\n        Returns an in operator\n\n        used where you'd typically want to use python's `in` operator\n        \"\"\"\n        return WhereClause(str(self), InOperator(), item)\n\n    def contains_(self, item):\n        \"\"\"\n        Returns a CONTAINS operator\n        \"\"\"\n        return WhereClause(str(self), ContainsOperator(), item)\n\n\n    def __eq__(self, other):\n        return WhereClause(str(self), EqualsOperator(), self._to_database(other))\n\n    def __gt__(self, other):\n        return WhereClause(str(self), GreaterThanOperator(), self._to_database(other))\n\n    def __ge__(self, other):\n        return WhereClause(str(self), GreaterThanOrEqualOperator(), self._to_database(other))\n\n    def __lt__(self, other):\n        return WhereClause(str(self), LessThanOperator(), self._to_database(other))\n\n    def __le__(self, other):\n        return WhereClause(str(self), LessThanOrEqualOperator(), self._to_database(other))\n\n\nclass BatchType(object):\n    Unlogged = 'UNLOGGED'\n    Counter = 'COUNTER'\n\n\nclass BatchQuery(object):\n    \"\"\"\n    Handles the batching of queries\n\n    http://docs.datastax.com/en/cql/3.0/cql/cql_reference/batch_r.html\n\n    See :doc:`/cqlengine/batches` for more details.\n    \"\"\"\n    warn_multiple_exec = True\n\n    _consistency = None\n\n    _connection = None\n    _connection_explicit = False\n\n\n    def __init__(self, batch_type=None, timestamp=None, consistency=None, execute_on_exception=False,\n                 timeout=conn.NOT_SET, connection=None):\n        \"\"\"\n        :param batch_type: (optional) One of batch type values available through BatchType enum\n        :type batch_type: BatchType, str or None\n        :param timestamp: (optional) A datetime or timedelta object with desired timestamp to be applied\n            to the batch conditional.\n        :type timestamp: datetime or timedelta or None\n        :param consistency: (optional) One of consistency values (\"ANY\", \"ONE\", \"QUORUM\" etc)\n        :type consistency: The :class:`.ConsistencyLevel` to be used for the batch query, or None.\n        :param execute_on_exception: (Defaults to False) Indicates that when the BatchQuery instance is used\n            as a context manager the queries accumulated within the context must be executed despite\n            encountering an error within the context. By default, any exception raised from within\n            the context scope will cause the batched queries not to be executed.\n        :type execute_on_exception: bool\n        :param timeout: (optional) Timeout for the entire batch (in seconds), if not specified fallback\n            to default session timeout\n        :type timeout: float or None\n        :param str connection: Connection name to use for the batch execution\n        \"\"\"\n        self.queries = []\n        self.batch_type = batch_type\n        if timestamp is not None and not isinstance(timestamp, (datetime, timedelta)):\n            raise CQLEngineException('timestamp object must be an instance of datetime')\n        self.timestamp = timestamp\n        self._consistency = consistency\n        self._execute_on_exception = execute_on_exception\n        self._timeout = timeout\n        self._callbacks = []\n        self._executed = False\n        self._context_entered = False\n        self._connection = connection\n        if connection:\n            self._connection_explicit = True\n\n    def add_query(self, query):\n        if not isinstance(query, BaseCQLStatement):\n            raise CQLEngineException('only BaseCQLStatements can be added to a batch query')\n        self.queries.append(query)\n\n    def consistency(self, consistency):\n        self._consistency = consistency\n\n    def _execute_callbacks(self):\n        for callback, args, kwargs in self._callbacks:\n            callback(*args, **kwargs)\n\n    def add_callback(self, fn, *args, **kwargs):\n        \"\"\"Add a function and arguments to be passed to it to be executed after the batch executes.\n\n        A batch can support multiple callbacks.\n\n        Note, that if the batch does not execute, the callbacks are not executed.\n        A callback, thus, is an \"on batch success\" handler.\n\n        :param fn: Callable object\n        :type fn: callable\n        :param \\*args: Positional arguments to be passed to the callback at the time of execution\n        :param \\*\\*kwargs: Named arguments to be passed to the callback at the time of execution\n        \"\"\"\n        if not callable(fn):\n            raise ValueError(\"Value for argument 'fn' is {0} and is not a callable object.\".format(type(fn)))\n        self._callbacks.append((fn, args, kwargs))\n\n    def execute(self):\n        if self._executed and self.warn_multiple_exec:\n            msg = \"Batch executed multiple times.\"\n            if self._context_entered:\n                msg += \" If using the batch as a context manager, there is no need to call execute directly.\"\n            warn(msg)\n        self._executed = True\n\n        if len(self.queries) == 0:\n            # Empty batch is a no-op\n            # except for callbacks\n            self._execute_callbacks()\n            return\n\n        batch_type = None if self.batch_type is CBatchType.LOGGED else self.batch_type\n        opener = 'BEGIN ' + (str(batch_type) + ' ' if batch_type else '') + ' BATCH'\n        if self.timestamp:\n\n            if isinstance(self.timestamp, int):\n                ts = self.timestamp\n            elif isinstance(self.timestamp, (datetime, timedelta)):\n                ts = self.timestamp\n                if isinstance(self.timestamp, timedelta):\n                    ts += datetime.now()  # Apply timedelta\n                ts = int(time.mktime(ts.timetuple()) * 1e+6 + ts.microsecond)\n            else:\n                raise ValueError(\"Batch expects a long, a timedelta, or a datetime\")\n\n            opener += ' USING TIMESTAMP {0}'.format(ts)\n\n        query_list = [opener]\n        parameters = {}\n        ctx_counter = 0\n        for query in self.queries:\n            query.update_context_id(ctx_counter)\n            ctx = query.get_context()\n            ctx_counter += len(ctx)\n            query_list.append('  ' + str(query))\n            parameters.update(ctx)\n\n        query_list.append('APPLY BATCH;')\n\n        tmp = conn.execute('\\n'.join(query_list), parameters, self._consistency, self._timeout, connection=self._connection)\n        check_applied(tmp)\n\n        self.queries = []\n        self._execute_callbacks()\n\n    def __enter__(self):\n        self._context_entered = True\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        # don't execute if there was an exception by default\n        if exc_type is not None and not self._execute_on_exception:\n            return\n        self.execute()\n\n\nclass ContextQuery(object):\n    \"\"\"\n    A Context manager to allow a Model to switch context easily. Presently, the context only\n    specifies a keyspace for model IO.\n\n    :param \\*args: One or more models. A model should be a class type, not an instance.\n    :param \\*\\*kwargs: (optional) Context parameters: can be *keyspace* or *connection*\n\n    For example:\n\n    .. code-block:: python\n\n            with ContextQuery(Automobile, keyspace='test2') as A:\n                A.objects.create(manufacturer='honda', year=2008, model='civic')\n                print(len(A.objects.all()))  # 1 result\n\n            with ContextQuery(Automobile, keyspace='test4') as A:\n                print(len(A.objects.all()))  # 0 result\n\n            # Multiple models\n            with ContextQuery(Automobile, Automobile2, connection='cluster2') as (A, A2):\n                print(len(A.objects.all()))\n                print(len(A2.objects.all()))\n\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        from cassandra.cqlengine import models\n\n        self.models = []\n\n        if len(args) < 1:\n            raise ValueError(\"No model provided.\")\n\n        keyspace = kwargs.pop('keyspace', None)\n        connection = kwargs.pop('connection', None)\n\n        if kwargs:\n            raise ValueError(\"Unknown keyword argument(s): {0}\".format(\n                ','.join(kwargs.keys())))\n\n        for model in args:\n            try:\n                issubclass(model, models.Model)\n            except TypeError:\n                raise ValueError(\"Models must be derived from base Model.\")\n\n            m = models._clone_model_class(model, {})\n\n            if keyspace:\n                m.__keyspace__ = keyspace\n            if connection:\n                m.__connection__ = connection\n\n            self.models.append(m)\n\n    def __enter__(self):\n        if len(self.models) > 1:\n            return tuple(self.models)\n        return self.models[0]\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        return\n\n\nclass AbstractQuerySet(object):\n\n    def __init__(self, model):\n        super(AbstractQuerySet, self).__init__()\n        self.model = model\n\n        # Where clause filters\n        self._where = []\n\n        # Conditional clause filters\n        self._conditional = []\n\n        # ordering arguments\n        self._order = []\n\n        self._allow_filtering = False\n\n        # CQL has a default limit of 10000, it's defined here\n        # because explicit is better than implicit\n        self._limit = 10000\n\n        # We store the fields for which we use the Equal operator\n        # in a query, so we don't select it from the DB. _defer_fields\n        # will contain the names of the fields in the DB, not the names\n        # of the variables used by the mapper\n        self._defer_fields = set()\n        self._deferred_values = {}\n\n        # This variable will hold the names in the database of the fields\n        # for which we want to query\n        self._only_fields = []\n\n        self._values_list = False\n        self._flat_values_list = False\n\n        # results cache\n        self._result_cache = None\n        self._result_idx = None\n        self._result_generator = None\n        self._materialize_results = True\n\n        self._distinct_fields = None\n\n        self._count = None\n\n        self._batch = None\n        self._ttl =  None\n        self._consistency = None\n        self._timestamp = None\n        self._if_not_exists = False\n        self._timeout = conn.NOT_SET\n        self._if_exists = False\n        self._fetch_size = None\n        self._connection = None\n\n    @property\n    def column_family_name(self):\n        return self.model.column_family_name()\n\n    def _execute(self, statement):\n        if self._batch:\n            return self._batch.add_query(statement)\n        else:\n            connection = self._connection or self.model._get_connection()\n            result = _execute_statement(self.model, statement, self._consistency, self._timeout, connection=connection)\n            if self._if_not_exists or self._if_exists or self._conditional:\n                check_applied(result)\n            return result\n\n    def __unicode__(self):\n        return str(self._select_query())\n\n    def __str__(self):\n        return str(self.__unicode__())\n\n    def __call__(self, *args, **kwargs):\n        return self.filter(*args, **kwargs)\n\n    def __deepcopy__(self, memo):\n        clone = self.__class__(self.model)\n        for k, v in self.__dict__.items():\n            if k in ['_con', '_cur', '_result_cache', '_result_idx', '_result_generator', '_construct_result']:  # don't clone these, which are per-request-execution\n                clone.__dict__[k] = None\n            elif k == '_batch':\n                # we need to keep the same batch instance across\n                # all queryset clones, otherwise the batched queries\n                # fly off into other batch instances which are never\n                # executed, thx @dokai\n                clone.__dict__[k] = self._batch\n            elif k == '_timeout':\n                clone.__dict__[k] = self._timeout\n            else:\n                clone.__dict__[k] = copy.deepcopy(v, memo)\n\n        return clone\n\n    def __len__(self):\n        self._execute_query()\n        return self.count()\n\n    # ----query generation / execution----\n\n    def _select_fields(self):\n        \"\"\" returns the fields to select \"\"\"\n        return []\n\n    def _validate_select_where(self):\n        \"\"\" put select query validation here \"\"\"\n\n    def _select_query(self):\n        \"\"\"\n        Returns a select clause based on the given filter args\n        \"\"\"\n        if self._where:\n            self._validate_select_where()\n        return SelectStatement(\n            self.column_family_name,\n            fields=self._select_fields(),\n            where=self._where,\n            order_by=self._order,\n            limit=self._limit,\n            allow_filtering=self._allow_filtering,\n            distinct_fields=self._distinct_fields,\n            fetch_size=self._fetch_size\n        )\n\n    # ----Reads------\n\n    def _execute_query(self):\n        if self._batch:\n            raise CQLEngineException(\"Only inserts, updates, and deletes are available in batch mode\")\n        if self._result_cache is None:\n            self._result_generator = (i for i in self._execute(self._select_query()))\n            self._result_cache = []\n            self._construct_result = self._maybe_inject_deferred(self._get_result_constructor())\n\n            # \"DISTINCT COUNT()\" is not supported in C* < 2.2, so we need to materialize all results to get\n            # len() and count() working with DISTINCT queries\n            if self._materialize_results or self._distinct_fields:\n                self._fill_result_cache()\n\n    def _fill_result_cache(self):\n        \"\"\"\n        Fill the result cache with all results.\n        \"\"\"\n\n        idx = 0\n        try:\n            while True:\n                idx += 1000\n                self._fill_result_cache_to_idx(idx)\n        except StopIteration:\n            pass\n\n        self._count = len(self._result_cache)\n\n    def _fill_result_cache_to_idx(self, idx):\n        self._execute_query()\n        if self._result_idx is None:\n            self._result_idx = -1\n\n        qty = idx - self._result_idx\n        if qty < 1:\n            return\n        else:\n            for idx in range(qty):\n                self._result_idx += 1\n                while True:\n                    try:\n                        self._result_cache[self._result_idx] = self._construct_result(self._result_cache[self._result_idx])\n                        break\n                    except IndexError:\n                        self._result_cache.append(next(self._result_generator))\n\n    def __iter__(self):\n        self._execute_query()\n\n        idx = 0\n        while True:\n            if len(self._result_cache) <= idx:\n                try:\n                    self._result_cache.append(next(self._result_generator))\n                except StopIteration:\n                    break\n\n            instance = self._result_cache[idx]\n            if isinstance(instance, dict):\n                self._fill_result_cache_to_idx(idx)\n            yield self._result_cache[idx]\n\n            idx += 1\n\n    def __getitem__(self, s):\n        self._execute_query()\n\n        if isinstance(s, slice):\n            start = s.start if s.start else 0\n\n            if start < 0 or (s.stop is not None and s.stop < 0):\n                warn(\"ModelQuerySet slicing with negative indices support will be removed in 4.0.\",\n                     DeprecationWarning)\n\n            # calculate the amount of results that need to be loaded\n            end = s.stop\n            if start < 0 or s.stop is None or s.stop < 0:\n                end = self.count()\n\n            try:\n                self._fill_result_cache_to_idx(end)\n            except StopIteration:\n                pass\n\n            return self._result_cache[start:s.stop:s.step]\n        else:\n            try:\n                s = int(s)\n            except (ValueError, TypeError):\n                raise TypeError('QuerySet indices must be integers')\n\n            if s < 0:\n                warn(\"ModelQuerySet indexing with negative indices support will be removed in 4.0.\",\n                     DeprecationWarning)\n\n            # Using negative indexing is costly since we have to execute a count()\n            if s < 0:\n                num_results = self.count()\n                s += num_results\n\n            try:\n                self._fill_result_cache_to_idx(s)\n            except StopIteration:\n                raise IndexError\n\n            return self._result_cache[s]\n\n    def _get_result_constructor(self):\n        \"\"\"\n        Returns a function that will be used to instantiate query results\n        \"\"\"\n        raise NotImplementedError\n\n    @staticmethod\n    def _construct_with_deferred(f, deferred, row):\n        row.update(deferred)\n        return f(row)\n\n    def _maybe_inject_deferred(self, constructor):\n        return partial(self._construct_with_deferred, constructor, self._deferred_values)\\\n            if self._deferred_values else constructor\n\n    def batch(self, batch_obj):\n        \"\"\"\n        Set a batch object to run the query on.\n\n        Note: running a select query with a batch object will raise an exception\n        \"\"\"\n        if self._connection:\n            raise CQLEngineException(\"Cannot specify the connection on model in batch mode.\")\n\n        if batch_obj is not None and not isinstance(batch_obj, BatchQuery):\n            raise CQLEngineException('batch_obj must be a BatchQuery instance or None')\n        clone = copy.deepcopy(self)\n        clone._batch = batch_obj\n        return clone\n\n    def first(self):\n        try:\n            return next(iter(self))\n        except StopIteration:\n            return None\n\n    def all(self):\n        \"\"\"\n        Returns a queryset matching all rows\n\n        .. code-block:: python\n\n            for user in User.objects().all():\n                print(user)\n        \"\"\"\n        return copy.deepcopy(self)\n\n    def consistency(self, consistency):\n        \"\"\"\n        Sets the consistency level for the operation. See :class:`.ConsistencyLevel`.\n\n        .. code-block:: python\n\n            for user in User.objects(id=3).consistency(CL.ONE):\n                print(user)\n        \"\"\"\n        clone = copy.deepcopy(self)\n        clone._consistency = consistency\n        return clone\n\n    def _parse_filter_arg(self, arg):\n        \"\"\"\n        Parses a filter arg in the format:\n        <colname>__<op>\n        :returns: colname, op tuple\n        \"\"\"\n        statement = arg.rsplit('__', 1)\n        if len(statement) == 1:\n            return arg, None\n        elif len(statement) == 2:\n            return (statement[0], statement[1]) if arg != 'pk__token' else (arg, None)\n        else:\n            raise QueryException(\"Can't parse '{0}'\".format(arg))\n\n    def iff(self, *args, **kwargs):\n        \"\"\"Adds IF statements to queryset\"\"\"\n        if len([x for x in kwargs.values() if x is None]):\n            raise CQLEngineException(\"None values on iff are not allowed\")\n\n        clone = copy.deepcopy(self)\n        for operator in args:\n            if not isinstance(operator, ConditionalClause):\n                raise QueryException('{0} is not a valid query operator'.format(operator))\n            clone._conditional.append(operator)\n\n        for arg, val in kwargs.items():\n            if isinstance(val, Token):\n                raise QueryException(\"Token() values are not valid in conditionals\")\n\n            col_name, col_op = self._parse_filter_arg(arg)\n            try:\n                column = self.model._get_column(col_name)\n            except KeyError:\n                raise QueryException(\"Can't resolve column name: '{0}'\".format(col_name))\n\n            if isinstance(val, BaseQueryFunction):\n                query_val = val\n            else:\n                query_val = column.to_database(val)\n\n            operator_class = BaseWhereOperator.get_operator(col_op or 'EQ')\n            operator = operator_class()\n            clone._conditional.append(WhereClause(column.db_field_name, operator, query_val))\n\n        return clone\n\n    def filter(self, *args, **kwargs):\n        \"\"\"\n        Adds WHERE arguments to the queryset, returning a new queryset\n\n        See :ref:`retrieving-objects-with-filters`\n\n        Returns a QuerySet filtered on the keyword arguments\n        \"\"\"\n        # add arguments to the where clause filters\n        if len([x for x in kwargs.values() if x is None]):\n            raise CQLEngineException(\"None values on filter are not allowed\")\n\n        clone = copy.deepcopy(self)\n        for operator in args:\n            if not isinstance(operator, WhereClause):\n                raise QueryException('{0} is not a valid query operator'.format(operator))\n            clone._where.append(operator)\n\n        for arg, val in kwargs.items():\n            col_name, col_op = self._parse_filter_arg(arg)\n            quote_field = True\n\n            if not isinstance(val, Token):\n                try:\n                    column = self.model._get_column(col_name)\n                except KeyError:\n                    raise QueryException(\"Can't resolve column name: '{0}'\".format(col_name))\n            else:\n                if col_name != 'pk__token':\n                    raise QueryException(\"Token() values may only be compared to the 'pk__token' virtual column\")\n\n                column = columns._PartitionKeysToken(self.model)\n                quote_field = False\n\n                partition_columns = column.partition_columns\n                if len(partition_columns) != len(val.value):\n                    raise QueryException(\n                        'Token() received {0} arguments but model has {1} partition keys'.format(\n                            len(val.value), len(partition_columns)))\n                val.set_columns(partition_columns)\n\n            # get query operator, or use equals if not supplied\n            operator_class = BaseWhereOperator.get_operator(col_op or 'EQ')\n            operator = operator_class()\n\n            if isinstance(operator, InOperator):\n                if not isinstance(val, (list, tuple)):\n                    raise QueryException('IN queries must use a list/tuple value')\n                query_val = [column.to_database(v) for v in val]\n            elif isinstance(val, BaseQueryFunction):\n                query_val = val\n            elif (isinstance(operator, ContainsOperator) and\n                  isinstance(column, (columns.List, columns.Set, columns.Map))):\n                # For ContainsOperator and collections, we query using the value, not the container\n                query_val = val\n            else:\n                query_val = column.to_database(val)\n                if not col_op:  # only equal values should be deferred\n                    clone._defer_fields.add(column.db_field_name)\n                    clone._deferred_values[column.db_field_name] = val  # map by db field name for substitution in results\n\n            clone._where.append(WhereClause(column.db_field_name, operator, query_val, quote_field=quote_field))\n\n        return clone\n\n    def get(self, *args, **kwargs):\n        \"\"\"\n        Returns a single instance matching this query, optionally with additional filter kwargs.\n\n        See :ref:`retrieving-objects-with-filters`\n\n        Returns a single object matching the QuerySet.\n\n        .. code-block:: python\n\n            user = User.get(id=1)\n\n        If no objects are matched, a :class:`~.DoesNotExist` exception is raised.\n\n        If more than one object is found, a :class:`~.MultipleObjectsReturned` exception is raised.\n        \"\"\"\n        if args or kwargs:\n            return self.filter(*args, **kwargs).get()\n\n        self._execute_query()\n\n        # Check that the resultset only contains one element, avoiding sending a COUNT query\n        try:\n            self[1]\n            raise self.model.MultipleObjectsReturned('Multiple objects found')\n        except IndexError:\n            pass\n\n        try:\n            obj = self[0]\n        except IndexError:\n            raise self.model.DoesNotExist\n\n        return obj\n\n    def _get_ordering_condition(self, colname):\n        order_type = 'DESC' if colname.startswith('-') else 'ASC'\n        colname = colname.replace('-', '')\n\n        return colname, order_type\n\n    def order_by(self, *colnames):\n        \"\"\"\n        Sets the column(s) to be used for ordering\n\n        Default order is ascending, prepend a '-' to any column name for descending\n\n        *Note: column names must be a clustering key*\n\n        .. code-block:: python\n\n            from uuid import uuid1,uuid4\n\n            class Comment(Model):\n                photo_id = UUID(primary_key=True)\n                comment_id = TimeUUID(primary_key=True, default=uuid1) # second primary key component is a clustering key\n                comment = Text()\n\n            sync_table(Comment)\n\n            u = uuid4()\n            for x in range(5):\n                Comment.create(photo_id=u, comment=\"test %d\" % x)\n\n            print(\"Normal\")\n            for comment in Comment.objects(photo_id=u):\n                print(comment.comment_id)\n\n            print(\"Reversed\")\n            for comment in Comment.objects(photo_id=u).order_by(\"-comment_id\"):\n                print(comment.comment_id)\n        \"\"\"\n        if len(colnames) == 0:\n            clone = copy.deepcopy(self)\n            clone._order = []\n            return clone\n\n        conditions = []\n        for colname in colnames:\n            conditions.append('\"{0}\" {1}'.format(*self._get_ordering_condition(colname)))\n\n        clone = copy.deepcopy(self)\n        clone._order.extend(conditions)\n        return clone\n\n    def count(self):\n        \"\"\"\n        Returns the number of rows matched by this query.\n\n        *Note: This function executes a SELECT COUNT() and has a performance cost on large datasets*\n        \"\"\"\n        if self._batch:\n            raise CQLEngineException(\"Only inserts, updates, and deletes are available in batch mode\")\n\n        if self._count is None:\n            query = self._select_query()\n            query.count = True\n            result = self._execute(query)\n            count_row = result.one().popitem()\n            self._count = count_row[1]\n        return self._count\n\n    def distinct(self, distinct_fields=None):\n        \"\"\"\n        Returns the DISTINCT rows matched by this query.\n\n        distinct_fields default to the partition key fields if not specified.\n\n        *Note: distinct_fields must be a partition key or a static column*\n\n        .. code-block:: python\n\n            class Automobile(Model):\n                manufacturer = columns.Text(partition_key=True)\n                year = columns.Integer(primary_key=True)\n                model = columns.Text(primary_key=True)\n                price = columns.Decimal()\n\n            sync_table(Automobile)\n\n            # create rows\n\n            Automobile.objects.distinct()\n\n            # or\n\n            Automobile.objects.distinct(['manufacturer'])\n\n        \"\"\"\n\n        clone = copy.deepcopy(self)\n        if distinct_fields:\n            clone._distinct_fields = distinct_fields\n        else:\n            clone._distinct_fields = [x.column_name for x in self.model._partition_keys.values()]\n\n        return clone\n\n    def limit(self, v):\n        \"\"\"\n        Limits the number of results returned by Cassandra. Use *0* or *None* to disable.\n\n        *Note that CQL's default limit is 10,000, so all queries without a limit set explicitly will have an implicit limit of 10,000*\n\n        .. code-block:: python\n\n            # Fetch 100 users\n            for user in User.objects().limit(100):\n                print(user)\n\n            # Fetch all users\n            for user in User.objects().limit(None):\n                print(user)\n        \"\"\"\n\n        if v is None:\n            v = 0\n\n        if not isinstance(v, int):\n            raise TypeError\n        if v == self._limit:\n            return self\n\n        if v < 0:\n            raise QueryException(\"Negative limit is not allowed\")\n\n        clone = copy.deepcopy(self)\n        clone._limit = v\n        return clone\n\n    def fetch_size(self, v):\n        \"\"\"\n        Sets the number of rows that are fetched at a time.\n\n        *Note that driver's default fetch size is 5000.*\n\n        .. code-block:: python\n\n            for user in User.objects().fetch_size(500):\n                print(user)\n        \"\"\"\n\n        if not isinstance(v, int):\n            raise TypeError\n        if v == self._fetch_size:\n            return self\n\n        if v < 1:\n            raise QueryException(\"fetch size less than 1 is not allowed\")\n\n        clone = copy.deepcopy(self)\n        clone._fetch_size = v\n        return clone\n\n    def allow_filtering(self):\n        \"\"\"\n        Enables the (usually) unwise practive of querying on a clustering key without also defining a partition key\n        \"\"\"\n        clone = copy.deepcopy(self)\n        clone._allow_filtering = True\n        return clone\n\n    def _only_or_defer(self, action, fields):\n        if action == 'only' and self._only_fields:\n            raise QueryException(\"QuerySet already has 'only' fields defined\")\n\n        clone = copy.deepcopy(self)\n\n        # check for strange fields\n        missing_fields = [f for f in fields if f not in self.model._columns.keys()]\n        if missing_fields:\n            raise QueryException(\n                \"Can't resolve fields {0} in {1}\".format(\n                    ', '.join(missing_fields), self.model.__name__))\n\n        fields = [self.model._columns[field].db_field_name for field in fields]\n\n        if action == 'defer':\n            clone._defer_fields.update(fields)\n        elif action == 'only':\n            clone._only_fields = fields\n        else:\n            raise ValueError\n\n        return clone\n\n    def only(self, fields):\n        \"\"\" Load only these fields for the returned query \"\"\"\n        return self._only_or_defer('only', fields)\n\n    def defer(self, fields):\n        \"\"\" Don't load these fields for the returned query \"\"\"\n        return self._only_or_defer('defer', fields)\n\n    def create(self, **kwargs):\n        return self.model(**kwargs) \\\n            .batch(self._batch) \\\n            .ttl(self._ttl) \\\n            .consistency(self._consistency) \\\n            .if_not_exists(self._if_not_exists) \\\n            .timestamp(self._timestamp) \\\n            .if_exists(self._if_exists) \\\n            .using(connection=self._connection) \\\n            .save()\n\n    def delete(self):\n        \"\"\"\n        Deletes the contents of a query\n        \"\"\"\n        # validate where clause\n        partition_keys = set(x.db_field_name for x in self.model._partition_keys.values())\n        if partition_keys - set(c.field for c in self._where):\n            raise QueryException(\"The partition key must be defined on delete queries\")\n\n        dq = DeleteStatement(\n            self.column_family_name,\n            where=self._where,\n            timestamp=self._timestamp,\n            conditionals=self._conditional,\n            if_exists=self._if_exists\n        )\n        self._execute(dq)\n\n    def __eq__(self, q):\n        if len(self._where) == len(q._where):\n            return all([w in q._where for w in self._where])\n        return False\n\n    def __ne__(self, q):\n        return not (self != q)\n\n    def timeout(self, timeout):\n        \"\"\"\n        :param timeout: Timeout for the query (in seconds)\n        :type timeout: float or None\n        \"\"\"\n        clone = copy.deepcopy(self)\n        clone._timeout = timeout\n        return clone\n\n    def using(self, keyspace=None, connection=None):\n        \"\"\"\n        Change the context on-the-fly of the Model class (keyspace, connection)\n        \"\"\"\n\n        if connection and self._batch:\n            raise CQLEngineException(\"Cannot specify a connection on model in batch mode.\")\n\n        clone = copy.deepcopy(self)\n        if keyspace:\n            from cassandra.cqlengine.models import _clone_model_class\n            clone.model = _clone_model_class(self.model, {'__keyspace__': keyspace})\n\n        if connection:\n            clone._connection = connection\n\n        return clone\n\n\nclass ResultObject(dict):\n    \"\"\"\n    adds attribute access to a dictionary\n    \"\"\"\n\n    def __getattr__(self, item):\n        try:\n            return self[item]\n        except KeyError:\n            raise AttributeError\n\n\nclass SimpleQuerySet(AbstractQuerySet):\n    \"\"\"\n    Overrides _get_result_constructor for querysets that do not define a model (e.g. NamedTable queries)\n    \"\"\"\n\n    def _get_result_constructor(self):\n        \"\"\"\n        Returns a function that will be used to instantiate query results\n        \"\"\"\n        return ResultObject\n\n\nclass ModelQuerySet(AbstractQuerySet):\n    \"\"\"\n    \"\"\"\n    def _validate_select_where(self):\n        \"\"\" Checks that a filterset will not create invalid select statement \"\"\"\n        # check that there's either a =, a IN or a CONTAINS (collection)\n        # relationship with a primary key or indexed field. We also allow\n        # custom indexes to be queried with any operator (a difference\n        # between a secondary index)\n        equal_ops = [self.model._get_column_by_db_name(w.field) \\\n                     for w in self._where if not isinstance(w.value, Token)\n                     and (isinstance(w.operator, EqualsOperator)\n                          or self.model._get_column_by_db_name(w.field).custom_index)]\n        token_comparison = any([w for w in self._where if isinstance(w.value, Token)])\n        if not any(w.primary_key or w.has_index for w in equal_ops) and not token_comparison and not self._allow_filtering:\n            raise QueryException(\n                ('Where clauses require either  =, a IN or a CONTAINS '\n                 '(collection) comparison with either a primary key or '\n                 'indexed field. You might want to consider setting '\n                 'custom_index on fields that you manage index outside '\n                 'cqlengine.'))\n\n        if not self._allow_filtering:\n            # if the query is not on an indexed field\n            if not any(w.has_index for w in equal_ops):\n                if not any([w.partition_key for w in equal_ops]) and not token_comparison:\n                    raise QueryException(\n                        ('Filtering on a clustering key without a partition '\n                         'key is not allowed unless allow_filtering() is '\n                         'called on the queryset. You might want to consider '\n                         'setting custom_index on fields that you manage '\n                         'index outside cqlengine.'))\n\n    def _select_fields(self):\n        if self._defer_fields or self._only_fields:\n            fields = [columns.db_field_name for columns in self.model._columns.values()]\n            if self._defer_fields:\n                fields = [f for f in fields if f not in self._defer_fields]\n                # select the partition keys if all model fields are set defer\n                if not fields:\n                    fields = [columns.db_field_name for columns in self.model._partition_keys.values()]\n            if self._only_fields:\n                fields = [f for f in fields if f in self._only_fields]\n            if not fields:\n                raise QueryException('No fields in select query. Only fields: \"{0}\", defer fields: \"{1}\"'.format(\n                    ','.join(self._only_fields), ','.join(self._defer_fields)))\n            return fields\n        return super(ModelQuerySet, self)._select_fields()\n\n    def _get_result_constructor(self):\n        \"\"\" Returns a function that will be used to instantiate query results \"\"\"\n        if not self._values_list:  # we want models\n            return self.model._construct_instance\n        elif self._flat_values_list:  # the user has requested flattened list (1 value per row)\n            key = self._only_fields[0]\n            return lambda row: row[key]\n        else:\n            return lambda row: [row[f] for f in self._only_fields]\n\n    def _get_ordering_condition(self, colname):\n        colname, order_type = super(ModelQuerySet, self)._get_ordering_condition(colname)\n\n        column = self.model._columns.get(colname)\n        if column is None:\n            raise QueryException(\"Can't resolve the column name: '{0}'\".format(colname))\n\n        # validate the column selection\n        if not column.primary_key:\n            raise QueryException(\n                \"Can't order on '{0}', can only order on (clustered) primary keys\".format(colname))\n\n        pks = [v for k, v in self.model._columns.items() if v.primary_key]\n        if column == pks[0]:\n            raise QueryException(\n                \"Can't order by the first primary key (partition key), clustering (secondary) keys only\")\n\n        return column.db_field_name, order_type\n\n    def values_list(self, *fields, **kwargs):\n        \"\"\" Instructs the query set to return tuples, not model instance \"\"\"\n        flat = kwargs.pop('flat', False)\n        if kwargs:\n            raise TypeError('Unexpected keyword arguments to values_list: %s'\n                            % (kwargs.keys(),))\n        if flat and len(fields) > 1:\n            raise TypeError(\"'flat' is not valid when values_list is called with more than one field.\")\n        clone = self.only(fields)\n        clone._values_list = True\n        clone._flat_values_list = flat\n        return clone\n\n    def ttl(self, ttl):\n        \"\"\"\n        Sets the ttl (in seconds) for modified data.\n\n        *Note that running a select query with a ttl value will raise an exception*\n        \"\"\"\n        clone = copy.deepcopy(self)\n        clone._ttl = ttl\n        return clone\n\n    def timestamp(self, timestamp):\n        \"\"\"\n        Allows for custom timestamps to be saved with the record.\n        \"\"\"\n        clone = copy.deepcopy(self)\n        clone._timestamp = timestamp\n        return clone\n\n    def if_not_exists(self):\n        \"\"\"\n        Check the existence of an object before insertion.\n\n        If the insertion isn't applied, a LWTException is raised.\n        \"\"\"\n        if self.model._has_counter:\n            raise IfNotExistsWithCounterColumn('if_not_exists cannot be used with tables containing counter columns')\n        clone = copy.deepcopy(self)\n        clone._if_not_exists = True\n        return clone\n\n    def if_exists(self):\n        \"\"\"\n        Check the existence of an object before an update or delete.\n\n        If the update or delete isn't applied, a LWTException is raised.\n        \"\"\"\n        if self.model._has_counter:\n            raise IfExistsWithCounterColumn('if_exists cannot be used with tables containing counter columns')\n        clone = copy.deepcopy(self)\n        clone._if_exists = True\n        return clone\n\n    def update(self, **values):\n        \"\"\"\n        Performs an update on the row selected by the queryset. Include values to update in the\n        update like so:\n\n        .. code-block:: python\n\n            Model.objects(key=n).update(value='x')\n\n        Passing in updates for columns which are not part of the model will raise a ValidationError.\n\n        Per column validation will be performed, but instance level validation will not\n        (i.e., `Model.validate` is not called).  This is sometimes referred to as a blind update.\n\n        For example:\n\n        .. code-block:: python\n\n            class User(Model):\n                id = Integer(primary_key=True)\n                name = Text()\n\n            setup([\"localhost\"], \"test\")\n            sync_table(User)\n\n            u = User.create(id=1, name=\"jon\")\n\n            User.objects(id=1).update(name=\"Steve\")\n\n            # sets name to null\n            User.objects(id=1).update(name=None)\n\n\n        Also supported is blindly adding and removing elements from container columns,\n        without loading a model instance from Cassandra.\n\n        Using the syntax `.update(column_name={x, y, z})` will overwrite the contents of the container, like updating a\n        non container column. However, adding `__<operation>` to the end of the keyword arg, makes the update call add\n        or remove items from the collection, without overwriting then entire column.\n\n        Given the model below, here are the operations that can be performed on the different container columns:\n\n        .. code-block:: python\n\n            class Row(Model):\n                row_id      = columns.Integer(primary_key=True)\n                set_column  = columns.Set(Integer)\n                list_column = columns.List(Integer)\n                map_column  = columns.Map(Integer, Integer)\n\n        :class:`~cqlengine.columns.Set`\n\n        - `add`: adds the elements of the given set to the column\n        - `remove`: removes the elements of the given set to the column\n\n\n        .. code-block:: python\n\n            # add elements to a set\n            Row.objects(row_id=5).update(set_column__add={6})\n\n            # remove elements to a set\n            Row.objects(row_id=5).update(set_column__remove={4})\n\n        :class:`~cqlengine.columns.List`\n\n        - `append`: appends the elements of the given list to the end of the column\n        - `prepend`: prepends the elements of the given list to the beginning of the column\n\n        .. code-block:: python\n\n            # append items to a list\n            Row.objects(row_id=5).update(list_column__append=[6, 7])\n\n            # prepend items to a list\n            Row.objects(row_id=5).update(list_column__prepend=[1, 2])\n\n\n        :class:`~cqlengine.columns.Map`\n\n        - `update`: adds the given keys/values to the columns, creating new entries if they didn't exist, and overwriting old ones if they did\n\n        .. code-block:: python\n\n            # add items to a map\n            Row.objects(row_id=5).update(map_column__update={1: 2, 3: 4})\n\n            # remove items from a map\n            Row.objects(row_id=5).update(map_column__remove={1, 2})\n        \"\"\"\n        if not values:\n            return\n\n        nulled_columns = set()\n        updated_columns = set()\n        us = UpdateStatement(self.column_family_name, where=self._where, ttl=self._ttl,\n                             timestamp=self._timestamp, conditionals=self._conditional, if_exists=self._if_exists)\n        for name, val in values.items():\n            col_name, col_op = self._parse_filter_arg(name)\n            col = self.model._columns.get(col_name)\n            # check for nonexistant columns\n            if col is None:\n                raise ValidationError(\"{0}.{1} has no column named: {2}\".format(self.__module__, self.model.__name__, col_name))\n            # check for primary key update attempts\n            if col.is_primary_key:\n                raise ValidationError(\"Cannot apply update to primary key '{0}' for {1}.{2}\".format(col_name, self.__module__, self.model.__name__))\n\n            if col_op == 'remove' and isinstance(col, columns.Map):\n                if not isinstance(val, set):\n                    raise ValidationError(\n                        \"Cannot apply update operation '{0}' on column '{1}' with value '{2}'. A set is required.\".format(col_op, col_name, val))\n                val = {v: None for v in val}\n            else:\n                # we should not provide default values in this use case.\n                val = col.validate(val)\n\n            if val is None:\n                nulled_columns.add(col_name)\n                continue\n\n            us.add_update(col, val, operation=col_op)\n            updated_columns.add(col_name)\n\n        if us.assignments:\n            self._execute(us)\n\n        if nulled_columns:\n            delete_conditional = [condition for condition in self._conditional\n                                  if condition.field not in updated_columns] if self._conditional else None\n            ds = DeleteStatement(self.column_family_name, fields=nulled_columns,\n                                 where=self._where, conditionals=delete_conditional, if_exists=self._if_exists)\n            self._execute(ds)\n\n\nclass DMLQuery(object):\n    \"\"\"\n    A query object used for queries performing inserts, updates, or deletes\n\n    this is usually instantiated by the model instance to be modified\n\n    unlike the read query object, this is mutable\n    \"\"\"\n    _ttl = None\n    _consistency = None\n    _timestamp = None\n    _if_not_exists = False\n    _if_exists = False\n\n    def __init__(self, model, instance=None, batch=None, ttl=None, consistency=None, timestamp=None,\n                 if_not_exists=False, conditional=None, timeout=conn.NOT_SET, if_exists=False):\n        self.model = model\n        self.column_family_name = self.model.column_family_name()\n        self.instance = instance\n        self._batch = batch\n        self._ttl = ttl\n        self._consistency = consistency\n        self._timestamp = timestamp\n        self._if_not_exists = if_not_exists\n        self._if_exists = if_exists\n        self._conditional = conditional\n        self._timeout = timeout\n\n    def _execute(self, statement):\n        connection = self.instance._get_connection() if self.instance else self.model._get_connection()\n        if self._batch:\n            if self._batch._connection:\n                if not self._batch._connection_explicit and connection and \\\n                        connection != self._batch._connection:\n                            raise CQLEngineException('BatchQuery queries must be executed on the same connection')\n            else:\n                # set the BatchQuery connection from the model\n                self._batch._connection = connection\n            return self._batch.add_query(statement)\n        else:\n            results = _execute_statement(self.model, statement, self._consistency, self._timeout, connection=connection)\n            if self._if_not_exists or self._if_exists or self._conditional:\n                check_applied(results)\n            return results\n\n    def batch(self, batch_obj):\n        if batch_obj is not None and not isinstance(batch_obj, BatchQuery):\n            raise CQLEngineException('batch_obj must be a BatchQuery instance or None')\n        self._batch = batch_obj\n        return self\n\n    def _delete_null_columns(self, conditionals=None):\n        \"\"\"\n        executes a delete query to remove columns that have changed to null\n        \"\"\"\n        ds = DeleteStatement(self.column_family_name, conditionals=conditionals, if_exists=self._if_exists)\n        deleted_fields = False\n        static_only = True\n        for _, v in self.instance._values.items():\n            col = v.column\n            if v.deleted:\n                ds.add_field(col.db_field_name)\n                deleted_fields = True\n                static_only &= col.static\n            elif isinstance(col, columns.Map):\n                uc = MapDeleteClause(col.db_field_name, v.value, v.previous_value)\n                if uc.get_context_size() > 0:\n                    ds.add_field(uc)\n                    deleted_fields = True\n                    static_only |= col.static\n\n        if deleted_fields:\n            keys = self.model._partition_keys if static_only else self.model._primary_keys\n            for name, col in keys.items():\n                ds.add_where(col, EqualsOperator(), getattr(self.instance, name))\n            self._execute(ds)\n\n    def update(self):\n        \"\"\"\n        updates a row.\n        This is a blind update call.\n        All validation and cleaning needs to happen\n        prior to calling this.\n        \"\"\"\n        if self.instance is None:\n            raise CQLEngineException(\"DML Query intance attribute is None\")\n        assert type(self.instance) == self.model\n        null_clustering_key = False if len(self.instance._clustering_keys) == 0 else True\n        static_changed_only = True\n        statement = UpdateStatement(self.column_family_name, ttl=self._ttl, timestamp=self._timestamp,\n                                    conditionals=self._conditional, if_exists=self._if_exists)\n        for name, col in self.instance._clustering_keys.items():\n            null_clustering_key = null_clustering_key and col._val_is_null(getattr(self.instance, name, None))\n\n        updated_columns = set()\n        # get defined fields and their column names\n        for name, col in self.model._columns.items():\n            # if clustering key is null, don't include non static columns\n            if null_clustering_key and not col.static and not col.partition_key:\n                continue\n            if not col.is_primary_key:\n                val = getattr(self.instance, name, None)\n                val_mgr = self.instance._values[name]\n\n                if val is None:\n                    continue\n\n                if not val_mgr.changed and not isinstance(col, columns.Counter):\n                    continue\n\n                static_changed_only = static_changed_only and col.static\n                statement.add_update(col, val, previous=val_mgr.previous_value)\n                updated_columns.add(col.db_field_name)\n\n        if statement.assignments:\n            for name, col in self.model._primary_keys.items():\n                # only include clustering key if clustering key is not null, and non static columns are changed to avoid cql error\n                if (null_clustering_key or static_changed_only) and (not col.partition_key):\n                    continue\n                statement.add_where(col, EqualsOperator(), getattr(self.instance, name))\n            self._execute(statement)\n\n        if not null_clustering_key:\n            # remove conditions on fields that have been updated\n            delete_conditionals = [condition for condition in self._conditional\n                                   if condition.field not in updated_columns] if self._conditional else None\n            self._delete_null_columns(delete_conditionals)\n\n    def save(self):\n        \"\"\"\n        Creates / updates a row.\n        This is a blind insert call.\n        All validation and cleaning needs to happen\n        prior to calling this.\n        \"\"\"\n        if self.instance is None:\n            raise CQLEngineException(\"DML Query intance attribute is None\")\n        assert type(self.instance) == self.model\n\n        nulled_fields = set()\n        if self.instance._has_counter or self.instance._can_update():\n            if self.instance._has_counter:\n                warn(\"'create' and 'save' actions on Counters are deprecated. It will be disallowed in 4.0. \"\n                    \"Use the 'update' mechanism instead.\", DeprecationWarning)\n            return self.update()\n        else:\n            insert = InsertStatement(self.column_family_name, ttl=self._ttl, timestamp=self._timestamp, if_not_exists=self._if_not_exists)\n            static_save_only = False if len(self.instance._clustering_keys) == 0 else True\n            for name, col in self.instance._clustering_keys.items():\n                static_save_only = static_save_only and col._val_is_null(getattr(self.instance, name, None))\n            for name, col in self.instance._columns.items():\n                if static_save_only and not col.static and not col.partition_key:\n                    continue\n                val = getattr(self.instance, name, None)\n                if col._val_is_null(val):\n                    if self.instance._values[name].changed:\n                        nulled_fields.add(col.db_field_name)\n                    continue\n                if col.has_default and not self.instance._values[name].changed:\n                    # Ensure default columns included in a save() are marked as explicit, to get them *persisted* properly\n                    self.instance._values[name].explicit = True\n                insert.add_assignment(col, getattr(self.instance, name, None))\n\n        # skip query execution if it's empty\n        # caused by pointless update queries\n        if not insert.is_empty:\n            self._execute(insert)\n        # delete any nulled columns\n        if not static_save_only:\n            self._delete_null_columns()\n\n    def delete(self):\n        \"\"\" Deletes one instance \"\"\"\n        if self.instance is None:\n            raise CQLEngineException(\"DML Query instance attribute is None\")\n\n        ds = DeleteStatement(self.column_family_name, timestamp=self._timestamp, conditionals=self._conditional, if_exists=self._if_exists)\n        for name, col in self.model._primary_keys.items():\n            val = getattr(self.instance, name)\n            if val is None and not col.partition_key:\n                continue\n            ds.add_where(col, EqualsOperator(), val)\n        self._execute(ds)\n\n\ndef _execute_statement(model, statement, consistency_level, timeout, connection=None):\n    params = statement.get_context()\n    s = SimpleStatement(str(statement), consistency_level=consistency_level, fetch_size=statement.fetch_size)\n    if model._partition_key_index:\n        key_values = statement.partition_key_values(model._partition_key_index)\n        if not any(v is None for v in key_values):\n            parts = model._routing_key_from_values(key_values, conn.get_cluster(connection).protocol_version)\n            s.routing_key = parts\n            s.keyspace = model._get_keyspace()\n    connection = connection or model._get_connection()\n    return conn.execute(s, params, timeout=timeout, connection=connection)\n",
    "cassandra/cqlengine/usertype.py": "# Copyright DataStax, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\n\nfrom cassandra.util import OrderedDict\nfrom cassandra.cqlengine import CQLEngineException\nfrom cassandra.cqlengine import columns\nfrom cassandra.cqlengine import connection as conn\nfrom cassandra.cqlengine import models\n\n\nclass UserTypeException(CQLEngineException):\n    pass\n\n\nclass UserTypeDefinitionException(UserTypeException):\n    pass\n\n\nclass BaseUserType(object):\n    \"\"\"\n    The base type class; don't inherit from this, inherit from UserType, defined below\n    \"\"\"\n    __type_name__ = None\n\n    _fields = None\n    _db_map = None\n\n    def __init__(self, **values):\n        self._values = {}\n        if self._db_map:\n            values = dict((self._db_map.get(k, k), v) for k, v in values.items())\n\n        for name, field in self._fields.items():\n            field_default = field.get_default() if field.has_default else None\n            value = values.get(name, field_default)\n            if value is not None or isinstance(field, columns.BaseContainerColumn):\n                value = field.to_python(value)\n            value_mngr = field.value_manager(self, field, value)\n            value_mngr.explicit = name in values\n            self._values[name] = value_mngr\n\n    def __eq__(self, other):\n        if self.__class__ != other.__class__:\n            return False\n\n        keys = set(self._fields.keys())\n        other_keys = set(other._fields.keys())\n        if keys != other_keys:\n            return False\n\n        for key in other_keys:\n            if getattr(self, key, None) != getattr(other, key, None):\n                return False\n\n        return True\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __str__(self):\n        return \"{{{0}}}\".format(', '.join(\"'{0}': {1}\".format(k, getattr(self, k)) for k, v in self._values.items()))\n\n    def has_changed_fields(self):\n        return any(v.changed for v in self._values.values())\n\n    def reset_changed_fields(self):\n        for v in self._values.values():\n            v.reset_previous_value()\n\n    def __iter__(self):\n        for field in self._fields.keys():\n            yield field\n\n    def __getattr__(self, attr):\n        # provides the mapping from db_field to fields\n        try:\n            return getattr(self, self._db_map[attr])\n        except KeyError:\n            raise AttributeError(attr)\n\n    def __getitem__(self, key):\n        if not isinstance(key, str):\n            raise TypeError\n        if key not in self._fields.keys():\n            raise KeyError\n        return getattr(self, key)\n\n    def __setitem__(self, key, val):\n        if not isinstance(key, str):\n            raise TypeError\n        if key not in self._fields.keys():\n            raise KeyError\n        return setattr(self, key, val)\n\n    def __len__(self):\n        try:\n            return self._len\n        except:\n            self._len = len(self._fields.keys())\n            return self._len\n\n    def keys(self):\n        \"\"\" Returns a list of column IDs. \"\"\"\n        return [k for k in self]\n\n    def values(self):\n        \"\"\" Returns list of column values. \"\"\"\n        return [self[k] for k in self]\n\n    def items(self):\n        \"\"\" Returns a list of column ID/value tuples. \"\"\"\n        return [(k, self[k]) for k in self]\n\n    @classmethod\n    def register_for_keyspace(cls, keyspace, connection=None):\n        conn.register_udt(keyspace, cls.type_name(), cls, connection=connection)\n\n    @classmethod\n    def type_name(cls):\n        \"\"\"\n        Returns the type name if it's been defined\n        otherwise, it creates it from the class name\n        \"\"\"\n        if cls.__type_name__:\n            type_name = cls.__type_name__.lower()\n        else:\n            camelcase = re.compile(r'([a-z])([A-Z])')\n            ccase = lambda s: camelcase.sub(lambda v: '{0}_{1}'.format(v.group(1), v.group(2)), s)\n\n            type_name = ccase(cls.__name__)\n            # trim to less than 48 characters or cassandra will complain\n            type_name = type_name[-48:]\n            type_name = type_name.lower()\n            type_name = re.sub(r'^_+', '', type_name)\n            cls.__type_name__ = type_name\n\n        return type_name\n\n    def validate(self):\n        \"\"\"\n        Cleans and validates the field values\n        \"\"\"\n        for name, field in self._fields.items():\n            v = getattr(self, name)\n            if v is None and not self._values[name].explicit and field.has_default:\n                v = field.get_default()\n            val = field.validate(v)\n            setattr(self, name, val)\n\n\nclass UserTypeMetaClass(type):\n\n    def __new__(cls, name, bases, attrs):\n        field_dict = OrderedDict()\n\n        field_defs = [(k, v) for k, v in attrs.items() if isinstance(v, columns.Column)]\n        field_defs = sorted(field_defs, key=lambda x: x[1].position)\n\n        def _transform_column(field_name, field_obj):\n            field_dict[field_name] = field_obj\n            field_obj.set_column_name(field_name)\n            attrs[field_name] = models.ColumnDescriptor(field_obj)\n\n        # transform field definitions\n        for k, v in field_defs:\n            # don't allow a field with the same name as a built-in attribute or method\n            if k in BaseUserType.__dict__:\n                raise UserTypeDefinitionException(\"field '{0}' conflicts with built-in attribute/method\".format(k))\n            _transform_column(k, v)\n\n        attrs['_fields'] = field_dict\n\n        db_map = {}\n        for field_name, field in field_dict.items():\n            db_field = field.db_field_name\n            if db_field != field_name:\n                if db_field in field_dict:\n                    raise UserTypeDefinitionException(\"db_field '{0}' for field '{1}' conflicts with another attribute name\".format(db_field, field_name))\n                db_map[db_field] = field_name\n        attrs['_db_map'] = db_map\n\n        klass = super(UserTypeMetaClass, cls).__new__(cls, name, bases, attrs)\n\n        return klass\n\n\nclass UserType(BaseUserType, metaclass=UserTypeMetaClass):\n    \"\"\"\n    This class is used to model User Defined Types. To define a type, declare a class inheriting from this,\n    and assign field types as class attributes:\n\n    .. code-block:: python\n\n        # connect with default keyspace ...\n\n        from cassandra.cqlengine.columns import Text, Integer\n        from cassandra.cqlengine.usertype import UserType\n\n        class address(UserType):\n            street = Text()\n            zipcode = Integer()\n\n        from cassandra.cqlengine import management\n        management.sync_type(address)\n\n    Please see :ref:`user_types` for a complete example and discussion.\n    \"\"\"\n\n    __type_name__ = None\n    \"\"\"\n    *Optional.* Sets the name of the CQL type for this type.\n\n    If not specified, the type name will be the name of the class, with it's module name as it's prefix.\n    \"\"\"\n",
    "cassandra/cqlengine/models.py": "# Copyright DataStax, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport re\nfrom warnings import warn\n\nfrom cassandra.cqlengine import CQLEngineException, ValidationError\nfrom cassandra.cqlengine import columns\nfrom cassandra.cqlengine import connection\nfrom cassandra.cqlengine import query\nfrom cassandra.cqlengine.query import DoesNotExist as _DoesNotExist\nfrom cassandra.cqlengine.query import MultipleObjectsReturned as _MultipleObjectsReturned\nfrom cassandra.metadata import protect_name\nfrom cassandra.util import OrderedDict\n\nlog = logging.getLogger(__name__)\n\n\ndef _clone_model_class(model, attrs):\n    new_type = type(model.__name__, (model,), attrs)\n    try:\n        new_type.__abstract__ = model.__abstract__\n        new_type.__discriminator_value__ = model.__discriminator_value__\n        new_type.__default_ttl__ = model.__default_ttl__\n    except AttributeError:\n        pass\n    return new_type\n\n\nclass ModelException(CQLEngineException):\n    pass\n\n\nclass ModelDefinitionException(ModelException):\n    pass\n\n\nclass PolymorphicModelException(ModelException):\n    pass\n\n\nclass UndefinedKeyspaceWarning(Warning):\n    pass\n\nDEFAULT_KEYSPACE = None\n\n\nclass hybrid_classmethod(object):\n    \"\"\"\n    Allows a method to behave as both a class method and\n    normal instance method depending on how it's called\n    \"\"\"\n    def __init__(self, clsmethod, instmethod):\n        self.clsmethod = clsmethod\n        self.instmethod = instmethod\n\n    def __get__(self, instance, owner):\n        if instance is None:\n            return self.clsmethod.__get__(owner, owner)\n        else:\n            return self.instmethod.__get__(instance, owner)\n\n    def __call__(self, *args, **kwargs):\n        \"\"\"\n        Just a hint to IDEs that it's ok to call this\n        \"\"\"\n        raise NotImplementedError\n\n\nclass QuerySetDescriptor(object):\n    \"\"\"\n    returns a fresh queryset for the given model\n    it's declared on everytime it's accessed\n    \"\"\"\n\n    def __get__(self, obj, model):\n        \"\"\" :rtype: ModelQuerySet \"\"\"\n        if model.__abstract__:\n            raise CQLEngineException('cannot execute queries against abstract models')\n        queryset = model.__queryset__(model)\n\n        # if this is a concrete polymorphic model, and the discriminator\n        # key is an indexed column, add a filter clause to only return\n        # logical rows of the proper type\n        if model._is_polymorphic and not model._is_polymorphic_base:\n            name, column = model._discriminator_column_name, model._discriminator_column\n            if column.partition_key or column.index:\n                # look for existing poly types\n                return queryset.filter(**{name: model.__discriminator_value__})\n\n        return queryset\n\n    def __call__(self, *args, **kwargs):\n        \"\"\"\n        Just a hint to IDEs that it's ok to call this\n\n        :rtype: ModelQuerySet\n        \"\"\"\n        raise NotImplementedError\n\n\nclass ConditionalDescriptor(object):\n    \"\"\"\n    returns a query set descriptor\n    \"\"\"\n    def __get__(self, instance, model):\n        if instance:\n            def conditional_setter(*prepared_conditional, **unprepared_conditionals):\n                if len(prepared_conditional) > 0:\n                    conditionals = prepared_conditional[0]\n                else:\n                    conditionals = instance.objects.iff(**unprepared_conditionals)._conditional\n                instance._conditional = conditionals\n                return instance\n\n            return conditional_setter\n        qs = model.__queryset__(model)\n\n        def conditional_setter(**unprepared_conditionals):\n            conditionals = model.objects.iff(**unprepared_conditionals)._conditional\n            qs._conditional = conditionals\n            return qs\n        return conditional_setter\n\n    def __call__(self, *args, **kwargs):\n        raise NotImplementedError\n\n\nclass TTLDescriptor(object):\n    \"\"\"\n    returns a query set descriptor\n    \"\"\"\n    def __get__(self, instance, model):\n        if instance:\n            # instance = copy.deepcopy(instance)\n            # instance method\n            def ttl_setter(ts):\n                instance._ttl = ts\n                return instance\n            return ttl_setter\n\n        qs = model.__queryset__(model)\n\n        def ttl_setter(ts):\n            qs._ttl = ts\n            return qs\n\n        return ttl_setter\n\n    def __call__(self, *args, **kwargs):\n        raise NotImplementedError\n\n\nclass TimestampDescriptor(object):\n    \"\"\"\n    returns a query set descriptor with a timestamp specified\n    \"\"\"\n    def __get__(self, instance, model):\n        if instance:\n            # instance method\n            def timestamp_setter(ts):\n                instance._timestamp = ts\n                return instance\n            return timestamp_setter\n\n        return model.objects.timestamp\n\n    def __call__(self, *args, **kwargs):\n        raise NotImplementedError\n\n\nclass IfNotExistsDescriptor(object):\n    \"\"\"\n    return a query set descriptor with a if_not_exists flag specified\n    \"\"\"\n    def __get__(self, instance, model):\n        if instance:\n            # instance method\n            def ifnotexists_setter(ife=True):\n                instance._if_not_exists = ife\n                return instance\n            return ifnotexists_setter\n\n        return model.objects.if_not_exists\n\n    def __call__(self, *args, **kwargs):\n        raise NotImplementedError\n\n\nclass IfExistsDescriptor(object):\n    \"\"\"\n    return a query set descriptor with a if_exists flag specified\n    \"\"\"\n    def __get__(self, instance, model):\n        if instance:\n            # instance method\n            def ifexists_setter(ife=True):\n                instance._if_exists = ife\n                return instance\n            return ifexists_setter\n\n        return model.objects.if_exists\n\n    def __call__(self, *args, **kwargs):\n        raise NotImplementedError\n\n\nclass ConsistencyDescriptor(object):\n    \"\"\"\n    returns a query set descriptor if called on Class, instance if it was an instance call\n    \"\"\"\n    def __get__(self, instance, model):\n        if instance:\n            # instance = copy.deepcopy(instance)\n            def consistency_setter(consistency):\n                instance.__consistency__ = consistency\n                return instance\n            return consistency_setter\n\n        qs = model.__queryset__(model)\n\n        def consistency_setter(consistency):\n            qs._consistency = consistency\n            return qs\n\n        return consistency_setter\n\n    def __call__(self, *args, **kwargs):\n        raise NotImplementedError\n\n\nclass UsingDescriptor(object):\n    \"\"\"\n    return a query set descriptor with a connection context specified\n    \"\"\"\n    def __get__(self, instance, model):\n        if instance:\n            # instance method\n            def using_setter(connection=None):\n                if connection:\n                    instance._connection = connection\n                return instance\n            return using_setter\n\n        return model.objects.using\n\n    def __call__(self, *args, **kwargs):\n        raise NotImplementedError\n\n\nclass ColumnQueryEvaluator(query.AbstractQueryableColumn):\n    \"\"\"\n    Wraps a column and allows it to be used in comparator\n    expressions, returning query operators\n\n    ie:\n    Model.column == 5\n    \"\"\"\n\n    def __init__(self, column):\n        self.column = column\n\n    def __unicode__(self):\n        return self.column.db_field_name\n\n    def _get_column(self):\n        return self.column\n\n\nclass ColumnDescriptor(object):\n    \"\"\"\n    Handles the reading and writing of column values to and from\n    a model instance's value manager, as well as creating\n    comparator queries\n    \"\"\"\n\n    def __init__(self, column):\n        \"\"\"\n        :param column:\n        :type column: columns.Column\n        :return:\n        \"\"\"\n        self.column = column\n        self.query_evaluator = ColumnQueryEvaluator(self.column)\n\n    def __get__(self, instance, owner):\n        \"\"\"\n        Returns either the value or column, depending\n        on if an instance is provided or not\n\n        :param instance: the model instance\n        :type instance: Model\n        \"\"\"\n        try:\n            return instance._values[self.column.column_name].getval()\n        except AttributeError:\n            return self.query_evaluator\n\n    def __set__(self, instance, value):\n        \"\"\"\n        Sets the value on an instance, raises an exception with classes\n        TODO: use None instance to create update statements\n        \"\"\"\n        if instance:\n            return instance._values[self.column.column_name].setval(value)\n        else:\n            raise AttributeError('cannot reassign column values')\n\n    def __delete__(self, instance):\n        \"\"\"\n        Sets the column value to None, if possible\n        \"\"\"\n        if instance:\n            if self.column.can_delete:\n                instance._values[self.column.column_name].delval()\n            else:\n                raise AttributeError('cannot delete {0} columns'.format(self.column.column_name))\n\n\nclass BaseModel(object):\n    \"\"\"\n    The base model class, don't inherit from this, inherit from Model, defined below\n    \"\"\"\n\n    class DoesNotExist(_DoesNotExist):\n        pass\n\n    class MultipleObjectsReturned(_MultipleObjectsReturned):\n        pass\n\n    objects = QuerySetDescriptor()\n    ttl = TTLDescriptor()\n    consistency = ConsistencyDescriptor()\n    iff = ConditionalDescriptor()\n\n    # custom timestamps, see USING TIMESTAMP X\n    timestamp = TimestampDescriptor()\n\n    if_not_exists = IfNotExistsDescriptor()\n\n    if_exists = IfExistsDescriptor()\n\n    using = UsingDescriptor()\n\n    # _len is lazily created by __len__\n\n    __table_name__ = None\n\n    __table_name_case_sensitive__ = False\n\n    __keyspace__ = None\n\n    __connection__ = None\n\n    __discriminator_value__ = None\n\n    __options__ = None\n\n    __compute_routing_key__ = True\n\n    # the queryset class used for this class\n    __queryset__ = query.ModelQuerySet\n    __dmlquery__ = query.DMLQuery\n\n    __consistency__ = None  # can be set per query\n\n    _timestamp = None  # optional timestamp to include with the operation (USING TIMESTAMP)\n\n    _if_not_exists = False  # optional if_not_exists flag to check existence before insertion\n\n    _if_exists = False  # optional if_exists flag to check existence before update\n\n    _table_name = None  # used internally to cache a derived table name\n\n    _connection = None\n\n    def __init__(self, **values):\n        self._ttl = None\n        self._timestamp = None\n        self._conditional = None\n        self._batch = None\n        self._timeout = connection.NOT_SET\n        self._is_persisted = False\n        self._connection = None\n\n        self._values = {}\n        for name, column in self._columns.items():\n            # Set default values on instantiation. Thanks to this, we don't have\n            # to wait anylonger for a call to validate() to have CQLengine set\n            # default columns values.\n            column_default = column.get_default() if column.has_default else None\n            value = values.get(name, column_default)\n            if value is not None or isinstance(column, columns.BaseContainerColumn):\n                value = column.to_python(value)\n            value_mngr = column.value_manager(self, column, value)\n            value_mngr.explicit = name in values\n            self._values[name] = value_mngr\n\n    def __repr__(self):\n        return '{0}({1})'.format(self.__class__.__name__,\n                               ', '.join('{0}={1!r}'.format(k, getattr(self, k))\n                                         for k in self._defined_columns.keys()\n                                         if k != self._discriminator_column_name))\n\n    def __str__(self):\n        \"\"\"\n        Pretty printing of models by their primary key\n        \"\"\"\n        return '{0} <{1}>'.format(self.__class__.__name__,\n                                ', '.join('{0}={1}'.format(k, getattr(self, k)) for k in self._primary_keys.keys()))\n\n    @classmethod\n    def _routing_key_from_values(cls, pk_values, protocol_version):\n        return cls._key_serializer(pk_values, protocol_version)\n\n    @classmethod\n    def _discover_polymorphic_submodels(cls):\n        if not cls._is_polymorphic_base:\n            raise ModelException('_discover_polymorphic_submodels can only be called on polymorphic base classes')\n\n        def _discover(klass):\n            if not klass._is_polymorphic_base and klass.__discriminator_value__ is not None:\n                cls._discriminator_map[klass.__discriminator_value__] = klass\n            for subklass in klass.__subclasses__():\n                _discover(subklass)\n        _discover(cls)\n\n    @classmethod\n    def _get_model_by_discriminator_value(cls, key):\n        if not cls._is_polymorphic_base:\n            raise ModelException('_get_model_by_discriminator_value can only be called on polymorphic base classes')\n        return cls._discriminator_map.get(key)\n\n    @classmethod\n    def _construct_instance(cls, values):\n        \"\"\"\n        method used to construct instances from query results\n        this is where polymorphic deserialization occurs\n        \"\"\"\n        # we're going to take the values, which is from the DB as a dict\n        # and translate that into our local fields\n        # the db_map is a db_field -> model field map\n        if cls._db_map:\n            values = dict((cls._db_map.get(k, k), v) for k, v in values.items())\n\n        if cls._is_polymorphic:\n            disc_key = values.get(cls._discriminator_column_name)\n\n            if disc_key is None:\n                raise PolymorphicModelException('discriminator value was not found in values')\n\n            poly_base = cls if cls._is_polymorphic_base else cls._polymorphic_base\n\n            klass = poly_base._get_model_by_discriminator_value(disc_key)\n            if klass is None:\n                poly_base._discover_polymorphic_submodels()\n                klass = poly_base._get_model_by_discriminator_value(disc_key)\n                if klass is None:\n                    raise PolymorphicModelException(\n                        'unrecognized discriminator column {0} for class {1}'.format(disc_key, poly_base.__name__)\n                    )\n\n            if not issubclass(klass, cls):\n                raise PolymorphicModelException(\n                    '{0} is not a subclass of {1}'.format(klass.__name__, cls.__name__)\n                )\n\n            values = dict((k, v) for k, v in values.items() if k in klass._columns.keys())\n\n        else:\n            klass = cls\n\n        instance = klass(**values)\n        instance._set_persisted(force=True)\n        return instance\n\n    def _set_persisted(self, force=False):\n        # ensure we don't modify to any values not affected by the last save/update\n        for v in [v for v in self._values.values() if v.changed or force]:\n            v.reset_previous_value()\n            v.explicit = False\n        self._is_persisted = True\n\n    def _can_update(self):\n        \"\"\"\n        Called by the save function to check if this should be\n        persisted with update or insert\n\n        :return:\n        \"\"\"\n        if not self._is_persisted:\n            return False\n\n        return all([not self._values[k].changed for k in self._primary_keys])\n\n    @classmethod\n    def _get_keyspace(cls):\n        \"\"\"\n        Returns the manual keyspace, if set, otherwise the default keyspace\n        \"\"\"\n        return cls.__keyspace__ or DEFAULT_KEYSPACE\n\n    @classmethod\n    def _get_column(cls, name):\n        \"\"\"\n        Returns the column matching the given name, raising a key error if\n        it doesn't exist\n\n        :param name: the name of the column to return\n        :rtype: Column\n        \"\"\"\n        return cls._columns[name]\n\n    @classmethod\n    def _get_column_by_db_name(cls, name):\n        \"\"\"\n        Returns the column, mapped by db_field name\n        \"\"\"\n        return cls._columns.get(cls._db_map.get(name, name))\n\n    def __eq__(self, other):\n        if self.__class__ != other.__class__:\n            return False\n\n        # check attribute keys\n        keys = set(self._columns.keys())\n        other_keys = set(other._columns.keys())\n        if keys != other_keys:\n            return False\n\n        return all(getattr(self, key, None) == getattr(other, key, None) for key in other_keys)\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    @classmethod\n    def column_family_name(cls, include_keyspace=True):\n        \"\"\"\n        Returns the column family name if it's been defined\n        otherwise, it creates it from the module and class name\n        \"\"\"\n        cf_name = protect_name(cls._raw_column_family_name())\n        if include_keyspace:\n            keyspace = cls._get_keyspace()\n            if not keyspace:\n                raise CQLEngineException(\"Model keyspace is not set and no default is available. Set model keyspace or setup connection before attempting to generate a query.\")\n            return '{0}.{1}'.format(protect_name(keyspace), cf_name)\n\n        return cf_name\n\n\n    @classmethod\n    def _raw_column_family_name(cls):\n        if not cls._table_name:\n            if cls.__table_name__:\n                if cls.__table_name_case_sensitive__:\n                    warn(\"Model __table_name_case_sensitive__ will be removed in 4.0.\", PendingDeprecationWarning)\n                    cls._table_name = cls.__table_name__\n                else:\n                    table_name = cls.__table_name__.lower()\n                    if cls.__table_name__ != table_name:\n                        warn((\"Model __table_name__ will be case sensitive by default in 4.0. \"\n                        \"You should fix the __table_name__ value of the '{0}' model.\").format(cls.__name__))\n                    cls._table_name = table_name\n            else:\n                if cls._is_polymorphic and not cls._is_polymorphic_base:\n                    cls._table_name = cls._polymorphic_base._raw_column_family_name()\n                else:\n                    camelcase = re.compile(r'([a-z])([A-Z])')\n                    ccase = lambda s: camelcase.sub(lambda v: '{0}_{1}'.format(v.group(1), v.group(2).lower()), s)\n\n                    cf_name = ccase(cls.__name__)\n                    # trim to less than 48 characters or cassandra will complain\n                    cf_name = cf_name[-48:]\n                    cf_name = cf_name.lower()\n                    cf_name = re.sub(r'^_+', '', cf_name)\n                    cls._table_name = cf_name\n\n        return cls._table_name\n\n    def _set_column_value(self, name, value):\n        \"\"\"Function to change a column value without changing the value manager states\"\"\"\n        self._values[name].value = value  # internal assignement, skip the main setter\n\n    def validate(self):\n        \"\"\"\n        Cleans and validates the field values\n        \"\"\"\n        for name, col in self._columns.items():\n            v = getattr(self, name)\n            if v is None and not self._values[name].explicit and col.has_default:\n                v = col.get_default()\n            val = col.validate(v)\n            self._set_column_value(name, val)\n\n    # Let an instance be used like a dict of its columns keys/values\n    def __iter__(self):\n        \"\"\" Iterate over column ids. \"\"\"\n        for column_id in self._columns.keys():\n            yield column_id\n\n    def __getitem__(self, key):\n        \"\"\" Returns column's value. \"\"\"\n        if not isinstance(key, str):\n            raise TypeError\n        if key not in self._columns.keys():\n            raise KeyError\n        return getattr(self, key)\n\n    def __setitem__(self, key, val):\n        \"\"\" Sets a column's value. \"\"\"\n        if not isinstance(key, str):\n            raise TypeError\n        if key not in self._columns.keys():\n            raise KeyError\n        return setattr(self, key, val)\n\n    def __len__(self):\n        \"\"\"\n        Returns the number of columns defined on that model.\n        \"\"\"\n        try:\n            return self._len\n        except:\n            self._len = len(self._columns.keys())\n            return self._len\n\n    def keys(self):\n        \"\"\" Returns a list of column IDs. \"\"\"\n        return [k for k in self]\n\n    def values(self):\n        \"\"\" Returns list of column values. \"\"\"\n        return [self[k] for k in self]\n\n    def items(self):\n        \"\"\" Returns a list of column ID/value tuples. \"\"\"\n        return [(k, self[k]) for k in self]\n\n    def _as_dict(self):\n        \"\"\" Returns a map of column names to cleaned values \"\"\"\n        values = self._dynamic_columns or {}\n        for name, col in self._columns.items():\n            values[name] = col.to_database(getattr(self, name, None))\n        return values\n\n    @classmethod\n    def create(cls, **kwargs):\n        \"\"\"\n        Create an instance of this model in the database.\n\n        Takes the model column values as keyword arguments. Setting a value to\n        `None` is equivalent to running a CQL `DELETE` on that column.\n\n        Returns the instance.\n        \"\"\"\n        extra_columns = set(kwargs.keys()) - set(cls._columns.keys())\n        if extra_columns:\n            raise ValidationError(\"Incorrect columns passed: {0}\".format(extra_columns))\n        return cls.objects.create(**kwargs)\n\n    @classmethod\n    def all(cls):\n        \"\"\"\n        Returns a queryset representing all stored objects\n\n        This is a pass-through to the model objects().all()\n        \"\"\"\n        return cls.objects.all()\n\n    @classmethod\n    def filter(cls, *args, **kwargs):\n        \"\"\"\n        Returns a queryset based on filter parameters.\n\n        This is a pass-through to the model objects().:method:`~cqlengine.queries.filter`.\n        \"\"\"\n        return cls.objects.filter(*args, **kwargs)\n\n    @classmethod\n    def get(cls, *args, **kwargs):\n        \"\"\"\n        Returns a single object based on the passed filter constraints.\n\n        This is a pass-through to the model objects().:method:`~cqlengine.queries.get`.\n        \"\"\"\n        return cls.objects.get(*args, **kwargs)\n\n    def timeout(self, timeout):\n        \"\"\"\n        Sets a timeout for use in :meth:`~.save`, :meth:`~.update`, and :meth:`~.delete`\n        operations\n        \"\"\"\n        assert self._batch is None, 'Setting both timeout and batch is not supported'\n        self._timeout = timeout\n        return self\n\n    def save(self):\n        \"\"\"\n        Saves an object to the database.\n\n        .. code-block:: python\n\n            #create a person instance\n            person = Person(first_name='Kimberly', last_name='Eggleston')\n            #saves it to Cassandra\n            person.save()\n        \"\"\"\n\n        # handle polymorphic models\n        if self._is_polymorphic:\n            if self._is_polymorphic_base:\n                raise PolymorphicModelException('cannot save polymorphic base model')\n            else:\n                setattr(self, self._discriminator_column_name, self.__discriminator_value__)\n\n        self.validate()\n        self.__dmlquery__(self.__class__, self,\n                          batch=self._batch,\n                          ttl=self._ttl,\n                          timestamp=self._timestamp,\n                          consistency=self.__consistency__,\n                          if_not_exists=self._if_not_exists,\n                          conditional=self._conditional,\n                          timeout=self._timeout,\n                          if_exists=self._if_exists).save()\n\n        self._set_persisted()\n\n        self._timestamp = None\n\n        return self\n\n    def update(self, **values):\n        \"\"\"\n        Performs an update on the model instance. You can pass in values to set on the model\n        for updating, or you can call without values to execute an update against any modified\n        fields. If no fields on the model have been modified since loading, no query will be\n        performed. Model validation is performed normally. Setting a value to `None` is\n        equivalent to running a CQL `DELETE` on that column.\n\n        It is possible to do a blind update, that is, to update a field without having first selected the object out of the database.\n        See :ref:`Blind Updates <blind_updates>`\n        \"\"\"\n        for column_id, v in values.items():\n            col = self._columns.get(column_id)\n\n            # check for nonexistant columns\n            if col is None:\n                raise ValidationError(\n                    \"{0}.{1} has no column named: {2}\".format(\n                        self.__module__, self.__class__.__name__, column_id))\n\n            # check for primary key update attempts\n            if col.is_primary_key:\n                current_value = getattr(self, column_id)\n                if v != current_value:\n                    raise ValidationError(\n                        \"Cannot apply update to primary key '{0}' for {1}.{2}\".format(\n                            column_id, self.__module__, self.__class__.__name__))\n\n            setattr(self, column_id, v)\n\n        # handle polymorphic models\n        if self._is_polymorphic:\n            if self._is_polymorphic_base:\n                raise PolymorphicModelException('cannot update polymorphic base model')\n            else:\n                setattr(self, self._discriminator_column_name, self.__discriminator_value__)\n\n        self.validate()\n        self.__dmlquery__(self.__class__, self,\n                          batch=self._batch,\n                          ttl=self._ttl,\n                          timestamp=self._timestamp,\n                          consistency=self.__consistency__,\n                          conditional=self._conditional,\n                          timeout=self._timeout,\n                          if_exists=self._if_exists).update()\n\n        self._set_persisted()\n\n        self._timestamp = None\n\n        return self\n\n    def delete(self):\n        \"\"\"\n        Deletes the object from the database\n        \"\"\"\n        self.__dmlquery__(self.__class__, self,\n                          batch=self._batch,\n                          timestamp=self._timestamp,\n                          consistency=self.__consistency__,\n                          timeout=self._timeout,\n                          conditional=self._conditional,\n                          if_exists=self._if_exists).delete()\n\n    def get_changed_columns(self):\n        \"\"\"\n        Returns a list of the columns that have been updated since instantiation or save\n        \"\"\"\n        return [k for k, v in self._values.items() if v.changed]\n\n    @classmethod\n    def _class_batch(cls, batch):\n        return cls.objects.batch(batch)\n\n    def _inst_batch(self, batch):\n        assert self._timeout is connection.NOT_SET, 'Setting both timeout and batch is not supported'\n        if self._connection:\n            raise CQLEngineException(\"Cannot specify a connection on model in batch mode.\")\n        self._batch = batch\n        return self\n\n    batch = hybrid_classmethod(_class_batch, _inst_batch)\n\n    @classmethod\n    def _class_get_connection(cls):\n        return cls.__connection__\n\n    def _inst_get_connection(self):\n        return self._connection or self.__connection__\n\n    _get_connection = hybrid_classmethod(_class_get_connection, _inst_get_connection)\n\n\nclass ModelMetaClass(type):\n\n    def __new__(cls, name, bases, attrs):\n        # move column definitions into columns dict\n        # and set default column names\n        column_dict = OrderedDict()\n        primary_keys = OrderedDict()\n        pk_name = None\n\n        # get inherited properties\n        inherited_columns = OrderedDict()\n        for base in bases:\n            for k, v in getattr(base, '_defined_columns', {}).items():\n                inherited_columns.setdefault(k, v)\n\n        # short circuit __abstract__ inheritance\n        is_abstract = attrs['__abstract__'] = attrs.get('__abstract__', False)\n\n        # short circuit __discriminator_value__ inheritance\n        attrs['__discriminator_value__'] = attrs.get('__discriminator_value__')\n\n        # TODO __default__ttl__ should be removed in the next major release\n        options = attrs.get('__options__') or {}\n        attrs['__default_ttl__'] = options.get('default_time_to_live')\n\n        column_definitions = [(k, v) for k, v in attrs.items() if isinstance(v, columns.Column)]\n        column_definitions = sorted(column_definitions, key=lambda x: x[1].position)\n\n        is_polymorphic_base = any([c[1].discriminator_column for c in column_definitions])\n\n        column_definitions = [x for x in inherited_columns.items()] + column_definitions\n        discriminator_columns = [c for c in column_definitions if c[1].discriminator_column]\n        is_polymorphic = len(discriminator_columns) > 0\n        if len(discriminator_columns) > 1:\n            raise ModelDefinitionException('only one discriminator_column can be defined in a model, {0} found'.format(len(discriminator_columns)))\n\n        if attrs['__discriminator_value__'] and not is_polymorphic:\n            raise ModelDefinitionException('__discriminator_value__ specified, but no base columns defined with discriminator_column=True')\n\n        discriminator_column_name, discriminator_column = discriminator_columns[0] if discriminator_columns else (None, None)\n\n        if isinstance(discriminator_column, (columns.BaseContainerColumn, columns.Counter)):\n            raise ModelDefinitionException('counter and container columns cannot be used as discriminator columns')\n\n        # find polymorphic base class\n        polymorphic_base = None\n        if is_polymorphic and not is_polymorphic_base:\n            def _get_polymorphic_base(bases):\n                for base in bases:\n                    if getattr(base, '_is_polymorphic_base', False):\n                        return base\n                    klass = _get_polymorphic_base(base.__bases__)\n                    if klass:\n                        return klass\n            polymorphic_base = _get_polymorphic_base(bases)\n\n        defined_columns = OrderedDict(column_definitions)\n\n        # check for primary key\n        if not is_abstract and not any([v.primary_key for k, v in column_definitions]):\n            raise ModelDefinitionException(\"At least 1 primary key is required.\")\n\n        counter_columns = [c for c in defined_columns.values() if isinstance(c, columns.Counter)]\n        data_columns = [c for c in defined_columns.values() if not c.primary_key and not isinstance(c, columns.Counter)]\n        if counter_columns and data_columns:\n            raise ModelDefinitionException('counter models may not have data columns')\n\n        has_partition_keys = any(v.partition_key for (k, v) in column_definitions)\n\n        def _transform_column(col_name, col_obj):\n            column_dict[col_name] = col_obj\n            if col_obj.primary_key:\n                primary_keys[col_name] = col_obj\n            col_obj.set_column_name(col_name)\n            # set properties\n            attrs[col_name] = ColumnDescriptor(col_obj)\n\n        partition_key_index = 0\n        # transform column definitions\n        for k, v in column_definitions:\n            # don't allow a column with the same name as a built-in attribute or method\n            if k in BaseModel.__dict__:\n                raise ModelDefinitionException(\"column '{0}' conflicts with built-in attribute/method\".format(k))\n\n            # counter column primary keys are not allowed\n            if (v.primary_key or v.partition_key) and isinstance(v, columns.Counter):\n                raise ModelDefinitionException('counter columns cannot be used as primary keys')\n\n            # this will mark the first primary key column as a partition\n            # key, if one hasn't been set already\n            if not has_partition_keys and v.primary_key:\n                v.partition_key = True\n                has_partition_keys = True\n            if v.partition_key:\n                v._partition_key_index = partition_key_index\n                partition_key_index += 1\n\n            overriding = column_dict.get(k)\n            if overriding:\n                v.position = overriding.position\n                v.partition_key = overriding.partition_key\n                v._partition_key_index = overriding._partition_key_index\n            _transform_column(k, v)\n\n        partition_keys = OrderedDict(k for k in primary_keys.items() if k[1].partition_key)\n        clustering_keys = OrderedDict(k for k in primary_keys.items() if not k[1].partition_key)\n\n        if attrs.get('__compute_routing_key__', True):\n            key_cols = [c for c in partition_keys.values()]\n            partition_key_index = dict((col.db_field_name, col._partition_key_index) for col in key_cols)\n            key_cql_types = [c.cql_type for c in key_cols]\n            key_serializer = staticmethod(lambda parts, proto_version: [t.to_binary(p, proto_version) for t, p in zip(key_cql_types, parts)])\n        else:\n            partition_key_index = {}\n            key_serializer = staticmethod(lambda parts, proto_version: None)\n\n        # setup partition key shortcut\n        if len(partition_keys) == 0:\n            if not is_abstract:\n                raise ModelException(\"at least one partition key must be defined\")\n        if len(partition_keys) == 1:\n            pk_name = [x for x in partition_keys.keys()][0]\n            attrs['pk'] = attrs[pk_name]\n        else:\n            # composite partition key case, get/set a tuple of values\n            _get = lambda self: tuple(self._values[c].getval() for c in partition_keys.keys())\n            _set = lambda self, val: tuple(self._values[c].setval(v) for (c, v) in zip(partition_keys.keys(), val))\n            attrs['pk'] = property(_get, _set)\n\n        # some validation\n        col_names = set()\n        for v in column_dict.values():\n            # check for duplicate column names\n            if v.db_field_name in col_names:\n                raise ModelException(\"{0} defines the column '{1}' more than once\".format(name, v.db_field_name))\n            if v.clustering_order and not (v.primary_key and not v.partition_key):\n                raise ModelException(\"clustering_order may be specified only for clustering primary keys\")\n            if v.clustering_order and v.clustering_order.lower() not in ('asc', 'desc'):\n                raise ModelException(\"invalid clustering order '{0}' for column '{1}'\".format(repr(v.clustering_order), v.db_field_name))\n            col_names.add(v.db_field_name)\n\n        # create db_name -> model name map for loading\n        db_map = {}\n        for col_name, field in column_dict.items():\n            db_field = field.db_field_name\n            if db_field != col_name:\n                db_map[db_field] = col_name\n\n        # add management members to the class\n        attrs['_columns'] = column_dict\n        attrs['_primary_keys'] = primary_keys\n        attrs['_defined_columns'] = defined_columns\n\n        # maps the database field to the models key\n        attrs['_db_map'] = db_map\n        attrs['_pk_name'] = pk_name\n        attrs['_dynamic_columns'] = {}\n\n        attrs['_partition_keys'] = partition_keys\n        attrs['_partition_key_index'] = partition_key_index\n        attrs['_key_serializer'] = key_serializer\n        attrs['_clustering_keys'] = clustering_keys\n        attrs['_has_counter'] = len(counter_columns) > 0\n\n        # add polymorphic management attributes\n        attrs['_is_polymorphic_base'] = is_polymorphic_base\n        attrs['_is_polymorphic'] = is_polymorphic\n        attrs['_polymorphic_base'] = polymorphic_base\n        attrs['_discriminator_column'] = discriminator_column\n        attrs['_discriminator_column_name'] = discriminator_column_name\n        attrs['_discriminator_map'] = {} if is_polymorphic_base else None\n\n        # setup class exceptions\n        DoesNotExistBase = None\n        for base in bases:\n            DoesNotExistBase = getattr(base, 'DoesNotExist', None)\n            if DoesNotExistBase is not None:\n                break\n\n        DoesNotExistBase = DoesNotExistBase or attrs.pop('DoesNotExist', BaseModel.DoesNotExist)\n        attrs['DoesNotExist'] = type('DoesNotExist', (DoesNotExistBase,), {})\n\n        MultipleObjectsReturnedBase = None\n        for base in bases:\n            MultipleObjectsReturnedBase = getattr(base, 'MultipleObjectsReturned', None)\n            if MultipleObjectsReturnedBase is not None:\n                break\n\n        MultipleObjectsReturnedBase = MultipleObjectsReturnedBase or attrs.pop('MultipleObjectsReturned', BaseModel.MultipleObjectsReturned)\n        attrs['MultipleObjectsReturned'] = type('MultipleObjectsReturned', (MultipleObjectsReturnedBase,), {})\n\n        # create the class and add a QuerySet to it\n        klass = super(ModelMetaClass, cls).__new__(cls, name, bases, attrs)\n\n        udts = []\n        for col in column_dict.values():\n            columns.resolve_udts(col, udts)\n\n        for user_type in set(udts):\n            user_type.register_for_keyspace(klass._get_keyspace())\n\n        return klass\n\n\nclass Model(BaseModel, metaclass=ModelMetaClass):\n    __abstract__ = True\n    \"\"\"\n    *Optional.* Indicates that this model is only intended to be used as a base class for other models.\n    You can't create tables for abstract models, but checks around schema validity are skipped during class construction.\n    \"\"\"\n\n    __table_name__ = None\n    \"\"\"\n    *Optional.* Sets the name of the CQL table for this model. If left blank, the table name will be the name of the model, with it's module name as it's prefix. Manually defined table names are not inherited.\n    \"\"\"\n\n    __table_name_case_sensitive__ = False\n    \"\"\"\n    *Optional.* By default, __table_name__ is case insensitive. Set this to True if you want to preserve the case sensitivity.\n    \"\"\"\n\n    __keyspace__ = None\n    \"\"\"\n    Sets the name of the keyspace used by this model.\n    \"\"\"\n\n    __connection__ = None\n    \"\"\"\n    Sets the name of the default connection used by this model.\n    \"\"\"\n\n    __options__ = None\n    \"\"\"\n    *Optional* Table options applied with this model\n\n    (e.g. compaction, default ttl, cache settings, tec.)\n    \"\"\"\n\n    __discriminator_value__ = None\n    \"\"\"\n    *Optional* Specifies a value for the discriminator column when using model inheritance.\n    \"\"\"\n\n    __compute_routing_key__ = True\n    \"\"\"\n    *Optional* Setting False disables computing the routing key for TokenAwareRouting\n    \"\"\"\n",
    "cassandra/__init__.py": "# Copyright DataStax, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom enum import Enum\nimport logging\n\n\nclass NullHandler(logging.Handler):\n\n    def emit(self, record):\n        pass\n\nlogging.getLogger('cassandra').addHandler(NullHandler())\n\n__version_info__ = (3, 28, 0)\n__version__ = '.'.join(map(str, __version_info__))\n\n\nclass ConsistencyLevel(object):\n    \"\"\"\n    Spcifies how many replicas must respond for an operation to be considered\n    a success.  By default, ``ONE`` is used for all operations.\n    \"\"\"\n\n    ANY = 0\n    \"\"\"\n    Only requires that one replica receives the write *or* the coordinator\n    stores a hint to replay later. Valid only for writes.\n    \"\"\"\n\n    ONE = 1\n    \"\"\"\n    Only one replica needs to respond to consider the operation a success\n    \"\"\"\n\n    TWO = 2\n    \"\"\"\n    Two replicas must respond to consider the operation a success\n    \"\"\"\n\n    THREE = 3\n    \"\"\"\n    Three replicas must respond to consider the operation a success\n    \"\"\"\n\n    QUORUM = 4\n    \"\"\"\n    ``ceil(RF/2) + 1`` replicas must respond to consider the operation a success\n    \"\"\"\n\n    ALL = 5\n    \"\"\"\n    All replicas must respond to consider the operation a success\n    \"\"\"\n\n    LOCAL_QUORUM = 6\n    \"\"\"\n    Requires a quorum of replicas in the local datacenter\n    \"\"\"\n\n    EACH_QUORUM = 7\n    \"\"\"\n    Requires a quorum of replicas in each datacenter\n    \"\"\"\n\n    SERIAL = 8\n    \"\"\"\n    For conditional inserts/updates that utilize Cassandra's lightweight\n    transactions, this requires consensus among all replicas for the\n    modified data.\n    \"\"\"\n\n    LOCAL_SERIAL = 9\n    \"\"\"\n    Like :attr:`~ConsistencyLevel.SERIAL`, but only requires consensus\n    among replicas in the local datacenter.\n    \"\"\"\n\n    LOCAL_ONE = 10\n    \"\"\"\n    Sends a request only to replicas in the local datacenter and waits for\n    one response.\n    \"\"\"\n\n    @staticmethod\n    def is_serial(cl):\n        return cl == ConsistencyLevel.SERIAL or cl == ConsistencyLevel.LOCAL_SERIAL\n\n\nConsistencyLevel.value_to_name = {\n    ConsistencyLevel.ANY: 'ANY',\n    ConsistencyLevel.ONE: 'ONE',\n    ConsistencyLevel.TWO: 'TWO',\n    ConsistencyLevel.THREE: 'THREE',\n    ConsistencyLevel.QUORUM: 'QUORUM',\n    ConsistencyLevel.ALL: 'ALL',\n    ConsistencyLevel.LOCAL_QUORUM: 'LOCAL_QUORUM',\n    ConsistencyLevel.EACH_QUORUM: 'EACH_QUORUM',\n    ConsistencyLevel.SERIAL: 'SERIAL',\n    ConsistencyLevel.LOCAL_SERIAL: 'LOCAL_SERIAL',\n    ConsistencyLevel.LOCAL_ONE: 'LOCAL_ONE'\n}\n\nConsistencyLevel.name_to_value = {\n    'ANY': ConsistencyLevel.ANY,\n    'ONE': ConsistencyLevel.ONE,\n    'TWO': ConsistencyLevel.TWO,\n    'THREE': ConsistencyLevel.THREE,\n    'QUORUM': ConsistencyLevel.QUORUM,\n    'ALL': ConsistencyLevel.ALL,\n    'LOCAL_QUORUM': ConsistencyLevel.LOCAL_QUORUM,\n    'EACH_QUORUM': ConsistencyLevel.EACH_QUORUM,\n    'SERIAL': ConsistencyLevel.SERIAL,\n    'LOCAL_SERIAL': ConsistencyLevel.LOCAL_SERIAL,\n    'LOCAL_ONE': ConsistencyLevel.LOCAL_ONE\n}\n\n\ndef consistency_value_to_name(value):\n    return ConsistencyLevel.value_to_name[value] if value is not None else \"Not Set\"\n\n\nclass ProtocolVersion(object):\n    \"\"\"\n    Defines native protocol versions supported by this driver.\n    \"\"\"\n    V1 = 1\n    \"\"\"\n    v1, supported in Cassandra 1.2-->2.2\n    \"\"\"\n\n    V2 = 2\n    \"\"\"\n    v2, supported in Cassandra 2.0-->2.2;\n    added support for lightweight transactions, batch operations, and automatic query paging.\n    \"\"\"\n\n    V3 = 3\n    \"\"\"\n    v3, supported in Cassandra 2.1-->3.x+;\n    added support for protocol-level client-side timestamps (see :attr:`.Session.use_client_timestamp`),\n    serial consistency levels for :class:`~.BatchStatement`, and an improved connection pool.\n    \"\"\"\n\n    V4 = 4\n    \"\"\"\n    v4, supported in Cassandra 2.2-->3.x+;\n    added a number of new types, server warnings, new failure messages, and custom payloads. Details in the\n    `project docs <https://github.com/apache/cassandra/blob/trunk/doc/native_protocol_v4.spec>`_\n    \"\"\"\n\n    V5 = 5\n    \"\"\"\n    v5, in beta from 3.x+. Finalised in 4.0-beta5\n    \"\"\"\n\n    V6 = 6\n    \"\"\"\n    v6, in beta from 4.0-beta5\n    \"\"\"\n\n    DSE_V1 = 0x41\n    \"\"\"\n    DSE private protocol v1, supported in DSE 5.1+\n    \"\"\"\n\n    DSE_V2 = 0x42\n    \"\"\"\n    DSE private protocol v2, supported in DSE 6.0+\n    \"\"\"\n\n    SUPPORTED_VERSIONS = (DSE_V2, DSE_V1, V6, V5, V4, V3, V2, V1)\n    \"\"\"\n    A tuple of all supported protocol versions\n    \"\"\"\n\n    BETA_VERSIONS = (V6,)\n    \"\"\"\n    A tuple of all beta protocol versions\n    \"\"\"\n\n    MIN_SUPPORTED = min(SUPPORTED_VERSIONS)\n    \"\"\"\n    Minimum protocol version supported by this driver.\n    \"\"\"\n\n    MAX_SUPPORTED = max(SUPPORTED_VERSIONS)\n    \"\"\"\n    Maximum protocol version supported by this driver.\n    \"\"\"\n\n    @classmethod\n    def get_lower_supported(cls, previous_version):\n        \"\"\"\n        Return the lower supported protocol version. Beta versions are omitted.\n        \"\"\"\n        try:\n            version = next(v for v in sorted(ProtocolVersion.SUPPORTED_VERSIONS, reverse=True) if\n                           v not in ProtocolVersion.BETA_VERSIONS and v < previous_version)\n        except StopIteration:\n            version = 0\n\n        return version\n\n    @classmethod\n    def uses_int_query_flags(cls, version):\n        return version >= cls.V5\n\n    @classmethod\n    def uses_prepare_flags(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def uses_prepared_metadata(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def uses_error_code_map(cls, version):\n        return version >= cls.V5\n\n    @classmethod\n    def uses_keyspace_flag(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def has_continuous_paging_support(cls, version):\n        return version >= cls.DSE_V1\n\n    @classmethod\n    def has_continuous_paging_next_pages(cls, version):\n        return version >= cls.DSE_V2\n\n    @classmethod\n    def has_checksumming_support(cls, version):\n        return cls.V5 <= version < cls.DSE_V1\n\n\nclass WriteType(object):\n    \"\"\"\n    For usage with :class:`.RetryPolicy`, this describe a type\n    of write operation.\n    \"\"\"\n\n    SIMPLE = 0\n    \"\"\"\n    A write to a single partition key. Such writes are guaranteed to be atomic\n    and isolated.\n    \"\"\"\n\n    BATCH = 1\n    \"\"\"\n    A write to multiple partition keys that used the distributed batch log to\n    ensure atomicity.\n    \"\"\"\n\n    UNLOGGED_BATCH = 2\n    \"\"\"\n    A write to multiple partition keys that did not use the distributed batch\n    log. Atomicity for such writes is not guaranteed.\n    \"\"\"\n\n    COUNTER = 3\n    \"\"\"\n    A counter write (for one or multiple partition keys). Such writes should\n    not be replayed in order to avoid overcount.\n    \"\"\"\n\n    BATCH_LOG = 4\n    \"\"\"\n    The initial write to the distributed batch log that Cassandra performs\n    internally before a BATCH write.\n    \"\"\"\n\n    CAS = 5\n    \"\"\"\n    A lighweight-transaction write, such as \"DELETE ... IF EXISTS\".\n    \"\"\"\n\n    VIEW = 6\n    \"\"\"\n    This WriteType is only seen in results for requests that were unable to\n    complete MV operations.\n    \"\"\"\n\n    CDC = 7\n    \"\"\"\n    This WriteType is only seen in results for requests that were unable to\n    complete CDC operations.\n    \"\"\"\n\n\nWriteType.name_to_value = {\n    'SIMPLE': WriteType.SIMPLE,\n    'BATCH': WriteType.BATCH,\n    'UNLOGGED_BATCH': WriteType.UNLOGGED_BATCH,\n    'COUNTER': WriteType.COUNTER,\n    'BATCH_LOG': WriteType.BATCH_LOG,\n    'CAS': WriteType.CAS,\n    'VIEW': WriteType.VIEW,\n    'CDC': WriteType.CDC\n}\n\n\nWriteType.value_to_name = {v: k for k, v in WriteType.name_to_value.items()}\n\n\nclass SchemaChangeType(object):\n    DROPPED = 'DROPPED'\n    CREATED = 'CREATED'\n    UPDATED = 'UPDATED'\n\n\nclass SchemaTargetType(object):\n    KEYSPACE = 'KEYSPACE'\n    TABLE = 'TABLE'\n    TYPE = 'TYPE'\n    FUNCTION = 'FUNCTION'\n    AGGREGATE = 'AGGREGATE'\n\n\nclass SignatureDescriptor(object):\n\n    def __init__(self, name, argument_types):\n        self.name = name\n        self.argument_types = argument_types\n\n    @property\n    def signature(self):\n        \"\"\"\n        function signature string in the form 'name([type0[,type1[...]]])'\n\n        can be used to uniquely identify overloaded function names within a keyspace\n        \"\"\"\n        return self.format_signature(self.name, self.argument_types)\n\n    @staticmethod\n    def format_signature(name, argument_types):\n        return \"%s(%s)\" % (name, ','.join(t for t in argument_types))\n\n    def __repr__(self):\n        return \"%s(%s, %s)\" % (self.__class__.__name__, self.name, self.argument_types)\n\n\nclass UserFunctionDescriptor(SignatureDescriptor):\n    \"\"\"\n    Describes a User function by name and argument signature\n    \"\"\"\n\n    name = None\n    \"\"\"\n    name of the function\n    \"\"\"\n\n    argument_types = None\n    \"\"\"\n    Ordered list of CQL argument type names comprising the type signature\n    \"\"\"\n\n\nclass UserAggregateDescriptor(SignatureDescriptor):\n    \"\"\"\n    Describes a User aggregate function by name and argument signature\n    \"\"\"\n\n    name = None\n    \"\"\"\n    name of the aggregate\n    \"\"\"\n\n    argument_types = None\n    \"\"\"\n    Ordered list of CQL argument type names comprising the type signature\n    \"\"\"\n\n\nclass DriverException(Exception):\n    \"\"\"\n    Base for all exceptions explicitly raised by the driver.\n    \"\"\"\n    pass\n\n\nclass RequestExecutionException(DriverException):\n    \"\"\"\n    Base for request execution exceptions returned from the server.\n    \"\"\"\n    pass\n\n\nclass Unavailable(RequestExecutionException):\n    \"\"\"\n    There were not enough live replicas to satisfy the requested consistency\n    level, so the coordinator node immediately failed the request without\n    forwarding it to any replicas.\n    \"\"\"\n\n    consistency = None\n    \"\"\" The requested :class:`ConsistencyLevel` \"\"\"\n\n    required_replicas = None\n    \"\"\" The number of replicas that needed to be live to complete the operation \"\"\"\n\n    alive_replicas = None\n    \"\"\" The number of replicas that were actually alive \"\"\"\n\n    def __init__(self, summary_message, consistency=None, required_replicas=None, alive_replicas=None):\n        self.consistency = consistency\n        self.required_replicas = required_replicas\n        self.alive_replicas = alive_replicas\n        Exception.__init__(self, summary_message + ' info=' +\n                           repr({'consistency': consistency_value_to_name(consistency),\n                                 'required_replicas': required_replicas,\n                                 'alive_replicas': alive_replicas}))\n\n\nclass Timeout(RequestExecutionException):\n    \"\"\"\n    Replicas failed to respond to the coordinator node before timing out.\n    \"\"\"\n\n    consistency = None\n    \"\"\" The requested :class:`ConsistencyLevel` \"\"\"\n\n    required_responses = None\n    \"\"\" The number of required replica responses \"\"\"\n\n    received_responses = None\n    \"\"\"\n    The number of replicas that responded before the coordinator timed out\n    the operation\n    \"\"\"\n\n    def __init__(self, summary_message, consistency=None, required_responses=None,\n                 received_responses=None, **kwargs):\n        self.consistency = consistency\n        self.required_responses = required_responses\n        self.received_responses = received_responses\n\n        if \"write_type\" in kwargs:\n            kwargs[\"write_type\"] = WriteType.value_to_name[kwargs[\"write_type\"]]\n\n        info = {'consistency': consistency_value_to_name(consistency),\n                'required_responses': required_responses,\n                'received_responses': received_responses}\n        info.update(kwargs)\n\n        Exception.__init__(self, summary_message + ' info=' + repr(info))\n\n\nclass ReadTimeout(Timeout):\n    \"\"\"\n    A subclass of :exc:`Timeout` for read operations.\n\n    This indicates that the replicas failed to respond to the coordinator\n    node before the configured timeout. This timeout is configured in\n    ``cassandra.yaml`` with the ``read_request_timeout_in_ms``\n    and ``range_request_timeout_in_ms`` options.\n    \"\"\"\n\n    data_retrieved = None\n    \"\"\"\n    A boolean indicating whether the requested data was retrieved\n    by the coordinator from any replicas before it timed out the\n    operation\n    \"\"\"\n\n    def __init__(self, message, data_retrieved=None, **kwargs):\n        Timeout.__init__(self, message, **kwargs)\n        self.data_retrieved = data_retrieved\n\n\nclass WriteTimeout(Timeout):\n    \"\"\"\n    A subclass of :exc:`Timeout` for write operations.\n\n    This indicates that the replicas failed to respond to the coordinator\n    node before the configured timeout. This timeout is configured in\n    ``cassandra.yaml`` with the ``write_request_timeout_in_ms``\n    option.\n    \"\"\"\n\n    write_type = None\n    \"\"\"\n    The type of write operation, enum on :class:`~cassandra.policies.WriteType`\n    \"\"\"\n\n    def __init__(self, message, write_type=None, **kwargs):\n        kwargs[\"write_type\"] = write_type\n        Timeout.__init__(self, message, **kwargs)\n        self.write_type = write_type\n\n\nclass CDCWriteFailure(RequestExecutionException):\n    \"\"\"\n    Hit limit on data in CDC folder, writes are rejected\n    \"\"\"\n    def __init__(self, message):\n        Exception.__init__(self, message)\n\n\nclass CoordinationFailure(RequestExecutionException):\n    \"\"\"\n    Replicas sent a failure to the coordinator.\n    \"\"\"\n\n    consistency = None\n    \"\"\" The requested :class:`ConsistencyLevel` \"\"\"\n\n    required_responses = None\n    \"\"\" The number of required replica responses \"\"\"\n\n    received_responses = None\n    \"\"\"\n    The number of replicas that responded before the coordinator timed out\n    the operation\n    \"\"\"\n\n    failures = None\n    \"\"\"\n    The number of replicas that sent a failure message\n    \"\"\"\n\n    error_code_map = None\n    \"\"\"\n    A map of inet addresses to error codes representing replicas that sent\n    a failure message.  Only set when `protocol_version` is 5 or higher.\n    \"\"\"\n\n    def __init__(self, summary_message, consistency=None, required_responses=None,\n                 received_responses=None, failures=None, error_code_map=None):\n        self.consistency = consistency\n        self.required_responses = required_responses\n        self.received_responses = received_responses\n        self.failures = failures\n        self.error_code_map = error_code_map\n\n        info_dict = {\n            'consistency': consistency_value_to_name(consistency),\n            'required_responses': required_responses,\n            'received_responses': received_responses,\n            'failures': failures\n        }\n\n        if error_code_map is not None:\n            # make error codes look like \"0x002a\"\n            formatted_map = dict((addr, '0x%04x' % err_code)\n                                 for (addr, err_code) in error_code_map.items())\n            info_dict['error_code_map'] = formatted_map\n\n        Exception.__init__(self, summary_message + ' info=' + repr(info_dict))\n\n\nclass ReadFailure(CoordinationFailure):\n    \"\"\"\n    A subclass of :exc:`CoordinationFailure` for read operations.\n\n    This indicates that the replicas sent a failure message to the coordinator.\n    \"\"\"\n\n    data_retrieved = None\n    \"\"\"\n    A boolean indicating whether the requested data was retrieved\n    by the coordinator from any replicas before it timed out the\n    operation\n    \"\"\"\n\n    def __init__(self, message, data_retrieved=None, **kwargs):\n        CoordinationFailure.__init__(self, message, **kwargs)\n        self.data_retrieved = data_retrieved\n\n\nclass WriteFailure(CoordinationFailure):\n    \"\"\"\n    A subclass of :exc:`CoordinationFailure` for write operations.\n\n    This indicates that the replicas sent a failure message to the coordinator.\n    \"\"\"\n\n    write_type = None\n    \"\"\"\n    The type of write operation, enum on :class:`~cassandra.policies.WriteType`\n    \"\"\"\n\n    def __init__(self, message, write_type=None, **kwargs):\n        CoordinationFailure.__init__(self, message, **kwargs)\n        self.write_type = write_type\n\n\nclass FunctionFailure(RequestExecutionException):\n    \"\"\"\n    User Defined Function failed during execution\n    \"\"\"\n\n    keyspace = None\n    \"\"\"\n    Keyspace of the function\n    \"\"\"\n\n    function = None\n    \"\"\"\n    Name of the function\n    \"\"\"\n\n    arg_types = None\n    \"\"\"\n    List of argument type names of the function\n    \"\"\"\n\n    def __init__(self, summary_message, keyspace, function, arg_types):\n        self.keyspace = keyspace\n        self.function = function\n        self.arg_types = arg_types\n        Exception.__init__(self, summary_message)\n\n\nclass RequestValidationException(DriverException):\n    \"\"\"\n    Server request validation failed\n    \"\"\"\n    pass\n\n\nclass ConfigurationException(RequestValidationException):\n    \"\"\"\n    Server indicated request errro due to current configuration\n    \"\"\"\n    pass\n\n\nclass AlreadyExists(ConfigurationException):\n    \"\"\"\n    An attempt was made to create a keyspace or table that already exists.\n    \"\"\"\n\n    keyspace = None\n    \"\"\"\n    The name of the keyspace that already exists, or, if an attempt was\n    made to create a new table, the keyspace that the table is in.\n    \"\"\"\n\n    table = None\n    \"\"\"\n    The name of the table that already exists, or, if an attempt was\n    make to create a keyspace, :const:`None`.\n    \"\"\"\n\n    def __init__(self, keyspace=None, table=None):\n        if table:\n            message = \"Table '%s.%s' already exists\" % (keyspace, table)\n        else:\n            message = \"Keyspace '%s' already exists\" % (keyspace,)\n\n        Exception.__init__(self, message)\n        self.keyspace = keyspace\n        self.table = table\n\n\nclass InvalidRequest(RequestValidationException):\n    \"\"\"\n    A query was made that was invalid for some reason, such as trying to set\n    the keyspace for a connection to a nonexistent keyspace.\n    \"\"\"\n    pass\n\n\nclass Unauthorized(RequestValidationException):\n    \"\"\"\n    The current user is not authorized to perform the requested operation.\n    \"\"\"\n    pass\n\n\nclass AuthenticationFailed(DriverException):\n    \"\"\"\n    Failed to authenticate.\n    \"\"\"\n    pass\n\n\nclass OperationTimedOut(DriverException):\n    \"\"\"\n    The operation took longer than the specified (client-side) timeout\n    to complete.  This is not an error generated by Cassandra, only\n    the driver.\n    \"\"\"\n\n    errors = None\n    \"\"\"\n    A dict of errors keyed by the :class:`~.Host` against which they occurred.\n    \"\"\"\n\n    last_host = None\n    \"\"\"\n    The last :class:`~.Host` this operation was attempted against.\n    \"\"\"\n\n    def __init__(self, errors=None, last_host=None):\n        self.errors = errors\n        self.last_host = last_host\n        message = \"errors=%s, last_host=%s\" % (self.errors, self.last_host)\n        Exception.__init__(self, message)\n\n\nclass UnsupportedOperation(DriverException):\n    \"\"\"\n    An attempt was made to use a feature that is not supported by the\n    selected protocol version.  See :attr:`Cluster.protocol_version`\n    for more details.\n    \"\"\"\n    pass\n\n\nclass UnresolvableContactPoints(DriverException):\n    \"\"\"\n    The driver was unable to resolve any provided hostnames.\n\n    Note that this is *not* raised when a :class:`.Cluster` is created with no\n    contact points, only when lookup fails for all hosts\n    \"\"\"\n    pass\n\n\nclass OperationType(Enum):\n    Read = 0\n    Write = 1\n\nclass RateLimitReached(ConfigurationException):\n    '''\n    Rate limit was exceeded for a partition affected by the request.\n    '''\n    op_type = None\n    rejected_by_coordinator = False\n\n    def __init__(self, op_type=None, rejected_by_coordinator=False):\n        self.op_type = op_type\n        self.rejected_by_coordinator = rejected_by_coordinator\n        message = f\"[request_error_rate_limit_reached OpType={op_type.name} RejectedByCoordinator={rejected_by_coordinator}]\"\n        Exception.__init__(self, message)\n",
    "cassandra/datastax/graph/types.py": "# Copyright DataStax, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n__all__ = ['Element', 'Vertex', 'Edge', 'VertexProperty', 'Path', 'T']\n\n\nclass Element(object):\n\n    element_type = None\n\n    _attrs = ('id', 'label', 'type', 'properties')\n\n    def __init__(self, id, label, type, properties):\n        if type != self.element_type:\n            raise TypeError(\"Attempted to create %s from %s element\", (type, self.element_type))\n\n        self.id = id\n        self.label = label\n        self.type = type\n        self.properties = self._extract_properties(properties)\n\n    @staticmethod\n    def _extract_properties(properties):\n        return dict(properties)\n\n    def __eq__(self, other):\n        return all(getattr(self, attr) == getattr(other, attr) for attr in self._attrs)\n\n    def __str__(self):\n        return str(dict((k, getattr(self, k)) for k in self._attrs))\n\n\nclass Vertex(Element):\n    \"\"\"\n    Represents a Vertex element from a graph query.\n\n    Vertex ``properties`` are extracted into a ``dict`` of property names to list of :class:`~VertexProperty` (list\n    because they are always encoded that way, and sometimes have multiple cardinality; VertexProperty because sometimes\n    the properties themselves have property maps).\n    \"\"\"\n\n    element_type = 'vertex'\n\n    @staticmethod\n    def _extract_properties(properties):\n        # vertex properties are always encoded as a list, regardless of Cardinality\n        return dict((k, [VertexProperty(k, p['value'], p.get('properties')) for p in v]) for k, v in properties.items())\n\n    def __repr__(self):\n        properties = dict((name, [{'label': prop.label, 'value': prop.value, 'properties': prop.properties} for prop in prop_list])\n                          for name, prop_list in self.properties.items())\n        return \"%s(%r, %r, %r, %r)\" % (self.__class__.__name__,\n                                       self.id, self.label,\n                                       self.type, properties)\n\n\nclass VertexProperty(object):\n    \"\"\"\n    Vertex properties have a top-level value and an optional ``dict`` of properties.\n    \"\"\"\n\n    label = None\n    \"\"\"\n    label of the property\n    \"\"\"\n\n    value = None\n    \"\"\"\n    Value of the property\n    \"\"\"\n\n    properties = None\n    \"\"\"\n    dict of properties attached to the property\n    \"\"\"\n\n    def __init__(self, label, value, properties=None):\n        self.label = label\n        self.value = value\n        self.properties = properties or {}\n\n    def __eq__(self, other):\n        return isinstance(other, VertexProperty) and self.label == other.label and self.value == other.value and self.properties == other.properties\n\n    def __repr__(self):\n        return \"%s(%r, %r, %r)\" % (self.__class__.__name__, self.label, self.value, self.properties)\n\n\nclass Edge(Element):\n    \"\"\"\n    Represents an Edge element from a graph query.\n\n    Attributes match initializer parameters.\n    \"\"\"\n\n    element_type = 'edge'\n\n    _attrs = Element._attrs + ('inV', 'inVLabel', 'outV', 'outVLabel')\n\n    def __init__(self, id, label, type, properties,\n                 inV, inVLabel, outV, outVLabel):\n        super(Edge, self).__init__(id, label, type, properties)\n        self.inV = inV\n        self.inVLabel = inVLabel\n        self.outV = outV\n        self.outVLabel = outVLabel\n\n    def __repr__(self):\n        return \"%s(%r, %r, %r, %r, %r, %r, %r, %r)\" %\\\n               (self.__class__.__name__,\n                self.id, self.label,\n                self.type, self.properties,\n                self.inV, self.inVLabel,\n                self.outV, self.outVLabel)\n\n\nclass Path(object):\n    \"\"\"\n    Represents a graph path.\n\n    Labels list is taken verbatim from the results.\n\n    Objects are either :class:`~.Result` or :class:`~.Vertex`/:class:`~.Edge` for recognized types\n    \"\"\"\n\n    labels = None\n    \"\"\"\n    List of labels in the path\n    \"\"\"\n\n    objects = None\n    \"\"\"\n    List of objects in the path\n    \"\"\"\n\n    def __init__(self, labels, objects):\n        # TODO fix next major\n        # The Path class should not do any deserialization by itself. To fix in the next major.\n        from cassandra.datastax.graph.query import _graph_object_sequence\n        self.labels = labels\n        self.objects = list(_graph_object_sequence(objects))\n\n    def __eq__(self, other):\n        return self.labels == other.labels and self.objects == other.objects\n\n    def __str__(self):\n        return str({'labels': self.labels, 'objects': self.objects})\n\n    def __repr__(self):\n        return \"%s(%r, %r)\" % (self.__class__.__name__, self.labels, [o.value for o in self.objects])\n\n\nclass T(object):\n    \"\"\"\n    Represents a collection of tokens for more concise Traversal definitions.\n    \"\"\"\n\n    name = None\n    val = None\n\n    # class attributes\n    id = None\n    \"\"\"\n    \"\"\"\n\n    key = None\n    \"\"\"\n    \"\"\"\n    label = None\n    \"\"\"\n    \"\"\"\n    value = None\n    \"\"\"\n    \"\"\"\n\n    def __init__(self, name, val):\n        self.name = name\n        self.val = val\n\n    def __str__(self):\n        return self.name\n\n    def __repr__(self):\n        return \"T.%s\" % (self.name, )\n\n\nT.id = T(\"id\", 1)\nT.id_ = T(\"id_\", 2)\nT.key = T(\"key\", 3)\nT.label = T(\"label\", 4)\nT.value = T(\"value\", 5)\n\nT.name_to_value = {\n    'id': T.id,\n    'id_': T.id_,\n    'key': T.key,\n    'label': T.label,\n    'value': T.value\n}\n"
  },
  "GT_src_dict": {
    "cassandra/policies.py": {},
    "cassandra/cqlengine/columns.py": {
      "Column.__init__": {
        "code": "    def __init__(self, primary_key=False, partition_key=False, index=False, db_field=None, default=None, required=False, clustering_order=None, discriminator_column=False, static=False, custom_index=False):\n        \"\"\"Initializes a Column object that represents a database column in a Cassandra model.\n\nParameters:\n- primary_key (bool): Indicates if this column is a primary key. Default is False.\n- partition_key (bool): Indicates if this column should be the partition key. Default is False.\n- index (bool): Indicates if an index should be created for this column. Default is False.\n- db_field (str or None): The field name this column will map to in the database. Default is None.\n- default (any or None): The default value for the column, can also be a callable. Default is None.\n- required (bool): Indicates if this field is required. Default is False.\n- clustering_order (str or None): Defines the order of clustering keys. Default is None.\n- discriminator_column (bool): Indicates if this column will be used for discriminating records of inherited models. Default is False.\n- static (bool): Indicates if this is a static column, which has a single value per partition. Default is False.\n- custom_index (bool): Indicates if an index is managed outside of the CQL engine. Default is False.\n\nThis constructor sets initial values for various attributes including `partition_key`, `primary_key`, `index`, and others, which control the behavior and attributes of the column in the database schema. It also manages the instantiation order through the `position` attribute, utilizing a class-level counter (`instance_counter`) to track the number of instances created.\"\"\"\n        self.partition_key = partition_key\n        self.primary_key = partition_key or primary_key\n        self.index = index\n        self.custom_index = custom_index\n        self.db_field = db_field\n        self.default = default\n        self.required = required\n        self.clustering_order = clustering_order\n        self.discriminator_column = discriminator_column\n        self.column_name = None\n        self._partition_key_index = None\n        self.static = static\n        self.value = None\n        self.position = Column.instance_counter\n        Column.instance_counter += 1",
        "docstring": "Initializes a Column object that represents a database column in a Cassandra model.\n\nParameters:\n- primary_key (bool): Indicates if this column is a primary key. Default is False.\n- partition_key (bool): Indicates if this column should be the partition key. Default is False.\n- index (bool): Indicates if an index should be created for this column. Default is False.\n- db_field (str or None): The field name this column will map to in the database. Default is None.\n- default (any or None): The default value for the column, can also be a callable. Default is None.\n- required (bool): Indicates if this field is required. Default is False.\n- clustering_order (str or None): Defines the order of clustering keys. Default is None.\n- discriminator_column (bool): Indicates if this column will be used for discriminating records of inherited models. Default is False.\n- static (bool): Indicates if this is a static column, which has a single value per partition. Default is False.\n- custom_index (bool): Indicates if an index is managed outside of the CQL engine. Default is False.\n\nThis constructor sets initial values for various attributes including `partition_key`, `primary_key`, `index`, and others, which control the behavior and attributes of the column in the database schema. It also manages the instantiation order through the `position` attribute, utilizing a class-level counter (`instance_counter`) to track the number of instances created.",
        "signature": "def __init__(self, primary_key=False, partition_key=False, index=False, db_field=None, default=None, required=False, clustering_order=None, discriminator_column=False, static=False, custom_index=False):",
        "type": "Method",
        "class_signature": "class Column(object):"
      },
      "Text.__init__": {
        "code": "    def __init__(self, min_length=None, max_length=None, **kwargs):\n        \"\"\"Initializes a Text column for storing UTF-8 encoded strings with optional length constraints.\n\n:param int min_length: Specifies the minimum allowable length of the string. Defaults to 1 if the column is required; otherwise, it can be None.\n:param int max_length: Specifies the maximum allowable length of the string.\n:param kwargs: Additional keyword arguments are passed to the parent Column class constructor.\n\nRaises ValueError: If min_length is negative, if max_length is negative, or if max_length is less than min_length.\n\nThis constructor interacts with the parent Column class, which manages database column properties and validation. It ensures that the string length constraints are within valid boundaries while also delegating other column functionalities to the parent class.\"\"\"\n        '\\n        :param int min_length: Sets the minimum length of this string, for validation purposes.\\n            Defaults to 1 if this is a ``required`` column. Otherwise, None.\\n        :param int max_length: Sets the maximum length of this string, for validation purposes.\\n        '\n        self.min_length = 1 if min_length is None and kwargs.get('required', False) else min_length\n        self.max_length = max_length\n        if self.min_length is not None:\n            if self.min_length < 0:\n                raise ValueError('Minimum length is not allowed to be negative.')\n        if self.max_length is not None:\n            if self.max_length < 0:\n                raise ValueError('Maximum length is not allowed to be negative.')\n        if self.min_length is not None and self.max_length is not None:\n            if self.max_length < self.min_length:\n                raise ValueError('Maximum length must be greater or equal to minimum length.')\n        super(Text, self).__init__(**kwargs)",
        "docstring": "Initializes a Text column for storing UTF-8 encoded strings with optional length constraints.\n\n:param int min_length: Specifies the minimum allowable length of the string. Defaults to 1 if the column is required; otherwise, it can be None.\n:param int max_length: Specifies the maximum allowable length of the string.\n:param kwargs: Additional keyword arguments are passed to the parent Column class constructor.\n\nRaises ValueError: If min_length is negative, if max_length is negative, or if max_length is less than min_length.\n\nThis constructor interacts with the parent Column class, which manages database column properties and validation. It ensures that the string length constraints are within valid boundaries while also delegating other column functionalities to the parent class.",
        "signature": "def __init__(self, min_length=None, max_length=None, **kwargs):",
        "type": "Method",
        "class_signature": "class Text(Column):"
      },
      "UserDefinedType.__init__": {
        "code": "    def __init__(self, user_type, **kwargs):\n        \"\"\"Initializes a UserDefinedType column, which represents a user-defined type in a Cassandra database.\n\nParameters:\n- user_type (type): A class representing the user-defined type model (subclass of `UserType`), which specifies the structure and types of the fields in the UDT.\n- **kwargs: Additional keyword arguments that are passed to the parent `Column` class.\n\nThis constructor sets the `db_type` attribute to a string formatted as \"frozen<user_type_name>\", indicating that the UDT is immutable when stored in the database. It also initializes the `user_type` attribute with the provided `user_type` argument. The class leverages the `Column` class's functionality, which includes methods for validation and data conversion. The `UserDefinedType` class can be utilized to define complex data structures representing related data in a single database column, thereby enhancing schema organization and data integrity.\"\"\"\n        '\\n        :param type user_type: specifies the :class:`~.cqlengine.usertype.UserType` model of the column\\n        '\n        self.user_type = user_type\n        self.db_type = 'frozen<%s>' % user_type.type_name()\n        super(UserDefinedType, self).__init__(**kwargs)",
        "docstring": "Initializes a UserDefinedType column, which represents a user-defined type in a Cassandra database.\n\nParameters:\n- user_type (type): A class representing the user-defined type model (subclass of `UserType`), which specifies the structure and types of the fields in the UDT.\n- **kwargs: Additional keyword arguments that are passed to the parent `Column` class.\n\nThis constructor sets the `db_type` attribute to a string formatted as \"frozen<user_type_name>\", indicating that the UDT is immutable when stored in the database. It also initializes the `user_type` attribute with the provided `user_type` argument. The class leverages the `Column` class's functionality, which includes methods for validation and data conversion. The `UserDefinedType` class can be utilized to define complex data structures representing related data in a single database column, thereby enhancing schema organization and data integrity.",
        "signature": "def __init__(self, user_type, **kwargs):",
        "type": "Method",
        "class_signature": "class UserDefinedType(Column):"
      }
    },
    "cassandra/cqlengine/query.py": {},
    "cassandra/cqlengine/usertype.py": {
      "UserTypeMetaClass.__new__": {
        "code": "    def __new__(cls, name, bases, attrs):\n        \"\"\"Creates a new instance of the UserTypeMetaClass, which is responsible for defining the structure of User Defined Types (UDTs) in Cassandra.\n\n    This method takes the class name, bases, and class attributes as parameters, processes field definitions (specifically those that are instances of `columns.Column`), and builds internal mappings for fields and their corresponding database column names. It raises a `UserTypeDefinitionException` if there are conflicts with built-in attributes or if there are naming conflicts between field names and database column names.\n\n    Parameters:\n        cls (type): The metaclass being defined.\n        name (str): The name of the class being created.\n        bases (tuple): A tuple of base classes from which the new class inherits.\n        attrs (dict): A dictionary of class attributes and their definitions.\n\n    Returns:\n        type: The newly created class object.\n\n    Side Effects:\n        Modifies the provided `attrs` dictionary to include a `_fields` entry containing an ordered mapping of field names to their corresponding column objects and a `_db_map` that maps database field names to class attributes.\n\n    Dependencies:\n        - `OrderedDict`: Used to maintain the order of field definitions.\n        - `columns.Column`: Must be defined and accessible to identify valid column definitions.\n        - `models.ColumnDescriptor`: Used to create a descriptor for each column in the UDT.\"\"\"\n        field_dict = OrderedDict()\n        field_defs = [(k, v) for k, v in attrs.items() if isinstance(v, columns.Column)]\n        field_defs = sorted(field_defs, key=lambda x: x[1].position)\n\n        def _transform_column(field_name, field_obj):\n            field_dict[field_name] = field_obj\n            field_obj.set_column_name(field_name)\n            attrs[field_name] = models.ColumnDescriptor(field_obj)\n        for k, v in field_defs:\n            if k in BaseUserType.__dict__:\n                raise UserTypeDefinitionException(\"field '{0}' conflicts with built-in attribute/method\".format(k))\n            _transform_column(k, v)\n        attrs['_fields'] = field_dict\n        db_map = {}\n        for field_name, field in field_dict.items():\n            db_field = field.db_field_name\n            if db_field != field_name:\n                if db_field in field_dict:\n                    raise UserTypeDefinitionException(\"db_field '{0}' for field '{1}' conflicts with another attribute name\".format(db_field, field_name))\n                db_map[db_field] = field_name\n        attrs['_db_map'] = db_map\n        klass = super(UserTypeMetaClass, cls).__new__(cls, name, bases, attrs)\n        return klass",
        "docstring": "Creates a new instance of the UserTypeMetaClass, which is responsible for defining the structure of User Defined Types (UDTs) in Cassandra.\n\nThis method takes the class name, bases, and class attributes as parameters, processes field definitions (specifically those that are instances of `columns.Column`), and builds internal mappings for fields and their corresponding database column names. It raises a `UserTypeDefinitionException` if there are conflicts with built-in attributes or if there are naming conflicts between field names and database column names.\n\nParameters:\n    cls (type): The metaclass being defined.\n    name (str): The name of the class being created.\n    bases (tuple): A tuple of base classes from which the new class inherits.\n    attrs (dict): A dictionary of class attributes and their definitions.\n\nReturns:\n    type: The newly created class object.\n\nSide Effects:\n    Modifies the provided `attrs` dictionary to include a `_fields` entry containing an ordered mapping of field names to their corresponding column objects and a `_db_map` that maps database field names to class attributes.\n\nDependencies:\n    - `OrderedDict`: Used to maintain the order of field definitions.\n    - `columns.Column`: Must be defined and accessible to identify valid column definitions.\n    - `models.ColumnDescriptor`: Used to create a descriptor for each column in the UDT.",
        "signature": "def __new__(cls, name, bases, attrs):",
        "type": "Method",
        "class_signature": "class UserTypeMetaClass(type):"
      }
    },
    "cassandra/cqlengine/models.py": {
      "ModelMetaClass.__new__": {
        "code": "    def __new__(cls, name, bases, attrs):\n        \"\"\"__new__(cls, name, bases, attrs) -> Type:\n        Creates a new model class with fields defined as columns, organizing and validating their attributes, column definitions, and relationships. This method is responsible for:\n        - Moving column definitions into a dedicated dictionary (`column_dict`), ensuring proper structure and validation.\n        - Inheriting properties from base classes and setting up primary keys, partition keys, and clustering keys if applicable.\n        - Handling model attributes related to polymorphism, including the setup of a discriminator column if defined.\n        - Validating constraints, such as the presence of primary keys and the unique naming of columns.\n\n        Parameters:\n        - cls: The Class type that is being created.\n        - name: The name of the model class being defined.\n        - bases: A tuple of base classes that this model inherits from.\n        - attrs: A dictionary of class attributes, including column definitions and other model-related metadata.\n\n        Returns:\n        - A new instance of the model class type.\n\n        Key Constants:\n        - `ModelDefinitionException`: Raised for issues related to the definition of the model, such as missing primary keys or conflicting column names.\n        - `BaseModel`: Serves as the foundation for all model classes, providing shared functionality.\n        - `DEFAULT_KEYSPACE`: Used to determine the keyspace for the model if one is not specified.\n        \n        This method also registers user-defined types (UDTs) for the appropriate keyspace and sets up error handling for instances that do not conform to the defined schema.\"\"\"\n        column_dict = OrderedDict()\n        primary_keys = OrderedDict()\n        pk_name = None\n        inherited_columns = OrderedDict()\n        for base in bases:\n            for k, v in getattr(base, '_defined_columns', {}).items():\n                inherited_columns.setdefault(k, v)\n        is_abstract = attrs['__abstract__'] = attrs.get('__abstract__', False)\n        attrs['__discriminator_value__'] = attrs.get('__discriminator_value__')\n        options = attrs.get('__options__') or {}\n        attrs['__default_ttl__'] = options.get('default_time_to_live')\n        column_definitions = [(k, v) for k, v in attrs.items() if isinstance(v, columns.Column)]\n        column_definitions = sorted(column_definitions, key=lambda x: x[1].position)\n        is_polymorphic_base = any([c[1].discriminator_column for c in column_definitions])\n        column_definitions = [x for x in inherited_columns.items()] + column_definitions\n        discriminator_columns = [c for c in column_definitions if c[1].discriminator_column]\n        is_polymorphic = len(discriminator_columns) > 0\n        if len(discriminator_columns) > 1:\n            raise ModelDefinitionException('only one discriminator_column can be defined in a model, {0} found'.format(len(discriminator_columns)))\n        if attrs['__discriminator_value__'] and (not is_polymorphic):\n            raise ModelDefinitionException('__discriminator_value__ specified, but no base columns defined with discriminator_column=True')\n        discriminator_column_name, discriminator_column = discriminator_columns[0] if discriminator_columns else (None, None)\n        if isinstance(discriminator_column, (columns.BaseContainerColumn, columns.Counter)):\n            raise ModelDefinitionException('counter and container columns cannot be used as discriminator columns')\n        polymorphic_base = None\n        if is_polymorphic and (not is_polymorphic_base):\n\n            def _get_polymorphic_base(bases):\n                for base in bases:\n                    if getattr(base, '_is_polymorphic_base', False):\n                        return base\n                    klass = _get_polymorphic_base(base.__bases__)\n                    if klass:\n                        return klass\n            polymorphic_base = _get_polymorphic_base(bases)\n        defined_columns = OrderedDict(column_definitions)\n        if not is_abstract and (not any([v.primary_key for k, v in column_definitions])):\n            raise ModelDefinitionException('At least 1 primary key is required.')\n        counter_columns = [c for c in defined_columns.values() if isinstance(c, columns.Counter)]\n        data_columns = [c for c in defined_columns.values() if not c.primary_key and (not isinstance(c, columns.Counter))]\n        if counter_columns and data_columns:\n            raise ModelDefinitionException('counter models may not have data columns')\n        has_partition_keys = any((v.partition_key for k, v in column_definitions))\n\n        def _transform_column(col_name, col_obj):\n            column_dict[col_name] = col_obj\n            if col_obj.primary_key:\n                primary_keys[col_name] = col_obj\n            col_obj.set_column_name(col_name)\n            attrs[col_name] = ColumnDescriptor(col_obj)\n        partition_key_index = 0\n        for k, v in column_definitions:\n            if k in BaseModel.__dict__:\n                raise ModelDefinitionException(\"column '{0}' conflicts with built-in attribute/method\".format(k))\n            if (v.primary_key or v.partition_key) and isinstance(v, columns.Counter):\n                raise ModelDefinitionException('counter columns cannot be used as primary keys')\n            if not has_partition_keys and v.primary_key:\n                v.partition_key = True\n                has_partition_keys = True\n            if v.partition_key:\n                v._partition_key_index = partition_key_index\n                partition_key_index += 1\n            overriding = column_dict.get(k)\n            if overriding:\n                v.position = overriding.position\n                v.partition_key = overriding.partition_key\n                v._partition_key_index = overriding._partition_key_index\n            _transform_column(k, v)\n        partition_keys = OrderedDict((k for k in primary_keys.items() if k[1].partition_key))\n        clustering_keys = OrderedDict((k for k in primary_keys.items() if not k[1].partition_key))\n        if attrs.get('__compute_routing_key__', True):\n            key_cols = [c for c in partition_keys.values()]\n            partition_key_index = dict(((col.db_field_name, col._partition_key_index) for col in key_cols))\n            key_cql_types = [c.cql_type for c in key_cols]\n            key_serializer = staticmethod(lambda parts, proto_version: [t.to_binary(p, proto_version) for t, p in zip(key_cql_types, parts)])\n        else:\n            partition_key_index = {}\n            key_serializer = staticmethod(lambda parts, proto_version: None)\n        if len(partition_keys) == 0:\n            if not is_abstract:\n                raise ModelException('at least one partition key must be defined')\n        if len(partition_keys) == 1:\n            pk_name = [x for x in partition_keys.keys()][0]\n            attrs['pk'] = attrs[pk_name]\n        else:\n            _get = lambda self: tuple((self._values[c].getval() for c in partition_keys.keys()))\n            _set = lambda self, val: tuple((self._values[c].setval(v) for c, v in zip(partition_keys.keys(), val)))\n            attrs['pk'] = property(_get, _set)\n        col_names = set()\n        for v in column_dict.values():\n            if v.db_field_name in col_names:\n                raise ModelException(\"{0} defines the column '{1}' more than once\".format(name, v.db_field_name))\n            if v.clustering_order and (not (v.primary_key and (not v.partition_key))):\n                raise ModelException('clustering_order may be specified only for clustering primary keys')\n            if v.clustering_order and v.clustering_order.lower() not in ('asc', 'desc'):\n                raise ModelException(\"invalid clustering order '{0}' for column '{1}'\".format(repr(v.clustering_order), v.db_field_name))\n            col_names.add(v.db_field_name)\n        db_map = {}\n        for col_name, field in column_dict.items():\n            db_field = field.db_field_name\n            if db_field != col_name:\n                db_map[db_field] = col_name\n        attrs['_columns'] = column_dict\n        attrs['_primary_keys'] = primary_keys\n        attrs['_defined_columns'] = defined_columns\n        attrs['_db_map'] = db_map\n        attrs['_pk_name'] = pk_name\n        attrs['_dynamic_columns'] = {}\n        attrs['_partition_keys'] = partition_keys\n        attrs['_partition_key_index'] = partition_key_index\n        attrs['_key_serializer'] = key_serializer\n        attrs['_clustering_keys'] = clustering_keys\n        attrs['_has_counter'] = len(counter_columns) > 0\n        attrs['_is_polymorphic_base'] = is_polymorphic_base\n        attrs['_is_polymorphic'] = is_polymorphic\n        attrs['_polymorphic_base'] = polymorphic_base\n        attrs['_discriminator_column'] = discriminator_column\n        attrs['_discriminator_column_name'] = discriminator_column_name\n        attrs['_discriminator_map'] = {} if is_polymorphic_base else None\n        DoesNotExistBase = None\n        for base in bases:\n            DoesNotExistBase = getattr(base, 'DoesNotExist', None)\n            if DoesNotExistBase is not None:\n                break\n        DoesNotExistBase = DoesNotExistBase or attrs.pop('DoesNotExist', BaseModel.DoesNotExist)\n        attrs['DoesNotExist'] = type('DoesNotExist', (DoesNotExistBase,), {})\n        MultipleObjectsReturnedBase = None\n        for base in bases:\n            MultipleObjectsReturnedBase = getattr(base, 'MultipleObjectsReturned', None)\n            if MultipleObjectsReturnedBase is not None:\n                break\n        MultipleObjectsReturnedBase = MultipleObjectsReturnedBase or attrs.pop('MultipleObjectsReturned', BaseModel.MultipleObjectsReturned)\n        attrs['MultipleObjectsReturned'] = type('MultipleObjectsReturned', (MultipleObjectsReturnedBase,), {})\n        klass = super(ModelMetaClass, cls).__new__(cls, name, bases, attrs)\n        udts = []\n        for col in column_dict.values():\n            columns.resolve_udts(col, udts)\n        for user_type in set(udts):\n            user_type.register_for_keyspace(klass._get_keyspace())\n        return klass",
        "docstring": "__new__(cls, name, bases, attrs) -> Type:\nCreates a new model class with fields defined as columns, organizing and validating their attributes, column definitions, and relationships. This method is responsible for:\n- Moving column definitions into a dedicated dictionary (`column_dict`), ensuring proper structure and validation.\n- Inheriting properties from base classes and setting up primary keys, partition keys, and clustering keys if applicable.\n- Handling model attributes related to polymorphism, including the setup of a discriminator column if defined.\n- Validating constraints, such as the presence of primary keys and the unique naming of columns.\n\nParameters:\n- cls: The Class type that is being created.\n- name: The name of the model class being defined.\n- bases: A tuple of base classes that this model inherits from.\n- attrs: A dictionary of class attributes, including column definitions and other model-related metadata.\n\nReturns:\n- A new instance of the model class type.\n\nKey Constants:\n- `ModelDefinitionException`: Raised for issues related to the definition of the model, such as missing primary keys or conflicting column names.\n- `BaseModel`: Serves as the foundation for all model classes, providing shared functionality.\n- `DEFAULT_KEYSPACE`: Used to determine the keyspace for the model if one is not specified.\n\nThis method also registers user-defined types (UDTs) for the appropriate keyspace and sets up error handling for instances that do not conform to the defined schema.",
        "signature": "def __new__(cls, name, bases, attrs):",
        "type": "Method",
        "class_signature": "class ModelMetaClass(type):"
      }
    },
    "cassandra/__init__.py": {},
    "cassandra/datastax/graph/types.py": {}
  },
  "dependency_dict": {},
  "call_tree": {
    "tests/unit/cqlengine/test_udt.py:UDTTest:test_initialization_without_existing_connection": {
      "tests/unit/cqlengine/test_udt.py:UDTTest:Value": {
        "cassandra/cqlengine/columns.py:Text:__init__": {
          "cassandra/cqlengine/columns.py:Column:__init__": {}
        }
      },
      "cassandra/cqlengine/usertype.py:UserTypeMetaClass:__new__": {
        "cassandra/cqlengine/usertype.py:UserTypeMetaClass:_transform_column": {
          "cassandra/cqlengine/columns.py:Column:set_column_name": {},
          "cassandra/cqlengine/models.py:ColumnDescriptor:__init__": {
            "cassandra/cqlengine/models.py:ColumnQueryEvaluator:__init__": {}
          }
        },
        "cassandra/cqlengine/columns.py:Column:db_field_name": {}
      },
      "tests/unit/cqlengine/test_udt.py:UDTTest:DummyUDT": {
        "cassandra/cqlengine/columns.py:Column:__init__": {},
        "cassandra/cqlengine/columns.py:UserDefinedType:__init__": {
          "cassandra/cqlengine/usertype.py:BaseUserType:type_name": {},
          "cassandra/cqlengine/columns.py:Column:__init__": {}
        }
      },
      "cassandra/cqlengine/models.py:ModelMetaClass:__new__": {
        "cassandra/cqlengine/models.py:ModelMetaClass:_transform_column": {
          "cassandra/cqlengine/columns.py:Column:set_column_name": {},
          "cassandra/cqlengine/models.py:ColumnDescriptor:__init__": {
            "cassandra/cqlengine/models.py:ColumnQueryEvaluator:__init__": {}
          }
        },
        "cassandra/cqlengine/columns.py:Column:db_field_name": {},
        "cassandra/cqlengine/columns.py:Column:cql_type": {},
        "cassandra/cqlengine/columns.py:resolve_udts": {
          "cassandra/cqlengine/columns.py:UserDefinedType:UserDefinedType": {},
          "cassandra/cqlengine/columns.py:Column:sub_types": {},
          "cassandra/cqlengine/columns.py:UserDefinedType:sub_types": {},
          "cassandra/cqlengine/columns.py:resolve_udts": {
            "[ignored_or_cut_off]": "..."
          }
        },
        "cassandra/cqlengine/models.py:MultipleObjectsReturned:_get_keyspace": {},
        "cassandra/cqlengine/usertype.py:BaseUserType:register_for_keyspace": {
          "cassandra/cqlengine/usertype.py:BaseUserType:type_name": {},
          "cassandra/cqlengine/connection.py:register_udt": {
            "cassandra/cqlengine/connection.py:get_cluster": {
              "cassandra/cqlengine/connection.py:get_connection": {}
            }
          }
        }
      }
    },
    "/mnt/sfs_turbo/yaxindu/tmp/scylla_driver-image-test_udt/scylla_driver-test_udt/tests/unit/test_policies.py:TestRackOrDCAwareRoundRobinPolicy:test_with_remotes": {
      "cassandra/policies.py:DCAwareRoundRobinPolicy:DCAwareRoundRobinPolicy": {},
      "cassandra/policies.py:RackAwareRoundRobinPolicy:RackAwareRoundRobinPolicy": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/scylla_driver-image-test_udt/scylla_driver-test_udt/tests/unit/test_policies.py:TestRackOrDCAwareRoundRobinPolicy:test_get_distance": {
      "cassandra/policies.py:DCAwareRoundRobinPolicy:DCAwareRoundRobinPolicy": {},
      "cassandra/policies.py:RackAwareRoundRobinPolicy:RackAwareRoundRobinPolicy": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/scylla_driver-image-test_udt/scylla_driver-test_udt/tests/integration/cqlengine/model/test_class_construction.py:TestAbstractModelClasses:test_abstract_columns_are_inherited": {
      "cassandra/cqlengine/models.py:ColumnQueryEvaluator:ColumnQueryEvaluator": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/scylla_driver-image-test_udt/scylla_driver-test_udt/tests/integration/cqlengine/query/test_named.py:TestQuerySetCountSelectionAndIteration:test_get_success_case": {
      "cassandra/cqlengine/query.py:ResultObject:ResultObject": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/scylla_driver-image-test_udt/scylla_driver-test_udt/tests/integration/cqlengine/query/test_named.py:TestQuerySetCountSelectionAndIteration:test_query_expression_get_success_case": {
      "cassandra/cqlengine/query.py:ResultObject:ResultObject": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/scylla_driver-image-test_udt/scylla_driver-test_udt/tests/integration/cqlengine/columns/test_validation.py:TestTimeUUIDFromDatetime:test_conversion_specific_date": {
      "cassandra/cqlengine/columns.py:UUID:UUID": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/scylla_driver-image-test_udt/scylla_driver-test_udt/tests/integration/long/test_loadbalancingpolicies.py:LoadBalancingPolicyTests:test_token_aware_is_used_by_default": {
      "cassandra/policies.py:TokenAwarePolicy:TokenAwarePolicy": {},
      "cassandra/policies.py:DCAwareRoundRobinPolicy:DCAwareRoundRobinPolicy": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/scylla_driver-image-test_udt/scylla_driver-test_udt/tests/integration/advanced/graph/test_graph.py:GraphTimeoutTests:test_server_timeout_less_then_request": {
      "cassandra/__init__.py:InvalidRequest:InvalidRequest": {},
      "cassandra/__init__.py:OperationTimedOut:OperationTimedOut": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/scylla_driver-image-test_udt/scylla_driver-test_udt/tests/integration/advanced/graph/test_graph.py:GraphProfileTests:test_graph_profile": {
      "cassandra/datastax/graph/types.py:Vertex:Vertex": {},
      "cassandra/__init__.py:InvalidRequest:InvalidRequest": {},
      "cassandra/__init__.py:OperationTimedOut:OperationTimedOut": {}
    }
  },
  "PRD": "# PROJECT NAME: scylla_driver-test_udt\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 cassandra/\n    \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 InvalidRequest.InvalidRequest\n    \u2502   \u2514\u2500\u2500 OperationTimedOut.OperationTimedOut\n    \u251c\u2500\u2500 cqlengine/\n    \u2502   \u251c\u2500\u2500 columns.py\n    \u2502   \u2502   \u251c\u2500\u2500 Column.__init__\n    \u2502   \u2502   \u251c\u2500\u2500 Text.__init__\n    \u2502   \u2502   \u251c\u2500\u2500 UUID.UUID\n    \u2502   \u2502   \u2514\u2500\u2500 UserDefinedType.__init__\n    \u2502   \u251c\u2500\u2500 models.py\n    \u2502   \u2502   \u251c\u2500\u2500 ColumnQueryEvaluator.ColumnQueryEvaluator\n    \u2502   \u2502   \u2514\u2500\u2500 ModelMetaClass.__new__\n    \u2502   \u251c\u2500\u2500 query.py\n    \u2502   \u2502   \u2514\u2500\u2500 ResultObject.ResultObject\n    \u2502   \u2514\u2500\u2500 usertype.py\n    \u2502       \u2514\u2500\u2500 UserTypeMetaClass.__new__\n    \u251c\u2500\u2500 datastax/\n    \u2502   \u2514\u2500\u2500 graph/\n    \u2502       \u2514\u2500\u2500 types.py\n    \u2502           \u2514\u2500\u2500 Vertex.Vertex\n    \u2514\u2500\u2500 policies.py\n        \u251c\u2500\u2500 DCAwareRoundRobinPolicy.DCAwareRoundRobinPolicy\n        \u251c\u2500\u2500 RackAwareRoundRobinPolicy.RackAwareRoundRobinPolicy\n        \u2514\u2500\u2500 TokenAwarePolicy.TokenAwarePolicy\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThis module facilitates the definition and use of user-defined types (UDTs) within Cassandra object models in Python applications. It enables developers to define schema models with complex UDT fields without requiring an active connection to a Cassandra database at the time of model initialization. By seamlessly integrating UDTs into schema definitions, the module allows for more expressive and structured data representations, supporting advanced use cases in Cassandra's data modeling. This addresses a critical need for flexibility and enhances the developer experience by decoupling schema design from database connectivity, streamlining application development and testing workflows.\n\n## FILE 1: cassandra/policies.py\n\n## FILE 2: cassandra/cqlengine/columns.py\n\n- CLASS METHOD: UserDefinedType.__init__\n  - CLASS SIGNATURE: class UserDefinedType(Column):\n  - SIGNATURE: def __init__(self, user_type, **kwargs):\n  - DOCSTRING: \n```python\n\"\"\"\nInitializes a UserDefinedType column, which represents a user-defined type in a Cassandra database.\n\nParameters:\n- user_type (type): A class representing the user-defined type model (subclass of `UserType`), which specifies the structure and types of the fields in the UDT.\n- **kwargs: Additional keyword arguments that are passed to the parent `Column` class.\n\nThis constructor sets the `db_type` attribute to a string formatted as \"frozen<user_type_name>\", indicating that the UDT is immutable when stored in the database. It also initializes the `user_type` attribute with the provided `user_type` argument. The class leverages the `Column` class's functionality, which includes methods for validation and data conversion. The `UserDefinedType` class can be utilized to define complex data structures representing related data in a single database column, thereby enhancing schema organization and data integrity.\n\"\"\"\n```\n\n- CLASS METHOD: Text.__init__\n  - CLASS SIGNATURE: class Text(Column):\n  - SIGNATURE: def __init__(self, min_length=None, max_length=None, **kwargs):\n  - DOCSTRING: \n```python\n\"\"\"\nInitializes a Text column for storing UTF-8 encoded strings with optional length constraints.\n\n:param int min_length: Specifies the minimum allowable length of the string. Defaults to 1 if the column is required; otherwise, it can be None.\n:param int max_length: Specifies the maximum allowable length of the string.\n:param kwargs: Additional keyword arguments are passed to the parent Column class constructor.\n\nRaises ValueError: If min_length is negative, if max_length is negative, or if max_length is less than min_length.\n\nThis constructor interacts with the parent Column class, which manages database column properties and validation. It ensures that the string length constraints are within valid boundaries while also delegating other column functionalities to the parent class.\n\"\"\"\n```\n\n- CLASS METHOD: Column.__init__\n  - CLASS SIGNATURE: class Column(object):\n  - SIGNATURE: def __init__(self, primary_key=False, partition_key=False, index=False, db_field=None, default=None, required=False, clustering_order=None, discriminator_column=False, static=False, custom_index=False):\n  - DOCSTRING: \n```python\n\"\"\"\nInitializes a Column object that represents a database column in a Cassandra model.\n\nParameters:\n- primary_key (bool): Indicates if this column is a primary key. Default is False.\n- partition_key (bool): Indicates if this column should be the partition key. Default is False.\n- index (bool): Indicates if an index should be created for this column. Default is False.\n- db_field (str or None): The field name this column will map to in the database. Default is None.\n- default (any or None): The default value for the column, can also be a callable. Default is None.\n- required (bool): Indicates if this field is required. Default is False.\n- clustering_order (str or None): Defines the order of clustering keys. Default is None.\n- discriminator_column (bool): Indicates if this column will be used for discriminating records of inherited models. Default is False.\n- static (bool): Indicates if this is a static column, which has a single value per partition. Default is False.\n- custom_index (bool): Indicates if an index is managed outside of the CQL engine. Default is False.\n\nThis constructor sets initial values for various attributes including `partition_key`, `primary_key`, `index`, and others, which control the behavior and attributes of the column in the database schema. It also manages the instantiation order through the `position` attribute, utilizing a class-level counter (`instance_counter`) to track the number of instances created.\n\"\"\"\n```\n\n## FILE 3: cassandra/cqlengine/query.py\n\n## FILE 4: cassandra/cqlengine/usertype.py\n\n- CLASS METHOD: UserTypeMetaClass.__new__\n  - CLASS SIGNATURE: class UserTypeMetaClass(type):\n  - SIGNATURE: def __new__(cls, name, bases, attrs):\n  - DOCSTRING: \n```python\n\"\"\"\nCreates a new instance of the UserTypeMetaClass, which is responsible for defining the structure of User Defined Types (UDTs) in Cassandra.\n\nThis method takes the class name, bases, and class attributes as parameters, processes field definitions (specifically those that are instances of `columns.Column`), and builds internal mappings for fields and their corresponding database column names. It raises a `UserTypeDefinitionException` if there are conflicts with built-in attributes or if there are naming conflicts between field names and database column names.\n\nParameters:\n    cls (type): The metaclass being defined.\n    name (str): The name of the class being created.\n    bases (tuple): A tuple of base classes from which the new class inherits.\n    attrs (dict): A dictionary of class attributes and their definitions.\n\nReturns:\n    type: The newly created class object.\n\nSide Effects:\n    Modifies the provided `attrs` dictionary to include a `_fields` entry containing an ordered mapping of field names to their corresponding column objects and a `_db_map` that maps database field names to class attributes.\n\nDependencies:\n    - `OrderedDict`: Used to maintain the order of field definitions.\n    - `columns.Column`: Must be defined and accessible to identify valid column definitions.\n    - `models.ColumnDescriptor`: Used to create a descriptor for each column in the UDT.\n\"\"\"\n```\n\n## FILE 5: cassandra/cqlengine/models.py\n\n- CLASS METHOD: ModelMetaClass.__new__\n  - CLASS SIGNATURE: class ModelMetaClass(type):\n  - SIGNATURE: def __new__(cls, name, bases, attrs):\n  - DOCSTRING: \n```python\n\"\"\"\n__new__(cls, name, bases, attrs) -> Type:\nCreates a new model class with fields defined as columns, organizing and validating their attributes, column definitions, and relationships. This method is responsible for:\n- Moving column definitions into a dedicated dictionary (`column_dict`), ensuring proper structure and validation.\n- Inheriting properties from base classes and setting up primary keys, partition keys, and clustering keys if applicable.\n- Handling model attributes related to polymorphism, including the setup of a discriminator column if defined.\n- Validating constraints, such as the presence of primary keys and the unique naming of columns.\n\nParameters:\n- cls: The Class type that is being created.\n- name: The name of the model class being defined.\n- bases: A tuple of base classes that this model inherits from.\n- attrs: A dictionary of class attributes, including column definitions and other model-related metadata.\n\nReturns:\n- A new instance of the model class type.\n\nKey Constants:\n- `ModelDefinitionException`: Raised for issues related to the definition of the model, such as missing primary keys or conflicting column names.\n- `BaseModel`: Serves as the foundation for all model classes, providing shared functionality.\n- `DEFAULT_KEYSPACE`: Used to determine the keyspace for the model if one is not specified.\n\nThis method also registers user-defined types (UDTs) for the appropriate keyspace and sets up error handling for instances that do not conform to the defined schema.\n\"\"\"\n```\n\n## FILE 6: cassandra/__init__.py\n\n## FILE 7: cassandra/datastax/graph/types.py\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "cassandra/policies.py": "import random\nfrom collections import namedtuple\nfrom functools import lru_cache\nfrom itertools import islice, cycle, groupby, repeat\nimport logging\nfrom random import randint, shuffle\nfrom threading import Lock\nimport socket\nimport warnings\nlog = logging.getLogger(__name__)\nfrom cassandra import WriteType as WT\nWriteType = WT\nfrom cassandra import ConsistencyLevel, OperationTimedOut\n\nclass HostDistance(object):\n    \"\"\"\n    A measure of how \"distant\" a node is from the client, which\n    may influence how the load balancer distributes requests\n    and how many connections are opened to the node.\n    \"\"\"\n    IGNORED = -1\n    '\\n    A node with this distance should never be queried or have\\n    connections opened to it.\\n    '\n    LOCAL_RACK = 0\n    '\\n    Nodes with ``LOCAL_RACK`` distance will be preferred for operations\\n    under some load balancing policies (such as :class:`.RackAwareRoundRobinPolicy`)\\n    and will have a greater number of connections opened against\\n    them by default.\\n\\n    This distance is typically used for nodes within the same\\n    datacenter and the same rack as the client.\\n    '\n    LOCAL = 1\n    '\\n    Nodes with ``LOCAL`` distance will be preferred for operations\\n    under some load balancing policies (such as :class:`.DCAwareRoundRobinPolicy`)\\n    and will have a greater number of connections opened against\\n    them by default.\\n\\n    This distance is typically used for nodes within the same\\n    datacenter as the client.\\n    '\n    REMOTE = 2\n    '\\n    Nodes with ``REMOTE`` distance will be treated as a last resort\\n    by some load balancing policies (such as :class:`.DCAwareRoundRobinPolicy`\\n    and :class:`.RackAwareRoundRobinPolicy`)and will have a smaller number of\\n    connections opened against them by default.\\n\\n    This distance is typically used for nodes outside of the\\n    datacenter that the client is running in.\\n    '\n\nclass HostStateListener(object):\n\n    def on_up(self, host):\n        \"\"\" Called when a node is marked up. \"\"\"\n        raise NotImplementedError()\n\n    def on_down(self, host):\n        \"\"\" Called when a node is marked down. \"\"\"\n        raise NotImplementedError()\n\n    def on_add(self, host):\n        \"\"\"\n        Called when a node is added to the cluster.  The newly added node\n        should be considered up.\n        \"\"\"\n        raise NotImplementedError()\n\n    def on_remove(self, host):\n        \"\"\" Called when a node is removed from the cluster. \"\"\"\n        raise NotImplementedError()\n\nclass LoadBalancingPolicy(HostStateListener):\n    \"\"\"\n    Load balancing policies are used to decide how to distribute\n    requests among all possible coordinator nodes in the cluster.\n\n    In particular, they may focus on querying \"near\" nodes (those\n    in a local datacenter) or on querying nodes who happen to\n    be replicas for the requested data.\n\n    You may also use subclasses of :class:`.LoadBalancingPolicy` for\n    custom behavior.\n\n    You should always use immutable collections (e.g., tuples or\n    frozensets) to store information about hosts to prevent accidental\n    modification. When there are changes to the hosts (e.g., a host is\n    down or up), the old collection should be replaced with a new one.\n    \"\"\"\n    _hosts_lock = None\n\n    def __init__(self):\n        self._hosts_lock = Lock()\n\n    def distance(self, host):\n        \"\"\"\n        Returns a measure of how remote a :class:`~.pool.Host` is in\n        terms of the :class:`.HostDistance` enums.\n        \"\"\"\n        raise NotImplementedError()\n\n    def populate(self, cluster, hosts):\n        \"\"\"\n        This method is called to initialize the load balancing\n        policy with a set of :class:`.Host` instances before its\n        first use.  The `cluster` parameter is an instance of\n        :class:`.Cluster`.\n        \"\"\"\n        raise NotImplementedError()\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        \"\"\"\n        Given a :class:`~.query.Statement` instance, return a iterable\n        of :class:`.Host` instances which should be queried in that\n        order.  A generator may work well for custom implementations\n        of this method.\n\n        Note that the `query` argument may be :const:`None` when preparing\n        statements.\n\n        `working_keyspace` should be the string name of the current keyspace,\n        as set through :meth:`.Session.set_keyspace()` or with a ``USE``\n        statement.\n        \"\"\"\n        raise NotImplementedError()\n\n    def check_supported(self):\n        \"\"\"\n        This will be called after the cluster Metadata has been initialized.\n        If the load balancing policy implementation cannot be supported for\n        some reason (such as a missing C extension), this is the point at\n        which it should raise an exception.\n        \"\"\"\n        pass\n\nclass RoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    A subclass of :class:`.LoadBalancingPolicy` which evenly\n    distributes queries across all nodes in the cluster,\n    regardless of what datacenter the nodes may be in.\n    \"\"\"\n    _live_hosts = frozenset(())\n    _position = 0\n\n    def populate(self, cluster, hosts):\n        self._live_hosts = frozenset(hosts)\n        if len(hosts) > 1:\n            self._position = randint(0, len(hosts) - 1)\n\n    def distance(self, host):\n        return HostDistance.LOCAL\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        pos = self._position\n        self._position += 1\n        hosts = self._live_hosts\n        length = len(hosts)\n        if length:\n            pos %= length\n            return islice(cycle(hosts), pos, pos + length)\n        else:\n            return []\n\n    def on_up(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.union((host,))\n\n    def on_down(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.difference((host,))\n\n    def on_add(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.union((host,))\n\n    def on_remove(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.difference((host,))\n\nclass DCAwareRoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    Similar to :class:`.RoundRobinPolicy`, but prefers hosts\n    in the local datacenter and only uses nodes in remote\n    datacenters as a last resort.\n    \"\"\"\n    local_dc = None\n    used_hosts_per_remote_dc = 0\n\n    def __init__(self, local_dc='', used_hosts_per_remote_dc=0):\n        \"\"\"\n        The `local_dc` parameter should be the name of the datacenter\n        (such as is reported by ``nodetool ring``) that should\n        be considered local. If not specified, the driver will choose\n        a local_dc based on the first host among :attr:`.Cluster.contact_points`\n        having a valid DC. If relying on this mechanism, all specified\n        contact points should be nodes in a single, local DC.\n\n        `used_hosts_per_remote_dc` controls how many nodes in\n        each remote datacenter will have connections opened\n        against them. In other words, `used_hosts_per_remote_dc` hosts\n        will be considered :attr:`~.HostDistance.REMOTE` and the\n        rest will be considered :attr:`~.HostDistance.IGNORED`.\n        By default, all remote hosts are ignored.\n        \"\"\"\n        self.local_dc = local_dc\n        self.used_hosts_per_remote_dc = used_hosts_per_remote_dc\n        self._dc_live_hosts = {}\n        self._position = 0\n        self._endpoints = []\n        LoadBalancingPolicy.__init__(self)\n\n    def _dc(self, host):\n        return host.datacenter or self.local_dc\n\n    def populate(self, cluster, hosts):\n        for dc, dc_hosts in groupby(hosts, lambda h: self._dc(h)):\n            self._dc_live_hosts[dc] = tuple(set(dc_hosts))\n        if not self.local_dc:\n            self._endpoints = [endpoint for endpoint in cluster.endpoints_resolved]\n        self._position = randint(0, len(hosts) - 1) if hosts else 0\n\n    def distance(self, host):\n        dc = self._dc(host)\n        if dc == self.local_dc:\n            return HostDistance.LOCAL\n        if not self.used_hosts_per_remote_dc:\n            return HostDistance.IGNORED\n        else:\n            dc_hosts = self._dc_live_hosts.get(dc)\n            if not dc_hosts:\n                return HostDistance.IGNORED\n            if host in list(dc_hosts)[:self.used_hosts_per_remote_dc]:\n                return HostDistance.REMOTE\n            else:\n                return HostDistance.IGNORED\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        pos = self._position\n        self._position += 1\n        local_live = self._dc_live_hosts.get(self.local_dc, ())\n        pos = pos % len(local_live) if local_live else 0\n        for host in islice(cycle(local_live), pos, pos + len(local_live)):\n            yield host\n        other_dcs = [dc for dc in self._dc_live_hosts.copy().keys() if dc != self.local_dc]\n        for dc in other_dcs:\n            remote_live = self._dc_live_hosts.get(dc, ())\n            for host in remote_live[:self.used_hosts_per_remote_dc]:\n                yield host\n\n    def on_up(self, host):\n        if not self.local_dc and host.datacenter:\n            if host.endpoint in self._endpoints:\n                self.local_dc = host.datacenter\n                log.info(\"Using datacenter '%s' for DCAwareRoundRobinPolicy (via host '%s'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\" % (self.local_dc, host.endpoint))\n                del self._endpoints\n        dc = self._dc(host)\n        with self._hosts_lock:\n            current_hosts = self._dc_live_hosts.get(dc, ())\n            if host not in current_hosts:\n                self._dc_live_hosts[dc] = current_hosts + (host,)\n\n    def on_down(self, host):\n        dc = self._dc(host)\n        with self._hosts_lock:\n            current_hosts = self._dc_live_hosts.get(dc, ())\n            if host in current_hosts:\n                hosts = tuple((h for h in current_hosts if h != host))\n                if hosts:\n                    self._dc_live_hosts[dc] = hosts\n                else:\n                    del self._dc_live_hosts[dc]\n\n    def on_add(self, host):\n        self.on_up(host)\n\n    def on_remove(self, host):\n        self.on_down(host)\n\nclass RackAwareRoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    Similar to :class:`.DCAwareRoundRobinPolicy`, but prefers hosts\n    in the local rack, before hosts in the local datacenter but a\n    different rack, before hosts in all other datercentres\n    \"\"\"\n    local_dc = None\n    local_rack = None\n    used_hosts_per_remote_dc = 0\n\n    def __init__(self, local_dc, local_rack, used_hosts_per_remote_dc=0):\n        \"\"\"\n        The `local_dc` and `local_rack` parameters should be the name of the\n        datacenter and rack (such as is reported by ``nodetool ring``) that\n        should be considered local.\n\n        `used_hosts_per_remote_dc` controls how many nodes in\n        each remote datacenter will have connections opened\n        against them. In other words, `used_hosts_per_remote_dc` hosts\n        will be considered :attr:`~.HostDistance.REMOTE` and the\n        rest will be considered :attr:`~.HostDistance.IGNORED`.\n        By default, all remote hosts are ignored.\n        \"\"\"\n        self.local_rack = local_rack\n        self.local_dc = local_dc\n        self.used_hosts_per_remote_dc = used_hosts_per_remote_dc\n        self._live_hosts = {}\n        self._dc_live_hosts = {}\n        self._endpoints = []\n        self._position = 0\n        LoadBalancingPolicy.__init__(self)\n\n    def _rack(self, host):\n        return host.rack or self.local_rack\n\n    def _dc(self, host):\n        return host.datacenter or self.local_dc\n\n    def populate(self, cluster, hosts):\n        for (dc, rack), rack_hosts in groupby(hosts, lambda host: (self._dc(host), self._rack(host))):\n            self._live_hosts[dc, rack] = tuple(set(rack_hosts))\n        for dc, dc_hosts in groupby(hosts, lambda host: self._dc(host)):\n            self._dc_live_hosts[dc] = tuple(set(dc_hosts))\n        self._position = randint(0, len(hosts) - 1) if hosts else 0\n\n    def distance(self, host):\n        rack = self._rack(host)\n        dc = self._dc(host)\n        if rack == self.local_rack and dc == self.local_dc:\n            return HostDistance.LOCAL_RACK\n        if dc == self.local_dc:\n            return HostDistance.LOCAL\n        if not self.used_hosts_per_remote_dc:\n            return HostDistance.IGNORED\n        dc_hosts = self._dc_live_hosts.get(dc, ())\n        if not dc_hosts:\n            return HostDistance.IGNORED\n        if host in dc_hosts and dc_hosts.index(host) < self.used_hosts_per_remote_dc:\n            return HostDistance.REMOTE\n        else:\n            return HostDistance.IGNORED\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        pos = self._position\n        self._position += 1\n        local_rack_live = self._live_hosts.get((self.local_dc, self.local_rack), ())\n        pos = pos % len(local_rack_live) if local_rack_live else 0\n        for host in islice(cycle(local_rack_live), pos, pos + len(local_rack_live)):\n            yield host\n        local_live = [host for host in self._dc_live_hosts.get(self.local_dc, ()) if host.rack != self.local_rack]\n        pos = pos % len(local_live) if local_live else 0\n        for host in islice(cycle(local_live), pos, pos + len(local_live)):\n            yield host\n        for dc, remote_live in self._dc_live_hosts.copy().items():\n            if dc != self.local_dc:\n                for host in remote_live[:self.used_hosts_per_remote_dc]:\n                    yield host\n\n    def on_up(self, host):\n        dc = self._dc(host)\n        rack = self._rack(host)\n        with self._hosts_lock:\n            current_rack_hosts = self._live_hosts.get((dc, rack), ())\n            if host not in current_rack_hosts:\n                self._live_hosts[dc, rack] = current_rack_hosts + (host,)\n            current_dc_hosts = self._dc_live_hosts.get(dc, ())\n            if host not in current_dc_hosts:\n                self._dc_live_hosts[dc] = current_dc_hosts + (host,)\n\n    def on_down(self, host):\n        dc = self._dc(host)\n        rack = self._rack(host)\n        with self._hosts_lock:\n            current_rack_hosts = self._live_hosts.get((dc, rack), ())\n            if host in current_rack_hosts:\n                hosts = tuple((h for h in current_rack_hosts if h != host))\n                if hosts:\n                    self._live_hosts[dc, rack] = hosts\n                else:\n                    del self._live_hosts[dc, rack]\n            current_dc_hosts = self._dc_live_hosts.get(dc, ())\n            if host in current_dc_hosts:\n                hosts = tuple((h for h in current_dc_hosts if h != host))\n                if hosts:\n                    self._dc_live_hosts[dc] = hosts\n                else:\n                    del self._dc_live_hosts[dc]\n\n    def on_add(self, host):\n        self.on_up(host)\n\n    def on_remove(self, host):\n        self.on_down(host)\n\nclass TokenAwarePolicy(LoadBalancingPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` wrapper that adds token awareness to\n    a child policy.\n\n    This alters the child policy's behavior so that it first attempts to\n    send queries to :attr:`~.HostDistance.LOCAL` replicas (as determined\n    by the child policy) based on the :class:`.Statement`'s\n    :attr:`~.Statement.routing_key`. If :attr:`.shuffle_replicas` is\n    truthy, these replicas will be yielded in a random order. Once those\n    hosts are exhausted, the remaining hosts in the child policy's query\n    plan will be used in the order provided by the child policy.\n\n    If no :attr:`~.Statement.routing_key` is set on the query, the child\n    policy's query plan will be used as is.\n    \"\"\"\n    _child_policy = None\n    _cluster_metadata = None\n    _tablets_routing_v1 = False\n    shuffle_replicas = False\n    '\\n    Yield local replicas in a random order.\\n    '\n\n    def __init__(self, child_policy, shuffle_replicas=False):\n        self._child_policy = child_policy\n        self.shuffle_replicas = shuffle_replicas\n\n    def populate(self, cluster, hosts):\n        self._cluster_metadata = cluster.metadata\n        self._tablets_routing_v1 = cluster.control_connection._tablets_routing_v1\n        self._child_policy.populate(cluster, hosts)\n\n    def check_supported(self):\n        if not self._cluster_metadata.can_support_partitioner():\n            raise RuntimeError('%s cannot be used with the cluster partitioner (%s) because the relevant C extension for this driver was not compiled. See the installation instructions for details on building and installing the C extensions.' % (self.__class__.__name__, self._cluster_metadata.partitioner))\n\n    def distance(self, *args, **kwargs):\n        return self._child_policy.distance(*args, **kwargs)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        keyspace = query.keyspace if query and query.keyspace else working_keyspace\n        child = self._child_policy\n        if query is None or query.routing_key is None or keyspace is None:\n            for host in child.make_query_plan(keyspace, query):\n                yield host\n            return\n        replicas = []\n        if self._tablets_routing_v1:\n            tablet = self._cluster_metadata._tablets.get_tablet_for_key(keyspace, query.table, self._cluster_metadata.token_map.token_class.from_key(query.routing_key))\n            if tablet is not None:\n                replicas_mapped = set(map(lambda r: r[0], tablet.replicas))\n                child_plan = child.make_query_plan(keyspace, query)\n                replicas = [host for host in child_plan if host.host_id in replicas_mapped]\n        if not replicas:\n            replicas = self._cluster_metadata.get_replicas(keyspace, query.routing_key)\n        if self.shuffle_replicas:\n            shuffle(replicas)\n        for replica in replicas:\n            if replica.is_up and child.distance(replica) in [HostDistance.LOCAL, HostDistance.LOCAL_RACK]:\n                yield replica\n        for host in child.make_query_plan(keyspace, query):\n            if host not in replicas or child.distance(host) == HostDistance.REMOTE:\n                yield host\n\n    def on_up(self, *args, **kwargs):\n        return self._child_policy.on_up(*args, **kwargs)\n\n    def on_down(self, *args, **kwargs):\n        return self._child_policy.on_down(*args, **kwargs)\n\n    def on_add(self, *args, **kwargs):\n        return self._child_policy.on_add(*args, **kwargs)\n\n    def on_remove(self, *args, **kwargs):\n        return self._child_policy.on_remove(*args, **kwargs)\n\nclass WhiteListRoundRobinPolicy(RoundRobinPolicy):\n    \"\"\"\n    A subclass of :class:`.RoundRobinPolicy` which evenly\n    distributes queries across all nodes in the cluster,\n    regardless of what datacenter the nodes may be in, but\n    only if that node exists in the list of allowed nodes\n\n    This policy is addresses the issue described in\n    https://datastax-oss.atlassian.net/browse/JAVA-145\n    Where connection errors occur when connection\n    attempts are made to private IP addresses remotely\n    \"\"\"\n\n    def __init__(self, hosts):\n        \"\"\"\n        The `hosts` parameter should be a sequence of hosts to permit\n        connections to.\n        \"\"\"\n        self._allowed_hosts = tuple(hosts)\n        self._allowed_hosts_resolved = []\n        for h in self._allowed_hosts:\n            unix_socket_path = getattr(h, '_unix_socket_path', None)\n            if unix_socket_path:\n                self._allowed_hosts_resolved.append(unix_socket_path)\n            else:\n                self._allowed_hosts_resolved.extend([endpoint[4][0] for endpoint in socket.getaddrinfo(h, None, socket.AF_UNSPEC, socket.SOCK_STREAM)])\n        RoundRobinPolicy.__init__(self)\n\n    def populate(self, cluster, hosts):\n        self._live_hosts = frozenset((h for h in hosts if h.address in self._allowed_hosts_resolved))\n        if len(hosts) <= 1:\n            self._position = 0\n        else:\n            self._position = randint(0, len(hosts) - 1)\n\n    def distance(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            return HostDistance.LOCAL\n        else:\n            return HostDistance.IGNORED\n\n    def on_up(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            RoundRobinPolicy.on_up(self, host)\n\n    def on_add(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            RoundRobinPolicy.on_add(self, host)\n\nclass HostFilterPolicy(LoadBalancingPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` subclass configured with a child policy,\n    and a single-argument predicate. This policy defers to the child policy for\n    hosts where ``predicate(host)`` is truthy. Hosts for which\n    ``predicate(host)`` is falsy will be considered :attr:`.IGNORED`, and will\n    not be used in a query plan.\n\n    This can be used in the cases where you need a whitelist or blacklist\n    policy, e.g. to prepare for decommissioning nodes or for testing:\n\n    .. code-block:: python\n\n        def address_is_ignored(host):\n            return host.address in [ignored_address0, ignored_address1]\n\n        blacklist_filter_policy = HostFilterPolicy(\n            child_policy=RoundRobinPolicy(),\n            predicate=address_is_ignored\n        )\n\n        cluster = Cluster(\n            primary_host,\n            load_balancing_policy=blacklist_filter_policy,\n        )\n\n    See the note in the :meth:`.make_query_plan` documentation for a caveat on\n    how wrapping ordering polices (e.g. :class:`.RoundRobinPolicy`) may break\n    desirable properties of the wrapped policy.\n\n    Please note that whitelist and blacklist policies are not recommended for\n    general, day-to-day use. You probably want something like\n    :class:`.DCAwareRoundRobinPolicy`, which prefers a local DC but has\n    fallbacks, over a brute-force method like whitelisting or blacklisting.\n    \"\"\"\n\n    def __init__(self, child_policy, predicate):\n        \"\"\"\n        :param child_policy: an instantiated :class:`.LoadBalancingPolicy`\n                             that this one will defer to.\n        :param predicate: a one-parameter function that takes a :class:`.Host`.\n                          If it returns a falsy value, the :class:`.Host` will\n                          be :attr:`.IGNORED` and not returned in query plans.\n        \"\"\"\n        super(HostFilterPolicy, self).__init__()\n        self._child_policy = child_policy\n        self._predicate = predicate\n\n    def on_up(self, host, *args, **kwargs):\n        return self._child_policy.on_up(host, *args, **kwargs)\n\n    def on_down(self, host, *args, **kwargs):\n        return self._child_policy.on_down(host, *args, **kwargs)\n\n    def on_add(self, host, *args, **kwargs):\n        return self._child_policy.on_add(host, *args, **kwargs)\n\n    def on_remove(self, host, *args, **kwargs):\n        return self._child_policy.on_remove(host, *args, **kwargs)\n\n    @property\n    def predicate(self):\n        \"\"\"\n        A predicate, set on object initialization, that takes a :class:`.Host`\n        and returns a value. If the value is falsy, the :class:`.Host` is\n        :class:`~HostDistance.IGNORED`. If the value is truthy,\n        :class:`.HostFilterPolicy` defers to the child policy to determine the\n        host's distance.\n\n        This is a read-only value set in ``__init__``, implemented as a\n        ``property``.\n        \"\"\"\n        return self._predicate\n\n    def distance(self, host):\n        \"\"\"\n        Checks if ``predicate(host)``, then returns\n        :attr:`~HostDistance.IGNORED` if falsy, and defers to the child policy\n        otherwise.\n        \"\"\"\n        if self.predicate(host):\n            return self._child_policy.distance(host)\n        else:\n            return HostDistance.IGNORED\n\n    def populate(self, cluster, hosts):\n        self._child_policy.populate(cluster=cluster, hosts=hosts)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        \"\"\"\n        Defers to the child policy's\n        :meth:`.LoadBalancingPolicy.make_query_plan` and filters the results.\n\n        Note that this filtering may break desirable properties of the wrapped\n        policy in some cases. For instance, imagine if you configure this\n        policy to filter out ``host2``, and to wrap a round-robin policy that\n        rotates through three hosts in the order ``host1, host2, host3``,\n        ``host2, host3, host1``, ``host3, host1, host2``, repeating. This\n        policy will yield ``host1, host3``, ``host3, host1``, ``host3, host1``,\n        disproportionately favoring ``host3``.\n        \"\"\"\n        child_qp = self._child_policy.make_query_plan(working_keyspace=working_keyspace, query=query)\n        for host in child_qp:\n            if self.predicate(host):\n                yield host\n\n    def check_supported(self):\n        return self._child_policy.check_supported()\n\nclass ConvictionPolicy(object):\n    \"\"\"\n    A policy which decides when hosts should be considered down\n    based on the types of failures and the number of failures.\n\n    If custom behavior is needed, this class may be subclassed.\n    \"\"\"\n\n    def __init__(self, host):\n        \"\"\"\n        `host` is an instance of :class:`.Host`.\n        \"\"\"\n        self.host = host\n\n    def add_failure(self, connection_exc):\n        \"\"\"\n        Implementations should return :const:`True` if the host should be\n        convicted, :const:`False` otherwise.\n        \"\"\"\n        raise NotImplementedError()\n\n    def reset(self):\n        \"\"\"\n        Implementations should clear out any convictions or state regarding\n        the host.\n        \"\"\"\n        raise NotImplementedError()\n\nclass SimpleConvictionPolicy(ConvictionPolicy):\n    \"\"\"\n    The default implementation of :class:`ConvictionPolicy`,\n    which simply marks a host as down after the first failure\n    of any kind.\n    \"\"\"\n\n    def add_failure(self, connection_exc):\n        return not isinstance(connection_exc, OperationTimedOut)\n\n    def reset(self):\n        pass\n\nclass ReconnectionPolicy(object):\n    \"\"\"\n    This class and its subclasses govern how frequently an attempt is made\n    to reconnect to nodes that are marked as dead.\n\n    If custom behavior is needed, this class may be subclassed.\n    \"\"\"\n\n    def new_schedule(self):\n        \"\"\"\n        This should return a finite or infinite iterable of delays (each as a\n        floating point number of seconds) in-between each failed reconnection\n        attempt.  Note that if the iterable is finite, reconnection attempts\n        will cease once the iterable is exhausted.\n        \"\"\"\n        raise NotImplementedError()\n\nclass ConstantReconnectionPolicy(ReconnectionPolicy):\n    \"\"\"\n    A :class:`.ReconnectionPolicy` subclass which sleeps for a fixed delay\n    in-between each reconnection attempt.\n    \"\"\"\n\n    def __init__(self, delay, max_attempts=64):\n        \"\"\"\n        `delay` should be a floating point number of seconds to wait in-between\n        each attempt.\n\n        `max_attempts` should be a total number of attempts to be made before\n        giving up, or :const:`None` to continue reconnection attempts forever.\n        The default is 64.\n        \"\"\"\n        if delay < 0:\n            raise ValueError('delay must not be negative')\n        if max_attempts is not None and max_attempts < 0:\n            raise ValueError('max_attempts must not be negative')\n        self.delay = delay\n        self.max_attempts = max_attempts\n\n    def new_schedule(self):\n        if self.max_attempts:\n            return repeat(self.delay, self.max_attempts)\n        return repeat(self.delay)\n\nclass ExponentialReconnectionPolicy(ReconnectionPolicy):\n    \"\"\"\n    A :class:`.ReconnectionPolicy` subclass which exponentially increases\n    the length of the delay in-between each reconnection attempt up to\n    a set maximum delay.\n\n    A random amount of jitter (+/- 15%) will be added to the pure exponential\n    delay value to avoid the situations where many reconnection handlers are\n    trying to reconnect at exactly the same time.\n    \"\"\"\n\n    def __init__(self, base_delay, max_delay, max_attempts=64):\n        \"\"\"\n        `base_delay` and `max_delay` should be in floating point units of\n        seconds.\n\n        `max_attempts` should be a total number of attempts to be made before\n        giving up, or :const:`None` to continue reconnection attempts forever.\n        The default is 64.\n        \"\"\"\n        if base_delay < 0 or max_delay < 0:\n            raise ValueError('Delays may not be negative')\n        if max_delay < base_delay:\n            raise ValueError('Max delay must be greater than base delay')\n        if max_attempts is not None and max_attempts < 0:\n            raise ValueError('max_attempts must not be negative')\n        self.base_delay = base_delay\n        self.max_delay = max_delay\n        self.max_attempts = max_attempts\n\n    def new_schedule(self):\n        i, overflowed = (0, False)\n        while self.max_attempts is None or i < self.max_attempts:\n            if overflowed:\n                yield self.max_delay\n            else:\n                try:\n                    yield self._add_jitter(min(self.base_delay * 2 ** i, self.max_delay))\n                except OverflowError:\n                    overflowed = True\n                    yield self.max_delay\n            i += 1\n\n    def _add_jitter(self, value):\n        jitter = randint(85, 115)\n        delay = jitter * value / 100\n        return min(max(self.base_delay, delay), self.max_delay)\n\nclass RetryPolicy(object):\n    \"\"\"\n    A policy that describes whether to retry, rethrow, or ignore coordinator\n    timeout and unavailable failures. These are failures reported from the\n    server side. Timeouts are configured by\n    `settings in cassandra.yaml <https://github.com/apache/cassandra/blob/cassandra-2.1.4/conf/cassandra.yaml#L568-L584>`_.\n    Unavailable failures occur when the coordinator cannot achieve the consistency\n    level for a request. For further information see the method descriptions\n    below.\n\n    To specify a default retry policy, set the\n    :attr:`.Cluster.default_retry_policy` attribute to an instance of this\n    class or one of its subclasses.\n\n    To specify a retry policy per query, set the :attr:`.Statement.retry_policy`\n    attribute to an instance of this class or one of its subclasses.\n\n    If custom behavior is needed for retrying certain operations,\n    this class may be subclassed.\n    \"\"\"\n    RETRY = 0\n    '\\n    This should be returned from the below methods if the operation\\n    should be retried on the same connection.\\n    '\n    RETHROW = 1\n    '\\n    This should be returned from the below methods if the failure\\n    should be propagated and no more retries attempted.\\n    '\n    IGNORE = 2\n    '\\n    This should be returned from the below methods if the failure\\n    should be ignored but no more retries should be attempted.\\n    '\n    RETRY_NEXT_HOST = 3\n    '\\n    This should be returned from the below methods if the operation\\n    should be retried on another connection.\\n    '\n\n    def on_read_timeout(self, query, consistency, required_responses, received_responses, data_retrieved, retry_num):\n        \"\"\"\n        This is called when a read operation times out from the coordinator's\n        perspective (i.e. a replica did not respond to the coordinator in time).\n        It should return a tuple with two items: one of the class enums (such\n        as :attr:`.RETRY`) and a :class:`.ConsistencyLevel` to retry the\n        operation at or :const:`None` to keep the same consistency level.\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        The `required_responses` and `received_responses` parameters describe\n        how many replicas needed to respond to meet the requested consistency\n        level and how many actually did respond before the coordinator timed\n        out the request. `data_retrieved` is a boolean indicating whether\n        any of those responses contained data (as opposed to just a digest).\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, operations will be retried at most once, and only if\n        a sufficient number of replicas responded (with data digests).\n        \"\"\"\n        if retry_num != 0:\n            return (self.RETHROW, None)\n        elif received_responses >= required_responses and (not data_retrieved):\n            return (self.RETRY, consistency)\n        else:\n            return (self.RETHROW, None)\n\n    def on_write_timeout(self, query, consistency, write_type, required_responses, received_responses, retry_num):\n        \"\"\"\n        This is called when a write operation times out from the coordinator's\n        perspective (i.e. a replica did not respond to the coordinator in time).\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `write_type` is one of the :class:`.WriteType` enums describing the\n        type of write operation.\n\n        The `required_responses` and `received_responses` parameters describe\n        how many replicas needed to acknowledge the write to meet the requested\n        consistency level and how many replicas actually did acknowledge the\n        write before the coordinator timed out the request.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, failed write operations will retried at most once, and\n        they will only be retried if the `write_type` was\n        :attr:`~.WriteType.BATCH_LOG`.\n        \"\"\"\n        if retry_num != 0:\n            return (self.RETHROW, None)\n        elif write_type == WriteType.BATCH_LOG:\n            return (self.RETRY, consistency)\n        else:\n            return (self.RETHROW, None)\n\n    def on_unavailable(self, query, consistency, required_replicas, alive_replicas, retry_num):\n        \"\"\"\n        This is called when the coordinator node determines that a read or\n        write operation cannot be successful because the number of live\n        replicas are too low to meet the requested :class:`.ConsistencyLevel`.\n        This means that the read or write operation was never forwarded to\n        any replicas.\n\n        `query` is the :class:`.Statement` that failed.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `required_replicas` is the number of replicas that would have needed to\n        acknowledge the operation to meet the requested consistency level.\n        `alive_replicas` is the number of replicas that the coordinator\n        considered alive at the time of the request.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, if this is the first retry, it triggers a retry on the next\n        host in the query plan with the same consistency level. If this is not the\n        first retry, no retries will be attempted and the error will be re-raised.\n        \"\"\"\n        return (self.RETRY_NEXT_HOST, None) if retry_num == 0 else (self.RETHROW, None)\n\n    def on_request_error(self, query, consistency, error, retry_num):\n        \"\"\"\n        This is called when an unexpected error happens. This can be in the\n        following situations:\n\n        * On a connection error\n        * On server errors: overloaded, isBootstrapping, serverError, etc.\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `error` the instance of the exception.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, it triggers a retry on the next host in the query plan\n        with the same consistency level.\n        \"\"\"\n        return (self.RETRY_NEXT_HOST, None)\n\nclass FallthroughRetryPolicy(RetryPolicy):\n    \"\"\"\n    A retry policy that never retries and always propagates failures to\n    the application.\n    \"\"\"\n\n    def on_read_timeout(self, *args, **kwargs):\n        return (self.RETHROW, None)\n\n    def on_write_timeout(self, *args, **kwargs):\n        return (self.RETHROW, None)\n\n    def on_unavailable(self, *args, **kwargs):\n        return (self.RETHROW, None)\n\n    def on_request_error(self, *args, **kwargs):\n        return (self.RETHROW, None)\n\nclass DowngradingConsistencyRetryPolicy(RetryPolicy):\n    \"\"\"\n    *Deprecated:* This retry policy will be removed in the next major release.\n\n    A retry policy that sometimes retries with a lower consistency level than\n    the one initially requested.\n\n    **BEWARE**: This policy may retry queries using a lower consistency\n    level than the one initially requested. By doing so, it may break\n    consistency guarantees. In other words, if you use this retry policy,\n    there are cases (documented below) where a read at :attr:`~.QUORUM`\n    *may not* see a preceding write at :attr:`~.QUORUM`. Do not use this\n    policy unless you have understood the cases where this can happen and\n    are ok with that. It is also recommended to subclass this class so\n    that queries that required a consistency level downgrade can be\n    recorded (so that repairs can be made later, etc).\n\n    This policy implements the same retries as :class:`.RetryPolicy`,\n    but on top of that, it also retries in the following cases:\n\n    * On a read timeout: if the number of replicas that responded is\n      greater than one but lower than is required by the requested\n      consistency level, the operation is retried at a lower consistency\n      level.\n    * On a write timeout: if the operation is an :attr:`~.UNLOGGED_BATCH`\n      and at least one replica acknowledged the write, the operation is\n      retried at a lower consistency level.  Furthermore, for other\n      write types, if at least one replica acknowledged the write, the\n      timeout is ignored.\n    * On an unavailable exception: if at least one replica is alive, the\n      operation is retried at a lower consistency level.\n\n    The reasoning behind this retry policy is as follows: if, based\n    on the information the Cassandra coordinator node returns, retrying the\n    operation with the initially requested consistency has a chance to\n    succeed, do it. Otherwise, if based on that information we know the\n    initially requested consistency level cannot be achieved currently, then:\n\n    * For writes, ignore the exception (thus silently failing the\n      consistency requirement) if we know the write has been persisted on at\n      least one replica.\n    * For reads, try reading at a lower consistency level (thus silently\n      failing the consistency requirement).\n\n    In other words, this policy implements the idea that if the requested\n    consistency level cannot be achieved, the next best thing for writes is\n    to make sure the data is persisted, and that reading something is better\n    than reading nothing, even if there is a risk of reading stale data.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(DowngradingConsistencyRetryPolicy, self).__init__(*args, **kwargs)\n        warnings.warn('DowngradingConsistencyRetryPolicy is deprecated and will be removed in the next major release.', DeprecationWarning)\n\n    def _pick_consistency(self, num_responses):\n        if num_responses >= 3:\n            return (self.RETRY, ConsistencyLevel.THREE)\n        elif num_responses >= 2:\n            return (self.RETRY, ConsistencyLevel.TWO)\n        elif num_responses >= 1:\n            return (self.RETRY, ConsistencyLevel.ONE)\n        else:\n            return (self.RETHROW, None)\n\n    def on_read_timeout(self, query, consistency, required_responses, received_responses, data_retrieved, retry_num):\n        if retry_num != 0:\n            return (self.RETHROW, None)\n        elif ConsistencyLevel.is_serial(consistency):\n            return (self.RETHROW, None)\n        elif received_responses < required_responses:\n            return self._pick_consistency(received_responses)\n        elif not data_retrieved:\n            return (self.RETRY, consistency)\n        else:\n            return (self.RETHROW, None)\n\n    def on_write_timeout(self, query, consistency, write_type, required_responses, received_responses, retry_num):\n        if retry_num != 0:\n            return (self.RETHROW, None)\n        if write_type in (WriteType.SIMPLE, WriteType.BATCH, WriteType.COUNTER):\n            if received_responses > 0:\n                return (self.IGNORE, None)\n            else:\n                return (self.RETHROW, None)\n        elif write_type == WriteType.UNLOGGED_BATCH:\n            return self._pick_consistency(received_responses)\n        elif write_type == WriteType.BATCH_LOG:\n            return (self.RETRY, consistency)\n        return (self.RETHROW, None)\n\n    def on_unavailable(self, query, consistency, required_replicas, alive_replicas, retry_num):\n        if retry_num != 0:\n            return (self.RETHROW, None)\n        elif ConsistencyLevel.is_serial(consistency):\n            return (self.RETRY_NEXT_HOST, None)\n        else:\n            return self._pick_consistency(alive_replicas)\n\nclass ExponentialBackoffRetryPolicy(RetryPolicy):\n    \"\"\"\n    A policy that do retries with exponential backoff\n    \"\"\"\n\n    def __init__(self, max_num_retries: float, min_interval: float=0.1, max_interval: float=10.0, *args, **kwargs):\n        \"\"\"\n        `max_num_retries` counts how many times the operation would be retried,\n        `min_interval` is the initial time in seconds to wait before first retry\n        `max_interval` is the maximum time to wait between retries\n        \"\"\"\n        self.min_interval = min_interval\n        self.max_num_retries = max_num_retries\n        self.max_interval = max_interval\n        super(ExponentialBackoffRetryPolicy).__init__(*args, **kwargs)\n\n    def _calculate_backoff(self, attempt: int):\n        delay = min(self.max_interval, self.min_interval * 2 ** attempt)\n        delay += random.random() * self.min_interval - self.min_interval / 2\n        return delay\n\n    def on_read_timeout(self, query, consistency, required_responses, received_responses, data_retrieved, retry_num):\n        if retry_num < self.max_num_retries and received_responses >= required_responses and (not data_retrieved):\n            return (self.RETRY, consistency, self._calculate_backoff(retry_num))\n        else:\n            return (self.RETHROW, None, None)\n\n    def on_write_timeout(self, query, consistency, write_type, required_responses, received_responses, retry_num):\n        if retry_num < self.max_num_retries and write_type == WriteType.BATCH_LOG:\n            return (self.RETRY, consistency, self._calculate_backoff(retry_num))\n        else:\n            return (self.RETHROW, None, None)\n\n    def on_unavailable(self, query, consistency, required_replicas, alive_replicas, retry_num):\n        if retry_num < self.max_num_retries:\n            return (self.RETRY_NEXT_HOST, None, self._calculate_backoff(retry_num))\n        else:\n            return (self.RETHROW, None, None)\n\n    def on_request_error(self, query, consistency, error, retry_num):\n        if retry_num < self.max_num_retries:\n            return (self.RETRY_NEXT_HOST, None, self._calculate_backoff(retry_num))\n        else:\n            return (self.RETHROW, None, None)\n\nclass AddressTranslator(object):\n    \"\"\"\n    Interface for translating cluster-defined endpoints.\n\n    The driver discovers nodes using server metadata and topology change events. Normally,\n    the endpoint defined by the server is the right way to connect to a node. In some environments,\n    these addresses may not be reachable, or not preferred (public vs. private IPs in cloud environments,\n    suboptimal routing, etc). This interface allows for translating from server defined endpoints to\n    preferred addresses for driver connections.\n\n    *Note:* :attr:`~Cluster.contact_points` provided while creating the :class:`~.Cluster` instance are not\n    translated using this mechanism -- only addresses received from Cassandra nodes are.\n    \"\"\"\n\n    def translate(self, addr):\n        \"\"\"\n        Accepts the node ip address, and returns a translated address to be used connecting to this node.\n        \"\"\"\n        raise NotImplementedError()\n\nclass IdentityTranslator(AddressTranslator):\n    \"\"\"\n    Returns the endpoint with no translation\n    \"\"\"\n\n    def translate(self, addr):\n        return addr\n\nclass EC2MultiRegionTranslator(AddressTranslator):\n    \"\"\"\n    Resolves private ips of the hosts in the same datacenter as the client, and public ips of hosts in other datacenters.\n    \"\"\"\n\n    def translate(self, addr):\n        \"\"\"\n        Reverse DNS the public broadcast_address, then lookup that hostname to get the AWS-resolved IP, which\n        will point to the private IP address within the same datacenter.\n        \"\"\"\n        family = socket.getaddrinfo(addr, 0, socket.AF_UNSPEC, socket.SOCK_STREAM)[0][0]\n        host = socket.getfqdn(addr)\n        for a in socket.getaddrinfo(host, 0, family, socket.SOCK_STREAM):\n            try:\n                return a[4][0]\n            except Exception:\n                pass\n        return addr\n\nclass SpeculativeExecutionPolicy(object):\n    \"\"\"\n    Interface for specifying speculative execution plans\n    \"\"\"\n\n    def new_plan(self, keyspace, statement):\n        \"\"\"\n        Returns\n\n        :param keyspace:\n        :param statement:\n        :return:\n        \"\"\"\n        raise NotImplementedError()\n\nclass SpeculativeExecutionPlan(object):\n\n    def next_execution(self, host):\n        raise NotImplementedError()\n\nclass NoSpeculativeExecutionPlan(SpeculativeExecutionPlan):\n\n    def next_execution(self, host):\n        return -1\n\nclass NoSpeculativeExecutionPolicy(SpeculativeExecutionPolicy):\n\n    def new_plan(self, keyspace, statement):\n        return NoSpeculativeExecutionPlan()\n\nclass ConstantSpeculativeExecutionPolicy(SpeculativeExecutionPolicy):\n    \"\"\"\n    A speculative execution policy that sends a new query every X seconds (**delay**) for a maximum of Y attempts (**max_attempts**).\n    \"\"\"\n\n    def __init__(self, delay, max_attempts):\n        self.delay = delay\n        self.max_attempts = max_attempts\n\n    class ConstantSpeculativeExecutionPlan(SpeculativeExecutionPlan):\n\n        def __init__(self, delay, max_attempts):\n            self.delay = delay\n            self.remaining = max_attempts\n\n        def next_execution(self, host):\n            if self.remaining > 0:\n                self.remaining -= 1\n                return self.delay\n            else:\n                return -1\n\n    def new_plan(self, keyspace, statement):\n        return self.ConstantSpeculativeExecutionPlan(self.delay, self.max_attempts)\n\nclass WrapperPolicy(LoadBalancingPolicy):\n\n    def __init__(self, child_policy):\n        self._child_policy = child_policy\n\n    def distance(self, *args, **kwargs):\n        return self._child_policy.distance(*args, **kwargs)\n\n    def populate(self, cluster, hosts):\n        self._child_policy.populate(cluster, hosts)\n\n    def on_up(self, *args, **kwargs):\n        return self._child_policy.on_up(*args, **kwargs)\n\n    def on_down(self, *args, **kwargs):\n        return self._child_policy.on_down(*args, **kwargs)\n\n    def on_add(self, *args, **kwargs):\n        return self._child_policy.on_add(*args, **kwargs)\n\n    def on_remove(self, *args, **kwargs):\n        return self._child_policy.on_remove(*args, **kwargs)\n\nclass DefaultLoadBalancingPolicy(WrapperPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` wrapper that adds the ability to target a specific host first.\n\n    If no host is set on the query, the child policy's query plan will be used as is.\n    \"\"\"\n    _cluster_metadata = None\n\n    def populate(self, cluster, hosts):\n        self._cluster_metadata = cluster.metadata\n        self._child_policy.populate(cluster, hosts)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        if query and query.keyspace:\n            keyspace = query.keyspace\n        else:\n            keyspace = working_keyspace\n        addr = getattr(query, 'target_host', None) if query else None\n        target_host = self._cluster_metadata.get_host(addr)\n        child = self._child_policy\n        if target_host and target_host.is_up:\n            yield target_host\n            for h in child.make_query_plan(keyspace, query):\n                if h != target_host:\n                    yield h\n        else:\n            for h in child.make_query_plan(keyspace, query):\n                yield h\n\nclass DSELoadBalancingPolicy(DefaultLoadBalancingPolicy):\n    \"\"\"\n    *Deprecated:* This will be removed in the next major release,\n    consider using :class:`.DefaultLoadBalancingPolicy`.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(DSELoadBalancingPolicy, self).__init__(*args, **kwargs)\n        warnings.warn('DSELoadBalancingPolicy will be removed in 4.0. Consider using DefaultLoadBalancingPolicy.', DeprecationWarning)\n\nclass NeverRetryPolicy(RetryPolicy):\n\n    def _rethrow(self, *args, **kwargs):\n        return (self.RETHROW, None)\n    on_read_timeout = _rethrow\n    on_write_timeout = _rethrow\n    on_unavailable = _rethrow\nColDesc = namedtuple('ColDesc', ['ks', 'table', 'col'])\n\nclass ColumnEncryptionPolicy(object):\n    \"\"\"\n    A policy enabling (mostly) transparent encryption and decryption of data before it is\n    sent to the cluster.\n\n    Key materials and other configurations are specified on a per-column basis.  This policy can\n    then be used by driver structures which are aware of the underlying columns involved in their\n    work.  In practice this includes the following cases:\n\n    * Prepared statements - data for columns specified by the cluster's policy will be transparently\n      encrypted before they are sent\n    * Rows returned from any query - data for columns specified by the cluster's policy will be\n      transparently decrypted before they are returned to the user\n\n    To enable this functionality, create an instance of this class (or more likely a subclass)\n    before creating a cluster.  This policy should then be configured and supplied to the Cluster\n    at creation time via the :attr:`.Cluster.column_encryption_policy` attribute.\n    \"\"\"\n\n    def encrypt(self, coldesc, obj_bytes):\n        \"\"\"\n        Encrypt the specified bytes using the cryptography materials for the specified column.\n        Largely used internally, although this could also be used to encrypt values supplied\n        to non-prepared statements in a way that is consistent with this policy.\n        \"\"\"\n        raise NotImplementedError()\n\n    def decrypt(self, coldesc, encrypted_bytes):\n        \"\"\"\n        Decrypt the specified (encrypted) bytes using the cryptography materials for the\n        specified column.  Used internally; could be used externally as well but there's\n        not currently an obvious use case.\n        \"\"\"\n        raise NotImplementedError()\n\n    def add_column(self, coldesc, key):\n        \"\"\"\n        Provide cryptography materials to be used when encrypted and/or decrypting data\n        for the specified column.\n        \"\"\"\n        raise NotImplementedError()\n\n    def contains_column(self, coldesc):\n        \"\"\"\n        Predicate to determine if a specific column is supported by this policy.\n        Currently only used internally.\n        \"\"\"\n        raise NotImplementedError()\n\n    def encode_and_encrypt(self, coldesc, obj):\n        \"\"\"\n        Helper function to enable use of this policy on simple (i.e. non-prepared)\n        statements.\n        \"\"\"\n        raise NotImplementedError()",
    "cassandra/cqlengine/columns.py": "from copy import deepcopy, copy\nfrom datetime import date, datetime, timedelta\nimport logging\nfrom uuid import UUID as _UUID\nfrom cassandra import util\nfrom cassandra.cqltypes import SimpleDateType, _cqltypes, UserType\nfrom cassandra.cqlengine import ValidationError\nfrom cassandra.cqlengine.functions import get_total_seconds\nfrom cassandra.util import Duration as _Duration\nlog = logging.getLogger(__name__)\n\nclass BaseValueManager(object):\n\n    def __init__(self, instance, column, value):\n        self.instance = instance\n        self.column = column\n        self.value = value\n        self.previous_value = None\n        self.explicit = False\n\n    @property\n    def deleted(self):\n        return self.column._val_is_null(self.value) and (self.explicit or not self.column._val_is_null(self.previous_value))\n\n    @property\n    def changed(self):\n        \"\"\"\n        Indicates whether or not this value has changed.\n\n        :rtype: boolean\n\n        \"\"\"\n        if self.explicit:\n            return self.value != self.previous_value\n        if isinstance(self.column, BaseContainerColumn):\n            default_value = self.column.get_default()\n            if self.column._val_is_null(default_value):\n                return not self.column._val_is_null(self.value) and self.value != self.previous_value\n            elif self.previous_value is None:\n                return self.value != default_value\n            return self.value != self.previous_value\n        return False\n\n    def reset_previous_value(self):\n        self.previous_value = deepcopy(self.value)\n\n    def getval(self):\n        return self.value\n\n    def setval(self, val):\n        self.value = val\n        self.explicit = True\n\n    def delval(self):\n        self.value = None\n\n    def get_property(self):\n        _get = lambda slf: self.getval()\n        _set = lambda slf, val: self.setval(val)\n        _del = lambda slf: self.delval()\n        if self.column.can_delete:\n            return property(_get, _set, _del)\n        else:\n            return property(_get, _set)\n\nclass Column(object):\n    db_type = None\n    value_manager = BaseValueManager\n    instance_counter = 0\n    _python_type_hashable = True\n    primary_key = False\n    '\\n    bool flag, indicates this column is a primary key. The first primary key defined\\n    on a model is the partition key (unless partition keys are set), all others are cluster keys\\n    '\n    partition_key = False\n    '\\n    indicates that this column should be the partition key, defining\\n    more than one partition key column creates a compound partition key\\n    '\n    index = False\n    '\\n    bool flag, indicates an index should be created for this column\\n    '\n    custom_index = False\n    '\\n    bool flag, indicates an index is managed outside of cqlengine. This is\\n    useful if you want to do filter queries on fields that have custom\\n    indexes.\\n    '\n    db_field = None\n    '\\n    the fieldname this field will map to in the database\\n    '\n    default = None\n    '\\n    the default value, can be a value or a callable (no args)\\n    '\n    required = False\n    '\\n    boolean, is the field required? Model validation will raise and\\n    exception if required is set to True and there is a None value assigned\\n    '\n    clustering_order = None\n    '\\n    only applicable on clustering keys (primary keys that are not partition keys)\\n    determines the order that the clustering keys are sorted on disk\\n    '\n    discriminator_column = False\n    '\\n    boolean, if set to True, this column will be used for discriminating records\\n    of inherited models.\\n\\n    Should only be set on a column of an abstract model being used for inheritance.\\n\\n    There may only be one discriminator column per model. See :attr:`~.__discriminator_value__`\\n    for how to specify the value of this column on specialized models.\\n    '\n    static = False\n    '\\n    boolean, if set to True, this is a static column, with a single value per partition\\n    '\n\n    def __ne__(self, other):\n        if isinstance(other, Column):\n            return self.position != other.position\n        return NotImplemented\n\n    def __eq__(self, other):\n        if isinstance(other, Column):\n            return self.position == other.position\n        return NotImplemented\n\n    def __lt__(self, other):\n        if isinstance(other, Column):\n            return self.position < other.position\n        return NotImplemented\n\n    def __le__(self, other):\n        if isinstance(other, Column):\n            return self.position <= other.position\n        return NotImplemented\n\n    def __gt__(self, other):\n        if isinstance(other, Column):\n            return self.position > other.position\n        return NotImplemented\n\n    def __ge__(self, other):\n        if isinstance(other, Column):\n            return self.position >= other.position\n        return NotImplemented\n\n    def __hash__(self):\n        return id(self)\n\n    def validate(self, value):\n        \"\"\"\n        Returns a cleaned and validated value. Raises a ValidationError\n        if there's a problem\n        \"\"\"\n        if value is None:\n            if self.required:\n                raise ValidationError('{0} - None values are not allowed'.format(self.column_name or self.db_field))\n        return value\n\n    def to_python(self, value):\n        \"\"\"\n        Converts data from the database into python values\n        raises a ValidationError if the value can't be converted\n        \"\"\"\n        return value\n\n    def to_database(self, value):\n        \"\"\"\n        Converts python value into database value\n        \"\"\"\n        return value\n\n    @property\n    def has_default(self):\n        return self.default is not None\n\n    @property\n    def is_primary_key(self):\n        return self.primary_key\n\n    @property\n    def can_delete(self):\n        return not self.primary_key\n\n    def get_default(self):\n        if self.has_default:\n            if callable(self.default):\n                return self.default()\n            else:\n                return self.default\n\n    def get_column_def(self):\n        \"\"\"\n        Returns a column definition for CQL table definition\n        \"\"\"\n        static = 'static' if self.static else ''\n        return '{0} {1} {2}'.format(self.cql, self.db_type, static)\n\n    def cql_parameterized_type(self):\n        return self.db_type\n\n    def set_column_name(self, name):\n        \"\"\"\n        Sets the column name during document class construction\n        This value will be ignored if db_field is set in __init__\n        \"\"\"\n        self.column_name = name\n\n    @property\n    def db_field_name(self):\n        \"\"\" Returns the name of the cql name of this column \"\"\"\n        return self.db_field if self.db_field is not None else self.column_name\n\n    @property\n    def db_index_name(self):\n        \"\"\" Returns the name of the cql index \"\"\"\n        return 'index_{0}'.format(self.db_field_name)\n\n    @property\n    def has_index(self):\n        return self.index or self.custom_index\n\n    @property\n    def cql(self):\n        return self.get_cql()\n\n    def get_cql(self):\n        return '\"{0}\"'.format(self.db_field_name)\n\n    def _val_is_null(self, val):\n        \"\"\" determines if the given value equates to a null value for the given column type \"\"\"\n        return val is None\n\n    @property\n    def sub_types(self):\n        return []\n\n    @property\n    def cql_type(self):\n        return _cqltypes[self.db_type]\n\nclass Blob(Column):\n    \"\"\"\n    Stores a raw binary value\n    \"\"\"\n    db_type = 'blob'\n\n    def to_database(self, value):\n        if not isinstance(value, (bytes, bytearray)):\n            raise Exception('expecting a binary, got a %s' % type(value))\n        val = super(Bytes, self).to_database(value)\n        return bytearray(val)\nBytes = Blob\n\nclass Inet(Column):\n    \"\"\"\n    Stores an IP address in IPv4 or IPv6 format\n    \"\"\"\n    db_type = 'inet'\n\nclass Text(Column):\n    \"\"\"\n    Stores a UTF-8 encoded string\n    \"\"\"\n    db_type = 'text'\n\n    def validate(self, value):\n        value = super(Text, self).validate(value)\n        if not isinstance(value, (str, bytearray)) and value is not None:\n            raise ValidationError('{0} {1} is not a string'.format(self.column_name, type(value)))\n        if self.max_length is not None:\n            if value and len(value) > self.max_length:\n                raise ValidationError('{0} is longer than {1} characters'.format(self.column_name, self.max_length))\n        if self.min_length:\n            if self.min_length and (not value) or len(value) < self.min_length:\n                raise ValidationError('{0} is shorter than {1} characters'.format(self.column_name, self.min_length))\n        return value\n\nclass Ascii(Text):\n    \"\"\"\n    Stores a US-ASCII character string\n    \"\"\"\n    db_type = 'ascii'\n\n    def validate(self, value):\n        \"\"\" Only allow ASCII and None values.\n\n        Check against US-ASCII, a.k.a. 7-bit ASCII, a.k.a. ISO646-US, a.k.a.\n        the Basic Latin block of the Unicode character set.\n\n        Source: https://github.com/apache/cassandra/blob\n        /3dcbe90e02440e6ee534f643c7603d50ca08482b/src/java/org/apache/cassandra\n        /serializers/AsciiSerializer.java#L29\n        \"\"\"\n        value = super(Ascii, self).validate(value)\n        if value:\n            charset = value if isinstance(value, (bytearray,)) else map(ord, value)\n            if not set(range(128)).issuperset(charset):\n                raise ValidationError('{!r} is not an ASCII string.'.format(value))\n        return value\n\nclass Integer(Column):\n    \"\"\"\n    Stores a 32-bit signed integer value\n    \"\"\"\n    db_type = 'int'\n\n    def validate(self, value):\n        val = super(Integer, self).validate(value)\n        if val is None:\n            return\n        try:\n            return int(val)\n        except (TypeError, ValueError):\n            raise ValidationError(\"{0} {1} can't be converted to integral value\".format(self.column_name, value))\n\n    def to_python(self, value):\n        return self.validate(value)\n\n    def to_database(self, value):\n        return self.validate(value)\n\nclass TinyInt(Integer):\n    \"\"\"\n    Stores an 8-bit signed integer value\n\n    .. versionadded:: 2.6.0\n\n    requires C* 2.2+ and protocol v4+\n    \"\"\"\n    db_type = 'tinyint'\n\nclass SmallInt(Integer):\n    \"\"\"\n    Stores a 16-bit signed integer value\n\n    .. versionadded:: 2.6.0\n\n    requires C* 2.2+ and protocol v4+\n    \"\"\"\n    db_type = 'smallint'\n\nclass BigInt(Integer):\n    \"\"\"\n    Stores a 64-bit signed integer value\n    \"\"\"\n    db_type = 'bigint'\n\nclass VarInt(Column):\n    \"\"\"\n    Stores an arbitrary-precision integer\n    \"\"\"\n    db_type = 'varint'\n\n    def validate(self, value):\n        val = super(VarInt, self).validate(value)\n        if val is None:\n            return\n        try:\n            return int(val)\n        except (TypeError, ValueError):\n            raise ValidationError(\"{0} {1} can't be converted to integral value\".format(self.column_name, value))\n\n    def to_python(self, value):\n        return self.validate(value)\n\n    def to_database(self, value):\n        return self.validate(value)\n\nclass CounterValueManager(BaseValueManager):\n\n    def __init__(self, instance, column, value):\n        super(CounterValueManager, self).__init__(instance, column, value)\n        self.value = self.value or 0\n        self.previous_value = self.previous_value or 0\n\nclass Counter(Integer):\n    \"\"\"\n    Stores a counter that can be incremented and decremented\n    \"\"\"\n    db_type = 'counter'\n    value_manager = CounterValueManager\n\n    def __init__(self, index=False, db_field=None, required=False):\n        super(Counter, self).__init__(primary_key=False, partition_key=False, index=index, db_field=db_field, default=0, required=required)\n\nclass DateTime(Column):\n    \"\"\"\n    Stores a datetime value\n    \"\"\"\n    db_type = 'timestamp'\n    truncate_microseconds = False\n    '\\n    Set this ``True`` to have model instances truncate the date, quantizing it in the same way it will be in the database.\\n    This allows equality comparison between assigned values and values read back from the database::\\n\\n        DateTime.truncate_microseconds = True\\n        assert Model.create(id=0, d=datetime.utcnow()) == Model.objects(id=0).first()\\n\\n    Defaults to ``False`` to preserve legacy behavior. May change in the future.\\n    '\n\n    def to_python(self, value):\n        if value is None:\n            return\n        if isinstance(value, datetime):\n            if DateTime.truncate_microseconds:\n                us = value.microsecond\n                truncated_us = us // 1000 * 1000\n                return value - timedelta(microseconds=us - truncated_us)\n            else:\n                return value\n        elif isinstance(value, date):\n            return datetime(*value.timetuple()[:6])\n        return datetime.utcfromtimestamp(value)\n\n    def to_database(self, value):\n        value = super(DateTime, self).to_database(value)\n        if value is None:\n            return\n        if not isinstance(value, datetime):\n            if isinstance(value, date):\n                value = datetime(value.year, value.month, value.day)\n            else:\n                raise ValidationError(\"{0} '{1}' is not a datetime object\".format(self.column_name, value))\n        epoch = datetime(1970, 1, 1, tzinfo=value.tzinfo)\n        offset = get_total_seconds(epoch.tzinfo.utcoffset(epoch)) if epoch.tzinfo else 0\n        return int((get_total_seconds(value - epoch) - offset) * 1000)\n\nclass Date(Column):\n    \"\"\"\n    Stores a simple date, with no time-of-day\n\n    .. versionchanged:: 2.6.0\n\n        removed overload of Date and DateTime. DateTime is a drop-in replacement for legacy models\n\n    requires C* 2.2+ and protocol v4+\n    \"\"\"\n    db_type = 'date'\n\n    def to_database(self, value):\n        if value is None:\n            return\n        d = value if isinstance(value, util.Date) else util.Date(value)\n        return d.days_from_epoch + SimpleDateType.EPOCH_OFFSET_DAYS\n\n    def to_python(self, value):\n        if value is None:\n            return\n        if isinstance(value, util.Date):\n            return value\n        if isinstance(value, datetime):\n            value = value.date()\n        return util.Date(value)\n\nclass Time(Column):\n    \"\"\"\n    Stores a timezone-naive time-of-day, with nanosecond precision\n\n    .. versionadded:: 2.6.0\n\n    requires C* 2.2+ and protocol v4+\n    \"\"\"\n    db_type = 'time'\n\n    def to_database(self, value):\n        value = super(Time, self).to_database(value)\n        if value is None:\n            return\n        return value if isinstance(value, util.Time) else util.Time(value)\n\n    def to_python(self, value):\n        value = super(Time, self).to_database(value)\n        if value is None:\n            return\n        if isinstance(value, util.Time):\n            return value\n        return util.Time(value)\n\nclass Duration(Column):\n    \"\"\"\n    Stores a duration (months, days, nanoseconds)\n\n    .. versionadded:: 3.10.0\n\n    requires C* 3.10+ and protocol v4+\n    \"\"\"\n    db_type = 'duration'\n\n    def validate(self, value):\n        val = super(Duration, self).validate(value)\n        if val is None:\n            return\n        if not isinstance(val, _Duration):\n            raise TypeError('{0} {1} is not a valid Duration.'.format(self.column_name, value))\n        return val\n\nclass UUID(Column):\n    \"\"\"\n    Stores a type 1 or 4 UUID\n    \"\"\"\n    db_type = 'uuid'\n\n    def validate(self, value):\n        val = super(UUID, self).validate(value)\n        if val is None:\n            return\n        if isinstance(val, _UUID):\n            return val\n        if isinstance(val, str):\n            try:\n                return _UUID(val)\n            except ValueError:\n                pass\n        raise ValidationError('{0} {1} is not a valid uuid'.format(self.column_name, value))\n\n    def to_python(self, value):\n        return self.validate(value)\n\n    def to_database(self, value):\n        return self.validate(value)\n\nclass TimeUUID(UUID):\n    \"\"\"\n    UUID containing timestamp\n    \"\"\"\n    db_type = 'timeuuid'\n\nclass Boolean(Column):\n    \"\"\"\n    Stores a boolean True or False value\n    \"\"\"\n    db_type = 'boolean'\n\n    def validate(self, value):\n        \"\"\" Always returns a Python boolean. \"\"\"\n        value = super(Boolean, self).validate(value)\n        if value is not None:\n            value = bool(value)\n        return value\n\n    def to_python(self, value):\n        return self.validate(value)\n\nclass BaseFloat(Column):\n\n    def validate(self, value):\n        value = super(BaseFloat, self).validate(value)\n        if value is None:\n            return\n        try:\n            return float(value)\n        except (TypeError, ValueError):\n            raise ValidationError('{0} {1} is not a valid float'.format(self.column_name, value))\n\n    def to_python(self, value):\n        return self.validate(value)\n\n    def to_database(self, value):\n        return self.validate(value)\n\nclass Float(BaseFloat):\n    \"\"\"\n    Stores a single-precision floating-point value\n    \"\"\"\n    db_type = 'float'\n\nclass Double(BaseFloat):\n    \"\"\"\n    Stores a double-precision floating-point value\n    \"\"\"\n    db_type = 'double'\n\nclass Decimal(Column):\n    \"\"\"\n    Stores a variable precision decimal value\n    \"\"\"\n    db_type = 'decimal'\n\n    def validate(self, value):\n        from decimal import Decimal as _Decimal\n        from decimal import InvalidOperation\n        val = super(Decimal, self).validate(value)\n        if val is None:\n            return\n        try:\n            return _Decimal(repr(val)) if isinstance(val, float) else _Decimal(val)\n        except InvalidOperation:\n            raise ValidationError(\"{0} '{1}' can't be coerced to decimal\".format(self.column_name, val))\n\n    def to_python(self, value):\n        return self.validate(value)\n\n    def to_database(self, value):\n        return self.validate(value)\n\nclass BaseCollectionColumn(Column):\n    \"\"\"\n    Base Container type for collection-like columns.\n\n    http://cassandra.apache.org/doc/cql3/CQL-3.0.html#collections\n    \"\"\"\n\n    def __init__(self, types, **kwargs):\n        \"\"\"\n        :param types: a sequence of sub types in this collection\n        \"\"\"\n        instances = []\n        for t in types:\n            inheritance_comparator = issubclass if isinstance(t, type) else isinstance\n            if not inheritance_comparator(t, Column):\n                raise ValidationError('%s is not a column class' % (t,))\n            if t.db_type is None:\n                raise ValidationError('%s is an abstract type' % (t,))\n            inst = t() if isinstance(t, type) else t\n            if isinstance(t, BaseCollectionColumn):\n                inst._freeze_db_type()\n            instances.append(inst)\n        self.types = instances\n        super(BaseCollectionColumn, self).__init__(**kwargs)\n\n    def validate(self, value):\n        value = super(BaseCollectionColumn, self).validate(value)\n        if value is not None and len(value) > 65535:\n            raise ValidationError(\"{0} Collection can't have more than 65535 elements.\".format(self.column_name))\n        return value\n\n    def _val_is_null(self, val):\n        return not val\n\n    def _freeze_db_type(self):\n        if not self.db_type.startswith('frozen'):\n            self.db_type = 'frozen<%s>' % (self.db_type,)\n\n    @property\n    def sub_types(self):\n        return self.types\n\n    @property\n    def cql_type(self):\n        return _cqltypes[self.__class__.__name__.lower()].apply_parameters([c.cql_type for c in self.types])\n\nclass Tuple(BaseCollectionColumn):\n    \"\"\"\n    Stores a fixed-length set of positional values\n\n    http://docs.datastax.com/en/cql/3.1/cql/cql_reference/tupleType.html\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        :param args: column types representing tuple composition\n        \"\"\"\n        if not args:\n            raise ValueError('Tuple must specify at least one inner type')\n        super(Tuple, self).__init__(args, **kwargs)\n        self.db_type = 'tuple<{0}>'.format(', '.join((typ.db_type for typ in self.types)))\n\n    def validate(self, value):\n        val = super(Tuple, self).validate(value)\n        if val is None:\n            return\n        if len(val) > len(self.types):\n            raise ValidationError('Value %r has more fields than tuple definition (%s)' % (val, ', '.join((t for t in self.types))))\n        return tuple((t.validate(v) for t, v in zip(self.types, val)))\n\n    def to_python(self, value):\n        if value is None:\n            return tuple()\n        return tuple((t.to_python(v) for t, v in zip(self.types, value)))\n\n    def to_database(self, value):\n        if value is None:\n            return\n        return tuple((t.to_database(v) for t, v in zip(self.types, value)))\n\nclass BaseContainerColumn(BaseCollectionColumn):\n    pass\n\nclass Set(BaseContainerColumn):\n    \"\"\"\n    Stores a set of unordered, unique values\n\n    http://www.datastax.com/documentation/cql/3.1/cql/cql_using/use_set_t.html\n    \"\"\"\n    _python_type_hashable = False\n\n    def __init__(self, value_type, strict=True, default=set, **kwargs):\n        \"\"\"\n        :param value_type: a column class indicating the types of the value\n        :param strict: sets whether non set values will be coerced to set\n            type on validation, or raise a validation error, defaults to True\n        \"\"\"\n        self.strict = strict\n        super(Set, self).__init__((value_type,), default=default, **kwargs)\n        self.value_col = self.types[0]\n        if not self.value_col._python_type_hashable:\n            raise ValidationError('Cannot create a Set with unhashable value type (see PYTHON-494)')\n        self.db_type = 'set<{0}>'.format(self.value_col.db_type)\n\n    def validate(self, value):\n        val = super(Set, self).validate(value)\n        if val is None:\n            return\n        types = (set, util.SortedSet) if self.strict else (set, util.SortedSet, list, tuple)\n        if not isinstance(val, types):\n            if self.strict:\n                raise ValidationError('{0} {1} is not a set object'.format(self.column_name, val))\n            else:\n                raise ValidationError('{0} {1} cannot be coerced to a set object'.format(self.column_name, val))\n        if None in val:\n            raise ValidationError('{0} None not allowed in a set'.format(self.column_name))\n        return set((self.value_col.validate(v) for v in val))\n\n    def to_python(self, value):\n        if value is None:\n            return set()\n        return set((self.value_col.to_python(v) for v in value))\n\n    def to_database(self, value):\n        if value is None:\n            return None\n        return set((self.value_col.to_database(v) for v in value))\n\nclass List(BaseContainerColumn):\n    \"\"\"\n    Stores a list of ordered values\n\n    http://www.datastax.com/documentation/cql/3.1/cql/cql_using/use_list_t.html\n    \"\"\"\n    _python_type_hashable = False\n\n    def __init__(self, value_type, default=list, **kwargs):\n        \"\"\"\n        :param value_type: a column class indicating the types of the value\n        \"\"\"\n        super(List, self).__init__((value_type,), default=default, **kwargs)\n        self.value_col = self.types[0]\n        self.db_type = 'list<{0}>'.format(self.value_col.db_type)\n\n    def validate(self, value):\n        val = super(List, self).validate(value)\n        if val is None:\n            return\n        if not isinstance(val, (set, list, tuple)):\n            raise ValidationError('{0} {1} is not a list object'.format(self.column_name, val))\n        if None in val:\n            raise ValidationError('{0} None is not allowed in a list'.format(self.column_name))\n        return [self.value_col.validate(v) for v in val]\n\n    def to_python(self, value):\n        if value is None:\n            return []\n        return [self.value_col.to_python(v) for v in value]\n\n    def to_database(self, value):\n        if value is None:\n            return None\n        return [self.value_col.to_database(v) for v in value]\n\nclass Map(BaseContainerColumn):\n    \"\"\"\n    Stores a key -> value map (dictionary)\n\n    https://docs.datastax.com/en/dse/6.7/cql/cql/cql_using/useMap.html\n    \"\"\"\n    _python_type_hashable = False\n\n    def __init__(self, key_type, value_type, default=dict, **kwargs):\n        \"\"\"\n        :param key_type: a column class indicating the types of the key\n        :param value_type: a column class indicating the types of the value\n        \"\"\"\n        super(Map, self).__init__((key_type, value_type), default=default, **kwargs)\n        self.key_col = self.types[0]\n        self.value_col = self.types[1]\n        if not self.key_col._python_type_hashable:\n            raise ValidationError('Cannot create a Map with unhashable key type (see PYTHON-494)')\n        self.db_type = 'map<{0}, {1}>'.format(self.key_col.db_type, self.value_col.db_type)\n\n    def validate(self, value):\n        val = super(Map, self).validate(value)\n        if val is None:\n            return\n        if not isinstance(val, (dict, util.OrderedMap)):\n            raise ValidationError('{0} {1} is not a dict object'.format(self.column_name, val))\n        if None in val:\n            raise ValidationError('{0} None is not allowed in a map'.format(self.column_name))\n        return dict(((self.key_col.validate(k), self.value_col.validate(v)) for k, v in val.items()))\n\n    def to_python(self, value):\n        if value is None:\n            return {}\n        if value is not None:\n            return dict(((self.key_col.to_python(k), self.value_col.to_python(v)) for k, v in value.items()))\n\n    def to_database(self, value):\n        if value is None:\n            return None\n        return dict(((self.key_col.to_database(k), self.value_col.to_database(v)) for k, v in value.items()))\n\nclass UDTValueManager(BaseValueManager):\n\n    @property\n    def changed(self):\n        if self.explicit:\n            return self.value != self.previous_value\n        default_value = self.column.get_default()\n        if not self.column._val_is_null(default_value):\n            return self.value != default_value\n        elif self.previous_value is None:\n            return not self.column._val_is_null(self.value) and self.value.has_changed_fields()\n        return False\n\n    def reset_previous_value(self):\n        if self.value is not None:\n            self.value.reset_changed_fields()\n        self.previous_value = copy(self.value)\n\nclass UserDefinedType(Column):\n    \"\"\"\n    User Defined Type column\n\n    http://www.datastax.com/documentation/cql/3.1/cql/cql_using/cqlUseUDT.html\n\n    These columns are represented by a specialization of :class:`cassandra.cqlengine.usertype.UserType`.\n\n    Please see :ref:`user_types` for examples and discussion.\n    \"\"\"\n    value_manager = UDTValueManager\n\n    @property\n    def sub_types(self):\n        return list(self.user_type._fields.values())\n\n    @property\n    def cql_type(self):\n        return UserType.make_udt_class(keyspace='', udt_name=self.user_type.type_name(), field_names=[c.db_field_name for c in self.user_type._fields.values()], field_types=[c.cql_type for c in self.user_type._fields.values()])\n\n    def validate(self, value):\n        val = super(UserDefinedType, self).validate(value)\n        if val is None:\n            return\n        val.validate()\n        return val\n\n    def to_python(self, value):\n        if value is None:\n            return\n        for name, field in self.user_type._fields.items():\n            if value[name] is not None or isinstance(field, BaseContainerColumn):\n                value[name] = field.to_python(value[name])\n        return value\n\n    def to_database(self, value):\n        if value is None:\n            return\n        copied_value = deepcopy(value)\n        for name, field in self.user_type._fields.items():\n            if copied_value[name] is not None or isinstance(field, BaseContainerColumn):\n                copied_value[name] = field.to_database(copied_value[name])\n        return copied_value\n\ndef resolve_udts(col_def, out_list):\n    for col in col_def.sub_types:\n        resolve_udts(col, out_list)\n    if isinstance(col_def, UserDefinedType):\n        out_list.append(col_def.user_type)\n\nclass _PartitionKeysToken(Column):\n    \"\"\"\n    virtual column representing token of partition columns.\n    Used by filter(pk__token=Token(...)) filters\n    \"\"\"\n\n    def __init__(self, model):\n        self.partition_columns = list(model._partition_keys.values())\n        super(_PartitionKeysToken, self).__init__(partition_key=True)\n\n    @property\n    def db_field_name(self):\n        return 'token({0})'.format(', '.join(['\"{0}\"'.format(c.db_field_name) for c in self.partition_columns]))",
    "cassandra/cqlengine/query.py": "import copy\nfrom datetime import datetime, timedelta\nfrom functools import partial\nimport time\nfrom warnings import warn\nfrom cassandra.query import SimpleStatement, BatchType as CBatchType, BatchStatement\nfrom cassandra.cqlengine import columns, CQLEngineException, ValidationError, UnicodeMixin\nfrom cassandra.cqlengine import connection as conn\nfrom cassandra.cqlengine.functions import Token, BaseQueryFunction, QueryValue\nfrom cassandra.cqlengine.operators import InOperator, EqualsOperator, GreaterThanOperator, GreaterThanOrEqualOperator, LessThanOperator, LessThanOrEqualOperator, ContainsOperator, BaseWhereOperator\nfrom cassandra.cqlengine.statements import WhereClause, SelectStatement, DeleteStatement, UpdateStatement, InsertStatement, BaseCQLStatement, MapDeleteClause, ConditionalClause\n\nclass QueryException(CQLEngineException):\n    pass\n\nclass IfNotExistsWithCounterColumn(CQLEngineException):\n    pass\n\nclass IfExistsWithCounterColumn(CQLEngineException):\n    pass\n\nclass LWTException(CQLEngineException):\n    \"\"\"Lightweight conditional exception.\n\n    This exception will be raised when a write using an `IF` clause could not be\n    applied due to existing data violating the condition. The existing data is\n    available through the ``existing`` attribute.\n\n    :param existing: The current state of the data which prevented the write.\n    \"\"\"\n\n    def __init__(self, existing):\n        super(LWTException, self).__init__('LWT Query was not applied')\n        self.existing = existing\n\nclass DoesNotExist(QueryException):\n    pass\n\nclass MultipleObjectsReturned(QueryException):\n    pass\n\ndef check_applied(result):\n    \"\"\"\n    Raises LWTException if it looks like a failed LWT request. A LWTException\n    won't be raised in the special case in which there are several failed LWT\n    in a  :class:`~cqlengine.query.BatchQuery`.\n    \"\"\"\n    try:\n        applied = result.was_applied\n    except Exception:\n        applied = True\n    if not applied:\n        raise LWTException(result.one())\n\nclass AbstractQueryableColumn(UnicodeMixin):\n    \"\"\"\n    exposes cql query operators through pythons\n    builtin comparator symbols\n    \"\"\"\n\n    def _get_column(self):\n        raise NotImplementedError\n\n    def __unicode__(self):\n        raise NotImplementedError\n\n    def _to_database(self, val):\n        if isinstance(val, QueryValue):\n            return val\n        else:\n            return self._get_column().to_database(val)\n\n    def in_(self, item):\n        \"\"\"\n        Returns an in operator\n\n        used where you'd typically want to use python's `in` operator\n        \"\"\"\n        return WhereClause(str(self), InOperator(), item)\n\n    def contains_(self, item):\n        \"\"\"\n        Returns a CONTAINS operator\n        \"\"\"\n        return WhereClause(str(self), ContainsOperator(), item)\n\n    def __eq__(self, other):\n        return WhereClause(str(self), EqualsOperator(), self._to_database(other))\n\n    def __gt__(self, other):\n        return WhereClause(str(self), GreaterThanOperator(), self._to_database(other))\n\n    def __ge__(self, other):\n        return WhereClause(str(self), GreaterThanOrEqualOperator(), self._to_database(other))\n\n    def __lt__(self, other):\n        return WhereClause(str(self), LessThanOperator(), self._to_database(other))\n\n    def __le__(self, other):\n        return WhereClause(str(self), LessThanOrEqualOperator(), self._to_database(other))\n\nclass BatchType(object):\n    Unlogged = 'UNLOGGED'\n    Counter = 'COUNTER'\n\nclass BatchQuery(object):\n    \"\"\"\n    Handles the batching of queries\n\n    http://docs.datastax.com/en/cql/3.0/cql/cql_reference/batch_r.html\n\n    See :doc:`/cqlengine/batches` for more details.\n    \"\"\"\n    warn_multiple_exec = True\n    _consistency = None\n    _connection = None\n    _connection_explicit = False\n\n    def __init__(self, batch_type=None, timestamp=None, consistency=None, execute_on_exception=False, timeout=conn.NOT_SET, connection=None):\n        \"\"\"\n        :param batch_type: (optional) One of batch type values available through BatchType enum\n        :type batch_type: BatchType, str or None\n        :param timestamp: (optional) A datetime or timedelta object with desired timestamp to be applied\n            to the batch conditional.\n        :type timestamp: datetime or timedelta or None\n        :param consistency: (optional) One of consistency values (\"ANY\", \"ONE\", \"QUORUM\" etc)\n        :type consistency: The :class:`.ConsistencyLevel` to be used for the batch query, or None.\n        :param execute_on_exception: (Defaults to False) Indicates that when the BatchQuery instance is used\n            as a context manager the queries accumulated within the context must be executed despite\n            encountering an error within the context. By default, any exception raised from within\n            the context scope will cause the batched queries not to be executed.\n        :type execute_on_exception: bool\n        :param timeout: (optional) Timeout for the entire batch (in seconds), if not specified fallback\n            to default session timeout\n        :type timeout: float or None\n        :param str connection: Connection name to use for the batch execution\n        \"\"\"\n        self.queries = []\n        self.batch_type = batch_type\n        if timestamp is not None and (not isinstance(timestamp, (datetime, timedelta))):\n            raise CQLEngineException('timestamp object must be an instance of datetime')\n        self.timestamp = timestamp\n        self._consistency = consistency\n        self._execute_on_exception = execute_on_exception\n        self._timeout = timeout\n        self._callbacks = []\n        self._executed = False\n        self._context_entered = False\n        self._connection = connection\n        if connection:\n            self._connection_explicit = True\n\n    def add_query(self, query):\n        if not isinstance(query, BaseCQLStatement):\n            raise CQLEngineException('only BaseCQLStatements can be added to a batch query')\n        self.queries.append(query)\n\n    def consistency(self, consistency):\n        self._consistency = consistency\n\n    def _execute_callbacks(self):\n        for callback, args, kwargs in self._callbacks:\n            callback(*args, **kwargs)\n\n    def add_callback(self, fn, *args, **kwargs):\n        \"\"\"Add a function and arguments to be passed to it to be executed after the batch executes.\n\n        A batch can support multiple callbacks.\n\n        Note, that if the batch does not execute, the callbacks are not executed.\n        A callback, thus, is an \"on batch success\" handler.\n\n        :param fn: Callable object\n        :type fn: callable\n        :param \\\\*args: Positional arguments to be passed to the callback at the time of execution\n        :param \\\\*\\\\*kwargs: Named arguments to be passed to the callback at the time of execution\n        \"\"\"\n        if not callable(fn):\n            raise ValueError(\"Value for argument 'fn' is {0} and is not a callable object.\".format(type(fn)))\n        self._callbacks.append((fn, args, kwargs))\n\n    def execute(self):\n        if self._executed and self.warn_multiple_exec:\n            msg = 'Batch executed multiple times.'\n            if self._context_entered:\n                msg += ' If using the batch as a context manager, there is no need to call execute directly.'\n            warn(msg)\n        self._executed = True\n        if len(self.queries) == 0:\n            self._execute_callbacks()\n            return\n        batch_type = None if self.batch_type is CBatchType.LOGGED else self.batch_type\n        opener = 'BEGIN ' + (str(batch_type) + ' ' if batch_type else '') + ' BATCH'\n        if self.timestamp:\n            if isinstance(self.timestamp, int):\n                ts = self.timestamp\n            elif isinstance(self.timestamp, (datetime, timedelta)):\n                ts = self.timestamp\n                if isinstance(self.timestamp, timedelta):\n                    ts += datetime.now()\n                ts = int(time.mktime(ts.timetuple()) * 1000000.0 + ts.microsecond)\n            else:\n                raise ValueError('Batch expects a long, a timedelta, or a datetime')\n            opener += ' USING TIMESTAMP {0}'.format(ts)\n        query_list = [opener]\n        parameters = {}\n        ctx_counter = 0\n        for query in self.queries:\n            query.update_context_id(ctx_counter)\n            ctx = query.get_context()\n            ctx_counter += len(ctx)\n            query_list.append('  ' + str(query))\n            parameters.update(ctx)\n        query_list.append('APPLY BATCH;')\n        tmp = conn.execute('\\n'.join(query_list), parameters, self._consistency, self._timeout, connection=self._connection)\n        check_applied(tmp)\n        self.queries = []\n        self._execute_callbacks()\n\n    def __enter__(self):\n        self._context_entered = True\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if exc_type is not None and (not self._execute_on_exception):\n            return\n        self.execute()\n\nclass ContextQuery(object):\n    \"\"\"\n    A Context manager to allow a Model to switch context easily. Presently, the context only\n    specifies a keyspace for model IO.\n\n    :param \\\\*args: One or more models. A model should be a class type, not an instance.\n    :param \\\\*\\\\*kwargs: (optional) Context parameters: can be *keyspace* or *connection*\n\n    For example:\n\n    .. code-block:: python\n\n            with ContextQuery(Automobile, keyspace='test2') as A:\n                A.objects.create(manufacturer='honda', year=2008, model='civic')\n                print(len(A.objects.all()))  # 1 result\n\n            with ContextQuery(Automobile, keyspace='test4') as A:\n                print(len(A.objects.all()))  # 0 result\n\n            # Multiple models\n            with ContextQuery(Automobile, Automobile2, connection='cluster2') as (A, A2):\n                print(len(A.objects.all()))\n                print(len(A2.objects.all()))\n\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        from cassandra.cqlengine import models\n        self.models = []\n        if len(args) < 1:\n            raise ValueError('No model provided.')\n        keyspace = kwargs.pop('keyspace', None)\n        connection = kwargs.pop('connection', None)\n        if kwargs:\n            raise ValueError('Unknown keyword argument(s): {0}'.format(','.join(kwargs.keys())))\n        for model in args:\n            try:\n                issubclass(model, models.Model)\n            except TypeError:\n                raise ValueError('Models must be derived from base Model.')\n            m = models._clone_model_class(model, {})\n            if keyspace:\n                m.__keyspace__ = keyspace\n            if connection:\n                m.__connection__ = connection\n            self.models.append(m)\n\n    def __enter__(self):\n        if len(self.models) > 1:\n            return tuple(self.models)\n        return self.models[0]\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        return\n\nclass AbstractQuerySet(object):\n\n    def __init__(self, model):\n        super(AbstractQuerySet, self).__init__()\n        self.model = model\n        self._where = []\n        self._conditional = []\n        self._order = []\n        self._allow_filtering = False\n        self._limit = 10000\n        self._defer_fields = set()\n        self._deferred_values = {}\n        self._only_fields = []\n        self._values_list = False\n        self._flat_values_list = False\n        self._result_cache = None\n        self._result_idx = None\n        self._result_generator = None\n        self._materialize_results = True\n        self._distinct_fields = None\n        self._count = None\n        self._batch = None\n        self._ttl = None\n        self._consistency = None\n        self._timestamp = None\n        self._if_not_exists = False\n        self._timeout = conn.NOT_SET\n        self._if_exists = False\n        self._fetch_size = None\n        self._connection = None\n\n    @property\n    def column_family_name(self):\n        return self.model.column_family_name()\n\n    def _execute(self, statement):\n        if self._batch:\n            return self._batch.add_query(statement)\n        else:\n            connection = self._connection or self.model._get_connection()\n            result = _execute_statement(self.model, statement, self._consistency, self._timeout, connection=connection)\n            if self._if_not_exists or self._if_exists or self._conditional:\n                check_applied(result)\n            return result\n\n    def __unicode__(self):\n        return str(self._select_query())\n\n    def __str__(self):\n        return str(self.__unicode__())\n\n    def __call__(self, *args, **kwargs):\n        return self.filter(*args, **kwargs)\n\n    def __deepcopy__(self, memo):\n        clone = self.__class__(self.model)\n        for k, v in self.__dict__.items():\n            if k in ['_con', '_cur', '_result_cache', '_result_idx', '_result_generator', '_construct_result']:\n                clone.__dict__[k] = None\n            elif k == '_batch':\n                clone.__dict__[k] = self._batch\n            elif k == '_timeout':\n                clone.__dict__[k] = self._timeout\n            else:\n                clone.__dict__[k] = copy.deepcopy(v, memo)\n        return clone\n\n    def __len__(self):\n        self._execute_query()\n        return self.count()\n\n    def _select_fields(self):\n        \"\"\" returns the fields to select \"\"\"\n        return []\n\n    def _validate_select_where(self):\n        \"\"\" put select query validation here \"\"\"\n\n    def _select_query(self):\n        \"\"\"\n        Returns a select clause based on the given filter args\n        \"\"\"\n        if self._where:\n            self._validate_select_where()\n        return SelectStatement(self.column_family_name, fields=self._select_fields(), where=self._where, order_by=self._order, limit=self._limit, allow_filtering=self._allow_filtering, distinct_fields=self._distinct_fields, fetch_size=self._fetch_size)\n\n    def _execute_query(self):\n        if self._batch:\n            raise CQLEngineException('Only inserts, updates, and deletes are available in batch mode')\n        if self._result_cache is None:\n            self._result_generator = (i for i in self._execute(self._select_query()))\n            self._result_cache = []\n            self._construct_result = self._maybe_inject_deferred(self._get_result_constructor())\n            if self._materialize_results or self._distinct_fields:\n                self._fill_result_cache()\n\n    def _fill_result_cache(self):\n        \"\"\"\n        Fill the result cache with all results.\n        \"\"\"\n        idx = 0\n        try:\n            while True:\n                idx += 1000\n                self._fill_result_cache_to_idx(idx)\n        except StopIteration:\n            pass\n        self._count = len(self._result_cache)\n\n    def _fill_result_cache_to_idx(self, idx):\n        self._execute_query()\n        if self._result_idx is None:\n            self._result_idx = -1\n        qty = idx - self._result_idx\n        if qty < 1:\n            return\n        else:\n            for idx in range(qty):\n                self._result_idx += 1\n                while True:\n                    try:\n                        self._result_cache[self._result_idx] = self._construct_result(self._result_cache[self._result_idx])\n                        break\n                    except IndexError:\n                        self._result_cache.append(next(self._result_generator))\n\n    def __iter__(self):\n        self._execute_query()\n        idx = 0\n        while True:\n            if len(self._result_cache) <= idx:\n                try:\n                    self._result_cache.append(next(self._result_generator))\n                except StopIteration:\n                    break\n            instance = self._result_cache[idx]\n            if isinstance(instance, dict):\n                self._fill_result_cache_to_idx(idx)\n            yield self._result_cache[idx]\n            idx += 1\n\n    def __getitem__(self, s):\n        self._execute_query()\n        if isinstance(s, slice):\n            start = s.start if s.start else 0\n            if start < 0 or (s.stop is not None and s.stop < 0):\n                warn('ModelQuerySet slicing with negative indices support will be removed in 4.0.', DeprecationWarning)\n            end = s.stop\n            if start < 0 or s.stop is None or s.stop < 0:\n                end = self.count()\n            try:\n                self._fill_result_cache_to_idx(end)\n            except StopIteration:\n                pass\n            return self._result_cache[start:s.stop:s.step]\n        else:\n            try:\n                s = int(s)\n            except (ValueError, TypeError):\n                raise TypeError('QuerySet indices must be integers')\n            if s < 0:\n                warn('ModelQuerySet indexing with negative indices support will be removed in 4.0.', DeprecationWarning)\n            if s < 0:\n                num_results = self.count()\n                s += num_results\n            try:\n                self._fill_result_cache_to_idx(s)\n            except StopIteration:\n                raise IndexError\n            return self._result_cache[s]\n\n    def _get_result_constructor(self):\n        \"\"\"\n        Returns a function that will be used to instantiate query results\n        \"\"\"\n        raise NotImplementedError\n\n    @staticmethod\n    def _construct_with_deferred(f, deferred, row):\n        row.update(deferred)\n        return f(row)\n\n    def _maybe_inject_deferred(self, constructor):\n        return partial(self._construct_with_deferred, constructor, self._deferred_values) if self._deferred_values else constructor\n\n    def batch(self, batch_obj):\n        \"\"\"\n        Set a batch object to run the query on.\n\n        Note: running a select query with a batch object will raise an exception\n        \"\"\"\n        if self._connection:\n            raise CQLEngineException('Cannot specify the connection on model in batch mode.')\n        if batch_obj is not None and (not isinstance(batch_obj, BatchQuery)):\n            raise CQLEngineException('batch_obj must be a BatchQuery instance or None')\n        clone = copy.deepcopy(self)\n        clone._batch = batch_obj\n        return clone\n\n    def first(self):\n        try:\n            return next(iter(self))\n        except StopIteration:\n            return None\n\n    def all(self):\n        \"\"\"\n        Returns a queryset matching all rows\n\n        .. code-block:: python\n\n            for user in User.objects().all():\n                print(user)\n        \"\"\"\n        return copy.deepcopy(self)\n\n    def consistency(self, consistency):\n        \"\"\"\n        Sets the consistency level for the operation. See :class:`.ConsistencyLevel`.\n\n        .. code-block:: python\n\n            for user in User.objects(id=3).consistency(CL.ONE):\n                print(user)\n        \"\"\"\n        clone = copy.deepcopy(self)\n        clone._consistency = consistency\n        return clone\n\n    def _parse_filter_arg(self, arg):\n        \"\"\"\n        Parses a filter arg in the format:\n        <colname>__<op>\n        :returns: colname, op tuple\n        \"\"\"\n        statement = arg.rsplit('__', 1)\n        if len(statement) == 1:\n            return (arg, None)\n        elif len(statement) == 2:\n            return (statement[0], statement[1]) if arg != 'pk__token' else (arg, None)\n        else:\n            raise QueryException(\"Can't parse '{0}'\".format(arg))\n\n    def iff(self, *args, **kwargs):\n        \"\"\"Adds IF statements to queryset\"\"\"\n        if len([x for x in kwargs.values() if x is None]):\n            raise CQLEngineException('None values on iff are not allowed')\n        clone = copy.deepcopy(self)\n        for operator in args:\n            if not isinstance(operator, ConditionalClause):\n                raise QueryException('{0} is not a valid query operator'.format(operator))\n            clone._conditional.append(operator)\n        for arg, val in kwargs.items():\n            if isinstance(val, Token):\n                raise QueryException('Token() values are not valid in conditionals')\n            col_name, col_op = self._parse_filter_arg(arg)\n            try:\n                column = self.model._get_column(col_name)\n            except KeyError:\n                raise QueryException(\"Can't resolve column name: '{0}'\".format(col_name))\n            if isinstance(val, BaseQueryFunction):\n                query_val = val\n            else:\n                query_val = column.to_database(val)\n            operator_class = BaseWhereOperator.get_operator(col_op or 'EQ')\n            operator = operator_class()\n            clone._conditional.append(WhereClause(column.db_field_name, operator, query_val))\n        return clone\n\n    def filter(self, *args, **kwargs):\n        \"\"\"\n        Adds WHERE arguments to the queryset, returning a new queryset\n\n        See :ref:`retrieving-objects-with-filters`\n\n        Returns a QuerySet filtered on the keyword arguments\n        \"\"\"\n        if len([x for x in kwargs.values() if x is None]):\n            raise CQLEngineException('None values on filter are not allowed')\n        clone = copy.deepcopy(self)\n        for operator in args:\n            if not isinstance(operator, WhereClause):\n                raise QueryException('{0} is not a valid query operator'.format(operator))\n            clone._where.append(operator)\n        for arg, val in kwargs.items():\n            col_name, col_op = self._parse_filter_arg(arg)\n            quote_field = True\n            if not isinstance(val, Token):\n                try:\n                    column = self.model._get_column(col_name)\n                except KeyError:\n                    raise QueryException(\"Can't resolve column name: '{0}'\".format(col_name))\n            else:\n                if col_name != 'pk__token':\n                    raise QueryException(\"Token() values may only be compared to the 'pk__token' virtual column\")\n                column = columns._PartitionKeysToken(self.model)\n                quote_field = False\n                partition_columns = column.partition_columns\n                if len(partition_columns) != len(val.value):\n                    raise QueryException('Token() received {0} arguments but model has {1} partition keys'.format(len(val.value), len(partition_columns)))\n                val.set_columns(partition_columns)\n            operator_class = BaseWhereOperator.get_operator(col_op or 'EQ')\n            operator = operator_class()\n            if isinstance(operator, InOperator):\n                if not isinstance(val, (list, tuple)):\n                    raise QueryException('IN queries must use a list/tuple value')\n                query_val = [column.to_database(v) for v in val]\n            elif isinstance(val, BaseQueryFunction):\n                query_val = val\n            elif isinstance(operator, ContainsOperator) and isinstance(column, (columns.List, columns.Set, columns.Map)):\n                query_val = val\n            else:\n                query_val = column.to_database(val)\n                if not col_op:\n                    clone._defer_fields.add(column.db_field_name)\n                    clone._deferred_values[column.db_field_name] = val\n            clone._where.append(WhereClause(column.db_field_name, operator, query_val, quote_field=quote_field))\n        return clone\n\n    def get(self, *args, **kwargs):\n        \"\"\"\n        Returns a single instance matching this query, optionally with additional filter kwargs.\n\n        See :ref:`retrieving-objects-with-filters`\n\n        Returns a single object matching the QuerySet.\n\n        .. code-block:: python\n\n            user = User.get(id=1)\n\n        If no objects are matched, a :class:`~.DoesNotExist` exception is raised.\n\n        If more than one object is found, a :class:`~.MultipleObjectsReturned` exception is raised.\n        \"\"\"\n        if args or kwargs:\n            return self.filter(*args, **kwargs).get()\n        self._execute_query()\n        try:\n            self[1]\n            raise self.model.MultipleObjectsReturned('Multiple objects found')\n        except IndexError:\n            pass\n        try:\n            obj = self[0]\n        except IndexError:\n            raise self.model.DoesNotExist\n        return obj\n\n    def _get_ordering_condition(self, colname):\n        order_type = 'DESC' if colname.startswith('-') else 'ASC'\n        colname = colname.replace('-', '')\n        return (colname, order_type)\n\n    def order_by(self, *colnames):\n        \"\"\"\n        Sets the column(s) to be used for ordering\n\n        Default order is ascending, prepend a '-' to any column name for descending\n\n        *Note: column names must be a clustering key*\n\n        .. code-block:: python\n\n            from uuid import uuid1,uuid4\n\n            class Comment(Model):\n                photo_id = UUID(primary_key=True)\n                comment_id = TimeUUID(primary_key=True, default=uuid1) # second primary key component is a clustering key\n                comment = Text()\n\n            sync_table(Comment)\n\n            u = uuid4()\n            for x in range(5):\n                Comment.create(photo_id=u, comment=\"test %d\" % x)\n\n            print(\"Normal\")\n            for comment in Comment.objects(photo_id=u):\n                print(comment.comment_id)\n\n            print(\"Reversed\")\n            for comment in Comment.objects(photo_id=u).order_by(\"-comment_id\"):\n                print(comment.comment_id)\n        \"\"\"\n        if len(colnames) == 0:\n            clone = copy.deepcopy(self)\n            clone._order = []\n            return clone\n        conditions = []\n        for colname in colnames:\n            conditions.append('\"{0}\" {1}'.format(*self._get_ordering_condition(colname)))\n        clone = copy.deepcopy(self)\n        clone._order.extend(conditions)\n        return clone\n\n    def count(self):\n        \"\"\"\n        Returns the number of rows matched by this query.\n\n        *Note: This function executes a SELECT COUNT() and has a performance cost on large datasets*\n        \"\"\"\n        if self._batch:\n            raise CQLEngineException('Only inserts, updates, and deletes are available in batch mode')\n        if self._count is None:\n            query = self._select_query()\n            query.count = True\n            result = self._execute(query)\n            count_row = result.one().popitem()\n            self._count = count_row[1]\n        return self._count\n\n    def distinct(self, distinct_fields=None):\n        \"\"\"\n        Returns the DISTINCT rows matched by this query.\n\n        distinct_fields default to the partition key fields if not specified.\n\n        *Note: distinct_fields must be a partition key or a static column*\n\n        .. code-block:: python\n\n            class Automobile(Model):\n                manufacturer = columns.Text(partition_key=True)\n                year = columns.Integer(primary_key=True)\n                model = columns.Text(primary_key=True)\n                price = columns.Decimal()\n\n            sync_table(Automobile)\n\n            # create rows\n\n            Automobile.objects.distinct()\n\n            # or\n\n            Automobile.objects.distinct(['manufacturer'])\n\n        \"\"\"\n        clone = copy.deepcopy(self)\n        if distinct_fields:\n            clone._distinct_fields = distinct_fields\n        else:\n            clone._distinct_fields = [x.column_name for x in self.model._partition_keys.values()]\n        return clone\n\n    def limit(self, v):\n        \"\"\"\n        Limits the number of results returned by Cassandra. Use *0* or *None* to disable.\n\n        *Note that CQL's default limit is 10,000, so all queries without a limit set explicitly will have an implicit limit of 10,000*\n\n        .. code-block:: python\n\n            # Fetch 100 users\n            for user in User.objects().limit(100):\n                print(user)\n\n            # Fetch all users\n            for user in User.objects().limit(None):\n                print(user)\n        \"\"\"\n        if v is None:\n            v = 0\n        if not isinstance(v, int):\n            raise TypeError\n        if v == self._limit:\n            return self\n        if v < 0:\n            raise QueryException('Negative limit is not allowed')\n        clone = copy.deepcopy(self)\n        clone._limit = v\n        return clone\n\n    def fetch_size(self, v):\n        \"\"\"\n        Sets the number of rows that are fetched at a time.\n\n        *Note that driver's default fetch size is 5000.*\n\n        .. code-block:: python\n\n            for user in User.objects().fetch_size(500):\n                print(user)\n        \"\"\"\n        if not isinstance(v, int):\n            raise TypeError\n        if v == self._fetch_size:\n            return self\n        if v < 1:\n            raise QueryException('fetch size less than 1 is not allowed')\n        clone = copy.deepcopy(self)\n        clone._fetch_size = v\n        return clone\n\n    def allow_filtering(self):\n        \"\"\"\n        Enables the (usually) unwise practive of querying on a clustering key without also defining a partition key\n        \"\"\"\n        clone = copy.deepcopy(self)\n        clone._allow_filtering = True\n        return clone\n\n    def _only_or_defer(self, action, fields):\n        if action == 'only' and self._only_fields:\n            raise QueryException(\"QuerySet already has 'only' fields defined\")\n        clone = copy.deepcopy(self)\n        missing_fields = [f for f in fields if f not in self.model._columns.keys()]\n        if missing_fields:\n            raise QueryException(\"Can't resolve fields {0} in {1}\".format(', '.join(missing_fields), self.model.__name__))\n        fields = [self.model._columns[field].db_field_name for field in fields]\n        if action == 'defer':\n            clone._defer_fields.update(fields)\n        elif action == 'only':\n            clone._only_fields = fields\n        else:\n            raise ValueError\n        return clone\n\n    def only(self, fields):\n        \"\"\" Load only these fields for the returned query \"\"\"\n        return self._only_or_defer('only', fields)\n\n    def defer(self, fields):\n        \"\"\" Don't load these fields for the returned query \"\"\"\n        return self._only_or_defer('defer', fields)\n\n    def create(self, **kwargs):\n        return self.model(**kwargs).batch(self._batch).ttl(self._ttl).consistency(self._consistency).if_not_exists(self._if_not_exists).timestamp(self._timestamp).if_exists(self._if_exists).using(connection=self._connection).save()\n\n    def delete(self):\n        \"\"\"\n        Deletes the contents of a query\n        \"\"\"\n        partition_keys = set((x.db_field_name for x in self.model._partition_keys.values()))\n        if partition_keys - set((c.field for c in self._where)):\n            raise QueryException('The partition key must be defined on delete queries')\n        dq = DeleteStatement(self.column_family_name, where=self._where, timestamp=self._timestamp, conditionals=self._conditional, if_exists=self._if_exists)\n        self._execute(dq)\n\n    def __eq__(self, q):\n        if len(self._where) == len(q._where):\n            return all([w in q._where for w in self._where])\n        return False\n\n    def __ne__(self, q):\n        return not self != q\n\n    def timeout(self, timeout):\n        \"\"\"\n        :param timeout: Timeout for the query (in seconds)\n        :type timeout: float or None\n        \"\"\"\n        clone = copy.deepcopy(self)\n        clone._timeout = timeout\n        return clone\n\n    def using(self, keyspace=None, connection=None):\n        \"\"\"\n        Change the context on-the-fly of the Model class (keyspace, connection)\n        \"\"\"\n        if connection and self._batch:\n            raise CQLEngineException('Cannot specify a connection on model in batch mode.')\n        clone = copy.deepcopy(self)\n        if keyspace:\n            from cassandra.cqlengine.models import _clone_model_class\n            clone.model = _clone_model_class(self.model, {'__keyspace__': keyspace})\n        if connection:\n            clone._connection = connection\n        return clone\n\nclass ResultObject(dict):\n    \"\"\"\n    adds attribute access to a dictionary\n    \"\"\"\n\n    def __getattr__(self, item):\n        try:\n            return self[item]\n        except KeyError:\n            raise AttributeError\n\nclass SimpleQuerySet(AbstractQuerySet):\n    \"\"\"\n    Overrides _get_result_constructor for querysets that do not define a model (e.g. NamedTable queries)\n    \"\"\"\n\n    def _get_result_constructor(self):\n        \"\"\"\n        Returns a function that will be used to instantiate query results\n        \"\"\"\n        return ResultObject\n\nclass ModelQuerySet(AbstractQuerySet):\n    \"\"\"\n    \"\"\"\n\n    def _validate_select_where(self):\n        \"\"\" Checks that a filterset will not create invalid select statement \"\"\"\n        equal_ops = [self.model._get_column_by_db_name(w.field) for w in self._where if not isinstance(w.value, Token) and (isinstance(w.operator, EqualsOperator) or self.model._get_column_by_db_name(w.field).custom_index)]\n        token_comparison = any([w for w in self._where if isinstance(w.value, Token)])\n        if not any((w.primary_key or w.has_index for w in equal_ops)) and (not token_comparison) and (not self._allow_filtering):\n            raise QueryException('Where clauses require either  =, a IN or a CONTAINS (collection) comparison with either a primary key or indexed field. You might want to consider setting custom_index on fields that you manage index outside cqlengine.')\n        if not self._allow_filtering:\n            if not any((w.has_index for w in equal_ops)):\n                if not any([w.partition_key for w in equal_ops]) and (not token_comparison):\n                    raise QueryException('Filtering on a clustering key without a partition key is not allowed unless allow_filtering() is called on the queryset. You might want to consider setting custom_index on fields that you manage index outside cqlengine.')\n\n    def _select_fields(self):\n        if self._defer_fields or self._only_fields:\n            fields = [columns.db_field_name for columns in self.model._columns.values()]\n            if self._defer_fields:\n                fields = [f for f in fields if f not in self._defer_fields]\n                if not fields:\n                    fields = [columns.db_field_name for columns in self.model._partition_keys.values()]\n            if self._only_fields:\n                fields = [f for f in fields if f in self._only_fields]\n            if not fields:\n                raise QueryException('No fields in select query. Only fields: \"{0}\", defer fields: \"{1}\"'.format(','.join(self._only_fields), ','.join(self._defer_fields)))\n            return fields\n        return super(ModelQuerySet, self)._select_fields()\n\n    def _get_result_constructor(self):\n        \"\"\" Returns a function that will be used to instantiate query results \"\"\"\n        if not self._values_list:\n            return self.model._construct_instance\n        elif self._flat_values_list:\n            key = self._only_fields[0]\n            return lambda row: row[key]\n        else:\n            return lambda row: [row[f] for f in self._only_fields]\n\n    def _get_ordering_condition(self, colname):\n        colname, order_type = super(ModelQuerySet, self)._get_ordering_condition(colname)\n        column = self.model._columns.get(colname)\n        if column is None:\n            raise QueryException(\"Can't resolve the column name: '{0}'\".format(colname))\n        if not column.primary_key:\n            raise QueryException(\"Can't order on '{0}', can only order on (clustered) primary keys\".format(colname))\n        pks = [v for k, v in self.model._columns.items() if v.primary_key]\n        if column == pks[0]:\n            raise QueryException(\"Can't order by the first primary key (partition key), clustering (secondary) keys only\")\n        return (column.db_field_name, order_type)\n\n    def values_list(self, *fields, **kwargs):\n        \"\"\" Instructs the query set to return tuples, not model instance \"\"\"\n        flat = kwargs.pop('flat', False)\n        if kwargs:\n            raise TypeError('Unexpected keyword arguments to values_list: %s' % (kwargs.keys(),))\n        if flat and len(fields) > 1:\n            raise TypeError(\"'flat' is not valid when values_list is called with more than one field.\")\n        clone = self.only(fields)\n        clone._values_list = True\n        clone._flat_values_list = flat\n        return clone\n\n    def ttl(self, ttl):\n        \"\"\"\n        Sets the ttl (in seconds) for modified data.\n\n        *Note that running a select query with a ttl value will raise an exception*\n        \"\"\"\n        clone = copy.deepcopy(self)\n        clone._ttl = ttl\n        return clone\n\n    def timestamp(self, timestamp):\n        \"\"\"\n        Allows for custom timestamps to be saved with the record.\n        \"\"\"\n        clone = copy.deepcopy(self)\n        clone._timestamp = timestamp\n        return clone\n\n    def if_not_exists(self):\n        \"\"\"\n        Check the existence of an object before insertion.\n\n        If the insertion isn't applied, a LWTException is raised.\n        \"\"\"\n        if self.model._has_counter:\n            raise IfNotExistsWithCounterColumn('if_not_exists cannot be used with tables containing counter columns')\n        clone = copy.deepcopy(self)\n        clone._if_not_exists = True\n        return clone\n\n    def if_exists(self):\n        \"\"\"\n        Check the existence of an object before an update or delete.\n\n        If the update or delete isn't applied, a LWTException is raised.\n        \"\"\"\n        if self.model._has_counter:\n            raise IfExistsWithCounterColumn('if_exists cannot be used with tables containing counter columns')\n        clone = copy.deepcopy(self)\n        clone._if_exists = True\n        return clone\n\n    def update(self, **values):\n        \"\"\"\n        Performs an update on the row selected by the queryset. Include values to update in the\n        update like so:\n\n        .. code-block:: python\n\n            Model.objects(key=n).update(value='x')\n\n        Passing in updates for columns which are not part of the model will raise a ValidationError.\n\n        Per column validation will be performed, but instance level validation will not\n        (i.e., `Model.validate` is not called).  This is sometimes referred to as a blind update.\n\n        For example:\n\n        .. code-block:: python\n\n            class User(Model):\n                id = Integer(primary_key=True)\n                name = Text()\n\n            setup([\"localhost\"], \"test\")\n            sync_table(User)\n\n            u = User.create(id=1, name=\"jon\")\n\n            User.objects(id=1).update(name=\"Steve\")\n\n            # sets name to null\n            User.objects(id=1).update(name=None)\n\n\n        Also supported is blindly adding and removing elements from container columns,\n        without loading a model instance from Cassandra.\n\n        Using the syntax `.update(column_name={x, y, z})` will overwrite the contents of the container, like updating a\n        non container column. However, adding `__<operation>` to the end of the keyword arg, makes the update call add\n        or remove items from the collection, without overwriting then entire column.\n\n        Given the model below, here are the operations that can be performed on the different container columns:\n\n        .. code-block:: python\n\n            class Row(Model):\n                row_id      = columns.Integer(primary_key=True)\n                set_column  = columns.Set(Integer)\n                list_column = columns.List(Integer)\n                map_column  = columns.Map(Integer, Integer)\n\n        :class:`~cqlengine.columns.Set`\n\n        - `add`: adds the elements of the given set to the column\n        - `remove`: removes the elements of the given set to the column\n\n\n        .. code-block:: python\n\n            # add elements to a set\n            Row.objects(row_id=5).update(set_column__add={6})\n\n            # remove elements to a set\n            Row.objects(row_id=5).update(set_column__remove={4})\n\n        :class:`~cqlengine.columns.List`\n\n        - `append`: appends the elements of the given list to the end of the column\n        - `prepend`: prepends the elements of the given list to the beginning of the column\n\n        .. code-block:: python\n\n            # append items to a list\n            Row.objects(row_id=5).update(list_column__append=[6, 7])\n\n            # prepend items to a list\n            Row.objects(row_id=5).update(list_column__prepend=[1, 2])\n\n\n        :class:`~cqlengine.columns.Map`\n\n        - `update`: adds the given keys/values to the columns, creating new entries if they didn't exist, and overwriting old ones if they did\n\n        .. code-block:: python\n\n            # add items to a map\n            Row.objects(row_id=5).update(map_column__update={1: 2, 3: 4})\n\n            # remove items from a map\n            Row.objects(row_id=5).update(map_column__remove={1, 2})\n        \"\"\"\n        if not values:\n            return\n        nulled_columns = set()\n        updated_columns = set()\n        us = UpdateStatement(self.column_family_name, where=self._where, ttl=self._ttl, timestamp=self._timestamp, conditionals=self._conditional, if_exists=self._if_exists)\n        for name, val in values.items():\n            col_name, col_op = self._parse_filter_arg(name)\n            col = self.model._columns.get(col_name)\n            if col is None:\n                raise ValidationError('{0}.{1} has no column named: {2}'.format(self.__module__, self.model.__name__, col_name))\n            if col.is_primary_key:\n                raise ValidationError(\"Cannot apply update to primary key '{0}' for {1}.{2}\".format(col_name, self.__module__, self.model.__name__))\n            if col_op == 'remove' and isinstance(col, columns.Map):\n                if not isinstance(val, set):\n                    raise ValidationError(\"Cannot apply update operation '{0}' on column '{1}' with value '{2}'. A set is required.\".format(col_op, col_name, val))\n                val = {v: None for v in val}\n            else:\n                val = col.validate(val)\n            if val is None:\n                nulled_columns.add(col_name)\n                continue\n            us.add_update(col, val, operation=col_op)\n            updated_columns.add(col_name)\n        if us.assignments:\n            self._execute(us)\n        if nulled_columns:\n            delete_conditional = [condition for condition in self._conditional if condition.field not in updated_columns] if self._conditional else None\n            ds = DeleteStatement(self.column_family_name, fields=nulled_columns, where=self._where, conditionals=delete_conditional, if_exists=self._if_exists)\n            self._execute(ds)\n\nclass DMLQuery(object):\n    \"\"\"\n    A query object used for queries performing inserts, updates, or deletes\n\n    this is usually instantiated by the model instance to be modified\n\n    unlike the read query object, this is mutable\n    \"\"\"\n    _ttl = None\n    _consistency = None\n    _timestamp = None\n    _if_not_exists = False\n    _if_exists = False\n\n    def __init__(self, model, instance=None, batch=None, ttl=None, consistency=None, timestamp=None, if_not_exists=False, conditional=None, timeout=conn.NOT_SET, if_exists=False):\n        self.model = model\n        self.column_family_name = self.model.column_family_name()\n        self.instance = instance\n        self._batch = batch\n        self._ttl = ttl\n        self._consistency = consistency\n        self._timestamp = timestamp\n        self._if_not_exists = if_not_exists\n        self._if_exists = if_exists\n        self._conditional = conditional\n        self._timeout = timeout\n\n    def _execute(self, statement):\n        connection = self.instance._get_connection() if self.instance else self.model._get_connection()\n        if self._batch:\n            if self._batch._connection:\n                if not self._batch._connection_explicit and connection and (connection != self._batch._connection):\n                    raise CQLEngineException('BatchQuery queries must be executed on the same connection')\n            else:\n                self._batch._connection = connection\n            return self._batch.add_query(statement)\n        else:\n            results = _execute_statement(self.model, statement, self._consistency, self._timeout, connection=connection)\n            if self._if_not_exists or self._if_exists or self._conditional:\n                check_applied(results)\n            return results\n\n    def batch(self, batch_obj):\n        if batch_obj is not None and (not isinstance(batch_obj, BatchQuery)):\n            raise CQLEngineException('batch_obj must be a BatchQuery instance or None')\n        self._batch = batch_obj\n        return self\n\n    def _delete_null_columns(self, conditionals=None):\n        \"\"\"\n        executes a delete query to remove columns that have changed to null\n        \"\"\"\n        ds = DeleteStatement(self.column_family_name, conditionals=conditionals, if_exists=self._if_exists)\n        deleted_fields = False\n        static_only = True\n        for _, v in self.instance._values.items():\n            col = v.column\n            if v.deleted:\n                ds.add_field(col.db_field_name)\n                deleted_fields = True\n                static_only &= col.static\n            elif isinstance(col, columns.Map):\n                uc = MapDeleteClause(col.db_field_name, v.value, v.previous_value)\n                if uc.get_context_size() > 0:\n                    ds.add_field(uc)\n                    deleted_fields = True\n                    static_only |= col.static\n        if deleted_fields:\n            keys = self.model._partition_keys if static_only else self.model._primary_keys\n            for name, col in keys.items():\n                ds.add_where(col, EqualsOperator(), getattr(self.instance, name))\n            self._execute(ds)\n\n    def update(self):\n        \"\"\"\n        updates a row.\n        This is a blind update call.\n        All validation and cleaning needs to happen\n        prior to calling this.\n        \"\"\"\n        if self.instance is None:\n            raise CQLEngineException('DML Query intance attribute is None')\n        assert type(self.instance) == self.model\n        null_clustering_key = False if len(self.instance._clustering_keys) == 0 else True\n        static_changed_only = True\n        statement = UpdateStatement(self.column_family_name, ttl=self._ttl, timestamp=self._timestamp, conditionals=self._conditional, if_exists=self._if_exists)\n        for name, col in self.instance._clustering_keys.items():\n            null_clustering_key = null_clustering_key and col._val_is_null(getattr(self.instance, name, None))\n        updated_columns = set()\n        for name, col in self.model._columns.items():\n            if null_clustering_key and (not col.static) and (not col.partition_key):\n                continue\n            if not col.is_primary_key:\n                val = getattr(self.instance, name, None)\n                val_mgr = self.instance._values[name]\n                if val is None:\n                    continue\n                if not val_mgr.changed and (not isinstance(col, columns.Counter)):\n                    continue\n                static_changed_only = static_changed_only and col.static\n                statement.add_update(col, val, previous=val_mgr.previous_value)\n                updated_columns.add(col.db_field_name)\n        if statement.assignments:\n            for name, col in self.model._primary_keys.items():\n                if (null_clustering_key or static_changed_only) and (not col.partition_key):\n                    continue\n                statement.add_where(col, EqualsOperator(), getattr(self.instance, name))\n            self._execute(statement)\n        if not null_clustering_key:\n            delete_conditionals = [condition for condition in self._conditional if condition.field not in updated_columns] if self._conditional else None\n            self._delete_null_columns(delete_conditionals)\n\n    def save(self):\n        \"\"\"\n        Creates / updates a row.\n        This is a blind insert call.\n        All validation and cleaning needs to happen\n        prior to calling this.\n        \"\"\"\n        if self.instance is None:\n            raise CQLEngineException('DML Query intance attribute is None')\n        assert type(self.instance) == self.model\n        nulled_fields = set()\n        if self.instance._has_counter or self.instance._can_update():\n            if self.instance._has_counter:\n                warn(\"'create' and 'save' actions on Counters are deprecated. It will be disallowed in 4.0. Use the 'update' mechanism instead.\", DeprecationWarning)\n            return self.update()\n        else:\n            insert = InsertStatement(self.column_family_name, ttl=self._ttl, timestamp=self._timestamp, if_not_exists=self._if_not_exists)\n            static_save_only = False if len(self.instance._clustering_keys) == 0 else True\n            for name, col in self.instance._clustering_keys.items():\n                static_save_only = static_save_only and col._val_is_null(getattr(self.instance, name, None))\n            for name, col in self.instance._columns.items():\n                if static_save_only and (not col.static) and (not col.partition_key):\n                    continue\n                val = getattr(self.instance, name, None)\n                if col._val_is_null(val):\n                    if self.instance._values[name].changed:\n                        nulled_fields.add(col.db_field_name)\n                    continue\n                if col.has_default and (not self.instance._values[name].changed):\n                    self.instance._values[name].explicit = True\n                insert.add_assignment(col, getattr(self.instance, name, None))\n        if not insert.is_empty:\n            self._execute(insert)\n        if not static_save_only:\n            self._delete_null_columns()\n\n    def delete(self):\n        \"\"\" Deletes one instance \"\"\"\n        if self.instance is None:\n            raise CQLEngineException('DML Query instance attribute is None')\n        ds = DeleteStatement(self.column_family_name, timestamp=self._timestamp, conditionals=self._conditional, if_exists=self._if_exists)\n        for name, col in self.model._primary_keys.items():\n            val = getattr(self.instance, name)\n            if val is None and (not col.partition_key):\n                continue\n            ds.add_where(col, EqualsOperator(), val)\n        self._execute(ds)\n\ndef _execute_statement(model, statement, consistency_level, timeout, connection=None):\n    params = statement.get_context()\n    s = SimpleStatement(str(statement), consistency_level=consistency_level, fetch_size=statement.fetch_size)\n    if model._partition_key_index:\n        key_values = statement.partition_key_values(model._partition_key_index)\n        if not any((v is None for v in key_values)):\n            parts = model._routing_key_from_values(key_values, conn.get_cluster(connection).protocol_version)\n            s.routing_key = parts\n            s.keyspace = model._get_keyspace()\n    connection = connection or model._get_connection()\n    return conn.execute(s, params, timeout=timeout, connection=connection)",
    "cassandra/cqlengine/usertype.py": "import re\nfrom cassandra.util import OrderedDict\nfrom cassandra.cqlengine import CQLEngineException\nfrom cassandra.cqlengine import columns\nfrom cassandra.cqlengine import connection as conn\nfrom cassandra.cqlengine import models\n\nclass UserTypeException(CQLEngineException):\n    pass\n\nclass UserTypeDefinitionException(UserTypeException):\n    pass\n\nclass BaseUserType(object):\n    \"\"\"\n    The base type class; don't inherit from this, inherit from UserType, defined below\n    \"\"\"\n    __type_name__ = None\n    _fields = None\n    _db_map = None\n\n    def __init__(self, **values):\n        self._values = {}\n        if self._db_map:\n            values = dict(((self._db_map.get(k, k), v) for k, v in values.items()))\n        for name, field in self._fields.items():\n            field_default = field.get_default() if field.has_default else None\n            value = values.get(name, field_default)\n            if value is not None or isinstance(field, columns.BaseContainerColumn):\n                value = field.to_python(value)\n            value_mngr = field.value_manager(self, field, value)\n            value_mngr.explicit = name in values\n            self._values[name] = value_mngr\n\n    def __eq__(self, other):\n        if self.__class__ != other.__class__:\n            return False\n        keys = set(self._fields.keys())\n        other_keys = set(other._fields.keys())\n        if keys != other_keys:\n            return False\n        for key in other_keys:\n            if getattr(self, key, None) != getattr(other, key, None):\n                return False\n        return True\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __str__(self):\n        return '{{{0}}}'.format(', '.join((\"'{0}': {1}\".format(k, getattr(self, k)) for k, v in self._values.items())))\n\n    def has_changed_fields(self):\n        return any((v.changed for v in self._values.values()))\n\n    def reset_changed_fields(self):\n        for v in self._values.values():\n            v.reset_previous_value()\n\n    def __iter__(self):\n        for field in self._fields.keys():\n            yield field\n\n    def __getattr__(self, attr):\n        try:\n            return getattr(self, self._db_map[attr])\n        except KeyError:\n            raise AttributeError(attr)\n\n    def __getitem__(self, key):\n        if not isinstance(key, str):\n            raise TypeError\n        if key not in self._fields.keys():\n            raise KeyError\n        return getattr(self, key)\n\n    def __setitem__(self, key, val):\n        if not isinstance(key, str):\n            raise TypeError\n        if key not in self._fields.keys():\n            raise KeyError\n        return setattr(self, key, val)\n\n    def __len__(self):\n        try:\n            return self._len\n        except:\n            self._len = len(self._fields.keys())\n            return self._len\n\n    def keys(self):\n        \"\"\" Returns a list of column IDs. \"\"\"\n        return [k for k in self]\n\n    def values(self):\n        \"\"\" Returns list of column values. \"\"\"\n        return [self[k] for k in self]\n\n    def items(self):\n        \"\"\" Returns a list of column ID/value tuples. \"\"\"\n        return [(k, self[k]) for k in self]\n\n    @classmethod\n    def register_for_keyspace(cls, keyspace, connection=None):\n        conn.register_udt(keyspace, cls.type_name(), cls, connection=connection)\n\n    @classmethod\n    def type_name(cls):\n        \"\"\"\n        Returns the type name if it's been defined\n        otherwise, it creates it from the class name\n        \"\"\"\n        if cls.__type_name__:\n            type_name = cls.__type_name__.lower()\n        else:\n            camelcase = re.compile('([a-z])([A-Z])')\n            ccase = lambda s: camelcase.sub(lambda v: '{0}_{1}'.format(v.group(1), v.group(2)), s)\n            type_name = ccase(cls.__name__)\n            type_name = type_name[-48:]\n            type_name = type_name.lower()\n            type_name = re.sub('^_+', '', type_name)\n            cls.__type_name__ = type_name\n        return type_name\n\n    def validate(self):\n        \"\"\"\n        Cleans and validates the field values\n        \"\"\"\n        for name, field in self._fields.items():\n            v = getattr(self, name)\n            if v is None and (not self._values[name].explicit) and field.has_default:\n                v = field.get_default()\n            val = field.validate(v)\n            setattr(self, name, val)\n\nclass UserTypeMetaClass(type):\n\nclass UserType(BaseUserType, metaclass=UserTypeMetaClass):\n    \"\"\"\n    This class is used to model User Defined Types. To define a type, declare a class inheriting from this,\n    and assign field types as class attributes:\n\n    .. code-block:: python\n\n        # connect with default keyspace ...\n\n        from cassandra.cqlengine.columns import Text, Integer\n        from cassandra.cqlengine.usertype import UserType\n\n        class address(UserType):\n            street = Text()\n            zipcode = Integer()\n\n        from cassandra.cqlengine import management\n        management.sync_type(address)\n\n    Please see :ref:`user_types` for a complete example and discussion.\n    \"\"\"\n    __type_name__ = None\n    \"\\n    *Optional.* Sets the name of the CQL type for this type.\\n\\n    If not specified, the type name will be the name of the class, with it's module name as it's prefix.\\n    \"",
    "cassandra/cqlengine/models.py": "import logging\nimport re\nfrom warnings import warn\nfrom cassandra.cqlengine import CQLEngineException, ValidationError\nfrom cassandra.cqlengine import columns\nfrom cassandra.cqlengine import connection\nfrom cassandra.cqlengine import query\nfrom cassandra.cqlengine.query import DoesNotExist as _DoesNotExist\nfrom cassandra.cqlengine.query import MultipleObjectsReturned as _MultipleObjectsReturned\nfrom cassandra.metadata import protect_name\nfrom cassandra.util import OrderedDict\nlog = logging.getLogger(__name__)\n\ndef _clone_model_class(model, attrs):\n    new_type = type(model.__name__, (model,), attrs)\n    try:\n        new_type.__abstract__ = model.__abstract__\n        new_type.__discriminator_value__ = model.__discriminator_value__\n        new_type.__default_ttl__ = model.__default_ttl__\n    except AttributeError:\n        pass\n    return new_type\n\nclass ModelException(CQLEngineException):\n    pass\n\nclass ModelDefinitionException(ModelException):\n    pass\n\nclass PolymorphicModelException(ModelException):\n    pass\n\nclass UndefinedKeyspaceWarning(Warning):\n    pass\nDEFAULT_KEYSPACE = None\n\nclass hybrid_classmethod(object):\n    \"\"\"\n    Allows a method to behave as both a class method and\n    normal instance method depending on how it's called\n    \"\"\"\n\n    def __init__(self, clsmethod, instmethod):\n        self.clsmethod = clsmethod\n        self.instmethod = instmethod\n\n    def __get__(self, instance, owner):\n        if instance is None:\n            return self.clsmethod.__get__(owner, owner)\n        else:\n            return self.instmethod.__get__(instance, owner)\n\n    def __call__(self, *args, **kwargs):\n        \"\"\"\n        Just a hint to IDEs that it's ok to call this\n        \"\"\"\n        raise NotImplementedError\n\nclass QuerySetDescriptor(object):\n    \"\"\"\n    returns a fresh queryset for the given model\n    it's declared on everytime it's accessed\n    \"\"\"\n\n    def __get__(self, obj, model):\n        \"\"\" :rtype: ModelQuerySet \"\"\"\n        if model.__abstract__:\n            raise CQLEngineException('cannot execute queries against abstract models')\n        queryset = model.__queryset__(model)\n        if model._is_polymorphic and (not model._is_polymorphic_base):\n            name, column = (model._discriminator_column_name, model._discriminator_column)\n            if column.partition_key or column.index:\n                return queryset.filter(**{name: model.__discriminator_value__})\n        return queryset\n\n    def __call__(self, *args, **kwargs):\n        \"\"\"\n        Just a hint to IDEs that it's ok to call this\n\n        :rtype: ModelQuerySet\n        \"\"\"\n        raise NotImplementedError\n\nclass ConditionalDescriptor(object):\n    \"\"\"\n    returns a query set descriptor\n    \"\"\"\n\n    def __get__(self, instance, model):\n        if instance:\n\n            def conditional_setter(*prepared_conditional, **unprepared_conditionals):\n                if len(prepared_conditional) > 0:\n                    conditionals = prepared_conditional[0]\n                else:\n                    conditionals = instance.objects.iff(**unprepared_conditionals)._conditional\n                instance._conditional = conditionals\n                return instance\n            return conditional_setter\n        qs = model.__queryset__(model)\n\n        def conditional_setter(**unprepared_conditionals):\n            conditionals = model.objects.iff(**unprepared_conditionals)._conditional\n            qs._conditional = conditionals\n            return qs\n        return conditional_setter\n\n    def __call__(self, *args, **kwargs):\n        raise NotImplementedError\n\nclass TTLDescriptor(object):\n    \"\"\"\n    returns a query set descriptor\n    \"\"\"\n\n    def __get__(self, instance, model):\n        if instance:\n\n            def ttl_setter(ts):\n                instance._ttl = ts\n                return instance\n            return ttl_setter\n        qs = model.__queryset__(model)\n\n        def ttl_setter(ts):\n            qs._ttl = ts\n            return qs\n        return ttl_setter\n\n    def __call__(self, *args, **kwargs):\n        raise NotImplementedError\n\nclass TimestampDescriptor(object):\n    \"\"\"\n    returns a query set descriptor with a timestamp specified\n    \"\"\"\n\n    def __get__(self, instance, model):\n        if instance:\n\n            def timestamp_setter(ts):\n                instance._timestamp = ts\n                return instance\n            return timestamp_setter\n        return model.objects.timestamp\n\n    def __call__(self, *args, **kwargs):\n        raise NotImplementedError\n\nclass IfNotExistsDescriptor(object):\n    \"\"\"\n    return a query set descriptor with a if_not_exists flag specified\n    \"\"\"\n\n    def __get__(self, instance, model):\n        if instance:\n\n            def ifnotexists_setter(ife=True):\n                instance._if_not_exists = ife\n                return instance\n            return ifnotexists_setter\n        return model.objects.if_not_exists\n\n    def __call__(self, *args, **kwargs):\n        raise NotImplementedError\n\nclass IfExistsDescriptor(object):\n    \"\"\"\n    return a query set descriptor with a if_exists flag specified\n    \"\"\"\n\n    def __get__(self, instance, model):\n        if instance:\n\n            def ifexists_setter(ife=True):\n                instance._if_exists = ife\n                return instance\n            return ifexists_setter\n        return model.objects.if_exists\n\n    def __call__(self, *args, **kwargs):\n        raise NotImplementedError\n\nclass ConsistencyDescriptor(object):\n    \"\"\"\n    returns a query set descriptor if called on Class, instance if it was an instance call\n    \"\"\"\n\n    def __get__(self, instance, model):\n        if instance:\n\n            def consistency_setter(consistency):\n                instance.__consistency__ = consistency\n                return instance\n            return consistency_setter\n        qs = model.__queryset__(model)\n\n        def consistency_setter(consistency):\n            qs._consistency = consistency\n            return qs\n        return consistency_setter\n\n    def __call__(self, *args, **kwargs):\n        raise NotImplementedError\n\nclass UsingDescriptor(object):\n    \"\"\"\n    return a query set descriptor with a connection context specified\n    \"\"\"\n\n    def __get__(self, instance, model):\n        if instance:\n\n            def using_setter(connection=None):\n                if connection:\n                    instance._connection = connection\n                return instance\n            return using_setter\n        return model.objects.using\n\n    def __call__(self, *args, **kwargs):\n        raise NotImplementedError\n\nclass ColumnQueryEvaluator(query.AbstractQueryableColumn):\n    \"\"\"\n    Wraps a column and allows it to be used in comparator\n    expressions, returning query operators\n\n    ie:\n    Model.column == 5\n    \"\"\"\n\n    def __init__(self, column):\n        self.column = column\n\n    def __unicode__(self):\n        return self.column.db_field_name\n\n    def _get_column(self):\n        return self.column\n\nclass ColumnDescriptor(object):\n    \"\"\"\n    Handles the reading and writing of column values to and from\n    a model instance's value manager, as well as creating\n    comparator queries\n    \"\"\"\n\n    def __init__(self, column):\n        \"\"\"\n        :param column:\n        :type column: columns.Column\n        :return:\n        \"\"\"\n        self.column = column\n        self.query_evaluator = ColumnQueryEvaluator(self.column)\n\n    def __get__(self, instance, owner):\n        \"\"\"\n        Returns either the value or column, depending\n        on if an instance is provided or not\n\n        :param instance: the model instance\n        :type instance: Model\n        \"\"\"\n        try:\n            return instance._values[self.column.column_name].getval()\n        except AttributeError:\n            return self.query_evaluator\n\n    def __set__(self, instance, value):\n        \"\"\"\n        Sets the value on an instance, raises an exception with classes\n        TODO: use None instance to create update statements\n        \"\"\"\n        if instance:\n            return instance._values[self.column.column_name].setval(value)\n        else:\n            raise AttributeError('cannot reassign column values')\n\n    def __delete__(self, instance):\n        \"\"\"\n        Sets the column value to None, if possible\n        \"\"\"\n        if instance:\n            if self.column.can_delete:\n                instance._values[self.column.column_name].delval()\n            else:\n                raise AttributeError('cannot delete {0} columns'.format(self.column.column_name))\n\nclass BaseModel(object):\n    \"\"\"\n    The base model class, don't inherit from this, inherit from Model, defined below\n    \"\"\"\n\n    class DoesNotExist(_DoesNotExist):\n        pass\n\n    class MultipleObjectsReturned(_MultipleObjectsReturned):\n        pass\n    objects = QuerySetDescriptor()\n    ttl = TTLDescriptor()\n    consistency = ConsistencyDescriptor()\n    iff = ConditionalDescriptor()\n    timestamp = TimestampDescriptor()\n    if_not_exists = IfNotExistsDescriptor()\n    if_exists = IfExistsDescriptor()\n    using = UsingDescriptor()\n    __table_name__ = None\n    __table_name_case_sensitive__ = False\n    __keyspace__ = None\n    __connection__ = None\n    __discriminator_value__ = None\n    __options__ = None\n    __compute_routing_key__ = True\n    __queryset__ = query.ModelQuerySet\n    __dmlquery__ = query.DMLQuery\n    __consistency__ = None\n    _timestamp = None\n    _if_not_exists = False\n    _if_exists = False\n    _table_name = None\n    _connection = None\n\n    def __init__(self, **values):\n        self._ttl = None\n        self._timestamp = None\n        self._conditional = None\n        self._batch = None\n        self._timeout = connection.NOT_SET\n        self._is_persisted = False\n        self._connection = None\n        self._values = {}\n        for name, column in self._columns.items():\n            column_default = column.get_default() if column.has_default else None\n            value = values.get(name, column_default)\n            if value is not None or isinstance(column, columns.BaseContainerColumn):\n                value = column.to_python(value)\n            value_mngr = column.value_manager(self, column, value)\n            value_mngr.explicit = name in values\n            self._values[name] = value_mngr\n\n    def __repr__(self):\n        return '{0}({1})'.format(self.__class__.__name__, ', '.join(('{0}={1!r}'.format(k, getattr(self, k)) for k in self._defined_columns.keys() if k != self._discriminator_column_name)))\n\n    def __str__(self):\n        \"\"\"\n        Pretty printing of models by their primary key\n        \"\"\"\n        return '{0} <{1}>'.format(self.__class__.__name__, ', '.join(('{0}={1}'.format(k, getattr(self, k)) for k in self._primary_keys.keys())))\n\n    @classmethod\n    def _routing_key_from_values(cls, pk_values, protocol_version):\n        return cls._key_serializer(pk_values, protocol_version)\n\n    @classmethod\n    def _discover_polymorphic_submodels(cls):\n        if not cls._is_polymorphic_base:\n            raise ModelException('_discover_polymorphic_submodels can only be called on polymorphic base classes')\n\n        def _discover(klass):\n            if not klass._is_polymorphic_base and klass.__discriminator_value__ is not None:\n                cls._discriminator_map[klass.__discriminator_value__] = klass\n            for subklass in klass.__subclasses__():\n                _discover(subklass)\n        _discover(cls)\n\n    @classmethod\n    def _get_model_by_discriminator_value(cls, key):\n        if not cls._is_polymorphic_base:\n            raise ModelException('_get_model_by_discriminator_value can only be called on polymorphic base classes')\n        return cls._discriminator_map.get(key)\n\n    @classmethod\n    def _construct_instance(cls, values):\n        \"\"\"\n        method used to construct instances from query results\n        this is where polymorphic deserialization occurs\n        \"\"\"\n        if cls._db_map:\n            values = dict(((cls._db_map.get(k, k), v) for k, v in values.items()))\n        if cls._is_polymorphic:\n            disc_key = values.get(cls._discriminator_column_name)\n            if disc_key is None:\n                raise PolymorphicModelException('discriminator value was not found in values')\n            poly_base = cls if cls._is_polymorphic_base else cls._polymorphic_base\n            klass = poly_base._get_model_by_discriminator_value(disc_key)\n            if klass is None:\n                poly_base._discover_polymorphic_submodels()\n                klass = poly_base._get_model_by_discriminator_value(disc_key)\n                if klass is None:\n                    raise PolymorphicModelException('unrecognized discriminator column {0} for class {1}'.format(disc_key, poly_base.__name__))\n            if not issubclass(klass, cls):\n                raise PolymorphicModelException('{0} is not a subclass of {1}'.format(klass.__name__, cls.__name__))\n            values = dict(((k, v) for k, v in values.items() if k in klass._columns.keys()))\n        else:\n            klass = cls\n        instance = klass(**values)\n        instance._set_persisted(force=True)\n        return instance\n\n    def _set_persisted(self, force=False):\n        for v in [v for v in self._values.values() if v.changed or force]:\n            v.reset_previous_value()\n            v.explicit = False\n        self._is_persisted = True\n\n    def _can_update(self):\n        \"\"\"\n        Called by the save function to check if this should be\n        persisted with update or insert\n\n        :return:\n        \"\"\"\n        if not self._is_persisted:\n            return False\n        return all([not self._values[k].changed for k in self._primary_keys])\n\n    @classmethod\n    def _get_keyspace(cls):\n        \"\"\"\n        Returns the manual keyspace, if set, otherwise the default keyspace\n        \"\"\"\n        return cls.__keyspace__ or DEFAULT_KEYSPACE\n\n    @classmethod\n    def _get_column(cls, name):\n        \"\"\"\n        Returns the column matching the given name, raising a key error if\n        it doesn't exist\n\n        :param name: the name of the column to return\n        :rtype: Column\n        \"\"\"\n        return cls._columns[name]\n\n    @classmethod\n    def _get_column_by_db_name(cls, name):\n        \"\"\"\n        Returns the column, mapped by db_field name\n        \"\"\"\n        return cls._columns.get(cls._db_map.get(name, name))\n\n    def __eq__(self, other):\n        if self.__class__ != other.__class__:\n            return False\n        keys = set(self._columns.keys())\n        other_keys = set(other._columns.keys())\n        if keys != other_keys:\n            return False\n        return all((getattr(self, key, None) == getattr(other, key, None) for key in other_keys))\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    @classmethod\n    def column_family_name(cls, include_keyspace=True):\n        \"\"\"\n        Returns the column family name if it's been defined\n        otherwise, it creates it from the module and class name\n        \"\"\"\n        cf_name = protect_name(cls._raw_column_family_name())\n        if include_keyspace:\n            keyspace = cls._get_keyspace()\n            if not keyspace:\n                raise CQLEngineException('Model keyspace is not set and no default is available. Set model keyspace or setup connection before attempting to generate a query.')\n            return '{0}.{1}'.format(protect_name(keyspace), cf_name)\n        return cf_name\n\n    @classmethod\n    def _raw_column_family_name(cls):\n        if not cls._table_name:\n            if cls.__table_name__:\n                if cls.__table_name_case_sensitive__:\n                    warn('Model __table_name_case_sensitive__ will be removed in 4.0.', PendingDeprecationWarning)\n                    cls._table_name = cls.__table_name__\n                else:\n                    table_name = cls.__table_name__.lower()\n                    if cls.__table_name__ != table_name:\n                        warn(\"Model __table_name__ will be case sensitive by default in 4.0. You should fix the __table_name__ value of the '{0}' model.\".format(cls.__name__))\n                    cls._table_name = table_name\n            elif cls._is_polymorphic and (not cls._is_polymorphic_base):\n                cls._table_name = cls._polymorphic_base._raw_column_family_name()\n            else:\n                camelcase = re.compile('([a-z])([A-Z])')\n                ccase = lambda s: camelcase.sub(lambda v: '{0}_{1}'.format(v.group(1), v.group(2).lower()), s)\n                cf_name = ccase(cls.__name__)\n                cf_name = cf_name[-48:]\n                cf_name = cf_name.lower()\n                cf_name = re.sub('^_+', '', cf_name)\n                cls._table_name = cf_name\n        return cls._table_name\n\n    def _set_column_value(self, name, value):\n        \"\"\"Function to change a column value without changing the value manager states\"\"\"\n        self._values[name].value = value\n\n    def validate(self):\n        \"\"\"\n        Cleans and validates the field values\n        \"\"\"\n        for name, col in self._columns.items():\n            v = getattr(self, name)\n            if v is None and (not self._values[name].explicit) and col.has_default:\n                v = col.get_default()\n            val = col.validate(v)\n            self._set_column_value(name, val)\n\n    def __iter__(self):\n        \"\"\" Iterate over column ids. \"\"\"\n        for column_id in self._columns.keys():\n            yield column_id\n\n    def __getitem__(self, key):\n        \"\"\" Returns column's value. \"\"\"\n        if not isinstance(key, str):\n            raise TypeError\n        if key not in self._columns.keys():\n            raise KeyError\n        return getattr(self, key)\n\n    def __setitem__(self, key, val):\n        \"\"\" Sets a column's value. \"\"\"\n        if not isinstance(key, str):\n            raise TypeError\n        if key not in self._columns.keys():\n            raise KeyError\n        return setattr(self, key, val)\n\n    def __len__(self):\n        \"\"\"\n        Returns the number of columns defined on that model.\n        \"\"\"\n        try:\n            return self._len\n        except:\n            self._len = len(self._columns.keys())\n            return self._len\n\n    def keys(self):\n        \"\"\" Returns a list of column IDs. \"\"\"\n        return [k for k in self]\n\n    def values(self):\n        \"\"\" Returns list of column values. \"\"\"\n        return [self[k] for k in self]\n\n    def items(self):\n        \"\"\" Returns a list of column ID/value tuples. \"\"\"\n        return [(k, self[k]) for k in self]\n\n    def _as_dict(self):\n        \"\"\" Returns a map of column names to cleaned values \"\"\"\n        values = self._dynamic_columns or {}\n        for name, col in self._columns.items():\n            values[name] = col.to_database(getattr(self, name, None))\n        return values\n\n    @classmethod\n    def create(cls, **kwargs):\n        \"\"\"\n        Create an instance of this model in the database.\n\n        Takes the model column values as keyword arguments. Setting a value to\n        `None` is equivalent to running a CQL `DELETE` on that column.\n\n        Returns the instance.\n        \"\"\"\n        extra_columns = set(kwargs.keys()) - set(cls._columns.keys())\n        if extra_columns:\n            raise ValidationError('Incorrect columns passed: {0}'.format(extra_columns))\n        return cls.objects.create(**kwargs)\n\n    @classmethod\n    def all(cls):\n        \"\"\"\n        Returns a queryset representing all stored objects\n\n        This is a pass-through to the model objects().all()\n        \"\"\"\n        return cls.objects.all()\n\n    @classmethod\n    def filter(cls, *args, **kwargs):\n        \"\"\"\n        Returns a queryset based on filter parameters.\n\n        This is a pass-through to the model objects().:method:`~cqlengine.queries.filter`.\n        \"\"\"\n        return cls.objects.filter(*args, **kwargs)\n\n    @classmethod\n    def get(cls, *args, **kwargs):\n        \"\"\"\n        Returns a single object based on the passed filter constraints.\n\n        This is a pass-through to the model objects().:method:`~cqlengine.queries.get`.\n        \"\"\"\n        return cls.objects.get(*args, **kwargs)\n\n    def timeout(self, timeout):\n        \"\"\"\n        Sets a timeout for use in :meth:`~.save`, :meth:`~.update`, and :meth:`~.delete`\n        operations\n        \"\"\"\n        assert self._batch is None, 'Setting both timeout and batch is not supported'\n        self._timeout = timeout\n        return self\n\n    def save(self):\n        \"\"\"\n        Saves an object to the database.\n\n        .. code-block:: python\n\n            #create a person instance\n            person = Person(first_name='Kimberly', last_name='Eggleston')\n            #saves it to Cassandra\n            person.save()\n        \"\"\"\n        if self._is_polymorphic:\n            if self._is_polymorphic_base:\n                raise PolymorphicModelException('cannot save polymorphic base model')\n            else:\n                setattr(self, self._discriminator_column_name, self.__discriminator_value__)\n        self.validate()\n        self.__dmlquery__(self.__class__, self, batch=self._batch, ttl=self._ttl, timestamp=self._timestamp, consistency=self.__consistency__, if_not_exists=self._if_not_exists, conditional=self._conditional, timeout=self._timeout, if_exists=self._if_exists).save()\n        self._set_persisted()\n        self._timestamp = None\n        return self\n\n    def update(self, **values):\n        \"\"\"\n        Performs an update on the model instance. You can pass in values to set on the model\n        for updating, or you can call without values to execute an update against any modified\n        fields. If no fields on the model have been modified since loading, no query will be\n        performed. Model validation is performed normally. Setting a value to `None` is\n        equivalent to running a CQL `DELETE` on that column.\n\n        It is possible to do a blind update, that is, to update a field without having first selected the object out of the database.\n        See :ref:`Blind Updates <blind_updates>`\n        \"\"\"\n        for column_id, v in values.items():\n            col = self._columns.get(column_id)\n            if col is None:\n                raise ValidationError('{0}.{1} has no column named: {2}'.format(self.__module__, self.__class__.__name__, column_id))\n            if col.is_primary_key:\n                current_value = getattr(self, column_id)\n                if v != current_value:\n                    raise ValidationError(\"Cannot apply update to primary key '{0}' for {1}.{2}\".format(column_id, self.__module__, self.__class__.__name__))\n            setattr(self, column_id, v)\n        if self._is_polymorphic:\n            if self._is_polymorphic_base:\n                raise PolymorphicModelException('cannot update polymorphic base model')\n            else:\n                setattr(self, self._discriminator_column_name, self.__discriminator_value__)\n        self.validate()\n        self.__dmlquery__(self.__class__, self, batch=self._batch, ttl=self._ttl, timestamp=self._timestamp, consistency=self.__consistency__, conditional=self._conditional, timeout=self._timeout, if_exists=self._if_exists).update()\n        self._set_persisted()\n        self._timestamp = None\n        return self\n\n    def delete(self):\n        \"\"\"\n        Deletes the object from the database\n        \"\"\"\n        self.__dmlquery__(self.__class__, self, batch=self._batch, timestamp=self._timestamp, consistency=self.__consistency__, timeout=self._timeout, conditional=self._conditional, if_exists=self._if_exists).delete()\n\n    def get_changed_columns(self):\n        \"\"\"\n        Returns a list of the columns that have been updated since instantiation or save\n        \"\"\"\n        return [k for k, v in self._values.items() if v.changed]\n\n    @classmethod\n    def _class_batch(cls, batch):\n        return cls.objects.batch(batch)\n\n    def _inst_batch(self, batch):\n        assert self._timeout is connection.NOT_SET, 'Setting both timeout and batch is not supported'\n        if self._connection:\n            raise CQLEngineException('Cannot specify a connection on model in batch mode.')\n        self._batch = batch\n        return self\n    batch = hybrid_classmethod(_class_batch, _inst_batch)\n\n    @classmethod\n    def _class_get_connection(cls):\n        return cls.__connection__\n\n    def _inst_get_connection(self):\n        return self._connection or self.__connection__\n    _get_connection = hybrid_classmethod(_class_get_connection, _inst_get_connection)\n\nclass ModelMetaClass(type):\n\nclass Model(BaseModel, metaclass=ModelMetaClass):\n    __abstract__ = True\n    \"\\n    *Optional.* Indicates that this model is only intended to be used as a base class for other models.\\n    You can't create tables for abstract models, but checks around schema validity are skipped during class construction.\\n    \"\n    __table_name__ = None\n    \"\\n    *Optional.* Sets the name of the CQL table for this model. If left blank, the table name will be the name of the model, with it's module name as it's prefix. Manually defined table names are not inherited.\\n    \"\n    __table_name_case_sensitive__ = False\n    '\\n    *Optional.* By default, __table_name__ is case insensitive. Set this to True if you want to preserve the case sensitivity.\\n    '\n    __keyspace__ = None\n    '\\n    Sets the name of the keyspace used by this model.\\n    '\n    __connection__ = None\n    '\\n    Sets the name of the default connection used by this model.\\n    '\n    __options__ = None\n    '\\n    *Optional* Table options applied with this model\\n\\n    (e.g. compaction, default ttl, cache settings, tec.)\\n    '\n    __discriminator_value__ = None\n    '\\n    *Optional* Specifies a value for the discriminator column when using model inheritance.\\n    '\n    __compute_routing_key__ = True\n    '\\n    *Optional* Setting False disables computing the routing key for TokenAwareRouting\\n    '",
    "cassandra/__init__.py": "from enum import Enum\nimport logging\n\nclass NullHandler(logging.Handler):\n\n    def emit(self, record):\n        pass\nlogging.getLogger('cassandra').addHandler(NullHandler())\n__version_info__ = (3, 28, 0)\n__version__ = '.'.join(map(str, __version_info__))\n\nclass ConsistencyLevel(object):\n    \"\"\"\n    Spcifies how many replicas must respond for an operation to be considered\n    a success.  By default, ``ONE`` is used for all operations.\n    \"\"\"\n    ANY = 0\n    '\\n    Only requires that one replica receives the write *or* the coordinator\\n    stores a hint to replay later. Valid only for writes.\\n    '\n    ONE = 1\n    '\\n    Only one replica needs to respond to consider the operation a success\\n    '\n    TWO = 2\n    '\\n    Two replicas must respond to consider the operation a success\\n    '\n    THREE = 3\n    '\\n    Three replicas must respond to consider the operation a success\\n    '\n    QUORUM = 4\n    '\\n    ``ceil(RF/2) + 1`` replicas must respond to consider the operation a success\\n    '\n    ALL = 5\n    '\\n    All replicas must respond to consider the operation a success\\n    '\n    LOCAL_QUORUM = 6\n    '\\n    Requires a quorum of replicas in the local datacenter\\n    '\n    EACH_QUORUM = 7\n    '\\n    Requires a quorum of replicas in each datacenter\\n    '\n    SERIAL = 8\n    \"\\n    For conditional inserts/updates that utilize Cassandra's lightweight\\n    transactions, this requires consensus among all replicas for the\\n    modified data.\\n    \"\n    LOCAL_SERIAL = 9\n    '\\n    Like :attr:`~ConsistencyLevel.SERIAL`, but only requires consensus\\n    among replicas in the local datacenter.\\n    '\n    LOCAL_ONE = 10\n    '\\n    Sends a request only to replicas in the local datacenter and waits for\\n    one response.\\n    '\n\n    @staticmethod\n    def is_serial(cl):\n        return cl == ConsistencyLevel.SERIAL or cl == ConsistencyLevel.LOCAL_SERIAL\nConsistencyLevel.value_to_name = {ConsistencyLevel.ANY: 'ANY', ConsistencyLevel.ONE: 'ONE', ConsistencyLevel.TWO: 'TWO', ConsistencyLevel.THREE: 'THREE', ConsistencyLevel.QUORUM: 'QUORUM', ConsistencyLevel.ALL: 'ALL', ConsistencyLevel.LOCAL_QUORUM: 'LOCAL_QUORUM', ConsistencyLevel.EACH_QUORUM: 'EACH_QUORUM', ConsistencyLevel.SERIAL: 'SERIAL', ConsistencyLevel.LOCAL_SERIAL: 'LOCAL_SERIAL', ConsistencyLevel.LOCAL_ONE: 'LOCAL_ONE'}\nConsistencyLevel.name_to_value = {'ANY': ConsistencyLevel.ANY, 'ONE': ConsistencyLevel.ONE, 'TWO': ConsistencyLevel.TWO, 'THREE': ConsistencyLevel.THREE, 'QUORUM': ConsistencyLevel.QUORUM, 'ALL': ConsistencyLevel.ALL, 'LOCAL_QUORUM': ConsistencyLevel.LOCAL_QUORUM, 'EACH_QUORUM': ConsistencyLevel.EACH_QUORUM, 'SERIAL': ConsistencyLevel.SERIAL, 'LOCAL_SERIAL': ConsistencyLevel.LOCAL_SERIAL, 'LOCAL_ONE': ConsistencyLevel.LOCAL_ONE}\n\ndef consistency_value_to_name(value):\n    return ConsistencyLevel.value_to_name[value] if value is not None else 'Not Set'\n\nclass ProtocolVersion(object):\n    \"\"\"\n    Defines native protocol versions supported by this driver.\n    \"\"\"\n    V1 = 1\n    '\\n    v1, supported in Cassandra 1.2-->2.2\\n    '\n    V2 = 2\n    '\\n    v2, supported in Cassandra 2.0-->2.2;\\n    added support for lightweight transactions, batch operations, and automatic query paging.\\n    '\n    V3 = 3\n    '\\n    v3, supported in Cassandra 2.1-->3.x+;\\n    added support for protocol-level client-side timestamps (see :attr:`.Session.use_client_timestamp`),\\n    serial consistency levels for :class:`~.BatchStatement`, and an improved connection pool.\\n    '\n    V4 = 4\n    '\\n    v4, supported in Cassandra 2.2-->3.x+;\\n    added a number of new types, server warnings, new failure messages, and custom payloads. Details in the\\n    `project docs <https://github.com/apache/cassandra/blob/trunk/doc/native_protocol_v4.spec>`_\\n    '\n    V5 = 5\n    '\\n    v5, in beta from 3.x+. Finalised in 4.0-beta5\\n    '\n    V6 = 6\n    '\\n    v6, in beta from 4.0-beta5\\n    '\n    DSE_V1 = 65\n    '\\n    DSE private protocol v1, supported in DSE 5.1+\\n    '\n    DSE_V2 = 66\n    '\\n    DSE private protocol v2, supported in DSE 6.0+\\n    '\n    SUPPORTED_VERSIONS = (DSE_V2, DSE_V1, V6, V5, V4, V3, V2, V1)\n    '\\n    A tuple of all supported protocol versions\\n    '\n    BETA_VERSIONS = (V6,)\n    '\\n    A tuple of all beta protocol versions\\n    '\n    MIN_SUPPORTED = min(SUPPORTED_VERSIONS)\n    '\\n    Minimum protocol version supported by this driver.\\n    '\n    MAX_SUPPORTED = max(SUPPORTED_VERSIONS)\n    '\\n    Maximum protocol version supported by this driver.\\n    '\n\n    @classmethod\n    def get_lower_supported(cls, previous_version):\n        \"\"\"\n        Return the lower supported protocol version. Beta versions are omitted.\n        \"\"\"\n        try:\n            version = next((v for v in sorted(ProtocolVersion.SUPPORTED_VERSIONS, reverse=True) if v not in ProtocolVersion.BETA_VERSIONS and v < previous_version))\n        except StopIteration:\n            version = 0\n        return version\n\n    @classmethod\n    def uses_int_query_flags(cls, version):\n        return version >= cls.V5\n\n    @classmethod\n    def uses_prepare_flags(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def uses_prepared_metadata(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def uses_error_code_map(cls, version):\n        return version >= cls.V5\n\n    @classmethod\n    def uses_keyspace_flag(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def has_continuous_paging_support(cls, version):\n        return version >= cls.DSE_V1\n\n    @classmethod\n    def has_continuous_paging_next_pages(cls, version):\n        return version >= cls.DSE_V2\n\n    @classmethod\n    def has_checksumming_support(cls, version):\n        return cls.V5 <= version < cls.DSE_V1\n\nclass WriteType(object):\n    \"\"\"\n    For usage with :class:`.RetryPolicy`, this describe a type\n    of write operation.\n    \"\"\"\n    SIMPLE = 0\n    '\\n    A write to a single partition key. Such writes are guaranteed to be atomic\\n    and isolated.\\n    '\n    BATCH = 1\n    '\\n    A write to multiple partition keys that used the distributed batch log to\\n    ensure atomicity.\\n    '\n    UNLOGGED_BATCH = 2\n    '\\n    A write to multiple partition keys that did not use the distributed batch\\n    log. Atomicity for such writes is not guaranteed.\\n    '\n    COUNTER = 3\n    '\\n    A counter write (for one or multiple partition keys). Such writes should\\n    not be replayed in order to avoid overcount.\\n    '\n    BATCH_LOG = 4\n    '\\n    The initial write to the distributed batch log that Cassandra performs\\n    internally before a BATCH write.\\n    '\n    CAS = 5\n    '\\n    A lighweight-transaction write, such as \"DELETE ... IF EXISTS\".\\n    '\n    VIEW = 6\n    '\\n    This WriteType is only seen in results for requests that were unable to\\n    complete MV operations.\\n    '\n    CDC = 7\n    '\\n    This WriteType is only seen in results for requests that were unable to\\n    complete CDC operations.\\n    '\nWriteType.name_to_value = {'SIMPLE': WriteType.SIMPLE, 'BATCH': WriteType.BATCH, 'UNLOGGED_BATCH': WriteType.UNLOGGED_BATCH, 'COUNTER': WriteType.COUNTER, 'BATCH_LOG': WriteType.BATCH_LOG, 'CAS': WriteType.CAS, 'VIEW': WriteType.VIEW, 'CDC': WriteType.CDC}\nWriteType.value_to_name = {v: k for k, v in WriteType.name_to_value.items()}\n\nclass SchemaChangeType(object):\n    DROPPED = 'DROPPED'\n    CREATED = 'CREATED'\n    UPDATED = 'UPDATED'\n\nclass SchemaTargetType(object):\n    KEYSPACE = 'KEYSPACE'\n    TABLE = 'TABLE'\n    TYPE = 'TYPE'\n    FUNCTION = 'FUNCTION'\n    AGGREGATE = 'AGGREGATE'\n\nclass SignatureDescriptor(object):\n\n    def __init__(self, name, argument_types):\n        self.name = name\n        self.argument_types = argument_types\n\n    @property\n    def signature(self):\n        \"\"\"\n        function signature string in the form 'name([type0[,type1[...]]])'\n\n        can be used to uniquely identify overloaded function names within a keyspace\n        \"\"\"\n        return self.format_signature(self.name, self.argument_types)\n\n    @staticmethod\n    def format_signature(name, argument_types):\n        return '%s(%s)' % (name, ','.join((t for t in argument_types)))\n\n    def __repr__(self):\n        return '%s(%s, %s)' % (self.__class__.__name__, self.name, self.argument_types)\n\nclass UserFunctionDescriptor(SignatureDescriptor):\n    \"\"\"\n    Describes a User function by name and argument signature\n    \"\"\"\n    name = None\n    '\\n    name of the function\\n    '\n    argument_types = None\n    '\\n    Ordered list of CQL argument type names comprising the type signature\\n    '\n\nclass UserAggregateDescriptor(SignatureDescriptor):\n    \"\"\"\n    Describes a User aggregate function by name and argument signature\n    \"\"\"\n    name = None\n    '\\n    name of the aggregate\\n    '\n    argument_types = None\n    '\\n    Ordered list of CQL argument type names comprising the type signature\\n    '\n\nclass DriverException(Exception):\n    \"\"\"\n    Base for all exceptions explicitly raised by the driver.\n    \"\"\"\n    pass\n\nclass RequestExecutionException(DriverException):\n    \"\"\"\n    Base for request execution exceptions returned from the server.\n    \"\"\"\n    pass\n\nclass Unavailable(RequestExecutionException):\n    \"\"\"\n    There were not enough live replicas to satisfy the requested consistency\n    level, so the coordinator node immediately failed the request without\n    forwarding it to any replicas.\n    \"\"\"\n    consistency = None\n    ' The requested :class:`ConsistencyLevel` '\n    required_replicas = None\n    ' The number of replicas that needed to be live to complete the operation '\n    alive_replicas = None\n    ' The number of replicas that were actually alive '\n\n    def __init__(self, summary_message, consistency=None, required_replicas=None, alive_replicas=None):\n        self.consistency = consistency\n        self.required_replicas = required_replicas\n        self.alive_replicas = alive_replicas\n        Exception.__init__(self, summary_message + ' info=' + repr({'consistency': consistency_value_to_name(consistency), 'required_replicas': required_replicas, 'alive_replicas': alive_replicas}))\n\nclass Timeout(RequestExecutionException):\n    \"\"\"\n    Replicas failed to respond to the coordinator node before timing out.\n    \"\"\"\n    consistency = None\n    ' The requested :class:`ConsistencyLevel` '\n    required_responses = None\n    ' The number of required replica responses '\n    received_responses = None\n    '\\n    The number of replicas that responded before the coordinator timed out\\n    the operation\\n    '\n\n    def __init__(self, summary_message, consistency=None, required_responses=None, received_responses=None, **kwargs):\n        self.consistency = consistency\n        self.required_responses = required_responses\n        self.received_responses = received_responses\n        if 'write_type' in kwargs:\n            kwargs['write_type'] = WriteType.value_to_name[kwargs['write_type']]\n        info = {'consistency': consistency_value_to_name(consistency), 'required_responses': required_responses, 'received_responses': received_responses}\n        info.update(kwargs)\n        Exception.__init__(self, summary_message + ' info=' + repr(info))\n\nclass ReadTimeout(Timeout):\n    \"\"\"\n    A subclass of :exc:`Timeout` for read operations.\n\n    This indicates that the replicas failed to respond to the coordinator\n    node before the configured timeout. This timeout is configured in\n    ``cassandra.yaml`` with the ``read_request_timeout_in_ms``\n    and ``range_request_timeout_in_ms`` options.\n    \"\"\"\n    data_retrieved = None\n    '\\n    A boolean indicating whether the requested data was retrieved\\n    by the coordinator from any replicas before it timed out the\\n    operation\\n    '\n\n    def __init__(self, message, data_retrieved=None, **kwargs):\n        Timeout.__init__(self, message, **kwargs)\n        self.data_retrieved = data_retrieved\n\nclass WriteTimeout(Timeout):\n    \"\"\"\n    A subclass of :exc:`Timeout` for write operations.\n\n    This indicates that the replicas failed to respond to the coordinator\n    node before the configured timeout. This timeout is configured in\n    ``cassandra.yaml`` with the ``write_request_timeout_in_ms``\n    option.\n    \"\"\"\n    write_type = None\n    '\\n    The type of write operation, enum on :class:`~cassandra.policies.WriteType`\\n    '\n\n    def __init__(self, message, write_type=None, **kwargs):\n        kwargs['write_type'] = write_type\n        Timeout.__init__(self, message, **kwargs)\n        self.write_type = write_type\n\nclass CDCWriteFailure(RequestExecutionException):\n    \"\"\"\n    Hit limit on data in CDC folder, writes are rejected\n    \"\"\"\n\n    def __init__(self, message):\n        Exception.__init__(self, message)\n\nclass CoordinationFailure(RequestExecutionException):\n    \"\"\"\n    Replicas sent a failure to the coordinator.\n    \"\"\"\n    consistency = None\n    ' The requested :class:`ConsistencyLevel` '\n    required_responses = None\n    ' The number of required replica responses '\n    received_responses = None\n    '\\n    The number of replicas that responded before the coordinator timed out\\n    the operation\\n    '\n    failures = None\n    '\\n    The number of replicas that sent a failure message\\n    '\n    error_code_map = None\n    '\\n    A map of inet addresses to error codes representing replicas that sent\\n    a failure message.  Only set when `protocol_version` is 5 or higher.\\n    '\n\n    def __init__(self, summary_message, consistency=None, required_responses=None, received_responses=None, failures=None, error_code_map=None):\n        self.consistency = consistency\n        self.required_responses = required_responses\n        self.received_responses = received_responses\n        self.failures = failures\n        self.error_code_map = error_code_map\n        info_dict = {'consistency': consistency_value_to_name(consistency), 'required_responses': required_responses, 'received_responses': received_responses, 'failures': failures}\n        if error_code_map is not None:\n            formatted_map = dict(((addr, '0x%04x' % err_code) for addr, err_code in error_code_map.items()))\n            info_dict['error_code_map'] = formatted_map\n        Exception.__init__(self, summary_message + ' info=' + repr(info_dict))\n\nclass ReadFailure(CoordinationFailure):\n    \"\"\"\n    A subclass of :exc:`CoordinationFailure` for read operations.\n\n    This indicates that the replicas sent a failure message to the coordinator.\n    \"\"\"\n    data_retrieved = None\n    '\\n    A boolean indicating whether the requested data was retrieved\\n    by the coordinator from any replicas before it timed out the\\n    operation\\n    '\n\n    def __init__(self, message, data_retrieved=None, **kwargs):\n        CoordinationFailure.__init__(self, message, **kwargs)\n        self.data_retrieved = data_retrieved\n\nclass WriteFailure(CoordinationFailure):\n    \"\"\"\n    A subclass of :exc:`CoordinationFailure` for write operations.\n\n    This indicates that the replicas sent a failure message to the coordinator.\n    \"\"\"\n    write_type = None\n    '\\n    The type of write operation, enum on :class:`~cassandra.policies.WriteType`\\n    '\n\n    def __init__(self, message, write_type=None, **kwargs):\n        CoordinationFailure.__init__(self, message, **kwargs)\n        self.write_type = write_type\n\nclass FunctionFailure(RequestExecutionException):\n    \"\"\"\n    User Defined Function failed during execution\n    \"\"\"\n    keyspace = None\n    '\\n    Keyspace of the function\\n    '\n    function = None\n    '\\n    Name of the function\\n    '\n    arg_types = None\n    '\\n    List of argument type names of the function\\n    '\n\n    def __init__(self, summary_message, keyspace, function, arg_types):\n        self.keyspace = keyspace\n        self.function = function\n        self.arg_types = arg_types\n        Exception.__init__(self, summary_message)\n\nclass RequestValidationException(DriverException):\n    \"\"\"\n    Server request validation failed\n    \"\"\"\n    pass\n\nclass ConfigurationException(RequestValidationException):\n    \"\"\"\n    Server indicated request errro due to current configuration\n    \"\"\"\n    pass\n\nclass AlreadyExists(ConfigurationException):\n    \"\"\"\n    An attempt was made to create a keyspace or table that already exists.\n    \"\"\"\n    keyspace = None\n    '\\n    The name of the keyspace that already exists, or, if an attempt was\\n    made to create a new table, the keyspace that the table is in.\\n    '\n    table = None\n    '\\n    The name of the table that already exists, or, if an attempt was\\n    make to create a keyspace, :const:`None`.\\n    '\n\n    def __init__(self, keyspace=None, table=None):\n        if table:\n            message = \"Table '%s.%s' already exists\" % (keyspace, table)\n        else:\n            message = \"Keyspace '%s' already exists\" % (keyspace,)\n        Exception.__init__(self, message)\n        self.keyspace = keyspace\n        self.table = table\n\nclass InvalidRequest(RequestValidationException):\n    \"\"\"\n    A query was made that was invalid for some reason, such as trying to set\n    the keyspace for a connection to a nonexistent keyspace.\n    \"\"\"\n    pass\n\nclass Unauthorized(RequestValidationException):\n    \"\"\"\n    The current user is not authorized to perform the requested operation.\n    \"\"\"\n    pass\n\nclass AuthenticationFailed(DriverException):\n    \"\"\"\n    Failed to authenticate.\n    \"\"\"\n    pass\n\nclass OperationTimedOut(DriverException):\n    \"\"\"\n    The operation took longer than the specified (client-side) timeout\n    to complete.  This is not an error generated by Cassandra, only\n    the driver.\n    \"\"\"\n    errors = None\n    '\\n    A dict of errors keyed by the :class:`~.Host` against which they occurred.\\n    '\n    last_host = None\n    '\\n    The last :class:`~.Host` this operation was attempted against.\\n    '\n\n    def __init__(self, errors=None, last_host=None):\n        self.errors = errors\n        self.last_host = last_host\n        message = 'errors=%s, last_host=%s' % (self.errors, self.last_host)\n        Exception.__init__(self, message)\n\nclass UnsupportedOperation(DriverException):\n    \"\"\"\n    An attempt was made to use a feature that is not supported by the\n    selected protocol version.  See :attr:`Cluster.protocol_version`\n    for more details.\n    \"\"\"\n    pass\n\nclass UnresolvableContactPoints(DriverException):\n    \"\"\"\n    The driver was unable to resolve any provided hostnames.\n\n    Note that this is *not* raised when a :class:`.Cluster` is created with no\n    contact points, only when lookup fails for all hosts\n    \"\"\"\n    pass\n\nclass OperationType(Enum):\n    Read = 0\n    Write = 1\n\nclass RateLimitReached(ConfigurationException):\n    \"\"\"\n    Rate limit was exceeded for a partition affected by the request.\n    \"\"\"\n    op_type = None\n    rejected_by_coordinator = False\n\n    def __init__(self, op_type=None, rejected_by_coordinator=False):\n        self.op_type = op_type\n        self.rejected_by_coordinator = rejected_by_coordinator\n        message = f'[request_error_rate_limit_reached OpType={op_type.name} RejectedByCoordinator={rejected_by_coordinator}]'\n        Exception.__init__(self, message)",
    "cassandra/datastax/graph/types.py": "__all__ = ['Element', 'Vertex', 'Edge', 'VertexProperty', 'Path', 'T']\n\nclass Element(object):\n    element_type = None\n    _attrs = ('id', 'label', 'type', 'properties')\n\n    def __init__(self, id, label, type, properties):\n        if type != self.element_type:\n            raise TypeError('Attempted to create %s from %s element', (type, self.element_type))\n        self.id = id\n        self.label = label\n        self.type = type\n        self.properties = self._extract_properties(properties)\n\n    @staticmethod\n    def _extract_properties(properties):\n        return dict(properties)\n\n    def __eq__(self, other):\n        return all((getattr(self, attr) == getattr(other, attr) for attr in self._attrs))\n\n    def __str__(self):\n        return str(dict(((k, getattr(self, k)) for k in self._attrs)))\n\nclass Vertex(Element):\n    \"\"\"\n    Represents a Vertex element from a graph query.\n\n    Vertex ``properties`` are extracted into a ``dict`` of property names to list of :class:`~VertexProperty` (list\n    because they are always encoded that way, and sometimes have multiple cardinality; VertexProperty because sometimes\n    the properties themselves have property maps).\n    \"\"\"\n    element_type = 'vertex'\n\n    @staticmethod\n    def _extract_properties(properties):\n        return dict(((k, [VertexProperty(k, p['value'], p.get('properties')) for p in v]) for k, v in properties.items()))\n\n    def __repr__(self):\n        properties = dict(((name, [{'label': prop.label, 'value': prop.value, 'properties': prop.properties} for prop in prop_list]) for name, prop_list in self.properties.items()))\n        return '%s(%r, %r, %r, %r)' % (self.__class__.__name__, self.id, self.label, self.type, properties)\n\nclass VertexProperty(object):\n    \"\"\"\n    Vertex properties have a top-level value and an optional ``dict`` of properties.\n    \"\"\"\n    label = None\n    '\\n    label of the property\\n    '\n    value = None\n    '\\n    Value of the property\\n    '\n    properties = None\n    '\\n    dict of properties attached to the property\\n    '\n\n    def __init__(self, label, value, properties=None):\n        self.label = label\n        self.value = value\n        self.properties = properties or {}\n\n    def __eq__(self, other):\n        return isinstance(other, VertexProperty) and self.label == other.label and (self.value == other.value) and (self.properties == other.properties)\n\n    def __repr__(self):\n        return '%s(%r, %r, %r)' % (self.__class__.__name__, self.label, self.value, self.properties)\n\nclass Edge(Element):\n    \"\"\"\n    Represents an Edge element from a graph query.\n\n    Attributes match initializer parameters.\n    \"\"\"\n    element_type = 'edge'\n    _attrs = Element._attrs + ('inV', 'inVLabel', 'outV', 'outVLabel')\n\n    def __init__(self, id, label, type, properties, inV, inVLabel, outV, outVLabel):\n        super(Edge, self).__init__(id, label, type, properties)\n        self.inV = inV\n        self.inVLabel = inVLabel\n        self.outV = outV\n        self.outVLabel = outVLabel\n\n    def __repr__(self):\n        return '%s(%r, %r, %r, %r, %r, %r, %r, %r)' % (self.__class__.__name__, self.id, self.label, self.type, self.properties, self.inV, self.inVLabel, self.outV, self.outVLabel)\n\nclass Path(object):\n    \"\"\"\n    Represents a graph path.\n\n    Labels list is taken verbatim from the results.\n\n    Objects are either :class:`~.Result` or :class:`~.Vertex`/:class:`~.Edge` for recognized types\n    \"\"\"\n    labels = None\n    '\\n    List of labels in the path\\n    '\n    objects = None\n    '\\n    List of objects in the path\\n    '\n\n    def __init__(self, labels, objects):\n        from cassandra.datastax.graph.query import _graph_object_sequence\n        self.labels = labels\n        self.objects = list(_graph_object_sequence(objects))\n\n    def __eq__(self, other):\n        return self.labels == other.labels and self.objects == other.objects\n\n    def __str__(self):\n        return str({'labels': self.labels, 'objects': self.objects})\n\n    def __repr__(self):\n        return '%s(%r, %r)' % (self.__class__.__name__, self.labels, [o.value for o in self.objects])\n\nclass T(object):\n    \"\"\"\n    Represents a collection of tokens for more concise Traversal definitions.\n    \"\"\"\n    name = None\n    val = None\n    id = None\n    '\\n    '\n    key = None\n    '\\n    '\n    label = None\n    '\\n    '\n    value = None\n    '\\n    '\n\n    def __init__(self, name, val):\n        self.name = name\n        self.val = val\n\n    def __str__(self):\n        return self.name\n\n    def __repr__(self):\n        return 'T.%s' % (self.name,)\nT.id = T('id', 1)\nT.id_ = T('id_', 2)\nT.key = T('key', 3)\nT.label = T('label', 4)\nT.value = T('value', 5)\nT.name_to_value = {'id': T.id, 'id_': T.id_, 'key': T.key, 'label': T.label, 'value': T.value}"
  }
}