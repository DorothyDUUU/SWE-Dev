{
  "dir_path": "/app/skforecast",
  "package_name": "skforecast",
  "sample_name": "skforecast-test_check_X_numpy_ndarray_1d",
  "src_dir": "skforecast/",
  "test_dir": "tests/",
  "test_file": "skforecast/preprocessing/tests/tests_preprocessing/test_check_X_numpy_ndarray_1d.py",
  "test_code": "# Unit test _check_X_numpy_ndarray_1d\n# ==============================================================================\nimport re\nimport numpy as np\nimport pandas as pd\nimport pytest\nfrom skforecast.preprocessing import TimeSeriesDifferentiator\n\n\ndef test_TypeError_decorator_check_X_numpy_ndarray_1d_when_X_not_numpy_ndarray():\n    \"\"\"\n    Test TypeError raised when X is not a numpy ndarray.\n    \"\"\"\n    y = [1, 2, 3, 4, 5]\n    differentiator = TimeSeriesDifferentiator(order=1)    \n\n    err_msg = re.escape(\n        (\"'X' must be a numpy ndarray. Found <class 'list'>.\")\n    )\n    with pytest.raises(TypeError, match = err_msg): \n        differentiator.fit(y)\n    with pytest.raises(TypeError, match = err_msg): \n        differentiator.transform(X=y)\n    with pytest.raises(TypeError, match = err_msg): \n        differentiator.fit_transform(y)\n    with pytest.raises(TypeError, match = err_msg): \n        differentiator.inverse_transform(y)\n    with pytest.raises(TypeError, match = err_msg): \n        differentiator.inverse_transform_next_window(y)\n\n\ndef test_ValueError_decorator_check_X_numpy_ndarray_1d_when_X_not_1D_numpy_ndarray():\n    \"\"\"\n    Test ValueError raised when X is not a 1D numpy ndarray.\n    \"\"\"\n    y = pd.Series(np.array([1, 2, 3, 4, 5])).to_frame().to_numpy()\n    differentiator = TimeSeriesDifferentiator(order=1)    \n\n    err_msg = re.escape(\n        (\"'X' must be a 1D array. Found 2 dimensions.\")\n    )\n    with pytest.raises(ValueError, match = err_msg): \n        differentiator.fit(y)\n    with pytest.raises(ValueError, match = err_msg): \n        differentiator.transform(X=y)\n    with pytest.raises(ValueError, match = err_msg): \n        differentiator.fit_transform(y)\n    with pytest.raises(ValueError, match = err_msg): \n        differentiator.inverse_transform(y)\n",
  "GT_file_code": {
    "skforecast/preprocessing/preprocessing.py": "################################################################################\n#                           skforecast.preprocessing                           #\n#                                                                              #\n# This work by skforecast team is licensed under the BSD 3-Clause License.     #\n################################################################################\n# coding=utf-8\n\nfrom typing import Any, Union, Optional\nfrom typing_extensions import Self\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom sklearn.base import BaseEstimator\nfrom sklearn.base import TransformerMixin\nfrom sklearn.exceptions import NotFittedError\nfrom ..exceptions import MissingValuesWarning\nfrom numba import njit\n\n\ndef _check_X_numpy_ndarray_1d(ensure_1d=True):\n    \"\"\"\n    This decorator checks if the argument X is a numpy ndarray with 1 dimension.\n\n    Parameters\n    ----------\n    ensure_1d : bool, default=True\n        Whether to ensure if X is a 1D numpy array.\n    \n    Returns\n    -------\n    decorator : Callable\n        A decorator function.\n\n    \"\"\"\n\n    def decorator(func):\n        def wrapper(self, *args, **kwargs):\n\n            if args:\n                X = args[0] \n            elif 'X' in kwargs:\n                X = kwargs['X']\n            else:\n                raise ValueError(\"Methods must be called with 'X' as argument.\")\n\n            if not isinstance(X, np.ndarray):\n                raise TypeError(f\"'X' must be a numpy ndarray. Found {type(X)}.\")\n            if ensure_1d and not X.ndim == 1:\n                raise ValueError(f\"'X' must be a 1D array. Found {X.ndim} dimensions.\")\n            \n            result = func(self, *args, **kwargs)\n            \n            return result\n        \n        return wrapper\n    \n    return decorator\n\n\nclass TimeSeriesDifferentiator(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transforms a time series into a differentiated time series of a specified order\n    and provides functionality to revert the differentiation. \n    \n    When using a `direct` module Forecaster, the model in step 1 must be \n    used if you want to reverse the differentiation of the training time \n    series with the `inverse_transform_training` method.\n\n    Parameters\n    ----------\n    order : int\n        The order of differentiation to be applied.\n    window_size : int, default None\n        The window size used by the forecaster. This is required to revert the \n        differentiation for the target variable `y` or its predicted values.\n\n    Attributes\n    ----------\n    order : int\n        The order of differentiation.\n    initial_values : list\n        List with the first value of the time series before each differentiation.\n        If `order = 2`, first value correspond with the first value of the original\n        time series and the second value correspond with the first value of the\n        differentiated time series of order 1. These values are necessary to \n        revert the differentiation and reconstruct the original time series.\n    pre_train_values : list\n        List with the first training value of the time series before each differentiation.\n        For `order = 1`, the value correspond with the last value of the window used to\n        create the predictors. For order > 1, the value correspond with the first\n        value of the differentiated time series prior to the next differentiation.\n        These values are necessary to revert the differentiation and reconstruct the\n        training time series.\n    last_values : list\n        List with the last value of the time series before each differentiation, \n        used to revert differentiation on subsequent data windows. If `order = 2`, \n        first value correspond with the last value of the original time series \n        and the second value correspond with the last value of the differentiated \n        time series of order 1. This is essential for correctly transforming a \n        time series that follows immediately after the series used to fit the \n        transformer.\n\n    \"\"\"\n\n    def __init__(\n        self, \n        order: int = 1,\n        window_size: int = None\n    ) -> None:\n\n        if not isinstance(order, (int, np.integer)):\n            raise TypeError(\n                f\"Parameter `order` must be an integer greater than 0. Found {type(order)}.\"\n            )\n        if order < 1:\n            raise ValueError(\n                f\"Parameter `order` must be an integer greater than 0. Found {order}.\"\n            )\n\n        if window_size is not None:\n            if not isinstance(window_size, (int, np.integer)):\n                raise TypeError(\n                    f\"Parameter `window_size` must be an integer greater than 0. \"\n                    f\"Found {type(window_size)}.\"\n                )\n            if window_size < 1:\n                raise ValueError(\n                    f\"Parameter `window_size` must be an integer greater than 0. \"\n                    f\"Found {window_size}.\"\n                )\n\n        self.order = order\n        self.window_size = window_size\n        self.initial_values = []\n        self.pre_train_values = []\n        self.last_values = []\n\n    def __repr__(\n        self\n    ) -> str:\n        \"\"\"\n        Information displayed when printed.\n        \"\"\"\n            \n        return (\n            f\"TimeSeriesDifferentiator(order={self.order}, window_size={self.window_size})\"\n        )\n\n    @_check_X_numpy_ndarray_1d()\n    def fit(\n        self, \n        X: np.ndarray, \n        y: Any = None\n    ) -> Self:\n        \"\"\"\n        Fits the transformer. Stores the values needed to revert the \n        differentiation of different window of the time series, original \n        time series, training time series, and a time series that follows\n        immediately after the series used to fit the transformer.\n\n        Parameters\n        ----------\n        X : numpy ndarray\n            Time series to be differentiated.\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self : TimeSeriesDifferentiator\n\n        \"\"\"\n\n        self.initial_values = []\n        self.pre_train_values = []\n        self.last_values = []\n\n        for i in range(self.order):\n            if i == 0:\n                self.initial_values.append(X[0])\n                if self.window_size is not None:\n                    self.pre_train_values.append(X[self.window_size - self.order])\n                self.last_values.append(X[-1])\n                X_diff = np.diff(X, n=1)\n            else:\n                self.initial_values.append(X_diff[0])\n                if self.window_size is not None:\n                    self.pre_train_values.append(X_diff[self.window_size - self.order])\n                self.last_values.append(X_diff[-1])\n                X_diff = np.diff(X_diff, n=1)\n\n        return self\n\n    @_check_X_numpy_ndarray_1d()\n    def transform(\n        self, \n        X: np.ndarray, \n        y: Any = None\n    ) -> np.ndarray:\n        \"\"\"\n        Transforms a time series into a differentiated time series of order n.\n\n        Parameters\n        ----------\n        X : numpy ndarray\n            Time series to be differentiated.\n        y : Ignored\n            Not used, present here for API consistency by convention.\n        \n        Returns\n        -------\n        X_diff : numpy ndarray\n            Differentiated time series. The length of the array is the same as\n            the original time series but the first n `order` values are nan.\n\n        \"\"\"\n\n        X_diff = np.diff(X, n=self.order)\n        X_diff = np.append((np.full(shape=self.order, fill_value=np.nan)), X_diff)\n\n        return X_diff\n\n    @_check_X_numpy_ndarray_1d()\n    def inverse_transform(\n        self, \n        X: np.ndarray, \n        y: Any = None\n    ) -> np.ndarray:\n        \"\"\"\n        Reverts the differentiation. To do so, the input array is assumed to be\n        the same time series used to fit the transformer but differentiated.\n\n        Parameters\n        ----------\n        X : numpy ndarray\n            Differentiated time series.\n        y : Ignored\n            Not used, present here for API consistency by convention.\n        \n        Returns\n        -------\n        X_diff : numpy ndarray\n            Reverted differentiated time series.\n        \n        \"\"\"\n\n        # Remove initial nan values if present\n        X = X[np.argmax(~np.isnan(X)):]\n        for i in range(self.order):\n            if i == 0:\n                X_undiff = np.insert(X, 0, self.initial_values[-1])\n                X_undiff = np.cumsum(X_undiff, dtype=float)\n            else:\n                X_undiff = np.insert(X_undiff, 0, self.initial_values[-(i + 1)])\n                X_undiff = np.cumsum(X_undiff, dtype=float)\n\n        return X_undiff\n\n    @_check_X_numpy_ndarray_1d()\n    def inverse_transform_training(\n        self, \n        X: np.ndarray, \n        y: Any = None\n    ) -> np.ndarray:\n        \"\"\"\n        Reverts the differentiation. To do so, the input array is assumed to be\n        the differentiated training time series generated with the original \n        time series used to fit the transformer.\n\n        When using a `direct` module Forecaster, the model in step 1 must be \n        used if you want to reverse the differentiation of the training time \n        series with the `inverse_transform_training` method.\n\n        Parameters\n        ----------\n        X : numpy ndarray\n            Differentiated time series.\n        y : Ignored\n            Not used, present here for API consistency by convention.\n        \n        Returns\n        -------\n        X_diff : numpy ndarray\n            Reverted differentiated time series.\n        \n        \"\"\"\n\n        if not self.pre_train_values:\n            raise ValueError(\n                \"The `window_size` parameter must be set before fitting the \"\n                \"transformer to revert the differentiation of the training \"\n                \"time series.\"\n            )\n\n        # Remove initial nan values if present\n        X = X[np.argmax(~np.isnan(X)):]\n        for i in range(self.order):\n            if i == 0:\n                X_undiff = np.insert(X, 0, self.pre_train_values[-1])\n                X_undiff = np.cumsum(X_undiff, dtype=float)\n            else:\n                X_undiff = np.insert(X_undiff, 0, self.pre_train_values[-(i + 1)])\n                X_undiff = np.cumsum(X_undiff, dtype=float)\n\n        # Remove initial values as they are not part of the training time series\n        X_undiff = X_undiff[self.order:]\n\n        return X_undiff\n\n    @_check_X_numpy_ndarray_1d(ensure_1d=False)\n    def inverse_transform_next_window(\n        self,\n        X: np.ndarray,\n        y: Any = None\n    ) -> np.ndarray:\n        \"\"\"\n        Reverts the differentiation. The input array `X` is assumed to be a \n        differentiated time series of order n that starts right after the\n        the time series used to fit the transformer.\n\n        Parameters\n        ----------\n        X : numpy ndarray\n            Differentiated time series. It is assumed o start right after\n            the time series used to fit the transformer.\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        X_undiff : numpy ndarray\n            Reverted differentiated time series.\n        \n        \"\"\"\n        \n        array_ndim = X.ndim\n        if array_ndim == 1:\n            X = X[:, np.newaxis]\n\n        # Remove initial rows with nan values if present\n        X = X[~np.isnan(X).any(axis=1)]\n\n        for i in range(self.order):\n            if i == 0:\n                X_undiff = np.cumsum(X, axis=0, dtype=float) + self.last_values[-1]\n            else:\n                X_undiff = np.cumsum(X_undiff, axis=0, dtype=float) + self.last_values[-(i + 1)]\n\n        if array_ndim == 1:\n            X_undiff = X_undiff.ravel()\n\n        return X_undiff\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of the TimeSeriesDifferentiator.\n        \n        Parameters\n        ----------\n        params : dict\n            A dictionary of the parameters to set.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        for param, value in params.items():\n            setattr(self, param, value)\n\n\ndef series_long_to_dict(\n    data: pd.DataFrame,\n    series_id: str,\n    index: str,\n    values: str,\n    freq: str,\n    suppress_warnings: bool = False\n) -> dict:\n    \"\"\"\n    Convert long format series to dictionary of pandas Series with frequency.\n    Input data must be a pandas DataFrame with columns for the series identifier,\n    time index, and values. The function will group the data by the series\n    identifier and convert the time index to a datetime index with the given\n    frequency.\n\n    Parameters\n    ----------\n    data: pandas DataFrame\n        Long format series.\n    series_id: str\n        Column name with the series identifier.\n    index: str\n        Column name with the time index.\n    values: str\n        Column name with the values.\n    freq: str\n        Frequency of the series.\n    suppress_warnings: bool, default `False`\n        If True, suppress warnings when a series is incomplete after setting the\n        frequency.\n\n    Returns\n    -------\n    series_dict: dict\n        Dictionary with the series.\n\n    \"\"\"\n\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"`data` must be a pandas DataFrame.\")\n\n    for col in [series_id, index, values]:\n        if col not in data.columns:\n            raise ValueError(f\"Column '{col}' not found in `data`.\")\n        \n    original_sizes = data.groupby(series_id).size()\n    series_dict = {}\n    for k, v in data.groupby(series_id):\n        series_dict[k] = v.set_index(index)[values].asfreq(freq).rename(k)\n        series_dict[k].index.name = None\n        if not suppress_warnings and len(series_dict[k]) != original_sizes[k]:\n            warnings.warn(\n                f\"Series '{k}' is incomplete. NaNs have been introduced after \"\n                f\"setting the frequency.\",\n                MissingValuesWarning\n            )\n\n    return series_dict\n\n\ndef exog_long_to_dict(\n    data: pd.DataFrame,\n    series_id: str,\n    index: str,\n    freq: str,\n    dropna: bool = False,\n    suppress_warnings: bool = False\n) -> dict:\n    \"\"\"\n    Convert long format exogenous variables to dictionary. Input data must be a\n    pandas DataFrame with columns for the series identifier, time index, and\n    exogenous variables. The function will group the data by the series identifier\n    and convert the time index to a datetime index with the given frequency.\n\n    Parameters\n    ----------\n    data: pandas DataFrame\n        Long format exogenous variables.\n    series_id: str\n        Column name with the series identifier.\n    index: str\n        Column name with the time index.\n    freq: str\n        Frequency of the series.\n    dropna: bool, default False\n        If True, drop columns with all values as NaN. This is useful when\n        there are series without some exogenous variables.\n    suppress_warnings: bool, default False\n        If True, suppress warnings when exog is incomplete after setting the\n        frequency.\n        \n    Returns\n    -------\n    exog_dict: dict\n        Dictionary with the exogenous variables.\n\n    \"\"\"\n\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"`data` must be a pandas DataFrame.\")\n\n    for col in [series_id, index]:\n        if col not in data.columns:\n            raise ValueError(f\"Column '{col}' not found in `data`.\")\n\n    original_sizes = data.groupby(series_id).size()\n    exog_dict = dict(tuple(data.groupby(series_id)))\n    exog_dict = {\n        k: v.set_index(index).asfreq(freq).drop(columns=series_id)\n        for k, v in exog_dict.items()\n    }\n\n    for k in exog_dict.keys():\n        exog_dict[k].index.name = None\n\n    if dropna:\n        exog_dict = {k: v.dropna(how=\"all\", axis=1) for k, v in exog_dict.items()}\n    else: \n        if not suppress_warnings:\n            for k, v in exog_dict.items():\n                if len(v) != original_sizes[k]:\n                    warnings.warn(\n                        f\"Exogenous variables for series '{k}' are incomplete. \"\n                        f\"NaNs have been introduced after setting the frequency.\",\n                        MissingValuesWarning\n                    )\n\n    return exog_dict\n\n\ndef create_datetime_features(\n    X: Union[pd.Series, pd.DataFrame],\n    features: Optional[list] = None,\n    encoding: str = \"cyclical\",\n    max_values: Optional[dict] = None,\n) -> pd.DataFrame:\n    \"\"\"\n    Extract datetime features from the DateTime index of a pandas DataFrame or Series.\n\n    Parameters\n    ----------\n    X : pandas Series, pandas DataFrame\n        Input DataFrame or Series with a datetime index.\n    features : list, default `None`\n        List of calendar features (strings) to extract from the index. When `None`,\n        the following features are extracted: 'year', 'month', 'week', 'day_of_week',\n        'day_of_month', 'day_of_year', 'weekend', 'hour', 'minute', 'second'.\n    encoding : str, default `'cyclical'`\n        Encoding method for the extracted features. Options are None, 'cyclical' or\n        'onehot'.\n    max_values : dict, default `None`\n        Dictionary of maximum values for the cyclical encoding of calendar features.\n        When `None`, the following values are used: {'month': 12, 'week': 52, \n        'day_of_week': 7, 'day_of_month': 31, 'day_of_year': 365, 'hour': 24, \n        'minute': 60, 'second': 60}.\n\n    Returns\n    -------\n    X_new : pandas DataFrame\n        DataFrame with the extracted (and optionally encoded) datetime features.\n    \n    \"\"\"\n\n    if not isinstance(X, (pd.DataFrame, pd.Series)):\n        raise TypeError(\"Input `X` must be a pandas Series or DataFrame\")\n    if not isinstance(X.index, pd.DatetimeIndex):\n        raise TypeError(\"Input `X` must have a pandas DatetimeIndex\")\n    if encoding not in [\"cyclical\", \"onehot\", None]:\n        raise ValueError(\"Encoding must be one of 'cyclical', 'onehot' or None\")\n\n    default_features = [\n        \"year\",\n        \"month\",\n        \"week\",\n        \"day_of_week\",\n        \"day_of_month\",\n        \"day_of_year\",\n        \"weekend\",\n        \"hour\",\n        \"minute\",\n        \"second\",\n    ]\n    features = features or default_features\n\n    default_max_values = {\n        \"month\": 12,\n        \"week\": 52,\n        \"day_of_week\": 7,\n        \"day_of_month\": 31,\n        \"day_of_year\": 365,\n        \"hour\": 24,\n        \"minute\": 60,\n        \"second\": 60,\n    }\n    max_values = max_values or default_max_values\n\n    X_new = pd.DataFrame(index=X.index)\n\n    datetime_attrs = {\n        \"year\": \"year\",\n        \"month\": \"month\",\n        \"week\": lambda idx: idx.isocalendar().week,\n        \"day_of_week\": \"dayofweek\",\n        \"day_of_year\": \"dayofyear\",\n        \"day_of_month\": \"day\",\n        \"weekend\": lambda idx: (idx.weekday >= 5).astype(int),\n        \"hour\": \"hour\",\n        \"minute\": \"minute\",\n        \"second\": \"second\",\n    }\n\n    not_supported_features = set(features) - set(datetime_attrs.keys())\n    if not_supported_features:\n        raise ValueError(\n            f\"Features {not_supported_features} are not supported. \"\n            f\"Supported features are {list(datetime_attrs.keys())}.\"\n        )\n\n    for feature in features:\n        attr = datetime_attrs[feature]\n        X_new[feature] = (\n            attr(X.index) if callable(attr) else getattr(X.index, attr).astype(int)\n        )\n\n    if encoding == \"cyclical\":\n        cols_to_drop = []\n        for feature, max_val in max_values.items():\n            if feature in X_new.columns:\n                X_new[f\"{feature}_sin\"] = np.sin(2 * np.pi * X_new[feature] / max_val)\n                X_new[f\"{feature}_cos\"] = np.cos(2 * np.pi * X_new[feature] / max_val)\n                cols_to_drop.append(feature)\n        X_new = X_new.drop(columns=cols_to_drop)\n    elif encoding == \"onehot\":\n        X_new = pd.get_dummies(\n            X_new, columns=features, drop_first=False, sparse=False, dtype=int\n        )\n\n    return X_new\n\n\nclass DateTimeFeatureTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    A transformer for extracting datetime features from the DateTime index of a\n    pandas DataFrame or Series. It can also apply encoding to the extracted features.\n\n    Parameters\n    ----------\n    features : list, default `None`\n        List of calendar features (strings) to extract from the index. When `None`,\n        the following features are extracted: 'year', 'month', 'week', 'day_of_week',\n        'day_of_month', 'day_of_year', 'weekend', 'hour', 'minute', 'second'.\n    encoding : str, default `'cyclical'`\n        Encoding method for the extracted features. Options are None, 'cyclical' or\n        'onehot'.\n    max_values : dict, default `None`\n        Dictionary of maximum values for the cyclical encoding of calendar features.\n        When `None`, the following values are used: {'month': 12, 'week': 52, \n        'day_of_week': 7, 'day_of_month': 31, 'day_of_year': 365, 'hour': 24, \n        'minute': 60, 'second': 60}.\n    \n    Attributes\n    ----------\n    features : list\n        List of calendar features to extract from the index.\n    encoding : str\n        Encoding method for the extracted features.\n    max_values : dict\n        Dictionary of maximum values for the cyclical encoding of calendar features.\n    \n    \"\"\"\n\n    def __init__(\n        self,\n        features: Optional[list] = None,\n        encoding: str = \"cyclical\",\n        max_values: Optional[dict] = None\n    ) -> None:\n\n        if encoding not in [\"cyclical\", \"onehot\", None]:\n            raise ValueError(\"Encoding must be one of 'cyclical', 'onehot' or None\")\n\n        self.features = (\n            features\n            if features is not None\n            else [\n                \"year\",\n                \"month\",\n                \"week\",\n                \"day_of_week\",\n                \"day_of_month\",\n                \"day_of_year\",\n                \"weekend\",\n                \"hour\",\n                \"minute\",\n                \"second\",\n            ]\n        )\n        self.encoding = encoding\n        self.max_values = (\n            max_values\n            if max_values is not None\n            else {\n                \"month\": 12,\n                \"week\": 52,\n                \"day_of_week\": 7,\n                \"day_of_month\": 31,\n                \"day_of_year\": 365,\n                \"hour\": 24,\n                \"minute\": 60,\n                \"second\": 60,\n            }\n        )\n\n    def fit(self, X, y=None):\n        \"\"\"\n        A no-op method to satisfy the scikit-learn API.\n        \"\"\"\n        return self\n\n    def transform(\n        self,\n        X: Union[pd.Series, pd.DataFrame]\n    ) -> pd.DataFrame:\n        \"\"\"\n        Create datetime features from the DateTime index of a pandas DataFrame or Series.\n\n        Parameters\n        ----------\n        X : pandas Series, pandas DataFrame\n            Input DataFrame or Series with a datetime index.\n        \n        Returns\n        -------\n        X_new : pandas DataFrame\n            DataFrame with the extracted (and optionally encoded) datetime features.\n\n        \"\"\"\n\n        X_new = create_datetime_features(\n                    X          = X,\n                    encoding   = self.encoding,\n                    features   = self.features,\n                    max_values = self.max_values,\n                )\n\n        return X_new\n\n\n@njit\ndef _np_mean_jit(x):  # pragma: no cover\n    \"\"\"\n    NumPy mean function implemented with Numba JIT.\n    \"\"\"\n    return np.mean(x)\n\n\n@njit\ndef _np_std_jit(x, ddof=1):  # pragma: no cover\n    \"\"\"\n    Standard deviation function implemented with Numba JIT.\n    If the array has only one element, the function returns 0.\n    \"\"\"\n    if len(x) == 1:\n        return 0.\n    \n    a_a, b_b = 0, 0\n    for i in x:\n        a_a = a_a + i\n        b_b = b_b + i * i\n    var = b_b / (len(x)) - ((a_a / (len(x))) ** 2)\n    var = var * (len(x) / (len(x) - ddof))\n    std = np.sqrt(var)\n\n    return std\n\n\n@njit\ndef _np_min_jit(x):  # pragma: no cover\n    \"\"\"\n    NumPy min function implemented with Numba JIT.\n    \"\"\"\n    return np.min(x)\n\n\n@njit\ndef _np_max_jit(x):  # pragma: no cover\n    \"\"\"\n    NumPy max function implemented with Numba JIT.\n    \"\"\"\n    return np.max(x)\n\n\n@njit\ndef _np_sum_jit(x):  # pragma: no cover\n    \"\"\"\n    NumPy sum function implemented with Numba JIT.\n    \"\"\"\n    return np.sum(x)\n\n\n@njit\ndef _np_median_jit(x):  # pragma: no cover\n    \"\"\"\n    NumPy median function implemented with Numba JIT.\n    \"\"\"\n    return np.median(x)\n\n\n@njit\ndef _np_min_max_ratio_jit(x):  # pragma: no cover\n    \"\"\"\n    NumPy min-max ratio function implemented with Numba JIT.\n    \"\"\"\n    return np.min(x) / np.max(x)\n\n\n@njit\ndef _np_cv_jit(x):  # pragma: no cover\n    \"\"\"\n    Coefficient of variation function implemented with Numba JIT.\n    If the array has only one element, the function returns 0.\n    \"\"\"\n    if len(x) == 1:\n        return 0.\n    \n    a_a, b_b = 0, 0\n    for i in x:\n        a_a = a_a + i\n        b_b = b_b + i * i\n    var = b_b / (len(x)) - ((a_a / (len(x))) ** 2)\n    var = var * (len(x) / (len(x) - 1))\n    std = np.sqrt(var)\n\n    return std / np.mean(x)\n\n\nclass RollingFeatures():\n    \"\"\"\n    This class computes rolling features. To avoid data leakage, the last point \n    in the window is excluded from calculations, ('closed': 'left' and \n    'center': False).\n\n    Parameters\n    ----------\n    stats : str, list\n        Statistics to compute over the rolling window. Can be a `string` or a `list`,\n        and can have repeats. Available statistics are: 'mean', 'std', 'min', 'max',\n        'sum', 'median', 'ratio_min_max', 'coef_variation'.\n    window_sizes : int, list\n        Size of the rolling window for each statistic. If an `int`, all stats share \n        the same window size. If a `list`, it should have the same length as stats.\n    min_periods : int, list, default `None`\n        Minimum number of observations in window required to have a value. \n        Same as the `min_periods` argument of pandas rolling. If `None`, \n        defaults to `window_sizes`.\n    features_names : list, default `None`\n        Names of the output features. If `None`, default names will be used in the \n        format 'roll_stat_window_size', for example 'roll_mean_7'.\n    fillna : str, float, default `None`\n        Fill missing values in `transform_batch` method. Available \n        methods are: 'mean', 'median', 'ffill', 'bfill', or a float value.\n    \n    Attributes\n    ----------\n    stats : list\n        Statistics to compute over the rolling window.\n    n_stats : int\n        Number of statistics to compute.\n    window_sizes : list\n        Size of the rolling window for each statistic.\n    max_window_size : int\n        Maximum window size.\n    min_periods : list\n        Minimum number of observations in window required to have a value.\n    features_names : list\n        Names of the output features.\n    fillna : str, float\n        Method to fill missing values in `transform_batch` method.\n    unique_rolling_windows : dict\n        Dictionary containing unique rolling window parameters and the corresponding\n        statistics.\n        \n    \"\"\"\n\n    def __init__(\n        self, \n        stats: Union[str, list],\n        window_sizes: Union[int, list],\n        min_periods: Optional[Union[int, list]] = None,\n        features_names: Optional[list] = None, \n        fillna: Optional[Union[str, float]] = None\n    ) -> None:\n        \n        self._validate_params(\n            stats,\n            window_sizes,\n            min_periods,\n            features_names,\n            fillna\n        )\n\n        if isinstance(stats, str):\n            stats = [stats]\n        self.stats = stats\n        self.n_stats = len(stats)\n\n        if isinstance(window_sizes, int):\n            window_sizes = [window_sizes] * self.n_stats\n        self.window_sizes = window_sizes\n        self.max_window_size = max(window_sizes)\n        \n        if min_periods is None:\n            min_periods = self.window_sizes\n        elif isinstance(min_periods, int):\n            min_periods = [min_periods] * self.n_stats\n        self.min_periods = min_periods\n\n        if features_names is None:\n            features_names = [\n                f\"roll_{stat}_{window_size}\" \n                for stat, window_size in zip(self.stats, self.window_sizes)\n            ]\n        self.features_names = features_names\n        \n        self.fillna = fillna\n\n        window_params_list = []\n        for i in range(len(self.stats)):\n            window_params = (self.window_sizes[i], self.min_periods[i])\n            window_params_list.append(window_params)\n\n        # Find unique window parameter combinations\n        unique_rolling_windows = {}\n        for i, params in enumerate(window_params_list):\n            key = f\"{params[0]}_{params[1]}\"\n            if key not in unique_rolling_windows:\n                unique_rolling_windows[key] = {\n                    'params': {\n                        'window': params[0], \n                        'min_periods': params[1], \n                        'center': False,\n                        'closed': 'left'\n                    },\n                    'stats_idx': [], \n                    'stats_names': [], \n                    'rolling_obj': None\n                }\n            unique_rolling_windows[key]['stats_idx'].append(i)\n            unique_rolling_windows[key]['stats_names'].append(self.features_names[i])\n\n        self.unique_rolling_windows = unique_rolling_windows\n\n    def __repr__(\n        self\n    ) -> str:\n        \"\"\"\n        Information displayed when printed.\n        \"\"\"\n            \n        return (\n            f\"RollingFeatures(\\n\"\n            f\"    stats           = {self.stats},\\n\"\n            f\"    window_sizes    = {self.window_sizes},\\n\"\n            f\"    Max window size = {self.max_window_size},\\n\"\n            f\"    min_periods     = {self.min_periods},\\n\"\n            f\"    features_names  = {self.features_names},\\n\"\n            f\"    fillna          = {self.fillna}\\n\"\n            f\")\"\n        )\n\n    def _validate_params(\n        self, \n        stats, \n        window_sizes, \n        min_periods: Optional[Union[int, list]] = None,\n        features_names: Optional[Union[str, list]] = None, \n        fillna: Optional[Union[str, float]] = None\n    ) -> None:\n        \"\"\"\n        Validate the parameters of the RollingFeatures class.\n\n        Parameters\n        ----------\n        stats : str, list\n            Statistics to compute over the rolling window. Can be a `string` or a `list`,\n            and can have repeats. Available statistics are: 'mean', 'std', 'min', 'max',\n            'sum', 'median', 'ratio_min_max', 'coef_variation'.\n        window_sizes : int, list\n            Size of the rolling window for each statistic. If an `int`, all stats share \n            the same window size. If a `list`, it should have the same length as stats.\n        min_periods : int, list, default `None`\n            Minimum number of observations in window required to have a value. \n            Same as the `min_periods` argument of pandas rolling. If `None`, \n            defaults to `window_sizes`.\n        features_names : list, default `None`\n            Names of the output features. If `None`, default names will be used in the \n            format 'roll_stat_window_size', for example 'roll_mean_7'.\n        fillna : str, float, default `None`\n            Fill missing values in `transform_batch` method. Available \n            methods are: 'mean', 'median', 'ffill', 'bfill', or a float value.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n\n        # stats\n        if not isinstance(stats, (str, list)):\n            raise TypeError(\n                f\"`stats` must be a string or a list of strings. Got {type(stats)}.\"\n            )        \n        \n        if isinstance(stats, str):\n            stats = [stats]\n        allowed_stats = ['mean', 'std', 'min', 'max', 'sum', 'median', \n                         'ratio_min_max', 'coef_variation']\n        for stat in set(stats):\n            if stat not in allowed_stats:\n                raise ValueError(\n                    f\"Statistic '{stat}' is not allowed. Allowed stats are: {allowed_stats}.\"\n                )\n        \n        n_stats = len(stats)\n        \n        # window_sizes\n        if not isinstance(window_sizes, (int, list)):\n            raise TypeError(\n                f\"`window_sizes` must be an int or a list of ints. Got {type(window_sizes)}.\"\n            )\n        \n        if isinstance(window_sizes, list):\n            n_window_sizes = len(window_sizes)\n            if n_window_sizes != n_stats:\n                raise ValueError(\n                    f\"Length of `window_sizes` list ({n_window_sizes}) \"\n                    f\"must match length of `stats` list ({n_stats}).\"\n                )\n            \n        # Check duplicates (stats, window_sizes)\n        if isinstance(window_sizes, int):\n            window_sizes = [window_sizes] * n_stats\n        if len(set(zip(stats, window_sizes))) != n_stats:\n            raise ValueError(\n                f\"Duplicate (stat, window_size) pairs are not allowed.\\n\"\n                f\"    `stats`       : {stats}\\n\"\n                f\"    `window_sizes : {window_sizes}\"\n            )\n        \n        # min_periods\n        if not isinstance(min_periods, (int, list, type(None))):\n            raise TypeError(\n                f\"`min_periods` must be an int, list of ints, or None. Got {type(min_periods)}.\"\n            )\n        \n        if min_periods is not None:\n            if isinstance(min_periods, int):\n                min_periods = [min_periods] * n_stats\n            elif isinstance(min_periods, list):\n                n_min_periods = len(min_periods)\n                if n_min_periods != n_stats:\n                    raise ValueError(\n                        f\"Length of `min_periods` list ({n_min_periods}) \"\n                        f\"must match length of `stats` list ({n_stats}).\"\n                    )\n            \n            for i, min_period in enumerate(min_periods):\n                if min_period > window_sizes[i]:\n                    raise ValueError(\n                        \"Each `min_period` must be less than or equal to its \"\n                        \"corresponding `window_size`.\"\n                    )\n        \n        # features_names\n        if not isinstance(features_names, (list, type(None))):\n            raise TypeError(\n                f\"`features_names` must be a list of strings or None. Got {type(features_names)}.\"\n            )\n        \n        if isinstance(features_names, list):\n            n_features_names = len(features_names)\n            if n_features_names != n_stats:\n                raise ValueError(\n                    f\"Length of `features_names` list ({n_features_names}) \"\n                    f\"must match length of `stats` list ({n_stats}).\"\n                )\n        \n        # fillna\n        if fillna is not None:\n            if not isinstance(fillna, (int, float, str)):\n                raise TypeError(\n                    f\"`fillna` must be a float, string, or None. Got {type(fillna)}.\"\n                )\n            \n            if isinstance(fillna, str):\n                allowed_fill_strategy = ['mean', 'median', 'ffill', 'bfill']\n                if fillna not in allowed_fill_strategy:\n                    raise ValueError(\n                        f\"'{fillna}' is not allowed. Allowed `fillna` \"\n                        f\"values are: {allowed_fill_strategy} or a float value.\"\n                    )\n\n    def _apply_stat_pandas(\n        self, \n        rolling_obj: pd.core.window.rolling.Rolling, \n        stat: str\n    ) -> pd.Series:\n        \"\"\"\n        Apply the specified statistic to a pandas rolling object.\n\n        Parameters\n        ----------\n        rolling_obj : pandas Rolling\n            Rolling object to apply the statistic.\n        stat : str\n            Statistic to compute.\n        \n        Returns\n        -------\n        stat_series : pandas Series\n            Series with the computed statistic.\n        \n        \"\"\"\n\n        if stat == 'mean':\n            return rolling_obj.mean()\n        elif stat == 'std':\n            return rolling_obj.std()\n        elif stat == 'min':\n            return rolling_obj.min()\n        elif stat == 'max':\n            return rolling_obj.max()\n        elif stat == 'sum':\n            return rolling_obj.sum()\n        elif stat == 'median':\n            return rolling_obj.median()\n        elif stat == 'ratio_min_max':\n            return rolling_obj.min() / rolling_obj.max()\n        elif stat == 'coef_variation':\n            return rolling_obj.std() / rolling_obj.mean()\n        else:\n            raise ValueError(f\"Statistic '{stat}' is not implemented.\")\n\n    def transform_batch(\n        self, \n        X: pd.Series\n    ) -> pd.DataFrame:\n        \"\"\"\n        Transform an entire pandas Series using rolling windows and compute the \n        specified statistics.\n\n        Parameters\n        ----------\n        X : pandas Series\n            The input data series to transform.\n\n        Returns\n        -------\n        rolling_features : pandas DataFrame\n            A DataFrame containing the rolling features.\n        \n        \"\"\"\n\n        for k in self.unique_rolling_windows.keys():\n            rolling_obj = X.rolling(**self.unique_rolling_windows[k]['params'])\n            self.unique_rolling_windows[k]['rolling_obj'] = rolling_obj\n        \n        rolling_features = []\n        for i, stat in enumerate(self.stats):\n            window_size = self.window_sizes[i]\n            min_periods = self.min_periods[i]\n\n            key = f\"{window_size}_{min_periods}\"\n            rolling_obj = self.unique_rolling_windows[key]['rolling_obj']\n\n            stat_series = self._apply_stat_pandas(rolling_obj=rolling_obj, stat=stat)            \n            rolling_features.append(stat_series)\n\n        rolling_features = pd.concat(rolling_features, axis=1)\n        rolling_features.columns = self.features_names\n        rolling_features = rolling_features.iloc[self.max_window_size:]\n\n        if self.fillna is not None:\n            if self.fillna == 'mean':\n                rolling_features = rolling_features.fillna(rolling_features.mean())\n            elif self.fillna == 'median':\n                rolling_features = rolling_features.fillna(rolling_features.median())\n            elif self.fillna == 'ffill':\n                rolling_features = rolling_features.ffill()\n            elif self.fillna == 'bfill':\n                rolling_features = rolling_features.bfill()\n            else:\n                rolling_features = rolling_features.fillna(self.fillna)\n        \n        return rolling_features\n\n    def _apply_stat_numpy_jit(\n        self, \n        X_window: np.ndarray, \n        stat: str\n    ) -> float:\n        \"\"\"\n        Apply the specified statistic to a numpy array using Numba JIT.\n\n        Parameters\n        ----------\n        X_window : numpy array\n            Array with the rolling window.\n        stat : str\n            Statistic to compute.\n\n        Returns\n        -------\n        stat_value : float\n            Value of the computed statistic.\n        \n        \"\"\"\n        \n        if stat == 'mean':\n            return _np_mean_jit(X_window)\n        elif stat == 'std':\n            return _np_std_jit(X_window)\n        elif stat == 'min':\n            return _np_min_jit(X_window)\n        elif stat == 'max':\n            return _np_max_jit(X_window)\n        elif stat == 'sum':\n            return _np_sum_jit(X_window)\n        elif stat == 'median':\n            return _np_median_jit(X_window)\n        elif stat == 'ratio_min_max':\n            return _np_min_max_ratio_jit(X_window)\n        elif stat == 'coef_variation':\n            return _np_cv_jit(X_window)\n        else:\n            raise ValueError(f\"Statistic '{stat}' is not implemented.\")\n\n    def transform(\n        self, \n        X: np.ndarray\n    ) -> np.ndarray:\n        \"\"\"\n        Transform a numpy array using rolling windows and compute the \n        specified statistics. The returned array will have the shape \n        (X.shape[1] if exists, n_stats). For example, if X is a flat\n        array, the output will have shape (n_stats,). If X is a 2D array,\n        the output will have shape (X.shape[1], n_stats).\n\n        Parameters\n        ----------\n        X : numpy ndarray\n            The input data array to transform.\n\n        Returns\n        -------\n        rolling_features : numpy ndarray\n            An array containing the computed statistics.\n        \n        \"\"\"\n\n        array_ndim = X.ndim\n        if array_ndim == 1:\n            X = X[:, np.newaxis]\n            \n        rolling_features = np.full(\n            shape=(X.shape[1], self.n_stats), fill_value=np.nan, dtype=float\n        )\n\n        for i in range(X.shape[1]):\n            for j, stat in enumerate(self.stats):\n                X_window = X[-self.window_sizes[j]:, i]\n                X_window = X_window[~np.isnan(X_window)]\n                if len(X_window) > 0: \n                    rolling_features[i, j] = self._apply_stat_numpy_jit(X_window, stat)\n                else:\n                    rolling_features[i, j] = np.nan\n\n        if array_ndim == 1:\n            rolling_features = rolling_features.ravel()\n        \n        return rolling_features\n    \n\nclass QuantileBinner:\n    \"\"\"\n    QuantileBinner class to bin data into quantile-based bins using `numpy.percentile`.\n    This class is similar to `KBinsDiscretizer` but faster for binning data into\n    quantile-based bins. Bin  intervals are defined following the convention:\n    bins[i-1] <= x < bins[i]. See more information in `numpy.percentile` and\n    `numpy.digitize`.\n    \n    Parameters\n    ----------\n    n_bins : int\n        The number of quantile-based bins to create.\n    method : str, default='linear'\n        The method used to compute the quantiles. This parameter is passed to \n        `numpy.percentile`. Default is 'linear'. Valid values are \"inverse_cdf\",\n        \"averaged_inverse_cdf\", \"closest_observation\", \"interpolated_inverse_cdf\",\n        \"hazen\", \"weibull\", \"linear\", \"median_unbiased\", \"normal_unbiased\".\n    subsample : int, default=200000\n        The number of samples to use for computing quantiles. If the dataset \n        has more samples than `subsample`, a random subset will be used.\n    random_state : int, default=789654\n        The random seed to use for generating a random subset of the data.\n    dtype : data type, default=numpy.float64\n        The data type to use for the bin indices. Default is `numpy.float64`.\n    \n    Attributes\n    ----------\n    n_bins : int\n        The number of quantile-based bins to create.\n    method : str, default='linear'\n        The method used to compute the quantiles. This parameter is passed to \n        `numpy.percentile`. Default is 'linear'. Valid values are 'linear',\n        'lower', 'higher', 'midpoint', 'nearest'.\n    subsample : int, default=200000\n        The number of samples to use for computing quantiles. If the dataset \n        has more samples than `subsample`, a random subset will be used.\n    random_state : int, default=789654\n        The random seed to use for generating a random subset of the data.\n    dtype : data type, default=numpy.float64\n        The data type to use for the bin indices. Default is `numpy.float64`.\n    n_bins_ : int\n        The number of bins learned during fitting.\n    bin_edges_ : numpy ndarray\n        The edges of the bins learned during fitting.\n    \n    \"\"\"\n\n    def __init__(\n        self,\n        n_bins: int,\n        method: Optional[str] = \"linear\",\n        subsample: int = 200000,\n        dtype: Optional[type] = np.float64,\n        random_state: Optional[int] = 789654\n    ):\n        \n        self._validate_params(\n            n_bins,\n            method,\n            subsample,\n            dtype,\n            random_state\n        )\n\n        self.n_bins       = n_bins\n        self.method       = method\n        self.subsample    = subsample\n        self.random_state = random_state\n        self.dtype        = dtype\n        self.n_bins_      = None\n        self.bin_edges_   = None\n        self.intervals_   = None\n\n    def _validate_params(\n        self,\n        n_bins: int,\n        method: str,\n        subsample: int,\n        dtype: type,\n        random_state: int\n    ):\n        \"\"\"\n        Validate the parameters passed to the class initializer.\n        \"\"\"\n    \n        if not isinstance(n_bins, int) or n_bins < 2:\n            raise ValueError(\n                f\"`n_bins` must be an int greater than 1. Got {n_bins}.\"\n            )\n\n        valid_methods = [\n            \"inverse_cdf\",\n            \"averaged_inverse_cdf\",\n            \"closest_observation\",\n            \"interpolated_inverse_cdf\",\n            \"hazen\",\n            \"weibull\",\n            \"linear\",\n            \"median_unbiased\",\n            \"normal_unbiased\",\n        ]\n        if method not in valid_methods:\n            raise ValueError(\n                f\"`method` must be one of {valid_methods}. Got {method}.\"\n            )\n        if not isinstance(subsample, int) or subsample < 1:\n            raise ValueError(\n                f\"`subsample` must be an integer greater than or equal to 1. \"\n                f\"Got {subsample}.\"\n            )\n        if not isinstance(random_state, int) or random_state < 0:\n            raise ValueError(\n                f\"`random_state` must be an integer greater than or equal to 0. \"\n                f\"Got {random_state}.\"\n            )\n        if not isinstance(dtype, type):\n            raise ValueError(\n                f\"`dtype` must be a valid numpy dtype. Got {dtype}.\"\n            )\n\n    def fit(self, X: np.ndarray):\n        \"\"\"\n        Learn the bin edges based on quantiles from the training data.\n        \n        Parameters\n        ----------\n        X : numpy ndarray\n            The training data used to compute the quantiles.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        if X.size == 0:\n            raise ValueError(\"Input data `X` cannot be empty.\")\n        if len(X) > self.subsample:\n            rng = np.random.default_rng(self.random_state)\n            X = X[rng.integers(0, len(X), self.subsample)]\n\n        self.bin_edges_ = np.percentile(\n            a      = X,\n            q      = np.linspace(0, 100, self.n_bins + 1),\n            method = self.method\n        )\n\n        self.n_bins_ = len(self.bin_edges_) - 1\n        self.intervals_ = {\n            float(i): (float(self.bin_edges_[i]), float(self.bin_edges_[i + 1]))\n            for i in range(self.n_bins_)\n        }\n\n    def transform(self, X: np.ndarray):\n        \"\"\"\n        Assign new data to the learned bins.\n        \n        Parameters\n        ----------\n        X : numpy ndarray\n            The data to assign to the bins.\n        \n        Returns\n        -------\n        bin_indices : numpy ndarray \n            The indices of the bins each value belongs to.\n            Values less than the smallest bin edge are assigned to the first bin,\n            and values greater than the largest bin edge are assigned to the last bin.\n       \n        \"\"\"\n\n        if self.bin_edges_ is None:\n            raise NotFittedError(\n                \"The model has not been fitted yet. Call 'fit' with training data first.\"\n            )\n\n        bin_indices = np.digitize(X, bins=self.bin_edges_, right=False)\n        bin_indices = np.clip(bin_indices, 1, self.n_bins_).astype(self.dtype) - 1\n\n        return bin_indices\n\n    def fit_transform(self, X):\n        \"\"\"\n        Fit the model to the data and return the bin indices for the same data.\n        \n        Parameters\n        ----------\n        X : numpy.ndarray\n            The data to fit and transform.\n        \n        Returns\n        -------\n        bin_indices : numpy.ndarray\n            The indices of the bins each value belongs to.\n            Values less than the smallest bin edge are assigned to the first bin,\n            and values greater than the largest bin edge are assigned to the last bin.\n        \n        \"\"\"\n\n        self.fit(X)\n\n        return self.transform(X)\n\n    def get_params(self):\n        \"\"\"\n        Get the parameters of the quantile binner.\n        \n        Parameters\n        ----------\n        self\n        \n        Returns\n        -------\n        params : dict\n            A dictionary of the parameters of the quantile binner.\n        \n        \"\"\"\n\n        return {\n            \"n_bins\": self.n_bins,\n            \"method\": self.method,\n            \"subsample\": self.subsample,\n            \"dtype\": self.dtype,\n            \"random_state\": self.random_state,\n        }\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of the QuantileBinner.\n        \n        Parameters\n        ----------\n        params : dict\n            A dictionary of the parameters to set.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        for param, value in params.items():\n            setattr(self, param, value)\n"
  },
  "GT_src_dict": {
    "skforecast/preprocessing/preprocessing.py": {
      "TimeSeriesDifferentiator.__init__": {
        "code": "    def __init__(self, order: int=1, window_size: int=None) -> None:\n        \"\"\"Initializes the TimeSeriesDifferentiator, a transformation class that applies\n    differentiation to time series data based on a specified order. This class is designed \n    to enable the differentiation of time series data for modeling purposes and to allow \n    for the inverse transformation to reconstruct the original data.\n\n    Parameters\n    ----------\n    order : int, default=1\n        The order of differentiation to apply to the time series. Must be an integer \n        greater than 0.\n    window_size : int, default=None\n        The size of the window to use for the forecaster. This parameter is required \n        for reverting the differentiation for the target variable or its predicted values. \n        If provided, it must also be an integer greater than 0.\n\n    Attributes\n    ----------\n    order : int\n        Stores the differentiation order.\n    window_size : int or None\n        Stores the window size for reverting the differentiation.\n    initial_values : list\n        Holds the initial values necessary for reverting differentiation during transformations.\n    pre_train_values : list\n        Stores values required for inverse transformation of the training time series.\n    last_values : list\n        Maintains the last values before each differentiation for reverting purposes.\"\"\"\n        if not isinstance(order, (int, np.integer)):\n            raise TypeError(f'Parameter `order` must be an integer greater than 0. Found {type(order)}.')\n        if order < 1:\n            raise ValueError(f'Parameter `order` must be an integer greater than 0. Found {order}.')\n        if window_size is not None:\n            if not isinstance(window_size, (int, np.integer)):\n                raise TypeError(f'Parameter `window_size` must be an integer greater than 0. Found {type(window_size)}.')\n            if window_size < 1:\n                raise ValueError(f'Parameter `window_size` must be an integer greater than 0. Found {window_size}.')\n        self.order = order\n        self.window_size = window_size\n        self.initial_values = []\n        self.pre_train_values = []\n        self.last_values = []",
        "docstring": "Initializes the TimeSeriesDifferentiator, a transformation class that applies\ndifferentiation to time series data based on a specified order. This class is designed \nto enable the differentiation of time series data for modeling purposes and to allow \nfor the inverse transformation to reconstruct the original data.\n\nParameters\n----------\norder : int, default=1\n    The order of differentiation to apply to the time series. Must be an integer \n    greater than 0.\nwindow_size : int, default=None\n    The size of the window to use for the forecaster. This parameter is required \n    for reverting the differentiation for the target variable or its predicted values. \n    If provided, it must also be an integer greater than 0.\n\nAttributes\n----------\norder : int\n    Stores the differentiation order.\nwindow_size : int or None\n    Stores the window size for reverting the differentiation.\ninitial_values : list\n    Holds the initial values necessary for reverting differentiation during transformations.\npre_train_values : list\n    Stores values required for inverse transformation of the training time series.\nlast_values : list\n    Maintains the last values before each differentiation for reverting purposes.",
        "signature": "def __init__(self, order: int=1, window_size: int=None) -> None:",
        "type": "Method",
        "class_signature": "class TimeSeriesDifferentiator(BaseEstimator, TransformerMixin):"
      },
      "wrapper": {
        "code": "        def wrapper(self, *args, **kwargs):\n            \"\"\"Wrapper function that decorates methods to enforce input validation for a numpy 1D array parameter 'X'. It checks whether 'X' is provided as a positional argument or a keyword argument. If 'ensure_1d' is set to True (the default), it ensures that 'X' is a 1D numpy array, raising errors if not.\n\nParameters\n----------\nself : object\n    The instance of the class where the method is defined.\n*args : tuple\n    Positional arguments passed to the method.\n**kwargs : dict\n    Keyword arguments passed to the method, allowing 'X' to be specified.\n\nReturns\n-------\nresult : Any\n    The return value of the decorated method after input validation.\n\nRaises\n------\nValueError\n    If 'X' is not provided in either args or kwargs or if 'X' is not a 1D numpy array when 'ensure_1d' is True.\nTypeError\n    If 'X' is not an instance of numpy ndarray.\n\nDependencies\n------------\nThis wrapper relies on numpy (np) for type checking and dimension verification of the input 'X'. It is used in the context of class methods that require a time series or array input.\"\"\"\n            if args:\n                X = args[0]\n            elif 'X' in kwargs:\n                X = kwargs['X']\n            else:\n                raise ValueError(\"Methods must be called with 'X' as argument.\")\n            if not isinstance(X, np.ndarray):\n                raise TypeError(f\"'X' must be a numpy ndarray. Found {type(X)}.\")\n            if ensure_1d and (not X.ndim == 1):\n                raise ValueError(f\"'X' must be a 1D array. Found {X.ndim} dimensions.\")\n            result = func(self, *args, **kwargs)\n            return result",
        "docstring": "Wrapper function that decorates methods to enforce input validation for a numpy 1D array parameter 'X'. It checks whether 'X' is provided as a positional argument or a keyword argument. If 'ensure_1d' is set to True (the default), it ensures that 'X' is a 1D numpy array, raising errors if not.\n\nParameters\n----------\nself : object\n    The instance of the class where the method is defined.\n*args : tuple\n    Positional arguments passed to the method.\n**kwargs : dict\n    Keyword arguments passed to the method, allowing 'X' to be specified.\n\nReturns\n-------\nresult : Any\n    The return value of the decorated method after input validation.\n\nRaises\n------\nValueError\n    If 'X' is not provided in either args or kwargs or if 'X' is not a 1D numpy array when 'ensure_1d' is True.\nTypeError\n    If 'X' is not an instance of numpy ndarray.\n\nDependencies\n------------\nThis wrapper relies on numpy (np) for type checking and dimension verification of the input 'X'. It is used in the context of class methods that require a time series or array input.",
        "signature": "def wrapper(self, *args, **kwargs):",
        "type": "Function",
        "class_signature": null
      }
    }
  },
  "dependency_dict": {},
  "PRD": "# PROJECT NAME: skforecast-test_check_X_numpy_ndarray_1d\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 skforecast/\n    \u2514\u2500\u2500 preprocessing/\n        \u2514\u2500\u2500 preprocessing.py\n            \u251c\u2500\u2500 TimeSeriesDifferentiator.__init__\n            \u2514\u2500\u2500 wrapper\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThe module is designed to ensure the robustness and correctness of data passed to the `TimeSeriesDifferentiator`, a tool used for performing time series differentiation. It validates that input data complies with strict requirements, including being a one-dimensional NumPy array, to prevent errors and maintain consistent functionality. By identifying and raising appropriate exceptions for unsupported data structures or incorrect dimensionality, the module reduces debugging time for developers and ensures the reliability of preprocessing workflows. This ultimately addresses the need for clean, predictable input handling in time-series data transformations.\n\n## FILE 1: skforecast/preprocessing/preprocessing.py\n\n- FUNCTION NAME: wrapper\n  - SIGNATURE: def wrapper(self, *args, **kwargs):\n  - DOCSTRING: \n```python\n\"\"\"\nWrapper function that decorates methods to enforce input validation for a numpy 1D array parameter 'X'. It checks whether 'X' is provided as a positional argument or a keyword argument. If 'ensure_1d' is set to True (the default), it ensures that 'X' is a 1D numpy array, raising errors if not.\n\nParameters\n----------\nself : object\n    The instance of the class where the method is defined.\n*args : tuple\n    Positional arguments passed to the method.\n**kwargs : dict\n    Keyword arguments passed to the method, allowing 'X' to be specified.\n\nReturns\n-------\nresult : Any\n    The return value of the decorated method after input validation.\n\nRaises\n------\nValueError\n    If 'X' is not provided in either args or kwargs or if 'X' is not a 1D numpy array when 'ensure_1d' is True.\nTypeError\n    If 'X' is not an instance of numpy ndarray.\n\nDependencies\n------------\nThis wrapper relies on numpy (np) for type checking and dimension verification of the input 'X'. It is used in the context of class methods that require a time series or array input.\n\"\"\"\n```\n\n- CLASS METHOD: TimeSeriesDifferentiator.__init__\n  - CLASS SIGNATURE: class TimeSeriesDifferentiator(BaseEstimator, TransformerMixin):\n  - SIGNATURE: def __init__(self, order: int=1, window_size: int=None) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nInitializes the TimeSeriesDifferentiator, a transformation class that applies\ndifferentiation to time series data based on a specified order. This class is designed \nto enable the differentiation of time series data for modeling purposes and to allow \nfor the inverse transformation to reconstruct the original data.\n\nParameters\n----------\norder : int, default=1\n    The order of differentiation to apply to the time series. Must be an integer \n    greater than 0.\nwindow_size : int, default=None\n    The size of the window to use for the forecaster. This parameter is required \n    for reverting the differentiation for the target variable or its predicted values. \n    If provided, it must also be an integer greater than 0.\n\nAttributes\n----------\norder : int\n    Stores the differentiation order.\nwindow_size : int or None\n    Stores the window size for reverting the differentiation.\ninitial_values : list\n    Holds the initial values necessary for reverting differentiation during transformations.\npre_train_values : list\n    Stores values required for inverse transformation of the training time series.\nlast_values : list\n    Maintains the last values before each differentiation for reverting purposes.\n\"\"\"\n```\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "skforecast/preprocessing/preprocessing.py": "from typing import Any, Union, Optional\nfrom typing_extensions import Self\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom sklearn.base import BaseEstimator\nfrom sklearn.base import TransformerMixin\nfrom sklearn.exceptions import NotFittedError\nfrom ..exceptions import MissingValuesWarning\nfrom numba import njit\n\ndef _check_X_numpy_ndarray_1d(ensure_1d=True):\n    \"\"\"\n    This decorator checks if the argument X is a numpy ndarray with 1 dimension.\n\n    Parameters\n    ----------\n    ensure_1d : bool, default=True\n        Whether to ensure if X is a 1D numpy array.\n    \n    Returns\n    -------\n    decorator : Callable\n        A decorator function.\n\n    \"\"\"\n\n    def decorator(func):\n        return wrapper\n    return decorator\n\nclass TimeSeriesDifferentiator(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transforms a time series into a differentiated time series of a specified order\n    and provides functionality to revert the differentiation. \n    \n    When using a `direct` module Forecaster, the model in step 1 must be \n    used if you want to reverse the differentiation of the training time \n    series with the `inverse_transform_training` method.\n\n    Parameters\n    ----------\n    order : int\n        The order of differentiation to be applied.\n    window_size : int, default None\n        The window size used by the forecaster. This is required to revert the \n        differentiation for the target variable `y` or its predicted values.\n\n    Attributes\n    ----------\n    order : int\n        The order of differentiation.\n    initial_values : list\n        List with the first value of the time series before each differentiation.\n        If `order = 2`, first value correspond with the first value of the original\n        time series and the second value correspond with the first value of the\n        differentiated time series of order 1. These values are necessary to \n        revert the differentiation and reconstruct the original time series.\n    pre_train_values : list\n        List with the first training value of the time series before each differentiation.\n        For `order = 1`, the value correspond with the last value of the window used to\n        create the predictors. For order > 1, the value correspond with the first\n        value of the differentiated time series prior to the next differentiation.\n        These values are necessary to revert the differentiation and reconstruct the\n        training time series.\n    last_values : list\n        List with the last value of the time series before each differentiation, \n        used to revert differentiation on subsequent data windows. If `order = 2`, \n        first value correspond with the last value of the original time series \n        and the second value correspond with the last value of the differentiated \n        time series of order 1. This is essential for correctly transforming a \n        time series that follows immediately after the series used to fit the \n        transformer.\n\n    \"\"\"\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Information displayed when printed.\n        \"\"\"\n        return f'TimeSeriesDifferentiator(order={self.order}, window_size={self.window_size})'\n\n    @_check_X_numpy_ndarray_1d()\n    def fit(self, X: np.ndarray, y: Any=None) -> Self:\n        \"\"\"\n        Fits the transformer. Stores the values needed to revert the \n        differentiation of different window of the time series, original \n        time series, training time series, and a time series that follows\n        immediately after the series used to fit the transformer.\n\n        Parameters\n        ----------\n        X : numpy ndarray\n            Time series to be differentiated.\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        self : TimeSeriesDifferentiator\n\n        \"\"\"\n        self.initial_values = []\n        self.pre_train_values = []\n        self.last_values = []\n        for i in range(self.order):\n            if i == 0:\n                self.initial_values.append(X[0])\n                if self.window_size is not None:\n                    self.pre_train_values.append(X[self.window_size - self.order])\n                self.last_values.append(X[-1])\n                X_diff = np.diff(X, n=1)\n            else:\n                self.initial_values.append(X_diff[0])\n                if self.window_size is not None:\n                    self.pre_train_values.append(X_diff[self.window_size - self.order])\n                self.last_values.append(X_diff[-1])\n                X_diff = np.diff(X_diff, n=1)\n        return self\n\n    @_check_X_numpy_ndarray_1d()\n    def transform(self, X: np.ndarray, y: Any=None) -> np.ndarray:\n        \"\"\"\n        Transforms a time series into a differentiated time series of order n.\n\n        Parameters\n        ----------\n        X : numpy ndarray\n            Time series to be differentiated.\n        y : Ignored\n            Not used, present here for API consistency by convention.\n        \n        Returns\n        -------\n        X_diff : numpy ndarray\n            Differentiated time series. The length of the array is the same as\n            the original time series but the first n `order` values are nan.\n\n        \"\"\"\n        X_diff = np.diff(X, n=self.order)\n        X_diff = np.append(np.full(shape=self.order, fill_value=np.nan), X_diff)\n        return X_diff\n\n    @_check_X_numpy_ndarray_1d()\n    def inverse_transform(self, X: np.ndarray, y: Any=None) -> np.ndarray:\n        \"\"\"\n        Reverts the differentiation. To do so, the input array is assumed to be\n        the same time series used to fit the transformer but differentiated.\n\n        Parameters\n        ----------\n        X : numpy ndarray\n            Differentiated time series.\n        y : Ignored\n            Not used, present here for API consistency by convention.\n        \n        Returns\n        -------\n        X_diff : numpy ndarray\n            Reverted differentiated time series.\n        \n        \"\"\"\n        X = X[np.argmax(~np.isnan(X)):]\n        for i in range(self.order):\n            if i == 0:\n                X_undiff = np.insert(X, 0, self.initial_values[-1])\n                X_undiff = np.cumsum(X_undiff, dtype=float)\n            else:\n                X_undiff = np.insert(X_undiff, 0, self.initial_values[-(i + 1)])\n                X_undiff = np.cumsum(X_undiff, dtype=float)\n        return X_undiff\n\n    @_check_X_numpy_ndarray_1d()\n    def inverse_transform_training(self, X: np.ndarray, y: Any=None) -> np.ndarray:\n        \"\"\"\n        Reverts the differentiation. To do so, the input array is assumed to be\n        the differentiated training time series generated with the original \n        time series used to fit the transformer.\n\n        When using a `direct` module Forecaster, the model in step 1 must be \n        used if you want to reverse the differentiation of the training time \n        series with the `inverse_transform_training` method.\n\n        Parameters\n        ----------\n        X : numpy ndarray\n            Differentiated time series.\n        y : Ignored\n            Not used, present here for API consistency by convention.\n        \n        Returns\n        -------\n        X_diff : numpy ndarray\n            Reverted differentiated time series.\n        \n        \"\"\"\n        if not self.pre_train_values:\n            raise ValueError('The `window_size` parameter must be set before fitting the transformer to revert the differentiation of the training time series.')\n        X = X[np.argmax(~np.isnan(X)):]\n        for i in range(self.order):\n            if i == 0:\n                X_undiff = np.insert(X, 0, self.pre_train_values[-1])\n                X_undiff = np.cumsum(X_undiff, dtype=float)\n            else:\n                X_undiff = np.insert(X_undiff, 0, self.pre_train_values[-(i + 1)])\n                X_undiff = np.cumsum(X_undiff, dtype=float)\n        X_undiff = X_undiff[self.order:]\n        return X_undiff\n\n    @_check_X_numpy_ndarray_1d(ensure_1d=False)\n    def inverse_transform_next_window(self, X: np.ndarray, y: Any=None) -> np.ndarray:\n        \"\"\"\n        Reverts the differentiation. The input array `X` is assumed to be a \n        differentiated time series of order n that starts right after the\n        the time series used to fit the transformer.\n\n        Parameters\n        ----------\n        X : numpy ndarray\n            Differentiated time series. It is assumed o start right after\n            the time series used to fit the transformer.\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        Returns\n        -------\n        X_undiff : numpy ndarray\n            Reverted differentiated time series.\n        \n        \"\"\"\n        array_ndim = X.ndim\n        if array_ndim == 1:\n            X = X[:, np.newaxis]\n        X = X[~np.isnan(X).any(axis=1)]\n        for i in range(self.order):\n            if i == 0:\n                X_undiff = np.cumsum(X, axis=0, dtype=float) + self.last_values[-1]\n            else:\n                X_undiff = np.cumsum(X_undiff, axis=0, dtype=float) + self.last_values[-(i + 1)]\n        if array_ndim == 1:\n            X_undiff = X_undiff.ravel()\n        return X_undiff\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of the TimeSeriesDifferentiator.\n        \n        Parameters\n        ----------\n        params : dict\n            A dictionary of the parameters to set.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        for param, value in params.items():\n            setattr(self, param, value)\n\ndef series_long_to_dict(data: pd.DataFrame, series_id: str, index: str, values: str, freq: str, suppress_warnings: bool=False) -> dict:\n    \"\"\"\n    Convert long format series to dictionary of pandas Series with frequency.\n    Input data must be a pandas DataFrame with columns for the series identifier,\n    time index, and values. The function will group the data by the series\n    identifier and convert the time index to a datetime index with the given\n    frequency.\n\n    Parameters\n    ----------\n    data: pandas DataFrame\n        Long format series.\n    series_id: str\n        Column name with the series identifier.\n    index: str\n        Column name with the time index.\n    values: str\n        Column name with the values.\n    freq: str\n        Frequency of the series.\n    suppress_warnings: bool, default `False`\n        If True, suppress warnings when a series is incomplete after setting the\n        frequency.\n\n    Returns\n    -------\n    series_dict: dict\n        Dictionary with the series.\n\n    \"\"\"\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError('`data` must be a pandas DataFrame.')\n    for col in [series_id, index, values]:\n        if col not in data.columns:\n            raise ValueError(f\"Column '{col}' not found in `data`.\")\n    original_sizes = data.groupby(series_id).size()\n    series_dict = {}\n    for k, v in data.groupby(series_id):\n        series_dict[k] = v.set_index(index)[values].asfreq(freq).rename(k)\n        series_dict[k].index.name = None\n        if not suppress_warnings and len(series_dict[k]) != original_sizes[k]:\n            warnings.warn(f\"Series '{k}' is incomplete. NaNs have been introduced after setting the frequency.\", MissingValuesWarning)\n    return series_dict\n\ndef exog_long_to_dict(data: pd.DataFrame, series_id: str, index: str, freq: str, dropna: bool=False, suppress_warnings: bool=False) -> dict:\n    \"\"\"\n    Convert long format exogenous variables to dictionary. Input data must be a\n    pandas DataFrame with columns for the series identifier, time index, and\n    exogenous variables. The function will group the data by the series identifier\n    and convert the time index to a datetime index with the given frequency.\n\n    Parameters\n    ----------\n    data: pandas DataFrame\n        Long format exogenous variables.\n    series_id: str\n        Column name with the series identifier.\n    index: str\n        Column name with the time index.\n    freq: str\n        Frequency of the series.\n    dropna: bool, default False\n        If True, drop columns with all values as NaN. This is useful when\n        there are series without some exogenous variables.\n    suppress_warnings: bool, default False\n        If True, suppress warnings when exog is incomplete after setting the\n        frequency.\n        \n    Returns\n    -------\n    exog_dict: dict\n        Dictionary with the exogenous variables.\n\n    \"\"\"\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError('`data` must be a pandas DataFrame.')\n    for col in [series_id, index]:\n        if col not in data.columns:\n            raise ValueError(f\"Column '{col}' not found in `data`.\")\n    original_sizes = data.groupby(series_id).size()\n    exog_dict = dict(tuple(data.groupby(series_id)))\n    exog_dict = {k: v.set_index(index).asfreq(freq).drop(columns=series_id) for k, v in exog_dict.items()}\n    for k in exog_dict.keys():\n        exog_dict[k].index.name = None\n    if dropna:\n        exog_dict = {k: v.dropna(how='all', axis=1) for k, v in exog_dict.items()}\n    elif not suppress_warnings:\n        for k, v in exog_dict.items():\n            if len(v) != original_sizes[k]:\n                warnings.warn(f\"Exogenous variables for series '{k}' are incomplete. NaNs have been introduced after setting the frequency.\", MissingValuesWarning)\n    return exog_dict\n\ndef create_datetime_features(X: Union[pd.Series, pd.DataFrame], features: Optional[list]=None, encoding: str='cyclical', max_values: Optional[dict]=None) -> pd.DataFrame:\n    \"\"\"\n    Extract datetime features from the DateTime index of a pandas DataFrame or Series.\n\n    Parameters\n    ----------\n    X : pandas Series, pandas DataFrame\n        Input DataFrame or Series with a datetime index.\n    features : list, default `None`\n        List of calendar features (strings) to extract from the index. When `None`,\n        the following features are extracted: 'year', 'month', 'week', 'day_of_week',\n        'day_of_month', 'day_of_year', 'weekend', 'hour', 'minute', 'second'.\n    encoding : str, default `'cyclical'`\n        Encoding method for the extracted features. Options are None, 'cyclical' or\n        'onehot'.\n    max_values : dict, default `None`\n        Dictionary of maximum values for the cyclical encoding of calendar features.\n        When `None`, the following values are used: {'month': 12, 'week': 52, \n        'day_of_week': 7, 'day_of_month': 31, 'day_of_year': 365, 'hour': 24, \n        'minute': 60, 'second': 60}.\n\n    Returns\n    -------\n    X_new : pandas DataFrame\n        DataFrame with the extracted (and optionally encoded) datetime features.\n    \n    \"\"\"\n    if not isinstance(X, (pd.DataFrame, pd.Series)):\n        raise TypeError('Input `X` must be a pandas Series or DataFrame')\n    if not isinstance(X.index, pd.DatetimeIndex):\n        raise TypeError('Input `X` must have a pandas DatetimeIndex')\n    if encoding not in ['cyclical', 'onehot', None]:\n        raise ValueError(\"Encoding must be one of 'cyclical', 'onehot' or None\")\n    default_features = ['year', 'month', 'week', 'day_of_week', 'day_of_month', 'day_of_year', 'weekend', 'hour', 'minute', 'second']\n    features = features or default_features\n    default_max_values = {'month': 12, 'week': 52, 'day_of_week': 7, 'day_of_month': 31, 'day_of_year': 365, 'hour': 24, 'minute': 60, 'second': 60}\n    max_values = max_values or default_max_values\n    X_new = pd.DataFrame(index=X.index)\n    datetime_attrs = {'year': 'year', 'month': 'month', 'week': lambda idx: idx.isocalendar().week, 'day_of_week': 'dayofweek', 'day_of_year': 'dayofyear', 'day_of_month': 'day', 'weekend': lambda idx: (idx.weekday >= 5).astype(int), 'hour': 'hour', 'minute': 'minute', 'second': 'second'}\n    not_supported_features = set(features) - set(datetime_attrs.keys())\n    if not_supported_features:\n        raise ValueError(f'Features {not_supported_features} are not supported. Supported features are {list(datetime_attrs.keys())}.')\n    for feature in features:\n        attr = datetime_attrs[feature]\n        X_new[feature] = attr(X.index) if callable(attr) else getattr(X.index, attr).astype(int)\n    if encoding == 'cyclical':\n        cols_to_drop = []\n        for feature, max_val in max_values.items():\n            if feature in X_new.columns:\n                X_new[f'{feature}_sin'] = np.sin(2 * np.pi * X_new[feature] / max_val)\n                X_new[f'{feature}_cos'] = np.cos(2 * np.pi * X_new[feature] / max_val)\n                cols_to_drop.append(feature)\n        X_new = X_new.drop(columns=cols_to_drop)\n    elif encoding == 'onehot':\n        X_new = pd.get_dummies(X_new, columns=features, drop_first=False, sparse=False, dtype=int)\n    return X_new\n\nclass DateTimeFeatureTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    A transformer for extracting datetime features from the DateTime index of a\n    pandas DataFrame or Series. It can also apply encoding to the extracted features.\n\n    Parameters\n    ----------\n    features : list, default `None`\n        List of calendar features (strings) to extract from the index. When `None`,\n        the following features are extracted: 'year', 'month', 'week', 'day_of_week',\n        'day_of_month', 'day_of_year', 'weekend', 'hour', 'minute', 'second'.\n    encoding : str, default `'cyclical'`\n        Encoding method for the extracted features. Options are None, 'cyclical' or\n        'onehot'.\n    max_values : dict, default `None`\n        Dictionary of maximum values for the cyclical encoding of calendar features.\n        When `None`, the following values are used: {'month': 12, 'week': 52, \n        'day_of_week': 7, 'day_of_month': 31, 'day_of_year': 365, 'hour': 24, \n        'minute': 60, 'second': 60}.\n    \n    Attributes\n    ----------\n    features : list\n        List of calendar features to extract from the index.\n    encoding : str\n        Encoding method for the extracted features.\n    max_values : dict\n        Dictionary of maximum values for the cyclical encoding of calendar features.\n    \n    \"\"\"\n\n    def __init__(self, features: Optional[list]=None, encoding: str='cyclical', max_values: Optional[dict]=None) -> None:\n        if encoding not in ['cyclical', 'onehot', None]:\n            raise ValueError(\"Encoding must be one of 'cyclical', 'onehot' or None\")\n        self.features = features if features is not None else ['year', 'month', 'week', 'day_of_week', 'day_of_month', 'day_of_year', 'weekend', 'hour', 'minute', 'second']\n        self.encoding = encoding\n        self.max_values = max_values if max_values is not None else {'month': 12, 'week': 52, 'day_of_week': 7, 'day_of_month': 31, 'day_of_year': 365, 'hour': 24, 'minute': 60, 'second': 60}\n\n    def fit(self, X, y=None):\n        \"\"\"\n        A no-op method to satisfy the scikit-learn API.\n        \"\"\"\n        return self\n\n    def transform(self, X: Union[pd.Series, pd.DataFrame]) -> pd.DataFrame:\n        \"\"\"\n        Create datetime features from the DateTime index of a pandas DataFrame or Series.\n\n        Parameters\n        ----------\n        X : pandas Series, pandas DataFrame\n            Input DataFrame or Series with a datetime index.\n        \n        Returns\n        -------\n        X_new : pandas DataFrame\n            DataFrame with the extracted (and optionally encoded) datetime features.\n\n        \"\"\"\n        X_new = create_datetime_features(X=X, encoding=self.encoding, features=self.features, max_values=self.max_values)\n        return X_new\n\n@njit\ndef _np_mean_jit(x):\n    \"\"\"\n    NumPy mean function implemented with Numba JIT.\n    \"\"\"\n    return np.mean(x)\n\n@njit\ndef _np_std_jit(x, ddof=1):\n    \"\"\"\n    Standard deviation function implemented with Numba JIT.\n    If the array has only one element, the function returns 0.\n    \"\"\"\n    if len(x) == 1:\n        return 0.0\n    a_a, b_b = (0, 0)\n    for i in x:\n        a_a = a_a + i\n        b_b = b_b + i * i\n    var = b_b / len(x) - (a_a / len(x)) ** 2\n    var = var * (len(x) / (len(x) - ddof))\n    std = np.sqrt(var)\n    return std\n\n@njit\ndef _np_min_jit(x):\n    \"\"\"\n    NumPy min function implemented with Numba JIT.\n    \"\"\"\n    return np.min(x)\n\n@njit\ndef _np_max_jit(x):\n    \"\"\"\n    NumPy max function implemented with Numba JIT.\n    \"\"\"\n    return np.max(x)\n\n@njit\ndef _np_sum_jit(x):\n    \"\"\"\n    NumPy sum function implemented with Numba JIT.\n    \"\"\"\n    return np.sum(x)\n\n@njit\ndef _np_median_jit(x):\n    \"\"\"\n    NumPy median function implemented with Numba JIT.\n    \"\"\"\n    return np.median(x)\n\n@njit\ndef _np_min_max_ratio_jit(x):\n    \"\"\"\n    NumPy min-max ratio function implemented with Numba JIT.\n    \"\"\"\n    return np.min(x) / np.max(x)\n\n@njit\ndef _np_cv_jit(x):\n    \"\"\"\n    Coefficient of variation function implemented with Numba JIT.\n    If the array has only one element, the function returns 0.\n    \"\"\"\n    if len(x) == 1:\n        return 0.0\n    a_a, b_b = (0, 0)\n    for i in x:\n        a_a = a_a + i\n        b_b = b_b + i * i\n    var = b_b / len(x) - (a_a / len(x)) ** 2\n    var = var * (len(x) / (len(x) - 1))\n    std = np.sqrt(var)\n    return std / np.mean(x)\n\nclass RollingFeatures:\n    \"\"\"\n    This class computes rolling features. To avoid data leakage, the last point \n    in the window is excluded from calculations, ('closed': 'left' and \n    'center': False).\n\n    Parameters\n    ----------\n    stats : str, list\n        Statistics to compute over the rolling window. Can be a `string` or a `list`,\n        and can have repeats. Available statistics are: 'mean', 'std', 'min', 'max',\n        'sum', 'median', 'ratio_min_max', 'coef_variation'.\n    window_sizes : int, list\n        Size of the rolling window for each statistic. If an `int`, all stats share \n        the same window size. If a `list`, it should have the same length as stats.\n    min_periods : int, list, default `None`\n        Minimum number of observations in window required to have a value. \n        Same as the `min_periods` argument of pandas rolling. If `None`, \n        defaults to `window_sizes`.\n    features_names : list, default `None`\n        Names of the output features. If `None`, default names will be used in the \n        format 'roll_stat_window_size', for example 'roll_mean_7'.\n    fillna : str, float, default `None`\n        Fill missing values in `transform_batch` method. Available \n        methods are: 'mean', 'median', 'ffill', 'bfill', or a float value.\n    \n    Attributes\n    ----------\n    stats : list\n        Statistics to compute over the rolling window.\n    n_stats : int\n        Number of statistics to compute.\n    window_sizes : list\n        Size of the rolling window for each statistic.\n    max_window_size : int\n        Maximum window size.\n    min_periods : list\n        Minimum number of observations in window required to have a value.\n    features_names : list\n        Names of the output features.\n    fillna : str, float\n        Method to fill missing values in `transform_batch` method.\n    unique_rolling_windows : dict\n        Dictionary containing unique rolling window parameters and the corresponding\n        statistics.\n        \n    \"\"\"\n\n    def __init__(self, stats: Union[str, list], window_sizes: Union[int, list], min_periods: Optional[Union[int, list]]=None, features_names: Optional[list]=None, fillna: Optional[Union[str, float]]=None) -> None:\n        self._validate_params(stats, window_sizes, min_periods, features_names, fillna)\n        if isinstance(stats, str):\n            stats = [stats]\n        self.stats = stats\n        self.n_stats = len(stats)\n        if isinstance(window_sizes, int):\n            window_sizes = [window_sizes] * self.n_stats\n        self.window_sizes = window_sizes\n        self.max_window_size = max(window_sizes)\n        if min_periods is None:\n            min_periods = self.window_sizes\n        elif isinstance(min_periods, int):\n            min_periods = [min_periods] * self.n_stats\n        self.min_periods = min_periods\n        if features_names is None:\n            features_names = [f'roll_{stat}_{window_size}' for stat, window_size in zip(self.stats, self.window_sizes)]\n        self.features_names = features_names\n        self.fillna = fillna\n        window_params_list = []\n        for i in range(len(self.stats)):\n            window_params = (self.window_sizes[i], self.min_periods[i])\n            window_params_list.append(window_params)\n        unique_rolling_windows = {}\n        for i, params in enumerate(window_params_list):\n            key = f'{params[0]}_{params[1]}'\n            if key not in unique_rolling_windows:\n                unique_rolling_windows[key] = {'params': {'window': params[0], 'min_periods': params[1], 'center': False, 'closed': 'left'}, 'stats_idx': [], 'stats_names': [], 'rolling_obj': None}\n            unique_rolling_windows[key]['stats_idx'].append(i)\n            unique_rolling_windows[key]['stats_names'].append(self.features_names[i])\n        self.unique_rolling_windows = unique_rolling_windows\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Information displayed when printed.\n        \"\"\"\n        return f'RollingFeatures(\\n    stats           = {self.stats},\\n    window_sizes    = {self.window_sizes},\\n    Max window size = {self.max_window_size},\\n    min_periods     = {self.min_periods},\\n    features_names  = {self.features_names},\\n    fillna          = {self.fillna}\\n)'\n\n    def _validate_params(self, stats, window_sizes, min_periods: Optional[Union[int, list]]=None, features_names: Optional[Union[str, list]]=None, fillna: Optional[Union[str, float]]=None) -> None:\n        \"\"\"\n        Validate the parameters of the RollingFeatures class.\n\n        Parameters\n        ----------\n        stats : str, list\n            Statistics to compute over the rolling window. Can be a `string` or a `list`,\n            and can have repeats. Available statistics are: 'mean', 'std', 'min', 'max',\n            'sum', 'median', 'ratio_min_max', 'coef_variation'.\n        window_sizes : int, list\n            Size of the rolling window for each statistic. If an `int`, all stats share \n            the same window size. If a `list`, it should have the same length as stats.\n        min_periods : int, list, default `None`\n            Minimum number of observations in window required to have a value. \n            Same as the `min_periods` argument of pandas rolling. If `None`, \n            defaults to `window_sizes`.\n        features_names : list, default `None`\n            Names of the output features. If `None`, default names will be used in the \n            format 'roll_stat_window_size', for example 'roll_mean_7'.\n        fillna : str, float, default `None`\n            Fill missing values in `transform_batch` method. Available \n            methods are: 'mean', 'median', 'ffill', 'bfill', or a float value.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n        if not isinstance(stats, (str, list)):\n            raise TypeError(f'`stats` must be a string or a list of strings. Got {type(stats)}.')\n        if isinstance(stats, str):\n            stats = [stats]\n        allowed_stats = ['mean', 'std', 'min', 'max', 'sum', 'median', 'ratio_min_max', 'coef_variation']\n        for stat in set(stats):\n            if stat not in allowed_stats:\n                raise ValueError(f\"Statistic '{stat}' is not allowed. Allowed stats are: {allowed_stats}.\")\n        n_stats = len(stats)\n        if not isinstance(window_sizes, (int, list)):\n            raise TypeError(f'`window_sizes` must be an int or a list of ints. Got {type(window_sizes)}.')\n        if isinstance(window_sizes, list):\n            n_window_sizes = len(window_sizes)\n            if n_window_sizes != n_stats:\n                raise ValueError(f'Length of `window_sizes` list ({n_window_sizes}) must match length of `stats` list ({n_stats}).')\n        if isinstance(window_sizes, int):\n            window_sizes = [window_sizes] * n_stats\n        if len(set(zip(stats, window_sizes))) != n_stats:\n            raise ValueError(f'Duplicate (stat, window_size) pairs are not allowed.\\n    `stats`       : {stats}\\n    `window_sizes : {window_sizes}')\n        if not isinstance(min_periods, (int, list, type(None))):\n            raise TypeError(f'`min_periods` must be an int, list of ints, or None. Got {type(min_periods)}.')\n        if min_periods is not None:\n            if isinstance(min_periods, int):\n                min_periods = [min_periods] * n_stats\n            elif isinstance(min_periods, list):\n                n_min_periods = len(min_periods)\n                if n_min_periods != n_stats:\n                    raise ValueError(f'Length of `min_periods` list ({n_min_periods}) must match length of `stats` list ({n_stats}).')\n            for i, min_period in enumerate(min_periods):\n                if min_period > window_sizes[i]:\n                    raise ValueError('Each `min_period` must be less than or equal to its corresponding `window_size`.')\n        if not isinstance(features_names, (list, type(None))):\n            raise TypeError(f'`features_names` must be a list of strings or None. Got {type(features_names)}.')\n        if isinstance(features_names, list):\n            n_features_names = len(features_names)\n            if n_features_names != n_stats:\n                raise ValueError(f'Length of `features_names` list ({n_features_names}) must match length of `stats` list ({n_stats}).')\n        if fillna is not None:\n            if not isinstance(fillna, (int, float, str)):\n                raise TypeError(f'`fillna` must be a float, string, or None. Got {type(fillna)}.')\n            if isinstance(fillna, str):\n                allowed_fill_strategy = ['mean', 'median', 'ffill', 'bfill']\n                if fillna not in allowed_fill_strategy:\n                    raise ValueError(f\"'{fillna}' is not allowed. Allowed `fillna` values are: {allowed_fill_strategy} or a float value.\")\n\n    def _apply_stat_pandas(self, rolling_obj: pd.core.window.rolling.Rolling, stat: str) -> pd.Series:\n        \"\"\"\n        Apply the specified statistic to a pandas rolling object.\n\n        Parameters\n        ----------\n        rolling_obj : pandas Rolling\n            Rolling object to apply the statistic.\n        stat : str\n            Statistic to compute.\n        \n        Returns\n        -------\n        stat_series : pandas Series\n            Series with the computed statistic.\n        \n        \"\"\"\n        if stat == 'mean':\n            return rolling_obj.mean()\n        elif stat == 'std':\n            return rolling_obj.std()\n        elif stat == 'min':\n            return rolling_obj.min()\n        elif stat == 'max':\n            return rolling_obj.max()\n        elif stat == 'sum':\n            return rolling_obj.sum()\n        elif stat == 'median':\n            return rolling_obj.median()\n        elif stat == 'ratio_min_max':\n            return rolling_obj.min() / rolling_obj.max()\n        elif stat == 'coef_variation':\n            return rolling_obj.std() / rolling_obj.mean()\n        else:\n            raise ValueError(f\"Statistic '{stat}' is not implemented.\")\n\n    def transform_batch(self, X: pd.Series) -> pd.DataFrame:\n        \"\"\"\n        Transform an entire pandas Series using rolling windows and compute the \n        specified statistics.\n\n        Parameters\n        ----------\n        X : pandas Series\n            The input data series to transform.\n\n        Returns\n        -------\n        rolling_features : pandas DataFrame\n            A DataFrame containing the rolling features.\n        \n        \"\"\"\n        for k in self.unique_rolling_windows.keys():\n            rolling_obj = X.rolling(**self.unique_rolling_windows[k]['params'])\n            self.unique_rolling_windows[k]['rolling_obj'] = rolling_obj\n        rolling_features = []\n        for i, stat in enumerate(self.stats):\n            window_size = self.window_sizes[i]\n            min_periods = self.min_periods[i]\n            key = f'{window_size}_{min_periods}'\n            rolling_obj = self.unique_rolling_windows[key]['rolling_obj']\n            stat_series = self._apply_stat_pandas(rolling_obj=rolling_obj, stat=stat)\n            rolling_features.append(stat_series)\n        rolling_features = pd.concat(rolling_features, axis=1)\n        rolling_features.columns = self.features_names\n        rolling_features = rolling_features.iloc[self.max_window_size:]\n        if self.fillna is not None:\n            if self.fillna == 'mean':\n                rolling_features = rolling_features.fillna(rolling_features.mean())\n            elif self.fillna == 'median':\n                rolling_features = rolling_features.fillna(rolling_features.median())\n            elif self.fillna == 'ffill':\n                rolling_features = rolling_features.ffill()\n            elif self.fillna == 'bfill':\n                rolling_features = rolling_features.bfill()\n            else:\n                rolling_features = rolling_features.fillna(self.fillna)\n        return rolling_features\n\n    def _apply_stat_numpy_jit(self, X_window: np.ndarray, stat: str) -> float:\n        \"\"\"\n        Apply the specified statistic to a numpy array using Numba JIT.\n\n        Parameters\n        ----------\n        X_window : numpy array\n            Array with the rolling window.\n        stat : str\n            Statistic to compute.\n\n        Returns\n        -------\n        stat_value : float\n            Value of the computed statistic.\n        \n        \"\"\"\n        if stat == 'mean':\n            return _np_mean_jit(X_window)\n        elif stat == 'std':\n            return _np_std_jit(X_window)\n        elif stat == 'min':\n            return _np_min_jit(X_window)\n        elif stat == 'max':\n            return _np_max_jit(X_window)\n        elif stat == 'sum':\n            return _np_sum_jit(X_window)\n        elif stat == 'median':\n            return _np_median_jit(X_window)\n        elif stat == 'ratio_min_max':\n            return _np_min_max_ratio_jit(X_window)\n        elif stat == 'coef_variation':\n            return _np_cv_jit(X_window)\n        else:\n            raise ValueError(f\"Statistic '{stat}' is not implemented.\")\n\n    def transform(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Transform a numpy array using rolling windows and compute the \n        specified statistics. The returned array will have the shape \n        (X.shape[1] if exists, n_stats). For example, if X is a flat\n        array, the output will have shape (n_stats,). If X is a 2D array,\n        the output will have shape (X.shape[1], n_stats).\n\n        Parameters\n        ----------\n        X : numpy ndarray\n            The input data array to transform.\n\n        Returns\n        -------\n        rolling_features : numpy ndarray\n            An array containing the computed statistics.\n        \n        \"\"\"\n        array_ndim = X.ndim\n        if array_ndim == 1:\n            X = X[:, np.newaxis]\n        rolling_features = np.full(shape=(X.shape[1], self.n_stats), fill_value=np.nan, dtype=float)\n        for i in range(X.shape[1]):\n            for j, stat in enumerate(self.stats):\n                X_window = X[-self.window_sizes[j]:, i]\n                X_window = X_window[~np.isnan(X_window)]\n                if len(X_window) > 0:\n                    rolling_features[i, j] = self._apply_stat_numpy_jit(X_window, stat)\n                else:\n                    rolling_features[i, j] = np.nan\n        if array_ndim == 1:\n            rolling_features = rolling_features.ravel()\n        return rolling_features\n\nclass QuantileBinner:\n    \"\"\"\n    QuantileBinner class to bin data into quantile-based bins using `numpy.percentile`.\n    This class is similar to `KBinsDiscretizer` but faster for binning data into\n    quantile-based bins. Bin  intervals are defined following the convention:\n    bins[i-1] <= x < bins[i]. See more information in `numpy.percentile` and\n    `numpy.digitize`.\n    \n    Parameters\n    ----------\n    n_bins : int\n        The number of quantile-based bins to create.\n    method : str, default='linear'\n        The method used to compute the quantiles. This parameter is passed to \n        `numpy.percentile`. Default is 'linear'. Valid values are \"inverse_cdf\",\n        \"averaged_inverse_cdf\", \"closest_observation\", \"interpolated_inverse_cdf\",\n        \"hazen\", \"weibull\", \"linear\", \"median_unbiased\", \"normal_unbiased\".\n    subsample : int, default=200000\n        The number of samples to use for computing quantiles. If the dataset \n        has more samples than `subsample`, a random subset will be used.\n    random_state : int, default=789654\n        The random seed to use for generating a random subset of the data.\n    dtype : data type, default=numpy.float64\n        The data type to use for the bin indices. Default is `numpy.float64`.\n    \n    Attributes\n    ----------\n    n_bins : int\n        The number of quantile-based bins to create.\n    method : str, default='linear'\n        The method used to compute the quantiles. This parameter is passed to \n        `numpy.percentile`. Default is 'linear'. Valid values are 'linear',\n        'lower', 'higher', 'midpoint', 'nearest'.\n    subsample : int, default=200000\n        The number of samples to use for computing quantiles. If the dataset \n        has more samples than `subsample`, a random subset will be used.\n    random_state : int, default=789654\n        The random seed to use for generating a random subset of the data.\n    dtype : data type, default=numpy.float64\n        The data type to use for the bin indices. Default is `numpy.float64`.\n    n_bins_ : int\n        The number of bins learned during fitting.\n    bin_edges_ : numpy ndarray\n        The edges of the bins learned during fitting.\n    \n    \"\"\"\n\n    def __init__(self, n_bins: int, method: Optional[str]='linear', subsample: int=200000, dtype: Optional[type]=np.float64, random_state: Optional[int]=789654):\n        self._validate_params(n_bins, method, subsample, dtype, random_state)\n        self.n_bins = n_bins\n        self.method = method\n        self.subsample = subsample\n        self.random_state = random_state\n        self.dtype = dtype\n        self.n_bins_ = None\n        self.bin_edges_ = None\n        self.intervals_ = None\n\n    def _validate_params(self, n_bins: int, method: str, subsample: int, dtype: type, random_state: int):\n        \"\"\"\n        Validate the parameters passed to the class initializer.\n        \"\"\"\n        if not isinstance(n_bins, int) or n_bins < 2:\n            raise ValueError(f'`n_bins` must be an int greater than 1. Got {n_bins}.')\n        valid_methods = ['inverse_cdf', 'averaged_inverse_cdf', 'closest_observation', 'interpolated_inverse_cdf', 'hazen', 'weibull', 'linear', 'median_unbiased', 'normal_unbiased']\n        if method not in valid_methods:\n            raise ValueError(f'`method` must be one of {valid_methods}. Got {method}.')\n        if not isinstance(subsample, int) or subsample < 1:\n            raise ValueError(f'`subsample` must be an integer greater than or equal to 1. Got {subsample}.')\n        if not isinstance(random_state, int) or random_state < 0:\n            raise ValueError(f'`random_state` must be an integer greater than or equal to 0. Got {random_state}.')\n        if not isinstance(dtype, type):\n            raise ValueError(f'`dtype` must be a valid numpy dtype. Got {dtype}.')\n\n    def fit(self, X: np.ndarray):\n        \"\"\"\n        Learn the bin edges based on quantiles from the training data.\n        \n        Parameters\n        ----------\n        X : numpy ndarray\n            The training data used to compute the quantiles.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        if X.size == 0:\n            raise ValueError('Input data `X` cannot be empty.')\n        if len(X) > self.subsample:\n            rng = np.random.default_rng(self.random_state)\n            X = X[rng.integers(0, len(X), self.subsample)]\n        self.bin_edges_ = np.percentile(a=X, q=np.linspace(0, 100, self.n_bins + 1), method=self.method)\n        self.n_bins_ = len(self.bin_edges_) - 1\n        self.intervals_ = {float(i): (float(self.bin_edges_[i]), float(self.bin_edges_[i + 1])) for i in range(self.n_bins_)}\n\n    def transform(self, X: np.ndarray):\n        \"\"\"\n        Assign new data to the learned bins.\n        \n        Parameters\n        ----------\n        X : numpy ndarray\n            The data to assign to the bins.\n        \n        Returns\n        -------\n        bin_indices : numpy ndarray \n            The indices of the bins each value belongs to.\n            Values less than the smallest bin edge are assigned to the first bin,\n            and values greater than the largest bin edge are assigned to the last bin.\n       \n        \"\"\"\n        if self.bin_edges_ is None:\n            raise NotFittedError(\"The model has not been fitted yet. Call 'fit' with training data first.\")\n        bin_indices = np.digitize(X, bins=self.bin_edges_, right=False)\n        bin_indices = np.clip(bin_indices, 1, self.n_bins_).astype(self.dtype) - 1\n        return bin_indices\n\n    def fit_transform(self, X):\n        \"\"\"\n        Fit the model to the data and return the bin indices for the same data.\n        \n        Parameters\n        ----------\n        X : numpy.ndarray\n            The data to fit and transform.\n        \n        Returns\n        -------\n        bin_indices : numpy.ndarray\n            The indices of the bins each value belongs to.\n            Values less than the smallest bin edge are assigned to the first bin,\n            and values greater than the largest bin edge are assigned to the last bin.\n        \n        \"\"\"\n        self.fit(X)\n        return self.transform(X)\n\n    def get_params(self):\n        \"\"\"\n        Get the parameters of the quantile binner.\n        \n        Parameters\n        ----------\n        self\n        \n        Returns\n        -------\n        params : dict\n            A dictionary of the parameters of the quantile binner.\n        \n        \"\"\"\n        return {'n_bins': self.n_bins, 'method': self.method, 'subsample': self.subsample, 'dtype': self.dtype, 'random_state': self.random_state}\n\n    def set_params(self, **params):\n        \"\"\"\n        Set the parameters of the QuantileBinner.\n        \n        Parameters\n        ----------\n        params : dict\n            A dictionary of the parameters to set.\n\n        Returns\n        -------\n        None\n        \n        \"\"\"\n        for param, value in params.items():\n            setattr(self, param, value)"
  },
  "call_tree": {
    "skforecast/preprocessing/tests/tests_preprocessing/test_check_X_numpy_ndarray_1d.py:test_TypeError_decorator_check_X_numpy_ndarray_1d_when_X_not_numpy_ndarray": {
      "skforecast/preprocessing/preprocessing.py:TimeSeriesDifferentiator:__init__": {},
      "skforecast/preprocessing/preprocessing.py:wrapper": {}
    },
    "skforecast/preprocessing/tests/tests_preprocessing/test_check_X_numpy_ndarray_1d.py:test_ValueError_decorator_check_X_numpy_ndarray_1d_when_X_not_1D_numpy_ndarray": {
      "skforecast/preprocessing/preprocessing.py:TimeSeriesDifferentiator:__init__": {},
      "skforecast/preprocessing/preprocessing.py:wrapper": {}
    }
  }
}