{
  "dir_path": "/app/elasticsearch_curator",
  "package_name": "elasticsearch_curator",
  "sample_name": "elasticsearch_curator-test_action_delete_snapshots",
  "src_dir": "curator/",
  "test_dir": "tests/",
  "test_file": "tests/unit/test_action_delete_snapshots.py",
  "test_code": "\"\"\"test_action_delete_snapshots\"\"\"\nfrom unittest import TestCase\nfrom unittest.mock import Mock\nfrom curator.actions import DeleteSnapshots\nfrom curator.exceptions import FailedExecution\nfrom curator import SnapshotList\n# Get test variables and constants from a single source\nfrom . import testvars as testvars\n\nclass TestActionDeleteSnapshots(TestCase):\n    def test_init_raise(self):\n        self.assertRaises(TypeError, DeleteSnapshots, 'invalid')\n    def test_init(self):\n        client = Mock()\n        client.snapshot.get.return_value = testvars.snapshots\n        client.snapshot.get_repository.return_value = testvars.test_repo\n        slo = SnapshotList(client, repository=testvars.repo_name)\n        do = DeleteSnapshots(slo)\n        self.assertEqual(slo, do.snapshot_list)\n        self.assertEqual(client, do.client)\n    def test_do_dry_run(self):\n        client = Mock()\n        client.snapshot.get.return_value = testvars.snapshots\n        client.snapshot.get_repository.return_value = testvars.test_repo\n        client.tasks.get.return_value = testvars.no_snap_tasks\n        client.snapshot.delete.return_value = None\n        slo = SnapshotList(client, repository=testvars.repo_name)\n        do = DeleteSnapshots(slo)\n        self.assertIsNone(do.do_dry_run())\n    def test_do_action(self):\n        client = Mock()\n        client.snapshot.get.return_value = testvars.snapshots\n        client.snapshot.get_repository.return_value = testvars.test_repo\n        client.tasks.list.return_value = testvars.no_snap_tasks\n        client.snapshot.delete.return_value = None\n        slo = SnapshotList(client, repository=testvars.repo_name)\n        do = DeleteSnapshots(slo)\n        self.assertIsNone(do.do_action())\n    def test_do_action_raises_exception(self):\n        client = Mock()\n        client.snapshot.get.return_value = testvars.snapshots\n        client.snapshot.get_repository.return_value = testvars.test_repo\n        client.snapshot.delete.return_value = None\n        client.tasks.list.return_value = testvars.no_snap_tasks\n        client.snapshot.delete.side_effect = testvars.fake_fail\n        slo = SnapshotList(client, repository=testvars.repo_name)\n        do = DeleteSnapshots(slo)\n        self.assertRaises(FailedExecution, do.do_action)\n    ### This check is not necessary after ES 7.16 as it is possible to have\n    ### up to 1000 concurrent snapshots\n    ###\n    ### https://www.elastic.co/guide/en/elasticsearch/reference/8.6/snapshot-settings.html\n    ### snapshot.max_concurrent_operations\n    ### (Dynamic, integer) Maximum number of concurrent snapshot operations. Defaults to 1000.\n    ###\n    ### This limit applies in total to all ongoing snapshot creation, cloning, and deletion\n    ### operations. Elasticsearch will reject any operations that would exceed this limit.\n    # def test_not_safe_to_snap_raises_exception(self):\n    #     client = Mock()\n    #     client.snapshot.get.return_value = testvars.inprogress\n    #     client.snapshot.get_repository.return_value = testvars.test_repo\n    #     client.tasks.list.return_value = testvars.no_snap_tasks\n    #     slo = SnapshotList(client, repository=testvars.repo_name)\n    #     do = DeleteSnapshots(slo, retry_interval=0, retry_count=1)\n    #     self.assertRaises(curator.FailedExecution, do.do_action)\n",
  "GT_file_code": {
    "curator/snapshotlist.py": "\"\"\"SnapshotList\"\"\"\nimport re\nimport logging\nfrom es_client.helpers.schemacheck import SchemaCheck\nfrom curator.exceptions import ConfigurationError, FailedExecution, MissingArgument, NoSnapshots\nfrom curator.helpers.date_ops import (\n    absolute_date_range, date_range, fix_epoch, get_date_regex, get_point_of_reference,\n    TimestringSearch\n)\nfrom curator.helpers.getters import get_snapshot_data\nfrom curator.helpers.testers import repository_exists, verify_client_object\nfrom curator.helpers.utils import report_failure\nfrom curator.defaults import settings\nfrom curator.validators.filter_functions import filterstructure\n\nclass SnapshotList:\n    \"\"\"Snapshot list object\"\"\"\n    def __init__(self, client, repository=None):\n        verify_client_object(client)\n        if not repository:\n            raise MissingArgument('No value for \"repository\" provided')\n        if not repository_exists(client, repository):\n            raise FailedExecution(\n                f'Unable to verify existence of repository {repository}')\n        self.loggit = logging.getLogger('curator.snapshotlist')\n        #: An :py:class:`~.elasticsearch.Elasticsearch` client object passed from param ``client``\n        self.client = client\n        #: The value passed as ``delete_aliases``\n        self.repository = repository\n        #: Information extracted from snapshots, such as age, etc.\n        #: Populated by internal method ``__get_snapshots`` at instance creation\n        #: time. **Type:** :py:class:`dict`\n        self.snapshot_info = {}\n        #: The running list of snapshots which will be used by an Action class.\n        #: Populated by internal methods ``__get_snapshots`` at instance creation\n        #: time. **Type:** :py:class:`list`\n        self.snapshots = []\n        #: Raw data dump of all snapshots in the repository at instance creation\n        #: time.  **Type:** :py:class:`list` of :py:class:`dict` data.\n        self.__get_snapshots()\n        self.age_keyfield = None\n\n    def __actionable(self, snap):\n        self.loggit.debug(\n            'Snapshot %s is actionable and remains in the list.', snap)\n\n    def __not_actionable(self, snap):\n        self.loggit.debug('Snapshot %s is not actionable, removing from list.', snap)\n        self.snapshots.remove(snap)\n\n    def __excludify(self, condition, exclude, snap, msg=None):\n        if condition:\n            if exclude:\n                text = \"Removed from actionable list\"\n                self.__not_actionable(snap)\n            else:\n                text = \"Remains in actionable list\"\n                self.__actionable(snap)\n        else:\n            if exclude:\n                text = \"Remains in actionable list\"\n                self.__actionable(snap)\n            else:\n                text = \"Removed from actionable list\"\n                self.__not_actionable(snap)\n        if msg:\n            self.loggit.debug('%s: %s', text, msg)\n\n    def __get_snapshots(self):\n        \"\"\"\n        Pull all snapshots into `snapshots` and populate ``snapshot_info``\n        \"\"\"\n        self.all_snapshots = get_snapshot_data(self.client, self.repository)\n        for list_item in self.all_snapshots:\n            if 'snapshot' in list_item.keys():\n                self.snapshots.append(list_item['snapshot'])\n                self.snapshot_info[list_item['snapshot']] = list_item\n        self.empty_list_check()\n\n    def __map_method(self, ftype):\n        methods = {\n            'age': self.filter_by_age,\n            'count': self.filter_by_count,\n            'none': self.filter_none,\n            'pattern': self.filter_by_regex,\n            'period': self.filter_period,\n            'state': self.filter_by_state,\n        }\n        return methods[ftype]\n\n    def empty_list_check(self):\n        \"\"\"Raise exception if ``snapshots`` is empty\"\"\"\n        if not self.snapshots:\n            raise NoSnapshots('snapshot_list object is empty.')\n\n    def working_list(self):\n        \"\"\"\n        Return the current value of ``snapshots`` as copy-by-value to prevent list stomping during\n        iterations\n        \"\"\"\n        # Copy by value, rather than reference to prevent list stomping during\n        # iterations\n        return self.snapshots[:]\n\n    def _get_name_based_ages(self, timestring):\n        \"\"\"\n        Add a snapshot age to ``snapshot_info`` based on the age as indicated by the snapshot name\n        pattern, if it matches ``timestring``.  This is stored at key ``age_by_name``.\n\n        :param timestring: A :py:func:`time.strftime` pattern\n        \"\"\"\n        # Check for empty list before proceeding here to prevent non-iterable\n        # condition\n        self.empty_list_check()\n        tstamp = TimestringSearch(timestring)\n        for snapshot in self.working_list():\n            epoch = tstamp.get_epoch(snapshot)\n            if epoch:\n                self.snapshot_info[snapshot]['age_by_name'] = epoch\n            else:\n                self.snapshot_info[snapshot]['age_by_name'] = None\n\n    def _calculate_ages(self, source='creation_date', timestring=None):\n        \"\"\"\n        This method initiates snapshot age calculation based on the given parameters.  Exceptions\n        are raised when they are improperly configured.\n\n        Set instance variable ``age_keyfield`` for use later, if needed.\n\n        :param source: Source of snapshot age. Can be ``name`` or ``creation_date``.\n        :param timestring: An :py:func:`time.strftime` string to match the datestamp in an snapshot name. Only used\n            if ``source=name``.\n        \"\"\"\n        if source == 'name':\n            self.age_keyfield = 'age_by_name'\n            if not timestring:\n                raise MissingArgument('source \"name\" requires the \"timestring\" keyword argument')\n            self._get_name_based_ages(timestring)\n        elif source == 'creation_date':\n            self.age_keyfield = 'start_time_in_millis'\n        else:\n            raise ValueError(f'Invalid source: {source}. Must be \"name\", or \"creation_date\".')\n\n    def _sort_by_age(self, snapshot_list, reverse=True):\n        \"\"\"\n        Take a list of snapshots and sort them by date.\n\n        By default, the youngest are first with ``reverse=True``, but the oldest can be first by\n        setting ``reverse=False``\n        \"\"\"\n        # Do the age-based sorting here.\n        # First, build an temporary dictionary with just snapshot and age\n        # as the key and value, respectively\n        temp = {}\n        for snap in snapshot_list:\n            if self.age_keyfield in self.snapshot_info[snap]:\n                # This fixes #1366. Catch None is a potential age value.\n                if self.snapshot_info[snap][self.age_keyfield]:\n                    temp[snap] = self.snapshot_info[snap][self.age_keyfield]\n                else:\n                    msg = ' snapshot %s has no age' % snap\n                    self.__excludify(True, True, snap, msg)\n            else:\n                msg = (\n                    f'{snap} does not have age key \"{self.age_keyfield}\" in SnapshotList metadata')\n                self.__excludify(True, True, snap, msg)\n\n        # If reverse is True, this will sort so the youngest snapshots are\n        # first.  However, if you want oldest first, set reverse to False.\n        # Effectively, this should set us up to act on everything older than\n        # meets the other set criteria.\n        # It starts as a tuple, but then becomes a list.\n        sorted_tuple = (\n            sorted(temp.items(), key=lambda k: k[1], reverse=reverse)\n        )\n        return [x[0] for x in sorted_tuple]\n\n    def most_recent(self):\n        \"\"\"\n        Return the most recent snapshot based on ``start_time_in_millis``.\n        \"\"\"\n        self.empty_list_check()\n        most_recent_time = 0\n        most_recent_snap = ''\n        for snapshot in self.snapshots:\n            snaptime = fix_epoch(self.snapshot_info[snapshot]['start_time_in_millis'])\n            if snaptime > most_recent_time:\n                most_recent_snap = snapshot\n                most_recent_time = snaptime\n        return most_recent_snap\n\n\n    def filter_by_regex(self, kind=None, value=None, exclude=False):\n        \"\"\"\n        Filter out snapshots not matching the pattern, or in the case of\n        exclude, filter those matching the pattern.\n\n        :param kind: Can be one of: ``suffix``, ``prefix``, ``regex``, or\n            ``timestring``. This option defines what kind of filter you will be\n            building.\n        :param value: Depends on ``kind``. It is the :py:func:`time.strftime` string if ``kind`` is\n            ``timestring``. It's used to build the regular expression for other kinds.\n        :param exclude: If ``exclude=True``, this filter will remove matching snapshots from\n            ``snapshots``. If ``exclude=False``, then only matching snapshots will be kept in\n            ``snapshots``. Default is ``False``\n        \"\"\"\n        if kind not in ['regex', 'prefix', 'suffix', 'timestring']:\n            raise ValueError(f'{kind}: Invalid value for kind')\n\n        # Stop here if None or empty value, but zero is okay\n        if value == 0:\n            pass\n        elif not value:\n            raise ValueError(\n                f'{value}: Invalid value for \"value\". Cannot be \"None\" type, empty, or False')\n\n        if kind == 'timestring':\n            regex = settings.regex_map()[kind].format(get_date_regex(value))\n        else:\n            regex = settings.regex_map()[kind].format(value)\n\n        self.empty_list_check()\n        pattern = re.compile(regex)\n        for snapshot in self.working_list():\n            match = pattern.search(snapshot)\n            self.loggit.debug('Filter by regex: Snapshot: %s', snapshot)\n            if match:\n                self.__excludify(True, exclude, snapshot)\n            else:\n                self.__excludify(False, exclude, snapshot)\n\n    def filter_by_age(\n            self, source='creation_date', direction=None, timestring=None, unit=None,\n            unit_count=None, epoch=None, exclude=False):\n        \"\"\"\n        Remove snapshots from ``snapshots`` by relative age calculations.\n\n        :param source: Source of snapshot age. Can be ``name``, or ``creation_date``.\n        :param direction: Time to filter, either ``older`` or ``younger``\n        :param timestring: A :py:func:`time.strftime` string to match the datestamp in an snapshot\n            name. Only used for snapshot filtering by ``name``.\n        :param unit: One of ``seconds``, ``minutes``, ``hours``, ``days``, ``weeks``, ``months``, or\n            ``years``.\n        :param unit_count: The number of ``unit`` (s). ``unit_count`` * ``unit`` will be calculated\n            out to the relative number of seconds.\n        :param epoch: An epoch timestamp used in conjunction with ``unit`` and ``unit_count`` to\n            establish a point of reference for calculations. If not provided, the current time will\n            be used.\n        :param exclude: If ``exclude=True``, this filter will remove matching snapshots from\n            ``snapshots``. If ``exclude=False``, then only matching snapshots will be kept in\n            ``snapshots``. Default is ``False``\n        \"\"\"\n        self.loggit.debug('Starting filter_by_age')\n        # Get timestamp point of reference, por\n        por = get_point_of_reference(unit, unit_count, epoch)\n        self.loggit.debug('Point of Reference: %s', por)\n        if not direction:\n            raise MissingArgument('Must provide a value for \"direction\"')\n        if direction not in ['older', 'younger']:\n            raise ValueError(f'Invalid value for \"direction\": {direction}')\n        self._calculate_ages(source=source, timestring=timestring)\n        for snapshot in self.working_list():\n            if not self.snapshot_info[snapshot][self.age_keyfield]:\n                self.loggit.debug('Removing snapshot %s for having no age', snapshot)\n                self.snapshots.remove(snapshot)\n                continue\n            age = fix_epoch(self.snapshot_info[snapshot][self.age_keyfield])\n            msg = (\n                f'Snapshot \"{snapshot}\" age ({age}), direction: \"{direction}\", point of '\n                f'reference, ({por})'\n            )\n            # Because time adds to epoch, smaller numbers are actually older\n            # timestamps.\n            snapshot_age = fix_epoch(self.snapshot_info[snapshot][self.age_keyfield])\n            if direction == 'older':\n                agetest = snapshot_age < por\n            else: # 'younger'\n                agetest = snapshot_age > por\n            self.__excludify(agetest, exclude, snapshot, msg)\n\n    def filter_by_state(self, state=None, exclude=False):\n        \"\"\"\n        Filter out snapshots not matching ``state``, or in the case of exclude, filter those\n        matching ``state``.\n\n        :param state: The snapshot state to filter for. Must be one of ``SUCCESS``, ``PARTIAL``,\n            ``FAILED``, or ``IN_PROGRESS``.\n        :param exclude: If ``exclude=True``, this filter will remove matching snapshots from\n            ``snapshots``. If ``exclude=False``, then only matching snapshots will be kept in\n            ``snapshots``. Default is ``False``\n        \"\"\"\n        if state.upper() not in ['SUCCESS', 'PARTIAL', 'FAILED', 'IN_PROGRESS']:\n            raise ValueError(f'{state}: Invalid value for state')\n        self.empty_list_check()\n        for snapshot in self.working_list():\n            self.loggit.debug('Filter by state: Snapshot: %s', snapshot)\n            if self.snapshot_info[snapshot]['state'] == state:\n                self.__excludify(True, exclude, snapshot)\n            else:\n                self.__excludify(False, exclude, snapshot)\n\n    def filter_none(self):\n        \"\"\"No filter at all\"\"\"\n        self.loggit.debug('\"None\" filter selected.  No filtering will be done.')\n\n    def filter_by_count(\n            self, count=None, reverse=True, use_age=False,\n            source='creation_date', timestring=None, exclude=True\n    ):\n        \"\"\"\n        Remove snapshots from the actionable list beyond the number ``count``, sorted\n        reverse-alphabetically by default.  If you set ``reverse=False``, it will be sorted\n        alphabetically.\n\n        The default is usually what you will want. If only one kind of snapshot is provided--for\n        example, snapshots matching ``curator-%Y%m%d%H%M%S``--then reverse alphabetical sorting\n        will mean the oldest will remain in the list, because lower numbers in the dates mean older\n        snapshots.\n\n        By setting ``reverse=False``, then ``snapshot3`` will be acted on before ``snapshot2``,\n        which will be acted on before ``snapshot1``\n\n        ``use_age`` allows ordering snapshots by age. Age is determined by the snapshot creation\n        date (as identified by ``start_time_in_millis``) by default, but you can also specify\n        ``source=name``.  The ``name`` ``source`` requires the timestring argument.\n\n        :param count: Filter snapshots beyond ``count``.\n        :param reverse: The filtering direction. (default: ``True``).\n        :param use_age: Sort snapshots by age.  ``source`` is required in this case.\n        :param source: Source of snapshot age. Can be one of ``name``, or ``creation_date``.\n            Default: ``creation_date``\n        :param timestring: A :py:func:`time.strftime` string to match the datestamp in a snapshot\n            name. Only used if ``source=name``.\n        :param exclude: If ``exclude=True``, this filter will remove matching snapshots from\n            ``snapshots``. If ``exclude=False``, then only matching snapshots will be kept in\n            ``snapshots``. Default is ``True``\n        \"\"\"\n        self.loggit.debug('Filtering snapshots by count')\n        if not count:\n            raise MissingArgument('No value for \"count\" provided')\n        # Create a copy-by-value working list\n        working_list = self.working_list()\n        if use_age:\n            self._calculate_ages(source=source, timestring=timestring)\n            # Using default value of reverse=True in self._sort_by_age()\n            sorted_snapshots = self._sort_by_age(working_list, reverse=reverse)\n        else:\n            # Default to sorting by snapshot name\n            sorted_snapshots = sorted(working_list, reverse=reverse)\n        idx = 1\n        for snap in sorted_snapshots:\n            msg = (f'{snap} is {idx} of specified count of {count}.')\n            condition = True if idx <= count else False\n            self.__excludify(condition, exclude, snap, msg)\n            idx += 1\n\n    def filter_period(self, period_type='relative', source='name', range_from=None,\n        range_to=None, date_from=None, date_to=None, date_from_format=None, date_to_format=None,\n        timestring=None, unit=None, week_starts_on='sunday', epoch=None, exclude=False):\n        \"\"\"\n        Match ``snapshots`` with ages within a given period.\n\n        :param period_type: Can be either ``absolute`` or ``relative``.  Default is ``relative``.\n            ``date_from`` and ``date_to`` are required when using ``period_type='absolute'``.\n            ``range_from`` and ``range_to`` are required with ``period_type='relative'``.\n        :param source: Source of snapshot age. Can be ``name``, or ``creation_date``.\n        :param range_from: How many ``unit`` (s) in the past/future is the origin?\n        :param range_to: How many ``unit`` (s) in the past/future is the end point?\n        :param date_from: The simplified date for the start of the range\n        :param date_to: The simplified date for the end of the range.  If this value\n            is the same as ``date_from``, the full value of ``unit`` will be\n            extrapolated for the range.  For example, if ``unit=months``,\n            and ``date_from`` and ``date_to`` are both ``2017.01``, then the entire\n            month of January 2017 will be the absolute date range.\n        :param date_from_format: The :py:func:`time.strftime` string used to parse ``date_from``\n        :param date_to_format: The :py:func:`time.strftime` string used to parse ``date_to``\n        :param timestring: An :py:func:`time.strftime` string to match the datestamp in an\n            snapshot name. Only used for snapshot filtering by ``name``.\n        :param unit: One of ``hours``, ``days``, ``weeks``, ``months``, or ``years``.\n        :param week_starts_on: Either ``sunday`` or ``monday``. Default is ``sunday``\n        :param epoch: An epoch timestamp used to establish a point of reference\n            for calculations. If not provided, the current time will be used.\n        :param exclude: If ``exclude=True``, this filter will remove matching indices from\n            ``indices``. If ``exclude=False``, then only matching indices will be kept in\n            ``indices``. Default is ``False``\n        \"\"\"\n        self.loggit.debug('Filtering snapshots by period')\n        if period_type not in ['absolute', 'relative']:\n            raise ValueError(\n                f'Unacceptable value: {period_type} -- \"period_type\" must be either '\n                f'\"absolute\" or \"relative\".'\n            )\n        self.loggit.debug('period_type = %s', period_type)\n        if period_type == 'relative':\n            func = date_range\n            args = [unit, range_from, range_to, epoch]\n            kwgs = {'week_starts_on': week_starts_on}\n            try:\n                range_from = int(range_from)\n                range_to = int(range_to)\n            except ValueError as err:\n                raise ConfigurationError(\n                    f'\"range_from\" and \"range_to\" must be integer values. Error: {err}') from err\n        else:\n            func = absolute_date_range\n            args = [unit, date_from, date_to]\n            kwgs = {\n                'date_from_format': date_from_format,\n                'date_to_format': date_to_format\n            }\n            for reqd in [date_from, date_to, date_from_format, date_to_format]:\n                if not reqd:\n                    raise ConfigurationError(\n                        'Must provide \"date_from\", \"date_to\", '\n                        '\"date_from_format\", and \"date_to_format\" with absolute period_type'\n                    )\n        try:\n            start, end = func(*args, **kwgs)\n        # pylint: disable=broad-except\n        except Exception as err:\n            report_failure(err)\n        self._calculate_ages(source=source, timestring=timestring)\n        for snapshot in self.working_list():\n            if not self.snapshot_info[snapshot][self.age_keyfield]:\n                self.loggit.debug('Removing snapshot {0} for having no age')\n                self.snapshots.remove(snapshot)\n                continue\n            age = fix_epoch(self.snapshot_info[snapshot][self.age_keyfield])\n            msg = (\n                f'Snapshot \"{snapshot}\" age ({age}), period start: \"{start}\", period '\n                f'end, ({end})'\n            )\n            # Because time adds to epoch, smaller numbers are actually older\n            # timestamps.\n            inrange = ((age >= start) and (age <= end))\n            self.__excludify(inrange, exclude, snapshot, msg)\n\n    def iterate_filters(self, config):\n        \"\"\"\n        Iterate over the filters defined in ``config`` and execute them.\n\n        :param config: A dictionary of filters, as extracted from the YAML configuration file.\n\n        .. note:: ``config`` should be a dictionary with the following form:\n        .. code-block:: python\n\n                { 'filters' : [\n                        {\n                            'filtertype': 'the_filter_type',\n                            'key1' : 'value1',\n                            ...\n                            'keyN' : 'valueN'\n                        }\n                    ]\n                }\n\n        \"\"\"\n        # Make sure we actually _have_ filters to act on\n        if not 'filters' in config or not config['filters']:\n            self.loggit.info('No filters in config.  Returning unaltered object.')\n            return\n        self.loggit.debug('All filters: %s', config['filters'])\n        for fltr in config['filters']:\n            self.loggit.debug('Top of the loop: %s', self.snapshots)\n            self.loggit.debug('Un-parsed filter args: %s', fltr)\n            filter_result = SchemaCheck(\n                fltr, filterstructure(), 'filter', 'SnapshotList.iterate_filters').result()\n            self.loggit.debug('Parsed filter args: %s', filter_result)\n            method = self.__map_method(fltr['filtertype'])\n            # Remove key 'filtertype' from dictionary 'fltr'\n            del fltr['filtertype']\n            # If it's a filtertype with arguments, update the defaults with the\n            # provided settings.\n            self.loggit.debug('Filter args: %s', fltr)\n            self.loggit.debug('Pre-instance: %s', self.snapshots)\n            method(**fltr)\n            self.loggit.debug('Post-instance: %s', self.snapshots)\n",
    "curator/helpers/testers.py": "\"\"\"Utility functions that get things\"\"\"\n\nimport logging\nfrom voluptuous import Schema\nfrom elasticsearch8 import Elasticsearch\nfrom elasticsearch8.exceptions import NotFoundError\nfrom es_client.helpers.schemacheck import SchemaCheck\nfrom es_client.helpers.utils import prune_nones\nfrom curator.helpers.getters import get_repository, get_write_index\nfrom curator.exceptions import (\n    ConfigurationError,\n    MissingArgument,\n    RepositoryException,\n    SearchableSnapshotException,\n)\nfrom curator.defaults.settings import (\n    index_filtertypes,\n    snapshot_actions,\n    snapshot_filtertypes,\n)\nfrom curator.validators import actions, options\nfrom curator.validators.filter_functions import validfilters\nfrom curator.helpers.utils import report_failure\n\n\ndef has_lifecycle_name(idx_settings):\n    \"\"\"\n    :param idx_settings: The settings for an index being tested\n    :type idx_settings: dict\n\n    :returns: ``True`` if a lifecycle name exists in settings, else ``False``\n    :rtype: bool\n    \"\"\"\n    if 'lifecycle' in idx_settings:\n        if 'name' in idx_settings['lifecycle']:\n            return True\n    return False\n\n\ndef is_idx_partial(idx_settings):\n    \"\"\"\n    :param idx_settings: The settings for an index being tested\n    :type idx_settings: dict\n\n    :returns: ``True`` if store.snapshot.partial exists in settings, else ``False``\n    :rtype: bool\n    \"\"\"\n    if 'store' in idx_settings:\n        if 'snapshot' in idx_settings['store']:\n            if 'partial' in idx_settings['store']['snapshot']:\n                if idx_settings['store']['snapshot']['partial']:\n                    return True\n                # store.snapshot.partial exists but is False -- Not a frozen tier mount\n                return False\n            # store.snapshot exists, but partial isn't there --\n            # Possibly a cold tier mount\n            return False\n        raise SearchableSnapshotException('Index not a mounted searchable snapshot')\n    raise SearchableSnapshotException('Index not a mounted searchable snapshot')\n\n\ndef ilm_policy_check(client, alias):\n    \"\"\"Test if alias is associated with an ILM policy\n\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.get_settings`\n\n    :param client: A client connection object\n    :param alias: The alias name\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type alias: str\n    :rtype: bool\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    # alias = action_obj.options['name']\n    write_index = get_write_index(client, alias)\n    try:\n        idx_settings = client.indices.get_settings(index=write_index)\n        if 'name' in idx_settings[write_index]['settings']['index']['lifecycle']:\n            # logger.info('Alias %s is associated with ILM policy.', alias)\n            # logger.info('Skipping action %s because allow_ilm_indices is false.', idx)\n            return True\n    except KeyError:\n        logger.debug('No ILM policies associated with %s', alias)\n    return False\n\n\ndef repository_exists(client, repository=None):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get_repository`\n\n    :param client: A client connection object\n    :param repository: The Elasticsearch snapshot repository to use\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type repository: str\n\n    :returns: ``True`` if ``repository`` exists, else ``False``\n    :rtype: bool\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    if not repository:\n        raise MissingArgument('No value for \"repository\" provided')\n    try:\n        test_result = get_repository(client, repository)\n        if repository in test_result:\n            logger.debug(\"Repository %s exists.\", repository)\n            response = True\n        else:\n            logger.debug(\"Repository %s not found...\", repository)\n            response = False\n    # pylint: disable=broad-except\n    except Exception as err:\n        logger.debug('Unable to find repository \"%s\": Error: %s', repository, err)\n        response = False\n    return response\n\n\ndef rollable_alias(client, alias):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.get_alias`\n\n    :param client: A client connection object\n    :param alias: An Elasticsearch alias\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type alias: str\n\n\n    :returns: ``True`` or ``False`` depending on whether ``alias`` is an alias that\n        points to an index that can be used by the ``_rollover`` API.\n    :rtype: bool\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    try:\n        response = client.indices.get_alias(name=alias)\n    except NotFoundError:\n        logger.error('Alias \"%s\" not found.', alias)\n        return False\n    # Response should be like:\n    # {'there_should_be_only_one': {'aliases': {'value of \"alias\" here': {}}}}\n    # where 'there_should_be_only_one' is a single index name that ends in a number,\n    # and 'value of \"alias\" here' reflects the value of the passed parameter, except\n    # where the ``is_write_index`` setting makes it possible to have more than one\n    # index associated with a rollover index\n    for idx in response:\n        if 'is_write_index' in response[idx]['aliases'][alias]:\n            if response[idx]['aliases'][alias]['is_write_index']:\n                return True\n    # implied ``else``: If not ``is_write_index``, it has to fit the following criteria:\n    if len(response) > 1:\n        logger.error(\n            '\"alias\" must only reference one index, but points to %s', response\n        )\n        return False\n    index = list(response.keys())[0]\n    rollable = False\n    # In order for `rollable` to be True, the last 2 digits of the index\n    # must be digits, or a hyphen followed by a digit.\n    # NOTE: This is not a guarantee that the rest of the index name is\n    # necessarily correctly formatted.\n    if index[-2:][1].isdigit():\n        if index[-2:][0].isdigit():\n            rollable = True\n        elif index[-2:][0] == '-':\n            rollable = True\n    return rollable\n\n\ndef snapshot_running(client):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get_repository`\n\n    Return ``True`` if a snapshot is in progress, and ``False`` if not\n\n    :param client: A client connection object\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n\n    :rtype: bool\n    \"\"\"\n    try:\n        status = client.snapshot.status()['snapshots']\n    # pylint: disable=broad-except\n    except Exception as exc:\n        report_failure(exc)\n    # We will only accept a positively identified False.  Anything else is\n    # suspect. That's why this statement, rather than just ``return status``\n    # pylint: disable=simplifiable-if-expression\n    return False if not status else True\n\n\ndef validate_actions(data):\n    \"\"\"\n    Validate the ``actions`` configuration dictionary, as imported from actions.yml,\n    for example.\n\n    :param data: The configuration dictionary\n\n    :type data: dict\n\n    :returns: The validated and sanitized configuration dictionary.\n    :rtype: dict\n    \"\"\"\n    # data is the ENTIRE schema...\n    clean_config = {}\n    # Let's break it down into smaller chunks...\n    # First, let's make sure it has \"actions\" as a key, with a subdictionary\n    root = SchemaCheck(data, actions.root(), 'Actions File', 'root').result()\n    # We've passed the first step.  Now let's iterate over the actions...\n    for action_id in root['actions']:\n        # Now, let's ensure that the basic action structure is correct, with\n        # the proper possibilities for 'action'\n        action_dict = root['actions'][action_id]\n        loc = f'Action ID \"{action_id}\"'\n        valid_structure = SchemaCheck(\n            action_dict, actions.structure(action_dict, loc), 'structure', loc\n        ).result()\n        # With the basic structure validated, now we extract the action name\n        current_action = valid_structure['action']\n        # And let's update the location with the action.\n        loc = f'Action ID \"{action_id}\", action \"{current_action}\"'\n        clean_options = SchemaCheck(\n            prune_nones(valid_structure['options']),\n            options.get_schema(current_action),\n            'options',\n            loc,\n        ).result()\n        clean_config[action_id] = {\n            'action': current_action,\n            'description': valid_structure['description'],\n            'options': clean_options,\n        }\n        if current_action == 'alias':\n            add_remove = {}\n            for k in ['add', 'remove']:\n                if k in valid_structure:\n                    current_filters = SchemaCheck(\n                        valid_structure[k]['filters'],\n                        Schema(validfilters(current_action, location=loc)),\n                        f'\"{k}\" filters',\n                        f'{loc}, \"filters\"',\n                    ).result()\n                    add_remove.update(\n                        {\n                            k: {\n                                'filters': SchemaCheck(\n                                    current_filters,\n                                    Schema(validfilters(current_action, location=loc)),\n                                    'filters',\n                                    f'{loc}, \"{k}\", \"filters\"',\n                                ).result()\n                            }\n                        }\n                    )\n            # Add/Remove here\n            clean_config[action_id].update(add_remove)\n        elif current_action in ['cluster_routing', 'create_index', 'rollover']:\n            # neither cluster_routing nor create_index should have filters\n            pass\n        else:  # Filters key only appears in non-alias actions\n            valid_filters = SchemaCheck(\n                valid_structure['filters'],\n                Schema(validfilters(current_action, location=loc)),\n                'filters',\n                f'{loc}, \"filters\"',\n            ).result()\n            clean_filters = validate_filters(current_action, valid_filters)\n            clean_config[action_id].update({'filters': clean_filters})\n        # This is a special case for remote reindex\n        if current_action == 'reindex':\n            # Check only if populated with something.\n            if 'remote_filters' in valid_structure['options']:\n                valid_filters = SchemaCheck(\n                    valid_structure['options']['remote_filters'],\n                    Schema(validfilters(current_action, location=loc)),\n                    'filters',\n                    f'{loc}, \"filters\"',\n                ).result()\n                clean_remote_filters = validate_filters(current_action, valid_filters)\n                clean_config[action_id]['options'].update(\n                    {'remote_filters': clean_remote_filters}\n                )\n\n    # if we've gotten this far without any Exceptions raised, it's valid!\n    return {'actions': clean_config}\n\n\ndef validate_filters(action, myfilters):\n    \"\"\"\n    Validate that myfilters are appropriate for the action type, e.g. no\n    index filters applied to a snapshot list.\n\n    :param action: An action name\n    :param myfilters: A list of filters to test.\n\n    :type action: str\n    :type myfilters: list\n\n    :returns: Validated list of filters\n    :rtype: list\n    \"\"\"\n    # Define which set of filtertypes to use for testing\n    if action in snapshot_actions():\n        filtertypes = snapshot_filtertypes()\n    else:\n        filtertypes = index_filtertypes()\n    for fil in myfilters:\n        if fil['filtertype'] not in filtertypes:\n            raise ConfigurationError(\n                f\"\\\"{fil['filtertype']}\\\" filtertype is not compatible with \"\n                f\"action \\\"{action}\\\"\"\n            )\n    # If we get to this point, we're still valid.  Return the original list\n    return myfilters\n\n\ndef verify_client_object(test):\n    \"\"\"\n    :param test: The variable or object to test\n\n    :type test: :py:class:`~.elasticsearch.Elasticsearch`\n\n    :returns: ``True`` if ``test`` is a proper :py:class:`~.elasticsearch.Elasticsearch`\n        client object and raise a :py:exc:`TypeError` exception if it is not.\n    :rtype: bool\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    # Ignore mock type for testing\n    if str(type(test)) == \"<class 'unittest.mock.Mock'>\":\n        pass\n    elif not isinstance(test, Elasticsearch):\n        msg = f'Not a valid client object. Type: {type(test)} was passed'\n        logger.error(msg)\n        raise TypeError(msg)\n\n\ndef verify_index_list(test):\n    \"\"\"\n    :param test: The variable or object to test\n\n    :type test: :py:class:`~.curator.IndexList`\n\n    :returns: ``None`` if ``test`` is a proper :py:class:`~.curator.indexlist.IndexList`\n        object, else raise a :py:class:`TypeError` exception.\n    :rtype: None\n    \"\"\"\n    # It breaks if this import isn't local to this function:\n    # ImportError: cannot import name 'IndexList' from partially initialized module\n    # 'curator.indexlist' (most likely due to a circular import)\n    # pylint: disable=import-outside-toplevel\n    from curator.indexlist import IndexList\n\n    logger = logging.getLogger(__name__)\n    if not isinstance(test, IndexList):\n        msg = f'Not a valid IndexList object. Type: {type(test)} was passed'\n        logger.error(msg)\n        raise TypeError(msg)\n\n\ndef verify_repository(client, repository=None):\n    \"\"\"\n    Do :py:meth:`~.elasticsearch.snapshot.verify_repository` call. If it fails, raise a\n    :py:exc:`~.curator.exceptions.RepositoryException`.\n\n    :param client: A client connection object\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :param repository: A repository name\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type repository: str\n\n    :rtype: None\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    try:\n        nodes = client.snapshot.verify_repository(name=repository)['nodes']\n        logger.debug('All nodes can write to the repository')\n        logger.debug('Nodes with verified repository access: %s', nodes)\n    except Exception as err:\n        try:\n            if err.status_code == 404:\n                msg = (\n                    f'--- Repository \"{repository}\" not found. Error: '\n                    f'{err.meta.status}, {err.error}'\n                )\n            else:\n                msg = (\n                    f'--- Got a {err.meta.status} response from Elasticsearch.  '\n                    f'Error message: {err.error}'\n                )\n        except AttributeError:\n            msg = f'--- Error message: {err}'.format()\n        report = f'Failed to verify all nodes have repository access: {msg}'\n        raise RepositoryException(report) from err\n\n\ndef verify_snapshot_list(test):\n    \"\"\"\n    :param test: The variable or object to test\n\n    :type test: :py:class:`~.curator.SnapshotList`\n\n    :returns: ``None`` if ``test`` is a proper\n        :py:class:`~.curator.snapshotlist.SnapshotList` object, else raise a\n        :py:class:`TypeError` exception.\n    :rtype: None\n    \"\"\"\n    # It breaks if this import isn't local to this function:\n    # ImportError: cannot import name 'SnapshotList' from partially initialized module\n    # 'curator.snapshotlist' (most likely due to a circular import)\n    # pylint: disable=import-outside-toplevel\n    from curator.snapshotlist import SnapshotList\n\n    logger = logging.getLogger(__name__)\n    if not isinstance(test, SnapshotList):\n        msg = f'Not a valid SnapshotList object. Type: {type(test)} was passed'\n        logger.error(msg)\n        raise TypeError(msg)\n",
    "curator/helpers/getters.py": "\"\"\"Utility functions that get things\"\"\"\n\nimport logging\nfrom elasticsearch8 import exceptions as es8exc\nfrom curator.exceptions import (\n    ConfigurationError,\n    CuratorException,\n    FailedExecution,\n    MissingArgument,\n)\n\n\ndef byte_size(num, suffix='B'):\n    \"\"\"\n    :param num: The number of byte\n    :param suffix: An arbitrary suffix, like ``Bytes``\n\n    :type num: int\n    :type suffix: str\n\n    :returns: A formatted string indicating the size in bytes, with the proper unit,\n        e.g. KB, MB, GB, TB, etc.\n    :rtype: float\n    \"\"\"\n    for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:\n        if abs(num) < 1024.0:\n            return f'{num:3.1f}{unit}{suffix}'\n        num /= 1024.0\n    return f'{num:.1f}Y{suffix}'\n\n\ndef escape_dots(stringval):\n    \"\"\"\n    Escape any dots (periods) in ``stringval``.\n\n    Primarily used for ``filter_path`` where dots are indicators of path nesting\n\n    :param stringval: A string, ostensibly an index name\n\n    :type stringval: str\n\n    :returns: ``stringval``, but with any periods escaped with a backslash\n    :retval: str\n    \"\"\"\n    return stringval.replace('.', r'\\.')\n\n\ndef get_alias_actions(oldidx, newidx, aliases):\n    \"\"\"\n    :param oldidx: The old index name\n    :param newidx: The new index name\n    :param aliases: The aliases\n\n    :type oldidx: str\n    :type newidx: str\n    :type aliases: dict\n\n    :returns: A list of actions suitable for\n        :py:meth:`~.elasticsearch.client.IndicesClient.update_aliases` ``actions``\n        kwarg.\n    :rtype: list\n    \"\"\"\n    actions = []\n    for alias in aliases.keys():\n        actions.append({'remove': {'index': oldidx, 'alias': alias}})\n        actions.append({'add': {'index': newidx, 'alias': alias}})\n    return actions\n\n\ndef get_data_tiers(client):\n    \"\"\"\n    Get all valid data tiers from the node roles of each node in the cluster by\n    polling each node\n\n    :param client: A client connection object\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n\n    :returns: The available data tiers in ``tier: bool`` form.\n    :rtype: dict\n    \"\"\"\n\n    def role_check(role, node_info):\n        if role in node_info['roles']:\n            return True\n        return False\n\n    info = client.nodes.info()['nodes']\n    retval = {\n        'data_hot': False,\n        'data_warm': False,\n        'data_cold': False,\n        'data_frozen': False,\n    }\n    for node in info:\n        for role in ['data_hot', 'data_warm', 'data_cold', 'data_frozen']:\n            # This guarantees we don't overwrite a True with a False.\n            # We only add True values\n            if role_check(role, info[node]):\n                retval[role] = True\n    return retval\n\n\ndef get_indices(client, search_pattern='_all'):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.CatClient.indices`\n\n    :param client: A client connection object\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n\n    :returns: The current list of indices from the cluster\n    :rtype: list\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    indices = []\n    try:\n        # Doing this in two stages because IndexList also calls for these args,\n        # and the unit tests need to Mock this call the same exact way.\n        resp = client.cat.indices(\n            index=search_pattern,\n            expand_wildcards='open,closed',\n            h='index,status',\n            format='json',\n        )\n    except Exception as err:\n        raise FailedExecution(f'Failed to get indices. Error: {err}') from err\n    if not resp:\n        return indices\n    for entry in resp:\n        indices.append(entry['index'])\n    logger.debug('All indices: %s', indices)\n    return indices\n\n\ndef get_repository(client, repository=''):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get_repository`\n\n    :param client: A client connection object\n    :param repository: The Elasticsearch snapshot repository to use\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type repository: str\n\n    :returns: Configuration information for ``repository``.\n    :rtype: dict\n    \"\"\"\n    try:\n        return client.snapshot.get_repository(name=repository)\n    except (es8exc.TransportError, es8exc.NotFoundError) as err:\n        msg = (\n            f'Unable to get repository {repository}.  Error: {err} Check Elasticsearch '\n            f'logs for more information.'\n        )\n        raise CuratorException(msg) from err\n\n\ndef get_snapshot(client, repository=None, snapshot=''):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get`\n\n    :param client: A client connection object\n    :param repository: The Elasticsearch snapshot repository to use\n    :param snapshot: The snapshot name, or a comma-separated list of snapshots\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type repository: str\n    :type snapshot: str\n\n    :returns: Information about the provided ``snapshot``, a snapshot (or a\n        comma-separated list of snapshots). If no snapshot specified, it will\n        collect info for all snapshots.  If none exist, an empty :py:class:`dict`\n        will be returned.\n    :rtype: dict\n    \"\"\"\n    if not repository:\n        raise MissingArgument('No value for \"repository\" provided')\n    snapname = '*' if snapshot == '' else snapshot\n    try:\n        return client.snapshot.get(repository=repository, snapshot=snapshot)\n    except (es8exc.TransportError, es8exc.NotFoundError) as err:\n        msg = (\n            f'Unable to get information about snapshot {snapname} from repository: '\n            f'{repository}.  Error: {err}'\n        )\n        raise FailedExecution(msg) from err\n\n\ndef get_snapshot_data(client, repository=None):\n    \"\"\"\n    Get all snapshots from repository and return a list.\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get`\n\n    :param client: A client connection object\n    :param repository: The Elasticsearch snapshot repository to use\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type repository: str\n\n    :returns: The list of all snapshots from ``repository``\n    :rtype: list\n    \"\"\"\n    if not repository:\n        raise MissingArgument('No value for \"repository\" provided')\n    try:\n        return client.snapshot.get(repository=repository, snapshot=\"*\")['snapshots']\n    except (es8exc.TransportError, es8exc.NotFoundError) as err:\n        msg = (\n            f'Unable to get snapshot information from repository: '\n            f'{repository}. Error: {err}'\n        )\n        raise FailedExecution(msg) from err\n\n\ndef get_tier_preference(client, target_tier='data_frozen'):\n    \"\"\"Do the tier preference thing in reverse order from coldest to hottest\n    Based on the value of ``target_tier``, build out the list to use.\n\n    :param client: A client connection object\n    :param target_tier: The target data tier, e.g. data_warm.\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type target_tier: str\n\n    :returns: A suitable tier preference string in csv format\n    :rtype: str\n    \"\"\"\n    tiermap = {\n        'data_content': 0,\n        'data_hot': 1,\n        'data_warm': 2,\n        'data_cold': 3,\n        'data_frozen': 4,\n    }\n    tiers = get_data_tiers(client)\n    test_list = []\n    for tier in ['data_hot', 'data_warm', 'data_cold', 'data_frozen']:\n        if tier in tiers and tiermap[tier] <= tiermap[target_tier]:\n            test_list.insert(0, tier)\n    if target_tier == 'data_frozen':\n        # We're migrating to frozen here. If a frozen tier exists, frozen searchable\n        # snapshot mounts should only ever go to the frozen tier.\n        if 'data_frozen' in tiers and tiers['data_frozen']:\n            return 'data_frozen'\n    # If there are no  nodes with the 'data_frozen' role...\n    preflist = []\n    for key in test_list:\n        # This ordering ensures that colder tiers are prioritized\n        if key in tiers and tiers[key]:\n            preflist.append(key)\n    # If all of these are false, then we have no data tiers and must use 'data_content'\n    if not preflist:\n        return 'data_content'\n    # This will join from coldest to hottest as csv string,\n    # e.g. 'data_cold,data_warm,data_hot'\n    return ','.join(preflist)\n\n\ndef get_write_index(client, alias):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.get_alias`\n\n    :param client: A client connection object\n    :param alias: An alias name\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type alias: str\n\n    :returns: The the index name associated with the alias that is designated\n        ``is_write_index``\n    :rtype: str\n    \"\"\"\n    try:\n        response = client.indices.get_alias(index=alias)\n    except Exception as exc:\n        raise CuratorException(f'Alias {alias} not found') from exc\n    # If there are more than one in the list, one needs to be the write index\n    # otherwise the alias is a one to many, and can't do rollover.\n    retval = None\n    if len(list(response.keys())) > 1:\n        for index in list(response.keys()):\n            try:\n                if response[index]['aliases'][alias]['is_write_index']:\n                    retval = index\n            except KeyError as exc:\n                raise FailedExecution(\n                    'Invalid alias: is_write_index not found in 1 to many alias'\n                ) from exc\n    else:\n        # There's only one, so this is it\n        retval = list(response.keys())[0]\n    return retval\n\n\ndef index_size(client, idx, value='total'):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.stats`\n\n    :param client: A client connection object\n    :param idx: An index name\n    :param value: One of either ``primaries`` or ``total``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type idx: str\n    :type value: str\n\n    :returns: The sum of either ``primaries`` or ``total`` shards for index ``idx``\n    :rtype: integer\n    \"\"\"\n    fpath = f'indices.{escape_dots(idx)}.{value}.store.size_in_bytes'\n    return client.indices.stats(index=idx, filter_path=fpath)['indices'][idx][value][\n        'store'\n    ]['size_in_bytes']\n\n\ndef meta_getter(client, idx, get=None):\n    \"\"\"Meta Getter\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.get_settings` or\n    :py:meth:`~.elasticsearch.client.IndicesClient.get_alias`\n\n    :param client: A client connection object\n    :param idx: An Elasticsearch index\n    :param get: The kind of get to perform, e.g. settings or alias\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type idx: str\n    :type get: str\n\n    :returns: The settings from the get call to the named index\n    :rtype: dict\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    acceptable = ['settings', 'alias']\n    if not get:\n        raise ConfigurationError('\"get\" can not be a NoneType')\n    if get not in acceptable:\n        raise ConfigurationError(f'\"get\" must be one of {acceptable}')\n    retval = {}\n    try:\n        if get == 'settings':\n            retval = client.indices.get_settings(index=idx)[idx]['settings']['index']\n        elif get == 'alias':\n            retval = client.indices.get_alias(index=idx)[idx]['aliases']\n    except es8exc.NotFoundError as missing:\n        logger.error('Index %s was not found!', idx)\n        raise es8exc.NotFoundError from missing\n    except KeyError as err:\n        logger.error('Key not found: %s', err)\n        raise KeyError from err\n    # pylint: disable=broad-except\n    except Exception as exc:\n        logger.error('Exception encountered: %s', exc)\n    return retval\n\n\ndef name_to_node_id(client, name):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.NodesClient.info`\n\n    :param client: A client connection object\n    :param name: The node ``name``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type name: str\n\n    :returns: The node_id of the node identified by ``name``\n    :rtype: str\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    fpath = 'nodes'\n    info = client.nodes.info(filter_path=fpath)\n    for node in info['nodes']:\n        if info['nodes'][node]['name'] == name:\n            logger.debug('Found node_id \"%s\" for name \"%s\".', node, name)\n            return node\n    logger.error('No node_id found matching name: \"%s\"', name)\n    return None\n\n\ndef node_id_to_name(client, node_id):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.NodesClient.info`\n\n    :param client: A client connection object\n    :param node_id: The node ``node_id``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type node_id: str\n\n    :returns: The name of the node identified by ``node_id``\n    :rtype: str\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    fpath = f'nodes.{node_id}.name'\n    info = client.nodes.info(filter_path=fpath)\n    name = None\n    if node_id in info['nodes']:\n        name = info['nodes'][node_id]['name']\n    else:\n        logger.error('No node_id found matching: \"%s\"', node_id)\n    logger.debug('Name associated with node_id \"%s\": %s', node_id, name)\n    return name\n\n\ndef node_roles(client, node_id):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.NodesClient.info`\n\n    :param client: A client connection object\n    :param node_id: The node ``node_id``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type node_id: str\n\n    :returns: The list of roles assigned to the node identified by ``node_id``\n    :rtype: list\n    \"\"\"\n    fpath = f'nodes.{node_id}.roles'\n    return client.nodes.info(filter_path=fpath)['nodes'][node_id]['roles']\n\n\ndef single_data_path(client, node_id):\n    \"\"\"\n    In order for a shrink to work, it should be on a single filesystem, as shards\n    cannot span filesystems. Calls :py:meth:`~.elasticsearch.client.NodesClient.stats`\n\n    :param client: A client connection object\n    :param node_id: The node ``node_id``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type node_id: str\n\n    :returns: ``True`` if the node has a single filesystem, else ``False``\n    :rtype: bool\n    \"\"\"\n    fpath = f'nodes.{node_id}.fs.data'\n    response = client.nodes.stats(filter_path=fpath)\n    return len(response['nodes'][node_id]['fs']['data']) == 1\n",
    "curator/helpers/utils.py": "\"\"\"Helper utilities\n\nThe kind that don't fit in testers, getters, date_ops, or converters\n\"\"\"\nimport logging\nfrom es_client.helpers.utils import ensure_list\nfrom curator.exceptions import FailedExecution\n\ndef chunk_index_list(indices):\n    \"\"\"\n    This utility chunks very large index lists into 3KB chunks.\n    It measures the size as a csv string, then converts back into a list for the return value.\n\n    :param indices: The list of indices\n\n    :type indices: list\n\n    :returns: A list of lists (each a piece of the original ``indices``)\n    :rtype: list\n    \"\"\"\n    chunks = []\n    chunk = \"\"\n    for index in indices:\n        if len(chunk) < 3072:\n            if not chunk:\n                chunk = index\n            else:\n                chunk += \",\" + index\n        else:\n            chunks.append(chunk.split(','))\n            chunk = index\n    chunks.append(chunk.split(','))\n    return chunks\n\ndef report_failure(exception):\n    \"\"\"\n    Raise a :py:exc:`~.curator.exceptions.FailedExecution` exception and include the original error\n    message.\n\n    :param exception: The upstream exception.\n\n    :type exception: :py:exc:Exception\n\n    :rtype: None\n    \"\"\"\n    raise FailedExecution(\n        f'Exception encountered.  Rerun with loglevel DEBUG and/or check Elasticsearch logs for'\n        f'more information. Exception: {exception}'\n    )\n\ndef show_dry_run(ilo, action, **kwargs):\n    \"\"\"\n    Log dry run output with the action which would have been executed.\n\n    :param ilo: An IndexList Object\n    :param action: The ``action`` to be performed.\n    :param kwargs: Any other args to show in the log output\n\n\n    :type ilo: :py:class:`~.curator.indexlist.IndexList`\n    :type action: str\n    :type kwargs: dict\n\n    :rtype: None\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info('DRY-RUN MODE.  No changes will be made.')\n    msg = f'(CLOSED) indices may be shown that may not be acted on by action \"{action}\".'\n    logger.info(msg)\n    indices = sorted(ilo.indices)\n    for idx in indices:\n        # Dry runs need index state, so we collect it here if it's not present.\n        try:\n            index_closed = ilo.index_info[idx]['state'] == 'close'\n        except KeyError:\n            ilo.get_index_state()\n            index_closed = ilo.index_info[idx]['state'] == 'close'\n        var = ' (CLOSED)' if index_closed else ''\n        msg = f'DRY-RUN: {action}: {idx}{var} with arguments: {kwargs}'\n        logger.info(msg)\n\ndef to_csv(indices):\n    \"\"\"\n    :param indices: A list of indices to act on, or a single value, which could be\n        in the format of a csv string already.\n\n    :type indices: list\n\n    :returns: A csv string from a list of indices, or a single value if only one value is present\n    :rtype: str\n    \"\"\"\n    indices = ensure_list(indices) # in case of a single value passed\n    if indices:\n        return ','.join(sorted(indices))\n    return None\n",
    "curator/actions/snapshot.py": "\"\"\"Snapshot and Restore action classes\"\"\"\nimport logging\nimport re\nfrom es_client.helpers.utils import ensure_list\nfrom curator.helpers.date_ops import parse_datemath, parse_date_pattern\nfrom curator.helpers.getters import get_indices\nfrom curator.helpers.testers import (\n    repository_exists, snapshot_running, verify_index_list, verify_repository, verify_snapshot_list\n)\nfrom curator.helpers.utils import report_failure, to_csv\nfrom curator.helpers.waiters import wait_for_it\n# pylint: disable=broad-except\nfrom curator.exceptions import (\n        ActionError, CuratorException, FailedRestore, FailedSnapshot, MissingArgument,\n        SnapshotInProgress\n    )\n\nclass Snapshot(object):\n    \"\"\"Snapshot Action Class\n\n    Read more about identically named settings at:\n    :py:meth:`elasticsearch.client.SnapshotClient.create`\n    \"\"\"\n    def __init__(self, ilo, repository=None, name=None, ignore_unavailable=False,\n        include_global_state=True, partial=False, wait_for_completion=True, wait_interval=9,\n        max_wait=-1, skip_repo_fs_check=True\n    ):\n        \"\"\"\n        :param ilo: An IndexList Object\n        :param repository: Repository name.\n        :param name: Snapshot name.\n        :param ignore_unavailable: Ignore unavailable shards/indices.\n        :param include_global_state: Store cluster global state with snapshot.\n        :param partial: Do not fail if primary shard is unavailable.\n        :param wait_for_completion: Wait for completion before returning.\n        :param wait_interval: Seconds to wait between completion checks.\n        :param max_wait: Maximum number of seconds to ``wait_for_completion``\n        :param skip_repo_fs_check: Do not validate write access to repository on all cluster nodes\n            before proceeding. Useful for shared filesystems where intermittent timeouts can affect\n            validation, but won't likely affect snapshot success. (Default: ``True``)\n\n        :type ilo: :py:class:`~.curator.indexlist.IndexList`\n        :type repository: str\n        :type name: str\n        :type ignore_unavailable: bool\n        :type include_global_state: bool\n        :type partial: bool\n        :type wait_for_completion: bool\n        :type wait_interval: int\n        :type max_wait: int\n        :type skip_repo_fs_check: bool\n        \"\"\"\n        verify_index_list(ilo)\n        # Check here and don't bother with the rest of this if there are no\n        # indices in the index list.\n        ilo.empty_list_check()\n        if not repository_exists(ilo.client, repository=repository):\n            raise ActionError(\n                f'Cannot snapshot indices to missing repository: {repository}')\n        if not name:\n            raise MissingArgument('No value for \"name\" provided.')\n        #: The :py:class:`~.curator.indexlist.IndexList` object passed from param ``ilo``\n        self.index_list = ilo\n        #: The :py:class:`~.elasticsearch.Elasticsearch` client object derived from\n        #: :py:attr:`index_list`\n        self.client = ilo.client\n        #: The :py:func:`~.curator.helpers.date_ops.parse_date_pattern` rendered\n        #: version of what was passed by param ``name``.\n        self.name = parse_datemath(self.client, parse_date_pattern(name))\n        #: Object attribute that gets the value of param ``repository``.\n        self.repository = repository\n        #: Object attribute that gets the value of param ``wait_for_completion``.\n        self.wait_for_completion = wait_for_completion\n        #: Object attribute that gets the value of param ``wait_interval``.\n        self.wait_interval = wait_interval\n        #: Object attribute that gets the value of param ``max_wait``.\n        self.max_wait = max_wait\n        #: Object attribute that gets the value of param ``skip_repo_fs_check``.\n        self.skip_repo_fs_check = skip_repo_fs_check\n        #: Object attribute that tracks the snapshot state.\n        self.state = None\n        #: Object attribute that contains the :py:func:`~.curator.helpers.utils.to_csv` output of\n        #: the indices in :py:attr:`index_list`.\n        self.indices = to_csv(ilo.indices)\n        #: Object attribute that gets the value of param ``ignore_unavailable``.\n        self.ignore_unavailable = ignore_unavailable\n        #: Object attribute that gets the value of param ``include_global_state``.\n        self.include_global_state = include_global_state\n        #: Object attribute that gets the value of param ``partial``.\n        self.partial = partial\n        #: Object attribute dictionary compiled from :py:attr:`indices`,\n        #: :py:attr:`ignore_unavailable`, :py:attr:`include_global_state`, and :py:attr:`partial`\n        self.settings = {\n            'indices': ilo.indices,\n            'ignore_unavailable': self.ignore_unavailable,\n            'include_global_state': self.include_global_state,\n            'partial': self.partial\n        }\n\n        self.loggit = logging.getLogger('curator.actions.snapshot')\n\n    def get_state(self):\n        \"\"\"Get the state of the snapshot and set :py:attr:`state`\"\"\"\n        try:\n            self.state = self.client.snapshot.get(\n                repository=self.repository, snapshot=self.name)['snapshots'][0]['state']\n            return self.state\n        except IndexError as exc:\n            raise CuratorException(\n                f'Snapshot \"{self.name}\" not found in repository \"{self.repository}\"') from exc\n\n    def report_state(self):\n        \"\"\"\n        Log the :py:attr:`state` of the snapshot and raise :py:exc:`FailedSnapshot` if\n        :py:attr:`state` is not ``SUCCESS``\n        \"\"\"\n        self.get_state()\n        if self.state == 'SUCCESS':\n            self.loggit.info('Snapshot %s successfully completed.', self.name)\n        else:\n            msg = f'Snapshot {self.name} completed with state: {self.state}'\n            self.loggit.error(msg)\n            raise FailedSnapshot(msg)\n\n    def do_dry_run(self):\n        \"\"\"Log what the output would be, but take no action.\"\"\"\n        self.loggit.info('DRY-RUN MODE.  No changes will be made.')\n        msg = (\n            f'DRY-RUN: snapshot: {self.name} in repository {self.repository} '\n            f'with arguments: {self.settings}'\n        )\n        self.loggit.info(msg)\n\n    def do_action(self):\n        \"\"\"\n        :py:meth:`elasticsearch.client.SnapshotClient.create` a snapshot of :py:attr:`indices`,\n        with passed parameters.\n        \"\"\"\n        if not self.skip_repo_fs_check:\n            verify_repository(self.client, self.repository)\n        if snapshot_running(self.client):\n            raise SnapshotInProgress('Snapshot already in progress.')\n        try:\n            self.loggit.info(\n                'Creating snapshot \"%s\" from indices: %s', self.name, self.index_list.indices)\n            # Always set wait_for_completion to False. Let 'wait_for_it' do its\n            # thing if wait_for_completion is set to True. Report the task_id\n            # either way.\n            self.client.snapshot.create(\n                repository=self.repository,\n                snapshot=self.name,\n                ignore_unavailable=self.ignore_unavailable,\n                include_global_state=self.include_global_state,\n                indices=self.indices,\n                partial=self.partial,\n                wait_for_completion=False\n            )\n            if self.wait_for_completion:\n                wait_for_it(\n                    self.client, 'snapshot', snapshot=self.name,\n                    repository=self.repository,\n                    wait_interval=self.wait_interval, max_wait=self.max_wait\n                )\n                self.report_state()\n            else:\n                msg = (\n                    f'\"wait_for_completion\" set to {self.wait_for_completion}. '\n                    f'Remember to check for successful completion manually.'\n                )\n                self.loggit.warning(msg)\n        except Exception as err:\n            report_failure(err)\n\nclass DeleteSnapshots:\n    \"\"\"Delete Snapshots Action Class\"\"\"\n    def __init__(self, slo, retry_interval=120, retry_count=3):\n        \"\"\"\n        :param slo: A SnapshotList object\n        :type slo: :py:class:`~.curator.snapshotlist.SnapshotList`\n        :param retry_interval: Seconds to delay betwen retries. (Default: ``120``)\n        :type retry_interval: int\n        :param retry_count: Number of attempts to make. (Default: ``3``)\n        :type retry_count: int\n        \"\"\"\n        verify_snapshot_list(slo)\n        #: The :py:class:`~.curator.snapshotlist.SnapshotList` object passed from param ``slo``\n        self.snapshot_list = slo\n        #: The :py:class:`~.elasticsearch.Elasticsearch` client object derived from\n        #: :py:attr:`snapshot_list`\n        self.client = slo.client\n        #: Object attribute that gets the value of param ``retry_interval``.\n        self.retry_interval = retry_interval\n        #: Object attribute that gets the value of param ``retry_count``.\n        self.retry_count = retry_count\n        #: Object attribute that gets its value from :py:attr:`snapshot_list`.\n        self.repository = slo.repository\n        self.loggit = logging.getLogger('curator.actions.delete_snapshots')\n\n    def do_dry_run(self):\n        \"\"\"Log what the output would be, but take no action.\"\"\"\n        self.loggit.info('DRY-RUN MODE.  No changes will be made.')\n        mykwargs = {\n            'repository' : self.repository,\n            'retry_interval' : self.retry_interval,\n            'retry_count' : self.retry_count,\n        }\n        for snap in self.snapshot_list.snapshots:\n            self.loggit.info('DRY-RUN: delete_snapshot: %s with arguments: %s', snap, mykwargs)\n\n    def do_action(self):\n        \"\"\"\n        :py:meth:`~.elasticsearch.client.SnapshotClient.delete` snapshots in\n        :py:attr:`snapshot_list`. Retry up to :py:attr:`retry_count` times, pausing\n        :py:attr:`retry_interval` seconds between retries.\n        \"\"\"\n        self.snapshot_list.empty_list_check()\n        msg = (\n            f'Deleting {len(self.snapshot_list.snapshots)} '\n            f'selected snapshots: {self.snapshot_list.snapshots}'\n        )\n        self.loggit.info(msg)\n        try:\n            for snap in self.snapshot_list.snapshots:\n                self.loggit.info('Deleting snapshot %s...', snap)\n                self.client.snapshot.delete(repository=self.repository, snapshot=snap)\n        # pylint: disable=broad-except\n        except Exception as err:\n            report_failure(err)\n\nclass Restore(object):\n    \"\"\"Restore Action Class\n\n    Read more about identically named settings at:\n    :py:meth:`elasticsearch.client.SnapshotClient.restore`\n    \"\"\"\n    def __init__(\n            self, slo, name=None, indices=None, include_aliases=False, ignore_unavailable=False,\n            include_global_state=False, partial=False, rename_pattern=None,\n            rename_replacement=None, extra_settings=None, wait_for_completion=True, wait_interval=9,\n            max_wait=-1, skip_repo_fs_check=True\n    ):\n        \"\"\"\n        :param slo: A SnapshotList object\n        :param name: Name of the snapshot to restore.  If ``None``, use the most recent snapshot.\n        :param indices: Indices to restore.  If ``None``, all in the snapshot will be restored.\n        :param include_aliases: Restore aliases with the indices.\n        :param ignore_unavailable: Ignore unavailable shards/indices.\n        :param include_global_state: Restore cluster global state with snapshot.\n        :param partial: Do not fail if primary shard is unavailable.\n        :param rename_pattern: A regular expression pattern with one or more captures, e.g.\n            ``index_(.+)``\n        :param rename_replacement: A target index name pattern with `$#` numbered references to the\n            captures in ``rename_pattern``, e.g. ``restored_index_$1``\n        :param extra_settings: Index settings to apply to restored indices.\n        :param wait_for_completion: Wait for completion before returning.\n        :param wait_interval: Seconds to wait between completion checks.\n        :param max_wait: Maximum number of seconds to ``wait_for_completion``\n        :param skip_repo_fs_check: Do not validate write access to repository on all cluster nodes\n            before proceeding. Useful for shared filesystems where intermittent timeouts can affect\n            validation, but won't likely affect snapshot success. (Default: ``True``)\n\n        :type slo: :py:class:`~.curator.snapshotlist.SnapshotList`\n        :type name: str\n        :type indices: list\n        :type include_aliases: bool\n        :type ignore_unavailable: bool\n        :type include_global_state: bool\n        :type partial: bool\n        :type rename_pattern: str\n        :type rename_replacement: str\n        :type extra_settings: dict\n        :type wait_for_completion: bool\n        :type wait_interval: int\n        :type max_wait: int\n        :type skip_repo_fs_check: bool\n        \"\"\"\n        if extra_settings is None:\n            extra_settings = {}\n        self.loggit = logging.getLogger('curator.actions.snapshot')\n        verify_snapshot_list(slo)\n        # Get the most recent snapshot.\n        most_recent = slo.most_recent()\n        self.loggit.debug('\"most_recent\" snapshot: %s', most_recent)\n        #: Object attribute that gets the value of param ``name`` if not ``None``, or the output\n        #: from :py:meth:`~.curator.SnapshotList.most_recent`.\n        self.name = name if name else most_recent\n        # Stop here now, if it's not a successful snapshot.\n        if slo.snapshot_info[self.name]['state'] == 'PARTIAL' and partial:\n            self.loggit.warning('Performing restore of snapshot in state PARTIAL.')\n        elif slo.snapshot_info[self.name]['state'] != 'SUCCESS':\n            raise CuratorException(\n                'Restore operation can only be performed on snapshots with '\n                'state \"SUCCESS\", or \"PARTIAL\" if partial=True.'\n            )\n\n        #: Internal reference to `slo`\n        self.snapshot_list = slo\n        #: The :py:class:`~.elasticsearch.Elasticsearch` client object derived from\n        #: :py:attr:`snapshot_list`\n        self.client = slo.client\n        #: Object attribute that gets the value of ``repository`` from :py:attr:`snapshot_list`.\n        self.repository = slo.repository\n\n        if indices:\n            self.indices = ensure_list(indices)\n        else:\n            self.indices = slo.snapshot_info[self.name]['indices']\n        #: Object attribute that gets the value of param ``wait_for_completion``.\n        self.wfc = wait_for_completion\n        #: Object attribute that gets the value of param ``wait_interval``.\n        self.wait_interval = wait_interval\n        #: Object attribute that gets the value of param ``max_wait``.\n        self.max_wait = max_wait\n        #: Object attribute that gets the value of param ``rename_pattern``. Empty :py:class:`str`\n        #: if ``None``\n        self.rename_pattern = rename_pattern if rename_replacement is not None \\\n            else ''\n        #: Object attribute that gets the value of param ``rename_replacement``. Empty\n        #: :py:class:`str` if ``None``\n        self.rename_replacement = rename_replacement if rename_replacement \\\n            is not None else ''\n        #: Object attribute derived from :py:attr:`rename_replacement`. but with Java regex group\n        #: designations of ``$#`` converted to Python's ``\\\\#`` style.\n        self.py_rename_replacement = self.rename_replacement.replace('$', '\\\\')\n        #: Object attribute that gets the value of param ``max_wait``.\n        self.skip_repo_fs_check = skip_repo_fs_check\n\n        #: Object attribute that gets populated from other params/attributes. Deprecated, but not\n        #: removed. Lazy way to keep from updating :py:meth:`do_dry_run`. Will fix later.\n        self.body = {\n            'indices' : self.indices,\n            'include_aliases' : include_aliases,\n            'ignore_unavailable' : ignore_unavailable,\n            'include_global_state' : include_global_state,\n            'partial' : partial,\n            'rename_pattern' : self.rename_pattern,\n            'rename_replacement' : self.rename_replacement,\n        }\n        #: Object attribute that gets the value of param ``include_aliases``.\n        self.include_aliases = include_aliases\n        #: Object attribute that gets the value of param ``ignore_unavailable``.\n        self.ignore_unavailable = ignore_unavailable\n        #: Object attribute that gets the value of param ``include_global_state``.\n        self.include_global_state = include_global_state\n        #: Object attribute that gets the value of param ``include_aliases``.\n        self.include_aliases = include_aliases\n        #: Object attribute that gets the value of param ``partial``.\n        self.partial = partial\n        #: Object attribute that gets the value of param ``extra_settings``.\n        self.index_settings = None\n\n        if extra_settings:\n            self.loggit.debug('Adding extra_settings to restore body: %s',extra_settings)\n            self.index_settings = extra_settings\n            try:\n                self.body.update(extra_settings)\n            except Exception:\n                self.loggit.error('Unable to apply extra settings to restore body')\n        self.loggit.debug('REPOSITORY: %s', self.repository)\n        self.loggit.debug('WAIT_FOR_COMPLETION: %s', self.wfc)\n        self.loggit.debug('SKIP_REPO_FS_CHECK: %s', self.skip_repo_fs_check)\n        self.loggit.debug('BODY: %s', self.body)\n        # Populate the expected output index list.\n        self._get_expected_output()\n\n    def _get_expected_output(self):\n        if not self.rename_pattern and not self.rename_replacement:\n            self.expected_output = self.indices\n            return # Don't stick around if we're not replacing anything\n        self.expected_output = []\n        for index in self.indices:\n            self.expected_output.append(\n                re.sub(self.rename_pattern, self.py_rename_replacement, index)\n            )\n            msg = f'index: {index} replacement: {self.expected_output[-1]}'\n            self.loggit.debug(msg)\n\n    def report_state(self):\n        \"\"\"\n        Log the state of the restore. This should only be done if ``wait_for_completion`` is\n        ``True``, and only after completing the restore.\n        \"\"\"\n        all_indices = get_indices(self.client)\n        found_count = 0\n        missing = []\n        for index in self.expected_output:\n            if index in all_indices:\n                found_count += 1\n                self.loggit.info('Found restored index %s', index)\n            else:\n                missing.append(index)\n        if found_count == len(self.expected_output):\n            self.loggit.info('All indices appear to have been restored.')\n        else:\n            msg = f'Some of the indices do not appear to have been restored. Missing: {missing}'\n            self.loggit.error(msg)\n            raise FailedRestore(msg)\n\n    def do_dry_run(self):\n        \"\"\"Log what the output would be, but take no action.\"\"\"\n        self.loggit.info('DRY-RUN MODE.  No changes will be made.')\n        args = {'wait_for_completion' : self.wfc, 'body' : self.body}\n        msg = (\n            f'DRY-RUN: restore: Repository: {self.repository} '\n            f'Snapshot name: {self.name} Arguments: {args}'\n        )\n        self.loggit.info(msg)\n\n        for index in self.indices:\n            if self.rename_pattern and self.rename_replacement:\n                rmsg = f'as {re.sub(self.rename_pattern, self.py_rename_replacement, index)}'\n            else:\n                rmsg = ''\n            self.loggit.info('DRY-RUN: restore: Index %s %s', index, rmsg)\n\n    def do_action(self):\n        \"\"\"\n        :py:meth:`~.elasticsearch.client.SnapshotClient.restore` :py:attr:`indices` from\n        :py:attr:`name` with passed params.\n        \"\"\"\n        if not self.skip_repo_fs_check:\n            verify_repository(self.client, self.repository)\n        if snapshot_running(self.client):\n            raise SnapshotInProgress('Cannot restore while a snapshot is in progress.')\n        try:\n            self.loggit.info('Restoring indices \"%s\" from snapshot: %s', self.indices, self.name)\n            # Always set wait_for_completion to False. Let 'wait_for_it' do its\n            # thing if wait_for_completion is set to True. Report the task_id\n            # either way.\n            self.client.snapshot.restore(\n                repository=self.repository,\n                snapshot=self.name,\n                ignore_index_settings=None,\n                ignore_unavailable=self.ignore_unavailable,\n                include_aliases=self.include_aliases,\n                include_global_state=self.include_global_state,\n                index_settings=self.index_settings,\n                indices=self.indices,\n                partial=self.partial,\n                rename_pattern=self.rename_pattern,\n                rename_replacement=self.rename_replacement,\n                wait_for_completion=False\n            )\n            if self.wfc:\n                wait_for_it(\n                    self.client, 'restore', index_list=self.expected_output,\n                    wait_interval=self.wait_interval, max_wait=self.max_wait\n                )\n                self.report_state()\n            else:\n                msg = (\n                    f'\"wait_for_completion\" set to {self.wfc}. '\n                    f'Remember to check for successful completion manually.'\n                )\n                self.loggit.warning(msg)\n        except Exception as err:\n            report_failure(err)\n",
    "curator/exceptions.py": "\"\"\"Curator Exceptions\"\"\"\nclass CuratorException(Exception):\n    \"\"\"\n    Base class for all exceptions raised by Curator which are not Elasticsearch\n    exceptions.\n    \"\"\"\n\nclass ConfigurationError(CuratorException):\n    \"\"\"\n    Exception raised when a misconfiguration is detected\n    \"\"\"\n\nclass MissingArgument(CuratorException):\n    \"\"\"\n    Exception raised when a needed argument is not passed.\n    \"\"\"\n\nclass NoIndices(CuratorException):\n    \"\"\"\n    Exception raised when an operation is attempted against an empty index_list\n    \"\"\"\n\nclass NoSnapshots(CuratorException):\n    \"\"\"\n    Exception raised when an operation is attempted against an empty snapshot_list\n    \"\"\"\n\nclass ActionError(CuratorException):\n    \"\"\"\n    Exception raised when an action (against an index_list or snapshot_list) cannot be taken.\n    \"\"\"\n\nclass FailedExecution(CuratorException):\n    \"\"\"\n    Exception raised when an action fails to execute for some reason.\n    \"\"\"\n\nclass SnapshotInProgress(ActionError):\n    \"\"\"\n    Exception raised when a snapshot is already in progress\n    \"\"\"\n\nclass ActionTimeout(CuratorException):\n    \"\"\"\n    Exception raised when an action fails to complete in the allotted time\n    \"\"\"\n\nclass FailedSnapshot(CuratorException):\n    \"\"\"\n    Exception raised when a snapshot does not complete with state SUCCESS\n    \"\"\"\n\nclass FailedRestore(CuratorException):\n    \"\"\"\n    Exception raised when a Snapshot Restore does not restore all selected indices\n    \"\"\"\n\nclass FailedReindex(CuratorException):\n    \"\"\"\n    Exception raised when failures are found in the reindex task response\n    \"\"\"\n\nclass ClientException(CuratorException):\n    \"\"\"\n    Exception raised when the Elasticsearch client and/or connection is the source of the problem.\n    \"\"\"\n\nclass LoggingException(CuratorException):\n    \"\"\"\n    Exception raised when Curator cannot either log or configure logging\n    \"\"\"\n\nclass RepositoryException(CuratorException):\n    \"\"\"\n    Exception raised when Curator cannot verify a snapshot repository\n    \"\"\"\n\nclass SearchableSnapshotException(CuratorException):\n    \"\"\"\n    Exception raised when Curator finds something out of order with a Searchable Snapshot\n    \"\"\"\n"
  },
  "GT_src_dict": {
    "curator/snapshotlist.py": {
      "SnapshotList.__init__": {
        "code": "    def __init__(self, client, repository=None):\n        \"\"\"Initializer for the SnapshotList class, which manages a list of snapshots from a specified Elasticsearch repository. This constructor verifies the client and repository parameters, initializes logging, and retrieves snapshots to populate the internal snapshot_info and snapshots attributes.\n\nParameters:\n    client (Elasticsearch): An instance of the Elasticsearch client used to connect to the cluster.\n    repository (str, optional): The name of the snapshot repository to be used. This is required.\n\nRaises:\n    MissingArgument: If the repository parameter is not provided.\n    FailedExecution: If the specified repository does not exist in the provided client.\n\nAttributes:\n    client: The Elasticsearch client passed as an argument.\n    repository: The name of the snapshot repository.\n    snapshot_info (dict): A dictionary storing metadata about the snapshots, populated by the __get_snapshots() method.\n    snapshots (list): A list of snapshot identifiers, also populated by the __get_snapshots() method. \n    age_keyfield (str): A placeholder for snapshot age information, set by other methods in the class.\n\nDependencies:\n- The method requires the verify_client_object and repository_exists functions to validate the client and repository.\n- It uses the logging module to set up logging, specifically targeting 'curator.snapshotlist'.\n- Retrieves snapshot data using the __get_snapshots method, which accesses the repository to populate snapshot information.\"\"\"\n        verify_client_object(client)\n        if not repository:\n            raise MissingArgument('No value for \"repository\" provided')\n        if not repository_exists(client, repository):\n            raise FailedExecution(f'Unable to verify existence of repository {repository}')\n        self.loggit = logging.getLogger('curator.snapshotlist')\n        self.client = client\n        self.repository = repository\n        self.snapshot_info = {}\n        self.snapshots = []\n        self.__get_snapshots()\n        self.age_keyfield = None",
        "docstring": "Initializer for the SnapshotList class, which manages a list of snapshots from a specified Elasticsearch repository. This constructor verifies the client and repository parameters, initializes logging, and retrieves snapshots to populate the internal snapshot_info and snapshots attributes.\n\nParameters:\n    client (Elasticsearch): An instance of the Elasticsearch client used to connect to the cluster.\n    repository (str, optional): The name of the snapshot repository to be used. This is required.\n\nRaises:\n    MissingArgument: If the repository parameter is not provided.\n    FailedExecution: If the specified repository does not exist in the provided client.\n\nAttributes:\n    client: The Elasticsearch client passed as an argument.\n    repository: The name of the snapshot repository.\n    snapshot_info (dict): A dictionary storing metadata about the snapshots, populated by the __get_snapshots() method.\n    snapshots (list): A list of snapshot identifiers, also populated by the __get_snapshots() method. \n    age_keyfield (str): A placeholder for snapshot age information, set by other methods in the class.\n\nDependencies:\n- The method requires the verify_client_object and repository_exists functions to validate the client and repository.\n- It uses the logging module to set up logging, specifically targeting 'curator.snapshotlist'.\n- Retrieves snapshot data using the __get_snapshots method, which accesses the repository to populate snapshot information.",
        "signature": "def __init__(self, client, repository=None):",
        "type": "Method",
        "class_signature": "class SnapshotList:"
      },
      "SnapshotList.__get_snapshots": {
        "code": "    def __get_snapshots(self):\n        \"\"\"Pulls all snapshots from the specified repository into the `snapshots` attribute and populates the `snapshot_info` dictionary with metadata about each snapshot.\n\nThis method interacts with the `get_snapshot_data` function from the `curator.helpers.getters` module to retrieve snapshot data using the `client` and `repository` provided during the initialization of the `SnapshotList` instance. It then iterates through the retrieved data, adding snapshot names to the `snapshots` list and their corresponding details to the `snapshot_info` dictionary, indexed by snapshot name. If no snapshots are found, it calls the `empty_list_check` method to verify that the `snapshots` list is not empty, raising a `NoSnapshots` exception if it is.\n\nAttributes:\n- `self.snapshots`: A list that holds the names of the snapshots.\n- `self.snapshot_info`: A dictionary that maps each snapshot name to its detailed information.\n\nDependencies:\n- Requires a valid Elasticsearch `client` and an existing `repository`.\n- Dependent on the `get_snapshot_data` function for fetching snapshots.\n\nSide Effects:\n- Modifies the state of `self.snapshots` and `self.snapshot_info`.\"\"\"\n        '\\n        Pull all snapshots into `snapshots` and populate ``snapshot_info``\\n        '\n        self.all_snapshots = get_snapshot_data(self.client, self.repository)\n        for list_item in self.all_snapshots:\n            if 'snapshot' in list_item.keys():\n                self.snapshots.append(list_item['snapshot'])\n                self.snapshot_info[list_item['snapshot']] = list_item\n        self.empty_list_check()",
        "docstring": "Pulls all snapshots from the specified repository into the `snapshots` attribute and populates the `snapshot_info` dictionary with metadata about each snapshot.\n\nThis method interacts with the `get_snapshot_data` function from the `curator.helpers.getters` module to retrieve snapshot data using the `client` and `repository` provided during the initialization of the `SnapshotList` instance. It then iterates through the retrieved data, adding snapshot names to the `snapshots` list and their corresponding details to the `snapshot_info` dictionary, indexed by snapshot name. If no snapshots are found, it calls the `empty_list_check` method to verify that the `snapshots` list is not empty, raising a `NoSnapshots` exception if it is.\n\nAttributes:\n- `self.snapshots`: A list that holds the names of the snapshots.\n- `self.snapshot_info`: A dictionary that maps each snapshot name to its detailed information.\n\nDependencies:\n- Requires a valid Elasticsearch `client` and an existing `repository`.\n- Dependent on the `get_snapshot_data` function for fetching snapshots.\n\nSide Effects:\n- Modifies the state of `self.snapshots` and `self.snapshot_info`.",
        "signature": "def __get_snapshots(self):",
        "type": "Method",
        "class_signature": "class SnapshotList:"
      },
      "SnapshotList.empty_list_check": {
        "code": "    def empty_list_check(self):\n        \"\"\"Check if the list of snapshots is empty and raise an exception if true.\n\nThis method verifies whether the `snapshots` attribute of the `SnapshotList` instance contains any items. If `snapshots` is empty, it raises a `NoSnapshots` exception with a message indicating that the snapshot list is empty. This is crucial for preventing operations on an empty list of snapshots, which would lead to errors in subsequent methods that expect the list to contain snapshot data.\n\nDependencies: \n- `NoSnapshots`: An exception class that is raised when there are no snapshots available in the list.\n\nExample usage: Typically called prior to operations that require non-empty snapshots to ensure safety and correctness.\"\"\"\n        'Raise exception if ``snapshots`` is empty'\n        if not self.snapshots:\n            raise NoSnapshots('snapshot_list object is empty.')",
        "docstring": "Check if the list of snapshots is empty and raise an exception if true.\n\nThis method verifies whether the `snapshots` attribute of the `SnapshotList` instance contains any items. If `snapshots` is empty, it raises a `NoSnapshots` exception with a message indicating that the snapshot list is empty. This is crucial for preventing operations on an empty list of snapshots, which would lead to errors in subsequent methods that expect the list to contain snapshot data.\n\nDependencies: \n- `NoSnapshots`: An exception class that is raised when there are no snapshots available in the list.\n\nExample usage: Typically called prior to operations that require non-empty snapshots to ensure safety and correctness.",
        "signature": "def empty_list_check(self):",
        "type": "Method",
        "class_signature": "class SnapshotList:"
      }
    },
    "curator/helpers/testers.py": {
      "repository_exists": {
        "code": "def repository_exists(client, repository=None):\n    \"\"\"Check the existence of an Elasticsearch snapshot repository.\n\nThis function checks if a specified snapshot repository exists in the Elasticsearch cluster defined by the provided client connection. It utilizes the `get_repository` function to retrieve repository information and returns a boolean indicating whether the specified repository exists. If the repository name is not provided, a `MissingArgument` exception is raised.\n\nParameters:\n- client (:py:class:`~.elasticsearch.Elasticsearch`): A client connection object to interact with the Elasticsearch cluster.\n- repository (str, optional): The name of the Elasticsearch snapshot repository to check for.\n\nReturns:\n- bool: Returns `True` if the specified repository exists, otherwise `False`.\n\nDependencies:\n- This function relies on the `get_repository` helper function from the `curator.helpers.getters` module to check for the repository's existence.\n- It imports the `MissingArgument` exception from the `curator.exceptions` module, which is raised if the repository parameter is not provided.\n- It makes use of the `logging` module for logging debug information regarding the repository check results.\"\"\"\n    '\\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get_repository`\\n\\n    :param client: A client connection object\\n    :param repository: The Elasticsearch snapshot repository to use\\n\\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\\n    :type repository: str\\n\\n    :returns: ``True`` if ``repository`` exists, else ``False``\\n    :rtype: bool\\n    '\n    logger = logging.getLogger(__name__)\n    if not repository:\n        raise MissingArgument('No value for \"repository\" provided')\n    try:\n        test_result = get_repository(client, repository)\n        if repository in test_result:\n            logger.debug('Repository %s exists.', repository)\n            response = True\n        else:\n            logger.debug('Repository %s not found...', repository)\n            response = False\n    except Exception as err:\n        logger.debug('Unable to find repository \"%s\": Error: %s', repository, err)\n        response = False\n    return response",
        "docstring": "Check the existence of an Elasticsearch snapshot repository.\n\nThis function checks if a specified snapshot repository exists in the Elasticsearch cluster defined by the provided client connection. It utilizes the `get_repository` function to retrieve repository information and returns a boolean indicating whether the specified repository exists. If the repository name is not provided, a `MissingArgument` exception is raised.\n\nParameters:\n- client (:py:class:`~.elasticsearch.Elasticsearch`): A client connection object to interact with the Elasticsearch cluster.\n- repository (str, optional): The name of the Elasticsearch snapshot repository to check for.\n\nReturns:\n- bool: Returns `True` if the specified repository exists, otherwise `False`.\n\nDependencies:\n- This function relies on the `get_repository` helper function from the `curator.helpers.getters` module to check for the repository's existence.\n- It imports the `MissingArgument` exception from the `curator.exceptions` module, which is raised if the repository parameter is not provided.\n- It makes use of the `logging` module for logging debug information regarding the repository check results.",
        "signature": "def repository_exists(client, repository=None):",
        "type": "Function",
        "class_signature": null
      },
      "verify_client_object": {
        "code": "def verify_client_object(test):\n    \"\"\"Verify the validity of an Elasticsearch client object.\n\nThis function checks whether the provided variable (test) is a valid instance of the Elasticsearch client class. It allows for compatibility with unit tests by excluding mock objects. If the input is not a valid Elasticsearch client, a TypeError is raised, and an error message is logged.\n\n:param test: The variable or object to test as a valid Elasticsearch client.\n:type test: :py:class:`~.elasticsearch.Elasticsearch`\n\n:raises TypeError: If the input is not a valid Elasticsearch client object.\n\n:return: None, raises an exception for invalid input.\"\"\"\n    '\\n    :param test: The variable or object to test\\n\\n    :type test: :py:class:`~.elasticsearch.Elasticsearch`\\n\\n    :returns: ``True`` if ``test`` is a proper :py:class:`~.elasticsearch.Elasticsearch`\\n        client object and raise a :py:exc:`TypeError` exception if it is not.\\n    :rtype: bool\\n    '\n    logger = logging.getLogger(__name__)\n    if str(type(test)) == \"<class 'unittest.mock.Mock'>\":\n        pass\n    elif not isinstance(test, Elasticsearch):\n        msg = f'Not a valid client object. Type: {type(test)} was passed'\n        logger.error(msg)\n        raise TypeError(msg)",
        "docstring": "Verify the validity of an Elasticsearch client object.\n\nThis function checks whether the provided variable (test) is a valid instance of the Elasticsearch client class. It allows for compatibility with unit tests by excluding mock objects. If the input is not a valid Elasticsearch client, a TypeError is raised, and an error message is logged.\n\n:param test: The variable or object to test as a valid Elasticsearch client.\n:type test: :py:class:`~.elasticsearch.Elasticsearch`\n\n:raises TypeError: If the input is not a valid Elasticsearch client object.\n\n:return: None, raises an exception for invalid input.",
        "signature": "def verify_client_object(test):",
        "type": "Function",
        "class_signature": null
      },
      "verify_snapshot_list": {
        "code": "def verify_snapshot_list(test):\n    \"\"\"Verify that the provided `test` variable is an instance of the `SnapshotList` class.\n\n:param test: The variable or object to test for the appropriate type.\n:type test: :py:class:`~.curator.SnapshotList`\n\n:raises TypeError: If `test` is not an instance of `SnapshotList`, an error is logged and a `TypeError` is raised.\n\nThis function relies on the `SnapshotList` class defined in the `curator.snapshotlist` module. If the import of `SnapshotList` fails due to circular dependencies, an import error may occur. The function uses the logging module to log errors related to type validation.\"\"\"\n    '\\n    :param test: The variable or object to test\\n\\n    :type test: :py:class:`~.curator.SnapshotList`\\n\\n    :returns: ``None`` if ``test`` is a proper\\n        :py:class:`~.curator.snapshotlist.SnapshotList` object, else raise a\\n        :py:class:`TypeError` exception.\\n    :rtype: None\\n    '\n    from curator.snapshotlist import SnapshotList\n    logger = logging.getLogger(__name__)\n    if not isinstance(test, SnapshotList):\n        msg = f'Not a valid SnapshotList object. Type: {type(test)} was passed'\n        logger.error(msg)\n        raise TypeError(msg)",
        "docstring": "Verify that the provided `test` variable is an instance of the `SnapshotList` class.\n\n:param test: The variable or object to test for the appropriate type.\n:type test: :py:class:`~.curator.SnapshotList`\n\n:raises TypeError: If `test` is not an instance of `SnapshotList`, an error is logged and a `TypeError` is raised.\n\nThis function relies on the `SnapshotList` class defined in the `curator.snapshotlist` module. If the import of `SnapshotList` fails due to circular dependencies, an import error may occur. The function uses the logging module to log errors related to type validation.",
        "signature": "def verify_snapshot_list(test):",
        "type": "Function",
        "class_signature": null
      }
    },
    "curator/helpers/getters.py": {
      "get_repository": {
        "code": "def get_repository(client, repository=''):\n    \"\"\"Retrieve configuration information for a specified Elasticsearch snapshot repository.\n\nParameters:\n- client (Elasticsearch): A client connection object to interact with the Elasticsearch cluster.\n- repository (str): The name of the Elasticsearch snapshot repository from which to retrieve information.\n\nReturns:\n- dict: A dictionary containing the configuration details of the specified repository.\n\nRaises:\n- CuratorException: If an error occurs while attempting to retrieve the repository, including transport errors or if the repository is not found.\n\nThis function relies on the `es8exc.TransportError` and `es8exc.NotFoundError` exceptions from the `elasticsearch8` library to handle errors during the repository retrieval. A `CuratorException` is raised with a detailed error message if the repository cannot be accessed, directing users to check Elasticsearch logs for more information.\"\"\"\n    '\\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get_repository`\\n\\n    :param client: A client connection object\\n    :param repository: The Elasticsearch snapshot repository to use\\n\\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\\n    :type repository: str\\n\\n    :returns: Configuration information for ``repository``.\\n    :rtype: dict\\n    '\n    try:\n        return client.snapshot.get_repository(name=repository)\n    except (es8exc.TransportError, es8exc.NotFoundError) as err:\n        msg = f'Unable to get repository {repository}.  Error: {err} Check Elasticsearch logs for more information.'\n        raise CuratorException(msg) from err",
        "docstring": "Retrieve configuration information for a specified Elasticsearch snapshot repository.\n\nParameters:\n- client (Elasticsearch): A client connection object to interact with the Elasticsearch cluster.\n- repository (str): The name of the Elasticsearch snapshot repository from which to retrieve information.\n\nReturns:\n- dict: A dictionary containing the configuration details of the specified repository.\n\nRaises:\n- CuratorException: If an error occurs while attempting to retrieve the repository, including transport errors or if the repository is not found.\n\nThis function relies on the `es8exc.TransportError` and `es8exc.NotFoundError` exceptions from the `elasticsearch8` library to handle errors during the repository retrieval. A `CuratorException` is raised with a detailed error message if the repository cannot be accessed, directing users to check Elasticsearch logs for more information.",
        "signature": "def get_repository(client, repository=''):",
        "type": "Function",
        "class_signature": null
      },
      "get_snapshot_data": {
        "code": "def get_snapshot_data(client, repository=None):\n    \"\"\"Retrieve all snapshots from a specified Elasticsearch repository.\n\nThis function interacts with the Elasticsearch client to fetch a list of all snapshots \navailable in the provided repository. It requires a repository name and raises a \nMissingArgument exception if the repository is not specified. Upon failure to retrieve \nthe snapshot information, it raises a FailedExecution exception detailing the error.\n\nParameters:\n- client (Elasticsearch): A client connection object used to interact with an Elasticsearch cluster.\n- repository (str): The name of the Elasticsearch snapshot repository to query.\n\nReturns:\n- list: A list containing all snapshots from the specified repository. If none exist, an empty list is returned.\n\nDependencies:\n- The function relies on the Elasticsearch client's snapshot.get method to fetch the snapshot data.\n- Interaction with the exceptions module from elasticsearch8 is utilized to handle potential transport errors or not found errors.\"\"\"\n    '\\n    Get all snapshots from repository and return a list.\\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get`\\n\\n    :param client: A client connection object\\n    :param repository: The Elasticsearch snapshot repository to use\\n\\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\\n    :type repository: str\\n\\n    :returns: The list of all snapshots from ``repository``\\n    :rtype: list\\n    '\n    if not repository:\n        raise MissingArgument('No value for \"repository\" provided')\n    try:\n        return client.snapshot.get(repository=repository, snapshot='*')['snapshots']\n    except (es8exc.TransportError, es8exc.NotFoundError) as err:\n        msg = f'Unable to get snapshot information from repository: {repository}. Error: {err}'\n        raise FailedExecution(msg) from err",
        "docstring": "Retrieve all snapshots from a specified Elasticsearch repository.\n\nThis function interacts with the Elasticsearch client to fetch a list of all snapshots \navailable in the provided repository. It requires a repository name and raises a \nMissingArgument exception if the repository is not specified. Upon failure to retrieve \nthe snapshot information, it raises a FailedExecution exception detailing the error.\n\nParameters:\n- client (Elasticsearch): A client connection object used to interact with an Elasticsearch cluster.\n- repository (str): The name of the Elasticsearch snapshot repository to query.\n\nReturns:\n- list: A list containing all snapshots from the specified repository. If none exist, an empty list is returned.\n\nDependencies:\n- The function relies on the Elasticsearch client's snapshot.get method to fetch the snapshot data.\n- Interaction with the exceptions module from elasticsearch8 is utilized to handle potential transport errors or not found errors.",
        "signature": "def get_snapshot_data(client, repository=None):",
        "type": "Function",
        "class_signature": null
      }
    },
    "curator/helpers/utils.py": {
      "report_failure": {
        "code": "def report_failure(exception):\n    \"\"\"Raises a `FailedExecution` exception when an upstream error occurs, providing a detailed error message for troubleshooting. The function captures the provided exception and formats it into a user-friendly message, advising users to check Elasticsearch logs or rerun with a DEBUG log level for more detailed insights.\n\nParameters:\n- exception (Exception): The upstream exception that triggered this failure. \n\nReturns:\n- None: This function does not return a value; it raises an exception instead.\n\nDependencies:\n- `FailedExecution`: This exception class is imported from `curator.exceptions` and is used to signify a failure within the execution flow.\"\"\"\n    '\\n    Raise a :py:exc:`~.curator.exceptions.FailedExecution` exception and include the original error\\n    message.\\n\\n    :param exception: The upstream exception.\\n\\n    :type exception: :py:exc:Exception\\n\\n    :rtype: None\\n    '\n    raise FailedExecution(f'Exception encountered.  Rerun with loglevel DEBUG and/or check Elasticsearch logs formore information. Exception: {exception}')",
        "docstring": "Raises a `FailedExecution` exception when an upstream error occurs, providing a detailed error message for troubleshooting. The function captures the provided exception and formats it into a user-friendly message, advising users to check Elasticsearch logs or rerun with a DEBUG log level for more detailed insights.\n\nParameters:\n- exception (Exception): The upstream exception that triggered this failure. \n\nReturns:\n- None: This function does not return a value; it raises an exception instead.\n\nDependencies:\n- `FailedExecution`: This exception class is imported from `curator.exceptions` and is used to signify a failure within the execution flow.",
        "signature": "def report_failure(exception):",
        "type": "Function",
        "class_signature": null
      }
    },
    "curator/actions/snapshot.py": {
      "DeleteSnapshots.__init__": {
        "code": "    def __init__(self, slo, retry_interval=120, retry_count=3):\n        \"\"\"Initializes the DeleteSnapshots action class to manage the deletion of snapshots from an Elasticsearch repository.\n\nParameters:\n- slo (SnapshotList): An object representing a list of snapshots to delete. It must be validated to ensure it contains valid snapshots.\n- retry_interval (int, optional): The time in seconds to wait between retry attempts if a deletion fails. Default is 120 seconds.\n- retry_count (int, optional): The maximum number of attempts to make for each snapshot deletion. Default is 3 attempts.\n\nAttributes:\n- snapshot_list: Stores the SnapshotList object passed as a parameter, providing access to the list of snapshots and associated methods.\n- client: The Elasticsearch client used to perform deletion operations derived from the snapshot_list.\n- retry_interval: The interval between retry attempts for deleting snapshots.\n- retry_count: The number of retry attempts for snapshot deletions.\n- repository: The repository from which snapshots will be deleted, derived from the snapshot_list.\n- loggit: Logger for logging actions and errors during the deletion process.\n\nThe class interacts with the verify_snapshot_list function to validate the input SnapshotList object and is designed to handle snapshot deletions, providing configurable retry behavior.\"\"\"\n        '\\n        :param slo: A SnapshotList object\\n        :type slo: :py:class:`~.curator.snapshotlist.SnapshotList`\\n        :param retry_interval: Seconds to delay betwen retries. (Default: ``120``)\\n        :type retry_interval: int\\n        :param retry_count: Number of attempts to make. (Default: ``3``)\\n        :type retry_count: int\\n        '\n        verify_snapshot_list(slo)\n        self.snapshot_list = slo\n        self.client = slo.client\n        self.retry_interval = retry_interval\n        self.retry_count = retry_count\n        self.repository = slo.repository\n        self.loggit = logging.getLogger('curator.actions.delete_snapshots')",
        "docstring": "Initializes the DeleteSnapshots action class to manage the deletion of snapshots from an Elasticsearch repository.\n\nParameters:\n- slo (SnapshotList): An object representing a list of snapshots to delete. It must be validated to ensure it contains valid snapshots.\n- retry_interval (int, optional): The time in seconds to wait between retry attempts if a deletion fails. Default is 120 seconds.\n- retry_count (int, optional): The maximum number of attempts to make for each snapshot deletion. Default is 3 attempts.\n\nAttributes:\n- snapshot_list: Stores the SnapshotList object passed as a parameter, providing access to the list of snapshots and associated methods.\n- client: The Elasticsearch client used to perform deletion operations derived from the snapshot_list.\n- retry_interval: The interval between retry attempts for deleting snapshots.\n- retry_count: The number of retry attempts for snapshot deletions.\n- repository: The repository from which snapshots will be deleted, derived from the snapshot_list.\n- loggit: Logger for logging actions and errors during the deletion process.\n\nThe class interacts with the verify_snapshot_list function to validate the input SnapshotList object and is designed to handle snapshot deletions, providing configurable retry behavior.",
        "signature": "def __init__(self, slo, retry_interval=120, retry_count=3):",
        "type": "Method",
        "class_signature": "class DeleteSnapshots:"
      },
      "DeleteSnapshots.do_dry_run": {
        "code": "    def do_dry_run(self):\n        \"\"\"Log the intended actions of the DeleteSnapshots class without executing any deletion. This method is used to simulate a dry run of snapshot deletions, providing details on which snapshots would be deleted along with their associated deletion arguments. \n\nAttributes:\n- `repository`: The repository from which snapshots will be deleted, derived from the SnapshotList object.\n- `retry_interval`: The time in seconds to wait between retries, allowing for error handling in case of deletion failures.\n- `retry_count`: The number of attempts to delete a snapshot if the first attempt fails.\n\nThis method interacts with the `snapshot_list` of the DeleteSnapshots class to retrieve the list of snapshots that would be deleted. It logs the outcome using the Logger instance (`loggit`). No parameters are used or returned, and it has no side effects as it does not modify any state or data.\"\"\"\n        'Log what the output would be, but take no action.'\n        self.loggit.info('DRY-RUN MODE.  No changes will be made.')\n        mykwargs = {'repository': self.repository, 'retry_interval': self.retry_interval, 'retry_count': self.retry_count}\n        for snap in self.snapshot_list.snapshots:\n            self.loggit.info('DRY-RUN: delete_snapshot: %s with arguments: %s', snap, mykwargs)",
        "docstring": "Log the intended actions of the DeleteSnapshots class without executing any deletion. This method is used to simulate a dry run of snapshot deletions, providing details on which snapshots would be deleted along with their associated deletion arguments. \n\nAttributes:\n- `repository`: The repository from which snapshots will be deleted, derived from the SnapshotList object.\n- `retry_interval`: The time in seconds to wait between retries, allowing for error handling in case of deletion failures.\n- `retry_count`: The number of attempts to delete a snapshot if the first attempt fails.\n\nThis method interacts with the `snapshot_list` of the DeleteSnapshots class to retrieve the list of snapshots that would be deleted. It logs the outcome using the Logger instance (`loggit`). No parameters are used or returned, and it has no side effects as it does not modify any state or data.",
        "signature": "def do_dry_run(self):",
        "type": "Method",
        "class_signature": "class DeleteSnapshots:"
      },
      "DeleteSnapshots.do_action": {
        "code": "    def do_action(self):\n        \"\"\"Perform the action of deleting snapshots specified in the snapshot list. This method attempts to delete each snapshot in the `snapshot_list` from the designated repository. The method will log the process and handle any exceptions by invoking the `report_failure` function for error reporting.\n\nIf the snapshot list is empty, the `empty_list_check` method is called to ensure that the operation doesn't proceed with no snapshots to delete.\n\nParameters:\n    - self: The instance of the DeleteSnapshots class, which contains attributes like `snapshot_list` (a SnapshotList object containing snapshots to delete), and `repository` (the repository name from which the snapshots will be deleted).\n\nReturns:\n    - None: The method does not return a value but instead performs operations to delete snapshots.\n\nDependencies:\n    - `self.snapshot_list`: Should contain valid snapshots and the associated client for deletion operations.\n    - `self.client`: The Elasticsearch client used to interact with the snapshot service.\n    - `self.loggit`: A logger instance initialized in the constructor of the DeleteSnapshots class for logging actions and exceptions.\n    \nSide effects:\n    - Deletes specified snapshots from the given repository and logs the deletion process.\n    - If an exception occurs, the `report_failure` function is invoked to handle the error.\"\"\"\n        '\\n        :py:meth:`~.elasticsearch.client.SnapshotClient.delete` snapshots in\\n        :py:attr:`snapshot_list`. Retry up to :py:attr:`retry_count` times, pausing\\n        :py:attr:`retry_interval` seconds between retries.\\n        '\n        self.snapshot_list.empty_list_check()\n        msg = f'Deleting {len(self.snapshot_list.snapshots)} selected snapshots: {self.snapshot_list.snapshots}'\n        self.loggit.info(msg)\n        try:\n            for snap in self.snapshot_list.snapshots:\n                self.loggit.info('Deleting snapshot %s...', snap)\n                self.client.snapshot.delete(repository=self.repository, snapshot=snap)\n        except Exception as err:\n            report_failure(err)",
        "docstring": "Perform the action of deleting snapshots specified in the snapshot list. This method attempts to delete each snapshot in the `snapshot_list` from the designated repository. The method will log the process and handle any exceptions by invoking the `report_failure` function for error reporting.\n\nIf the snapshot list is empty, the `empty_list_check` method is called to ensure that the operation doesn't proceed with no snapshots to delete.\n\nParameters:\n    - self: The instance of the DeleteSnapshots class, which contains attributes like `snapshot_list` (a SnapshotList object containing snapshots to delete), and `repository` (the repository name from which the snapshots will be deleted).\n\nReturns:\n    - None: The method does not return a value but instead performs operations to delete snapshots.\n\nDependencies:\n    - `self.snapshot_list`: Should contain valid snapshots and the associated client for deletion operations.\n    - `self.client`: The Elasticsearch client used to interact with the snapshot service.\n    - `self.loggit`: A logger instance initialized in the constructor of the DeleteSnapshots class for logging actions and exceptions.\n    \nSide effects:\n    - Deletes specified snapshots from the given repository and logs the deletion process.\n    - If an exception occurs, the `report_failure` function is invoked to handle the error.",
        "signature": "def do_action(self):",
        "type": "Method",
        "class_signature": "class DeleteSnapshots:"
      }
    },
    "curator/exceptions.py": {}
  },
  "dependency_dict": {
    "curator/snapshotlist.py:SnapshotList:__init__": {},
    "curator/helpers/testers.py:verify_client_object": {},
    "curator/helpers/testers.py:repository_exists": {},
    "curator/snapshotlist.py:SnapshotList:__get_snapshots": {},
    "curator/actions/snapshot.py:DeleteSnapshots:__init__": {},
    "curator/helpers/testers.py:verify_snapshot_list": {},
    "curator/actions/snapshot.py:DeleteSnapshots:do_action": {},
    "curator/snapshotlist.py:SnapshotList:empty_list_check": {},
    "curator/helpers/utils.py:report_failure": {}
  },
  "call_tree": {
    "tests/unit/test_action_delete_snapshots.py:TestActionDeleteSnapshots:test_do_action": {
      "curator/snapshotlist.py:SnapshotList:__init__": {
        "curator/helpers/testers.py:verify_client_object": {},
        "curator/helpers/testers.py:repository_exists": {
          "curator/helpers/getters.py:get_repository": {}
        },
        "curator/snapshotlist.py:SnapshotList:__get_snapshots": {
          "curator/helpers/getters.py:get_snapshot_data": {},
          "curator/snapshotlist.py:SnapshotList:empty_list_check": {}
        }
      },
      "curator/actions/snapshot.py:DeleteSnapshots:__init__": {
        "curator/helpers/testers.py:verify_snapshot_list": {
          "curator/snapshotlist.py:SnapshotList:SnapshotList": {}
        }
      },
      "curator/actions/snapshot.py:DeleteSnapshots:do_action": {
        "curator/snapshotlist.py:SnapshotList:empty_list_check": {}
      }
    },
    "tests/unit/test_action_delete_snapshots.py:TestActionDeleteSnapshots:test_do_action_raises_exception": {
      "curator/snapshotlist.py:SnapshotList:__init__": {
        "curator/helpers/testers.py:verify_client_object": {},
        "curator/helpers/testers.py:repository_exists": {
          "curator/helpers/getters.py:get_repository": {}
        },
        "curator/snapshotlist.py:SnapshotList:__get_snapshots": {
          "curator/helpers/getters.py:get_snapshot_data": {},
          "curator/snapshotlist.py:SnapshotList:empty_list_check": {}
        }
      },
      "curator/actions/snapshot.py:DeleteSnapshots:__init__": {
        "curator/helpers/testers.py:verify_snapshot_list": {}
      },
      "curator/actions/snapshot.py:DeleteSnapshots:do_action": {
        "curator/snapshotlist.py:SnapshotList:empty_list_check": {},
        "curator/helpers/utils.py:report_failure": {}
      }
    },
    "tests/unit/test_action_delete_snapshots.py:TestActionDeleteSnapshots:test_do_dry_run": {
      "curator/snapshotlist.py:SnapshotList:__init__": {
        "curator/helpers/testers.py:verify_client_object": {},
        "curator/helpers/testers.py:repository_exists": {
          "curator/helpers/getters.py:get_repository": {}
        },
        "curator/snapshotlist.py:SnapshotList:__get_snapshots": {
          "curator/helpers/getters.py:get_snapshot_data": {},
          "curator/snapshotlist.py:SnapshotList:empty_list_check": {}
        }
      },
      "curator/actions/snapshot.py:DeleteSnapshots:__init__": {
        "curator/helpers/testers.py:verify_snapshot_list": {}
      },
      "curator/actions/snapshot.py:DeleteSnapshots:do_dry_run": {}
    },
    "tests/unit/test_action_delete_snapshots.py:TestActionDeleteSnapshots:test_init": {
      "curator/snapshotlist.py:SnapshotList:__init__": {
        "curator/helpers/testers.py:verify_client_object": {},
        "curator/helpers/testers.py:repository_exists": {
          "curator/helpers/getters.py:get_repository": {}
        },
        "curator/snapshotlist.py:SnapshotList:__get_snapshots": {
          "curator/helpers/getters.py:get_snapshot_data": {},
          "curator/snapshotlist.py:SnapshotList:empty_list_check": {}
        }
      },
      "curator/actions/snapshot.py:DeleteSnapshots:__init__": {
        "curator/helpers/testers.py:verify_snapshot_list": {}
      }
    },
    "tests/unit/test_action_delete_snapshots.py:TestActionDeleteSnapshots:test_init_raise": {
      "curator/actions/snapshot.py:DeleteSnapshots:__init__": {
        "curator/helpers/testers.py:verify_snapshot_list": {}
      }
    },
    "/mnt/sfs_turbo/yaxindu/tmp/elasticsearch_curator-image-test_action_delete_snapshots/elasticsearch_curator-test_action_delete_snapshots/tests/integration/test_cli.py:TestCLIMethods:test_action_is_none": {
      "curator/exceptions.py:ConfigurationError:ConfigurationError": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/elasticsearch_curator-image-test_action_delete_snapshots/elasticsearch_curator-test_action_delete_snapshots/tests/integration/test_cli.py:TestCLIMethods:test_no_action": {
      "curator/exceptions.py:ConfigurationError:ConfigurationError": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/elasticsearch_curator-image-test_action_delete_snapshots/elasticsearch_curator-test_action_delete_snapshots/tests/integration/test_integrations.py:TestFilters:test_filter_by_alias_bad_aliases": {
      "curator/exceptions.py:ConfigurationError:ConfigurationError": {}
    }
  },
  "PRD": "# PROJECT NAME: elasticsearch_curator-test_action_delete_snapshots\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 curator/\n    \u251c\u2500\u2500 actions/\n    \u2502   \u2514\u2500\u2500 snapshot.py\n    \u2502       \u251c\u2500\u2500 DeleteSnapshots.__init__\n    \u2502       \u251c\u2500\u2500 DeleteSnapshots.do_action\n    \u2502       \u2514\u2500\u2500 DeleteSnapshots.do_dry_run\n    \u251c\u2500\u2500 exceptions.py\n    \u2502   \u2514\u2500\u2500 ConfigurationError.ConfigurationError\n    \u251c\u2500\u2500 helpers/\n    \u2502   \u251c\u2500\u2500 getters.py\n    \u2502   \u2502   \u251c\u2500\u2500 get_repository\n    \u2502   \u2502   \u2514\u2500\u2500 get_snapshot_data\n    \u2502   \u251c\u2500\u2500 testers.py\n    \u2502   \u2502   \u251c\u2500\u2500 repository_exists\n    \u2502   \u2502   \u251c\u2500\u2500 verify_client_object\n    \u2502   \u2502   \u2514\u2500\u2500 verify_snapshot_list\n    \u2502   \u2514\u2500\u2500 utils.py\n    \u2502       \u2514\u2500\u2500 report_failure\n    \u2514\u2500\u2500 snapshotlist.py\n        \u251c\u2500\u2500 SnapshotList.SnapshotList\n        \u251c\u2500\u2500 SnapshotList.__get_snapshots\n        \u251c\u2500\u2500 SnapshotList.__init__\n        \u2514\u2500\u2500 SnapshotList.empty_list_check\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThe module is designed to facilitate and streamline the process of managing and deleting snapshots in an Elasticsearch cluster. It provides functionality to interact with snapshot repositories, validate snapshot lists, and execute deletion operations, including support for both dry runs and actual execution of snapshot deletions. By leveraging these capabilities, the module ensures that snapshot removal is handled efficiently while allowing for pre-emptive validation to reduce the risk of errors or disruptions in ongoing operations. This functionality addresses the need for developers and operators to automate and safely manage snapshots in environments where effective lifecycle management of backups is critical to maintaining storage efficiency and operational reliability.\n\n## FILE 1: curator/snapshotlist.py\n\n- CLASS METHOD: SnapshotList.empty_list_check\n  - CLASS SIGNATURE: class SnapshotList:\n  - SIGNATURE: def empty_list_check(self):\n  - DOCSTRING: \n```python\n\"\"\"\nCheck if the list of snapshots is empty and raise an exception if true.\n\nThis method verifies whether the `snapshots` attribute of the `SnapshotList` instance contains any items. If `snapshots` is empty, it raises a `NoSnapshots` exception with a message indicating that the snapshot list is empty. This is crucial for preventing operations on an empty list of snapshots, which would lead to errors in subsequent methods that expect the list to contain snapshot data.\n\nDependencies: \n- `NoSnapshots`: An exception class that is raised when there are no snapshots available in the list.\n\nExample usage: Typically called prior to operations that require non-empty snapshots to ensure safety and correctness.\n\"\"\"\n```\n\n- CLASS METHOD: SnapshotList.__get_snapshots\n  - CLASS SIGNATURE: class SnapshotList:\n  - SIGNATURE: def __get_snapshots(self):\n  - DOCSTRING: \n```python\n\"\"\"\nPulls all snapshots from the specified repository into the `snapshots` attribute and populates the `snapshot_info` dictionary with metadata about each snapshot.\n\nThis method interacts with the `get_snapshot_data` function from the `curator.helpers.getters` module to retrieve snapshot data using the `client` and `repository` provided during the initialization of the `SnapshotList` instance. It then iterates through the retrieved data, adding snapshot names to the `snapshots` list and their corresponding details to the `snapshot_info` dictionary, indexed by snapshot name. If no snapshots are found, it calls the `empty_list_check` method to verify that the `snapshots` list is not empty, raising a `NoSnapshots` exception if it is.\n\nAttributes:\n- `self.snapshots`: A list that holds the names of the snapshots.\n- `self.snapshot_info`: A dictionary that maps each snapshot name to its detailed information.\n\nDependencies:\n- Requires a valid Elasticsearch `client` and an existing `repository`.\n- Dependent on the `get_snapshot_data` function for fetching snapshots.\n\nSide Effects:\n- Modifies the state of `self.snapshots` and `self.snapshot_info`.\n\"\"\"\n```\n\n- CLASS METHOD: SnapshotList.__init__\n  - CLASS SIGNATURE: class SnapshotList:\n  - SIGNATURE: def __init__(self, client, repository=None):\n  - DOCSTRING: \n```python\n\"\"\"\nInitializer for the SnapshotList class, which manages a list of snapshots from a specified Elasticsearch repository. This constructor verifies the client and repository parameters, initializes logging, and retrieves snapshots to populate the internal snapshot_info and snapshots attributes.\n\nParameters:\n    client (Elasticsearch): An instance of the Elasticsearch client used to connect to the cluster.\n    repository (str, optional): The name of the snapshot repository to be used. This is required.\n\nRaises:\n    MissingArgument: If the repository parameter is not provided.\n    FailedExecution: If the specified repository does not exist in the provided client.\n\nAttributes:\n    client: The Elasticsearch client passed as an argument.\n    repository: The name of the snapshot repository.\n    snapshot_info (dict): A dictionary storing metadata about the snapshots, populated by the __get_snapshots() method.\n    snapshots (list): A list of snapshot identifiers, also populated by the __get_snapshots() method. \n    age_keyfield (str): A placeholder for snapshot age information, set by other methods in the class.\n\nDependencies:\n- The method requires the verify_client_object and repository_exists functions to validate the client and repository.\n- It uses the logging module to set up logging, specifically targeting 'curator.snapshotlist'.\n- Retrieves snapshot data using the __get_snapshots method, which accesses the repository to populate snapshot information.\n\"\"\"\n```\n\n## FILE 2: curator/helpers/testers.py\n\n- FUNCTION NAME: repository_exists\n  - SIGNATURE: def repository_exists(client, repository=None):\n  - DOCSTRING: \n```python\n\"\"\"\nCheck the existence of an Elasticsearch snapshot repository.\n\nThis function checks if a specified snapshot repository exists in the Elasticsearch cluster defined by the provided client connection. It utilizes the `get_repository` function to retrieve repository information and returns a boolean indicating whether the specified repository exists. If the repository name is not provided, a `MissingArgument` exception is raised.\n\nParameters:\n- client (:py:class:`~.elasticsearch.Elasticsearch`): A client connection object to interact with the Elasticsearch cluster.\n- repository (str, optional): The name of the Elasticsearch snapshot repository to check for.\n\nReturns:\n- bool: Returns `True` if the specified repository exists, otherwise `False`.\n\nDependencies:\n- This function relies on the `get_repository` helper function from the `curator.helpers.getters` module to check for the repository's existence.\n- It imports the `MissingArgument` exception from the `curator.exceptions` module, which is raised if the repository parameter is not provided.\n- It makes use of the `logging` module for logging debug information regarding the repository check results.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - curator/snapshotlist.py:SnapshotList:__init__\n    - curator/helpers/getters.py:get_repository\n\n- FUNCTION NAME: verify_snapshot_list\n  - SIGNATURE: def verify_snapshot_list(test):\n  - DOCSTRING: \n```python\n\"\"\"\nVerify that the provided `test` variable is an instance of the `SnapshotList` class.\n\n:param test: The variable or object to test for the appropriate type.\n:type test: :py:class:`~.curator.SnapshotList`\n\n:raises TypeError: If `test` is not an instance of `SnapshotList`, an error is logged and a `TypeError` is raised.\n\nThis function relies on the `SnapshotList` class defined in the `curator.snapshotlist` module. If the import of `SnapshotList` fails due to circular dependencies, an import error may occur. The function uses the logging module to log errors related to type validation.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - curator/actions/snapshot.py:DeleteSnapshots:__init__\n    - curator/snapshotlist.py:SnapshotList:SnapshotList\n\n- FUNCTION NAME: verify_client_object\n  - SIGNATURE: def verify_client_object(test):\n  - DOCSTRING: \n```python\n\"\"\"\nVerify the validity of an Elasticsearch client object.\n\nThis function checks whether the provided variable (test) is a valid instance of the Elasticsearch client class. It allows for compatibility with unit tests by excluding mock objects. If the input is not a valid Elasticsearch client, a TypeError is raised, and an error message is logged.\n\n:param test: The variable or object to test as a valid Elasticsearch client.\n:type test: :py:class:`~.elasticsearch.Elasticsearch`\n\n:raises TypeError: If the input is not a valid Elasticsearch client object.\n\n:return: None, raises an exception for invalid input.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - curator/snapshotlist.py:SnapshotList:__init__\n\n## FILE 3: curator/helpers/getters.py\n\n- FUNCTION NAME: get_snapshot_data\n  - SIGNATURE: def get_snapshot_data(client, repository=None):\n  - DOCSTRING: \n```python\n\"\"\"\nRetrieve all snapshots from a specified Elasticsearch repository.\n\nThis function interacts with the Elasticsearch client to fetch a list of all snapshots \navailable in the provided repository. It requires a repository name and raises a \nMissingArgument exception if the repository is not specified. Upon failure to retrieve \nthe snapshot information, it raises a FailedExecution exception detailing the error.\n\nParameters:\n- client (Elasticsearch): A client connection object used to interact with an Elasticsearch cluster.\n- repository (str): The name of the Elasticsearch snapshot repository to query.\n\nReturns:\n- list: A list containing all snapshots from the specified repository. If none exist, an empty list is returned.\n\nDependencies:\n- The function relies on the Elasticsearch client's snapshot.get method to fetch the snapshot data.\n- Interaction with the exceptions module from elasticsearch8 is utilized to handle potential transport errors or not found errors.\n\"\"\"\n```\n\n- FUNCTION NAME: get_repository\n  - SIGNATURE: def get_repository(client, repository=''):\n  - DOCSTRING: \n```python\n\"\"\"\nRetrieve configuration information for a specified Elasticsearch snapshot repository.\n\nParameters:\n- client (Elasticsearch): A client connection object to interact with the Elasticsearch cluster.\n- repository (str): The name of the Elasticsearch snapshot repository from which to retrieve information.\n\nReturns:\n- dict: A dictionary containing the configuration details of the specified repository.\n\nRaises:\n- CuratorException: If an error occurs while attempting to retrieve the repository, including transport errors or if the repository is not found.\n\nThis function relies on the `es8exc.TransportError` and `es8exc.NotFoundError` exceptions from the `elasticsearch8` library to handle errors during the repository retrieval. A `CuratorException` is raised with a detailed error message if the repository cannot be accessed, directing users to check Elasticsearch logs for more information.\n\"\"\"\n```\n\n## FILE 4: curator/helpers/utils.py\n\n- FUNCTION NAME: report_failure\n  - SIGNATURE: def report_failure(exception):\n  - DOCSTRING: \n```python\n\"\"\"\nRaises a `FailedExecution` exception when an upstream error occurs, providing a detailed error message for troubleshooting. The function captures the provided exception and formats it into a user-friendly message, advising users to check Elasticsearch logs or rerun with a DEBUG log level for more detailed insights.\n\nParameters:\n- exception (Exception): The upstream exception that triggered this failure. \n\nReturns:\n- None: This function does not return a value; it raises an exception instead.\n\nDependencies:\n- `FailedExecution`: This exception class is imported from `curator.exceptions` and is used to signify a failure within the execution flow.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - curator/actions/snapshot.py:DeleteSnapshots:do_action\n\n## FILE 5: curator/actions/snapshot.py\n\n- CLASS METHOD: DeleteSnapshots.do_action\n  - CLASS SIGNATURE: class DeleteSnapshots:\n  - SIGNATURE: def do_action(self):\n  - DOCSTRING: \n```python\n\"\"\"\nPerform the action of deleting snapshots specified in the snapshot list. This method attempts to delete each snapshot in the `snapshot_list` from the designated repository. The method will log the process and handle any exceptions by invoking the `report_failure` function for error reporting.\n\nIf the snapshot list is empty, the `empty_list_check` method is called to ensure that the operation doesn't proceed with no snapshots to delete.\n\nParameters:\n    - self: The instance of the DeleteSnapshots class, which contains attributes like `snapshot_list` (a SnapshotList object containing snapshots to delete), and `repository` (the repository name from which the snapshots will be deleted).\n\nReturns:\n    - None: The method does not return a value but instead performs operations to delete snapshots.\n\nDependencies:\n    - `self.snapshot_list`: Should contain valid snapshots and the associated client for deletion operations.\n    - `self.client`: The Elasticsearch client used to interact with the snapshot service.\n    - `self.loggit`: A logger instance initialized in the constructor of the DeleteSnapshots class for logging actions and exceptions.\n    \nSide effects:\n    - Deletes specified snapshots from the given repository and logs the deletion process.\n    - If an exception occurs, the `report_failure` function is invoked to handle the error.\n\"\"\"\n```\n\n- CLASS METHOD: DeleteSnapshots.do_dry_run\n  - CLASS SIGNATURE: class DeleteSnapshots:\n  - SIGNATURE: def do_dry_run(self):\n  - DOCSTRING: \n```python\n\"\"\"\nLog the intended actions of the DeleteSnapshots class without executing any deletion. This method is used to simulate a dry run of snapshot deletions, providing details on which snapshots would be deleted along with their associated deletion arguments. \n\nAttributes:\n- `repository`: The repository from which snapshots will be deleted, derived from the SnapshotList object.\n- `retry_interval`: The time in seconds to wait between retries, allowing for error handling in case of deletion failures.\n- `retry_count`: The number of attempts to delete a snapshot if the first attempt fails.\n\nThis method interacts with the `snapshot_list` of the DeleteSnapshots class to retrieve the list of snapshots that would be deleted. It logs the outcome using the Logger instance (`loggit`). No parameters are used or returned, and it has no side effects as it does not modify any state or data.\n\"\"\"\n```\n\n- CLASS METHOD: DeleteSnapshots.__init__\n  - CLASS SIGNATURE: class DeleteSnapshots:\n  - SIGNATURE: def __init__(self, slo, retry_interval=120, retry_count=3):\n  - DOCSTRING: \n```python\n\"\"\"\nInitializes the DeleteSnapshots action class to manage the deletion of snapshots from an Elasticsearch repository.\n\nParameters:\n- slo (SnapshotList): An object representing a list of snapshots to delete. It must be validated to ensure it contains valid snapshots.\n- retry_interval (int, optional): The time in seconds to wait between retry attempts if a deletion fails. Default is 120 seconds.\n- retry_count (int, optional): The maximum number of attempts to make for each snapshot deletion. Default is 3 attempts.\n\nAttributes:\n- snapshot_list: Stores the SnapshotList object passed as a parameter, providing access to the list of snapshots and associated methods.\n- client: The Elasticsearch client used to perform deletion operations derived from the snapshot_list.\n- retry_interval: The interval between retry attempts for deleting snapshots.\n- retry_count: The number of retry attempts for snapshot deletions.\n- repository: The repository from which snapshots will be deleted, derived from the snapshot_list.\n- loggit: Logger for logging actions and errors during the deletion process.\n\nThe class interacts with the verify_snapshot_list function to validate the input SnapshotList object and is designed to handle snapshot deletions, providing configurable retry behavior.\n\"\"\"\n```\n\n## FILE 6: curator/exceptions.py\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "curator/snapshotlist.py": "\"\"\"SnapshotList\"\"\"\nimport re\nimport logging\nfrom es_client.helpers.schemacheck import SchemaCheck\nfrom curator.exceptions import ConfigurationError, FailedExecution, MissingArgument, NoSnapshots\nfrom curator.helpers.date_ops import absolute_date_range, date_range, fix_epoch, get_date_regex, get_point_of_reference, TimestringSearch\nfrom curator.helpers.getters import get_snapshot_data\nfrom curator.helpers.testers import repository_exists, verify_client_object\nfrom curator.helpers.utils import report_failure\nfrom curator.defaults import settings\nfrom curator.validators.filter_functions import filterstructure\n\nclass SnapshotList:\n    \"\"\"Snapshot list object\"\"\"\n\n    def __actionable(self, snap):\n        self.loggit.debug('Snapshot %s is actionable and remains in the list.', snap)\n\n    def __not_actionable(self, snap):\n        self.loggit.debug('Snapshot %s is not actionable, removing from list.', snap)\n        self.snapshots.remove(snap)\n\n    def __excludify(self, condition, exclude, snap, msg=None):\n        if condition:\n            if exclude:\n                text = 'Removed from actionable list'\n                self.__not_actionable(snap)\n            else:\n                text = 'Remains in actionable list'\n                self.__actionable(snap)\n        elif exclude:\n            text = 'Remains in actionable list'\n            self.__actionable(snap)\n        else:\n            text = 'Removed from actionable list'\n            self.__not_actionable(snap)\n        if msg:\n            self.loggit.debug('%s: %s', text, msg)\n\n    def __map_method(self, ftype):\n        methods = {'age': self.filter_by_age, 'count': self.filter_by_count, 'none': self.filter_none, 'pattern': self.filter_by_regex, 'period': self.filter_period, 'state': self.filter_by_state}\n        return methods[ftype]\n\n    def working_list(self):\n        \"\"\"\n        Return the current value of ``snapshots`` as copy-by-value to prevent list stomping during\n        iterations\n        \"\"\"\n        return self.snapshots[:]\n\n    def _get_name_based_ages(self, timestring):\n        \"\"\"\n        Add a snapshot age to ``snapshot_info`` based on the age as indicated by the snapshot name\n        pattern, if it matches ``timestring``.  This is stored at key ``age_by_name``.\n\n        :param timestring: A :py:func:`time.strftime` pattern\n        \"\"\"\n        self.empty_list_check()\n        tstamp = TimestringSearch(timestring)\n        for snapshot in self.working_list():\n            epoch = tstamp.get_epoch(snapshot)\n            if epoch:\n                self.snapshot_info[snapshot]['age_by_name'] = epoch\n            else:\n                self.snapshot_info[snapshot]['age_by_name'] = None\n\n    def _calculate_ages(self, source='creation_date', timestring=None):\n        \"\"\"\n        This method initiates snapshot age calculation based on the given parameters.  Exceptions\n        are raised when they are improperly configured.\n\n        Set instance variable ``age_keyfield`` for use later, if needed.\n\n        :param source: Source of snapshot age. Can be ``name`` or ``creation_date``.\n        :param timestring: An :py:func:`time.strftime` string to match the datestamp in an snapshot name. Only used\n            if ``source=name``.\n        \"\"\"\n        if source == 'name':\n            self.age_keyfield = 'age_by_name'\n            if not timestring:\n                raise MissingArgument('source \"name\" requires the \"timestring\" keyword argument')\n            self._get_name_based_ages(timestring)\n        elif source == 'creation_date':\n            self.age_keyfield = 'start_time_in_millis'\n        else:\n            raise ValueError(f'Invalid source: {source}. Must be \"name\", or \"creation_date\".')\n\n    def _sort_by_age(self, snapshot_list, reverse=True):\n        \"\"\"\n        Take a list of snapshots and sort them by date.\n\n        By default, the youngest are first with ``reverse=True``, but the oldest can be first by\n        setting ``reverse=False``\n        \"\"\"\n        temp = {}\n        for snap in snapshot_list:\n            if self.age_keyfield in self.snapshot_info[snap]:\n                if self.snapshot_info[snap][self.age_keyfield]:\n                    temp[snap] = self.snapshot_info[snap][self.age_keyfield]\n                else:\n                    msg = ' snapshot %s has no age' % snap\n                    self.__excludify(True, True, snap, msg)\n            else:\n                msg = f'{snap} does not have age key \"{self.age_keyfield}\" in SnapshotList metadata'\n                self.__excludify(True, True, snap, msg)\n        sorted_tuple = sorted(temp.items(), key=lambda k: k[1], reverse=reverse)\n        return [x[0] for x in sorted_tuple]\n\n    def most_recent(self):\n        \"\"\"\n        Return the most recent snapshot based on ``start_time_in_millis``.\n        \"\"\"\n        self.empty_list_check()\n        most_recent_time = 0\n        most_recent_snap = ''\n        for snapshot in self.snapshots:\n            snaptime = fix_epoch(self.snapshot_info[snapshot]['start_time_in_millis'])\n            if snaptime > most_recent_time:\n                most_recent_snap = snapshot\n                most_recent_time = snaptime\n        return most_recent_snap\n\n    def filter_by_regex(self, kind=None, value=None, exclude=False):\n        \"\"\"\n        Filter out snapshots not matching the pattern, or in the case of\n        exclude, filter those matching the pattern.\n\n        :param kind: Can be one of: ``suffix``, ``prefix``, ``regex``, or\n            ``timestring``. This option defines what kind of filter you will be\n            building.\n        :param value: Depends on ``kind``. It is the :py:func:`time.strftime` string if ``kind`` is\n            ``timestring``. It's used to build the regular expression for other kinds.\n        :param exclude: If ``exclude=True``, this filter will remove matching snapshots from\n            ``snapshots``. If ``exclude=False``, then only matching snapshots will be kept in\n            ``snapshots``. Default is ``False``\n        \"\"\"\n        if kind not in ['regex', 'prefix', 'suffix', 'timestring']:\n            raise ValueError(f'{kind}: Invalid value for kind')\n        if value == 0:\n            pass\n        elif not value:\n            raise ValueError(f'{value}: Invalid value for \"value\". Cannot be \"None\" type, empty, or False')\n        if kind == 'timestring':\n            regex = settings.regex_map()[kind].format(get_date_regex(value))\n        else:\n            regex = settings.regex_map()[kind].format(value)\n        self.empty_list_check()\n        pattern = re.compile(regex)\n        for snapshot in self.working_list():\n            match = pattern.search(snapshot)\n            self.loggit.debug('Filter by regex: Snapshot: %s', snapshot)\n            if match:\n                self.__excludify(True, exclude, snapshot)\n            else:\n                self.__excludify(False, exclude, snapshot)\n\n    def filter_by_age(self, source='creation_date', direction=None, timestring=None, unit=None, unit_count=None, epoch=None, exclude=False):\n        \"\"\"\n        Remove snapshots from ``snapshots`` by relative age calculations.\n\n        :param source: Source of snapshot age. Can be ``name``, or ``creation_date``.\n        :param direction: Time to filter, either ``older`` or ``younger``\n        :param timestring: A :py:func:`time.strftime` string to match the datestamp in an snapshot\n            name. Only used for snapshot filtering by ``name``.\n        :param unit: One of ``seconds``, ``minutes``, ``hours``, ``days``, ``weeks``, ``months``, or\n            ``years``.\n        :param unit_count: The number of ``unit`` (s). ``unit_count`` * ``unit`` will be calculated\n            out to the relative number of seconds.\n        :param epoch: An epoch timestamp used in conjunction with ``unit`` and ``unit_count`` to\n            establish a point of reference for calculations. If not provided, the current time will\n            be used.\n        :param exclude: If ``exclude=True``, this filter will remove matching snapshots from\n            ``snapshots``. If ``exclude=False``, then only matching snapshots will be kept in\n            ``snapshots``. Default is ``False``\n        \"\"\"\n        self.loggit.debug('Starting filter_by_age')\n        por = get_point_of_reference(unit, unit_count, epoch)\n        self.loggit.debug('Point of Reference: %s', por)\n        if not direction:\n            raise MissingArgument('Must provide a value for \"direction\"')\n        if direction not in ['older', 'younger']:\n            raise ValueError(f'Invalid value for \"direction\": {direction}')\n        self._calculate_ages(source=source, timestring=timestring)\n        for snapshot in self.working_list():\n            if not self.snapshot_info[snapshot][self.age_keyfield]:\n                self.loggit.debug('Removing snapshot %s for having no age', snapshot)\n                self.snapshots.remove(snapshot)\n                continue\n            age = fix_epoch(self.snapshot_info[snapshot][self.age_keyfield])\n            msg = f'Snapshot \"{snapshot}\" age ({age}), direction: \"{direction}\", point of reference, ({por})'\n            snapshot_age = fix_epoch(self.snapshot_info[snapshot][self.age_keyfield])\n            if direction == 'older':\n                agetest = snapshot_age < por\n            else:\n                agetest = snapshot_age > por\n            self.__excludify(agetest, exclude, snapshot, msg)\n\n    def filter_by_state(self, state=None, exclude=False):\n        \"\"\"\n        Filter out snapshots not matching ``state``, or in the case of exclude, filter those\n        matching ``state``.\n\n        :param state: The snapshot state to filter for. Must be one of ``SUCCESS``, ``PARTIAL``,\n            ``FAILED``, or ``IN_PROGRESS``.\n        :param exclude: If ``exclude=True``, this filter will remove matching snapshots from\n            ``snapshots``. If ``exclude=False``, then only matching snapshots will be kept in\n            ``snapshots``. Default is ``False``\n        \"\"\"\n        if state.upper() not in ['SUCCESS', 'PARTIAL', 'FAILED', 'IN_PROGRESS']:\n            raise ValueError(f'{state}: Invalid value for state')\n        self.empty_list_check()\n        for snapshot in self.working_list():\n            self.loggit.debug('Filter by state: Snapshot: %s', snapshot)\n            if self.snapshot_info[snapshot]['state'] == state:\n                self.__excludify(True, exclude, snapshot)\n            else:\n                self.__excludify(False, exclude, snapshot)\n\n    def filter_none(self):\n        \"\"\"No filter at all\"\"\"\n        self.loggit.debug('\"None\" filter selected.  No filtering will be done.')\n\n    def filter_by_count(self, count=None, reverse=True, use_age=False, source='creation_date', timestring=None, exclude=True):\n        \"\"\"\n        Remove snapshots from the actionable list beyond the number ``count``, sorted\n        reverse-alphabetically by default.  If you set ``reverse=False``, it will be sorted\n        alphabetically.\n\n        The default is usually what you will want. If only one kind of snapshot is provided--for\n        example, snapshots matching ``curator-%Y%m%d%H%M%S``--then reverse alphabetical sorting\n        will mean the oldest will remain in the list, because lower numbers in the dates mean older\n        snapshots.\n\n        By setting ``reverse=False``, then ``snapshot3`` will be acted on before ``snapshot2``,\n        which will be acted on before ``snapshot1``\n\n        ``use_age`` allows ordering snapshots by age. Age is determined by the snapshot creation\n        date (as identified by ``start_time_in_millis``) by default, but you can also specify\n        ``source=name``.  The ``name`` ``source`` requires the timestring argument.\n\n        :param count: Filter snapshots beyond ``count``.\n        :param reverse: The filtering direction. (default: ``True``).\n        :param use_age: Sort snapshots by age.  ``source`` is required in this case.\n        :param source: Source of snapshot age. Can be one of ``name``, or ``creation_date``.\n            Default: ``creation_date``\n        :param timestring: A :py:func:`time.strftime` string to match the datestamp in a snapshot\n            name. Only used if ``source=name``.\n        :param exclude: If ``exclude=True``, this filter will remove matching snapshots from\n            ``snapshots``. If ``exclude=False``, then only matching snapshots will be kept in\n            ``snapshots``. Default is ``True``\n        \"\"\"\n        self.loggit.debug('Filtering snapshots by count')\n        if not count:\n            raise MissingArgument('No value for \"count\" provided')\n        working_list = self.working_list()\n        if use_age:\n            self._calculate_ages(source=source, timestring=timestring)\n            sorted_snapshots = self._sort_by_age(working_list, reverse=reverse)\n        else:\n            sorted_snapshots = sorted(working_list, reverse=reverse)\n        idx = 1\n        for snap in sorted_snapshots:\n            msg = f'{snap} is {idx} of specified count of {count}.'\n            condition = True if idx <= count else False\n            self.__excludify(condition, exclude, snap, msg)\n            idx += 1\n\n    def filter_period(self, period_type='relative', source='name', range_from=None, range_to=None, date_from=None, date_to=None, date_from_format=None, date_to_format=None, timestring=None, unit=None, week_starts_on='sunday', epoch=None, exclude=False):\n        \"\"\"\n        Match ``snapshots`` with ages within a given period.\n\n        :param period_type: Can be either ``absolute`` or ``relative``.  Default is ``relative``.\n            ``date_from`` and ``date_to`` are required when using ``period_type='absolute'``.\n            ``range_from`` and ``range_to`` are required with ``period_type='relative'``.\n        :param source: Source of snapshot age. Can be ``name``, or ``creation_date``.\n        :param range_from: How many ``unit`` (s) in the past/future is the origin?\n        :param range_to: How many ``unit`` (s) in the past/future is the end point?\n        :param date_from: The simplified date for the start of the range\n        :param date_to: The simplified date for the end of the range.  If this value\n            is the same as ``date_from``, the full value of ``unit`` will be\n            extrapolated for the range.  For example, if ``unit=months``,\n            and ``date_from`` and ``date_to`` are both ``2017.01``, then the entire\n            month of January 2017 will be the absolute date range.\n        :param date_from_format: The :py:func:`time.strftime` string used to parse ``date_from``\n        :param date_to_format: The :py:func:`time.strftime` string used to parse ``date_to``\n        :param timestring: An :py:func:`time.strftime` string to match the datestamp in an\n            snapshot name. Only used for snapshot filtering by ``name``.\n        :param unit: One of ``hours``, ``days``, ``weeks``, ``months``, or ``years``.\n        :param week_starts_on: Either ``sunday`` or ``monday``. Default is ``sunday``\n        :param epoch: An epoch timestamp used to establish a point of reference\n            for calculations. If not provided, the current time will be used.\n        :param exclude: If ``exclude=True``, this filter will remove matching indices from\n            ``indices``. If ``exclude=False``, then only matching indices will be kept in\n            ``indices``. Default is ``False``\n        \"\"\"\n        self.loggit.debug('Filtering snapshots by period')\n        if period_type not in ['absolute', 'relative']:\n            raise ValueError(f'Unacceptable value: {period_type} -- \"period_type\" must be either \"absolute\" or \"relative\".')\n        self.loggit.debug('period_type = %s', period_type)\n        if period_type == 'relative':\n            func = date_range\n            args = [unit, range_from, range_to, epoch]\n            kwgs = {'week_starts_on': week_starts_on}\n            try:\n                range_from = int(range_from)\n                range_to = int(range_to)\n            except ValueError as err:\n                raise ConfigurationError(f'\"range_from\" and \"range_to\" must be integer values. Error: {err}') from err\n        else:\n            func = absolute_date_range\n            args = [unit, date_from, date_to]\n            kwgs = {'date_from_format': date_from_format, 'date_to_format': date_to_format}\n            for reqd in [date_from, date_to, date_from_format, date_to_format]:\n                if not reqd:\n                    raise ConfigurationError('Must provide \"date_from\", \"date_to\", \"date_from_format\", and \"date_to_format\" with absolute period_type')\n        try:\n            start, end = func(*args, **kwgs)\n        except Exception as err:\n            report_failure(err)\n        self._calculate_ages(source=source, timestring=timestring)\n        for snapshot in self.working_list():\n            if not self.snapshot_info[snapshot][self.age_keyfield]:\n                self.loggit.debug('Removing snapshot {0} for having no age')\n                self.snapshots.remove(snapshot)\n                continue\n            age = fix_epoch(self.snapshot_info[snapshot][self.age_keyfield])\n            msg = f'Snapshot \"{snapshot}\" age ({age}), period start: \"{start}\", period end, ({end})'\n            inrange = age >= start and age <= end\n            self.__excludify(inrange, exclude, snapshot, msg)\n\n    def iterate_filters(self, config):\n        \"\"\"\n        Iterate over the filters defined in ``config`` and execute them.\n\n        :param config: A dictionary of filters, as extracted from the YAML configuration file.\n\n        .. note:: ``config`` should be a dictionary with the following form:\n        .. code-block:: python\n\n                { 'filters' : [\n                        {\n                            'filtertype': 'the_filter_type',\n                            'key1' : 'value1',\n                            ...\n                            'keyN' : 'valueN'\n                        }\n                    ]\n                }\n\n        \"\"\"\n        if not 'filters' in config or not config['filters']:\n            self.loggit.info('No filters in config.  Returning unaltered object.')\n            return\n        self.loggit.debug('All filters: %s', config['filters'])\n        for fltr in config['filters']:\n            self.loggit.debug('Top of the loop: %s', self.snapshots)\n            self.loggit.debug('Un-parsed filter args: %s', fltr)\n            filter_result = SchemaCheck(fltr, filterstructure(), 'filter', 'SnapshotList.iterate_filters').result()\n            self.loggit.debug('Parsed filter args: %s', filter_result)\n            method = self.__map_method(fltr['filtertype'])\n            del fltr['filtertype']\n            self.loggit.debug('Filter args: %s', fltr)\n            self.loggit.debug('Pre-instance: %s', self.snapshots)\n            method(**fltr)\n            self.loggit.debug('Post-instance: %s', self.snapshots)",
    "curator/helpers/testers.py": "\"\"\"Utility functions that get things\"\"\"\nimport logging\nfrom voluptuous import Schema\nfrom elasticsearch8 import Elasticsearch\nfrom elasticsearch8.exceptions import NotFoundError\nfrom es_client.helpers.schemacheck import SchemaCheck\nfrom es_client.helpers.utils import prune_nones\nfrom curator.helpers.getters import get_repository, get_write_index\nfrom curator.exceptions import ConfigurationError, MissingArgument, RepositoryException, SearchableSnapshotException\nfrom curator.defaults.settings import index_filtertypes, snapshot_actions, snapshot_filtertypes\nfrom curator.validators import actions, options\nfrom curator.validators.filter_functions import validfilters\nfrom curator.helpers.utils import report_failure\n\ndef has_lifecycle_name(idx_settings):\n    \"\"\"\n    :param idx_settings: The settings for an index being tested\n    :type idx_settings: dict\n\n    :returns: ``True`` if a lifecycle name exists in settings, else ``False``\n    :rtype: bool\n    \"\"\"\n    if 'lifecycle' in idx_settings:\n        if 'name' in idx_settings['lifecycle']:\n            return True\n    return False\n\ndef is_idx_partial(idx_settings):\n    \"\"\"\n    :param idx_settings: The settings for an index being tested\n    :type idx_settings: dict\n\n    :returns: ``True`` if store.snapshot.partial exists in settings, else ``False``\n    :rtype: bool\n    \"\"\"\n    if 'store' in idx_settings:\n        if 'snapshot' in idx_settings['store']:\n            if 'partial' in idx_settings['store']['snapshot']:\n                if idx_settings['store']['snapshot']['partial']:\n                    return True\n                return False\n            return False\n        raise SearchableSnapshotException('Index not a mounted searchable snapshot')\n    raise SearchableSnapshotException('Index not a mounted searchable snapshot')\n\ndef ilm_policy_check(client, alias):\n    \"\"\"Test if alias is associated with an ILM policy\n\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.get_settings`\n\n    :param client: A client connection object\n    :param alias: The alias name\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type alias: str\n    :rtype: bool\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    write_index = get_write_index(client, alias)\n    try:\n        idx_settings = client.indices.get_settings(index=write_index)\n        if 'name' in idx_settings[write_index]['settings']['index']['lifecycle']:\n            return True\n    except KeyError:\n        logger.debug('No ILM policies associated with %s', alias)\n    return False\n\ndef rollable_alias(client, alias):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.get_alias`\n\n    :param client: A client connection object\n    :param alias: An Elasticsearch alias\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type alias: str\n\n\n    :returns: ``True`` or ``False`` depending on whether ``alias`` is an alias that\n        points to an index that can be used by the ``_rollover`` API.\n    :rtype: bool\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    try:\n        response = client.indices.get_alias(name=alias)\n    except NotFoundError:\n        logger.error('Alias \"%s\" not found.', alias)\n        return False\n    for idx in response:\n        if 'is_write_index' in response[idx]['aliases'][alias]:\n            if response[idx]['aliases'][alias]['is_write_index']:\n                return True\n    if len(response) > 1:\n        logger.error('\"alias\" must only reference one index, but points to %s', response)\n        return False\n    index = list(response.keys())[0]\n    rollable = False\n    if index[-2:][1].isdigit():\n        if index[-2:][0].isdigit():\n            rollable = True\n        elif index[-2:][0] == '-':\n            rollable = True\n    return rollable\n\ndef snapshot_running(client):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get_repository`\n\n    Return ``True`` if a snapshot is in progress, and ``False`` if not\n\n    :param client: A client connection object\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n\n    :rtype: bool\n    \"\"\"\n    try:\n        status = client.snapshot.status()['snapshots']\n    except Exception as exc:\n        report_failure(exc)\n    return False if not status else True\n\ndef validate_actions(data):\n    \"\"\"\n    Validate the ``actions`` configuration dictionary, as imported from actions.yml,\n    for example.\n\n    :param data: The configuration dictionary\n\n    :type data: dict\n\n    :returns: The validated and sanitized configuration dictionary.\n    :rtype: dict\n    \"\"\"\n    clean_config = {}\n    root = SchemaCheck(data, actions.root(), 'Actions File', 'root').result()\n    for action_id in root['actions']:\n        action_dict = root['actions'][action_id]\n        loc = f'Action ID \"{action_id}\"'\n        valid_structure = SchemaCheck(action_dict, actions.structure(action_dict, loc), 'structure', loc).result()\n        current_action = valid_structure['action']\n        loc = f'Action ID \"{action_id}\", action \"{current_action}\"'\n        clean_options = SchemaCheck(prune_nones(valid_structure['options']), options.get_schema(current_action), 'options', loc).result()\n        clean_config[action_id] = {'action': current_action, 'description': valid_structure['description'], 'options': clean_options}\n        if current_action == 'alias':\n            add_remove = {}\n            for k in ['add', 'remove']:\n                if k in valid_structure:\n                    current_filters = SchemaCheck(valid_structure[k]['filters'], Schema(validfilters(current_action, location=loc)), f'\"{k}\" filters', f'{loc}, \"filters\"').result()\n                    add_remove.update({k: {'filters': SchemaCheck(current_filters, Schema(validfilters(current_action, location=loc)), 'filters', f'{loc}, \"{k}\", \"filters\"').result()}})\n            clean_config[action_id].update(add_remove)\n        elif current_action in ['cluster_routing', 'create_index', 'rollover']:\n            pass\n        else:\n            valid_filters = SchemaCheck(valid_structure['filters'], Schema(validfilters(current_action, location=loc)), 'filters', f'{loc}, \"filters\"').result()\n            clean_filters = validate_filters(current_action, valid_filters)\n            clean_config[action_id].update({'filters': clean_filters})\n        if current_action == 'reindex':\n            if 'remote_filters' in valid_structure['options']:\n                valid_filters = SchemaCheck(valid_structure['options']['remote_filters'], Schema(validfilters(current_action, location=loc)), 'filters', f'{loc}, \"filters\"').result()\n                clean_remote_filters = validate_filters(current_action, valid_filters)\n                clean_config[action_id]['options'].update({'remote_filters': clean_remote_filters})\n    return {'actions': clean_config}\n\ndef validate_filters(action, myfilters):\n    \"\"\"\n    Validate that myfilters are appropriate for the action type, e.g. no\n    index filters applied to a snapshot list.\n\n    :param action: An action name\n    :param myfilters: A list of filters to test.\n\n    :type action: str\n    :type myfilters: list\n\n    :returns: Validated list of filters\n    :rtype: list\n    \"\"\"\n    if action in snapshot_actions():\n        filtertypes = snapshot_filtertypes()\n    else:\n        filtertypes = index_filtertypes()\n    for fil in myfilters:\n        if fil['filtertype'] not in filtertypes:\n            raise ConfigurationError(f'\"{fil['filtertype']}\" filtertype is not compatible with action \"{action}\"')\n    return myfilters\n\ndef verify_index_list(test):\n    \"\"\"\n    :param test: The variable or object to test\n\n    :type test: :py:class:`~.curator.IndexList`\n\n    :returns: ``None`` if ``test`` is a proper :py:class:`~.curator.indexlist.IndexList`\n        object, else raise a :py:class:`TypeError` exception.\n    :rtype: None\n    \"\"\"\n    from curator.indexlist import IndexList\n    logger = logging.getLogger(__name__)\n    if not isinstance(test, IndexList):\n        msg = f'Not a valid IndexList object. Type: {type(test)} was passed'\n        logger.error(msg)\n        raise TypeError(msg)\n\ndef verify_repository(client, repository=None):\n    \"\"\"\n    Do :py:meth:`~.elasticsearch.snapshot.verify_repository` call. If it fails, raise a\n    :py:exc:`~.curator.exceptions.RepositoryException`.\n\n    :param client: A client connection object\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :param repository: A repository name\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type repository: str\n\n    :rtype: None\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    try:\n        nodes = client.snapshot.verify_repository(name=repository)['nodes']\n        logger.debug('All nodes can write to the repository')\n        logger.debug('Nodes with verified repository access: %s', nodes)\n    except Exception as err:\n        try:\n            if err.status_code == 404:\n                msg = f'--- Repository \"{repository}\" not found. Error: {err.meta.status}, {err.error}'\n            else:\n                msg = f'--- Got a {err.meta.status} response from Elasticsearch.  Error message: {err.error}'\n        except AttributeError:\n            msg = f'--- Error message: {err}'.format()\n        report = f'Failed to verify all nodes have repository access: {msg}'\n        raise RepositoryException(report) from err",
    "curator/helpers/getters.py": "\"\"\"Utility functions that get things\"\"\"\nimport logging\nfrom elasticsearch8 import exceptions as es8exc\nfrom curator.exceptions import ConfigurationError, CuratorException, FailedExecution, MissingArgument\n\ndef byte_size(num, suffix='B'):\n    \"\"\"\n    :param num: The number of byte\n    :param suffix: An arbitrary suffix, like ``Bytes``\n\n    :type num: int\n    :type suffix: str\n\n    :returns: A formatted string indicating the size in bytes, with the proper unit,\n        e.g. KB, MB, GB, TB, etc.\n    :rtype: float\n    \"\"\"\n    for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:\n        if abs(num) < 1024.0:\n            return f'{num:3.1f}{unit}{suffix}'\n        num /= 1024.0\n    return f'{num:.1f}Y{suffix}'\n\ndef escape_dots(stringval):\n    \"\"\"\n    Escape any dots (periods) in ``stringval``.\n\n    Primarily used for ``filter_path`` where dots are indicators of path nesting\n\n    :param stringval: A string, ostensibly an index name\n\n    :type stringval: str\n\n    :returns: ``stringval``, but with any periods escaped with a backslash\n    :retval: str\n    \"\"\"\n    return stringval.replace('.', '\\\\.')\n\ndef get_alias_actions(oldidx, newidx, aliases):\n    \"\"\"\n    :param oldidx: The old index name\n    :param newidx: The new index name\n    :param aliases: The aliases\n\n    :type oldidx: str\n    :type newidx: str\n    :type aliases: dict\n\n    :returns: A list of actions suitable for\n        :py:meth:`~.elasticsearch.client.IndicesClient.update_aliases` ``actions``\n        kwarg.\n    :rtype: list\n    \"\"\"\n    actions = []\n    for alias in aliases.keys():\n        actions.append({'remove': {'index': oldidx, 'alias': alias}})\n        actions.append({'add': {'index': newidx, 'alias': alias}})\n    return actions\n\ndef get_data_tiers(client):\n    \"\"\"\n    Get all valid data tiers from the node roles of each node in the cluster by\n    polling each node\n\n    :param client: A client connection object\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n\n    :returns: The available data tiers in ``tier: bool`` form.\n    :rtype: dict\n    \"\"\"\n\n    def role_check(role, node_info):\n        if role in node_info['roles']:\n            return True\n        return False\n    info = client.nodes.info()['nodes']\n    retval = {'data_hot': False, 'data_warm': False, 'data_cold': False, 'data_frozen': False}\n    for node in info:\n        for role in ['data_hot', 'data_warm', 'data_cold', 'data_frozen']:\n            if role_check(role, info[node]):\n                retval[role] = True\n    return retval\n\ndef get_indices(client, search_pattern='_all'):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.CatClient.indices`\n\n    :param client: A client connection object\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n\n    :returns: The current list of indices from the cluster\n    :rtype: list\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    indices = []\n    try:\n        resp = client.cat.indices(index=search_pattern, expand_wildcards='open,closed', h='index,status', format='json')\n    except Exception as err:\n        raise FailedExecution(f'Failed to get indices. Error: {err}') from err\n    if not resp:\n        return indices\n    for entry in resp:\n        indices.append(entry['index'])\n    logger.debug('All indices: %s', indices)\n    return indices\n\ndef get_snapshot(client, repository=None, snapshot=''):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.SnapshotClient.get`\n\n    :param client: A client connection object\n    :param repository: The Elasticsearch snapshot repository to use\n    :param snapshot: The snapshot name, or a comma-separated list of snapshots\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type repository: str\n    :type snapshot: str\n\n    :returns: Information about the provided ``snapshot``, a snapshot (or a\n        comma-separated list of snapshots). If no snapshot specified, it will\n        collect info for all snapshots.  If none exist, an empty :py:class:`dict`\n        will be returned.\n    :rtype: dict\n    \"\"\"\n    if not repository:\n        raise MissingArgument('No value for \"repository\" provided')\n    snapname = '*' if snapshot == '' else snapshot\n    try:\n        return client.snapshot.get(repository=repository, snapshot=snapshot)\n    except (es8exc.TransportError, es8exc.NotFoundError) as err:\n        msg = f'Unable to get information about snapshot {snapname} from repository: {repository}.  Error: {err}'\n        raise FailedExecution(msg) from err\n\ndef get_tier_preference(client, target_tier='data_frozen'):\n    \"\"\"Do the tier preference thing in reverse order from coldest to hottest\n    Based on the value of ``target_tier``, build out the list to use.\n\n    :param client: A client connection object\n    :param target_tier: The target data tier, e.g. data_warm.\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type target_tier: str\n\n    :returns: A suitable tier preference string in csv format\n    :rtype: str\n    \"\"\"\n    tiermap = {'data_content': 0, 'data_hot': 1, 'data_warm': 2, 'data_cold': 3, 'data_frozen': 4}\n    tiers = get_data_tiers(client)\n    test_list = []\n    for tier in ['data_hot', 'data_warm', 'data_cold', 'data_frozen']:\n        if tier in tiers and tiermap[tier] <= tiermap[target_tier]:\n            test_list.insert(0, tier)\n    if target_tier == 'data_frozen':\n        if 'data_frozen' in tiers and tiers['data_frozen']:\n            return 'data_frozen'\n    preflist = []\n    for key in test_list:\n        if key in tiers and tiers[key]:\n            preflist.append(key)\n    if not preflist:\n        return 'data_content'\n    return ','.join(preflist)\n\ndef get_write_index(client, alias):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.get_alias`\n\n    :param client: A client connection object\n    :param alias: An alias name\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type alias: str\n\n    :returns: The the index name associated with the alias that is designated\n        ``is_write_index``\n    :rtype: str\n    \"\"\"\n    try:\n        response = client.indices.get_alias(index=alias)\n    except Exception as exc:\n        raise CuratorException(f'Alias {alias} not found') from exc\n    retval = None\n    if len(list(response.keys())) > 1:\n        for index in list(response.keys()):\n            try:\n                if response[index]['aliases'][alias]['is_write_index']:\n                    retval = index\n            except KeyError as exc:\n                raise FailedExecution('Invalid alias: is_write_index not found in 1 to many alias') from exc\n    else:\n        retval = list(response.keys())[0]\n    return retval\n\ndef index_size(client, idx, value='total'):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.stats`\n\n    :param client: A client connection object\n    :param idx: An index name\n    :param value: One of either ``primaries`` or ``total``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type idx: str\n    :type value: str\n\n    :returns: The sum of either ``primaries`` or ``total`` shards for index ``idx``\n    :rtype: integer\n    \"\"\"\n    fpath = f'indices.{escape_dots(idx)}.{value}.store.size_in_bytes'\n    return client.indices.stats(index=idx, filter_path=fpath)['indices'][idx][value]['store']['size_in_bytes']\n\ndef meta_getter(client, idx, get=None):\n    \"\"\"Meta Getter\n    Calls :py:meth:`~.elasticsearch.client.IndicesClient.get_settings` or\n    :py:meth:`~.elasticsearch.client.IndicesClient.get_alias`\n\n    :param client: A client connection object\n    :param idx: An Elasticsearch index\n    :param get: The kind of get to perform, e.g. settings or alias\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type idx: str\n    :type get: str\n\n    :returns: The settings from the get call to the named index\n    :rtype: dict\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    acceptable = ['settings', 'alias']\n    if not get:\n        raise ConfigurationError('\"get\" can not be a NoneType')\n    if get not in acceptable:\n        raise ConfigurationError(f'\"get\" must be one of {acceptable}')\n    retval = {}\n    try:\n        if get == 'settings':\n            retval = client.indices.get_settings(index=idx)[idx]['settings']['index']\n        elif get == 'alias':\n            retval = client.indices.get_alias(index=idx)[idx]['aliases']\n    except es8exc.NotFoundError as missing:\n        logger.error('Index %s was not found!', idx)\n        raise es8exc.NotFoundError from missing\n    except KeyError as err:\n        logger.error('Key not found: %s', err)\n        raise KeyError from err\n    except Exception as exc:\n        logger.error('Exception encountered: %s', exc)\n    return retval\n\ndef name_to_node_id(client, name):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.NodesClient.info`\n\n    :param client: A client connection object\n    :param name: The node ``name``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type name: str\n\n    :returns: The node_id of the node identified by ``name``\n    :rtype: str\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    fpath = 'nodes'\n    info = client.nodes.info(filter_path=fpath)\n    for node in info['nodes']:\n        if info['nodes'][node]['name'] == name:\n            logger.debug('Found node_id \"%s\" for name \"%s\".', node, name)\n            return node\n    logger.error('No node_id found matching name: \"%s\"', name)\n    return None\n\ndef node_id_to_name(client, node_id):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.NodesClient.info`\n\n    :param client: A client connection object\n    :param node_id: The node ``node_id``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type node_id: str\n\n    :returns: The name of the node identified by ``node_id``\n    :rtype: str\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    fpath = f'nodes.{node_id}.name'\n    info = client.nodes.info(filter_path=fpath)\n    name = None\n    if node_id in info['nodes']:\n        name = info['nodes'][node_id]['name']\n    else:\n        logger.error('No node_id found matching: \"%s\"', node_id)\n    logger.debug('Name associated with node_id \"%s\": %s', node_id, name)\n    return name\n\ndef node_roles(client, node_id):\n    \"\"\"\n    Calls :py:meth:`~.elasticsearch.client.NodesClient.info`\n\n    :param client: A client connection object\n    :param node_id: The node ``node_id``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type node_id: str\n\n    :returns: The list of roles assigned to the node identified by ``node_id``\n    :rtype: list\n    \"\"\"\n    fpath = f'nodes.{node_id}.roles'\n    return client.nodes.info(filter_path=fpath)['nodes'][node_id]['roles']\n\ndef single_data_path(client, node_id):\n    \"\"\"\n    In order for a shrink to work, it should be on a single filesystem, as shards\n    cannot span filesystems. Calls :py:meth:`~.elasticsearch.client.NodesClient.stats`\n\n    :param client: A client connection object\n    :param node_id: The node ``node_id``\n\n    :type client: :py:class:`~.elasticsearch.Elasticsearch`\n    :type node_id: str\n\n    :returns: ``True`` if the node has a single filesystem, else ``False``\n    :rtype: bool\n    \"\"\"\n    fpath = f'nodes.{node_id}.fs.data'\n    response = client.nodes.stats(filter_path=fpath)\n    return len(response['nodes'][node_id]['fs']['data']) == 1",
    "curator/helpers/utils.py": "\"\"\"Helper utilities\n\nThe kind that don't fit in testers, getters, date_ops, or converters\n\"\"\"\nimport logging\nfrom es_client.helpers.utils import ensure_list\nfrom curator.exceptions import FailedExecution\n\ndef chunk_index_list(indices):\n    \"\"\"\n    This utility chunks very large index lists into 3KB chunks.\n    It measures the size as a csv string, then converts back into a list for the return value.\n\n    :param indices: The list of indices\n\n    :type indices: list\n\n    :returns: A list of lists (each a piece of the original ``indices``)\n    :rtype: list\n    \"\"\"\n    chunks = []\n    chunk = ''\n    for index in indices:\n        if len(chunk) < 3072:\n            if not chunk:\n                chunk = index\n            else:\n                chunk += ',' + index\n        else:\n            chunks.append(chunk.split(','))\n            chunk = index\n    chunks.append(chunk.split(','))\n    return chunks\n\ndef show_dry_run(ilo, action, **kwargs):\n    \"\"\"\n    Log dry run output with the action which would have been executed.\n\n    :param ilo: An IndexList Object\n    :param action: The ``action`` to be performed.\n    :param kwargs: Any other args to show in the log output\n\n\n    :type ilo: :py:class:`~.curator.indexlist.IndexList`\n    :type action: str\n    :type kwargs: dict\n\n    :rtype: None\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    logger.info('DRY-RUN MODE.  No changes will be made.')\n    msg = f'(CLOSED) indices may be shown that may not be acted on by action \"{action}\".'\n    logger.info(msg)\n    indices = sorted(ilo.indices)\n    for idx in indices:\n        try:\n            index_closed = ilo.index_info[idx]['state'] == 'close'\n        except KeyError:\n            ilo.get_index_state()\n            index_closed = ilo.index_info[idx]['state'] == 'close'\n        var = ' (CLOSED)' if index_closed else ''\n        msg = f'DRY-RUN: {action}: {idx}{var} with arguments: {kwargs}'\n        logger.info(msg)\n\ndef to_csv(indices):\n    \"\"\"\n    :param indices: A list of indices to act on, or a single value, which could be\n        in the format of a csv string already.\n\n    :type indices: list\n\n    :returns: A csv string from a list of indices, or a single value if only one value is present\n    :rtype: str\n    \"\"\"\n    indices = ensure_list(indices)\n    if indices:\n        return ','.join(sorted(indices))\n    return None",
    "curator/actions/snapshot.py": "\"\"\"Snapshot and Restore action classes\"\"\"\nimport logging\nimport re\nfrom es_client.helpers.utils import ensure_list\nfrom curator.helpers.date_ops import parse_datemath, parse_date_pattern\nfrom curator.helpers.getters import get_indices\nfrom curator.helpers.testers import repository_exists, snapshot_running, verify_index_list, verify_repository, verify_snapshot_list\nfrom curator.helpers.utils import report_failure, to_csv\nfrom curator.helpers.waiters import wait_for_it\nfrom curator.exceptions import ActionError, CuratorException, FailedRestore, FailedSnapshot, MissingArgument, SnapshotInProgress\n\nclass Snapshot(object):\n    \"\"\"Snapshot Action Class\n\n    Read more about identically named settings at:\n    :py:meth:`elasticsearch.client.SnapshotClient.create`\n    \"\"\"\n\n    def __init__(self, ilo, repository=None, name=None, ignore_unavailable=False, include_global_state=True, partial=False, wait_for_completion=True, wait_interval=9, max_wait=-1, skip_repo_fs_check=True):\n        \"\"\"\n        :param ilo: An IndexList Object\n        :param repository: Repository name.\n        :param name: Snapshot name.\n        :param ignore_unavailable: Ignore unavailable shards/indices.\n        :param include_global_state: Store cluster global state with snapshot.\n        :param partial: Do not fail if primary shard is unavailable.\n        :param wait_for_completion: Wait for completion before returning.\n        :param wait_interval: Seconds to wait between completion checks.\n        :param max_wait: Maximum number of seconds to ``wait_for_completion``\n        :param skip_repo_fs_check: Do not validate write access to repository on all cluster nodes\n            before proceeding. Useful for shared filesystems where intermittent timeouts can affect\n            validation, but won't likely affect snapshot success. (Default: ``True``)\n\n        :type ilo: :py:class:`~.curator.indexlist.IndexList`\n        :type repository: str\n        :type name: str\n        :type ignore_unavailable: bool\n        :type include_global_state: bool\n        :type partial: bool\n        :type wait_for_completion: bool\n        :type wait_interval: int\n        :type max_wait: int\n        :type skip_repo_fs_check: bool\n        \"\"\"\n        verify_index_list(ilo)\n        ilo.empty_list_check()\n        if not repository_exists(ilo.client, repository=repository):\n            raise ActionError(f'Cannot snapshot indices to missing repository: {repository}')\n        if not name:\n            raise MissingArgument('No value for \"name\" provided.')\n        self.index_list = ilo\n        self.client = ilo.client\n        self.name = parse_datemath(self.client, parse_date_pattern(name))\n        self.repository = repository\n        self.wait_for_completion = wait_for_completion\n        self.wait_interval = wait_interval\n        self.max_wait = max_wait\n        self.skip_repo_fs_check = skip_repo_fs_check\n        self.state = None\n        self.indices = to_csv(ilo.indices)\n        self.ignore_unavailable = ignore_unavailable\n        self.include_global_state = include_global_state\n        self.partial = partial\n        self.settings = {'indices': ilo.indices, 'ignore_unavailable': self.ignore_unavailable, 'include_global_state': self.include_global_state, 'partial': self.partial}\n        self.loggit = logging.getLogger('curator.actions.snapshot')\n\n    def get_state(self):\n        \"\"\"Get the state of the snapshot and set :py:attr:`state`\"\"\"\n        try:\n            self.state = self.client.snapshot.get(repository=self.repository, snapshot=self.name)['snapshots'][0]['state']\n            return self.state\n        except IndexError as exc:\n            raise CuratorException(f'Snapshot \"{self.name}\" not found in repository \"{self.repository}\"') from exc\n\n    def report_state(self):\n        \"\"\"\n        Log the :py:attr:`state` of the snapshot and raise :py:exc:`FailedSnapshot` if\n        :py:attr:`state` is not ``SUCCESS``\n        \"\"\"\n        self.get_state()\n        if self.state == 'SUCCESS':\n            self.loggit.info('Snapshot %s successfully completed.', self.name)\n        else:\n            msg = f'Snapshot {self.name} completed with state: {self.state}'\n            self.loggit.error(msg)\n            raise FailedSnapshot(msg)\n\n    def do_dry_run(self):\n        \"\"\"Log what the output would be, but take no action.\"\"\"\n        self.loggit.info('DRY-RUN MODE.  No changes will be made.')\n        msg = f'DRY-RUN: snapshot: {self.name} in repository {self.repository} with arguments: {self.settings}'\n        self.loggit.info(msg)\n\n    def do_action(self):\n        \"\"\"\n        :py:meth:`elasticsearch.client.SnapshotClient.create` a snapshot of :py:attr:`indices`,\n        with passed parameters.\n        \"\"\"\n        if not self.skip_repo_fs_check:\n            verify_repository(self.client, self.repository)\n        if snapshot_running(self.client):\n            raise SnapshotInProgress('Snapshot already in progress.')\n        try:\n            self.loggit.info('Creating snapshot \"%s\" from indices: %s', self.name, self.index_list.indices)\n            self.client.snapshot.create(repository=self.repository, snapshot=self.name, ignore_unavailable=self.ignore_unavailable, include_global_state=self.include_global_state, indices=self.indices, partial=self.partial, wait_for_completion=False)\n            if self.wait_for_completion:\n                wait_for_it(self.client, 'snapshot', snapshot=self.name, repository=self.repository, wait_interval=self.wait_interval, max_wait=self.max_wait)\n                self.report_state()\n            else:\n                msg = f'\"wait_for_completion\" set to {self.wait_for_completion}. Remember to check for successful completion manually.'\n                self.loggit.warning(msg)\n        except Exception as err:\n            report_failure(err)\n\nclass DeleteSnapshots:\n    \"\"\"Delete Snapshots Action Class\"\"\"\n\nclass Restore(object):\n    \"\"\"Restore Action Class\n\n    Read more about identically named settings at:\n    :py:meth:`elasticsearch.client.SnapshotClient.restore`\n    \"\"\"\n\n    def __init__(self, slo, name=None, indices=None, include_aliases=False, ignore_unavailable=False, include_global_state=False, partial=False, rename_pattern=None, rename_replacement=None, extra_settings=None, wait_for_completion=True, wait_interval=9, max_wait=-1, skip_repo_fs_check=True):\n        \"\"\"\n        :param slo: A SnapshotList object\n        :param name: Name of the snapshot to restore.  If ``None``, use the most recent snapshot.\n        :param indices: Indices to restore.  If ``None``, all in the snapshot will be restored.\n        :param include_aliases: Restore aliases with the indices.\n        :param ignore_unavailable: Ignore unavailable shards/indices.\n        :param include_global_state: Restore cluster global state with snapshot.\n        :param partial: Do not fail if primary shard is unavailable.\n        :param rename_pattern: A regular expression pattern with one or more captures, e.g.\n            ``index_(.+)``\n        :param rename_replacement: A target index name pattern with `$#` numbered references to the\n            captures in ``rename_pattern``, e.g. ``restored_index_$1``\n        :param extra_settings: Index settings to apply to restored indices.\n        :param wait_for_completion: Wait for completion before returning.\n        :param wait_interval: Seconds to wait between completion checks.\n        :param max_wait: Maximum number of seconds to ``wait_for_completion``\n        :param skip_repo_fs_check: Do not validate write access to repository on all cluster nodes\n            before proceeding. Useful for shared filesystems where intermittent timeouts can affect\n            validation, but won't likely affect snapshot success. (Default: ``True``)\n\n        :type slo: :py:class:`~.curator.snapshotlist.SnapshotList`\n        :type name: str\n        :type indices: list\n        :type include_aliases: bool\n        :type ignore_unavailable: bool\n        :type include_global_state: bool\n        :type partial: bool\n        :type rename_pattern: str\n        :type rename_replacement: str\n        :type extra_settings: dict\n        :type wait_for_completion: bool\n        :type wait_interval: int\n        :type max_wait: int\n        :type skip_repo_fs_check: bool\n        \"\"\"\n        if extra_settings is None:\n            extra_settings = {}\n        self.loggit = logging.getLogger('curator.actions.snapshot')\n        verify_snapshot_list(slo)\n        most_recent = slo.most_recent()\n        self.loggit.debug('\"most_recent\" snapshot: %s', most_recent)\n        self.name = name if name else most_recent\n        if slo.snapshot_info[self.name]['state'] == 'PARTIAL' and partial:\n            self.loggit.warning('Performing restore of snapshot in state PARTIAL.')\n        elif slo.snapshot_info[self.name]['state'] != 'SUCCESS':\n            raise CuratorException('Restore operation can only be performed on snapshots with state \"SUCCESS\", or \"PARTIAL\" if partial=True.')\n        self.snapshot_list = slo\n        self.client = slo.client\n        self.repository = slo.repository\n        if indices:\n            self.indices = ensure_list(indices)\n        else:\n            self.indices = slo.snapshot_info[self.name]['indices']\n        self.wfc = wait_for_completion\n        self.wait_interval = wait_interval\n        self.max_wait = max_wait\n        self.rename_pattern = rename_pattern if rename_replacement is not None else ''\n        self.rename_replacement = rename_replacement if rename_replacement is not None else ''\n        self.py_rename_replacement = self.rename_replacement.replace('$', '\\\\')\n        self.skip_repo_fs_check = skip_repo_fs_check\n        self.body = {'indices': self.indices, 'include_aliases': include_aliases, 'ignore_unavailable': ignore_unavailable, 'include_global_state': include_global_state, 'partial': partial, 'rename_pattern': self.rename_pattern, 'rename_replacement': self.rename_replacement}\n        self.include_aliases = include_aliases\n        self.ignore_unavailable = ignore_unavailable\n        self.include_global_state = include_global_state\n        self.include_aliases = include_aliases\n        self.partial = partial\n        self.index_settings = None\n        if extra_settings:\n            self.loggit.debug('Adding extra_settings to restore body: %s', extra_settings)\n            self.index_settings = extra_settings\n            try:\n                self.body.update(extra_settings)\n            except Exception:\n                self.loggit.error('Unable to apply extra settings to restore body')\n        self.loggit.debug('REPOSITORY: %s', self.repository)\n        self.loggit.debug('WAIT_FOR_COMPLETION: %s', self.wfc)\n        self.loggit.debug('SKIP_REPO_FS_CHECK: %s', self.skip_repo_fs_check)\n        self.loggit.debug('BODY: %s', self.body)\n        self._get_expected_output()\n\n    def _get_expected_output(self):\n        if not self.rename_pattern and (not self.rename_replacement):\n            self.expected_output = self.indices\n            return\n        self.expected_output = []\n        for index in self.indices:\n            self.expected_output.append(re.sub(self.rename_pattern, self.py_rename_replacement, index))\n            msg = f'index: {index} replacement: {self.expected_output[-1]}'\n            self.loggit.debug(msg)\n\n    def report_state(self):\n        \"\"\"\n        Log the state of the restore. This should only be done if ``wait_for_completion`` is\n        ``True``, and only after completing the restore.\n        \"\"\"\n        all_indices = get_indices(self.client)\n        found_count = 0\n        missing = []\n        for index in self.expected_output:\n            if index in all_indices:\n                found_count += 1\n                self.loggit.info('Found restored index %s', index)\n            else:\n                missing.append(index)\n        if found_count == len(self.expected_output):\n            self.loggit.info('All indices appear to have been restored.')\n        else:\n            msg = f'Some of the indices do not appear to have been restored. Missing: {missing}'\n            self.loggit.error(msg)\n            raise FailedRestore(msg)\n\n    def do_dry_run(self):\n        \"\"\"Log what the output would be, but take no action.\"\"\"\n        self.loggit.info('DRY-RUN MODE.  No changes will be made.')\n        args = {'wait_for_completion': self.wfc, 'body': self.body}\n        msg = f'DRY-RUN: restore: Repository: {self.repository} Snapshot name: {self.name} Arguments: {args}'\n        self.loggit.info(msg)\n        for index in self.indices:\n            if self.rename_pattern and self.rename_replacement:\n                rmsg = f'as {re.sub(self.rename_pattern, self.py_rename_replacement, index)}'\n            else:\n                rmsg = ''\n            self.loggit.info('DRY-RUN: restore: Index %s %s', index, rmsg)\n\n    def do_action(self):\n        \"\"\"\n        :py:meth:`~.elasticsearch.client.SnapshotClient.restore` :py:attr:`indices` from\n        :py:attr:`name` with passed params.\n        \"\"\"\n        if not self.skip_repo_fs_check:\n            verify_repository(self.client, self.repository)\n        if snapshot_running(self.client):\n            raise SnapshotInProgress('Cannot restore while a snapshot is in progress.')\n        try:\n            self.loggit.info('Restoring indices \"%s\" from snapshot: %s', self.indices, self.name)\n            self.client.snapshot.restore(repository=self.repository, snapshot=self.name, ignore_index_settings=None, ignore_unavailable=self.ignore_unavailable, include_aliases=self.include_aliases, include_global_state=self.include_global_state, index_settings=self.index_settings, indices=self.indices, partial=self.partial, rename_pattern=self.rename_pattern, rename_replacement=self.rename_replacement, wait_for_completion=False)\n            if self.wfc:\n                wait_for_it(self.client, 'restore', index_list=self.expected_output, wait_interval=self.wait_interval, max_wait=self.max_wait)\n                self.report_state()\n            else:\n                msg = f'\"wait_for_completion\" set to {self.wfc}. Remember to check for successful completion manually.'\n                self.loggit.warning(msg)\n        except Exception as err:\n            report_failure(err)",
    "curator/exceptions.py": "\"\"\"Curator Exceptions\"\"\"\n\nclass CuratorException(Exception):\n    \"\"\"\n    Base class for all exceptions raised by Curator which are not Elasticsearch\n    exceptions.\n    \"\"\"\n\nclass ConfigurationError(CuratorException):\n    \"\"\"\n    Exception raised when a misconfiguration is detected\n    \"\"\"\n\nclass MissingArgument(CuratorException):\n    \"\"\"\n    Exception raised when a needed argument is not passed.\n    \"\"\"\n\nclass NoIndices(CuratorException):\n    \"\"\"\n    Exception raised when an operation is attempted against an empty index_list\n    \"\"\"\n\nclass NoSnapshots(CuratorException):\n    \"\"\"\n    Exception raised when an operation is attempted against an empty snapshot_list\n    \"\"\"\n\nclass ActionError(CuratorException):\n    \"\"\"\n    Exception raised when an action (against an index_list or snapshot_list) cannot be taken.\n    \"\"\"\n\nclass FailedExecution(CuratorException):\n    \"\"\"\n    Exception raised when an action fails to execute for some reason.\n    \"\"\"\n\nclass SnapshotInProgress(ActionError):\n    \"\"\"\n    Exception raised when a snapshot is already in progress\n    \"\"\"\n\nclass ActionTimeout(CuratorException):\n    \"\"\"\n    Exception raised when an action fails to complete in the allotted time\n    \"\"\"\n\nclass FailedSnapshot(CuratorException):\n    \"\"\"\n    Exception raised when a snapshot does not complete with state SUCCESS\n    \"\"\"\n\nclass FailedRestore(CuratorException):\n    \"\"\"\n    Exception raised when a Snapshot Restore does not restore all selected indices\n    \"\"\"\n\nclass FailedReindex(CuratorException):\n    \"\"\"\n    Exception raised when failures are found in the reindex task response\n    \"\"\"\n\nclass ClientException(CuratorException):\n    \"\"\"\n    Exception raised when the Elasticsearch client and/or connection is the source of the problem.\n    \"\"\"\n\nclass LoggingException(CuratorException):\n    \"\"\"\n    Exception raised when Curator cannot either log or configure logging\n    \"\"\"\n\nclass RepositoryException(CuratorException):\n    \"\"\"\n    Exception raised when Curator cannot verify a snapshot repository\n    \"\"\"\n\nclass SearchableSnapshotException(CuratorException):\n    \"\"\"\n    Exception raised when Curator finds something out of order with a Searchable Snapshot\n    \"\"\""
  }
}