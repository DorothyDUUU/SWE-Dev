{
  "dir_path": "/app/skforecast",
  "package_name": "skforecast",
  "sample_name": "skforecast-test_onestepaheadfold",
  "src_dir": "skforecast/",
  "test_dir": "tests/",
  "test_file": "skforecast/model_selection/tests/tests_split/test_onestepaheadfold.py",
  "test_code": "# Unit test OneStepAheadFold\n# ==============================================================================\nimport pytest\nimport numpy as np\nimport pandas as pd\nfrom skforecast.model_selection._split import OneStepAheadFold\n\n\ndef test_OneStepAheadFold_split_raise_error_when_X_is_not_series_dataframe_or_dict():\n    \"\"\"\n    Test that ValueError is raised when X is not a pd.Series, pd.DataFrame or dict.\n    \"\"\"\n    X = np.arange(100)\n    cv = OneStepAheadFold(initial_train_size=70)\n    \n    msg = (\n        f\"X must be a pandas Series, DataFrame, Index or a dictionary. \"\n        f\"Got {type(X)}.\"\n    )\n    with pytest.raises(TypeError, match=msg):\n        cv.split(X=X)\n\n\n@pytest.mark.parametrize(\"return_all_indexes, expected\",\n                         [(True, [[range(0, 70)], [range(70, 100)], True]),\n                          (False, [[0, 70], [70, 100], True])], \n                         ids = lambda argument: f'{argument}')\ndef test_OneStepAhead_split_initial_train_size_and_window_size(capfd, return_all_indexes, expected):\n    \"\"\"\n    Test OneStepAhead splits when initial_train_size and window_size are provided.\n    \"\"\"\n    y = pd.Series(np.arange(100))\n    y.index = pd.date_range(start='2022-01-01', periods=100, freq='D')\n    cv = OneStepAheadFold(\n            initial_train_size = 70,\n            window_size        = 3,\n            differentiation    = None,\n            return_all_indexes = return_all_indexes,\n        )\n    folds = cv.split(X=y)\n    out, _ = capfd.readouterr()\n    expected_out = (\n        \"Information of folds\\n\"\n        \"--------------------\\n\"\n        \"Number of observations in train: 70\\n\"\n        \"Number of observations in test: 30\\n\"\n        \"Training : 2022-01-01 00:00:00 -- 2022-03-12 00:00:00 (n=70)\\n\"\n        \"Test     : 2022-03-12 00:00:00 -- 2022-04-10 00:00:00 (n=30)\\n\\n\"\n    )\n\n    assert out == expected_out\n    assert folds == expected\n\n\ndef test_OneStepAhead_split_initial_train_size_and_window_size_diferentiation_is_1(capfd):\n    \"\"\"\n    Test OneStepAhead splits when initial_train_size and window_size are provided, and\n    differentiation is 1.\n    \"\"\"\n    y = pd.Series(np.arange(100))\n    y.index = pd.date_range(start='2022-01-01', periods=100, freq='D')\n    cv = OneStepAheadFold(\n            initial_train_size = 70,\n            window_size        = 3,\n            differentiation    = 1,\n            return_all_indexes = False,\n        )\n    folds = cv.split(X=y)\n    out, _ = capfd.readouterr()\n    expected_folds = [[0, 70], [70, 100], True]\n    expected_out = (\n        \"Information of folds\\n\"\n        \"--------------------\\n\"\n        \"Number of observations in train: 69\\n\"\n        \"    First 1 observation/s in training set are used for differentiation\\n\"\n        \"Number of observations in test: 30\\n\"\n        \"Training : 2022-01-02 00:00:00 -- 2022-03-12 00:00:00 (n=69)\\n\"\n        \"Test     : 2022-03-12 00:00:00 -- 2022-04-10 00:00:00 (n=30)\\n\\n\"\n    )\n\n    assert out == expected_out\n    assert folds == expected_folds\n\n\ndef test_OneStepAhead_split_initial_train_size_window_size_return_all_indexes_true_as_pandas_true(capfd):\n    \"\"\"\n    Test OneStepAhead splits when initial_train_size and window_size are provided, output as\n    pandas DataFrame.\n    \"\"\"\n    y = pd.Series(np.arange(100))\n    y.index = pd.date_range(start='2022-01-01', periods=100, freq='D')\n    cv = OneStepAheadFold(\n            initial_train_size = 70,\n            window_size        = 3,\n            differentiation    = None,\n            return_all_indexes = True,\n        )\n    folds = cv.split(X=y, as_pandas=True)\n    out, _ = capfd.readouterr()\n    expected_folds = pd.DataFrame(\n        {\n            \"fold\": {0: 0},\n            \"train_index\": {0: [range(0, 70)]},\n            \"test_index\": {0: [range(70, 100)]},\n            \"fit_forecaster\": {0: True},\n        }\n    )\n    expected_out = (\n        \"Information of folds\\n\"\n        \"--------------------\\n\"\n        \"Number of observations in train: 70\\n\"\n        \"Number of observations in test: 30\\n\"\n        \"Training : 2022-01-01 00:00:00 -- 2022-03-12 00:00:00 (n=70)\\n\"\n        \"Test     : 2022-03-12 00:00:00 -- 2022-04-10 00:00:00 (n=30)\\n\\n\"\n    )\n\n    assert out == expected_out\n    pd.testing.assert_frame_equal(folds, expected_folds)\n\n\ndef test_OneStepAhead_split_initial_train_size_window_size_return_all_indexes_false_as_pandas_true(capfd):\n    \"\"\"\n    Test OneStepAhead splits when initial_train_size and window_size are provided, output as\n    pandas DataFrame.\n    \"\"\"\n    y = pd.Series(np.arange(100))\n    y.index = pd.date_range(start='2022-01-01', periods=100, freq='D')\n    cv = OneStepAheadFold(\n            initial_train_size = 70,\n            window_size        = 3,\n            differentiation    = None,\n            return_all_indexes = False,\n        )\n    folds = cv.split(X=y, as_pandas=True)\n    out, _ = capfd.readouterr()\n    expected_folds = pd.DataFrame(\n        {\n            'fold': [0],\n            'train_start': [0],\n            'train_end': [70],\n            'test_start': [70],\n            'test_end': [100],\n            'fit_forecaster': [True]\n        }\n    )\n    expected_out = (\n        \"Information of folds\\n\"\n        \"--------------------\\n\"\n        \"Number of observations in train: 70\\n\"\n        \"Number of observations in test: 30\\n\"\n        \"Training : 2022-01-01 00:00:00 -- 2022-03-12 00:00:00 (n=70)\\n\"\n        \"Test     : 2022-03-12 00:00:00 -- 2022-04-10 00:00:00 (n=30)\\n\\n\"\n    )\n\n    assert out == expected_out\n    pd.testing.assert_frame_equal(folds, expected_folds)\n",
  "GT_file_code": {
    "skforecast/model_selection/_split.py": "################################################################################\n#                     skforecast.model_selection._split                        #\n#                                                                              #\n# This work by skforecast team is licensed under the BSD 3-Clause License.     #\n################################################################################\n# coding=utf-8\n\nfrom copy import deepcopy\nfrom typing import Union, Optional, Any\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport itertools\nfrom ..exceptions import IgnoredArgumentWarning\n\n\nclass BaseFold():\n    \"\"\"\n    Base class for all Fold classes in skforecast. All fold classes should specify\n    all the parameters that can be set at the class level in their ``__init__``.\n\n    Parameters\n    ----------\n    steps : int, default `None`\n        Number of observations used to be predicted in each fold. This is also commonly\n        referred to as the forecast horizon or test size.\n    initial_train_size : int, default `None`\n        Number of observations used for initial training.\n    window_size : int, default `None`\n        Number of observations needed to generate the autoregressive predictors.\n    differentiation : int, default `None`\n        Number of observations to use for differentiation. This is used to extend the\n        `last_window` as many observations as the differentiation order.\n    refit : bool, int, default `False`\n        Whether to refit the forecaster in each fold.\n\n        - If `True`, the forecaster is refitted in each fold.\n        - If `False`, the forecaster is trained only in the first fold.\n        - If an integer, the forecaster is trained in the first fold and then refitted\n          every `refit` folds.\n    fixed_train_size : bool, default `True`\n        Whether the training size is fixed or increases in each fold.\n    gap : int, default `0`\n        Number of observations between the end of the training set and the start of the\n        test set.\n    skip_folds : int, list, default `None`\n        Number of folds to skip.\n\n        - If an integer, every 'skip_folds'-th is returned.\n        - If a list, the indexes of the folds to skip.\n\n        For example, if `skip_folds=3` and there are 10 folds, the returned folds are\n        0, 3, 6, and 9. If `skip_folds=[1, 2, 3]`, the returned folds are 0, 4, 5, 6, 7,\n        8, and 9.\n    allow_incomplete_fold : bool, default `True`\n        Whether to allow the last fold to include fewer observations than `steps`.\n        If `False`, the last fold is excluded if it is incomplete.\n    return_all_indexes : bool, default `False`\n        Whether to return all indexes or only the start and end indexes of each fold.\n    verbose : bool, default `True`\n        Whether to print information about generated folds.\n\n    Attributes\n    ----------\n    steps : int\n        Number of observations used to be predicted in each fold. This is also commonly\n        referred to as the forecast horizon or test size.\n    initial_train_size : int\n        Number of observations used for initial training.\n    window_size : int\n        Number of observations needed to generate the autoregressive predictors.\n    differentiation : int\n        Number of observations to use for differentiation. This is used to extend the\n        `last_window` as many observations as the differentiation order.\n    refit : bool, int\n        Whether to refit the forecaster in each fold.\n    fixed_train_size : bool\n        Whether the training size is fixed or increases in each fold.\n    gap : int\n        Number of observations between the end of the training set and the start of the\n        test set.\n    skip_folds : int, list\n        Number of folds to skip.\n    allow_incomplete_fold : bool\n        Whether to allow the last fold to include fewer observations than `steps`.\n    return_all_indexes : bool\n        Whether to return all indexes or only the start and end indexes of each fold.\n    verbose : bool\n        Whether to print information about generated folds.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        steps: Optional[int] = None,\n        initial_train_size: Optional[int] = None,\n        window_size: Optional[int] = None,\n        differentiation: Optional[int] = None,\n        refit: Union[bool, int] = False,\n        fixed_train_size: bool = True,\n        gap: int = 0,\n        skip_folds: Optional[Union[int, list]] = None,\n        allow_incomplete_fold: bool = True,\n        return_all_indexes: bool = False,\n        verbose: bool = True\n    ) -> None:\n\n        self._validate_params(\n            cv_name               = type(self).__name__,\n            steps                 = steps,\n            initial_train_size    = initial_train_size,\n            window_size           = window_size,\n            differentiation       = differentiation,\n            refit                 = refit,\n            fixed_train_size      = fixed_train_size,\n            gap                   = gap,\n            skip_folds            = skip_folds,\n            allow_incomplete_fold = allow_incomplete_fold,\n            return_all_indexes    = return_all_indexes,\n            verbose               = verbose\n        )\n\n        self.steps                 = steps\n        self.initial_train_size    = initial_train_size\n        self.window_size           = window_size\n        self.differentiation       = differentiation\n        self.refit                 = refit\n        self.fixed_train_size      = fixed_train_size\n        self.gap                   = gap\n        self.skip_folds            = skip_folds\n        self.allow_incomplete_fold = allow_incomplete_fold\n        self.return_all_indexes    = return_all_indexes\n        self.verbose               = verbose\n\n    def _validate_params(\n        self,\n        cv_name: str,\n        steps: Optional[int] = None,\n        initial_train_size: Optional[int] = None,\n        window_size: Optional[int] = None,\n        differentiation: Optional[int] = None,\n        refit: Union[bool, int] = False,\n        fixed_train_size: bool = True,\n        gap: int = 0,\n        skip_folds: Optional[Union[int, list]] = None,\n        allow_incomplete_fold: bool = True,\n        return_all_indexes: bool = False,\n        verbose: bool = True\n    ) -> None: \n        \"\"\"\n        Validate all input parameters to ensure correctness.\n        \"\"\"\n\n        if cv_name == \"TimeSeriesFold\":\n            if not isinstance(steps, (int, np.integer)) or steps < 1:\n                raise ValueError(\n                    f\"`steps` must be an integer greater than 0. Got {steps}.\"\n                )\n            if not isinstance(initial_train_size, (int, np.integer, type(None))):\n                raise ValueError(\n                    f\"`initial_train_size` must be an integer greater than 0 or None. \"\n                    f\"Got {initial_train_size}.\"\n                )\n            if initial_train_size is not None and initial_train_size < 1:\n                raise ValueError(\n                    f\"`initial_train_size` must be an integer greater than 0 or None. \"\n                    f\"Got {initial_train_size}.\"\n                )\n            if not isinstance(refit, (bool, int, np.integer)):\n                raise TypeError(\n                    f\"`refit` must be a boolean or an integer equal or greater than 0. \"\n                    f\"Got {refit}.\"\n                )\n            if isinstance(refit, (int, np.integer)) and not isinstance(refit, bool) and refit < 0:\n                raise TypeError(\n                    f\"`refit` must be a boolean or an integer equal or greater than 0. \"\n                    f\"Got {refit}.\"\n                )\n            if not isinstance(fixed_train_size, bool):\n                raise TypeError(\n                    f\"`fixed_train_size` must be a boolean: `True`, `False`. \"\n                    f\"Got {fixed_train_size}.\"\n                )\n            if not isinstance(gap, (int, np.integer)) or gap < 0:\n                raise ValueError(\n                    f\"`gap` must be an integer greater than or equal to 0. Got {gap}.\"\n                )\n            if skip_folds is not None:\n                if not isinstance(skip_folds, (int, np.integer, list, type(None))):\n                    raise TypeError(\n                        f\"`skip_folds` must be an integer greater than 0, a list of \"\n                        f\"integers or `None`. Got {skip_folds}.\"\n                    )\n                if isinstance(skip_folds, (int, np.integer)) and skip_folds < 1:\n                    raise ValueError(\n                        f\"`skip_folds` must be an integer greater than 0, a list of \"\n                        f\"integers or `None`. Got {skip_folds}.\"\n                    )\n                if isinstance(skip_folds, list) and any([x < 1 for x in skip_folds]):\n                    raise ValueError(\n                        f\"`skip_folds` list must contain integers greater than or \"\n                        f\"equal to 1. The first fold is always needed to train the \"\n                        f\"forecaster. Got {skip_folds}.\"\n                    ) \n            if not isinstance(allow_incomplete_fold, bool):\n                raise TypeError(\n                    f\"`allow_incomplete_fold` must be a boolean: `True`, `False`. \"\n                    f\"Got {allow_incomplete_fold}.\"\n                )\n            \n        if cv_name == \"OneStepAheadFold\":\n            if (\n                not isinstance(initial_train_size, (int, np.integer))\n                or initial_train_size < 1\n            ):\n                raise ValueError(\n                    f\"`initial_train_size` must be an integer greater than 0. \"\n                    f\"Got {initial_train_size}.\"\n                )\n        \n        if (\n            not isinstance(window_size, (int, np.integer, pd.DateOffset, type(None)))\n            or isinstance(window_size, (int, np.integer))\n            and window_size < 1\n        ):\n            raise ValueError(\n                f\"`window_size` must be an integer greater than 0. Got {window_size}.\"\n            )\n        \n        if not isinstance(return_all_indexes, bool):\n            raise TypeError(\n                f\"`return_all_indexes` must be a boolean: `True`, `False`. \"\n                f\"Got {return_all_indexes}.\"\n            )\n        if differentiation is not None:\n            if not isinstance(differentiation, (int, np.integer)) or differentiation < 0:\n                raise ValueError(\n                    f\"`differentiation` must be None or an integer greater than or \"\n                    f\"equal to 0. Got {differentiation}.\"\n                )\n        if not isinstance(verbose, bool):\n            raise TypeError(\n                f\"`verbose` must be a boolean: `True`, `False`. \"\n                f\"Got {verbose}.\"\n            )\n\n    def _extract_index(\n        self,\n        X: Union[pd.Series, pd.DataFrame, pd.Index, dict]\n    ) -> pd.Index:\n        \"\"\"\n        Extracts and returns the index from the input data X.\n\n        Parameters\n        ----------\n        X : pandas Series, pandas DataFrame, pandas Index, dict\n            Time series data or index to split.\n\n        Returns\n        -------\n        idx : pandas Index\n            Index extracted from the input data.\n        \n        \"\"\"\n\n        if isinstance(X, (pd.Series, pd.DataFrame)):\n            idx = X.index\n        elif isinstance(X, dict):\n            freqs = [s.index.freq for s in X.values() if s.index.freq is not None]\n            if not freqs:\n                raise ValueError(\"At least one series must have a frequency.\")\n            if not all(f == freqs[0] for f in freqs):\n                raise ValueError(\n                    \"All series with frequency must have the same frequency.\"\n                )\n            min_idx = min([v.index[0] for v in X.values()])\n            max_idx = max([v.index[-1] for v in X.values()])\n            idx = pd.date_range(start=min_idx, end=max_idx, freq=freqs[0])\n        else:\n            idx = X\n            \n        return idx\n\n    def set_params(\n        self, \n        params: dict\n    ) -> None:\n        \"\"\"\n        Set the parameters of the Fold object. Before overwriting the current \n        parameters, the input parameters are validated to ensure correctness.\n\n        Parameters\n        ----------\n        params : dict\n            Dictionary with the parameters to set.\n        \n        Returns\n        -------\n        None\n        \n        \"\"\"\n\n        if not isinstance(params, dict):\n            raise TypeError(\n                f\"`params` must be a dictionary. Got {type(params)}.\"\n            )\n\n        current_params = deepcopy(vars(self))\n        unknown_params = set(params.keys()) - set(current_params.keys())\n        if unknown_params:\n            warnings.warn(\n                f\"Unknown parameters: {unknown_params}. They have been ignored.\",\n                IgnoredArgumentWarning\n            )\n\n        filtered_params = {k: v for k, v in params.items() if k in current_params}\n        updated_params = {'cv_name': type(self).__name__, **current_params, **filtered_params}\n\n        self._validate_params(**updated_params)\n        for key, value in updated_params.items():\n            setattr(self, key, value)\n\n\nclass OneStepAheadFold(BaseFold):\n    \"\"\"\n    Class to split time series data into train and test folds for one-step-ahead\n    forecasting.\n\n    Parameters\n    ----------\n    initial_train_size : int\n        Number of observations used for initial training.\n    window_size : int, default `None`\n        Number of observations needed to generate the autoregressive predictors.\n    differentiation : int, default `None`\n        Number of observations to use for differentiation. This is used to extend the\n        `last_window` as many observations as the differentiation order.\n    return_all_indexes : bool, default `False`\n        Whether to return all indexes or only the start and end indexes of each fold.\n    verbose : bool, default `True`\n        Whether to print information about generated folds.\n\n    Attributes\n    ----------\n    initial_train_size : int\n        Number of observations used for initial training.\n    window_size : int\n        Number of observations needed to generate the autoregressive predictors.\n    differentiation : int \n        Number of observations to use for differentiation. This is used to extend the\n        `last_window` as many observations as the differentiation order.\n    return_all_indexes : bool\n        Whether to return all indexes or only the start and end indexes of each fold.\n    verbose : bool\n        Whether to print information about generated folds.\n    steps : Any\n        This attribute is not used in this class. It is included for API consistency.\n    fixed_train_size : Any\n        This attribute is not used in this class. It is included for API consistency.\n    gap : Any\n        This attribute is not used in this class. It is included for API consistency.\n    skip_folds : Any\n        This attribute is not used in this class. It is included for API consistency.\n    allow_incomplete_fold : Any\n        This attribute is not used in this class. It is included for API consistency.\n    refit : Any\n        This attribute is not used in this class. It is included for API consistency.\n    \n    \"\"\"\n\n    def __init__(\n        self,\n        initial_train_size: int,\n        window_size: Optional[int] = None,\n        differentiation: Optional[int] = None,\n        return_all_indexes: bool = False,\n        verbose: bool = True,\n    ) -> None:\n        \n        super().__init__(\n            initial_train_size = initial_train_size,\n            window_size        = window_size,\n            differentiation    = differentiation,\n            return_all_indexes = return_all_indexes,\n            verbose            = verbose\n        )\n\n    def __repr__(\n        self\n    ) -> str:\n        \"\"\"\n        Information displayed when printed.\n        \"\"\"\n            \n        return (\n            f\"OneStepAheadFold(\\n\"\n            f\"    initial_train_size = {self.initial_train_size},\\n\"\n            f\"    window_size        = {self.window_size},\\n\"\n            f\"    differentiation    = {self.differentiation},\\n\"\n            f\"    return_all_indexes = {self.return_all_indexes},\\n\"\n            f\"    verbose            = {self.verbose}\\n\"\n            f\")\"\n        )\n    \n    def split(\n        self,\n        X: Union[pd.Series, pd.DataFrame, pd.Index, dict],\n        as_pandas: bool = False,\n        externally_fitted: Any = None\n    ) -> Union[list, pd.DataFrame]:\n        \"\"\"\n        Split the time series data into train and test folds.\n\n        Parameters\n        ----------\n        X : pandas Series, DataFrame, Index, or dictionary\n            Time series data or index to split.\n        as_pandas : bool, default `False`\n            If True, the folds are returned as a DataFrame. This is useful to visualize\n            the folds in a more interpretable way.\n        externally_fitted : Any\n            This argument is not used in this class. It is included for API consistency.\n        \n        Returns\n        -------\n        fold : list, pandas DataFrame\n            A list of lists containing the indices (position) for for each fold. Each list\n            contains 2 lists the following information:\n\n            - [train_start, train_end]: list with the start and end positions of the\n            training set.\n            - [test_start, test_end]: list with the start and end positions of the test\n            set. These are the observations used to evaluate the forecaster.\n        \n            It is important to note that the returned values are the positions of the\n            observations and not the actual values of the index, so they can be used to\n            slice the data directly using iloc.\n\n            If `as_pandas` is `True`, the folds are returned as a DataFrame with the\n            following columns: 'fold', 'train_start', 'train_end', 'test_start', 'test_end'.\n\n            Following the python convention, the start index is inclusive and the end\n            index is exclusive. This means that the last index is not included in the\n            slice.\n        \n        \"\"\"\n\n        if not isinstance(X, (pd.Series, pd.DataFrame, pd.Index, dict)):\n            raise TypeError(\n                f\"X must be a pandas Series, DataFrame, Index or a dictionary. \"\n                f\"Got {type(X)}.\"\n            )\n\n        index = self._extract_index(X)\n        fold = [\n            [0, self.initial_train_size],\n            [self.initial_train_size, len(X)],\n            True\n        ]\n\n        if self.verbose:\n            self._print_info(\n                index = index,\n                fold = fold,\n            )\n\n        if self.return_all_indexes:\n            fold = [\n                [range(fold[0][0], fold[0][1])],\n                [range(fold[1][0], fold[1][1])],\n                fold[2]\n            ]\n\n        if as_pandas:\n            if not self.return_all_indexes:\n                fold = pd.DataFrame(\n                    data = [list(itertools.chain(*fold[:-1])) + [fold[-1]]],\n                    columns = [\n                        'train_start',\n                        'train_end',\n                        'test_start',\n                        'test_end',\n                        'fit_forecaster'\n                    ],\n                )\n            else:\n                fold = pd.DataFrame(\n                    data = [fold],\n                    columns = [\n                        'train_index',\n                        'test_index',\n                        'fit_forecaster'\n                    ],\n                )\n            fold.insert(0, 'fold', range(len(fold)))\n\n        return fold\n\n    def _print_info(\n        self,\n        index: pd.Index,\n        fold: list,\n    ) -> None:\n        \"\"\"\n        Print information about folds.\n        \"\"\"\n\n        if self.differentiation is None:\n            differentiation = 0\n        else:\n            differentiation = self.differentiation\n\n        initial_train_size = self.initial_train_size - differentiation\n        test_length = len(index) - (initial_train_size + differentiation)\n\n        print(\"Information of folds\")\n        print(\"--------------------\")\n        print(\n            f\"Number of observations in train: {initial_train_size}\"\n        )\n        if self.differentiation is not None:\n            print(\n                f\"    First {differentiation} observation/s in training set \"\n                f\"are used for differentiation\"\n            )\n        print(\n            f\"Number of observations in test: {test_length}\"\n        )\n        \n        training_start = index[fold[0][0] + differentiation]\n        training_end = index[fold[0][-1]]\n        test_start  = index[fold[1][0]]\n        test_end    = index[fold[1][-1] - 1]\n        \n        print(\n            f\"Training : {training_start} -- {training_end} (n={initial_train_size})\"\n        )\n        print(\n            f\"Test     : {test_start} -- {test_end} (n={test_length})\"\n        )\n        print(\"\")\n\n\nclass TimeSeriesFold(BaseFold):\n    \"\"\"\n    Class to split time series data into train and test folds. \n    When used within a backtesting or hyperparameter search, the arguments\n    'initial_train_size', 'window_size' and 'differentiation' are not required\n    as they are automatically set by the backtesting or hyperparameter search\n    functions.\n\n    Parameters\n    ----------\n    steps : int\n        Number of observations used to be predicted in each fold. This is also commonly\n        referred to as the forecast horizon or test size.\n    initial_train_size : int, default `None`\n        Number of observations used for initial training. If `None` or 0, the initial\n        forecaster is not trained in the first fold.\n    window_size : int, default `None`\n        Number of observations needed to generate the autoregressive predictors.\n    differentiation : int, default `None`\n        Number of observations to use for differentiation. This is used to extend the\n        `last_window` as many observations as the differentiation order.\n    refit : bool, int, default `False`\n        Whether to refit the forecaster in each fold.\n\n        - If `True`, the forecaster is refitted in each fold.\n        - If `False`, the forecaster is trained only in the first fold.\n        - If an integer, the forecaster is trained in the first fold and then refitted\n          every `refit` folds.\n    fixed_train_size : bool, default `True`\n        Whether the training size is fixed or increases in each fold.\n    gap : int, default `0`\n        Number of observations between the end of the training set and the start of the\n        test set.\n    skip_folds : int, list, default `None`\n        Number of folds to skip.\n\n        - If an integer, every 'skip_folds'-th is returned.\n        - If a list, the indexes of the folds to skip.\n\n        For example, if `skip_folds=3` and there are 10 folds, the returned folds are\n        0, 3, 6, and 9. If `skip_folds=[1, 2, 3]`, the returned folds are 0, 4, 5, 6, 7,\n        8, and 9.\n    allow_incomplete_fold : bool, default `True`\n        Whether to allow the last fold to include fewer observations than `steps`.\n        If `False`, the last fold is excluded if it is incomplete.\n    return_all_indexes : bool, default `False`\n        Whether to return all indexes or only the start and end indexes of each fold.\n    verbose : bool, default `True`\n        Whether to print information about generated folds.\n\n    Attributes\n    ----------\n    steps : int\n        Number of observations used to be predicted in each fold. This is also commonly\n        referred to as the forecast horizon or test size.\n    initial_train_size : int\n        Number of observations used for initial training. If `None` or 0, the initial\n        forecaster is not trained in the first fold.\n    window_size : int\n        Number of observations needed to generate the autoregressive predictors.\n    differentiation : int\n        Number of observations to use for differentiation. This is used to extend the\n        `last_window` as many observations as the differentiation order.\n    refit : bool, int\n        Whether to refit the forecaster in each fold.\n    fixed_train_size : bool\n        Whether the training size is fixed or increases in each fold.\n    gap : int\n        Number of observations between the end of the training set and the start of the\n        test set.\n    skip_folds : int, list\n        Number of folds to skip.\n    allow_incomplete_fold : bool\n        Whether to allow the last fold to include fewer observations than `steps`.\n    return_all_indexes : bool\n        Whether to return all indexes or only the start and end indexes of each fold.\n    verbose : bool\n        Whether to print information about generated folds.\n\n    Notes\n    -----\n    Returned values are the positions of the observations and not the actual values of\n    the index, so they can be used to slice the data directly using iloc. For example,\n    if the input series is `X = [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]`, the \n    `initial_train_size = 3`, `window_size = 2`, `steps = 4`, and `gap = 1`,\n    the output of the first fold will: [[0, 3], [1, 3], [3, 8], [4, 8], True].\n\n    The first list `[0, 3]` indicates that the training set goes from the first to the\n    third observation. The second list `[1, 3]` indicates that the last window seen by\n    the forecaster during training goes from the second to the third observation. The\n    third list `[3, 8]` indicates that the test set goes from the fourth to the eighth\n    observation. The fourth list `[4, 8]` indicates that the test set including the gap\n    goes from the fifth to the eighth observation. The boolean `False` indicates that the\n    forecaster should not be trained in this fold.\n\n    Following the python convention, the start index is inclusive and the end index is\n    exclusive. This means that the last index is not included in the slice.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        steps: int,\n        initial_train_size: Optional[int] = None,\n        window_size: Optional[int] = None,\n        differentiation: Optional[int] = None,\n        refit: Union[bool, int] = False,\n        fixed_train_size: bool = True,\n        gap: int = 0,\n        skip_folds: Optional[Union[int, list]] = None,\n        allow_incomplete_fold: bool = True,\n        return_all_indexes: bool = False,\n        verbose: bool = True\n    ) -> None:\n        \n        super().__init__(\n            steps                 = steps,\n            initial_train_size    = initial_train_size,\n            window_size           = window_size,\n            differentiation       = differentiation,\n            refit                 = refit,\n            fixed_train_size      = fixed_train_size,\n            gap                   = gap,\n            skip_folds            = skip_folds,\n            allow_incomplete_fold = allow_incomplete_fold,\n            return_all_indexes    = return_all_indexes,\n            verbose               = verbose\n        )\n\n    def __repr__(\n        self\n    ) -> str:\n        \"\"\"\n        Information displayed when printed.\n        \"\"\"\n            \n        return (\n            f\"TimeSeriesFold(\\n\"\n            f\"    steps                 = {self.steps},\\n\"\n            f\"    initial_train_size    = {self.initial_train_size},\\n\"\n            f\"    window_size           = {self.window_size},\\n\"\n            f\"    differentiation       = {self.differentiation},\\n\"\n            f\"    refit                 = {self.refit},\\n\"\n            f\"    fixed_train_size      = {self.fixed_train_size},\\n\"\n            f\"    gap                   = {self.gap},\\n\"\n            f\"    skip_folds            = {self.skip_folds},\\n\"\n            f\"    allow_incomplete_fold = {self.allow_incomplete_fold},\\n\"\n            f\"    return_all_indexes    = {self.return_all_indexes},\\n\"\n            f\"    verbose               = {self.verbose}\\n\"\n            f\")\"\n        )\n\n    def split(\n        self,\n        X: Union[pd.Series, pd.DataFrame, pd.Index, dict],\n        as_pandas: bool = False\n    ) -> Union[list, pd.DataFrame]:\n        \"\"\"\n        Split the time series data into train and test folds.\n\n        Parameters\n        ----------\n        X : pandas Series, pandas DataFrame, pandas Index, dict\n            Time series data or index to split.\n        as_pandas : bool, default `False`\n            If True, the folds are returned as a DataFrame. This is useful to visualize\n            the folds in a more interpretable way.\n\n        Returns\n        -------\n        folds : list, pandas DataFrame\n            A list of lists containing the indices (position) for for each fold. Each list\n            contains 4 lists and a boolean with the following information:\n\n            - [train_start, train_end]: list with the start and end positions of the\n            training set.\n            - [last_window_start, last_window_end]: list with the start and end positions\n            of the last window seen by the forecaster during training. The last window\n            is used to generate the lags use as predictors. If `differentiation` is\n            included, the interval is extended as many observations as the\n            differentiation order. If the argument `window_size` is `None`, this list is\n            empty.\n            - [test_start, test_end]: list with the start and end positions of the test\n            set. These are the observations used to evaluate the forecaster.\n            - [test_start_with_gap, test_end_with_gap]: list with the start and end\n            positions of the test set including the gap. The gap is the number of\n            observations between the end of the training set and the start of the test\n            set.\n            - fit_forecaster: boolean indicating whether the forecaster should be fitted\n            in this fold.\n\n            It is important to note that the returned values are the positions of the\n            observations and not the actual values of the index, so they can be used to\n            slice the data directly using iloc.\n\n            If `as_pandas` is `True`, the folds are returned as a DataFrame with the\n            following columns: 'fold', 'train_start', 'train_end', 'last_window_start',\n            'last_window_end', 'test_start', 'test_end', 'test_start_with_gap',\n            'test_end_with_gap', 'fit_forecaster'.\n\n            Following the python convention, the start index is inclusive and the end\n            index is exclusive. This means that the last index is not included in the\n            slice.\n\n        \"\"\"\n\n        if not isinstance(X, (pd.Series, pd.DataFrame, pd.Index, dict)):\n            raise TypeError(\n                f\"X must be a pandas Series, DataFrame, Index or a dictionary. \"\n                f\"Got {type(X)}.\"\n            )\n        \n        if isinstance(self.window_size, pd.tseries.offsets.DateOffset):\n            # Calculate the window_size in steps. This is not a exact calculation\n            # because the offset follows the calendar rules and the distance between\n            # two dates may not be constant.\n            first_valid_index = X.index[-1] - self.window_size\n            try:\n                window_size_idx_start = X.index.get_loc(first_valid_index)\n                window_size_idx_end = X.index.get_loc(X.index[-1])\n                self.window_size = window_size_idx_end - window_size_idx_start\n            except KeyError:\n                raise ValueError(\n                    f\"The length of `X` ({len(X)}), must be greater than or equal \"\n                    f\"to the window size ({self.window_size}). Try to decrease the \"\n                    f\"size of the offset (forecaster.offset), or increase the \"\n                    f\"size of `y`.\"\n                )\n        \n        if self.initial_train_size is None:\n            if self.window_size is None:\n                raise ValueError(\n                    \"To use split method when `initial_train_size` is None, \"\n                    \"`window_size` must be an integer greater than 0. \"\n                    \"Although no initial training is done and all data is used to \"\n                    \"evaluate the model, the first `window_size` observations are \"\n                    \"needed to create the initial predictors. Got `window_size` = None.\"\n                )\n            if self.refit:\n                raise ValueError(\n                    \"`refit` is only allowed when `initial_train_size` is not `None`. \"\n                    \"Set `refit` to `False` if you want to use `initial_train_size = None`.\"\n                )\n            externally_fitted = True\n            self.initial_train_size = self.window_size  # Reset to None later\n        else:\n            if self.window_size is None:\n                warnings.warn(\n                    \"Last window cannot be calculated because `window_size` is None.\"\n                )\n            externally_fitted = False\n\n        index = self._extract_index(X)\n        idx = range(len(index))\n        folds = []\n        i = 0\n        last_fold_excluded = False\n\n        if len(index) < self.initial_train_size + self.steps:\n            raise ValueError(\n                f\"The time series must have at least `initial_train_size + steps` \"\n                f\"observations. Got {len(index)} observations.\"\n            )\n\n        while self.initial_train_size + (i * self.steps) + self.gap < len(index):\n\n            if self.refit:\n                # If `fixed_train_size` the train size doesn't increase but moves by \n                # `steps` positions in each iteration. If `False`, the train size\n                # increases by `steps` in each iteration.\n                train_iloc_start = i * (self.steps) if self.fixed_train_size else 0\n                train_iloc_end = self.initial_train_size + i * (self.steps)\n                test_iloc_start = train_iloc_end\n            else:\n                # The train size doesn't increase and doesn't move.\n                train_iloc_start = 0\n                train_iloc_end = self.initial_train_size\n                test_iloc_start = self.initial_train_size + i * (self.steps)\n            \n            if self.window_size is not None:\n                last_window_iloc_start = test_iloc_start - self.window_size\n            test_iloc_end = test_iloc_start + self.gap + self.steps\n        \n            partitions = [\n                idx[train_iloc_start : train_iloc_end],\n                idx[last_window_iloc_start : test_iloc_start] if self.window_size is not None else [],\n                idx[test_iloc_start : test_iloc_end],\n                idx[test_iloc_start + self.gap : test_iloc_end]\n            ]\n            folds.append(partitions)\n            i += 1\n\n        if not self.allow_incomplete_fold and len(folds[-1][3]) < self.steps:\n            folds = folds[:-1]\n            last_fold_excluded = True\n\n        # Replace partitions inside folds with length 0 with `None`\n        folds = [\n            [partition if len(partition) > 0 else None for partition in fold] \n             for fold in folds\n        ]\n\n        # Create a flag to know whether to train the forecaster\n        if self.refit == 0:\n            self.refit = False\n            \n        if isinstance(self.refit, bool):\n            fit_forecaster = [self.refit] * len(folds)\n            fit_forecaster[0] = True\n        else:\n            fit_forecaster = [False] * len(folds)\n            for i in range(0, len(fit_forecaster), self.refit): \n                fit_forecaster[i] = True\n        \n        for i in range(len(folds)): \n            folds[i].append(fit_forecaster[i])\n            if fit_forecaster[i] is False:\n                folds[i][0] = folds[i - 1][0]\n\n        index_to_skip = []\n        if self.skip_folds is not None:\n            if isinstance(self.skip_folds, (int, np.integer)) and self.skip_folds > 0:\n                index_to_keep = np.arange(0, len(folds), self.skip_folds)\n                index_to_skip = np.setdiff1d(np.arange(0, len(folds)), index_to_keep, assume_unique=True)\n                index_to_skip = [int(x) for x in index_to_skip]  # Required since numpy 2.0\n            if isinstance(self.skip_folds, list):\n                index_to_skip = [i for i in self.skip_folds if i < len(folds)]        \n        \n        if self.verbose:\n            self._print_info(\n                index              = index,\n                folds              = folds,\n                externally_fitted  = externally_fitted,\n                last_fold_excluded = last_fold_excluded,\n                index_to_skip      = index_to_skip\n            )\n\n        folds = [fold for i, fold in enumerate(folds) if i not in index_to_skip]\n        if not self.return_all_indexes:\n            # +1 to prevent iloc pandas from deleting the last observation\n            folds = [\n                [[fold[0][0], fold[0][-1] + 1], \n                 [fold[1][0], fold[1][-1] + 1] if self.window_size is not None else [],\n                 [fold[2][0], fold[2][-1] + 1],\n                 [fold[3][0], fold[3][-1] + 1],\n                 fold[4]] \n                for fold in folds\n            ]\n\n        if externally_fitted:\n            self.initial_train_size = None\n            folds[0][4] = False\n\n        if as_pandas:\n            if self.window_size is None:\n                for fold in folds:\n                    fold[1] = [None, None]\n\n            if not self.return_all_indexes:\n                folds = pd.DataFrame(\n                    data = [list(itertools.chain(*fold[:-1])) + [fold[-1]] for fold in folds],\n                    columns = [\n                        'train_start',\n                        'train_end',\n                        'last_window_start',\n                        'last_window_end',\n                        'test_start',\n                        'test_end',\n                        'test_start_with_gap',\n                        'test_end_with_gap',\n                        'fit_forecaster'\n                    ],\n                )\n            else:\n                folds = pd.DataFrame(\n                    data = folds,\n                    columns = [\n                        'train_index',\n                        'last_window_index',\n                        'test_index',\n                        'test_index_with_gap',\n                        'fit_forecaster'\n                    ],\n                )\n            folds.insert(0, 'fold', range(len(folds)))\n\n        return folds\n\n    def _print_info(\n        self,\n        index: pd.Index,\n        folds: list,\n        externally_fitted: bool,\n        last_fold_excluded: bool,\n        index_to_skip: list,\n    ) -> None:\n        \"\"\"\n        Print information about folds.\n        \"\"\"\n\n        print(\"Information of folds\")\n        print(\"--------------------\")\n        if externally_fitted:\n            print(\n                f\"An already trained forecaster is to be used. Window size: \"\n                f\"{self.window_size}\"\n            )\n        else:\n            if self.differentiation is None:\n                print(\n                    f\"Number of observations used for initial training: \"\n                    f\"{self.initial_train_size}\"\n                )\n            else:\n                print(\n                    f\"Number of observations used for initial training: \"\n                    f\"{self.initial_train_size - self.differentiation}\"\n                )\n                print(\n                    f\"    First {self.differentiation} observation/s in training sets \"\n                    f\"are used for differentiation\"\n                )\n        print(\n            f\"Number of observations used for backtesting: \"\n            f\"{len(index) - self.initial_train_size}\"\n        )\n        print(f\"    Number of folds: {len(folds)}\")\n        print(\n            f\"    Number skipped folds: \"\n            f\"{len(index_to_skip)} {index_to_skip if index_to_skip else ''}\"\n        )\n        print(f\"    Number of steps per fold: {self.steps}\")\n        print(\n            f\"    Number of steps to exclude between last observed data \"\n            f\"(last window) and predictions (gap): {self.gap}\"\n        )\n        if last_fold_excluded:\n            print(\"    Last fold has been excluded because it was incomplete.\")\n        if len(folds[-1][3]) < self.steps:\n            print(f\"    Last fold only includes {len(folds[-1][3])} observations.\")\n        print(\"\")\n\n        if self.differentiation is None:\n            differentiation = 0\n        else:\n            differentiation = self.differentiation\n        \n        for i, fold in enumerate(folds):\n            is_fold_skipped   = i in index_to_skip\n            has_training      = fold[-1] if i != 0 else True\n            training_start    = (\n                index[fold[0][0] + differentiation] if fold[0] is not None else None\n            )\n            training_end      = index[fold[0][-1]] if fold[0] is not None else None\n            training_length   = (\n                len(fold[0]) - differentiation if fold[0] is not None else 0\n            )\n            validation_start  = index[fold[3][0]]\n            validation_end    = index[fold[3][-1]]\n            validation_length = len(fold[3])\n\n            print(f\"Fold: {i}\")\n            if is_fold_skipped:\n                print(\"    Fold skipped\")\n            elif not externally_fitted and has_training:\n                print(\n                    f\"    Training:   {training_start} -- {training_end}  \"\n                    f\"(n={training_length})\"\n                )\n                print(\n                    f\"    Validation: {validation_start} -- {validation_end}  \"\n                    f\"(n={validation_length})\"\n                )\n            else:\n                print(\"    Training:   No training in this fold\")\n                print(\n                    f\"    Validation: {validation_start} -- {validation_end}  \"\n                    f\"(n={validation_length})\"\n                )\n\n        print(\"\")\n"
  },
  "GT_src_dict": {
    "skforecast/model_selection/_split.py": {
      "BaseFold.__init__": {
        "code": "    def __init__(self, steps: Optional[int]=None, initial_train_size: Optional[int]=None, window_size: Optional[int]=None, differentiation: Optional[int]=None, refit: Union[bool, int]=False, fixed_train_size: bool=True, gap: int=0, skip_folds: Optional[Union[int, list]]=None, allow_incomplete_fold: bool=True, return_all_indexes: bool=False, verbose: bool=True) -> None:\n        \"\"\"Initializes a BaseFold object, which serves as a base class for creating folds in time series cross-validation. This constructor validates input parameters related to the split configuration and assigns them to instance attributes.\n\nParameters\n----------\nsteps : Optional[int], default `None`\n    The number of observations to predict in each fold (forecast horizon/test size).\ninitial_train_size : Optional[int], default `None`\n    The number of observations for initial training.\nwindow_size : Optional[int], default `None`\n    The size of the window necessary for generating autoregressive predictors.\ndifferentiation : Optional[int], default `None`\n    Number of observations used for differentiation, allowing to extend the last window.\nrefit : Union[bool, int], default `False`\n    Control whether to refit the forecaster in each fold, either a boolean or integer for specific intervals.\nfixed_train_size : bool, default `True`\n    Indicates if training size remains constant or increases with each fold.\ngap : int, default `0`\n    Number of observations between the end of the training set and the start of the test set.\nskip_folds : Optional[Union[int, list]], default `None`\n    Number of folds to skip or list of specific fold indexes to skip.\nallow_incomplete_fold : bool, default `True`\n    Enables/disables the inclusion of incomplete folds.\nreturn_all_indexes : bool, default `False`\n    Determines if all indexes or only start/end indexes of each fold are returned.\nverbose : bool, default `True`\n    Activates verbose mode to print fold information during generation.\n\nReturn\n------\nNone\n\nSide Effects\n------------\nValidates parameters for correctness and raises exceptions for invalid values. This initialization prepares the instance for generating folds based on time series data using methods defined in this and inherited classes.\"\"\"\n        self._validate_params(cv_name=type(self).__name__, steps=steps, initial_train_size=initial_train_size, window_size=window_size, differentiation=differentiation, refit=refit, fixed_train_size=fixed_train_size, gap=gap, skip_folds=skip_folds, allow_incomplete_fold=allow_incomplete_fold, return_all_indexes=return_all_indexes, verbose=verbose)\n        self.steps = steps\n        self.initial_train_size = initial_train_size\n        self.window_size = window_size\n        self.differentiation = differentiation\n        self.refit = refit\n        self.fixed_train_size = fixed_train_size\n        self.gap = gap\n        self.skip_folds = skip_folds\n        self.allow_incomplete_fold = allow_incomplete_fold\n        self.return_all_indexes = return_all_indexes\n        self.verbose = verbose",
        "docstring": "Initializes a BaseFold object, which serves as a base class for creating folds in time series cross-validation. This constructor validates input parameters related to the split configuration and assigns them to instance attributes.\n\nParameters\n----------\nsteps : Optional[int], default `None`\n    The number of observations to predict in each fold (forecast horizon/test size).\ninitial_train_size : Optional[int], default `None`\n    The number of observations for initial training.\nwindow_size : Optional[int], default `None`\n    The size of the window necessary for generating autoregressive predictors.\ndifferentiation : Optional[int], default `None`\n    Number of observations used for differentiation, allowing to extend the last window.\nrefit : Union[bool, int], default `False`\n    Control whether to refit the forecaster in each fold, either a boolean or integer for specific intervals.\nfixed_train_size : bool, default `True`\n    Indicates if training size remains constant or increases with each fold.\ngap : int, default `0`\n    Number of observations between the end of the training set and the start of the test set.\nskip_folds : Optional[Union[int, list]], default `None`\n    Number of folds to skip or list of specific fold indexes to skip.\nallow_incomplete_fold : bool, default `True`\n    Enables/disables the inclusion of incomplete folds.\nreturn_all_indexes : bool, default `False`\n    Determines if all indexes or only start/end indexes of each fold are returned.\nverbose : bool, default `True`\n    Activates verbose mode to print fold information during generation.\n\nReturn\n------\nNone\n\nSide Effects\n------------\nValidates parameters for correctness and raises exceptions for invalid values. This initialization prepares the instance for generating folds based on time series data using methods defined in this and inherited classes.",
        "signature": "def __init__(self, steps: Optional[int]=None, initial_train_size: Optional[int]=None, window_size: Optional[int]=None, differentiation: Optional[int]=None, refit: Union[bool, int]=False, fixed_train_size: bool=True, gap: int=0, skip_folds: Optional[Union[int, list]]=None, allow_incomplete_fold: bool=True, return_all_indexes: bool=False, verbose: bool=True) -> None:",
        "type": "Method",
        "class_signature": "class BaseFold:"
      },
      "BaseFold._validate_params": {
        "code": "    def _validate_params(self, cv_name: str, steps: Optional[int]=None, initial_train_size: Optional[int]=None, window_size: Optional[int]=None, differentiation: Optional[int]=None, refit: Union[bool, int]=False, fixed_train_size: bool=True, gap: int=0, skip_folds: Optional[Union[int, list]]=None, allow_incomplete_fold: bool=True, return_all_indexes: bool=False, verbose: bool=True) -> None:\n        \"\"\"Validate all input parameters to ensure correctness for time series cross-validation setups.\n\n    Parameters\n    ----------\n    cv_name : str\n        The name of the cross-validation class (either \"TimeSeriesFold\" or \"OneStepAheadFold\").\n    steps : Optional[int], default `None`\n        Number of observations used to be predicted in each fold (forecast horizon).\n    initial_train_size : Optional[int], default `None`\n        Number of observations used for initial training; must be greater than 0 if specified.\n    window_size : Optional[int], default `None`\n        Number of observations needed to generate autoregressive predictors; must be greater than 0 if specified.\n    differentiation : Optional[int], default `None`\n        Number of observations to consider for differentiation; must be non-negative if specified.\n    refit : Union[bool, int], default `False`\n        Controls whether to refit the forecaster in each fold (True for always, False for only first fold, or an integer for specific folds).\n    fixed_train_size : bool, default `True`\n        Indicates whether the training size is fixed or increases in each fold.\n    gap : int, default `0`\n        Number of observations between the end of training set and start of test set; must be non-negative.\n    skip_folds : Optional[Union[int, list]], default `None`\n        Specifies which folds to skip; can be an integer for interval skipping or a list of fold indexes.\n    allow_incomplete_fold : bool, default `True`\n        Determines if the last fold can have fewer observations than `steps`.\n    return_all_indexes : bool, default `False`\n        Specifies whether to return all indexes or just start and end indexes of each fold.\n    verbose : bool, default `True`\n        Controls printing of information about generated folds.\n\n    Raises\n    ------\n    ValueError\n        If `steps`, `initial_train_size`, `gap` or `window_size` do not meet the specified constraints.\n    TypeError\n        If `refit`, `fixed_train_size`, `allow_incomplete_fold`, or `return_all_indexes` are not of the expected types.\n\n    Notes\n    -----\n    The method primarily provides validation to avoid incorrect configurations that could lead to runtime errors during time series cross-validation processes.\"\"\"\n        '\\n        Validate all input parameters to ensure correctness.\\n        '\n        if cv_name == 'TimeSeriesFold':\n            if not isinstance(steps, (int, np.integer)) or steps < 1:\n                raise ValueError(f'`steps` must be an integer greater than 0. Got {steps}.')\n            if not isinstance(initial_train_size, (int, np.integer, type(None))):\n                raise ValueError(f'`initial_train_size` must be an integer greater than 0 or None. Got {initial_train_size}.')\n            if initial_train_size is not None and initial_train_size < 1:\n                raise ValueError(f'`initial_train_size` must be an integer greater than 0 or None. Got {initial_train_size}.')\n            if not isinstance(refit, (bool, int, np.integer)):\n                raise TypeError(f'`refit` must be a boolean or an integer equal or greater than 0. Got {refit}.')\n            if isinstance(refit, (int, np.integer)) and (not isinstance(refit, bool)) and (refit < 0):\n                raise TypeError(f'`refit` must be a boolean or an integer equal or greater than 0. Got {refit}.')\n            if not isinstance(fixed_train_size, bool):\n                raise TypeError(f'`fixed_train_size` must be a boolean: `True`, `False`. Got {fixed_train_size}.')\n            if not isinstance(gap, (int, np.integer)) or gap < 0:\n                raise ValueError(f'`gap` must be an integer greater than or equal to 0. Got {gap}.')\n            if skip_folds is not None:\n                if not isinstance(skip_folds, (int, np.integer, list, type(None))):\n                    raise TypeError(f'`skip_folds` must be an integer greater than 0, a list of integers or `None`. Got {skip_folds}.')\n                if isinstance(skip_folds, (int, np.integer)) and skip_folds < 1:\n                    raise ValueError(f'`skip_folds` must be an integer greater than 0, a list of integers or `None`. Got {skip_folds}.')\n                if isinstance(skip_folds, list) and any([x < 1 for x in skip_folds]):\n                    raise ValueError(f'`skip_folds` list must contain integers greater than or equal to 1. The first fold is always needed to train the forecaster. Got {skip_folds}.')\n            if not isinstance(allow_incomplete_fold, bool):\n                raise TypeError(f'`allow_incomplete_fold` must be a boolean: `True`, `False`. Got {allow_incomplete_fold}.')\n        if cv_name == 'OneStepAheadFold':\n            if not isinstance(initial_train_size, (int, np.integer)) or initial_train_size < 1:\n                raise ValueError(f'`initial_train_size` must be an integer greater than 0. Got {initial_train_size}.')\n        if not isinstance(window_size, (int, np.integer, pd.DateOffset, type(None))) or (isinstance(window_size, (int, np.integer)) and window_size < 1):\n            raise ValueError(f'`window_size` must be an integer greater than 0. Got {window_size}.')\n        if not isinstance(return_all_indexes, bool):\n            raise TypeError(f'`return_all_indexes` must be a boolean: `True`, `False`. Got {return_all_indexes}.')\n        if differentiation is not None:\n            if not isinstance(differentiation, (int, np.integer)) or differentiation < 0:\n                raise ValueError(f'`differentiation` must be None or an integer greater than or equal to 0. Got {differentiation}.')\n        if not isinstance(verbose, bool):\n            raise TypeError(f'`verbose` must be a boolean: `True`, `False`. Got {verbose}.')",
        "docstring": "Validate all input parameters to ensure correctness for time series cross-validation setups.\n\nParameters\n----------\ncv_name : str\n    The name of the cross-validation class (either \"TimeSeriesFold\" or \"OneStepAheadFold\").\nsteps : Optional[int], default `None`\n    Number of observations used to be predicted in each fold (forecast horizon).\ninitial_train_size : Optional[int], default `None`\n    Number of observations used for initial training; must be greater than 0 if specified.\nwindow_size : Optional[int], default `None`\n    Number of observations needed to generate autoregressive predictors; must be greater than 0 if specified.\ndifferentiation : Optional[int], default `None`\n    Number of observations to consider for differentiation; must be non-negative if specified.\nrefit : Union[bool, int], default `False`\n    Controls whether to refit the forecaster in each fold (True for always, False for only first fold, or an integer for specific folds).\nfixed_train_size : bool, default `True`\n    Indicates whether the training size is fixed or increases in each fold.\ngap : int, default `0`\n    Number of observations between the end of training set and start of test set; must be non-negative.\nskip_folds : Optional[Union[int, list]], default `None`\n    Specifies which folds to skip; can be an integer for interval skipping or a list of fold indexes.\nallow_incomplete_fold : bool, default `True`\n    Determines if the last fold can have fewer observations than `steps`.\nreturn_all_indexes : bool, default `False`\n    Specifies whether to return all indexes or just start and end indexes of each fold.\nverbose : bool, default `True`\n    Controls printing of information about generated folds.\n\nRaises\n------\nValueError\n    If `steps`, `initial_train_size`, `gap` or `window_size` do not meet the specified constraints.\nTypeError\n    If `refit`, `fixed_train_size`, `allow_incomplete_fold`, or `return_all_indexes` are not of the expected types.\n\nNotes\n-----\nThe method primarily provides validation to avoid incorrect configurations that could lead to runtime errors during time series cross-validation processes.",
        "signature": "def _validate_params(self, cv_name: str, steps: Optional[int]=None, initial_train_size: Optional[int]=None, window_size: Optional[int]=None, differentiation: Optional[int]=None, refit: Union[bool, int]=False, fixed_train_size: bool=True, gap: int=0, skip_folds: Optional[Union[int, list]]=None, allow_incomplete_fold: bool=True, return_all_indexes: bool=False, verbose: bool=True) -> None:",
        "type": "Method",
        "class_signature": "class BaseFold:"
      },
      "BaseFold._extract_index": {
        "code": "    def _extract_index(self, X: Union[pd.Series, pd.DataFrame, pd.Index, dict]) -> pd.Index:\n        \"\"\"Extracts and returns the index from the provided time series data or index to facilitate the creation of training and testing folds for time series forecasting.\n\nParameters\n----------\nX : Union[pd.Series, pd.DataFrame, pd.Index, dict]\n    The input data can be a pandas Series, DataFrame, Index, or a dictionary containing Series or DataFrames. If `X` is a dictionary, all contained Series must share the same time frequency.\n\nReturns\n-------\nidx : pd.Index\n    The extracted index from the input data. If `X` is a dictionary, it generates a date range covering the earliest and latest timestamps using the common frequency.\n\nRaises\n------\nValueError\n    - If `X` is a dictionary and none of the Series have a defined frequency.\n    - If the Series within the dictionary have different frequencies.\n\nThis method is crucial for preparing the data for splitting into training and test sets within subclasses like `OneStepAheadFold` and `TimeSeriesFold`, where the structure of the input index is essential for ensuring correct temporal alignment of the data.\"\"\"\n        '\\n        Extracts and returns the index from the input data X.\\n\\n        Parameters\\n        ----------\\n        X : pandas Series, pandas DataFrame, pandas Index, dict\\n            Time series data or index to split.\\n\\n        Returns\\n        -------\\n        idx : pandas Index\\n            Index extracted from the input data.\\n        \\n        '\n        if isinstance(X, (pd.Series, pd.DataFrame)):\n            idx = X.index\n        elif isinstance(X, dict):\n            freqs = [s.index.freq for s in X.values() if s.index.freq is not None]\n            if not freqs:\n                raise ValueError('At least one series must have a frequency.')\n            if not all((f == freqs[0] for f in freqs)):\n                raise ValueError('All series with frequency must have the same frequency.')\n            min_idx = min([v.index[0] for v in X.values()])\n            max_idx = max([v.index[-1] for v in X.values()])\n            idx = pd.date_range(start=min_idx, end=max_idx, freq=freqs[0])\n        else:\n            idx = X\n        return idx",
        "docstring": "Extracts and returns the index from the provided time series data or index to facilitate the creation of training and testing folds for time series forecasting.\n\nParameters\n----------\nX : Union[pd.Series, pd.DataFrame, pd.Index, dict]\n    The input data can be a pandas Series, DataFrame, Index, or a dictionary containing Series or DataFrames. If `X` is a dictionary, all contained Series must share the same time frequency.\n\nReturns\n-------\nidx : pd.Index\n    The extracted index from the input data. If `X` is a dictionary, it generates a date range covering the earliest and latest timestamps using the common frequency.\n\nRaises\n------\nValueError\n    - If `X` is a dictionary and none of the Series have a defined frequency.\n    - If the Series within the dictionary have different frequencies.\n\nThis method is crucial for preparing the data for splitting into training and test sets within subclasses like `OneStepAheadFold` and `TimeSeriesFold`, where the structure of the input index is essential for ensuring correct temporal alignment of the data.",
        "signature": "def _extract_index(self, X: Union[pd.Series, pd.DataFrame, pd.Index, dict]) -> pd.Index:",
        "type": "Method",
        "class_signature": "class BaseFold:"
      },
      "OneStepAheadFold.__init__": {
        "code": "    def __init__(self, initial_train_size: int, window_size: Optional[int]=None, differentiation: Optional[int]=None, return_all_indexes: bool=False, verbose: bool=True) -> None:\n        \"\"\"Initialize the OneStepAheadFold object, which is designed to split time series data into training and testing folds specifically for one-step-ahead forecasting.\n\nParameters\n----------\ninitial_train_size : int\n    The number of observations to be used for initial training.\nwindow_size : Optional[int], default `None`\n    The number of observations required to construct autoregressive predictors. If `None`, no autoregressive predictors will be generated.\ndifferentiation : Optional[int], default `None`\n    The number of observations used for differentiation, extending the last window based on the differentiation order. If `None`, differentiation is not applied.\nreturn_all_indexes : bool, default `False`\n    If set to `True`, all indexes of the folds will be returned; otherwise, only the start and end indexes will be provided.\nverbose : bool, default `True`\n    If `True`, additional information about the folds will be printed during processing.\n\nReturns\n-------\nNone\n\nNotes\n-----\nThis constructor calls the parent class BaseFold's `__init__` method to initialize the base parameters related to time series folding. It ensures that the object is configured correctly for performing time series cross-validation tailored for one-step-ahead forecasting, leveraging various parameters such as initial training size, window size, and differentiation. Attributes like `initial_train_size`, `window_size`, and `differentiation` are directly set and utilized in the fold generation process for validating and predicting time series models.\"\"\"\n        super().__init__(initial_train_size=initial_train_size, window_size=window_size, differentiation=differentiation, return_all_indexes=return_all_indexes, verbose=verbose)",
        "docstring": "Initialize the OneStepAheadFold object, which is designed to split time series data into training and testing folds specifically for one-step-ahead forecasting.\n\nParameters\n----------\ninitial_train_size : int\n    The number of observations to be used for initial training.\nwindow_size : Optional[int], default `None`\n    The number of observations required to construct autoregressive predictors. If `None`, no autoregressive predictors will be generated.\ndifferentiation : Optional[int], default `None`\n    The number of observations used for differentiation, extending the last window based on the differentiation order. If `None`, differentiation is not applied.\nreturn_all_indexes : bool, default `False`\n    If set to `True`, all indexes of the folds will be returned; otherwise, only the start and end indexes will be provided.\nverbose : bool, default `True`\n    If `True`, additional information about the folds will be printed during processing.\n\nReturns\n-------\nNone\n\nNotes\n-----\nThis constructor calls the parent class BaseFold's `__init__` method to initialize the base parameters related to time series folding. It ensures that the object is configured correctly for performing time series cross-validation tailored for one-step-ahead forecasting, leveraging various parameters such as initial training size, window size, and differentiation. Attributes like `initial_train_size`, `window_size`, and `differentiation` are directly set and utilized in the fold generation process for validating and predicting time series models.",
        "signature": "def __init__(self, initial_train_size: int, window_size: Optional[int]=None, differentiation: Optional[int]=None, return_all_indexes: bool=False, verbose: bool=True) -> None:",
        "type": "Method",
        "class_signature": "class OneStepAheadFold(BaseFold):"
      },
      "OneStepAheadFold.split": {
        "code": "    def split(self, X: Union[pd.Series, pd.DataFrame, pd.Index, dict], as_pandas: bool=False, externally_fitted: Any=None) -> Union[list, pd.DataFrame]:\n        \"\"\"Split the time series data into training and testing folds for one-step-ahead forecasting.\n\nParameters\n----------\nX : Union[pd.Series, pd.DataFrame, pd.Index, dict]\n    Time series data or index to split into folds.\nas_pandas : bool, default `False`\n    If `True`, returns the folds as a pandas DataFrame for better interpretability.\nexternally_fitted : Any\n    This parameter is not utilized in this method and is included only for API consistency.\n\nReturns\n-------\nUnion[list, pd.DataFrame]\n    A list or DataFrame containing the indices for each fold. Each entry includes:\n    - [train_start, train_end]: positions of the training set.\n    - [test_start, test_end]: positions of the test set for evaluation.\n    \n    If `as_pandas=True`, the DataFrame contains columns: 'fold', 'train_start', \n    'train_end', 'test_start', and 'test_end'.\n\nNotes\n-----\nThe method interacts with the instance attributes such as `initial_train_size`, `return_all_indexes`, and `verbose`, setting the size of training and testing sets according to the defined parameters. It requires a valid index extracted from the input data X using the `_extract_index` method. The returned indices can be used directly to slice the input data for training and testing purposes.\"\"\"\n        \"\\n        Split the time series data into train and test folds.\\n\\n        Parameters\\n        ----------\\n        X : pandas Series, DataFrame, Index, or dictionary\\n            Time series data or index to split.\\n        as_pandas : bool, default `False`\\n            If True, the folds are returned as a DataFrame. This is useful to visualize\\n            the folds in a more interpretable way.\\n        externally_fitted : Any\\n            This argument is not used in this class. It is included for API consistency.\\n        \\n        Returns\\n        -------\\n        fold : list, pandas DataFrame\\n            A list of lists containing the indices (position) for for each fold. Each list\\n            contains 2 lists the following information:\\n\\n            - [train_start, train_end]: list with the start and end positions of the\\n            training set.\\n            - [test_start, test_end]: list with the start and end positions of the test\\n            set. These are the observations used to evaluate the forecaster.\\n        \\n            It is important to note that the returned values are the positions of the\\n            observations and not the actual values of the index, so they can be used to\\n            slice the data directly using iloc.\\n\\n            If `as_pandas` is `True`, the folds are returned as a DataFrame with the\\n            following columns: 'fold', 'train_start', 'train_end', 'test_start', 'test_end'.\\n\\n            Following the python convention, the start index is inclusive and the end\\n            index is exclusive. This means that the last index is not included in the\\n            slice.\\n        \\n        \"\n        if not isinstance(X, (pd.Series, pd.DataFrame, pd.Index, dict)):\n            raise TypeError(f'X must be a pandas Series, DataFrame, Index or a dictionary. Got {type(X)}.')\n        index = self._extract_index(X)\n        fold = [[0, self.initial_train_size], [self.initial_train_size, len(X)], True]\n        if self.verbose:\n            self._print_info(index=index, fold=fold)\n        if self.return_all_indexes:\n            fold = [[range(fold[0][0], fold[0][1])], [range(fold[1][0], fold[1][1])], fold[2]]\n        if as_pandas:\n            if not self.return_all_indexes:\n                fold = pd.DataFrame(data=[list(itertools.chain(*fold[:-1])) + [fold[-1]]], columns=['train_start', 'train_end', 'test_start', 'test_end', 'fit_forecaster'])\n            else:\n                fold = pd.DataFrame(data=[fold], columns=['train_index', 'test_index', 'fit_forecaster'])\n            fold.insert(0, 'fold', range(len(fold)))\n        return fold",
        "docstring": "Split the time series data into training and testing folds for one-step-ahead forecasting.\n\nParameters\n----------\nX : Union[pd.Series, pd.DataFrame, pd.Index, dict]\n    Time series data or index to split into folds.\nas_pandas : bool, default `False`\n    If `True`, returns the folds as a pandas DataFrame for better interpretability.\nexternally_fitted : Any\n    This parameter is not utilized in this method and is included only for API consistency.\n\nReturns\n-------\nUnion[list, pd.DataFrame]\n    A list or DataFrame containing the indices for each fold. Each entry includes:\n    - [train_start, train_end]: positions of the training set.\n    - [test_start, test_end]: positions of the test set for evaluation.\n    \n    If `as_pandas=True`, the DataFrame contains columns: 'fold', 'train_start', \n    'train_end', 'test_start', and 'test_end'.\n\nNotes\n-----\nThe method interacts with the instance attributes such as `initial_train_size`, `return_all_indexes`, and `verbose`, setting the size of training and testing sets according to the defined parameters. It requires a valid index extracted from the input data X using the `_extract_index` method. The returned indices can be used directly to slice the input data for training and testing purposes.",
        "signature": "def split(self, X: Union[pd.Series, pd.DataFrame, pd.Index, dict], as_pandas: bool=False, externally_fitted: Any=None) -> Union[list, pd.DataFrame]:",
        "type": "Method",
        "class_signature": "class OneStepAheadFold(BaseFold):"
      },
      "OneStepAheadFold._print_info": {
        "code": "    def _print_info(self, index: pd.Index, fold: list) -> None:\n        \"\"\"Prints detailed information about the train and test folds generated for one-step-ahead forecasting.\n\nParameters\n----------\nindex : pd.Index\n    The index of the input time series, which corresponds to the observations used in the folds.\nfold : list\n    A list containing the indices for the training and test sets, where the first sublist represents the training set index and the second sublist indicates the test set index.\n\nReturns\n-------\nNone\n\nThis method outputs information regarding the number of observations in both the training and test sets, along with their respective start and end dates based on the provided index. It also incorporates the differentiation value when applicable to determine the training size. The differentiation attribute is set during the initialization of the OneStepAheadFold class and is used to modify the effective size of the training set printed in this method.\"\"\"\n        '\\n        Print information about folds.\\n        '\n        if self.differentiation is None:\n            differentiation = 0\n        else:\n            differentiation = self.differentiation\n        initial_train_size = self.initial_train_size - differentiation\n        test_length = len(index) - (initial_train_size + differentiation)\n        print('Information of folds')\n        print('--------------------')\n        print(f'Number of observations in train: {initial_train_size}')\n        if self.differentiation is not None:\n            print(f'    First {differentiation} observation/s in training set are used for differentiation')\n        print(f'Number of observations in test: {test_length}')\n        training_start = index[fold[0][0] + differentiation]\n        training_end = index[fold[0][-1]]\n        test_start = index[fold[1][0]]\n        test_end = index[fold[1][-1] - 1]\n        print(f'Training : {training_start} -- {training_end} (n={initial_train_size})')\n        print(f'Test     : {test_start} -- {test_end} (n={test_length})')\n        print('')",
        "docstring": "Prints detailed information about the train and test folds generated for one-step-ahead forecasting.\n\nParameters\n----------\nindex : pd.Index\n    The index of the input time series, which corresponds to the observations used in the folds.\nfold : list\n    A list containing the indices for the training and test sets, where the first sublist represents the training set index and the second sublist indicates the test set index.\n\nReturns\n-------\nNone\n\nThis method outputs information regarding the number of observations in both the training and test sets, along with their respective start and end dates based on the provided index. It also incorporates the differentiation value when applicable to determine the training size. The differentiation attribute is set during the initialization of the OneStepAheadFold class and is used to modify the effective size of the training set printed in this method.",
        "signature": "def _print_info(self, index: pd.Index, fold: list) -> None:",
        "type": "Method",
        "class_signature": "class OneStepAheadFold(BaseFold):"
      }
    }
  },
  "dependency_dict": {
    "skforecast/model_selection/_split.py:OneStepAheadFold:__init__": {},
    "skforecast/model_selection/_split.py:BaseFold:__init__": {},
    "skforecast/model_selection/_split.py:OneStepAheadFold:split": {},
    "skforecast/model_selection/_split.py:BaseFold:_extract_index": {},
    "skforecast/model_selection/_split.py:OneStepAheadFold:_print_info": {}
  },
  "PRD": "# PROJECT NAME: skforecast-test_onestepaheadfold\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 skforecast/\n    \u2514\u2500\u2500 model_selection/\n        \u2514\u2500\u2500 _split.py\n            \u251c\u2500\u2500 BaseFold.__init__\n            \u251c\u2500\u2500 BaseFold._extract_index\n            \u251c\u2500\u2500 BaseFold._validate_params\n            \u251c\u2500\u2500 OneStepAheadFold.__init__\n            \u251c\u2500\u2500 OneStepAheadFold._print_info\n            \u2514\u2500\u2500 OneStepAheadFold.split\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThe module provides functionality to perform custom time-series cross-validation using a \"One Step Ahead\" splitting strategy. It enables users or developers to partition datasets into sequential time-based training and testing folds, ensuring the training set always precedes the test set while maintaining temporal integrity. Key features include configurable initial training sizes, window sizes for test folds, optional differentiation of data, and the ability to return fold outputs as pandas data structures for seamless integration into data pipelines. By addressing the challenge of proper temporal data splitting for forecasting models, this module ensures rigorous evaluation and enhances model development in time-series analysis workflows.\n\n## FILE 1: skforecast/model_selection/_split.py\n\n- CLASS METHOD: BaseFold._validate_params\n  - CLASS SIGNATURE: class BaseFold:\n  - SIGNATURE: def _validate_params(self, cv_name: str, steps: Optional[int]=None, initial_train_size: Optional[int]=None, window_size: Optional[int]=None, differentiation: Optional[int]=None, refit: Union[bool, int]=False, fixed_train_size: bool=True, gap: int=0, skip_folds: Optional[Union[int, list]]=None, allow_incomplete_fold: bool=True, return_all_indexes: bool=False, verbose: bool=True) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nValidate all input parameters to ensure correctness for time series cross-validation setups.\n\nParameters\n----------\ncv_name : str\n    The name of the cross-validation class (either \"TimeSeriesFold\" or \"OneStepAheadFold\").\nsteps : Optional[int], default `None`\n    Number of observations used to be predicted in each fold (forecast horizon).\ninitial_train_size : Optional[int], default `None`\n    Number of observations used for initial training; must be greater than 0 if specified.\nwindow_size : Optional[int], default `None`\n    Number of observations needed to generate autoregressive predictors; must be greater than 0 if specified.\ndifferentiation : Optional[int], default `None`\n    Number of observations to consider for differentiation; must be non-negative if specified.\nrefit : Union[bool, int], default `False`\n    Controls whether to refit the forecaster in each fold (True for always, False for only first fold, or an integer for specific folds).\nfixed_train_size : bool, default `True`\n    Indicates whether the training size is fixed or increases in each fold.\ngap : int, default `0`\n    Number of observations between the end of training set and start of test set; must be non-negative.\nskip_folds : Optional[Union[int, list]], default `None`\n    Specifies which folds to skip; can be an integer for interval skipping or a list of fold indexes.\nallow_incomplete_fold : bool, default `True`\n    Determines if the last fold can have fewer observations than `steps`.\nreturn_all_indexes : bool, default `False`\n    Specifies whether to return all indexes or just start and end indexes of each fold.\nverbose : bool, default `True`\n    Controls printing of information about generated folds.\n\nRaises\n------\nValueError\n    If `steps`, `initial_train_size`, `gap` or `window_size` do not meet the specified constraints.\nTypeError\n    If `refit`, `fixed_train_size`, `allow_incomplete_fold`, or `return_all_indexes` are not of the expected types.\n\nNotes\n-----\nThe method primarily provides validation to avoid incorrect configurations that could lead to runtime errors during time series cross-validation processes.\n\"\"\"\n```\n\n- CLASS METHOD: BaseFold._extract_index\n  - CLASS SIGNATURE: class BaseFold:\n  - SIGNATURE: def _extract_index(self, X: Union[pd.Series, pd.DataFrame, pd.Index, dict]) -> pd.Index:\n  - DOCSTRING: \n```python\n\"\"\"\nExtracts and returns the index from the provided time series data or index to facilitate the creation of training and testing folds for time series forecasting.\n\nParameters\n----------\nX : Union[pd.Series, pd.DataFrame, pd.Index, dict]\n    The input data can be a pandas Series, DataFrame, Index, or a dictionary containing Series or DataFrames. If `X` is a dictionary, all contained Series must share the same time frequency.\n\nReturns\n-------\nidx : pd.Index\n    The extracted index from the input data. If `X` is a dictionary, it generates a date range covering the earliest and latest timestamps using the common frequency.\n\nRaises\n------\nValueError\n    - If `X` is a dictionary and none of the Series have a defined frequency.\n    - If the Series within the dictionary have different frequencies.\n\nThis method is crucial for preparing the data for splitting into training and test sets within subclasses like `OneStepAheadFold` and `TimeSeriesFold`, where the structure of the input index is essential for ensuring correct temporal alignment of the data.\n\"\"\"\n```\n\n- CLASS METHOD: OneStepAheadFold.split\n  - CLASS SIGNATURE: class OneStepAheadFold(BaseFold):\n  - SIGNATURE: def split(self, X: Union[pd.Series, pd.DataFrame, pd.Index, dict], as_pandas: bool=False, externally_fitted: Any=None) -> Union[list, pd.DataFrame]:\n  - DOCSTRING: \n```python\n\"\"\"\nSplit the time series data into training and testing folds for one-step-ahead forecasting.\n\nParameters\n----------\nX : Union[pd.Series, pd.DataFrame, pd.Index, dict]\n    Time series data or index to split into folds.\nas_pandas : bool, default `False`\n    If `True`, returns the folds as a pandas DataFrame for better interpretability.\nexternally_fitted : Any\n    This parameter is not utilized in this method and is included only for API consistency.\n\nReturns\n-------\nUnion[list, pd.DataFrame]\n    A list or DataFrame containing the indices for each fold. Each entry includes:\n    - [train_start, train_end]: positions of the training set.\n    - [test_start, test_end]: positions of the test set for evaluation.\n    \n    If `as_pandas=True`, the DataFrame contains columns: 'fold', 'train_start', \n    'train_end', 'test_start', and 'test_end'.\n\nNotes\n-----\nThe method interacts with the instance attributes such as `initial_train_size`, `return_all_indexes`, and `verbose`, setting the size of training and testing sets according to the defined parameters. It requires a valid index extracted from the input data X using the `_extract_index` method. The returned indices can be used directly to slice the input data for training and testing purposes.\n\"\"\"\n```\n\n- CLASS METHOD: OneStepAheadFold._print_info\n  - CLASS SIGNATURE: class OneStepAheadFold(BaseFold):\n  - SIGNATURE: def _print_info(self, index: pd.Index, fold: list) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nPrints detailed information about the train and test folds generated for one-step-ahead forecasting.\n\nParameters\n----------\nindex : pd.Index\n    The index of the input time series, which corresponds to the observations used in the folds.\nfold : list\n    A list containing the indices for the training and test sets, where the first sublist represents the training set index and the second sublist indicates the test set index.\n\nReturns\n-------\nNone\n\nThis method outputs information regarding the number of observations in both the training and test sets, along with their respective start and end dates based on the provided index. It also incorporates the differentiation value when applicable to determine the training size. The differentiation attribute is set during the initialization of the OneStepAheadFold class and is used to modify the effective size of the training set printed in this method.\n\"\"\"\n```\n\n- CLASS METHOD: OneStepAheadFold.__init__\n  - CLASS SIGNATURE: class OneStepAheadFold(BaseFold):\n  - SIGNATURE: def __init__(self, initial_train_size: int, window_size: Optional[int]=None, differentiation: Optional[int]=None, return_all_indexes: bool=False, verbose: bool=True) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nInitialize the OneStepAheadFold object, which is designed to split time series data into training and testing folds specifically for one-step-ahead forecasting.\n\nParameters\n----------\ninitial_train_size : int\n    The number of observations to be used for initial training.\nwindow_size : Optional[int], default `None`\n    The number of observations required to construct autoregressive predictors. If `None`, no autoregressive predictors will be generated.\ndifferentiation : Optional[int], default `None`\n    The number of observations used for differentiation, extending the last window based on the differentiation order. If `None`, differentiation is not applied.\nreturn_all_indexes : bool, default `False`\n    If set to `True`, all indexes of the folds will be returned; otherwise, only the start and end indexes will be provided.\nverbose : bool, default `True`\n    If `True`, additional information about the folds will be printed during processing.\n\nReturns\n-------\nNone\n\nNotes\n-----\nThis constructor calls the parent class BaseFold's `__init__` method to initialize the base parameters related to time series folding. It ensures that the object is configured correctly for performing time series cross-validation tailored for one-step-ahead forecasting, leveraging various parameters such as initial training size, window size, and differentiation. Attributes like `initial_train_size`, `window_size`, and `differentiation` are directly set and utilized in the fold generation process for validating and predicting time series models.\n\"\"\"\n```\n\n- CLASS METHOD: BaseFold.__init__\n  - CLASS SIGNATURE: class BaseFold:\n  - SIGNATURE: def __init__(self, steps: Optional[int]=None, initial_train_size: Optional[int]=None, window_size: Optional[int]=None, differentiation: Optional[int]=None, refit: Union[bool, int]=False, fixed_train_size: bool=True, gap: int=0, skip_folds: Optional[Union[int, list]]=None, allow_incomplete_fold: bool=True, return_all_indexes: bool=False, verbose: bool=True) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nInitializes a BaseFold object, which serves as a base class for creating folds in time series cross-validation. This constructor validates input parameters related to the split configuration and assigns them to instance attributes.\n\nParameters\n----------\nsteps : Optional[int], default `None`\n    The number of observations to predict in each fold (forecast horizon/test size).\ninitial_train_size : Optional[int], default `None`\n    The number of observations for initial training.\nwindow_size : Optional[int], default `None`\n    The size of the window necessary for generating autoregressive predictors.\ndifferentiation : Optional[int], default `None`\n    Number of observations used for differentiation, allowing to extend the last window.\nrefit : Union[bool, int], default `False`\n    Control whether to refit the forecaster in each fold, either a boolean or integer for specific intervals.\nfixed_train_size : bool, default `True`\n    Indicates if training size remains constant or increases with each fold.\ngap : int, default `0`\n    Number of observations between the end of the training set and the start of the test set.\nskip_folds : Optional[Union[int, list]], default `None`\n    Number of folds to skip or list of specific fold indexes to skip.\nallow_incomplete_fold : bool, default `True`\n    Enables/disables the inclusion of incomplete folds.\nreturn_all_indexes : bool, default `False`\n    Determines if all indexes or only start/end indexes of each fold are returned.\nverbose : bool, default `True`\n    Activates verbose mode to print fold information during generation.\n\nReturn\n------\nNone\n\nSide Effects\n------------\nValidates parameters for correctness and raises exceptions for invalid values. This initialization prepares the instance for generating folds based on time series data using methods defined in this and inherited classes.\n\"\"\"\n```\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "skforecast/model_selection/_split.py": "from copy import deepcopy\nfrom typing import Union, Optional, Any\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport itertools\nfrom ..exceptions import IgnoredArgumentWarning\n\nclass BaseFold:\n    \"\"\"\n    Base class for all Fold classes in skforecast. All fold classes should specify\n    all the parameters that can be set at the class level in their ``__init__``.\n\n    Parameters\n    ----------\n    steps : int, default `None`\n        Number of observations used to be predicted in each fold. This is also commonly\n        referred to as the forecast horizon or test size.\n    initial_train_size : int, default `None`\n        Number of observations used for initial training.\n    window_size : int, default `None`\n        Number of observations needed to generate the autoregressive predictors.\n    differentiation : int, default `None`\n        Number of observations to use for differentiation. This is used to extend the\n        `last_window` as many observations as the differentiation order.\n    refit : bool, int, default `False`\n        Whether to refit the forecaster in each fold.\n\n        - If `True`, the forecaster is refitted in each fold.\n        - If `False`, the forecaster is trained only in the first fold.\n        - If an integer, the forecaster is trained in the first fold and then refitted\n          every `refit` folds.\n    fixed_train_size : bool, default `True`\n        Whether the training size is fixed or increases in each fold.\n    gap : int, default `0`\n        Number of observations between the end of the training set and the start of the\n        test set.\n    skip_folds : int, list, default `None`\n        Number of folds to skip.\n\n        - If an integer, every 'skip_folds'-th is returned.\n        - If a list, the indexes of the folds to skip.\n\n        For example, if `skip_folds=3` and there are 10 folds, the returned folds are\n        0, 3, 6, and 9. If `skip_folds=[1, 2, 3]`, the returned folds are 0, 4, 5, 6, 7,\n        8, and 9.\n    allow_incomplete_fold : bool, default `True`\n        Whether to allow the last fold to include fewer observations than `steps`.\n        If `False`, the last fold is excluded if it is incomplete.\n    return_all_indexes : bool, default `False`\n        Whether to return all indexes or only the start and end indexes of each fold.\n    verbose : bool, default `True`\n        Whether to print information about generated folds.\n\n    Attributes\n    ----------\n    steps : int\n        Number of observations used to be predicted in each fold. This is also commonly\n        referred to as the forecast horizon or test size.\n    initial_train_size : int\n        Number of observations used for initial training.\n    window_size : int\n        Number of observations needed to generate the autoregressive predictors.\n    differentiation : int\n        Number of observations to use for differentiation. This is used to extend the\n        `last_window` as many observations as the differentiation order.\n    refit : bool, int\n        Whether to refit the forecaster in each fold.\n    fixed_train_size : bool\n        Whether the training size is fixed or increases in each fold.\n    gap : int\n        Number of observations between the end of the training set and the start of the\n        test set.\n    skip_folds : int, list\n        Number of folds to skip.\n    allow_incomplete_fold : bool\n        Whether to allow the last fold to include fewer observations than `steps`.\n    return_all_indexes : bool\n        Whether to return all indexes or only the start and end indexes of each fold.\n    verbose : bool\n        Whether to print information about generated folds.\n\n    \"\"\"\n\n    def set_params(self, params: dict) -> None:\n        \"\"\"\n        Set the parameters of the Fold object. Before overwriting the current \n        parameters, the input parameters are validated to ensure correctness.\n\n        Parameters\n        ----------\n        params : dict\n            Dictionary with the parameters to set.\n        \n        Returns\n        -------\n        None\n        \n        \"\"\"\n        if not isinstance(params, dict):\n            raise TypeError(f'`params` must be a dictionary. Got {type(params)}.')\n        current_params = deepcopy(vars(self))\n        unknown_params = set(params.keys()) - set(current_params.keys())\n        if unknown_params:\n            warnings.warn(f'Unknown parameters: {unknown_params}. They have been ignored.', IgnoredArgumentWarning)\n        filtered_params = {k: v for k, v in params.items() if k in current_params}\n        updated_params = {'cv_name': type(self).__name__, **current_params, **filtered_params}\n        self._validate_params(**updated_params)\n        for key, value in updated_params.items():\n            setattr(self, key, value)\n\nclass OneStepAheadFold(BaseFold):\n    \"\"\"\n    Class to split time series data into train and test folds for one-step-ahead\n    forecasting.\n\n    Parameters\n    ----------\n    initial_train_size : int\n        Number of observations used for initial training.\n    window_size : int, default `None`\n        Number of observations needed to generate the autoregressive predictors.\n    differentiation : int, default `None`\n        Number of observations to use for differentiation. This is used to extend the\n        `last_window` as many observations as the differentiation order.\n    return_all_indexes : bool, default `False`\n        Whether to return all indexes or only the start and end indexes of each fold.\n    verbose : bool, default `True`\n        Whether to print information about generated folds.\n\n    Attributes\n    ----------\n    initial_train_size : int\n        Number of observations used for initial training.\n    window_size : int\n        Number of observations needed to generate the autoregressive predictors.\n    differentiation : int \n        Number of observations to use for differentiation. This is used to extend the\n        `last_window` as many observations as the differentiation order.\n    return_all_indexes : bool\n        Whether to return all indexes or only the start and end indexes of each fold.\n    verbose : bool\n        Whether to print information about generated folds.\n    steps : Any\n        This attribute is not used in this class. It is included for API consistency.\n    fixed_train_size : Any\n        This attribute is not used in this class. It is included for API consistency.\n    gap : Any\n        This attribute is not used in this class. It is included for API consistency.\n    skip_folds : Any\n        This attribute is not used in this class. It is included for API consistency.\n    allow_incomplete_fold : Any\n        This attribute is not used in this class. It is included for API consistency.\n    refit : Any\n        This attribute is not used in this class. It is included for API consistency.\n    \n    \"\"\"\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Information displayed when printed.\n        \"\"\"\n        return f'OneStepAheadFold(\\n    initial_train_size = {self.initial_train_size},\\n    window_size        = {self.window_size},\\n    differentiation    = {self.differentiation},\\n    return_all_indexes = {self.return_all_indexes},\\n    verbose            = {self.verbose}\\n)'\n\nclass TimeSeriesFold(BaseFold):\n    \"\"\"\n    Class to split time series data into train and test folds. \n    When used within a backtesting or hyperparameter search, the arguments\n    'initial_train_size', 'window_size' and 'differentiation' are not required\n    as they are automatically set by the backtesting or hyperparameter search\n    functions.\n\n    Parameters\n    ----------\n    steps : int\n        Number of observations used to be predicted in each fold. This is also commonly\n        referred to as the forecast horizon or test size.\n    initial_train_size : int, default `None`\n        Number of observations used for initial training. If `None` or 0, the initial\n        forecaster is not trained in the first fold.\n    window_size : int, default `None`\n        Number of observations needed to generate the autoregressive predictors.\n    differentiation : int, default `None`\n        Number of observations to use for differentiation. This is used to extend the\n        `last_window` as many observations as the differentiation order.\n    refit : bool, int, default `False`\n        Whether to refit the forecaster in each fold.\n\n        - If `True`, the forecaster is refitted in each fold.\n        - If `False`, the forecaster is trained only in the first fold.\n        - If an integer, the forecaster is trained in the first fold and then refitted\n          every `refit` folds.\n    fixed_train_size : bool, default `True`\n        Whether the training size is fixed or increases in each fold.\n    gap : int, default `0`\n        Number of observations between the end of the training set and the start of the\n        test set.\n    skip_folds : int, list, default `None`\n        Number of folds to skip.\n\n        - If an integer, every 'skip_folds'-th is returned.\n        - If a list, the indexes of the folds to skip.\n\n        For example, if `skip_folds=3` and there are 10 folds, the returned folds are\n        0, 3, 6, and 9. If `skip_folds=[1, 2, 3]`, the returned folds are 0, 4, 5, 6, 7,\n        8, and 9.\n    allow_incomplete_fold : bool, default `True`\n        Whether to allow the last fold to include fewer observations than `steps`.\n        If `False`, the last fold is excluded if it is incomplete.\n    return_all_indexes : bool, default `False`\n        Whether to return all indexes or only the start and end indexes of each fold.\n    verbose : bool, default `True`\n        Whether to print information about generated folds.\n\n    Attributes\n    ----------\n    steps : int\n        Number of observations used to be predicted in each fold. This is also commonly\n        referred to as the forecast horizon or test size.\n    initial_train_size : int\n        Number of observations used for initial training. If `None` or 0, the initial\n        forecaster is not trained in the first fold.\n    window_size : int\n        Number of observations needed to generate the autoregressive predictors.\n    differentiation : int\n        Number of observations to use for differentiation. This is used to extend the\n        `last_window` as many observations as the differentiation order.\n    refit : bool, int\n        Whether to refit the forecaster in each fold.\n    fixed_train_size : bool\n        Whether the training size is fixed or increases in each fold.\n    gap : int\n        Number of observations between the end of the training set and the start of the\n        test set.\n    skip_folds : int, list\n        Number of folds to skip.\n    allow_incomplete_fold : bool\n        Whether to allow the last fold to include fewer observations than `steps`.\n    return_all_indexes : bool\n        Whether to return all indexes or only the start and end indexes of each fold.\n    verbose : bool\n        Whether to print information about generated folds.\n\n    Notes\n    -----\n    Returned values are the positions of the observations and not the actual values of\n    the index, so they can be used to slice the data directly using iloc. For example,\n    if the input series is `X = [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]`, the \n    `initial_train_size = 3`, `window_size = 2`, `steps = 4`, and `gap = 1`,\n    the output of the first fold will: [[0, 3], [1, 3], [3, 8], [4, 8], True].\n\n    The first list `[0, 3]` indicates that the training set goes from the first to the\n    third observation. The second list `[1, 3]` indicates that the last window seen by\n    the forecaster during training goes from the second to the third observation. The\n    third list `[3, 8]` indicates that the test set goes from the fourth to the eighth\n    observation. The fourth list `[4, 8]` indicates that the test set including the gap\n    goes from the fifth to the eighth observation. The boolean `False` indicates that the\n    forecaster should not be trained in this fold.\n\n    Following the python convention, the start index is inclusive and the end index is\n    exclusive. This means that the last index is not included in the slice.\n\n    \"\"\"\n\n    def __init__(self, steps: int, initial_train_size: Optional[int]=None, window_size: Optional[int]=None, differentiation: Optional[int]=None, refit: Union[bool, int]=False, fixed_train_size: bool=True, gap: int=0, skip_folds: Optional[Union[int, list]]=None, allow_incomplete_fold: bool=True, return_all_indexes: bool=False, verbose: bool=True) -> None:\n        super().__init__(steps=steps, initial_train_size=initial_train_size, window_size=window_size, differentiation=differentiation, refit=refit, fixed_train_size=fixed_train_size, gap=gap, skip_folds=skip_folds, allow_incomplete_fold=allow_incomplete_fold, return_all_indexes=return_all_indexes, verbose=verbose)\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Information displayed when printed.\n        \"\"\"\n        return f'TimeSeriesFold(\\n    steps                 = {self.steps},\\n    initial_train_size    = {self.initial_train_size},\\n    window_size           = {self.window_size},\\n    differentiation       = {self.differentiation},\\n    refit                 = {self.refit},\\n    fixed_train_size      = {self.fixed_train_size},\\n    gap                   = {self.gap},\\n    skip_folds            = {self.skip_folds},\\n    allow_incomplete_fold = {self.allow_incomplete_fold},\\n    return_all_indexes    = {self.return_all_indexes},\\n    verbose               = {self.verbose}\\n)'\n\n    def split(self, X: Union[pd.Series, pd.DataFrame, pd.Index, dict], as_pandas: bool=False) -> Union[list, pd.DataFrame]:\n        \"\"\"\n        Split the time series data into train and test folds.\n\n        Parameters\n        ----------\n        X : pandas Series, pandas DataFrame, pandas Index, dict\n            Time series data or index to split.\n        as_pandas : bool, default `False`\n            If True, the folds are returned as a DataFrame. This is useful to visualize\n            the folds in a more interpretable way.\n\n        Returns\n        -------\n        folds : list, pandas DataFrame\n            A list of lists containing the indices (position) for for each fold. Each list\n            contains 4 lists and a boolean with the following information:\n\n            - [train_start, train_end]: list with the start and end positions of the\n            training set.\n            - [last_window_start, last_window_end]: list with the start and end positions\n            of the last window seen by the forecaster during training. The last window\n            is used to generate the lags use as predictors. If `differentiation` is\n            included, the interval is extended as many observations as the\n            differentiation order. If the argument `window_size` is `None`, this list is\n            empty.\n            - [test_start, test_end]: list with the start and end positions of the test\n            set. These are the observations used to evaluate the forecaster.\n            - [test_start_with_gap, test_end_with_gap]: list with the start and end\n            positions of the test set including the gap. The gap is the number of\n            observations between the end of the training set and the start of the test\n            set.\n            - fit_forecaster: boolean indicating whether the forecaster should be fitted\n            in this fold.\n\n            It is important to note that the returned values are the positions of the\n            observations and not the actual values of the index, so they can be used to\n            slice the data directly using iloc.\n\n            If `as_pandas` is `True`, the folds are returned as a DataFrame with the\n            following columns: 'fold', 'train_start', 'train_end', 'last_window_start',\n            'last_window_end', 'test_start', 'test_end', 'test_start_with_gap',\n            'test_end_with_gap', 'fit_forecaster'.\n\n            Following the python convention, the start index is inclusive and the end\n            index is exclusive. This means that the last index is not included in the\n            slice.\n\n        \"\"\"\n        if not isinstance(X, (pd.Series, pd.DataFrame, pd.Index, dict)):\n            raise TypeError(f'X must be a pandas Series, DataFrame, Index or a dictionary. Got {type(X)}.')\n        if isinstance(self.window_size, pd.tseries.offsets.DateOffset):\n            first_valid_index = X.index[-1] - self.window_size\n            try:\n                window_size_idx_start = X.index.get_loc(first_valid_index)\n                window_size_idx_end = X.index.get_loc(X.index[-1])\n                self.window_size = window_size_idx_end - window_size_idx_start\n            except KeyError:\n                raise ValueError(f'The length of `X` ({len(X)}), must be greater than or equal to the window size ({self.window_size}). Try to decrease the size of the offset (forecaster.offset), or increase the size of `y`.')\n        if self.initial_train_size is None:\n            if self.window_size is None:\n                raise ValueError('To use split method when `initial_train_size` is None, `window_size` must be an integer greater than 0. Although no initial training is done and all data is used to evaluate the model, the first `window_size` observations are needed to create the initial predictors. Got `window_size` = None.')\n            if self.refit:\n                raise ValueError('`refit` is only allowed when `initial_train_size` is not `None`. Set `refit` to `False` if you want to use `initial_train_size = None`.')\n            externally_fitted = True\n            self.initial_train_size = self.window_size\n        else:\n            if self.window_size is None:\n                warnings.warn('Last window cannot be calculated because `window_size` is None.')\n            externally_fitted = False\n        index = self._extract_index(X)\n        idx = range(len(index))\n        folds = []\n        i = 0\n        last_fold_excluded = False\n        if len(index) < self.initial_train_size + self.steps:\n            raise ValueError(f'The time series must have at least `initial_train_size + steps` observations. Got {len(index)} observations.')\n        while self.initial_train_size + i * self.steps + self.gap < len(index):\n            if self.refit:\n                train_iloc_start = i * self.steps if self.fixed_train_size else 0\n                train_iloc_end = self.initial_train_size + i * self.steps\n                test_iloc_start = train_iloc_end\n            else:\n                train_iloc_start = 0\n                train_iloc_end = self.initial_train_size\n                test_iloc_start = self.initial_train_size + i * self.steps\n            if self.window_size is not None:\n                last_window_iloc_start = test_iloc_start - self.window_size\n            test_iloc_end = test_iloc_start + self.gap + self.steps\n            partitions = [idx[train_iloc_start:train_iloc_end], idx[last_window_iloc_start:test_iloc_start] if self.window_size is not None else [], idx[test_iloc_start:test_iloc_end], idx[test_iloc_start + self.gap:test_iloc_end]]\n            folds.append(partitions)\n            i += 1\n        if not self.allow_incomplete_fold and len(folds[-1][3]) < self.steps:\n            folds = folds[:-1]\n            last_fold_excluded = True\n        folds = [[partition if len(partition) > 0 else None for partition in fold] for fold in folds]\n        if self.refit == 0:\n            self.refit = False\n        if isinstance(self.refit, bool):\n            fit_forecaster = [self.refit] * len(folds)\n            fit_forecaster[0] = True\n        else:\n            fit_forecaster = [False] * len(folds)\n            for i in range(0, len(fit_forecaster), self.refit):\n                fit_forecaster[i] = True\n        for i in range(len(folds)):\n            folds[i].append(fit_forecaster[i])\n            if fit_forecaster[i] is False:\n                folds[i][0] = folds[i - 1][0]\n        index_to_skip = []\n        if self.skip_folds is not None:\n            if isinstance(self.skip_folds, (int, np.integer)) and self.skip_folds > 0:\n                index_to_keep = np.arange(0, len(folds), self.skip_folds)\n                index_to_skip = np.setdiff1d(np.arange(0, len(folds)), index_to_keep, assume_unique=True)\n                index_to_skip = [int(x) for x in index_to_skip]\n            if isinstance(self.skip_folds, list):\n                index_to_skip = [i for i in self.skip_folds if i < len(folds)]\n        if self.verbose:\n            self._print_info(index=index, folds=folds, externally_fitted=externally_fitted, last_fold_excluded=last_fold_excluded, index_to_skip=index_to_skip)\n        folds = [fold for i, fold in enumerate(folds) if i not in index_to_skip]\n        if not self.return_all_indexes:\n            folds = [[[fold[0][0], fold[0][-1] + 1], [fold[1][0], fold[1][-1] + 1] if self.window_size is not None else [], [fold[2][0], fold[2][-1] + 1], [fold[3][0], fold[3][-1] + 1], fold[4]] for fold in folds]\n        if externally_fitted:\n            self.initial_train_size = None\n            folds[0][4] = False\n        if as_pandas:\n            if self.window_size is None:\n                for fold in folds:\n                    fold[1] = [None, None]\n            if not self.return_all_indexes:\n                folds = pd.DataFrame(data=[list(itertools.chain(*fold[:-1])) + [fold[-1]] for fold in folds], columns=['train_start', 'train_end', 'last_window_start', 'last_window_end', 'test_start', 'test_end', 'test_start_with_gap', 'test_end_with_gap', 'fit_forecaster'])\n            else:\n                folds = pd.DataFrame(data=folds, columns=['train_index', 'last_window_index', 'test_index', 'test_index_with_gap', 'fit_forecaster'])\n            folds.insert(0, 'fold', range(len(folds)))\n        return folds\n\n    def _print_info(self, index: pd.Index, folds: list, externally_fitted: bool, last_fold_excluded: bool, index_to_skip: list) -> None:\n        \"\"\"\n        Print information about folds.\n        \"\"\"\n        print('Information of folds')\n        print('--------------------')\n        if externally_fitted:\n            print(f'An already trained forecaster is to be used. Window size: {self.window_size}')\n        elif self.differentiation is None:\n            print(f'Number of observations used for initial training: {self.initial_train_size}')\n        else:\n            print(f'Number of observations used for initial training: {self.initial_train_size - self.differentiation}')\n            print(f'    First {self.differentiation} observation/s in training sets are used for differentiation')\n        print(f'Number of observations used for backtesting: {len(index) - self.initial_train_size}')\n        print(f'    Number of folds: {len(folds)}')\n        print(f'    Number skipped folds: {len(index_to_skip)} {(index_to_skip if index_to_skip else '')}')\n        print(f'    Number of steps per fold: {self.steps}')\n        print(f'    Number of steps to exclude between last observed data (last window) and predictions (gap): {self.gap}')\n        if last_fold_excluded:\n            print('    Last fold has been excluded because it was incomplete.')\n        if len(folds[-1][3]) < self.steps:\n            print(f'    Last fold only includes {len(folds[-1][3])} observations.')\n        print('')\n        if self.differentiation is None:\n            differentiation = 0\n        else:\n            differentiation = self.differentiation\n        for i, fold in enumerate(folds):\n            is_fold_skipped = i in index_to_skip\n            has_training = fold[-1] if i != 0 else True\n            training_start = index[fold[0][0] + differentiation] if fold[0] is not None else None\n            training_end = index[fold[0][-1]] if fold[0] is not None else None\n            training_length = len(fold[0]) - differentiation if fold[0] is not None else 0\n            validation_start = index[fold[3][0]]\n            validation_end = index[fold[3][-1]]\n            validation_length = len(fold[3])\n            print(f'Fold: {i}')\n            if is_fold_skipped:\n                print('    Fold skipped')\n            elif not externally_fitted and has_training:\n                print(f'    Training:   {training_start} -- {training_end}  (n={training_length})')\n                print(f'    Validation: {validation_start} -- {validation_end}  (n={validation_length})')\n            else:\n                print('    Training:   No training in this fold')\n                print(f'    Validation: {validation_start} -- {validation_end}  (n={validation_length})')\n        print('')"
  },
  "call_tree": {
    "skforecast/model_selection/tests/tests_split/test_onestepaheadfold.py:test_OneStepAheadFold_split_raise_error_when_X_is_not_series_dataframe_or_dict": {
      "skforecast/model_selection/_split.py:OneStepAheadFold:__init__": {
        "skforecast/model_selection/_split.py:BaseFold:__init__": {
          "skforecast/model_selection/_split.py:BaseFold:_validate_params": {}
        }
      },
      "skforecast/model_selection/_split.py:OneStepAheadFold:split": {}
    },
    "skforecast/model_selection/tests/tests_split/test_onestepaheadfold.py:test_OneStepAhead_split_initial_train_size_and_window_size": {
      "skforecast/model_selection/_split.py:OneStepAheadFold:__init__": {
        "skforecast/model_selection/_split.py:BaseFold:__init__": {
          "skforecast/model_selection/_split.py:BaseFold:_validate_params": {}
        }
      },
      "skforecast/model_selection/_split.py:OneStepAheadFold:split": {
        "skforecast/model_selection/_split.py:BaseFold:_extract_index": {},
        "skforecast/model_selection/_split.py:OneStepAheadFold:_print_info": {}
      }
    },
    "skforecast/model_selection/tests/tests_split/test_onestepaheadfold.py:test_OneStepAhead_split_initial_train_size_and_window_size_diferentiation_is_1": {
      "skforecast/model_selection/_split.py:OneStepAheadFold:__init__": {
        "skforecast/model_selection/_split.py:BaseFold:__init__": {
          "skforecast/model_selection/_split.py:BaseFold:_validate_params": {}
        }
      },
      "skforecast/model_selection/_split.py:OneStepAheadFold:split": {
        "skforecast/model_selection/_split.py:BaseFold:_extract_index": {},
        "skforecast/model_selection/_split.py:OneStepAheadFold:_print_info": {}
      }
    },
    "skforecast/model_selection/tests/tests_split/test_onestepaheadfold.py:test_OneStepAhead_split_initial_train_size_window_size_return_all_indexes_true_as_pandas_true": {
      "skforecast/model_selection/_split.py:OneStepAheadFold:__init__": {
        "skforecast/model_selection/_split.py:BaseFold:__init__": {
          "skforecast/model_selection/_split.py:BaseFold:_validate_params": {}
        }
      },
      "skforecast/model_selection/_split.py:OneStepAheadFold:split": {
        "skforecast/model_selection/_split.py:BaseFold:_extract_index": {},
        "skforecast/model_selection/_split.py:OneStepAheadFold:_print_info": {}
      }
    },
    "skforecast/model_selection/tests/tests_split/test_onestepaheadfold.py:test_OneStepAhead_split_initial_train_size_window_size_return_all_indexes_false_as_pandas_true": {
      "skforecast/model_selection/_split.py:OneStepAheadFold:__init__": {
        "skforecast/model_selection/_split.py:BaseFold:__init__": {
          "skforecast/model_selection/_split.py:BaseFold:_validate_params": {}
        }
      },
      "skforecast/model_selection/_split.py:OneStepAheadFold:split": {
        "skforecast/model_selection/_split.py:BaseFold:_extract_index": {},
        "skforecast/model_selection/_split.py:OneStepAheadFold:_print_info": {}
      }
    }
  }
}