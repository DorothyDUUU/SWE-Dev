{
  "dir_path": "/app/getschema",
  "package_name": "getschema",
  "sample_name": "getschema-test_types",
  "src_dir": "getschema/",
  "test_dir": "tests/",
  "test_file": "tests/test_types.py",
  "test_code": "import getschema\n\n\ndef test_null_records():\n    records = [\n        {\n            \"field\": \"1\",\n            \"null_field\": None,\n            \"array\": [\n            ],\n            \"null_array\": [\n            ],\n            \"nested_field\": {\n                \"some_date\": \"2021-05-25\",\n                \"number\": 1,\n                \"null_subfield\": None,\n            },\n        },\n        {\n            \"field\": \"10.0\",\n            \"null_field\": None,\n            \"array\": [\n                \"1\",\n                \"a\",\n            ],\n            \"null_array\": [\n            ],\n            \"nested_field\": {\n                \"some_date\": \"2021-05-25\",\n                \"integer\": 1,\n                \"number\": 1.5,\n                \"null_subfield\": None,\n            },\n        },\n    ]\n    schema = getschema.infer_schema(records)\n    assert(schema[\"properties\"][\"field\"][\"type\"] == [\"null\", \"number\"])\n    assert(schema[\"properties\"][\"null_field\"][\"type\"] == [\"null\", \"string\"])\n    assert(schema[\"properties\"][\"nested_field\"][\"properties\"][\"some_date\"][\"type\"] == [\"null\", \"string\"])\n    assert(schema[\"properties\"][\"nested_field\"][\"properties\"][\"some_date\"][\"format\"] == \"date-time\")\n    assert(schema[\"properties\"][\"nested_field\"][\"properties\"][\"integer\"][\"type\"] == [\"null\", \"integer\"])\n    assert(schema[\"properties\"][\"nested_field\"][\"properties\"][\"number\"][\"type\"] == [\"null\", \"number\"])\n    assert(schema[\"properties\"][\"nested_field\"][\"properties\"][\"null_subfield\"][\"type\"] == [\"null\", \"string\"])\n\n",
  "GT_file_code": {
    "getschema/impl.py": "#!/usr/bin/env python3\nimport argparse, csv, datetime, logging, os, re, sys\nfrom dateutil import parser as dateutil_parser\nfrom dateutil.tz import tzoffset\nimport jsonpath_ng as jsonpath\nimport simplejson as json\nimport yaml\n\n# JSON schema follows:\n# https://json-schema.org/\nCOMMAND = \"json2schema\"\n\nLOGGER = logging.getLogger(__name__)\nDEFAULT_TYPE = [\"null\", \"string\"]\n\n\ndef _convert_key(old_key, lower=False, replace_special=False, snake_case=False):\n    new_key = old_key\n    if lower:\n        new_key = new_key.lower()\n    if replace_special:\n        new_key = re.sub('[^ a-zA-Z0-9_]', '_', new_key)\n    if snake_case:\n        new_key = new_key.strip().replace(\" \", \"_\")\n    return new_key\n\n\ndef _get_jsonpath(raw, path):\n    jsonpath_expr = jsonpath.parse(path)\n    record = [match.value for match in jsonpath_expr.find(raw)]\n    return record\n\n\ndef _is_datetime(obj):\n    # TODO: This is a very loose regex for date-time.\n    return (\n        type(obj) is datetime.datetime or\n        type(obj) is datetime.date or\n        (type(obj) is str and\n         re.match(\"(19|20)\\d\\d-(0[1-9]|1[012])-([1-9]|0[1-9]|[12][0-9]|3[01])\",\n                  obj) is not None)\n    )\n\n\ndef _do_infer_schema(obj, record_level=None, lower=False,\n                     replace_special=False, snake_case=False):\n    schema = dict()\n\n    # Go down to the record level if specified\n    if record_level:\n        obj = _get_jsonpath(obj, record_level)[0]\n\n    if obj is None:\n        schema[\"type\"] = [\"null\"]\n    elif type(obj) is dict and obj.keys():\n        schema[\"type\"] = [\"null\", \"object\"]\n        schema[\"properties\"] = dict()\n        for key in obj.keys():\n            ret = _do_infer_schema(obj[key])\n            new_key = _convert_key(\n                key, lower=lower, replace_special=replace_special,\n                snake_case=snake_case)\n            if ret:\n                schema[\"properties\"][new_key] = ret\n\n    elif type(obj) is list:\n        schema[\"type\"] = [\"null\", \"array\"]\n        if not obj:\n            schema[\"items\"] = None\n        # TODO: Check more than the first record\n        else:\n            ret = _do_infer_schema(\n                obj[0], lower=lower, replace_special=replace_special,\n                snake_case=snake_case)\n            schema[\"items\"] = ret\n    else:\n        try:\n            float(obj)\n        except (ValueError, TypeError):\n            schema[\"type\"] = [\"null\", \"string\"]\n            if _is_datetime(obj):\n                schema[\"format\"] = \"date-time\"\n        else:\n            if type(obj) == bool:\n                schema[\"type\"] = [\"null\", \"boolean\"]\n            elif type(obj) == float or (type(obj) == str and \".\" in obj):\n                schema[\"type\"] = [\"null\", \"number\"]\n            # Let's assume it's a code such as zipcode if there is a leading 0\n            elif type(obj) == int or (type(obj) == str and obj[0] != \"0\"):\n                schema[\"type\"] = [\"null\", \"integer\"]\n            else:\n                schema[\"type\"] = [\"null\", \"string\"]\n    return schema\n\n\ndef _compare_props(prop1, prop2):\n    if not prop2 or prop2.get(\"type\") == [\"null\"]:\n        return prop1\n    elif not prop1 or prop1.get(\"type\") == [\"null\"]:\n        return prop2\n    prop = prop2\n    t1 = prop1[\"type\"]\n    t2 = prop2[\"type\"]\n    f1 = prop1.get(\"format\")\n    f2 = prop2.get(\"format\")\n    if t1[1] == \"object\":\n        if t1[1] != t2[1]:\n            raise ValueError(\n                \"While traversing object %s, two records differ in types: %s %s\" %\n                (prop, t1[1], t2[1]))\n        for key in prop[\"properties\"]:\n            prop[\"properties\"][key] = _compare_props(\n                prop1[\"properties\"].get(key), prop2[\"properties\"].get(key))\n    if t1[1] == \"array\":\n        if t1[1] != t2[1]:\n            raise ValueError(\n                \"While traversing array %s, two records differ in types: %s %s\" %\n                       (prop, t1[1], t2[1]))\n        prop[\"items\"] = _compare_props(prop1[\"items\"], prop2[\"items\"])\n\n    numbers = [\"integer\", \"number\"]\n    if not (t1[1] == t2[1] and f1 == f2):\n        if t1[1] in numbers and t2[1] in numbers:\n            prop[\"type\"] = [\"null\", \"number\"]\n        else:\n            prop[\"type\"] = [\"null\", \"string\"]\n            if \"format\" in prop.keys():\n                prop.pop(\"format\")\n\n    return prop\n\n\ndef _infer_from_two(schema1, schema2):\n    \"\"\"\n    Compare between currently the most conservative and the new record schemas\n    and keep the more conservative one.\n    \"\"\"\n    if schema1 is None:\n        return schema2\n    if schema2 is None:\n        return schema1\n    schema = schema2\n    for key in schema1[\"properties\"]:\n        prop1 = schema1[\"properties\"][key]\n        prop2 = schema2[\"properties\"].get(key, prop1)\n        try:\n            schema[\"properties\"][key] = _compare_props(prop1, prop2)\n        except Exception as e:\n            raise Exception(\"Key: %s\\n%s\" % (key, e))\n    return schema\n\n\ndef _replace_null_type(schema, path=\"\"):\n    new_schema = {}\n    new_schema.update(schema)\n    if schema[\"type\"] in (\"object\", [\"object\"], [\"null\", \"object\"]):\n        new_schema [\"properties\"] = {}\n        for key in schema.get(\"properties\", {}).keys():\n            new_path = path + \".\" + key\n            new_schema[\"properties\"][key] = _replace_null_type(schema[\"properties\"][key], new_path)\n    elif schema[\"type\"] == [\"null\", \"array\"]:\n        if not schema.get(\"items\"):\n            new_schema[\"items\"] = {}\n        if new_schema[\"items\"].get(\"type\") in (None, \"null\", [\"null\"]):\n            LOGGER.warning(\n                f\"{path} is an array without non-null values.\"\n                f\"Replacing with the default {DEFAULT_TYPE}\")\n            new_schema[\"items\"][\"type\"] = DEFAULT_TYPE\n    elif schema[\"type\"] == [\"null\"]:\n        LOGGER.warning(f\"{path} contained non-null values only. Replacing with the default {DEFAULT_TYPE}\")\n        new_schema[\"type\"] = DEFAULT_TYPE\n    return new_schema\n\n\ndef _nested_get(input_dict, nested_key):\n    internal_dict_value = input_dict\n    for k in nested_key:\n        internal_dict_value = internal_dict_value.get(k, None)\n        if internal_dict_value is None:\n            return None\n    return internal_dict_value\n\n\ndef _parse_datetime_tz(datetime_str, default_tz_offset=0):\n    d = dateutil_parser.parse(datetime_str)\n    if not d.tzinfo:\n        d = d.replace(tzinfo=tzoffset(None, default_tz_offset))\n    return d\n\n\ndef _on_invalid_property(policy, dict_path, obj_type, obj, err_msg):\n    if policy == \"raise\":\n        raise Exception(err_msg + \" dict_path\" + str(dict_path) +\n                        \" object type: \" + obj_type + \" object: \" + str(obj))\n    elif policy == \"force\":\n        cleaned = str(obj)\n    elif policy == \"null\":\n        cleaned = None\n    else:\n        raise ValueError(\"Unknown policy: %s\" % policy)\n    return cleaned\n\n\ndef infer_schema(obj, record_level=None,\n                 lower=False, replace_special=False, snake_case=False):\n    \"\"\"Infer schema from a given object or a list of objects\n    - record_level:\n    - lower: Convert the key to all lower case\n    - replace_special: Replace letters to _ if not 0-9, A-Z, a-z, _ and -, or \" \"\n    - snake_case: Replace space to _\n    \"\"\"\n    if type(obj) is not list:\n        obj = [obj]\n    if type(obj[0]) is not dict:\n        raise ValueError(\"Input must be a dict object.\")\n    schema = None\n    # Go through the list of objects and find the most safe type assumption\n    for o in obj:\n        cur_schema = _do_infer_schema(\n            o, record_level, lower, replace_special, snake_case)\n        # Compare between currently the most conservative and the new record\n        # and keep the more conservative.\n        schema = _infer_from_two(schema, cur_schema)\n\n    schema[\"type\"] = \"object\"\n\n    schema = _replace_null_type(schema)\n\n    LOGGER.info(f\"Inference completed from {len(obj)} records\")\n\n    return schema\n\n\ndef infer_from_json_file(filename, skip=0, lower=False, replace_special=False,\n                         snake_case=False):\n    with open(filename, \"r\") as f:\n        content = f.read()\n    data = json.loads(content)\n    if type(data) is list:\n        data = data[skip:]\n    schema = infer_schema(data, lower=lower, replace_special=replace_special,\n                          snake_case=snake_case)\n\n    return schema\n\n\ndef infer_from_yaml_file(filename, skip=0, lower=False, replace_special=False,\n                         snake_case=False):\n    with open(filename, \"r\") as f:\n        content = f.read()\n    data = yaml.load(content, Loader=yaml.FullLoader)\n    if type(data) is list:\n        data = data[skip:]\n    schema = infer_schema(data, lower=lower, replace_special=replace_special,\n                          snake_case=snake_case)\n\n    return schema\n\n\ndef infer_from_csv_file(filename, skip=0, lower=False, replace_special=False,\n                        snake_case=False):\n    with open(filename) as f:\n        count = 0\n        while count < skip:\n            count = count + 1\n            f.readline()\n        reader = csv.DictReader(f)\n        data = [dict(row) for row in reader]\n    schema = infer_schema(data, lower=lower, replace_special=replace_special,\n                          snake_case=snake_case)\n\n    return schema\n\n\ndef infer_from_file(filename, fmt=\"json\", skip=0, lower=False,\n                    replace_special=False, snake_case=False):\n    if fmt == \"json\":\n        schema = infer_from_json_file(\n            filename, skip, lower, replace_special, snake_case)\n    elif fmt == \"yaml\":\n        schema = infer_from_yaml_file(\n            filename, skip, lower, replace_special, snake_case)\n    elif fmt == \"csv\":\n        schema = infer_from_csv_file(\n            filename, skip, lower, replace_special, snake_case)\n    else:\n        raise KeyError(\"Unsupported format : \" + fmt)\n    return schema\n\n\nclass DroppedProperty(object):\n    pass\n\n\ndef fix_type(\n        obj,\n        schema,\n        dict_path=[],\n        on_invalid_property=\"raise\",\n        drop_unknown_properties=False,\n        lower=False,\n        replace_special=False,\n        snake_case=False,\n        date_to_datetime=False,\n    ):\n    \"\"\"Convert the fields into the proper object types.\n    e.g. {\"number\": \"1.0\"} -> {\"number\": 1.0}\n\n    - on_invalid_property: [\"raise\", \"null\", \"force\"]\n      What to do when the value cannot be converted.\n      - raise: Raise exception\n      - null: Impute with null\n      - force: Keep it as is (string)\n    - drop_unknown_properties: True/False\n      If true, the returned object will exclude unknown (sub-)properties\n    \"\"\"\n    kwargs = {\n        \"on_invalid_property\": on_invalid_property,\n        \"drop_unknown_properties\": drop_unknown_properties,\n        \"lower\": lower,\n        \"replace_special\": replace_special,\n        \"snake_case\": snake_case,\n        \"date_to_datetime\": date_to_datetime,\n    }\n    invalid_actions = [\"raise\", \"null\", \"force\"]\n    if on_invalid_property not in invalid_actions:\n        raise ValueError(\n            \"on_invalid_property is not one of %s\" % invalid_actions)\n\n    obj_type = _nested_get(schema, dict_path + [\"type\"])\n    obj_format = _nested_get(schema, dict_path + [\"format\"])\n\n    nullable = False\n    if obj_type is None:\n        if on_invalid_property == \"raise\":\n            raise ValueError(\"Unknown property found at: %s\" % dict_path)\n        return None\n    if type(obj_type) is list:\n        if len(obj_type) > 2:\n            raise Exception(\"Sorry, getschema does not support multiple types\")\n        nullable = (\"null\" in obj_type)\n        obj_type = obj_type[1] if obj_type[0] == \"null\" else obj_type[0]\n\n    if obj is None:\n        if not nullable:\n            if on_invalid_property == \"raise\":\n                raise ValueError(\"Null object given at %s\" % dict_path)\n        return None\n\n    # Recurse if object or array types\n    if obj_type == \"object\":\n        if type(obj) is not dict:\n            raise KeyError(\"property type (object) Expected a dict object.\" +\n                           \"Got: %s %s at %s\" % (type(obj), str(obj), str(dict_path)))\n        cleaned = dict()\n        keys = obj.keys()\n        for key in keys:\n            if drop_unknown_properties and not _nested_get(schema, dict_path + [\"properties\", key] + [\"type\"]):\n                continue\n            try:\n                ret = fix_type(obj[key], schema, dict_path + [\"properties\", key],\n                               **kwargs)\n            except Exception as e:\n                raise Exception(f\"{str(e)} at {dict_path}\")\n\n            cleaned[key] = ret\n            new_key = _convert_key(key, lower, replace_special, snake_case)\n            if key != new_key:\n                cleaned[new_key] = cleaned.pop(key)\n    elif obj_type == \"array\":\n        assert(type(obj) is list)\n        cleaned = list()\n        for o in obj:\n            try:\n                ret = fix_type(o, schema, dict_path + [\"items\"],\n                               **kwargs)\n            except Exception as e:\n                raise Exception(f\"{str(e)} at {dict_path}\")\n\n            if ret is not None:\n                cleaned.append(ret)\n    else:\n        if obj_type == \"string\":\n            if obj is None:\n                cleaned = None\n            else:\n                cleaned = str(obj)\n                if obj_format == \"date-time\":\n                    # Just test parsing for now. Not converting to Python's\n                    # datetime as re-JSONifying datetime is not straight-foward\n                    if not _is_datetime(cleaned):\n                        cleaned = _on_invalid_property(\n                            on_invalid_property,\n                            dict_path, obj_type, cleaned,\n                            err_msg=\"Not in a valid datetime format\",\n                        )\n                    elif date_to_datetime and len(cleaned) == 10:  # \"2023-10-19\"\n                        cleaned += \" 00:00:00.000\"\n\n        elif obj_type == \"number\":\n            if obj is None:\n                cleaned = None\n            else:\n                try:\n                    cleaned = float(obj)\n                except ValueError as e:\n                    cleaned = _on_invalid_property(\n                        on_invalid_property, dict_path, obj_type, obj,\n                        err_msg=str(e))\n        elif obj_type == \"integer\":\n            if obj is None:\n                cleaned = None\n            else:\n                try:\n                    cleaned = int(obj)\n                except ValueError as e:\n                    cleaned = _on_invalid_property(\n                        on_invalid_property, dict_path, obj_type, obj,\n                        err_msg=str(e))\n        elif obj_type == \"boolean\":\n            if obj is None:\n                cleaned = None\n            elif str(obj).lower() == \"true\":\n                cleaned = True\n            elif str(obj).lower() == \"false\":\n                cleaned = False\n            else:\n                cleaned = _on_invalid_property(\n                    on_invalid_property, dict_path, obj_type, obj,\n                    err_msg=(str(obj) +\n                             \" is not a valid value for boolean type\"))\n        else:\n            raise Exception(\"Invalid type in schema: %s\" % obj_type)\n    return cleaned\n"
  },
  "GT_src_dict": {
    "getschema/impl.py": {
      "_do_infer_schema": {
        "code": "def _do_infer_schema(obj, record_level=None, lower=False, replace_special=False, snake_case=False):\n    \"\"\"Infer the schema of a given object by recursively analyzing its structure and data types.\n\nParameters:\n- obj: The input object (could be a dict, list, or basic data types) from which the schema is inferred.\n- record_level: (Optional) A JSONPath expression string that specifies a path to navigate within the input object to target a specific record.\n- lower: (Optional) A boolean that indicates whether to convert keys to lowercase.\n- replace_special: (Optional) A boolean that indicates whether to replace special characters in keys with underscores.\n- snake_case: (Optional) A boolean that indicates whether to convert keys with spaces to snake_case format.\n\nReturns:\n- A dictionary representing the inferred JSON schema, detailing the types and properties of the input object.\n\nThis function utilizes the helper function _convert_key for key formatting and _get_jsonpath for extracting specific records from nested data structures. It also checks for null values and handles different data types (e.g., object, array, string, number, boolean) and their respective schema representations. If the input object is a list, the function assumes the first item to infer the schema for arrays.\"\"\"\n    schema = dict()\n    if record_level:\n        obj = _get_jsonpath(obj, record_level)[0]\n    if obj is None:\n        schema['type'] = ['null']\n    elif type(obj) is dict and obj.keys():\n        schema['type'] = ['null', 'object']\n        schema['properties'] = dict()\n        for key in obj.keys():\n            ret = _do_infer_schema(obj[key])\n            new_key = _convert_key(key, lower=lower, replace_special=replace_special, snake_case=snake_case)\n            if ret:\n                schema['properties'][new_key] = ret\n    elif type(obj) is list:\n        schema['type'] = ['null', 'array']\n        if not obj:\n            schema['items'] = None\n        else:\n            ret = _do_infer_schema(obj[0], lower=lower, replace_special=replace_special, snake_case=snake_case)\n            schema['items'] = ret\n    else:\n        try:\n            float(obj)\n        except (ValueError, TypeError):\n            schema['type'] = ['null', 'string']\n            if _is_datetime(obj):\n                schema['format'] = 'date-time'\n        else:\n            if type(obj) == bool:\n                schema['type'] = ['null', 'boolean']\n            elif type(obj) == float or (type(obj) == str and '.' in obj):\n                schema['type'] = ['null', 'number']\n            elif type(obj) == int or (type(obj) == str and obj[0] != '0'):\n                schema['type'] = ['null', 'integer']\n            else:\n                schema['type'] = ['null', 'string']\n    return schema",
        "docstring": "Infer the schema of a given object by recursively analyzing its structure and data types.\n\nParameters:\n- obj: The input object (could be a dict, list, or basic data types) from which the schema is inferred.\n- record_level: (Optional) A JSONPath expression string that specifies a path to navigate within the input object to target a specific record.\n- lower: (Optional) A boolean that indicates whether to convert keys to lowercase.\n- replace_special: (Optional) A boolean that indicates whether to replace special characters in keys with underscores.\n- snake_case: (Optional) A boolean that indicates whether to convert keys with spaces to snake_case format.\n\nReturns:\n- A dictionary representing the inferred JSON schema, detailing the types and properties of the input object.\n\nThis function utilizes the helper function _convert_key for key formatting and _get_jsonpath for extracting specific records from nested data structures. It also checks for null values and handles different data types (e.g., object, array, string, number, boolean) and their respective schema representations. If the input object is a list, the function assumes the first item to infer the schema for arrays.",
        "signature": "def _do_infer_schema(obj, record_level=None, lower=False, replace_special=False, snake_case=False):",
        "type": "Function",
        "class_signature": null
      },
      "_infer_from_two": {
        "code": "def _infer_from_two(schema1, schema2):\n    \"\"\"Compare two JSON schema dictionaries, `schema1` and `schema2`, to deduce a more conservative schema that captures all provided properties without losing information. The function iterates through the properties of `schema1` and compares them to the corresponding properties in `schema2`. If conflicts arise in property types or formats, it retains the more restrictive or conservative option using the `_compare_props` function.\n\nParameters:\n- schema1 (dict): The first schema to compare, which may be more conservative.\n- schema2 (dict): The second schema to compare, which may include broader definitions.\n\nReturns:\n- dict: A new schema that represents the most conservative interpretation of the two input schemas, combining properties while resolving conflicts based on type safety.\n\nExceptions:\n- Raises an Exception if incompatible types are detected for the same property between the two schemas.\n\nDependencies:\n- Uses the `_compare_props` function, which handles the logic of comparing individual properties and merging their attributes based on type and other rules.\"\"\"\n    '\\n    Compare between currently the most conservative and the new record schemas\\n    and keep the more conservative one.\\n    '\n    if schema1 is None:\n        return schema2\n    if schema2 is None:\n        return schema1\n    schema = schema2\n    for key in schema1['properties']:\n        prop1 = schema1['properties'][key]\n        prop2 = schema2['properties'].get(key, prop1)\n        try:\n            schema['properties'][key] = _compare_props(prop1, prop2)\n        except Exception as e:\n            raise Exception('Key: %s\\n%s' % (key, e))\n    return schema",
        "docstring": "Compare two JSON schema dictionaries, `schema1` and `schema2`, to deduce a more conservative schema that captures all provided properties without losing information. The function iterates through the properties of `schema1` and compares them to the corresponding properties in `schema2`. If conflicts arise in property types or formats, it retains the more restrictive or conservative option using the `_compare_props` function.\n\nParameters:\n- schema1 (dict): The first schema to compare, which may be more conservative.\n- schema2 (dict): The second schema to compare, which may include broader definitions.\n\nReturns:\n- dict: A new schema that represents the most conservative interpretation of the two input schemas, combining properties while resolving conflicts based on type safety.\n\nExceptions:\n- Raises an Exception if incompatible types are detected for the same property between the two schemas.\n\nDependencies:\n- Uses the `_compare_props` function, which handles the logic of comparing individual properties and merging their attributes based on type and other rules.",
        "signature": "def _infer_from_two(schema1, schema2):",
        "type": "Function",
        "class_signature": null
      },
      "_replace_null_type": {
        "code": "def _replace_null_type(schema, path=''):\n    \"\"\"Replace null types in a JSON schema representation with default types. This function traverses the schema and updates any properties or items that are defined as \"null\" or have no type, replacing them with a default type defined by the global constant DEFAULT_TYPE, which defaults to a list containing \"null\" and \"string\".\n\nArgs:\n    schema (dict): The JSON schema to be modified, which must include a \"type\" key.\n    path (str): The current path in the schema being processed, used for logging warnings.\n\nReturns:\n    dict: A new schema where null types have been replaced according to the specified rules.\n\nSide Effects:\n    Logs warnings when encountering arrays or properties that do not conform to expected types.\n\nConstants:\n    DEFAULT_TYPE (list): Defined at the module level, this constant represents the default type used to replace \"null\" types, facilitating the function's task of ensuring a schema can effectively represent data types.\"\"\"\n    new_schema = {}\n    new_schema.update(schema)\n    if schema['type'] in ('object', ['object'], ['null', 'object']):\n        new_schema['properties'] = {}\n        for key in schema.get('properties', {}).keys():\n            new_path = path + '.' + key\n            new_schema['properties'][key] = _replace_null_type(schema['properties'][key], new_path)\n    elif schema['type'] == ['null', 'array']:\n        if not schema.get('items'):\n            new_schema['items'] = {}\n        if new_schema['items'].get('type') in (None, 'null', ['null']):\n            LOGGER.warning(f'{path} is an array without non-null values.Replacing with the default {DEFAULT_TYPE}')\n            new_schema['items']['type'] = DEFAULT_TYPE\n    elif schema['type'] == ['null']:\n        LOGGER.warning(f'{path} contained non-null values only. Replacing with the default {DEFAULT_TYPE}')\n        new_schema['type'] = DEFAULT_TYPE\n    return new_schema",
        "docstring": "Replace null types in a JSON schema representation with default types. This function traverses the schema and updates any properties or items that are defined as \"null\" or have no type, replacing them with a default type defined by the global constant DEFAULT_TYPE, which defaults to a list containing \"null\" and \"string\".\n\nArgs:\n    schema (dict): The JSON schema to be modified, which must include a \"type\" key.\n    path (str): The current path in the schema being processed, used for logging warnings.\n\nReturns:\n    dict: A new schema where null types have been replaced according to the specified rules.\n\nSide Effects:\n    Logs warnings when encountering arrays or properties that do not conform to expected types.\n\nConstants:\n    DEFAULT_TYPE (list): Defined at the module level, this constant represents the default type used to replace \"null\" types, facilitating the function's task of ensuring a schema can effectively represent data types.",
        "signature": "def _replace_null_type(schema, path=''):",
        "type": "Function",
        "class_signature": null
      },
      "infer_schema": {
        "code": "def infer_schema(obj, record_level=None, lower=False, replace_special=False, snake_case=False):\n    \"\"\"Infer the JSON schema from a given object or a list of objects.\n\nThis function analyzes the structure of the provided input data and infers the most conservative JSON schema, denoting types and properties. It supports options for transforming keys to lower case, replacing special characters, and converting spaces to snake_case. The inferred schema can be generated from a specified record level if required.\n\nParameters:\n- obj: A dictionary or a list of dictionaries from which to infer the schema.\n- record_level (optional): A JSONPath expression that specifies a record level to analyze within the provided object.\n- lower (optional): A boolean flag that, if set to True, converts all keys in the schema to lower case.\n- replace_special (optional): A boolean flag that, if set to True, replaces characters that are not alphanumeric, underscores, hyphens, or spaces with underscores.\n- snake_case (optional): A boolean flag that, if set to True, converts spaces in keys to underscores.\n\nReturns:\n- A dictionary representing the inferred schema, indicating the types and structure of the input data. The schema will have a \"type\" key set to \"object\", and additional properties will be nested accordingly.\n\nDependencies:\n- Uses the `_do_infer_schema` function to recursively infer types from the object(s) and `_infer_from_two` to compare and combine schemas. \n- Utilizes the `_replace_null_type` function to handle potential null values in the schema and ensure default types are assigned.\n\nSide Effects:\n- Logs the completion of the inference process, including the number of records processed, using the configured logger.\"\"\"\n    'Infer schema from a given object or a list of objects\\n    - record_level:\\n    - lower: Convert the key to all lower case\\n    - replace_special: Replace letters to _ if not 0-9, A-Z, a-z, _ and -, or \" \"\\n    - snake_case: Replace space to _\\n    '\n    if type(obj) is not list:\n        obj = [obj]\n    if type(obj[0]) is not dict:\n        raise ValueError('Input must be a dict object.')\n    schema = None\n    for o in obj:\n        cur_schema = _do_infer_schema(o, record_level, lower, replace_special, snake_case)\n        schema = _infer_from_two(schema, cur_schema)\n    schema['type'] = 'object'\n    schema = _replace_null_type(schema)\n    LOGGER.info(f'Inference completed from {len(obj)} records')\n    return schema",
        "docstring": "Infer the JSON schema from a given object or a list of objects.\n\nThis function analyzes the structure of the provided input data and infers the most conservative JSON schema, denoting types and properties. It supports options for transforming keys to lower case, replacing special characters, and converting spaces to snake_case. The inferred schema can be generated from a specified record level if required.\n\nParameters:\n- obj: A dictionary or a list of dictionaries from which to infer the schema.\n- record_level (optional): A JSONPath expression that specifies a record level to analyze within the provided object.\n- lower (optional): A boolean flag that, if set to True, converts all keys in the schema to lower case.\n- replace_special (optional): A boolean flag that, if set to True, replaces characters that are not alphanumeric, underscores, hyphens, or spaces with underscores.\n- snake_case (optional): A boolean flag that, if set to True, converts spaces in keys to underscores.\n\nReturns:\n- A dictionary representing the inferred schema, indicating the types and structure of the input data. The schema will have a \"type\" key set to \"object\", and additional properties will be nested accordingly.\n\nDependencies:\n- Uses the `_do_infer_schema` function to recursively infer types from the object(s) and `_infer_from_two` to compare and combine schemas. \n- Utilizes the `_replace_null_type` function to handle potential null values in the schema and ensure default types are assigned.\n\nSide Effects:\n- Logs the completion of the inference process, including the number of records processed, using the configured logger.",
        "signature": "def infer_schema(obj, record_level=None, lower=False, replace_special=False, snake_case=False):",
        "type": "Function",
        "class_signature": null
      }
    }
  },
  "dependency_dict": {
    "getschema/impl.py:infer_schema": {},
    "getschema/impl.py:_do_infer_schema": {
      "getschema/impl.py": {
        "_convert_key": {
          "code": "def _convert_key(old_key, lower=False, replace_special=False, snake_case=False):\n    new_key = old_key\n    if lower:\n        new_key = new_key.lower()\n    if replace_special:\n        new_key = re.sub('[^ a-zA-Z0-9_]', '_', new_key)\n    if snake_case:\n        new_key = new_key.strip().replace(' ', '_')\n    return new_key",
          "docstring": "",
          "signature": "def _convert_key(old_key, lower=False, replace_special=False, snake_case=False):",
          "type": "Function",
          "class_signature": null
        }
      }
    },
    "getschema/impl.py:_infer_from_two": {
      "getschema/impl.py": {
        "_compare_props": {
          "code": "def _compare_props(prop1, prop2):\n    if not prop2 or prop2.get('type') == ['null']:\n        return prop1\n    elif not prop1 or prop1.get('type') == ['null']:\n        return prop2\n    prop = prop2\n    t1 = prop1['type']\n    t2 = prop2['type']\n    f1 = prop1.get('format')\n    f2 = prop2.get('format')\n    if t1[1] == 'object':\n        if t1[1] != t2[1]:\n            raise ValueError('While traversing object %s, two records differ in types: %s %s' % (prop, t1[1], t2[1]))\n        for key in prop['properties']:\n            prop['properties'][key] = _compare_props(prop1['properties'].get(key), prop2['properties'].get(key))\n    if t1[1] == 'array':\n        if t1[1] != t2[1]:\n            raise ValueError('While traversing array %s, two records differ in types: %s %s' % (prop, t1[1], t2[1]))\n        prop['items'] = _compare_props(prop1['items'], prop2['items'])\n    numbers = ['integer', 'number']\n    if not (t1[1] == t2[1] and f1 == f2):\n        if t1[1] in numbers and t2[1] in numbers:\n            prop['type'] = ['null', 'number']\n        else:\n            prop['type'] = ['null', 'string']\n            if 'format' in prop.keys():\n                prop.pop('format')\n    return prop",
          "docstring": "",
          "signature": "def _compare_props(prop1, prop2):",
          "type": "Function",
          "class_signature": null
        }
      }
    },
    "getschema/impl.py:_replace_null_type": {}
  },
  "call_tree": {
    "tests/test_types.py:test_null_records": {
      "getschema/impl.py:infer_schema": {
        "getschema/impl.py:_do_infer_schema": {
          "getschema/impl.py:_do_infer_schema": {
            "[ignored_or_cut_off]": "..."
          },
          "getschema/impl.py:_convert_key": {}
        },
        "getschema/impl.py:_infer_from_two": {
          "getschema/impl.py:_compare_props": {
            "getschema/impl.py:_compare_props": {
              "[ignored_or_cut_off]": "..."
            }
          }
        },
        "getschema/impl.py:_replace_null_type": {
          "getschema/impl.py:_replace_null_type": {
            "[ignored_or_cut_off]": "..."
          }
        }
      }
    }
  },
  "PRD": "# PROJECT NAME: getschema-test_types\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 getschema/\n    \u2514\u2500\u2500 impl.py\n        \u251c\u2500\u2500 _do_infer_schema\n        \u251c\u2500\u2500 _infer_from_two\n        \u251c\u2500\u2500 _replace_null_type\n        \u2514\u2500\u2500 infer_schema\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThis module is designed to infer and validate schemas by analyzing a dataset comprising records with potentially null and nested fields. It provides capabilities to extract the structure and data types of fields, including handling null values, arrays, and nested objects, while accommodating various data formats such as dates and integers. By programmatically determining schema properties, it streamlines the process of schema generation and ensures data consistency, solving the common challenge of schema validation and integration for developers working with dynamic or semi-structured data sources.\n\n## FILE 1: getschema/impl.py\n\n- FUNCTION NAME: _replace_null_type\n  - SIGNATURE: def _replace_null_type(schema, path=''):\n  - DOCSTRING: \n```python\n\"\"\"\nReplace null types in a JSON schema representation with default types. This function traverses the schema and updates any properties or items that are defined as \"null\" or have no type, replacing them with a default type defined by the global constant DEFAULT_TYPE, which defaults to a list containing \"null\" and \"string\".\n\nArgs:\n    schema (dict): The JSON schema to be modified, which must include a \"type\" key.\n    path (str): The current path in the schema being processed, used for logging warnings.\n\nReturns:\n    dict: A new schema where null types have been replaced according to the specified rules.\n\nSide Effects:\n    Logs warnings when encountering arrays or properties that do not conform to expected types.\n\nConstants:\n    DEFAULT_TYPE (list): Defined at the module level, this constant represents the default type used to replace \"null\" types, facilitating the function's task of ensuring a schema can effectively represent data types.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - getschema/impl.py:_replace_null_type\n    - getschema/impl.py:infer_schema\n\n- FUNCTION NAME: _infer_from_two\n  - SIGNATURE: def _infer_from_two(schema1, schema2):\n  - DOCSTRING: \n```python\n\"\"\"\nCompare two JSON schema dictionaries, `schema1` and `schema2`, to deduce a more conservative schema that captures all provided properties without losing information. The function iterates through the properties of `schema1` and compares them to the corresponding properties in `schema2`. If conflicts arise in property types or formats, it retains the more restrictive or conservative option using the `_compare_props` function.\n\nParameters:\n- schema1 (dict): The first schema to compare, which may be more conservative.\n- schema2 (dict): The second schema to compare, which may include broader definitions.\n\nReturns:\n- dict: A new schema that represents the most conservative interpretation of the two input schemas, combining properties while resolving conflicts based on type safety.\n\nExceptions:\n- Raises an Exception if incompatible types are detected for the same property between the two schemas.\n\nDependencies:\n- Uses the `_compare_props` function, which handles the logic of comparing individual properties and merging their attributes based on type and other rules.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - getschema/impl.py:_compare_props\n    - getschema/impl.py:infer_schema\n\n- FUNCTION NAME: infer_schema\n  - SIGNATURE: def infer_schema(obj, record_level=None, lower=False, replace_special=False, snake_case=False):\n  - DOCSTRING: \n```python\n\"\"\"\nInfer the JSON schema from a given object or a list of objects.\n\nThis function analyzes the structure of the provided input data and infers the most conservative JSON schema, denoting types and properties. It supports options for transforming keys to lower case, replacing special characters, and converting spaces to snake_case. The inferred schema can be generated from a specified record level if required.\n\nParameters:\n- obj: A dictionary or a list of dictionaries from which to infer the schema.\n- record_level (optional): A JSONPath expression that specifies a record level to analyze within the provided object.\n- lower (optional): A boolean flag that, if set to True, converts all keys in the schema to lower case.\n- replace_special (optional): A boolean flag that, if set to True, replaces characters that are not alphanumeric, underscores, hyphens, or spaces with underscores.\n- snake_case (optional): A boolean flag that, if set to True, converts spaces in keys to underscores.\n\nReturns:\n- A dictionary representing the inferred schema, indicating the types and structure of the input data. The schema will have a \"type\" key set to \"object\", and additional properties will be nested accordingly.\n\nDependencies:\n- Uses the `_do_infer_schema` function to recursively infer types from the object(s) and `_infer_from_two` to compare and combine schemas. \n- Utilizes the `_replace_null_type` function to handle potential null values in the schema and ensure default types are assigned.\n\nSide Effects:\n- Logs the completion of the inference process, including the number of records processed, using the configured logger.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - getschema/impl.py:_replace_null_type\n    - getschema/impl.py:_infer_from_two\n    - getschema/impl.py:_do_infer_schema\n\n- FUNCTION NAME: _do_infer_schema\n  - SIGNATURE: def _do_infer_schema(obj, record_level=None, lower=False, replace_special=False, snake_case=False):\n  - DOCSTRING: \n```python\n\"\"\"\nInfer the schema of a given object by recursively analyzing its structure and data types.\n\nParameters:\n- obj: The input object (could be a dict, list, or basic data types) from which the schema is inferred.\n- record_level: (Optional) A JSONPath expression string that specifies a path to navigate within the input object to target a specific record.\n- lower: (Optional) A boolean that indicates whether to convert keys to lowercase.\n- replace_special: (Optional) A boolean that indicates whether to replace special characters in keys with underscores.\n- snake_case: (Optional) A boolean that indicates whether to convert keys with spaces to snake_case format.\n\nReturns:\n- A dictionary representing the inferred JSON schema, detailing the types and properties of the input object.\n\nThis function utilizes the helper function _convert_key for key formatting and _get_jsonpath for extracting specific records from nested data structures. It also checks for null values and handles different data types (e.g., object, array, string, number, boolean) and their respective schema representations. If the input object is a list, the function assumes the first item to infer the schema for arrays.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - getschema/impl.py:_convert_key\n    - getschema/impl.py:infer_schema\n    - getschema/impl.py:_do_infer_schema\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "getschema/impl.py": "import argparse, csv, datetime, logging, os, re, sys\nfrom dateutil import parser as dateutil_parser\nfrom dateutil.tz import tzoffset\nimport jsonpath_ng as jsonpath\nimport simplejson as json\nimport yaml\nCOMMAND = 'json2schema'\nLOGGER = logging.getLogger(__name__)\nDEFAULT_TYPE = ['null', 'string']\n\ndef _convert_key(old_key, lower=False, replace_special=False, snake_case=False):\n    new_key = old_key\n    if lower:\n        new_key = new_key.lower()\n    if replace_special:\n        new_key = re.sub('[^ a-zA-Z0-9_]', '_', new_key)\n    if snake_case:\n        new_key = new_key.strip().replace(' ', '_')\n    return new_key\n\ndef _get_jsonpath(raw, path):\n    jsonpath_expr = jsonpath.parse(path)\n    record = [match.value for match in jsonpath_expr.find(raw)]\n    return record\n\ndef _is_datetime(obj):\n    return type(obj) is datetime.datetime or type(obj) is datetime.date or (type(obj) is str and re.match('(19|20)\\\\d\\\\d-(0[1-9]|1[012])-([1-9]|0[1-9]|[12][0-9]|3[01])', obj) is not None)\n\ndef _compare_props(prop1, prop2):\n    if not prop2 or prop2.get('type') == ['null']:\n        return prop1\n    elif not prop1 or prop1.get('type') == ['null']:\n        return prop2\n    prop = prop2\n    t1 = prop1['type']\n    t2 = prop2['type']\n    f1 = prop1.get('format')\n    f2 = prop2.get('format')\n    if t1[1] == 'object':\n        if t1[1] != t2[1]:\n            raise ValueError('While traversing object %s, two records differ in types: %s %s' % (prop, t1[1], t2[1]))\n        for key in prop['properties']:\n            prop['properties'][key] = _compare_props(prop1['properties'].get(key), prop2['properties'].get(key))\n    if t1[1] == 'array':\n        if t1[1] != t2[1]:\n            raise ValueError('While traversing array %s, two records differ in types: %s %s' % (prop, t1[1], t2[1]))\n        prop['items'] = _compare_props(prop1['items'], prop2['items'])\n    numbers = ['integer', 'number']\n    if not (t1[1] == t2[1] and f1 == f2):\n        if t1[1] in numbers and t2[1] in numbers:\n            prop['type'] = ['null', 'number']\n        else:\n            prop['type'] = ['null', 'string']\n            if 'format' in prop.keys():\n                prop.pop('format')\n    return prop\n\ndef _nested_get(input_dict, nested_key):\n    internal_dict_value = input_dict\n    for k in nested_key:\n        internal_dict_value = internal_dict_value.get(k, None)\n        if internal_dict_value is None:\n            return None\n    return internal_dict_value\n\ndef _parse_datetime_tz(datetime_str, default_tz_offset=0):\n    d = dateutil_parser.parse(datetime_str)\n    if not d.tzinfo:\n        d = d.replace(tzinfo=tzoffset(None, default_tz_offset))\n    return d\n\ndef _on_invalid_property(policy, dict_path, obj_type, obj, err_msg):\n    if policy == 'raise':\n        raise Exception(err_msg + ' dict_path' + str(dict_path) + ' object type: ' + obj_type + ' object: ' + str(obj))\n    elif policy == 'force':\n        cleaned = str(obj)\n    elif policy == 'null':\n        cleaned = None\n    else:\n        raise ValueError('Unknown policy: %s' % policy)\n    return cleaned\n\ndef infer_from_json_file(filename, skip=0, lower=False, replace_special=False, snake_case=False):\n    with open(filename, 'r') as f:\n        content = f.read()\n    data = json.loads(content)\n    if type(data) is list:\n        data = data[skip:]\n    schema = infer_schema(data, lower=lower, replace_special=replace_special, snake_case=snake_case)\n    return schema\n\ndef infer_from_yaml_file(filename, skip=0, lower=False, replace_special=False, snake_case=False):\n    with open(filename, 'r') as f:\n        content = f.read()\n    data = yaml.load(content, Loader=yaml.FullLoader)\n    if type(data) is list:\n        data = data[skip:]\n    schema = infer_schema(data, lower=lower, replace_special=replace_special, snake_case=snake_case)\n    return schema\n\ndef infer_from_csv_file(filename, skip=0, lower=False, replace_special=False, snake_case=False):\n    with open(filename) as f:\n        count = 0\n        while count < skip:\n            count = count + 1\n            f.readline()\n        reader = csv.DictReader(f)\n        data = [dict(row) for row in reader]\n    schema = infer_schema(data, lower=lower, replace_special=replace_special, snake_case=snake_case)\n    return schema\n\ndef infer_from_file(filename, fmt='json', skip=0, lower=False, replace_special=False, snake_case=False):\n    if fmt == 'json':\n        schema = infer_from_json_file(filename, skip, lower, replace_special, snake_case)\n    elif fmt == 'yaml':\n        schema = infer_from_yaml_file(filename, skip, lower, replace_special, snake_case)\n    elif fmt == 'csv':\n        schema = infer_from_csv_file(filename, skip, lower, replace_special, snake_case)\n    else:\n        raise KeyError('Unsupported format : ' + fmt)\n    return schema\n\nclass DroppedProperty(object):\n    pass\n\ndef fix_type(obj, schema, dict_path=[], on_invalid_property='raise', drop_unknown_properties=False, lower=False, replace_special=False, snake_case=False, date_to_datetime=False):\n    \"\"\"Convert the fields into the proper object types.\n    e.g. {\"number\": \"1.0\"} -> {\"number\": 1.0}\n\n    - on_invalid_property: [\"raise\", \"null\", \"force\"]\n      What to do when the value cannot be converted.\n      - raise: Raise exception\n      - null: Impute with null\n      - force: Keep it as is (string)\n    - drop_unknown_properties: True/False\n      If true, the returned object will exclude unknown (sub-)properties\n    \"\"\"\n    kwargs = {'on_invalid_property': on_invalid_property, 'drop_unknown_properties': drop_unknown_properties, 'lower': lower, 'replace_special': replace_special, 'snake_case': snake_case, 'date_to_datetime': date_to_datetime}\n    invalid_actions = ['raise', 'null', 'force']\n    if on_invalid_property not in invalid_actions:\n        raise ValueError('on_invalid_property is not one of %s' % invalid_actions)\n    obj_type = _nested_get(schema, dict_path + ['type'])\n    obj_format = _nested_get(schema, dict_path + ['format'])\n    nullable = False\n    if obj_type is None:\n        if on_invalid_property == 'raise':\n            raise ValueError('Unknown property found at: %s' % dict_path)\n        return None\n    if type(obj_type) is list:\n        if len(obj_type) > 2:\n            raise Exception('Sorry, getschema does not support multiple types')\n        nullable = 'null' in obj_type\n        obj_type = obj_type[1] if obj_type[0] == 'null' else obj_type[0]\n    if obj is None:\n        if not nullable:\n            if on_invalid_property == 'raise':\n                raise ValueError('Null object given at %s' % dict_path)\n        return None\n    if obj_type == 'object':\n        if type(obj) is not dict:\n            raise KeyError('property type (object) Expected a dict object.' + 'Got: %s %s at %s' % (type(obj), str(obj), str(dict_path)))\n        cleaned = dict()\n        keys = obj.keys()\n        for key in keys:\n            if drop_unknown_properties and (not _nested_get(schema, dict_path + ['properties', key] + ['type'])):\n                continue\n            try:\n                ret = fix_type(obj[key], schema, dict_path + ['properties', key], **kwargs)\n            except Exception as e:\n                raise Exception(f'{str(e)} at {dict_path}')\n            cleaned[key] = ret\n            new_key = _convert_key(key, lower, replace_special, snake_case)\n            if key != new_key:\n                cleaned[new_key] = cleaned.pop(key)\n    elif obj_type == 'array':\n        assert type(obj) is list\n        cleaned = list()\n        for o in obj:\n            try:\n                ret = fix_type(o, schema, dict_path + ['items'], **kwargs)\n            except Exception as e:\n                raise Exception(f'{str(e)} at {dict_path}')\n            if ret is not None:\n                cleaned.append(ret)\n    elif obj_type == 'string':\n        if obj is None:\n            cleaned = None\n        else:\n            cleaned = str(obj)\n            if obj_format == 'date-time':\n                if not _is_datetime(cleaned):\n                    cleaned = _on_invalid_property(on_invalid_property, dict_path, obj_type, cleaned, err_msg='Not in a valid datetime format')\n                elif date_to_datetime and len(cleaned) == 10:\n                    cleaned += ' 00:00:00.000'\n    elif obj_type == 'number':\n        if obj is None:\n            cleaned = None\n        else:\n            try:\n                cleaned = float(obj)\n            except ValueError as e:\n                cleaned = _on_invalid_property(on_invalid_property, dict_path, obj_type, obj, err_msg=str(e))\n    elif obj_type == 'integer':\n        if obj is None:\n            cleaned = None\n        else:\n            try:\n                cleaned = int(obj)\n            except ValueError as e:\n                cleaned = _on_invalid_property(on_invalid_property, dict_path, obj_type, obj, err_msg=str(e))\n    elif obj_type == 'boolean':\n        if obj is None:\n            cleaned = None\n        elif str(obj).lower() == 'true':\n            cleaned = True\n        elif str(obj).lower() == 'false':\n            cleaned = False\n        else:\n            cleaned = _on_invalid_property(on_invalid_property, dict_path, obj_type, obj, err_msg=str(obj) + ' is not a valid value for boolean type')\n    else:\n        raise Exception('Invalid type in schema: %s' % obj_type)\n    return cleaned"
  }
}