{
  "dir_path": "/app/scylla_driver",
  "package_name": "scylla_driver",
  "sample_name": "scylla_driver-test_row_factories",
  "src_dir": "cassandra/",
  "test_dir": "tests/",
  "test_file": "tests/unit/test_row_factories.py",
  "test_code": "# Copyright DataStax, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom cassandra.query import named_tuple_factory\n\nimport logging\nimport warnings\n\nimport sys\n\nfrom unittest import TestCase\n\n\nlog = logging.getLogger(__name__)\n\n\nNAMEDTUPLE_CREATION_BUG = sys.version_info >= (3,) and sys.version_info < (3, 7)\n\nclass TestNamedTupleFactory(TestCase):\n\n    long_colnames, long_rows = (\n        ['col{}'.format(x) for x in range(300)],\n        [\n            ['value{}'.format(x) for x in range(300)]\n            for _ in range(100)\n        ]\n    )\n    short_colnames, short_rows = (\n        ['col{}'.format(x) for x in range(200)],\n        [\n            ['value{}'.format(x) for x in range(200)]\n            for _ in range(100)\n        ]\n    )\n\n    def test_creation_warning_on_long_column_list(self):\n        \"\"\"\n        Reproduces the failure described in PYTHON-893\n\n        @since 3.15\n        @jira_ticket PYTHON-893\n        @expected_result creation fails on Python > 3 and < 3.7\n\n        @test_category row_factory\n        \"\"\"\n        if not NAMEDTUPLE_CREATION_BUG:\n            named_tuple_factory(self.long_colnames, self.long_rows)\n            return\n\n        with warnings.catch_warnings(record=True) as w:\n            rows = named_tuple_factory(self.long_colnames, self.long_rows)\n        self.assertEqual(len(w), 1)\n        warning = w[0]\n        self.assertIn('pseudo_namedtuple_factory', str(warning))\n        self.assertIn('3.7', str(warning))\n\n        for r in rows:\n            self.assertEqual(r.col0, self.long_rows[0][0])\n\n    def test_creation_no_warning_on_short_column_list(self):\n        \"\"\"\n        Tests that normal namedtuple row creation still works after PYTHON-893 fix\n\n        @since 3.15\n        @jira_ticket PYTHON-893\n        @expected_result creates namedtuple-based Rows\n\n        @test_category row_factory\n        \"\"\"\n        with warnings.catch_warnings(record=True) as w:\n            rows = named_tuple_factory(self.short_colnames, self.short_rows)\n        self.assertEqual(len(w), 0)\n        # check that this is a real namedtuple\n        self.assertTrue(hasattr(rows[0], '_fields'))\n        self.assertIsInstance(rows[0], tuple)\n",
  "GT_file_code": {
    "cassandra/query.py": "# Copyright DataStax, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nThis module holds classes for working with prepared statements and\nspecifying consistency levels and retry policies for individual\nqueries.\n\"\"\"\n\nfrom collections import namedtuple\nfrom datetime import datetime, timedelta, timezone\nimport re\nimport struct\nimport time\nimport warnings\n\nfrom cassandra import ConsistencyLevel, OperationTimedOut\nfrom cassandra.util import unix_time_from_uuid1, maybe_add_timeout_to_query\nfrom cassandra.encoder import Encoder\nimport cassandra.encoder\nfrom cassandra.policies import ColDesc\nfrom cassandra.protocol import _UNSET_VALUE\nfrom cassandra.util import OrderedDict, _sanitize_identifiers\n\nimport logging\nlog = logging.getLogger(__name__)\n\nUNSET_VALUE = _UNSET_VALUE\n\"\"\"\nSpecifies an unset value when binding a prepared statement.\n\nUnset values are ignored, allowing prepared statements to be used without specify\n\nSee https://issues.apache.org/jira/browse/CASSANDRA-7304 for further details on semantics.\n\n.. versionadded:: 2.6.0\n\nOnly valid when using native protocol v4+\n\"\"\"\n\nNON_ALPHA_REGEX = re.compile('[^a-zA-Z0-9]')\nSTART_BADCHAR_REGEX = re.compile('^[^a-zA-Z0-9]*')\nEND_BADCHAR_REGEX = re.compile('[^a-zA-Z0-9_]*$')\n\n_clean_name_cache = {}\n\n\ndef _clean_column_name(name):\n    try:\n        return _clean_name_cache[name]\n    except KeyError:\n        clean = NON_ALPHA_REGEX.sub(\"_\", START_BADCHAR_REGEX.sub(\"\", END_BADCHAR_REGEX.sub(\"\", name)))\n        _clean_name_cache[name] = clean\n        return clean\n\n\ndef tuple_factory(colnames, rows):\n    \"\"\"\n    Returns each row as a tuple\n\n    Example::\n\n        >>> from cassandra.query import tuple_factory\n        >>> session = cluster.connect('mykeyspace')\n        >>> session.row_factory = tuple_factory\n        >>> rows = session.execute(\"SELECT name, age FROM users LIMIT 1\")\n        >>> print(rows[0])\n        ('Bob', 42)\n\n    .. versionchanged:: 2.0.0\n        moved from ``cassandra.decoder`` to ``cassandra.query``\n    \"\"\"\n    return rows\n\nclass PseudoNamedTupleRow(object):\n    \"\"\"\n    Helper class for pseudo_named_tuple_factory. These objects provide an\n    __iter__ interface, as well as index- and attribute-based access to values,\n    but otherwise do not attempt to implement the full namedtuple or iterable\n    interface.\n    \"\"\"\n    def __init__(self, ordered_dict):\n        self._dict = ordered_dict\n        self._tuple = tuple(ordered_dict.values())\n\n    def __getattr__(self, name):\n        return self._dict[name]\n\n    def __getitem__(self, idx):\n        return self._tuple[idx]\n\n    def __iter__(self):\n        return iter(self._tuple)\n\n    def __repr__(self):\n        return '{t}({od})'.format(t=self.__class__.__name__,\n                                  od=self._dict)\n\n\ndef pseudo_namedtuple_factory(colnames, rows):\n    \"\"\"\n    Returns each row as a :class:`.PseudoNamedTupleRow`. This is the fallback\n    factory for cases where :meth:`.named_tuple_factory` fails to create rows.\n    \"\"\"\n    return [PseudoNamedTupleRow(od)\n            for od in ordered_dict_factory(colnames, rows)]\n\n\ndef named_tuple_factory(colnames, rows):\n    \"\"\"\n    Returns each row as a `namedtuple <https://docs.python.org/2/library/collections.html#collections.namedtuple>`_.\n    This is the default row factory.\n\n    Example::\n\n        >>> from cassandra.query import named_tuple_factory\n        >>> session = cluster.connect('mykeyspace')\n        >>> session.row_factory = named_tuple_factory\n        >>> rows = session.execute(\"SELECT name, age FROM users LIMIT 1\")\n        >>> user = rows[0]\n\n        >>> # you can access field by their name:\n        >>> print(\"name: %s, age: %d\" % (user.name, user.age))\n        name: Bob, age: 42\n\n        >>> # or you can access fields by their position (like a tuple)\n        >>> name, age = user\n        >>> print(\"name: %s, age: %d\" % (name, age))\n        name: Bob, age: 42\n        >>> name = user[0]\n        >>> age = user[1]\n        >>> print(\"name: %s, age: %d\" % (name, age))\n        name: Bob, age: 42\n\n    .. versionchanged:: 2.0.0\n        moved from ``cassandra.decoder`` to ``cassandra.query``\n    \"\"\"\n    clean_column_names = map(_clean_column_name, colnames)\n    try:\n        Row = namedtuple('Row', clean_column_names)\n    except SyntaxError:\n        warnings.warn(\n            \"Failed creating namedtuple for a result because there were too \"\n            \"many columns. This is due to a Python limitation that affects \"\n            \"namedtuple in Python 3.0-3.6 (see issue18896). The row will be \"\n            \"created with {substitute_factory_name}, which lacks some namedtuple \"\n            \"features and is slower. To avoid slower performance accessing \"\n            \"values on row objects, Upgrade to Python 3.7, or use a different \"\n            \"row factory. (column names: {colnames})\".format(\n                substitute_factory_name=pseudo_namedtuple_factory.__name__,\n                colnames=colnames\n            )\n        )\n        return pseudo_namedtuple_factory(colnames, rows)\n    except Exception:\n        clean_column_names = list(map(_clean_column_name, colnames))  # create list because py3 map object will be consumed by first attempt\n        log.warning(\"Failed creating named tuple for results with column names %s (cleaned: %s) \"\n                    \"(see Python 'namedtuple' documentation for details on name rules). \"\n                    \"Results will be returned with positional names. \"\n                    \"Avoid this by choosing different names, using SELECT \\\"<col name>\\\" AS aliases, \"\n                    \"or specifying a different row_factory on your Session\" %\n                    (colnames, clean_column_names))\n        Row = namedtuple('Row', _sanitize_identifiers(clean_column_names))\n\n    return [Row(*row) for row in rows]\n\n\ndef dict_factory(colnames, rows):\n    \"\"\"\n    Returns each row as a dict.\n\n    Example::\n\n        >>> from cassandra.query import dict_factory\n        >>> session = cluster.connect('mykeyspace')\n        >>> session.row_factory = dict_factory\n        >>> rows = session.execute(\"SELECT name, age FROM users LIMIT 1\")\n        >>> print(rows[0])\n        {u'age': 42, u'name': u'Bob'}\n\n    .. versionchanged:: 2.0.0\n        moved from ``cassandra.decoder`` to ``cassandra.query``\n    \"\"\"\n    return [dict(zip(colnames, row)) for row in rows]\n\n\ndef ordered_dict_factory(colnames, rows):\n    \"\"\"\n    Like :meth:`~cassandra.query.dict_factory`, but returns each row as an OrderedDict,\n    so the order of the columns is preserved.\n\n    .. versionchanged:: 2.0.0\n        moved from ``cassandra.decoder`` to ``cassandra.query``\n    \"\"\"\n    return [OrderedDict(zip(colnames, row)) for row in rows]\n\n\nFETCH_SIZE_UNSET = object()\n\n\nclass Statement(object):\n    \"\"\"\n    An abstract class representing a single query. There are three subclasses:\n    :class:`.SimpleStatement`, :class:`.BoundStatement`, and :class:`.BatchStatement`.\n    These can be passed to :meth:`.Session.execute()`.\n    \"\"\"\n\n    retry_policy = None\n    \"\"\"\n    An instance of a :class:`cassandra.policies.RetryPolicy` or one of its\n    subclasses.  This controls when a query will be retried and how it\n    will be retried.\n    \"\"\"\n\n    consistency_level = None\n    \"\"\"\n    The :class:`.ConsistencyLevel` to be used for this operation.  Defaults\n    to :const:`None`, which means that the default consistency level for\n    the Session this is executed in will be used.\n    \"\"\"\n\n    fetch_size = FETCH_SIZE_UNSET\n    \"\"\"\n    How many rows will be fetched at a time.  This overrides the default\n    of :attr:`.Session.default_fetch_size`\n\n    This only takes effect when protocol version 2 or higher is used.\n    See :attr:`.Cluster.protocol_version` for details.\n\n    .. versionadded:: 2.0.0\n    \"\"\"\n\n    keyspace = None\n    \"\"\"\n    The string name of the keyspace this query acts on. This is used when\n    :class:`~.TokenAwarePolicy` is configured in the profile load balancing policy.\n\n    It is set implicitly on :class:`.BoundStatement`, and :class:`.BatchStatement`,\n    but must be set explicitly on :class:`.SimpleStatement`.\n\n    .. versionadded:: 2.1.3\n    \"\"\"\n\n    table = None\n    \"\"\"\n    The string name of the table this query acts on. This is used when the tablet\n    feature is enabled and in the same time :class`~.TokenAwarePolicy` is configured\n    in the profile load balancing policy.\n    \"\"\"\n\n    custom_payload = None\n    \"\"\"\n    :ref:`custom_payload` to be passed to the server.\n\n    These are only allowed when using protocol version 4 or higher.\n\n    .. versionadded:: 2.6.0\n    \"\"\"\n\n    is_idempotent = False\n    \"\"\"\n    Flag indicating whether this statement is safe to run multiple times in speculative execution.\n    \"\"\"\n\n    _serial_consistency_level = None\n    _routing_key = None\n\n    def __init__(self, retry_policy=None, consistency_level=None, routing_key=None,\n                 serial_consistency_level=None, fetch_size=FETCH_SIZE_UNSET, keyspace=None, custom_payload=None,\n                 is_idempotent=False, table=None):\n        if retry_policy and not hasattr(retry_policy, 'on_read_timeout'):  # just checking one method to detect positional parameter errors\n            raise ValueError('retry_policy should implement cassandra.policies.RetryPolicy')\n        if retry_policy is not None:\n            self.retry_policy = retry_policy\n        if consistency_level is not None:\n            self.consistency_level = consistency_level\n        self._routing_key = routing_key\n        if serial_consistency_level is not None:\n            self.serial_consistency_level = serial_consistency_level\n        if fetch_size is not FETCH_SIZE_UNSET:\n            self.fetch_size = fetch_size\n        if keyspace is not None:\n            self.keyspace = keyspace\n        if table is not None:\n            self.table = table\n        if custom_payload is not None:\n            self.custom_payload = custom_payload\n        self.is_idempotent = is_idempotent\n\n    def _key_parts_packed(self, parts):\n        for p in parts:\n            l = len(p)\n            yield struct.pack(\">H%dsB\" % l, l, p, 0)\n\n    def _get_routing_key(self):\n        return self._routing_key\n\n    def _set_routing_key(self, key):\n        if isinstance(key, (list, tuple)):\n            if len(key) == 1:\n                self._routing_key = key[0]\n            else:\n                self._routing_key = b\"\".join(self._key_parts_packed(key))\n        else:\n            self._routing_key = key\n\n    def _del_routing_key(self):\n        self._routing_key = None\n\n    routing_key = property(\n        _get_routing_key,\n        _set_routing_key,\n        _del_routing_key,\n        \"\"\"\n        The :attr:`~.TableMetadata.partition_key` portion of the primary key,\n        which can be used to determine which nodes are replicas for the query.\n\n        If the partition key is a composite, a list or tuple must be passed in.\n        Each key component should be in its packed (binary) format, so all\n        components should be strings.\n        \"\"\")\n\n    def _get_serial_consistency_level(self):\n        return self._serial_consistency_level\n\n    def _set_serial_consistency_level(self, serial_consistency_level):\n        if (serial_consistency_level is not None and\n                not ConsistencyLevel.is_serial(serial_consistency_level)):\n            raise ValueError(\n                \"serial_consistency_level must be either ConsistencyLevel.SERIAL \"\n                \"or ConsistencyLevel.LOCAL_SERIAL\")\n        self._serial_consistency_level = serial_consistency_level\n\n    def _del_serial_consistency_level(self):\n        self._serial_consistency_level = None\n\n    serial_consistency_level = property(\n        _get_serial_consistency_level,\n        _set_serial_consistency_level,\n        _del_serial_consistency_level,\n        \"\"\"\n        The serial consistency level is only used by conditional updates\n        (``INSERT``, ``UPDATE`` and ``DELETE`` with an ``IF`` condition).  For\n        those, the ``serial_consistency_level`` defines the consistency level of\n        the serial phase (or \"paxos\" phase) while the normal\n        :attr:`~.consistency_level` defines the consistency for the \"learn\" phase,\n        i.e. what type of reads will be guaranteed to see the update right away.\n        For example, if a conditional write has a :attr:`~.consistency_level` of\n        :attr:`~.ConsistencyLevel.QUORUM` (and is successful), then a\n        :attr:`~.ConsistencyLevel.QUORUM` read is guaranteed to see that write.\n        But if the regular :attr:`~.consistency_level` of that write is\n        :attr:`~.ConsistencyLevel.ANY`, then only a read with a\n        :attr:`~.consistency_level` of :attr:`~.ConsistencyLevel.SERIAL` is\n        guaranteed to see it (even a read with consistency\n        :attr:`~.ConsistencyLevel.ALL` is not guaranteed to be enough).\n\n        The serial consistency can only be one of :attr:`~.ConsistencyLevel.SERIAL`\n        or :attr:`~.ConsistencyLevel.LOCAL_SERIAL`. While ``SERIAL`` guarantees full\n        linearizability (with other ``SERIAL`` updates), ``LOCAL_SERIAL`` only\n        guarantees it in the local data center.\n\n        The serial consistency level is ignored for any query that is not a\n        conditional update. Serial reads should use the regular\n        :attr:`consistency_level`.\n\n        Serial consistency levels may only be used against Cassandra 2.0+\n        and the :attr:`~.Cluster.protocol_version` must be set to 2 or higher.\n\n        See :doc:`/lwt` for a discussion on how to work with results returned from\n        conditional statements.\n\n        .. versionadded:: 2.0.0\n        \"\"\")\n\n\nclass SimpleStatement(Statement):\n    \"\"\"\n    A simple, un-prepared query.\n    \"\"\"\n\n    def __init__(self, query_string, retry_policy=None, consistency_level=None, routing_key=None,\n                 serial_consistency_level=None, fetch_size=FETCH_SIZE_UNSET, keyspace=None,\n                 custom_payload=None, is_idempotent=False):\n        \"\"\"\n        `query_string` should be a literal CQL statement with the exception\n        of parameter placeholders that will be filled through the\n        `parameters` argument of :meth:`.Session.execute()`.\n\n        See :class:`Statement` attributes for a description of the other parameters.\n        \"\"\"\n        Statement.__init__(self, retry_policy, consistency_level, routing_key,\n                           serial_consistency_level, fetch_size, keyspace, custom_payload, is_idempotent)\n        self._query_string = query_string\n\n    @property\n    def query_string(self):\n        return self._query_string\n\n    def __str__(self):\n        consistency = ConsistencyLevel.value_to_name.get(self.consistency_level, 'Not Set')\n        return (u'<SimpleStatement query=\"%s\", consistency=%s>' %\n                (self.query_string, consistency))\n    __repr__ = __str__\n\n\nclass PreparedStatement(object):\n    \"\"\"\n    A statement that has been prepared against at least one Cassandra node.\n    Instances of this class should not be created directly, but through\n    :meth:`.Session.prepare()`.\n\n    A :class:`.PreparedStatement` should be prepared only once. Re-preparing a statement\n    may affect performance (as the operation requires a network roundtrip).\n\n    |prepared_stmt_head|: Do not use ``*`` in prepared statements if you might\n    change the schema of the table being queried. The driver and server each\n    maintain a map between metadata for a schema and statements that were\n    prepared against that schema. When a user changes a schema, e.g. by adding\n    or removing a column, the server invalidates its mappings involving that\n    schema. However, there is currently no way to propagate that invalidation\n    to drivers. Thus, after a schema change, the driver will incorrectly\n    interpret the results of ``SELECT *`` queries prepared before the schema\n    change. This is currently being addressed in `CASSANDRA-10786\n    <https://issues.apache.org/jira/browse/CASSANDRA-10786>`_.\n\n    .. |prepared_stmt_head| raw:: html\n\n       <b>A note about <code>*</code> in prepared statements</b>\n    \"\"\"\n\n    column_metadata = None  #TODO: make this bind_metadata in next major\n    retry_policy = None\n    consistency_level = None\n    custom_payload = None\n    fetch_size = FETCH_SIZE_UNSET\n    keyspace = None  # change to prepared_keyspace in major release\n    protocol_version = None\n    query_id = None\n    query_string = None\n    result_metadata = None\n    result_metadata_id = None\n    column_encryption_policy = None\n    routing_key_indexes = None\n    _routing_key_index_set = None\n    serial_consistency_level = None  # TODO never used?\n\n    def __init__(self, column_metadata, query_id, routing_key_indexes, query,\n                 keyspace, protocol_version, result_metadata, result_metadata_id,\n                 column_encryption_policy=None):\n        self.column_metadata = column_metadata\n        self.query_id = query_id\n        self.routing_key_indexes = routing_key_indexes\n        self.query_string = query\n        self.keyspace = keyspace\n        self.protocol_version = protocol_version\n        self.result_metadata = result_metadata\n        self.result_metadata_id = result_metadata_id\n        self.column_encryption_policy = column_encryption_policy\n        self.is_idempotent = False\n\n    @classmethod\n    def from_message(cls, query_id, column_metadata, pk_indexes, cluster_metadata,\n                     query, prepared_keyspace, protocol_version, result_metadata,\n                     result_metadata_id, column_encryption_policy=None):\n        if not column_metadata:\n            return PreparedStatement(column_metadata, query_id, None,\n                                     query, prepared_keyspace, protocol_version, result_metadata,\n                                     result_metadata_id, column_encryption_policy)\n\n        if pk_indexes:\n            routing_key_indexes = pk_indexes\n        else:\n            routing_key_indexes = None\n\n            first_col = column_metadata[0]\n            ks_meta = cluster_metadata.keyspaces.get(first_col.keyspace_name)\n            if ks_meta:\n                table_meta = ks_meta.tables.get(first_col.table_name)\n                if table_meta:\n                    partition_key_columns = table_meta.partition_key\n\n                    # make a map of {column_name: index} for each column in the statement\n                    statement_indexes = dict((c.name, i) for i, c in enumerate(column_metadata))\n\n                    # a list of which indexes in the statement correspond to partition key items\n                    try:\n                        routing_key_indexes = [statement_indexes[c.name]\n                                               for c in partition_key_columns]\n                    except KeyError:  # we're missing a partition key component in the prepared\n                        pass          # statement; just leave routing_key_indexes as None\n\n        return PreparedStatement(column_metadata, query_id, routing_key_indexes,\n                                 query, prepared_keyspace, protocol_version, result_metadata,\n                                 result_metadata_id, column_encryption_policy)\n\n    def bind(self, values):\n        \"\"\"\n        Creates and returns a :class:`BoundStatement` instance using `values`.\n\n        See :meth:`BoundStatement.bind` for rules on input ``values``.\n        \"\"\"\n        return BoundStatement(self).bind(values)\n\n    def is_routing_key_index(self, i):\n        if self._routing_key_index_set is None:\n            self._routing_key_index_set = set(self.routing_key_indexes) if self.routing_key_indexes else set()\n        return i in self._routing_key_index_set\n\n    def __str__(self):\n        consistency = ConsistencyLevel.value_to_name.get(self.consistency_level, 'Not Set')\n        return (u'<PreparedStatement query=\"%s\", consistency=%s>' %\n                (self.query_string, consistency))\n    __repr__ = __str__\n\n\nclass BoundStatement(Statement):\n    \"\"\"\n    A prepared statement that has been bound to a particular set of values.\n    These may be created directly or through :meth:`.PreparedStatement.bind()`.\n    \"\"\"\n\n    prepared_statement = None\n    \"\"\"\n    The :class:`PreparedStatement` instance that this was created from.\n    \"\"\"\n\n    values = None\n    \"\"\"\n    The sequence of values that were bound to the prepared statement.\n    \"\"\"\n\n    def __init__(self, prepared_statement, retry_policy=None, consistency_level=None, routing_key=None,\n                 serial_consistency_level=None, fetch_size=FETCH_SIZE_UNSET, keyspace=None,\n                 custom_payload=None):\n        \"\"\"\n        `prepared_statement` should be an instance of :class:`PreparedStatement`.\n\n        See :class:`Statement` attributes for a description of the other parameters.\n        \"\"\"\n        self.prepared_statement = prepared_statement\n\n        self.retry_policy = prepared_statement.retry_policy\n        self.consistency_level = prepared_statement.consistency_level\n        self.serial_consistency_level = prepared_statement.serial_consistency_level\n        self.fetch_size = prepared_statement.fetch_size\n        self.custom_payload = prepared_statement.custom_payload\n        self.is_idempotent = prepared_statement.is_idempotent\n        self.values = []\n\n        meta = prepared_statement.column_metadata\n        if meta:\n            self.keyspace = meta[0].keyspace_name\n            self.table = meta[0].table_name\n\n        Statement.__init__(self, retry_policy, consistency_level, routing_key,\n                           serial_consistency_level, fetch_size, keyspace, custom_payload,\n                           prepared_statement.is_idempotent)\n\n    def bind(self, values):\n        \"\"\"\n        Binds a sequence of values for the prepared statement parameters\n        and returns this instance.  Note that `values` *must* be:\n\n        * a sequence, even if you are only binding one value, or\n        * a dict that relates 1-to-1 between dict keys and columns\n\n        .. versionchanged:: 2.6.0\n\n            :data:`~.UNSET_VALUE` was introduced. These can be bound as positional parameters\n            in a sequence, or by name in a dict. Additionally, when using protocol v4+:\n\n            * short sequences will be extended to match bind parameters with UNSET_VALUE\n            * names may be omitted from a dict with UNSET_VALUE implied.\n\n        .. versionchanged:: 3.0.0\n\n            method will not throw if extra keys are present in bound dict (PYTHON-178)\n        \"\"\"\n        if values is None:\n            values = ()\n        proto_version = self.prepared_statement.protocol_version\n        col_meta = self.prepared_statement.column_metadata\n        ce_policy = self.prepared_statement.column_encryption_policy\n\n        # special case for binding dicts\n        if isinstance(values, dict):\n            values_dict = values\n            values = []\n\n            # sort values accordingly\n            for col in col_meta:\n                try:\n                    values.append(values_dict[col.name])\n                except KeyError:\n                    if proto_version >= 4:\n                        values.append(UNSET_VALUE)\n                    else:\n                        raise KeyError(\n                            'Column name `%s` not found in bound dict.' %\n                            (col.name))\n\n        value_len = len(values)\n        col_meta_len = len(col_meta)\n\n        if value_len > col_meta_len:\n            raise ValueError(\n                \"Too many arguments provided to bind() (got %d, expected %d)\" %\n                (len(values), len(col_meta)))\n\n        # this is fail-fast for clarity pre-v4. When v4 can be assumed,\n        # the error will be better reported when UNSET_VALUE is implicitly added.\n        if proto_version < 4 and self.prepared_statement.routing_key_indexes and \\\n           value_len < len(self.prepared_statement.routing_key_indexes):\n            raise ValueError(\n                \"Too few arguments provided to bind() (got %d, required %d for routing key)\" %\n                (value_len, len(self.prepared_statement.routing_key_indexes)))\n\n        self.raw_values = values\n        self.values = []\n        for value, col_spec in zip(values, col_meta):\n            if value is None:\n                self.values.append(None)\n            elif value is UNSET_VALUE:\n                if proto_version >= 4:\n                    self._append_unset_value()\n                else:\n                    raise ValueError(\"Attempt to bind UNSET_VALUE while using unsuitable protocol version (%d < 4)\" % proto_version)\n            else:\n                try:\n                    col_desc = ColDesc(col_spec.keyspace_name, col_spec.table_name, col_spec.name)\n                    uses_ce = ce_policy and ce_policy.contains_column(col_desc)\n                    col_type = ce_policy.column_type(col_desc) if uses_ce else col_spec.type\n                    col_bytes = col_type.serialize(value, proto_version)\n                    if uses_ce:\n                        col_bytes = ce_policy.encrypt(col_desc, col_bytes)\n                    self.values.append(col_bytes)\n                except (TypeError, struct.error) as exc:\n                    actual_type = type(value)\n                    message = ('Received an argument of invalid type for column \"%s\". '\n                               'Expected: %s, Got: %s; (%s)' % (col_spec.name, col_spec.type, actual_type, exc))\n                    raise TypeError(message)\n\n        if proto_version >= 4:\n            diff = col_meta_len - len(self.values)\n            if diff:\n                for _ in range(diff):\n                    self._append_unset_value()\n\n        return self\n\n    def _append_unset_value(self):\n        next_index = len(self.values)\n        if self.prepared_statement.is_routing_key_index(next_index):\n            col_meta = self.prepared_statement.column_metadata[next_index]\n            raise ValueError(\"Cannot bind UNSET_VALUE as a part of the routing key '%s'\" % col_meta.name)\n        self.values.append(UNSET_VALUE)\n\n    @property\n    def routing_key(self):\n        if not self.prepared_statement.routing_key_indexes:\n            return None\n\n        if self._routing_key is not None:\n            return self._routing_key\n\n        routing_indexes = self.prepared_statement.routing_key_indexes\n        if len(routing_indexes) == 1:\n            self._routing_key = self.values[routing_indexes[0]]\n        else:\n            self._routing_key = b\"\".join(self._key_parts_packed(self.values[i] for i in routing_indexes))\n\n        return self._routing_key\n\n    def __str__(self):\n        consistency = ConsistencyLevel.value_to_name.get(self.consistency_level, 'Not Set')\n        return (u'<BoundStatement query=\"%s\", values=%s, consistency=%s>' %\n                (self.prepared_statement.query_string, self.raw_values, consistency))\n    __repr__ = __str__\n\n\nclass BatchType(object):\n    \"\"\"\n    A BatchType is used with :class:`.BatchStatement` instances to control\n    the atomicity of the batch operation.\n\n    .. versionadded:: 2.0.0\n    \"\"\"\n\n    LOGGED = None\n    \"\"\"\n    Atomic batch operation.\n    \"\"\"\n\n    UNLOGGED = None\n    \"\"\"\n    Non-atomic batch operation.\n    \"\"\"\n\n    COUNTER = None\n    \"\"\"\n    Batches of counter operations.\n    \"\"\"\n\n    def __init__(self, name, value):\n        self.name = name\n        self.value = value\n\n    def __str__(self):\n        return self.name\n\n    def __repr__(self):\n        return \"BatchType.%s\" % (self.name, )\n\n\nBatchType.LOGGED = BatchType(\"LOGGED\", 0)\nBatchType.UNLOGGED = BatchType(\"UNLOGGED\", 1)\nBatchType.COUNTER = BatchType(\"COUNTER\", 2)\n\n\nclass BatchStatement(Statement):\n    \"\"\"\n    A protocol-level batch of operations which are applied atomically\n    by default.\n\n    .. versionadded:: 2.0.0\n    \"\"\"\n\n    batch_type = None\n    \"\"\"\n    The :class:`.BatchType` for the batch operation.  Defaults to\n    :attr:`.BatchType.LOGGED`.\n    \"\"\"\n\n    serial_consistency_level = None\n    \"\"\"\n    The same as :attr:`.Statement.serial_consistency_level`, but is only\n    supported when using protocol version 3 or higher.\n    \"\"\"\n\n    _statements_and_parameters = None\n    _session = None\n\n    def __init__(self, batch_type=BatchType.LOGGED, retry_policy=None,\n                 consistency_level=None, serial_consistency_level=None,\n                 session=None, custom_payload=None):\n        \"\"\"\n        `batch_type` specifies The :class:`.BatchType` for the batch operation.\n        Defaults to :attr:`.BatchType.LOGGED`.\n\n        `retry_policy` should be a :class:`~.RetryPolicy` instance for\n        controlling retries on the operation.\n\n        `consistency_level` should be a :class:`~.ConsistencyLevel` value\n        to be used for all operations in the batch.\n\n        `custom_payload` is a :ref:`custom_payload` passed to the server.\n        Note: as Statement objects are added to the batch, this map is\n        updated with any values found in their custom payloads. These are\n        only allowed when using protocol version 4 or higher.\n\n        Example usage:\n\n        .. code-block:: python\n\n            insert_user = session.prepare(\"INSERT INTO users (name, age) VALUES (?, ?)\")\n            batch = BatchStatement(consistency_level=ConsistencyLevel.QUORUM)\n\n            for (name, age) in users_to_insert:\n                batch.add(insert_user, (name, age))\n\n            session.execute(batch)\n\n        You can also mix different types of operations within a batch:\n\n        .. code-block:: python\n\n            batch = BatchStatement()\n            batch.add(SimpleStatement(\"INSERT INTO users (name, age) VALUES (%s, %s)\"), (name, age))\n            batch.add(SimpleStatement(\"DELETE FROM pending_users WHERE name=%s\"), (name,))\n            session.execute(batch)\n\n        .. versionadded:: 2.0.0\n\n        .. versionchanged:: 2.1.0\n            Added `serial_consistency_level` as a parameter\n\n        .. versionchanged:: 2.6.0\n            Added `custom_payload` as a parameter\n        \"\"\"\n        self.batch_type = batch_type\n        self._statements_and_parameters = []\n        self._session = session\n        Statement.__init__(self, retry_policy=retry_policy, consistency_level=consistency_level,\n                           serial_consistency_level=serial_consistency_level, custom_payload=custom_payload)\n\n    def clear(self):\n        \"\"\"\n        This is a convenience method to clear a batch statement for reuse.\n\n        *Note:* it should not be used concurrently with uncompleted execution futures executing the same\n        ``BatchStatement``.\n        \"\"\"\n        del self._statements_and_parameters[:]\n        self.keyspace = None\n        self.routing_key = None\n        if self.custom_payload:\n            self.custom_payload.clear()\n\n    def add(self, statement, parameters=None):\n        \"\"\"\n        Adds a :class:`.Statement` and optional sequence of parameters\n        to be used with the statement to the batch.\n\n        Like with other statements, parameters must be a sequence, even\n        if there is only one item.\n        \"\"\"\n        if isinstance(statement, str):\n            if parameters:\n                encoder = Encoder() if self._session is None else self._session.encoder\n                statement = bind_params(statement, parameters, encoder)\n            self._add_statement_and_params(False, statement, ())\n        elif isinstance(statement, PreparedStatement):\n            query_id = statement.query_id\n            bound_statement = statement.bind(() if parameters is None else parameters)\n            self._update_state(bound_statement)\n            self._add_statement_and_params(True, query_id, bound_statement.values)\n        elif isinstance(statement, BoundStatement):\n            if parameters:\n                raise ValueError(\n                    \"Parameters cannot be passed with a BoundStatement \"\n                    \"to BatchStatement.add()\")\n            self._update_state(statement)\n            self._add_statement_and_params(True, statement.prepared_statement.query_id, statement.values)\n        else:\n            # it must be a SimpleStatement\n            query_string = statement.query_string\n            if parameters:\n                encoder = Encoder() if self._session is None else self._session.encoder\n                query_string = bind_params(query_string, parameters, encoder)\n            self._update_state(statement)\n            self._add_statement_and_params(False, query_string, ())\n        return self\n\n    def add_all(self, statements, parameters):\n        \"\"\"\n        Adds a sequence of :class:`.Statement` objects and a matching sequence\n        of parameters to the batch. Statement and parameter sequences must be of equal length or\n        one will be truncated. :const:`None` can be used in the parameters position where are needed.\n        \"\"\"\n        for statement, value in zip(statements, parameters):\n            self.add(statement, value)\n\n    def _add_statement_and_params(self, is_prepared, statement, parameters):\n        if len(self._statements_and_parameters) >= 0xFFFF:\n            raise ValueError(\"Batch statement cannot contain more than %d statements.\" % 0xFFFF)\n        self._statements_and_parameters.append((is_prepared, statement, parameters))\n\n    def _maybe_set_routing_attributes(self, statement):\n        if self.routing_key is None:\n            if statement.keyspace and statement.routing_key:\n                self.routing_key = statement.routing_key\n                self.keyspace = statement.keyspace\n\n    def _update_custom_payload(self, statement):\n        if statement.custom_payload:\n            if self.custom_payload is None:\n                self.custom_payload = {}\n            self.custom_payload.update(statement.custom_payload)\n\n    def _update_state(self, statement):\n        self._maybe_set_routing_attributes(statement)\n        self._update_custom_payload(statement)\n\n    def __len__(self):\n        return len(self._statements_and_parameters)\n\n    def __str__(self):\n        consistency = ConsistencyLevel.value_to_name.get(self.consistency_level, 'Not Set')\n        return (u'<BatchStatement type=%s, statements=%d, consistency=%s>' %\n                (self.batch_type, len(self), consistency))\n    __repr__ = __str__\n\n\nValueSequence = cassandra.encoder.ValueSequence\n\"\"\"\nA wrapper class that is used to specify that a sequence of values should\nbe treated as a CQL list of values instead of a single column collection when used\nas part of the `parameters` argument for :meth:`.Session.execute()`.\n\nThis is typically needed when supplying a list of keys to select.\nFor example::\n\n    >>> my_user_ids = ('alice', 'bob', 'charles')\n    >>> query = \"SELECT * FROM users WHERE user_id IN %s\"\n    >>> session.execute(query, parameters=[ValueSequence(my_user_ids)])\n\n\"\"\"\n\n\ndef bind_params(query, params, encoder):\n    if isinstance(params, dict):\n        return query % dict((k, encoder.cql_encode_all_types(v)) for k, v in params.items())\n    else:\n        return query % tuple(encoder.cql_encode_all_types(v) for v in params)\n\n\nclass TraceUnavailable(Exception):\n    \"\"\"\n    Raised when complete trace details cannot be fetched from Cassandra.\n    \"\"\"\n    pass\n\n\nclass QueryTrace(object):\n    \"\"\"\n    A trace of the duration and events that occurred when executing\n    an operation.\n    \"\"\"\n\n    trace_id = None\n    \"\"\"\n    :class:`uuid.UUID` unique identifier for this tracing session.  Matches\n    the ``session_id`` column in ``system_traces.sessions`` and\n    ``system_traces.events``.\n    \"\"\"\n\n    request_type = None\n    \"\"\"\n    A string that very generally describes the traced operation.\n    \"\"\"\n\n    duration = None\n    \"\"\"\n    A :class:`datetime.timedelta` measure of the duration of the query.\n    \"\"\"\n\n    client = None\n    \"\"\"\n    The IP address of the client that issued this request\n\n    This is only available when using Cassandra 2.2+\n    \"\"\"\n\n    coordinator = None\n    \"\"\"\n    The IP address of the host that acted as coordinator for this request.\n    \"\"\"\n\n    parameters = None\n    \"\"\"\n    A :class:`dict` of parameters for the traced operation, such as the\n    specific query string.\n    \"\"\"\n\n    started_at = None\n    \"\"\"\n    A UTC :class:`datetime.datetime` object describing when the operation\n    was started.\n    \"\"\"\n\n    events = None\n    \"\"\"\n    A chronologically sorted list of :class:`.TraceEvent` instances\n    representing the steps the traced operation went through.  This\n    corresponds to the rows in ``system_traces.events`` for this tracing\n    session.\n    \"\"\"\n\n    _session = None\n\n    _SELECT_SESSIONS_FORMAT = \"SELECT * FROM system_traces.sessions WHERE session_id = %s\"\n    _SELECT_EVENTS_FORMAT = \"SELECT * FROM system_traces.events WHERE session_id = %s\"\n    _BASE_RETRY_SLEEP = 0.003\n\n    def __init__(self, trace_id, session):\n        self.trace_id = trace_id\n        self._session = session\n\n    def populate(self, max_wait=2.0, wait_for_complete=True, query_cl=None):\n        \"\"\"\n        Retrieves the actual tracing details from Cassandra and populates the\n        attributes of this instance.  Because tracing details are stored\n        asynchronously by Cassandra, this may need to retry the session\n        detail fetch.  If the trace is still not available after `max_wait`\n        seconds, :exc:`.TraceUnavailable` will be raised; if `max_wait` is\n        :const:`None`, this will retry forever.\n\n        `wait_for_complete=False` bypasses the wait for duration to be populated.\n        This can be used to query events from partial sessions.\n\n        `query_cl` specifies a consistency level to use for polling the trace tables,\n        if it should be different than the session default.\n        \"\"\"\n        attempt = 0\n        start = time.time()\n        while True:\n            time_spent = time.time() - start\n            if max_wait is not None and time_spent >= max_wait:\n                raise TraceUnavailable(\n                    \"Trace information was not available within %f seconds. Consider raising Session.max_trace_wait.\" % (max_wait,))\n\n            log.debug(\"Attempting to fetch trace info for trace ID: %s\", self.trace_id)\n            metadata_request_timeout = self._session.cluster.control_connection and self._session.cluster.control_connection._metadata_request_timeout\n            session_results = self._execute(\n                SimpleStatement(maybe_add_timeout_to_query(self._SELECT_SESSIONS_FORMAT, metadata_request_timeout), consistency_level=query_cl), (self.trace_id,), time_spent, max_wait)\n\n            # PYTHON-730: There is race condition that the duration mutation is written before started_at the for fast queries\n            session_row = session_results.one() if session_results else None\n            is_complete = session_row is not None and session_row.duration is not None and session_row.started_at is not None\n            if not session_results or (wait_for_complete and not is_complete):\n                time.sleep(self._BASE_RETRY_SLEEP * (2 ** attempt))\n                attempt += 1\n                continue\n            if is_complete:\n                log.debug(\"Fetched trace info for trace ID: %s\", self.trace_id)\n            else:\n                log.debug(\"Fetching parital trace info for trace ID: %s\", self.trace_id)\n\n            self.request_type = session_row.request\n            self.duration = timedelta(microseconds=session_row.duration) if is_complete else None\n            self.started_at = session_row.started_at\n            self.coordinator = session_row.coordinator\n            self.parameters = session_row.parameters\n            # since C* 2.2\n            self.client = getattr(session_row, 'client', None)\n\n            log.debug(\"Attempting to fetch trace events for trace ID: %s\", self.trace_id)\n            time_spent = time.time() - start\n            event_results = self._execute(\n                SimpleStatement(maybe_add_timeout_to_query(self._SELECT_EVENTS_FORMAT, metadata_request_timeout),\n                                consistency_level=query_cl),\n                (self.trace_id,),\n                time_spent,\n                max_wait)\n            log.debug(\"Fetched trace events for trace ID: %s\", self.trace_id)\n            self.events = tuple(TraceEvent(r.activity, r.event_id, r.source, r.source_elapsed, r.thread)\n                                for r in event_results)\n            break\n\n    def _execute(self, query, parameters, time_spent, max_wait):\n        timeout = (max_wait - time_spent) if max_wait is not None else None\n        future = self._session._create_response_future(query, parameters, trace=False, custom_payload=None, timeout=timeout)\n        # in case the user switched the row factory, set it to namedtuple for this query\n        future.row_factory = named_tuple_factory\n        future.send_request()\n\n        try:\n            return future.result()\n        except OperationTimedOut:\n            raise TraceUnavailable(\"Trace information was not available within %f seconds\" % (max_wait,))\n\n    def __str__(self):\n        return \"%s [%s] coordinator: %s, started at: %s, duration: %s, parameters: %s\" \\\n               % (self.request_type, self.trace_id, self.coordinator, self.started_at,\n                  self.duration, self.parameters)\n\n\nclass TraceEvent(object):\n    \"\"\"\n    Representation of a single event within a query trace.\n    \"\"\"\n\n    description = None\n    \"\"\"\n    A brief description of the event.\n    \"\"\"\n\n    datetime = None\n    \"\"\"\n    A UTC :class:`datetime.datetime` marking when the event occurred.\n    \"\"\"\n\n    source = None\n    \"\"\"\n    The IP address of the node this event occurred on.\n    \"\"\"\n\n    source_elapsed = None\n    \"\"\"\n    A :class:`datetime.timedelta` measuring the amount of time until\n    this event occurred starting from when :attr:`.source` first\n    received the query.\n    \"\"\"\n\n    thread_name = None\n    \"\"\"\n    The name of the thread that this event occurred on.\n    \"\"\"\n\n    def __init__(self, description, timeuuid, source, source_elapsed, thread_name):\n        self.description = description\n        self.datetime = datetime.fromtimestamp(unix_time_from_uuid1(timeuuid), tz=timezone.utc)\n        self.source = source\n        if source_elapsed is not None:\n            self.source_elapsed = timedelta(microseconds=source_elapsed)\n        else:\n            self.source_elapsed = None\n        self.thread_name = thread_name\n\n    def __str__(self):\n        return \"%s on %s[%s] at %s\" % (self.description, self.source, self.thread_name, self.datetime)\n\n\n# TODO remove next major since we can target using the `host` attribute of session.execute\nclass HostTargetingStatement(object):\n    \"\"\"\n    Wraps any query statement and attaches a target host, making\n    it usable in a targeted LBP without modifying the user's statement.\n    \"\"\"\n    def __init__(self, inner_statement, target_host):\n            self.__class__ = type(inner_statement.__class__.__name__,\n                                  (self.__class__, inner_statement.__class__),\n                                  {})\n            self.__dict__ = inner_statement.__dict__\n            self.target_host = target_host\n",
    "cassandra/__init__.py": "# Copyright DataStax, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom enum import Enum\nimport logging\n\n\nclass NullHandler(logging.Handler):\n\n    def emit(self, record):\n        pass\n\nlogging.getLogger('cassandra').addHandler(NullHandler())\n\n__version_info__ = (3, 28, 0)\n__version__ = '.'.join(map(str, __version_info__))\n\n\nclass ConsistencyLevel(object):\n    \"\"\"\n    Spcifies how many replicas must respond for an operation to be considered\n    a success.  By default, ``ONE`` is used for all operations.\n    \"\"\"\n\n    ANY = 0\n    \"\"\"\n    Only requires that one replica receives the write *or* the coordinator\n    stores a hint to replay later. Valid only for writes.\n    \"\"\"\n\n    ONE = 1\n    \"\"\"\n    Only one replica needs to respond to consider the operation a success\n    \"\"\"\n\n    TWO = 2\n    \"\"\"\n    Two replicas must respond to consider the operation a success\n    \"\"\"\n\n    THREE = 3\n    \"\"\"\n    Three replicas must respond to consider the operation a success\n    \"\"\"\n\n    QUORUM = 4\n    \"\"\"\n    ``ceil(RF/2) + 1`` replicas must respond to consider the operation a success\n    \"\"\"\n\n    ALL = 5\n    \"\"\"\n    All replicas must respond to consider the operation a success\n    \"\"\"\n\n    LOCAL_QUORUM = 6\n    \"\"\"\n    Requires a quorum of replicas in the local datacenter\n    \"\"\"\n\n    EACH_QUORUM = 7\n    \"\"\"\n    Requires a quorum of replicas in each datacenter\n    \"\"\"\n\n    SERIAL = 8\n    \"\"\"\n    For conditional inserts/updates that utilize Cassandra's lightweight\n    transactions, this requires consensus among all replicas for the\n    modified data.\n    \"\"\"\n\n    LOCAL_SERIAL = 9\n    \"\"\"\n    Like :attr:`~ConsistencyLevel.SERIAL`, but only requires consensus\n    among replicas in the local datacenter.\n    \"\"\"\n\n    LOCAL_ONE = 10\n    \"\"\"\n    Sends a request only to replicas in the local datacenter and waits for\n    one response.\n    \"\"\"\n\n    @staticmethod\n    def is_serial(cl):\n        return cl == ConsistencyLevel.SERIAL or cl == ConsistencyLevel.LOCAL_SERIAL\n\n\nConsistencyLevel.value_to_name = {\n    ConsistencyLevel.ANY: 'ANY',\n    ConsistencyLevel.ONE: 'ONE',\n    ConsistencyLevel.TWO: 'TWO',\n    ConsistencyLevel.THREE: 'THREE',\n    ConsistencyLevel.QUORUM: 'QUORUM',\n    ConsistencyLevel.ALL: 'ALL',\n    ConsistencyLevel.LOCAL_QUORUM: 'LOCAL_QUORUM',\n    ConsistencyLevel.EACH_QUORUM: 'EACH_QUORUM',\n    ConsistencyLevel.SERIAL: 'SERIAL',\n    ConsistencyLevel.LOCAL_SERIAL: 'LOCAL_SERIAL',\n    ConsistencyLevel.LOCAL_ONE: 'LOCAL_ONE'\n}\n\nConsistencyLevel.name_to_value = {\n    'ANY': ConsistencyLevel.ANY,\n    'ONE': ConsistencyLevel.ONE,\n    'TWO': ConsistencyLevel.TWO,\n    'THREE': ConsistencyLevel.THREE,\n    'QUORUM': ConsistencyLevel.QUORUM,\n    'ALL': ConsistencyLevel.ALL,\n    'LOCAL_QUORUM': ConsistencyLevel.LOCAL_QUORUM,\n    'EACH_QUORUM': ConsistencyLevel.EACH_QUORUM,\n    'SERIAL': ConsistencyLevel.SERIAL,\n    'LOCAL_SERIAL': ConsistencyLevel.LOCAL_SERIAL,\n    'LOCAL_ONE': ConsistencyLevel.LOCAL_ONE\n}\n\n\ndef consistency_value_to_name(value):\n    return ConsistencyLevel.value_to_name[value] if value is not None else \"Not Set\"\n\n\nclass ProtocolVersion(object):\n    \"\"\"\n    Defines native protocol versions supported by this driver.\n    \"\"\"\n    V1 = 1\n    \"\"\"\n    v1, supported in Cassandra 1.2-->2.2\n    \"\"\"\n\n    V2 = 2\n    \"\"\"\n    v2, supported in Cassandra 2.0-->2.2;\n    added support for lightweight transactions, batch operations, and automatic query paging.\n    \"\"\"\n\n    V3 = 3\n    \"\"\"\n    v3, supported in Cassandra 2.1-->3.x+;\n    added support for protocol-level client-side timestamps (see :attr:`.Session.use_client_timestamp`),\n    serial consistency levels for :class:`~.BatchStatement`, and an improved connection pool.\n    \"\"\"\n\n    V4 = 4\n    \"\"\"\n    v4, supported in Cassandra 2.2-->3.x+;\n    added a number of new types, server warnings, new failure messages, and custom payloads. Details in the\n    `project docs <https://github.com/apache/cassandra/blob/trunk/doc/native_protocol_v4.spec>`_\n    \"\"\"\n\n    V5 = 5\n    \"\"\"\n    v5, in beta from 3.x+. Finalised in 4.0-beta5\n    \"\"\"\n\n    V6 = 6\n    \"\"\"\n    v6, in beta from 4.0-beta5\n    \"\"\"\n\n    DSE_V1 = 0x41\n    \"\"\"\n    DSE private protocol v1, supported in DSE 5.1+\n    \"\"\"\n\n    DSE_V2 = 0x42\n    \"\"\"\n    DSE private protocol v2, supported in DSE 6.0+\n    \"\"\"\n\n    SUPPORTED_VERSIONS = (DSE_V2, DSE_V1, V6, V5, V4, V3, V2, V1)\n    \"\"\"\n    A tuple of all supported protocol versions\n    \"\"\"\n\n    BETA_VERSIONS = (V6,)\n    \"\"\"\n    A tuple of all beta protocol versions\n    \"\"\"\n\n    MIN_SUPPORTED = min(SUPPORTED_VERSIONS)\n    \"\"\"\n    Minimum protocol version supported by this driver.\n    \"\"\"\n\n    MAX_SUPPORTED = max(SUPPORTED_VERSIONS)\n    \"\"\"\n    Maximum protocol version supported by this driver.\n    \"\"\"\n\n    @classmethod\n    def get_lower_supported(cls, previous_version):\n        \"\"\"\n        Return the lower supported protocol version. Beta versions are omitted.\n        \"\"\"\n        try:\n            version = next(v for v in sorted(ProtocolVersion.SUPPORTED_VERSIONS, reverse=True) if\n                           v not in ProtocolVersion.BETA_VERSIONS and v < previous_version)\n        except StopIteration:\n            version = 0\n\n        return version\n\n    @classmethod\n    def uses_int_query_flags(cls, version):\n        return version >= cls.V5\n\n    @classmethod\n    def uses_prepare_flags(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def uses_prepared_metadata(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def uses_error_code_map(cls, version):\n        return version >= cls.V5\n\n    @classmethod\n    def uses_keyspace_flag(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def has_continuous_paging_support(cls, version):\n        return version >= cls.DSE_V1\n\n    @classmethod\n    def has_continuous_paging_next_pages(cls, version):\n        return version >= cls.DSE_V2\n\n    @classmethod\n    def has_checksumming_support(cls, version):\n        return cls.V5 <= version < cls.DSE_V1\n\n\nclass WriteType(object):\n    \"\"\"\n    For usage with :class:`.RetryPolicy`, this describe a type\n    of write operation.\n    \"\"\"\n\n    SIMPLE = 0\n    \"\"\"\n    A write to a single partition key. Such writes are guaranteed to be atomic\n    and isolated.\n    \"\"\"\n\n    BATCH = 1\n    \"\"\"\n    A write to multiple partition keys that used the distributed batch log to\n    ensure atomicity.\n    \"\"\"\n\n    UNLOGGED_BATCH = 2\n    \"\"\"\n    A write to multiple partition keys that did not use the distributed batch\n    log. Atomicity for such writes is not guaranteed.\n    \"\"\"\n\n    COUNTER = 3\n    \"\"\"\n    A counter write (for one or multiple partition keys). Such writes should\n    not be replayed in order to avoid overcount.\n    \"\"\"\n\n    BATCH_LOG = 4\n    \"\"\"\n    The initial write to the distributed batch log that Cassandra performs\n    internally before a BATCH write.\n    \"\"\"\n\n    CAS = 5\n    \"\"\"\n    A lighweight-transaction write, such as \"DELETE ... IF EXISTS\".\n    \"\"\"\n\n    VIEW = 6\n    \"\"\"\n    This WriteType is only seen in results for requests that were unable to\n    complete MV operations.\n    \"\"\"\n\n    CDC = 7\n    \"\"\"\n    This WriteType is only seen in results for requests that were unable to\n    complete CDC operations.\n    \"\"\"\n\n\nWriteType.name_to_value = {\n    'SIMPLE': WriteType.SIMPLE,\n    'BATCH': WriteType.BATCH,\n    'UNLOGGED_BATCH': WriteType.UNLOGGED_BATCH,\n    'COUNTER': WriteType.COUNTER,\n    'BATCH_LOG': WriteType.BATCH_LOG,\n    'CAS': WriteType.CAS,\n    'VIEW': WriteType.VIEW,\n    'CDC': WriteType.CDC\n}\n\n\nWriteType.value_to_name = {v: k for k, v in WriteType.name_to_value.items()}\n\n\nclass SchemaChangeType(object):\n    DROPPED = 'DROPPED'\n    CREATED = 'CREATED'\n    UPDATED = 'UPDATED'\n\n\nclass SchemaTargetType(object):\n    KEYSPACE = 'KEYSPACE'\n    TABLE = 'TABLE'\n    TYPE = 'TYPE'\n    FUNCTION = 'FUNCTION'\n    AGGREGATE = 'AGGREGATE'\n\n\nclass SignatureDescriptor(object):\n\n    def __init__(self, name, argument_types):\n        self.name = name\n        self.argument_types = argument_types\n\n    @property\n    def signature(self):\n        \"\"\"\n        function signature string in the form 'name([type0[,type1[...]]])'\n\n        can be used to uniquely identify overloaded function names within a keyspace\n        \"\"\"\n        return self.format_signature(self.name, self.argument_types)\n\n    @staticmethod\n    def format_signature(name, argument_types):\n        return \"%s(%s)\" % (name, ','.join(t for t in argument_types))\n\n    def __repr__(self):\n        return \"%s(%s, %s)\" % (self.__class__.__name__, self.name, self.argument_types)\n\n\nclass UserFunctionDescriptor(SignatureDescriptor):\n    \"\"\"\n    Describes a User function by name and argument signature\n    \"\"\"\n\n    name = None\n    \"\"\"\n    name of the function\n    \"\"\"\n\n    argument_types = None\n    \"\"\"\n    Ordered list of CQL argument type names comprising the type signature\n    \"\"\"\n\n\nclass UserAggregateDescriptor(SignatureDescriptor):\n    \"\"\"\n    Describes a User aggregate function by name and argument signature\n    \"\"\"\n\n    name = None\n    \"\"\"\n    name of the aggregate\n    \"\"\"\n\n    argument_types = None\n    \"\"\"\n    Ordered list of CQL argument type names comprising the type signature\n    \"\"\"\n\n\nclass DriverException(Exception):\n    \"\"\"\n    Base for all exceptions explicitly raised by the driver.\n    \"\"\"\n    pass\n\n\nclass RequestExecutionException(DriverException):\n    \"\"\"\n    Base for request execution exceptions returned from the server.\n    \"\"\"\n    pass\n\n\nclass Unavailable(RequestExecutionException):\n    \"\"\"\n    There were not enough live replicas to satisfy the requested consistency\n    level, so the coordinator node immediately failed the request without\n    forwarding it to any replicas.\n    \"\"\"\n\n    consistency = None\n    \"\"\" The requested :class:`ConsistencyLevel` \"\"\"\n\n    required_replicas = None\n    \"\"\" The number of replicas that needed to be live to complete the operation \"\"\"\n\n    alive_replicas = None\n    \"\"\" The number of replicas that were actually alive \"\"\"\n\n    def __init__(self, summary_message, consistency=None, required_replicas=None, alive_replicas=None):\n        self.consistency = consistency\n        self.required_replicas = required_replicas\n        self.alive_replicas = alive_replicas\n        Exception.__init__(self, summary_message + ' info=' +\n                           repr({'consistency': consistency_value_to_name(consistency),\n                                 'required_replicas': required_replicas,\n                                 'alive_replicas': alive_replicas}))\n\n\nclass Timeout(RequestExecutionException):\n    \"\"\"\n    Replicas failed to respond to the coordinator node before timing out.\n    \"\"\"\n\n    consistency = None\n    \"\"\" The requested :class:`ConsistencyLevel` \"\"\"\n\n    required_responses = None\n    \"\"\" The number of required replica responses \"\"\"\n\n    received_responses = None\n    \"\"\"\n    The number of replicas that responded before the coordinator timed out\n    the operation\n    \"\"\"\n\n    def __init__(self, summary_message, consistency=None, required_responses=None,\n                 received_responses=None, **kwargs):\n        self.consistency = consistency\n        self.required_responses = required_responses\n        self.received_responses = received_responses\n\n        if \"write_type\" in kwargs:\n            kwargs[\"write_type\"] = WriteType.value_to_name[kwargs[\"write_type\"]]\n\n        info = {'consistency': consistency_value_to_name(consistency),\n                'required_responses': required_responses,\n                'received_responses': received_responses}\n        info.update(kwargs)\n\n        Exception.__init__(self, summary_message + ' info=' + repr(info))\n\n\nclass ReadTimeout(Timeout):\n    \"\"\"\n    A subclass of :exc:`Timeout` for read operations.\n\n    This indicates that the replicas failed to respond to the coordinator\n    node before the configured timeout. This timeout is configured in\n    ``cassandra.yaml`` with the ``read_request_timeout_in_ms``\n    and ``range_request_timeout_in_ms`` options.\n    \"\"\"\n\n    data_retrieved = None\n    \"\"\"\n    A boolean indicating whether the requested data was retrieved\n    by the coordinator from any replicas before it timed out the\n    operation\n    \"\"\"\n\n    def __init__(self, message, data_retrieved=None, **kwargs):\n        Timeout.__init__(self, message, **kwargs)\n        self.data_retrieved = data_retrieved\n\n\nclass WriteTimeout(Timeout):\n    \"\"\"\n    A subclass of :exc:`Timeout` for write operations.\n\n    This indicates that the replicas failed to respond to the coordinator\n    node before the configured timeout. This timeout is configured in\n    ``cassandra.yaml`` with the ``write_request_timeout_in_ms``\n    option.\n    \"\"\"\n\n    write_type = None\n    \"\"\"\n    The type of write operation, enum on :class:`~cassandra.policies.WriteType`\n    \"\"\"\n\n    def __init__(self, message, write_type=None, **kwargs):\n        kwargs[\"write_type\"] = write_type\n        Timeout.__init__(self, message, **kwargs)\n        self.write_type = write_type\n\n\nclass CDCWriteFailure(RequestExecutionException):\n    \"\"\"\n    Hit limit on data in CDC folder, writes are rejected\n    \"\"\"\n    def __init__(self, message):\n        Exception.__init__(self, message)\n\n\nclass CoordinationFailure(RequestExecutionException):\n    \"\"\"\n    Replicas sent a failure to the coordinator.\n    \"\"\"\n\n    consistency = None\n    \"\"\" The requested :class:`ConsistencyLevel` \"\"\"\n\n    required_responses = None\n    \"\"\" The number of required replica responses \"\"\"\n\n    received_responses = None\n    \"\"\"\n    The number of replicas that responded before the coordinator timed out\n    the operation\n    \"\"\"\n\n    failures = None\n    \"\"\"\n    The number of replicas that sent a failure message\n    \"\"\"\n\n    error_code_map = None\n    \"\"\"\n    A map of inet addresses to error codes representing replicas that sent\n    a failure message.  Only set when `protocol_version` is 5 or higher.\n    \"\"\"\n\n    def __init__(self, summary_message, consistency=None, required_responses=None,\n                 received_responses=None, failures=None, error_code_map=None):\n        self.consistency = consistency\n        self.required_responses = required_responses\n        self.received_responses = received_responses\n        self.failures = failures\n        self.error_code_map = error_code_map\n\n        info_dict = {\n            'consistency': consistency_value_to_name(consistency),\n            'required_responses': required_responses,\n            'received_responses': received_responses,\n            'failures': failures\n        }\n\n        if error_code_map is not None:\n            # make error codes look like \"0x002a\"\n            formatted_map = dict((addr, '0x%04x' % err_code)\n                                 for (addr, err_code) in error_code_map.items())\n            info_dict['error_code_map'] = formatted_map\n\n        Exception.__init__(self, summary_message + ' info=' + repr(info_dict))\n\n\nclass ReadFailure(CoordinationFailure):\n    \"\"\"\n    A subclass of :exc:`CoordinationFailure` for read operations.\n\n    This indicates that the replicas sent a failure message to the coordinator.\n    \"\"\"\n\n    data_retrieved = None\n    \"\"\"\n    A boolean indicating whether the requested data was retrieved\n    by the coordinator from any replicas before it timed out the\n    operation\n    \"\"\"\n\n    def __init__(self, message, data_retrieved=None, **kwargs):\n        CoordinationFailure.__init__(self, message, **kwargs)\n        self.data_retrieved = data_retrieved\n\n\nclass WriteFailure(CoordinationFailure):\n    \"\"\"\n    A subclass of :exc:`CoordinationFailure` for write operations.\n\n    This indicates that the replicas sent a failure message to the coordinator.\n    \"\"\"\n\n    write_type = None\n    \"\"\"\n    The type of write operation, enum on :class:`~cassandra.policies.WriteType`\n    \"\"\"\n\n    def __init__(self, message, write_type=None, **kwargs):\n        CoordinationFailure.__init__(self, message, **kwargs)\n        self.write_type = write_type\n\n\nclass FunctionFailure(RequestExecutionException):\n    \"\"\"\n    User Defined Function failed during execution\n    \"\"\"\n\n    keyspace = None\n    \"\"\"\n    Keyspace of the function\n    \"\"\"\n\n    function = None\n    \"\"\"\n    Name of the function\n    \"\"\"\n\n    arg_types = None\n    \"\"\"\n    List of argument type names of the function\n    \"\"\"\n\n    def __init__(self, summary_message, keyspace, function, arg_types):\n        self.keyspace = keyspace\n        self.function = function\n        self.arg_types = arg_types\n        Exception.__init__(self, summary_message)\n\n\nclass RequestValidationException(DriverException):\n    \"\"\"\n    Server request validation failed\n    \"\"\"\n    pass\n\n\nclass ConfigurationException(RequestValidationException):\n    \"\"\"\n    Server indicated request errro due to current configuration\n    \"\"\"\n    pass\n\n\nclass AlreadyExists(ConfigurationException):\n    \"\"\"\n    An attempt was made to create a keyspace or table that already exists.\n    \"\"\"\n\n    keyspace = None\n    \"\"\"\n    The name of the keyspace that already exists, or, if an attempt was\n    made to create a new table, the keyspace that the table is in.\n    \"\"\"\n\n    table = None\n    \"\"\"\n    The name of the table that already exists, or, if an attempt was\n    make to create a keyspace, :const:`None`.\n    \"\"\"\n\n    def __init__(self, keyspace=None, table=None):\n        if table:\n            message = \"Table '%s.%s' already exists\" % (keyspace, table)\n        else:\n            message = \"Keyspace '%s' already exists\" % (keyspace,)\n\n        Exception.__init__(self, message)\n        self.keyspace = keyspace\n        self.table = table\n\n\nclass InvalidRequest(RequestValidationException):\n    \"\"\"\n    A query was made that was invalid for some reason, such as trying to set\n    the keyspace for a connection to a nonexistent keyspace.\n    \"\"\"\n    pass\n\n\nclass Unauthorized(RequestValidationException):\n    \"\"\"\n    The current user is not authorized to perform the requested operation.\n    \"\"\"\n    pass\n\n\nclass AuthenticationFailed(DriverException):\n    \"\"\"\n    Failed to authenticate.\n    \"\"\"\n    pass\n\n\nclass OperationTimedOut(DriverException):\n    \"\"\"\n    The operation took longer than the specified (client-side) timeout\n    to complete.  This is not an error generated by Cassandra, only\n    the driver.\n    \"\"\"\n\n    errors = None\n    \"\"\"\n    A dict of errors keyed by the :class:`~.Host` against which they occurred.\n    \"\"\"\n\n    last_host = None\n    \"\"\"\n    The last :class:`~.Host` this operation was attempted against.\n    \"\"\"\n\n    def __init__(self, errors=None, last_host=None):\n        self.errors = errors\n        self.last_host = last_host\n        message = \"errors=%s, last_host=%s\" % (self.errors, self.last_host)\n        Exception.__init__(self, message)\n\n\nclass UnsupportedOperation(DriverException):\n    \"\"\"\n    An attempt was made to use a feature that is not supported by the\n    selected protocol version.  See :attr:`Cluster.protocol_version`\n    for more details.\n    \"\"\"\n    pass\n\n\nclass UnresolvableContactPoints(DriverException):\n    \"\"\"\n    The driver was unable to resolve any provided hostnames.\n\n    Note that this is *not* raised when a :class:`.Cluster` is created with no\n    contact points, only when lookup fails for all hosts\n    \"\"\"\n    pass\n\n\nclass OperationType(Enum):\n    Read = 0\n    Write = 1\n\nclass RateLimitReached(ConfigurationException):\n    '''\n    Rate limit was exceeded for a partition affected by the request.\n    '''\n    op_type = None\n    rejected_by_coordinator = False\n\n    def __init__(self, op_type=None, rejected_by_coordinator=False):\n        self.op_type = op_type\n        self.rejected_by_coordinator = rejected_by_coordinator\n        message = f\"[request_error_rate_limit_reached OpType={op_type.name} RejectedByCoordinator={rejected_by_coordinator}]\"\n        Exception.__init__(self, message)\n",
    "cassandra/policies.py": "# Copyright DataStax, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport random\n\nfrom collections import namedtuple\nfrom functools import lru_cache\nfrom itertools import islice, cycle, groupby, repeat\nimport logging\nfrom random import randint, shuffle\nfrom threading import Lock\nimport socket\nimport warnings\n\nlog = logging.getLogger(__name__)\n\nfrom cassandra import WriteType as WT\n\n\n# This is done this way because WriteType was originally\n# defined here and in order not to break the API.\n# It may removed in the next mayor.\nWriteType = WT\n\nfrom cassandra import ConsistencyLevel, OperationTimedOut\n\nclass HostDistance(object):\n    \"\"\"\n    A measure of how \"distant\" a node is from the client, which\n    may influence how the load balancer distributes requests\n    and how many connections are opened to the node.\n    \"\"\"\n\n    IGNORED = -1\n    \"\"\"\n    A node with this distance should never be queried or have\n    connections opened to it.\n    \"\"\"\n\n    LOCAL_RACK = 0\n    \"\"\"\n    Nodes with ``LOCAL_RACK`` distance will be preferred for operations\n    under some load balancing policies (such as :class:`.RackAwareRoundRobinPolicy`)\n    and will have a greater number of connections opened against\n    them by default.\n\n    This distance is typically used for nodes within the same\n    datacenter and the same rack as the client.\n    \"\"\"\n\n    LOCAL = 1\n    \"\"\"\n    Nodes with ``LOCAL`` distance will be preferred for operations\n    under some load balancing policies (such as :class:`.DCAwareRoundRobinPolicy`)\n    and will have a greater number of connections opened against\n    them by default.\n\n    This distance is typically used for nodes within the same\n    datacenter as the client.\n    \"\"\"\n\n    REMOTE = 2\n    \"\"\"\n    Nodes with ``REMOTE`` distance will be treated as a last resort\n    by some load balancing policies (such as :class:`.DCAwareRoundRobinPolicy`\n    and :class:`.RackAwareRoundRobinPolicy`)and will have a smaller number of\n    connections opened against them by default.\n\n    This distance is typically used for nodes outside of the\n    datacenter that the client is running in.\n    \"\"\"\n\n\nclass HostStateListener(object):\n\n    def on_up(self, host):\n        \"\"\" Called when a node is marked up. \"\"\"\n        raise NotImplementedError()\n\n    def on_down(self, host):\n        \"\"\" Called when a node is marked down. \"\"\"\n        raise NotImplementedError()\n\n    def on_add(self, host):\n        \"\"\"\n        Called when a node is added to the cluster.  The newly added node\n        should be considered up.\n        \"\"\"\n        raise NotImplementedError()\n\n    def on_remove(self, host):\n        \"\"\" Called when a node is removed from the cluster. \"\"\"\n        raise NotImplementedError()\n\n\nclass LoadBalancingPolicy(HostStateListener):\n    \"\"\"\n    Load balancing policies are used to decide how to distribute\n    requests among all possible coordinator nodes in the cluster.\n\n    In particular, they may focus on querying \"near\" nodes (those\n    in a local datacenter) or on querying nodes who happen to\n    be replicas for the requested data.\n\n    You may also use subclasses of :class:`.LoadBalancingPolicy` for\n    custom behavior.\n\n    You should always use immutable collections (e.g., tuples or\n    frozensets) to store information about hosts to prevent accidental\n    modification. When there are changes to the hosts (e.g., a host is\n    down or up), the old collection should be replaced with a new one.\n    \"\"\"\n\n    _hosts_lock = None\n\n    def __init__(self):\n        self._hosts_lock = Lock()\n\n    def distance(self, host):\n        \"\"\"\n        Returns a measure of how remote a :class:`~.pool.Host` is in\n        terms of the :class:`.HostDistance` enums.\n        \"\"\"\n        raise NotImplementedError()\n\n    def populate(self, cluster, hosts):\n        \"\"\"\n        This method is called to initialize the load balancing\n        policy with a set of :class:`.Host` instances before its\n        first use.  The `cluster` parameter is an instance of\n        :class:`.Cluster`.\n        \"\"\"\n        raise NotImplementedError()\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        \"\"\"\n        Given a :class:`~.query.Statement` instance, return a iterable\n        of :class:`.Host` instances which should be queried in that\n        order.  A generator may work well for custom implementations\n        of this method.\n\n        Note that the `query` argument may be :const:`None` when preparing\n        statements.\n\n        `working_keyspace` should be the string name of the current keyspace,\n        as set through :meth:`.Session.set_keyspace()` or with a ``USE``\n        statement.\n        \"\"\"\n        raise NotImplementedError()\n\n    def check_supported(self):\n        \"\"\"\n        This will be called after the cluster Metadata has been initialized.\n        If the load balancing policy implementation cannot be supported for\n        some reason (such as a missing C extension), this is the point at\n        which it should raise an exception.\n        \"\"\"\n        pass\n\n\nclass RoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    A subclass of :class:`.LoadBalancingPolicy` which evenly\n    distributes queries across all nodes in the cluster,\n    regardless of what datacenter the nodes may be in.\n    \"\"\"\n    _live_hosts = frozenset(())\n    _position = 0\n\n    def populate(self, cluster, hosts):\n        self._live_hosts = frozenset(hosts)\n        if len(hosts) > 1:\n            self._position = randint(0, len(hosts) - 1)\n\n    def distance(self, host):\n        return HostDistance.LOCAL\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        # not thread-safe, but we don't care much about lost increments\n        # for the purposes of load balancing\n        pos = self._position\n        self._position += 1\n\n        hosts = self._live_hosts\n        length = len(hosts)\n        if length:\n            pos %= length\n            return islice(cycle(hosts), pos, pos + length)\n        else:\n            return []\n\n    def on_up(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.union((host, ))\n\n    def on_down(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.difference((host, ))\n\n    def on_add(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.union((host, ))\n\n    def on_remove(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.difference((host, ))\n\n\nclass DCAwareRoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    Similar to :class:`.RoundRobinPolicy`, but prefers hosts\n    in the local datacenter and only uses nodes in remote\n    datacenters as a last resort.\n    \"\"\"\n\n    local_dc = None\n    used_hosts_per_remote_dc = 0\n\n    def __init__(self, local_dc='', used_hosts_per_remote_dc=0):\n        \"\"\"\n        The `local_dc` parameter should be the name of the datacenter\n        (such as is reported by ``nodetool ring``) that should\n        be considered local. If not specified, the driver will choose\n        a local_dc based on the first host among :attr:`.Cluster.contact_points`\n        having a valid DC. If relying on this mechanism, all specified\n        contact points should be nodes in a single, local DC.\n\n        `used_hosts_per_remote_dc` controls how many nodes in\n        each remote datacenter will have connections opened\n        against them. In other words, `used_hosts_per_remote_dc` hosts\n        will be considered :attr:`~.HostDistance.REMOTE` and the\n        rest will be considered :attr:`~.HostDistance.IGNORED`.\n        By default, all remote hosts are ignored.\n        \"\"\"\n        self.local_dc = local_dc\n        self.used_hosts_per_remote_dc = used_hosts_per_remote_dc\n        self._dc_live_hosts = {}\n        self._position = 0\n        self._endpoints = []\n        LoadBalancingPolicy.__init__(self)\n\n    def _dc(self, host):\n        return host.datacenter or self.local_dc\n\n    def populate(self, cluster, hosts):\n        for dc, dc_hosts in groupby(hosts, lambda h: self._dc(h)):\n            self._dc_live_hosts[dc] = tuple(set(dc_hosts))\n\n        if not self.local_dc:\n            self._endpoints = [\n                endpoint\n                for endpoint in cluster.endpoints_resolved]\n\n        self._position = randint(0, len(hosts) - 1) if hosts else 0\n\n    def distance(self, host):\n        dc = self._dc(host)\n        if dc == self.local_dc:\n            return HostDistance.LOCAL\n\n        if not self.used_hosts_per_remote_dc:\n            return HostDistance.IGNORED\n        else:\n            dc_hosts = self._dc_live_hosts.get(dc)\n            if not dc_hosts:\n                return HostDistance.IGNORED\n\n            if host in list(dc_hosts)[:self.used_hosts_per_remote_dc]:\n                return HostDistance.REMOTE\n            else:\n                return HostDistance.IGNORED\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        # not thread-safe, but we don't care much about lost increments\n        # for the purposes of load balancing\n        pos = self._position\n        self._position += 1\n\n        local_live = self._dc_live_hosts.get(self.local_dc, ())\n        pos = (pos % len(local_live)) if local_live else 0\n        for host in islice(cycle(local_live), pos, pos + len(local_live)):\n            yield host\n\n        # the dict can change, so get candidate DCs iterating over keys of a copy\n        other_dcs = [dc for dc in self._dc_live_hosts.copy().keys() if dc != self.local_dc]\n        for dc in other_dcs:\n            remote_live = self._dc_live_hosts.get(dc, ())\n            for host in remote_live[:self.used_hosts_per_remote_dc]:\n                yield host\n\n    def on_up(self, host):\n        # not worrying about threads because this will happen during\n        # control connection startup/refresh\n        if not self.local_dc and host.datacenter:\n            if host.endpoint in self._endpoints:\n                self.local_dc = host.datacenter\n                log.info(\"Using datacenter '%s' for DCAwareRoundRobinPolicy (via host '%s'); \"\n                         \"if incorrect, please specify a local_dc to the constructor, \"\n                         \"or limit contact points to local cluster nodes\" %\n                         (self.local_dc, host.endpoint))\n                del self._endpoints\n\n        dc = self._dc(host)\n        with self._hosts_lock:\n            current_hosts = self._dc_live_hosts.get(dc, ())\n            if host not in current_hosts:\n                self._dc_live_hosts[dc] = current_hosts + (host, )\n\n    def on_down(self, host):\n        dc = self._dc(host)\n        with self._hosts_lock:\n            current_hosts = self._dc_live_hosts.get(dc, ())\n            if host in current_hosts:\n                hosts = tuple(h for h in current_hosts if h != host)\n                if hosts:\n                    self._dc_live_hosts[dc] = hosts\n                else:\n                    del self._dc_live_hosts[dc]\n\n    def on_add(self, host):\n        self.on_up(host)\n\n    def on_remove(self, host):\n        self.on_down(host)\n\nclass RackAwareRoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    Similar to :class:`.DCAwareRoundRobinPolicy`, but prefers hosts\n    in the local rack, before hosts in the local datacenter but a\n    different rack, before hosts in all other datercentres\n    \"\"\"\n\n    local_dc = None\n    local_rack = None\n    used_hosts_per_remote_dc = 0\n\n    def __init__(self, local_dc, local_rack, used_hosts_per_remote_dc=0):\n        \"\"\"\n        The `local_dc` and `local_rack` parameters should be the name of the\n        datacenter and rack (such as is reported by ``nodetool ring``) that\n        should be considered local.\n\n        `used_hosts_per_remote_dc` controls how many nodes in\n        each remote datacenter will have connections opened\n        against them. In other words, `used_hosts_per_remote_dc` hosts\n        will be considered :attr:`~.HostDistance.REMOTE` and the\n        rest will be considered :attr:`~.HostDistance.IGNORED`.\n        By default, all remote hosts are ignored.\n        \"\"\"\n        self.local_rack = local_rack\n        self.local_dc = local_dc\n        self.used_hosts_per_remote_dc = used_hosts_per_remote_dc\n        self._live_hosts = {}\n        self._dc_live_hosts = {}\n        self._endpoints = []\n        self._position = 0\n        LoadBalancingPolicy.__init__(self)\n\n    def _rack(self, host):\n        return host.rack or self.local_rack\n\n    def _dc(self, host):\n        return host.datacenter or self.local_dc\n\n    def populate(self, cluster, hosts):\n        for (dc, rack), rack_hosts in groupby(hosts, lambda host: (self._dc(host), self._rack(host))):\n            self._live_hosts[(dc, rack)] = tuple(set(rack_hosts))\n        for dc, dc_hosts in groupby(hosts, lambda host: self._dc(host)):\n            self._dc_live_hosts[dc] = tuple(set(dc_hosts))\n\n        self._position = randint(0, len(hosts) - 1) if hosts else 0\n\n    def distance(self, host):\n        rack = self._rack(host)\n        dc = self._dc(host)\n        if rack == self.local_rack and dc == self.local_dc:\n            return HostDistance.LOCAL_RACK\n\n        if dc == self.local_dc:\n            return HostDistance.LOCAL\n\n        if not self.used_hosts_per_remote_dc:\n            return HostDistance.IGNORED\n\n        dc_hosts = self._dc_live_hosts.get(dc, ())\n        if not dc_hosts:\n            return HostDistance.IGNORED\n        if host in dc_hosts and dc_hosts.index(host) < self.used_hosts_per_remote_dc:\n            return HostDistance.REMOTE\n        else:\n            return HostDistance.IGNORED\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        pos = self._position\n        self._position += 1\n\n        local_rack_live = self._live_hosts.get((self.local_dc, self.local_rack), ())\n        pos = (pos % len(local_rack_live)) if local_rack_live else 0\n        # Slice the cyclic iterator to start from pos and include the next len(local_live) elements\n        # This ensures we get exactly one full cycle starting from pos\n        for host in islice(cycle(local_rack_live), pos, pos + len(local_rack_live)):\n            yield host\n\n        local_live = [host for host in self._dc_live_hosts.get(self.local_dc, ()) if host.rack != self.local_rack]\n        pos = (pos % len(local_live)) if local_live else 0\n        for host in islice(cycle(local_live), pos, pos + len(local_live)):\n            yield host\n\n        # the dict can change, so get candidate DCs iterating over keys of a copy\n        for dc, remote_live in self._dc_live_hosts.copy().items():\n            if dc != self.local_dc:\n                for host in remote_live[:self.used_hosts_per_remote_dc]:\n                    yield host\n\n    def on_up(self, host):\n        dc = self._dc(host)\n        rack = self._rack(host)\n        with self._hosts_lock:\n            current_rack_hosts = self._live_hosts.get((dc, rack), ())\n            if host not in current_rack_hosts:\n                self._live_hosts[(dc, rack)] = current_rack_hosts + (host, )\n            current_dc_hosts = self._dc_live_hosts.get(dc, ())\n            if host not in current_dc_hosts:\n                self._dc_live_hosts[dc] = current_dc_hosts + (host, )\n\n    def on_down(self, host):\n        dc = self._dc(host)\n        rack = self._rack(host)\n        with self._hosts_lock:\n            current_rack_hosts = self._live_hosts.get((dc, rack), ())\n            if host in current_rack_hosts:\n                hosts = tuple(h for h in current_rack_hosts if h != host)\n                if hosts:\n                    self._live_hosts[(dc, rack)] = hosts\n                else:\n                    del self._live_hosts[(dc, rack)]\n            current_dc_hosts = self._dc_live_hosts.get(dc, ())\n            if host in current_dc_hosts:\n                hosts = tuple(h for h in current_dc_hosts if h != host)\n                if hosts:\n                    self._dc_live_hosts[dc] = hosts\n                else:\n                    del self._dc_live_hosts[dc]\n\n    def on_add(self, host):\n        self.on_up(host)\n\n    def on_remove(self, host):\n        self.on_down(host)\n\nclass TokenAwarePolicy(LoadBalancingPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` wrapper that adds token awareness to\n    a child policy.\n\n    This alters the child policy's behavior so that it first attempts to\n    send queries to :attr:`~.HostDistance.LOCAL` replicas (as determined\n    by the child policy) based on the :class:`.Statement`'s\n    :attr:`~.Statement.routing_key`. If :attr:`.shuffle_replicas` is\n    truthy, these replicas will be yielded in a random order. Once those\n    hosts are exhausted, the remaining hosts in the child policy's query\n    plan will be used in the order provided by the child policy.\n\n    If no :attr:`~.Statement.routing_key` is set on the query, the child\n    policy's query plan will be used as is.\n    \"\"\"\n\n    _child_policy = None\n    _cluster_metadata = None\n    _tablets_routing_v1 = False\n    shuffle_replicas = False\n    \"\"\"\n    Yield local replicas in a random order.\n    \"\"\"\n\n    def __init__(self, child_policy, shuffle_replicas=False):\n        self._child_policy = child_policy\n        self.shuffle_replicas = shuffle_replicas\n\n    def populate(self, cluster, hosts):\n        self._cluster_metadata = cluster.metadata\n        self._tablets_routing_v1 = cluster.control_connection._tablets_routing_v1\n        self._child_policy.populate(cluster, hosts)\n\n    def check_supported(self):\n        if not self._cluster_metadata.can_support_partitioner():\n            raise RuntimeError(\n                '%s cannot be used with the cluster partitioner (%s) because '\n                'the relevant C extension for this driver was not compiled. '\n                'See the installation instructions for details on building '\n                'and installing the C extensions.' %\n                (self.__class__.__name__, self._cluster_metadata.partitioner))\n\n    def distance(self, *args, **kwargs):\n        return self._child_policy.distance(*args, **kwargs)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        keyspace = query.keyspace if query and query.keyspace else working_keyspace\n\n        child = self._child_policy\n        if query is None or query.routing_key is None or keyspace is None:\n            for host in child.make_query_plan(keyspace, query):\n                yield host\n            return\n\n        replicas = []\n        if self._tablets_routing_v1:\n            tablet = self._cluster_metadata._tablets.get_tablet_for_key(\n                keyspace, query.table, self._cluster_metadata.token_map.token_class.from_key(query.routing_key))\n\n            if tablet is not None:\n                replicas_mapped = set(map(lambda r: r[0], tablet.replicas))\n                child_plan = child.make_query_plan(keyspace, query)\n\n                replicas = [host for host in child_plan if host.host_id in replicas_mapped]\n\n        if not replicas:\n            replicas = self._cluster_metadata.get_replicas(keyspace, query.routing_key)\n\n        if self.shuffle_replicas:\n            shuffle(replicas)\n\n        for replica in replicas:\n            if replica.is_up and child.distance(replica) in [HostDistance.LOCAL, HostDistance.LOCAL_RACK]:\n                yield replica\n\n        for host in child.make_query_plan(keyspace, query):\n            # skip if we've already listed this host\n            if host not in replicas or child.distance(host) == HostDistance.REMOTE:\n                yield host\n\n    def on_up(self, *args, **kwargs):\n        return self._child_policy.on_up(*args, **kwargs)\n\n    def on_down(self, *args, **kwargs):\n        return self._child_policy.on_down(*args, **kwargs)\n\n    def on_add(self, *args, **kwargs):\n        return self._child_policy.on_add(*args, **kwargs)\n\n    def on_remove(self, *args, **kwargs):\n        return self._child_policy.on_remove(*args, **kwargs)\n\n\nclass WhiteListRoundRobinPolicy(RoundRobinPolicy):\n    \"\"\"\n    A subclass of :class:`.RoundRobinPolicy` which evenly\n    distributes queries across all nodes in the cluster,\n    regardless of what datacenter the nodes may be in, but\n    only if that node exists in the list of allowed nodes\n\n    This policy is addresses the issue described in\n    https://datastax-oss.atlassian.net/browse/JAVA-145\n    Where connection errors occur when connection\n    attempts are made to private IP addresses remotely\n    \"\"\"\n\n    def __init__(self, hosts):\n        \"\"\"\n        The `hosts` parameter should be a sequence of hosts to permit\n        connections to.\n        \"\"\"\n        self._allowed_hosts = tuple(hosts)\n        self._allowed_hosts_resolved = []\n        for h in self._allowed_hosts:\n            unix_socket_path = getattr(h, \"_unix_socket_path\", None)\n            if unix_socket_path:\n                self._allowed_hosts_resolved.append(unix_socket_path)\n            else:\n                self._allowed_hosts_resolved.extend([endpoint[4][0]\n                                        for endpoint in socket.getaddrinfo(h, None, socket.AF_UNSPEC, socket.SOCK_STREAM)])\n\n        RoundRobinPolicy.__init__(self)\n\n    def populate(self, cluster, hosts):\n        self._live_hosts = frozenset(h for h in hosts if h.address in self._allowed_hosts_resolved)\n\n        if len(hosts) <= 1:\n            self._position = 0\n        else:\n            self._position = randint(0, len(hosts) - 1)\n\n    def distance(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            return HostDistance.LOCAL\n        else:\n            return HostDistance.IGNORED\n\n    def on_up(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            RoundRobinPolicy.on_up(self, host)\n\n    def on_add(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            RoundRobinPolicy.on_add(self, host)\n\n\nclass HostFilterPolicy(LoadBalancingPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` subclass configured with a child policy,\n    and a single-argument predicate. This policy defers to the child policy for\n    hosts where ``predicate(host)`` is truthy. Hosts for which\n    ``predicate(host)`` is falsy will be considered :attr:`.IGNORED`, and will\n    not be used in a query plan.\n\n    This can be used in the cases where you need a whitelist or blacklist\n    policy, e.g. to prepare for decommissioning nodes or for testing:\n\n    .. code-block:: python\n\n        def address_is_ignored(host):\n            return host.address in [ignored_address0, ignored_address1]\n\n        blacklist_filter_policy = HostFilterPolicy(\n            child_policy=RoundRobinPolicy(),\n            predicate=address_is_ignored\n        )\n\n        cluster = Cluster(\n            primary_host,\n            load_balancing_policy=blacklist_filter_policy,\n        )\n\n    See the note in the :meth:`.make_query_plan` documentation for a caveat on\n    how wrapping ordering polices (e.g. :class:`.RoundRobinPolicy`) may break\n    desirable properties of the wrapped policy.\n\n    Please note that whitelist and blacklist policies are not recommended for\n    general, day-to-day use. You probably want something like\n    :class:`.DCAwareRoundRobinPolicy`, which prefers a local DC but has\n    fallbacks, over a brute-force method like whitelisting or blacklisting.\n    \"\"\"\n\n    def __init__(self, child_policy, predicate):\n        \"\"\"\n        :param child_policy: an instantiated :class:`.LoadBalancingPolicy`\n                             that this one will defer to.\n        :param predicate: a one-parameter function that takes a :class:`.Host`.\n                          If it returns a falsy value, the :class:`.Host` will\n                          be :attr:`.IGNORED` and not returned in query plans.\n        \"\"\"\n        super(HostFilterPolicy, self).__init__()\n        self._child_policy = child_policy\n        self._predicate = predicate\n\n    def on_up(self, host, *args, **kwargs):\n        return self._child_policy.on_up(host, *args, **kwargs)\n\n    def on_down(self, host, *args, **kwargs):\n        return self._child_policy.on_down(host, *args, **kwargs)\n\n    def on_add(self, host, *args, **kwargs):\n        return self._child_policy.on_add(host, *args, **kwargs)\n\n    def on_remove(self, host, *args, **kwargs):\n        return self._child_policy.on_remove(host, *args, **kwargs)\n\n    @property\n    def predicate(self):\n        \"\"\"\n        A predicate, set on object initialization, that takes a :class:`.Host`\n        and returns a value. If the value is falsy, the :class:`.Host` is\n        :class:`~HostDistance.IGNORED`. If the value is truthy,\n        :class:`.HostFilterPolicy` defers to the child policy to determine the\n        host's distance.\n\n        This is a read-only value set in ``__init__``, implemented as a\n        ``property``.\n        \"\"\"\n        return self._predicate\n\n    def distance(self, host):\n        \"\"\"\n        Checks if ``predicate(host)``, then returns\n        :attr:`~HostDistance.IGNORED` if falsy, and defers to the child policy\n        otherwise.\n        \"\"\"\n        if self.predicate(host):\n            return self._child_policy.distance(host)\n        else:\n            return HostDistance.IGNORED\n\n    def populate(self, cluster, hosts):\n        self._child_policy.populate(cluster=cluster, hosts=hosts)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        \"\"\"\n        Defers to the child policy's\n        :meth:`.LoadBalancingPolicy.make_query_plan` and filters the results.\n\n        Note that this filtering may break desirable properties of the wrapped\n        policy in some cases. For instance, imagine if you configure this\n        policy to filter out ``host2``, and to wrap a round-robin policy that\n        rotates through three hosts in the order ``host1, host2, host3``,\n        ``host2, host3, host1``, ``host3, host1, host2``, repeating. This\n        policy will yield ``host1, host3``, ``host3, host1``, ``host3, host1``,\n        disproportionately favoring ``host3``.\n        \"\"\"\n        child_qp = self._child_policy.make_query_plan(\n            working_keyspace=working_keyspace, query=query\n        )\n        for host in child_qp:\n            if self.predicate(host):\n                yield host\n\n    def check_supported(self):\n        return self._child_policy.check_supported()\n\n\nclass ConvictionPolicy(object):\n    \"\"\"\n    A policy which decides when hosts should be considered down\n    based on the types of failures and the number of failures.\n\n    If custom behavior is needed, this class may be subclassed.\n    \"\"\"\n\n    def __init__(self, host):\n        \"\"\"\n        `host` is an instance of :class:`.Host`.\n        \"\"\"\n        self.host = host\n\n    def add_failure(self, connection_exc):\n        \"\"\"\n        Implementations should return :const:`True` if the host should be\n        convicted, :const:`False` otherwise.\n        \"\"\"\n        raise NotImplementedError()\n\n    def reset(self):\n        \"\"\"\n        Implementations should clear out any convictions or state regarding\n        the host.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass SimpleConvictionPolicy(ConvictionPolicy):\n    \"\"\"\n    The default implementation of :class:`ConvictionPolicy`,\n    which simply marks a host as down after the first failure\n    of any kind.\n    \"\"\"\n\n    def add_failure(self, connection_exc):\n        return not isinstance(connection_exc, OperationTimedOut)\n\n    def reset(self):\n        pass\n\n\nclass ReconnectionPolicy(object):\n    \"\"\"\n    This class and its subclasses govern how frequently an attempt is made\n    to reconnect to nodes that are marked as dead.\n\n    If custom behavior is needed, this class may be subclassed.\n    \"\"\"\n\n    def new_schedule(self):\n        \"\"\"\n        This should return a finite or infinite iterable of delays (each as a\n        floating point number of seconds) in-between each failed reconnection\n        attempt.  Note that if the iterable is finite, reconnection attempts\n        will cease once the iterable is exhausted.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass ConstantReconnectionPolicy(ReconnectionPolicy):\n    \"\"\"\n    A :class:`.ReconnectionPolicy` subclass which sleeps for a fixed delay\n    in-between each reconnection attempt.\n    \"\"\"\n\n    def __init__(self, delay, max_attempts=64):\n        \"\"\"\n        `delay` should be a floating point number of seconds to wait in-between\n        each attempt.\n\n        `max_attempts` should be a total number of attempts to be made before\n        giving up, or :const:`None` to continue reconnection attempts forever.\n        The default is 64.\n        \"\"\"\n        if delay < 0:\n            raise ValueError(\"delay must not be negative\")\n        if max_attempts is not None and max_attempts < 0:\n            raise ValueError(\"max_attempts must not be negative\")\n\n        self.delay = delay\n        self.max_attempts = max_attempts\n\n    def new_schedule(self):\n        if self.max_attempts:\n            return repeat(self.delay, self.max_attempts)\n        return repeat(self.delay)\n\n\nclass ExponentialReconnectionPolicy(ReconnectionPolicy):\n    \"\"\"\n    A :class:`.ReconnectionPolicy` subclass which exponentially increases\n    the length of the delay in-between each reconnection attempt up to\n    a set maximum delay.\n\n    A random amount of jitter (+/- 15%) will be added to the pure exponential\n    delay value to avoid the situations where many reconnection handlers are\n    trying to reconnect at exactly the same time.\n    \"\"\"\n\n    # TODO: max_attempts is 64 to preserve legacy default behavior\n    # consider changing to None in major release to prevent the policy\n    # giving up forever\n    def __init__(self, base_delay, max_delay, max_attempts=64):\n        \"\"\"\n        `base_delay` and `max_delay` should be in floating point units of\n        seconds.\n\n        `max_attempts` should be a total number of attempts to be made before\n        giving up, or :const:`None` to continue reconnection attempts forever.\n        The default is 64.\n        \"\"\"\n        if base_delay < 0 or max_delay < 0:\n            raise ValueError(\"Delays may not be negative\")\n\n        if max_delay < base_delay:\n            raise ValueError(\"Max delay must be greater than base delay\")\n\n        if max_attempts is not None and max_attempts < 0:\n            raise ValueError(\"max_attempts must not be negative\")\n\n        self.base_delay = base_delay\n        self.max_delay = max_delay\n        self.max_attempts = max_attempts\n\n    def new_schedule(self):\n        i, overflowed = 0, False\n        while self.max_attempts is None or i < self.max_attempts:\n            if overflowed:\n                yield self.max_delay\n            else:\n                try:\n                    yield self._add_jitter(min(self.base_delay * (2 ** i), self.max_delay))\n                except OverflowError:\n                    overflowed = True\n                    yield self.max_delay\n\n            i += 1\n\n    # Adds -+ 15% to the delay provided\n    def _add_jitter(self, value):\n        jitter = randint(85, 115)\n        delay = (jitter * value) / 100\n        return min(max(self.base_delay, delay), self.max_delay)\n\n\nclass RetryPolicy(object):\n    \"\"\"\n    A policy that describes whether to retry, rethrow, or ignore coordinator\n    timeout and unavailable failures. These are failures reported from the\n    server side. Timeouts are configured by\n    `settings in cassandra.yaml <https://github.com/apache/cassandra/blob/cassandra-2.1.4/conf/cassandra.yaml#L568-L584>`_.\n    Unavailable failures occur when the coordinator cannot achieve the consistency\n    level for a request. For further information see the method descriptions\n    below.\n\n    To specify a default retry policy, set the\n    :attr:`.Cluster.default_retry_policy` attribute to an instance of this\n    class or one of its subclasses.\n\n    To specify a retry policy per query, set the :attr:`.Statement.retry_policy`\n    attribute to an instance of this class or one of its subclasses.\n\n    If custom behavior is needed for retrying certain operations,\n    this class may be subclassed.\n    \"\"\"\n\n    RETRY = 0\n    \"\"\"\n    This should be returned from the below methods if the operation\n    should be retried on the same connection.\n    \"\"\"\n\n    RETHROW = 1\n    \"\"\"\n    This should be returned from the below methods if the failure\n    should be propagated and no more retries attempted.\n    \"\"\"\n\n    IGNORE = 2\n    \"\"\"\n    This should be returned from the below methods if the failure\n    should be ignored but no more retries should be attempted.\n    \"\"\"\n\n    RETRY_NEXT_HOST = 3\n    \"\"\"\n    This should be returned from the below methods if the operation\n    should be retried on another connection.\n    \"\"\"\n\n    def on_read_timeout(self, query, consistency, required_responses,\n                        received_responses, data_retrieved, retry_num):\n        \"\"\"\n        This is called when a read operation times out from the coordinator's\n        perspective (i.e. a replica did not respond to the coordinator in time).\n        It should return a tuple with two items: one of the class enums (such\n        as :attr:`.RETRY`) and a :class:`.ConsistencyLevel` to retry the\n        operation at or :const:`None` to keep the same consistency level.\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        The `required_responses` and `received_responses` parameters describe\n        how many replicas needed to respond to meet the requested consistency\n        level and how many actually did respond before the coordinator timed\n        out the request. `data_retrieved` is a boolean indicating whether\n        any of those responses contained data (as opposed to just a digest).\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, operations will be retried at most once, and only if\n        a sufficient number of replicas responded (with data digests).\n        \"\"\"\n        if retry_num != 0:\n            return self.RETHROW, None\n        elif received_responses >= required_responses and not data_retrieved:\n            return self.RETRY, consistency\n        else:\n            return self.RETHROW, None\n\n    def on_write_timeout(self, query, consistency, write_type,\n                         required_responses, received_responses, retry_num):\n        \"\"\"\n        This is called when a write operation times out from the coordinator's\n        perspective (i.e. a replica did not respond to the coordinator in time).\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `write_type` is one of the :class:`.WriteType` enums describing the\n        type of write operation.\n\n        The `required_responses` and `received_responses` parameters describe\n        how many replicas needed to acknowledge the write to meet the requested\n        consistency level and how many replicas actually did acknowledge the\n        write before the coordinator timed out the request.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, failed write operations will retried at most once, and\n        they will only be retried if the `write_type` was\n        :attr:`~.WriteType.BATCH_LOG`.\n        \"\"\"\n        if retry_num != 0:\n            return self.RETHROW, None\n        elif write_type == WriteType.BATCH_LOG:\n            return self.RETRY, consistency\n        else:\n            return self.RETHROW, None\n\n    def on_unavailable(self, query, consistency, required_replicas, alive_replicas, retry_num):\n        \"\"\"\n        This is called when the coordinator node determines that a read or\n        write operation cannot be successful because the number of live\n        replicas are too low to meet the requested :class:`.ConsistencyLevel`.\n        This means that the read or write operation was never forwarded to\n        any replicas.\n\n        `query` is the :class:`.Statement` that failed.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `required_replicas` is the number of replicas that would have needed to\n        acknowledge the operation to meet the requested consistency level.\n        `alive_replicas` is the number of replicas that the coordinator\n        considered alive at the time of the request.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, if this is the first retry, it triggers a retry on the next\n        host in the query plan with the same consistency level. If this is not the\n        first retry, no retries will be attempted and the error will be re-raised.\n        \"\"\"\n        return (self.RETRY_NEXT_HOST, None) if retry_num == 0 else (self.RETHROW, None)\n\n    def on_request_error(self, query, consistency, error, retry_num):\n        \"\"\"\n        This is called when an unexpected error happens. This can be in the\n        following situations:\n\n        * On a connection error\n        * On server errors: overloaded, isBootstrapping, serverError, etc.\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `error` the instance of the exception.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, it triggers a retry on the next host in the query plan\n        with the same consistency level.\n        \"\"\"\n        # TODO revisit this for the next major\n        # To preserve the same behavior than before, we don't take retry_num into account\n        return self.RETRY_NEXT_HOST, None\n\n\nclass FallthroughRetryPolicy(RetryPolicy):\n    \"\"\"\n    A retry policy that never retries and always propagates failures to\n    the application.\n    \"\"\"\n\n    def on_read_timeout(self, *args, **kwargs):\n        return self.RETHROW, None\n\n    def on_write_timeout(self, *args, **kwargs):\n        return self.RETHROW, None\n\n    def on_unavailable(self, *args, **kwargs):\n        return self.RETHROW, None\n\n    def on_request_error(self, *args, **kwargs):\n        return self.RETHROW, None\n\n\nclass DowngradingConsistencyRetryPolicy(RetryPolicy):\n    \"\"\"\n    *Deprecated:* This retry policy will be removed in the next major release.\n\n    A retry policy that sometimes retries with a lower consistency level than\n    the one initially requested.\n\n    **BEWARE**: This policy may retry queries using a lower consistency\n    level than the one initially requested. By doing so, it may break\n    consistency guarantees. In other words, if you use this retry policy,\n    there are cases (documented below) where a read at :attr:`~.QUORUM`\n    *may not* see a preceding write at :attr:`~.QUORUM`. Do not use this\n    policy unless you have understood the cases where this can happen and\n    are ok with that. It is also recommended to subclass this class so\n    that queries that required a consistency level downgrade can be\n    recorded (so that repairs can be made later, etc).\n\n    This policy implements the same retries as :class:`.RetryPolicy`,\n    but on top of that, it also retries in the following cases:\n\n    * On a read timeout: if the number of replicas that responded is\n      greater than one but lower than is required by the requested\n      consistency level, the operation is retried at a lower consistency\n      level.\n    * On a write timeout: if the operation is an :attr:`~.UNLOGGED_BATCH`\n      and at least one replica acknowledged the write, the operation is\n      retried at a lower consistency level.  Furthermore, for other\n      write types, if at least one replica acknowledged the write, the\n      timeout is ignored.\n    * On an unavailable exception: if at least one replica is alive, the\n      operation is retried at a lower consistency level.\n\n    The reasoning behind this retry policy is as follows: if, based\n    on the information the Cassandra coordinator node returns, retrying the\n    operation with the initially requested consistency has a chance to\n    succeed, do it. Otherwise, if based on that information we know the\n    initially requested consistency level cannot be achieved currently, then:\n\n    * For writes, ignore the exception (thus silently failing the\n      consistency requirement) if we know the write has been persisted on at\n      least one replica.\n    * For reads, try reading at a lower consistency level (thus silently\n      failing the consistency requirement).\n\n    In other words, this policy implements the idea that if the requested\n    consistency level cannot be achieved, the next best thing for writes is\n    to make sure the data is persisted, and that reading something is better\n    than reading nothing, even if there is a risk of reading stale data.\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(DowngradingConsistencyRetryPolicy, self).__init__(*args, **kwargs)\n        warnings.warn('DowngradingConsistencyRetryPolicy is deprecated '\n                      'and will be removed in the next major release.',\n                      DeprecationWarning)\n\n    def _pick_consistency(self, num_responses):\n        if num_responses >= 3:\n            return self.RETRY, ConsistencyLevel.THREE\n        elif num_responses >= 2:\n            return self.RETRY, ConsistencyLevel.TWO\n        elif num_responses >= 1:\n            return self.RETRY, ConsistencyLevel.ONE\n        else:\n            return self.RETHROW, None\n\n    def on_read_timeout(self, query, consistency, required_responses,\n                        received_responses, data_retrieved, retry_num):\n        if retry_num != 0:\n            return self.RETHROW, None\n        elif ConsistencyLevel.is_serial(consistency):\n            # Downgrading does not make sense for a CAS read query\n            return self.RETHROW, None\n        elif received_responses < required_responses:\n            return self._pick_consistency(received_responses)\n        elif not data_retrieved:\n            return self.RETRY, consistency\n        else:\n            return self.RETHROW, None\n\n    def on_write_timeout(self, query, consistency, write_type,\n                         required_responses, received_responses, retry_num):\n        if retry_num != 0:\n            return self.RETHROW, None\n\n        if write_type in (WriteType.SIMPLE, WriteType.BATCH, WriteType.COUNTER):\n            if received_responses > 0:\n                # persisted on at least one replica\n                return self.IGNORE, None\n            else:\n                return self.RETHROW, None\n        elif write_type == WriteType.UNLOGGED_BATCH:\n            return self._pick_consistency(received_responses)\n        elif write_type == WriteType.BATCH_LOG:\n            return self.RETRY, consistency\n\n        return self.RETHROW, None\n\n    def on_unavailable(self, query, consistency, required_replicas, alive_replicas, retry_num):\n        if retry_num != 0:\n            return self.RETHROW, None\n        elif ConsistencyLevel.is_serial(consistency):\n            # failed at the paxos phase of a LWT, retry on the next host\n            return self.RETRY_NEXT_HOST, None\n        else:\n            return self._pick_consistency(alive_replicas)\n\n\nclass ExponentialBackoffRetryPolicy(RetryPolicy):\n    \"\"\"\n    A policy that do retries with exponential backoff\n    \"\"\"\n\n    def __init__(self, max_num_retries: float, min_interval: float = 0.1, max_interval: float = 10.0,\n                 *args, **kwargs):\n        \"\"\"\n        `max_num_retries` counts how many times the operation would be retried,\n        `min_interval` is the initial time in seconds to wait before first retry\n        `max_interval` is the maximum time to wait between retries\n        \"\"\"\n        self.min_interval = min_interval\n        self.max_num_retries = max_num_retries\n        self.max_interval = max_interval\n        super(ExponentialBackoffRetryPolicy).__init__(*args, **kwargs)\n\n    def _calculate_backoff(self, attempt: int):\n        delay = min(self.max_interval, self.min_interval * 2 ** attempt)\n        # add some jitter\n        delay += random.random() * self.min_interval - (self.min_interval / 2)\n        return delay\n\n    def on_read_timeout(self, query, consistency, required_responses,\n                        received_responses, data_retrieved, retry_num):\n        if retry_num < self.max_num_retries and received_responses >= required_responses and not data_retrieved:\n            return self.RETRY, consistency, self._calculate_backoff(retry_num)\n        else:\n            return self.RETHROW, None, None\n\n    def on_write_timeout(self, query, consistency, write_type,\n                         required_responses, received_responses, retry_num):\n        if retry_num < self.max_num_retries and write_type == WriteType.BATCH_LOG:\n            return self.RETRY, consistency, self._calculate_backoff(retry_num)\n        else:\n            return self.RETHROW, None, None\n\n    def on_unavailable(self, query, consistency, required_replicas,\n                       alive_replicas, retry_num):\n        if retry_num < self.max_num_retries:\n            return self.RETRY_NEXT_HOST, None, self._calculate_backoff(retry_num)\n        else:\n            return self.RETHROW, None, None\n\n    def on_request_error(self, query, consistency, error, retry_num):\n        if retry_num < self.max_num_retries:\n            return self.RETRY_NEXT_HOST, None, self._calculate_backoff(retry_num)\n        else:\n            return self.RETHROW, None, None\n\n\nclass AddressTranslator(object):\n    \"\"\"\n    Interface for translating cluster-defined endpoints.\n\n    The driver discovers nodes using server metadata and topology change events. Normally,\n    the endpoint defined by the server is the right way to connect to a node. In some environments,\n    these addresses may not be reachable, or not preferred (public vs. private IPs in cloud environments,\n    suboptimal routing, etc). This interface allows for translating from server defined endpoints to\n    preferred addresses for driver connections.\n\n    *Note:* :attr:`~Cluster.contact_points` provided while creating the :class:`~.Cluster` instance are not\n    translated using this mechanism -- only addresses received from Cassandra nodes are.\n    \"\"\"\n    def translate(self, addr):\n        \"\"\"\n        Accepts the node ip address, and returns a translated address to be used connecting to this node.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass IdentityTranslator(AddressTranslator):\n    \"\"\"\n    Returns the endpoint with no translation\n    \"\"\"\n    def translate(self, addr):\n        return addr\n\n\nclass EC2MultiRegionTranslator(AddressTranslator):\n    \"\"\"\n    Resolves private ips of the hosts in the same datacenter as the client, and public ips of hosts in other datacenters.\n    \"\"\"\n    def translate(self, addr):\n        \"\"\"\n        Reverse DNS the public broadcast_address, then lookup that hostname to get the AWS-resolved IP, which\n        will point to the private IP address within the same datacenter.\n        \"\"\"\n        # get family of this address so we translate to the same\n        family = socket.getaddrinfo(addr, 0, socket.AF_UNSPEC, socket.SOCK_STREAM)[0][0]\n        host = socket.getfqdn(addr)\n        for a in socket.getaddrinfo(host, 0, family, socket.SOCK_STREAM):\n            try:\n                return a[4][0]\n            except Exception:\n                pass\n        return addr\n\n\nclass SpeculativeExecutionPolicy(object):\n    \"\"\"\n    Interface for specifying speculative execution plans\n    \"\"\"\n\n    def new_plan(self, keyspace, statement):\n        \"\"\"\n        Returns\n\n        :param keyspace:\n        :param statement:\n        :return:\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass SpeculativeExecutionPlan(object):\n    def next_execution(self, host):\n        raise NotImplementedError()\n\n\nclass NoSpeculativeExecutionPlan(SpeculativeExecutionPlan):\n    def next_execution(self, host):\n        return -1\n\n\nclass NoSpeculativeExecutionPolicy(SpeculativeExecutionPolicy):\n\n    def new_plan(self, keyspace, statement):\n        return NoSpeculativeExecutionPlan()\n\n\nclass ConstantSpeculativeExecutionPolicy(SpeculativeExecutionPolicy):\n    \"\"\"\n    A speculative execution policy that sends a new query every X seconds (**delay**) for a maximum of Y attempts (**max_attempts**).\n    \"\"\"\n\n    def __init__(self, delay, max_attempts):\n        self.delay = delay\n        self.max_attempts = max_attempts\n\n    class ConstantSpeculativeExecutionPlan(SpeculativeExecutionPlan):\n        def __init__(self, delay, max_attempts):\n            self.delay = delay\n            self.remaining = max_attempts\n\n        def next_execution(self, host):\n            if self.remaining > 0:\n                self.remaining -= 1\n                return self.delay\n            else:\n                return -1\n\n    def new_plan(self, keyspace, statement):\n        return self.ConstantSpeculativeExecutionPlan(self.delay, self.max_attempts)\n\n\nclass WrapperPolicy(LoadBalancingPolicy):\n\n    def __init__(self, child_policy):\n        self._child_policy = child_policy\n\n    def distance(self, *args, **kwargs):\n        return self._child_policy.distance(*args, **kwargs)\n\n    def populate(self, cluster, hosts):\n        self._child_policy.populate(cluster, hosts)\n\n    def on_up(self, *args, **kwargs):\n        return self._child_policy.on_up(*args, **kwargs)\n\n    def on_down(self, *args, **kwargs):\n        return self._child_policy.on_down(*args, **kwargs)\n\n    def on_add(self, *args, **kwargs):\n        return self._child_policy.on_add(*args, **kwargs)\n\n    def on_remove(self, *args, **kwargs):\n        return self._child_policy.on_remove(*args, **kwargs)\n\n\nclass DefaultLoadBalancingPolicy(WrapperPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` wrapper that adds the ability to target a specific host first.\n\n    If no host is set on the query, the child policy's query plan will be used as is.\n    \"\"\"\n\n    _cluster_metadata = None\n\n    def populate(self, cluster, hosts):\n        self._cluster_metadata = cluster.metadata\n        self._child_policy.populate(cluster, hosts)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        if query and query.keyspace:\n            keyspace = query.keyspace\n        else:\n            keyspace = working_keyspace\n\n        # TODO remove next major since execute(..., host=XXX) is now available\n        addr = getattr(query, 'target_host', None) if query else None\n        target_host = self._cluster_metadata.get_host(addr)\n\n        child = self._child_policy\n        if target_host and target_host.is_up:\n            yield target_host\n            for h in child.make_query_plan(keyspace, query):\n                if h != target_host:\n                    yield h\n        else:\n            for h in child.make_query_plan(keyspace, query):\n                yield h\n\n\n# TODO for backward compatibility, remove in next major\nclass DSELoadBalancingPolicy(DefaultLoadBalancingPolicy):\n    \"\"\"\n    *Deprecated:* This will be removed in the next major release,\n    consider using :class:`.DefaultLoadBalancingPolicy`.\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(DSELoadBalancingPolicy, self).__init__(*args, **kwargs)\n        warnings.warn(\"DSELoadBalancingPolicy will be removed in 4.0. Consider using \"\n                      \"DefaultLoadBalancingPolicy.\", DeprecationWarning)\n\n\nclass NeverRetryPolicy(RetryPolicy):\n    def _rethrow(self, *args, **kwargs):\n        return self.RETHROW, None\n\n    on_read_timeout = _rethrow\n    on_write_timeout = _rethrow\n    on_unavailable = _rethrow\n\n\nColDesc = namedtuple('ColDesc', ['ks', 'table', 'col'])\n\nclass ColumnEncryptionPolicy(object):\n    \"\"\"\n    A policy enabling (mostly) transparent encryption and decryption of data before it is\n    sent to the cluster.\n\n    Key materials and other configurations are specified on a per-column basis.  This policy can\n    then be used by driver structures which are aware of the underlying columns involved in their\n    work.  In practice this includes the following cases:\n\n    * Prepared statements - data for columns specified by the cluster's policy will be transparently\n      encrypted before they are sent\n    * Rows returned from any query - data for columns specified by the cluster's policy will be\n      transparently decrypted before they are returned to the user\n\n    To enable this functionality, create an instance of this class (or more likely a subclass)\n    before creating a cluster.  This policy should then be configured and supplied to the Cluster\n    at creation time via the :attr:`.Cluster.column_encryption_policy` attribute.\n    \"\"\"\n\n    def encrypt(self, coldesc, obj_bytes):\n        \"\"\"\n        Encrypt the specified bytes using the cryptography materials for the specified column.\n        Largely used internally, although this could also be used to encrypt values supplied\n        to non-prepared statements in a way that is consistent with this policy.\n        \"\"\"\n        raise NotImplementedError()\n\n    def decrypt(self, coldesc, encrypted_bytes):\n        \"\"\"\n        Decrypt the specified (encrypted) bytes using the cryptography materials for the\n        specified column.  Used internally; could be used externally as well but there's\n        not currently an obvious use case.\n        \"\"\"\n        raise NotImplementedError()\n\n    def add_column(self, coldesc, key):\n        \"\"\"\n        Provide cryptography materials to be used when encrypted and/or decrypting data\n        for the specified column.\n        \"\"\"\n        raise NotImplementedError()\n\n    def contains_column(self, coldesc):\n        \"\"\"\n        Predicate to determine if a specific column is supported by this policy.\n        Currently only used internally.\n        \"\"\"\n        raise NotImplementedError()\n\n    def encode_and_encrypt(self, coldesc, obj):\n        \"\"\"\n        Helper function to enable use of this policy on simple (i.e. non-prepared)\n        statements.\n        \"\"\"\n        raise NotImplementedError()\n"
  },
  "GT_src_dict": {
    "cassandra/query.py": {
      "_clean_column_name": {
        "code": "def _clean_column_name(name):\n    \"\"\"Cleans a column name by replacing non-alphanumeric characters with underscores and removing leading or trailing bad characters.\n\nParameters:\n- name (str): The column name to be cleaned.\n\nReturns:\n- str: The sanitized column name with non-alphanumeric characters replaced and boundaries adjusted.\n\nThis function utilizes the following constants:\n- NON_ALPHA_REGEX: A regex pattern to match any character that is not alphanumeric, defined at the module level.\n- START_BADCHAR_REGEX: A regex pattern that matches non-alphanumeric characters at the start of the string, which is also defined at the module level.\n- END_BADCHAR_REGEX: A regex pattern for non-alphanumeric characters at the end of the string, defined at the module level.\n- _clean_name_cache: A dictionary that caches previously cleaned names to avoid redundant processing, improving efficiency. It is defined and used within this function to store and retrieve cleaned names based on input.\n\nThis function is part of a data processing flow that ensures column names conform to specific identifier standards in the context of a Cassandra database interaction.\"\"\"\n    try:\n        return _clean_name_cache[name]\n    except KeyError:\n        clean = NON_ALPHA_REGEX.sub('_', START_BADCHAR_REGEX.sub('', END_BADCHAR_REGEX.sub('', name)))\n        _clean_name_cache[name] = clean\n        return clean",
        "docstring": "Cleans a column name by replacing non-alphanumeric characters with underscores and removing leading or trailing bad characters.\n\nParameters:\n- name (str): The column name to be cleaned.\n\nReturns:\n- str: The sanitized column name with non-alphanumeric characters replaced and boundaries adjusted.\n\nThis function utilizes the following constants:\n- NON_ALPHA_REGEX: A regex pattern to match any character that is not alphanumeric, defined at the module level.\n- START_BADCHAR_REGEX: A regex pattern that matches non-alphanumeric characters at the start of the string, which is also defined at the module level.\n- END_BADCHAR_REGEX: A regex pattern for non-alphanumeric characters at the end of the string, defined at the module level.\n- _clean_name_cache: A dictionary that caches previously cleaned names to avoid redundant processing, improving efficiency. It is defined and used within this function to store and retrieve cleaned names based on input.\n\nThis function is part of a data processing flow that ensures column names conform to specific identifier standards in the context of a Cassandra database interaction.",
        "signature": "def _clean_column_name(name):",
        "type": "Function",
        "class_signature": null
      },
      "named_tuple_factory": {
        "code": "def named_tuple_factory(colnames, rows):\n    \"\"\"Returns each row from a Cassandra query result as a `namedtuple`, allowing for attribute-style access to row fields. The function relies on clean column names processed by the `_clean_column_name` function and returns a list of namedtuples for each row in the result set.\n\nParameters:\n- colnames (iterable): An iterable containing the original column names returned from the query.\n- rows (iterable): An iterable of rows, where each row is a sequence of values corresponding to the columns defined in `colnames`.\n\nReturns:\n- list: A list of namedtuples representing each row from the query result. If the creation of namedtuples fails due to too many columns or incompatible names, it falls back to using `pseudo_namedtuple_factory`, which returns a more basic object with similar access capabilities.\n\nNotes:\n- This function is designed to handle potential issues with column names by sanitizing them through `_clean_column_name` to ensure valid namedtuple field names.\n- If the number of columns exceeds Python's limits for namedtuples, a warning is logged, and a different factory is used to maintain performance.\n- The dependency on `pseudo_namedtuple_factory` indicates fallback behavior to avoid failures when creating namedtuples.\"\"\"\n    '\\n    Returns each row as a `namedtuple <https://docs.python.org/2/library/collections.html#collections.namedtuple>`_.\\n    This is the default row factory.\\n\\n    Example::\\n\\n        >>> from cassandra.query import named_tuple_factory\\n        >>> session = cluster.connect(\\'mykeyspace\\')\\n        >>> session.row_factory = named_tuple_factory\\n        >>> rows = session.execute(\"SELECT name, age FROM users LIMIT 1\")\\n        >>> user = rows[0]\\n\\n        >>> # you can access field by their name:\\n        >>> print(\"name: %s, age: %d\" % (user.name, user.age))\\n        name: Bob, age: 42\\n\\n        >>> # or you can access fields by their position (like a tuple)\\n        >>> name, age = user\\n        >>> print(\"name: %s, age: %d\" % (name, age))\\n        name: Bob, age: 42\\n        >>> name = user[0]\\n        >>> age = user[1]\\n        >>> print(\"name: %s, age: %d\" % (name, age))\\n        name: Bob, age: 42\\n\\n    .. versionchanged:: 2.0.0\\n        moved from ``cassandra.decoder`` to ``cassandra.query``\\n    '\n    clean_column_names = map(_clean_column_name, colnames)\n    try:\n        Row = namedtuple('Row', clean_column_names)\n    except SyntaxError:\n        warnings.warn('Failed creating namedtuple for a result because there were too many columns. This is due to a Python limitation that affects namedtuple in Python 3.0-3.6 (see issue18896). The row will be created with {substitute_factory_name}, which lacks some namedtuple features and is slower. To avoid slower performance accessing values on row objects, Upgrade to Python 3.7, or use a different row factory. (column names: {colnames})'.format(substitute_factory_name=pseudo_namedtuple_factory.__name__, colnames=colnames))\n        return pseudo_namedtuple_factory(colnames, rows)\n    except Exception:\n        clean_column_names = list(map(_clean_column_name, colnames))\n        log.warning('Failed creating named tuple for results with column names %s (cleaned: %s) (see Python \\'namedtuple\\' documentation for details on name rules). Results will be returned with positional names. Avoid this by choosing different names, using SELECT \"<col name>\" AS aliases, or specifying a different row_factory on your Session' % (colnames, clean_column_names))\n        Row = namedtuple('Row', _sanitize_identifiers(clean_column_names))\n    return [Row(*row) for row in rows]",
        "docstring": "Returns each row from a Cassandra query result as a `namedtuple`, allowing for attribute-style access to row fields. The function relies on clean column names processed by the `_clean_column_name` function and returns a list of namedtuples for each row in the result set.\n\nParameters:\n- colnames (iterable): An iterable containing the original column names returned from the query.\n- rows (iterable): An iterable of rows, where each row is a sequence of values corresponding to the columns defined in `colnames`.\n\nReturns:\n- list: A list of namedtuples representing each row from the query result. If the creation of namedtuples fails due to too many columns or incompatible names, it falls back to using `pseudo_namedtuple_factory`, which returns a more basic object with similar access capabilities.\n\nNotes:\n- This function is designed to handle potential issues with column names by sanitizing them through `_clean_column_name` to ensure valid namedtuple field names.\n- If the number of columns exceeds Python's limits for namedtuples, a warning is logged, and a different factory is used to maintain performance.\n- The dependency on `pseudo_namedtuple_factory` indicates fallback behavior to avoid failures when creating namedtuples.",
        "signature": "def named_tuple_factory(colnames, rows):",
        "type": "Function",
        "class_signature": null
      }
    },
    "cassandra/__init__.py": {},
    "cassandra/policies.py": {}
  },
  "dependency_dict": {
    "cassandra/query.py:named_tuple_factory": {},
    "cassandra/query.py:_clean_column_name": {}
  },
  "PRD": "# PROJECT NAME: scylla_driver-test_row_factories\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 cassandra/\n    \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 InvalidRequest.InvalidRequest\n    \u2502   \u2514\u2500\u2500 OperationTimedOut.OperationTimedOut\n    \u251c\u2500\u2500 policies.py\n    \u2502   \u251c\u2500\u2500 DCAwareRoundRobinPolicy.DCAwareRoundRobinPolicy\n    \u2502   \u251c\u2500\u2500 RackAwareRoundRobinPolicy.RackAwareRoundRobinPolicy\n    \u2502   \u2514\u2500\u2500 TokenAwarePolicy.TokenAwarePolicy\n    \u2514\u2500\u2500 query.py\n        \u251c\u2500\u2500 _clean_column_name\n        \u2514\u2500\u2500 named_tuple_factory\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThe module facilitates the reliable conversion of Cassandra query results into named tuples for efficient data access and manipulation in Python applications. Its primary purpose is to ensure compatibility and functionality of the `named_tuple_factory` row factory, particularly addressing issues related to Python versions where long column lists cause limitations or compatibility warnings. It provides capabilities for generating named tuple structures from query results, enabling intuitive, attribute-based data retrieval while ensuring compatibility with Python\u2019s built-in data types. By resolving version-specific inconsistencies, the module eliminates potential errors, streamlines data handling, and enhances developer productivity when working with Cassandra query results in Python environments.\n\n## FILE 1: cassandra/query.py\n\n- FUNCTION NAME: named_tuple_factory\n  - SIGNATURE: def named_tuple_factory(colnames, rows):\n  - DOCSTRING: \n```python\n\"\"\"\nReturns each row from a Cassandra query result as a `namedtuple`, allowing for attribute-style access to row fields. The function relies on clean column names processed by the `_clean_column_name` function and returns a list of namedtuples for each row in the result set.\n\nParameters:\n- colnames (iterable): An iterable containing the original column names returned from the query.\n- rows (iterable): An iterable of rows, where each row is a sequence of values corresponding to the columns defined in `colnames`.\n\nReturns:\n- list: A list of namedtuples representing each row from the query result. If the creation of namedtuples fails due to too many columns or incompatible names, it falls back to using `pseudo_namedtuple_factory`, which returns a more basic object with similar access capabilities.\n\nNotes:\n- This function is designed to handle potential issues with column names by sanitizing them through `_clean_column_name` to ensure valid namedtuple field names.\n- If the number of columns exceeds Python's limits for namedtuples, a warning is logged, and a different factory is used to maintain performance.\n- The dependency on `pseudo_namedtuple_factory` indicates fallback behavior to avoid failures when creating namedtuples.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - cassandra/query.py:_clean_column_name\n\n- FUNCTION NAME: _clean_column_name\n  - SIGNATURE: def _clean_column_name(name):\n  - DOCSTRING: \n```python\n\"\"\"\nCleans a column name by replacing non-alphanumeric characters with underscores and removing leading or trailing bad characters.\n\nParameters:\n- name (str): The column name to be cleaned.\n\nReturns:\n- str: The sanitized column name with non-alphanumeric characters replaced and boundaries adjusted.\n\nThis function utilizes the following constants:\n- NON_ALPHA_REGEX: A regex pattern to match any character that is not alphanumeric, defined at the module level.\n- START_BADCHAR_REGEX: A regex pattern that matches non-alphanumeric characters at the start of the string, which is also defined at the module level.\n- END_BADCHAR_REGEX: A regex pattern for non-alphanumeric characters at the end of the string, defined at the module level.\n- _clean_name_cache: A dictionary that caches previously cleaned names to avoid redundant processing, improving efficiency. It is defined and used within this function to store and retrieve cleaned names based on input.\n\nThis function is part of a data processing flow that ensures column names conform to specific identifier standards in the context of a Cassandra database interaction.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - cassandra/query.py:named_tuple_factory\n\n## FILE 2: cassandra/__init__.py\n\n## FILE 3: cassandra/policies.py\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "cassandra/query.py": "\"\"\"\nThis module holds classes for working with prepared statements and\nspecifying consistency levels and retry policies for individual\nqueries.\n\"\"\"\nfrom collections import namedtuple\nfrom datetime import datetime, timedelta, timezone\nimport re\nimport struct\nimport time\nimport warnings\nfrom cassandra import ConsistencyLevel, OperationTimedOut\nfrom cassandra.util import unix_time_from_uuid1, maybe_add_timeout_to_query\nfrom cassandra.encoder import Encoder\nimport cassandra.encoder\nfrom cassandra.policies import ColDesc\nfrom cassandra.protocol import _UNSET_VALUE\nfrom cassandra.util import OrderedDict, _sanitize_identifiers\nimport logging\nlog = logging.getLogger(__name__)\nUNSET_VALUE = _UNSET_VALUE\n'\\nSpecifies an unset value when binding a prepared statement.\\n\\nUnset values are ignored, allowing prepared statements to be used without specify\\n\\nSee https://issues.apache.org/jira/browse/CASSANDRA-7304 for further details on semantics.\\n\\n.. versionadded:: 2.6.0\\n\\nOnly valid when using native protocol v4+\\n'\nNON_ALPHA_REGEX = re.compile('[^a-zA-Z0-9]')\nSTART_BADCHAR_REGEX = re.compile('^[^a-zA-Z0-9]*')\nEND_BADCHAR_REGEX = re.compile('[^a-zA-Z0-9_]*$')\n_clean_name_cache = {}\n\ndef tuple_factory(colnames, rows):\n    \"\"\"\n    Returns each row as a tuple\n\n    Example::\n\n        >>> from cassandra.query import tuple_factory\n        >>> session = cluster.connect('mykeyspace')\n        >>> session.row_factory = tuple_factory\n        >>> rows = session.execute(\"SELECT name, age FROM users LIMIT 1\")\n        >>> print(rows[0])\n        ('Bob', 42)\n\n    .. versionchanged:: 2.0.0\n        moved from ``cassandra.decoder`` to ``cassandra.query``\n    \"\"\"\n    return rows\n\nclass PseudoNamedTupleRow(object):\n    \"\"\"\n    Helper class for pseudo_named_tuple_factory. These objects provide an\n    __iter__ interface, as well as index- and attribute-based access to values,\n    but otherwise do not attempt to implement the full namedtuple or iterable\n    interface.\n    \"\"\"\n\n    def __init__(self, ordered_dict):\n        self._dict = ordered_dict\n        self._tuple = tuple(ordered_dict.values())\n\n    def __getattr__(self, name):\n        return self._dict[name]\n\n    def __getitem__(self, idx):\n        return self._tuple[idx]\n\n    def __iter__(self):\n        return iter(self._tuple)\n\n    def __repr__(self):\n        return '{t}({od})'.format(t=self.__class__.__name__, od=self._dict)\n\ndef pseudo_namedtuple_factory(colnames, rows):\n    \"\"\"\n    Returns each row as a :class:`.PseudoNamedTupleRow`. This is the fallback\n    factory for cases where :meth:`.named_tuple_factory` fails to create rows.\n    \"\"\"\n    return [PseudoNamedTupleRow(od) for od in ordered_dict_factory(colnames, rows)]\n\ndef dict_factory(colnames, rows):\n    \"\"\"\n    Returns each row as a dict.\n\n    Example::\n\n        >>> from cassandra.query import dict_factory\n        >>> session = cluster.connect('mykeyspace')\n        >>> session.row_factory = dict_factory\n        >>> rows = session.execute(\"SELECT name, age FROM users LIMIT 1\")\n        >>> print(rows[0])\n        {u'age': 42, u'name': u'Bob'}\n\n    .. versionchanged:: 2.0.0\n        moved from ``cassandra.decoder`` to ``cassandra.query``\n    \"\"\"\n    return [dict(zip(colnames, row)) for row in rows]\n\ndef ordered_dict_factory(colnames, rows):\n    \"\"\"\n    Like :meth:`~cassandra.query.dict_factory`, but returns each row as an OrderedDict,\n    so the order of the columns is preserved.\n\n    .. versionchanged:: 2.0.0\n        moved from ``cassandra.decoder`` to ``cassandra.query``\n    \"\"\"\n    return [OrderedDict(zip(colnames, row)) for row in rows]\nFETCH_SIZE_UNSET = object()\n\nclass Statement(object):\n    \"\"\"\n    An abstract class representing a single query. There are three subclasses:\n    :class:`.SimpleStatement`, :class:`.BoundStatement`, and :class:`.BatchStatement`.\n    These can be passed to :meth:`.Session.execute()`.\n    \"\"\"\n    retry_policy = None\n    '\\n    An instance of a :class:`cassandra.policies.RetryPolicy` or one of its\\n    subclasses.  This controls when a query will be retried and how it\\n    will be retried.\\n    '\n    consistency_level = None\n    '\\n    The :class:`.ConsistencyLevel` to be used for this operation.  Defaults\\n    to :const:`None`, which means that the default consistency level for\\n    the Session this is executed in will be used.\\n    '\n    fetch_size = FETCH_SIZE_UNSET\n    '\\n    How many rows will be fetched at a time.  This overrides the default\\n    of :attr:`.Session.default_fetch_size`\\n\\n    This only takes effect when protocol version 2 or higher is used.\\n    See :attr:`.Cluster.protocol_version` for details.\\n\\n    .. versionadded:: 2.0.0\\n    '\n    keyspace = None\n    '\\n    The string name of the keyspace this query acts on. This is used when\\n    :class:`~.TokenAwarePolicy` is configured in the profile load balancing policy.\\n\\n    It is set implicitly on :class:`.BoundStatement`, and :class:`.BatchStatement`,\\n    but must be set explicitly on :class:`.SimpleStatement`.\\n\\n    .. versionadded:: 2.1.3\\n    '\n    table = None\n    '\\n    The string name of the table this query acts on. This is used when the tablet\\n    feature is enabled and in the same time :class`~.TokenAwarePolicy` is configured\\n    in the profile load balancing policy.\\n    '\n    custom_payload = None\n    '\\n    :ref:`custom_payload` to be passed to the server.\\n\\n    These are only allowed when using protocol version 4 or higher.\\n\\n    .. versionadded:: 2.6.0\\n    '\n    is_idempotent = False\n    '\\n    Flag indicating whether this statement is safe to run multiple times in speculative execution.\\n    '\n    _serial_consistency_level = None\n    _routing_key = None\n\n    def __init__(self, retry_policy=None, consistency_level=None, routing_key=None, serial_consistency_level=None, fetch_size=FETCH_SIZE_UNSET, keyspace=None, custom_payload=None, is_idempotent=False, table=None):\n        if retry_policy and (not hasattr(retry_policy, 'on_read_timeout')):\n            raise ValueError('retry_policy should implement cassandra.policies.RetryPolicy')\n        if retry_policy is not None:\n            self.retry_policy = retry_policy\n        if consistency_level is not None:\n            self.consistency_level = consistency_level\n        self._routing_key = routing_key\n        if serial_consistency_level is not None:\n            self.serial_consistency_level = serial_consistency_level\n        if fetch_size is not FETCH_SIZE_UNSET:\n            self.fetch_size = fetch_size\n        if keyspace is not None:\n            self.keyspace = keyspace\n        if table is not None:\n            self.table = table\n        if custom_payload is not None:\n            self.custom_payload = custom_payload\n        self.is_idempotent = is_idempotent\n\n    def _key_parts_packed(self, parts):\n        for p in parts:\n            l = len(p)\n            yield struct.pack('>H%dsB' % l, l, p, 0)\n\n    def _get_routing_key(self):\n        return self._routing_key\n\n    def _set_routing_key(self, key):\n        if isinstance(key, (list, tuple)):\n            if len(key) == 1:\n                self._routing_key = key[0]\n            else:\n                self._routing_key = b''.join(self._key_parts_packed(key))\n        else:\n            self._routing_key = key\n\n    def _del_routing_key(self):\n        self._routing_key = None\n    routing_key = property(_get_routing_key, _set_routing_key, _del_routing_key, '\\n        The :attr:`~.TableMetadata.partition_key` portion of the primary key,\\n        which can be used to determine which nodes are replicas for the query.\\n\\n        If the partition key is a composite, a list or tuple must be passed in.\\n        Each key component should be in its packed (binary) format, so all\\n        components should be strings.\\n        ')\n\n    def _get_serial_consistency_level(self):\n        return self._serial_consistency_level\n\n    def _set_serial_consistency_level(self, serial_consistency_level):\n        if serial_consistency_level is not None and (not ConsistencyLevel.is_serial(serial_consistency_level)):\n            raise ValueError('serial_consistency_level must be either ConsistencyLevel.SERIAL or ConsistencyLevel.LOCAL_SERIAL')\n        self._serial_consistency_level = serial_consistency_level\n\n    def _del_serial_consistency_level(self):\n        self._serial_consistency_level = None\n    serial_consistency_level = property(_get_serial_consistency_level, _set_serial_consistency_level, _del_serial_consistency_level, '\\n        The serial consistency level is only used by conditional updates\\n        (``INSERT``, ``UPDATE`` and ``DELETE`` with an ``IF`` condition).  For\\n        those, the ``serial_consistency_level`` defines the consistency level of\\n        the serial phase (or \"paxos\" phase) while the normal\\n        :attr:`~.consistency_level` defines the consistency for the \"learn\" phase,\\n        i.e. what type of reads will be guaranteed to see the update right away.\\n        For example, if a conditional write has a :attr:`~.consistency_level` of\\n        :attr:`~.ConsistencyLevel.QUORUM` (and is successful), then a\\n        :attr:`~.ConsistencyLevel.QUORUM` read is guaranteed to see that write.\\n        But if the regular :attr:`~.consistency_level` of that write is\\n        :attr:`~.ConsistencyLevel.ANY`, then only a read with a\\n        :attr:`~.consistency_level` of :attr:`~.ConsistencyLevel.SERIAL` is\\n        guaranteed to see it (even a read with consistency\\n        :attr:`~.ConsistencyLevel.ALL` is not guaranteed to be enough).\\n\\n        The serial consistency can only be one of :attr:`~.ConsistencyLevel.SERIAL`\\n        or :attr:`~.ConsistencyLevel.LOCAL_SERIAL`. While ``SERIAL`` guarantees full\\n        linearizability (with other ``SERIAL`` updates), ``LOCAL_SERIAL`` only\\n        guarantees it in the local data center.\\n\\n        The serial consistency level is ignored for any query that is not a\\n        conditional update. Serial reads should use the regular\\n        :attr:`consistency_level`.\\n\\n        Serial consistency levels may only be used against Cassandra 2.0+\\n        and the :attr:`~.Cluster.protocol_version` must be set to 2 or higher.\\n\\n        See :doc:`/lwt` for a discussion on how to work with results returned from\\n        conditional statements.\\n\\n        .. versionadded:: 2.0.0\\n        ')\n\nclass SimpleStatement(Statement):\n    \"\"\"\n    A simple, un-prepared query.\n    \"\"\"\n\n    def __init__(self, query_string, retry_policy=None, consistency_level=None, routing_key=None, serial_consistency_level=None, fetch_size=FETCH_SIZE_UNSET, keyspace=None, custom_payload=None, is_idempotent=False):\n        \"\"\"\n        `query_string` should be a literal CQL statement with the exception\n        of parameter placeholders that will be filled through the\n        `parameters` argument of :meth:`.Session.execute()`.\n\n        See :class:`Statement` attributes for a description of the other parameters.\n        \"\"\"\n        Statement.__init__(self, retry_policy, consistency_level, routing_key, serial_consistency_level, fetch_size, keyspace, custom_payload, is_idempotent)\n        self._query_string = query_string\n\n    @property\n    def query_string(self):\n        return self._query_string\n\n    def __str__(self):\n        consistency = ConsistencyLevel.value_to_name.get(self.consistency_level, 'Not Set')\n        return u'<SimpleStatement query=\"%s\", consistency=%s>' % (self.query_string, consistency)\n    __repr__ = __str__\n\nclass PreparedStatement(object):\n    \"\"\"\n    A statement that has been prepared against at least one Cassandra node.\n    Instances of this class should not be created directly, but through\n    :meth:`.Session.prepare()`.\n\n    A :class:`.PreparedStatement` should be prepared only once. Re-preparing a statement\n    may affect performance (as the operation requires a network roundtrip).\n\n    |prepared_stmt_head|: Do not use ``*`` in prepared statements if you might\n    change the schema of the table being queried. The driver and server each\n    maintain a map between metadata for a schema and statements that were\n    prepared against that schema. When a user changes a schema, e.g. by adding\n    or removing a column, the server invalidates its mappings involving that\n    schema. However, there is currently no way to propagate that invalidation\n    to drivers. Thus, after a schema change, the driver will incorrectly\n    interpret the results of ``SELECT *`` queries prepared before the schema\n    change. This is currently being addressed in `CASSANDRA-10786\n    <https://issues.apache.org/jira/browse/CASSANDRA-10786>`_.\n\n    .. |prepared_stmt_head| raw:: html\n\n       <b>A note about <code>*</code> in prepared statements</b>\n    \"\"\"\n    column_metadata = None\n    retry_policy = None\n    consistency_level = None\n    custom_payload = None\n    fetch_size = FETCH_SIZE_UNSET\n    keyspace = None\n    protocol_version = None\n    query_id = None\n    query_string = None\n    result_metadata = None\n    result_metadata_id = None\n    column_encryption_policy = None\n    routing_key_indexes = None\n    _routing_key_index_set = None\n    serial_consistency_level = None\n\n    def __init__(self, column_metadata, query_id, routing_key_indexes, query, keyspace, protocol_version, result_metadata, result_metadata_id, column_encryption_policy=None):\n        self.column_metadata = column_metadata\n        self.query_id = query_id\n        self.routing_key_indexes = routing_key_indexes\n        self.query_string = query\n        self.keyspace = keyspace\n        self.protocol_version = protocol_version\n        self.result_metadata = result_metadata\n        self.result_metadata_id = result_metadata_id\n        self.column_encryption_policy = column_encryption_policy\n        self.is_idempotent = False\n\n    @classmethod\n    def from_message(cls, query_id, column_metadata, pk_indexes, cluster_metadata, query, prepared_keyspace, protocol_version, result_metadata, result_metadata_id, column_encryption_policy=None):\n        if not column_metadata:\n            return PreparedStatement(column_metadata, query_id, None, query, prepared_keyspace, protocol_version, result_metadata, result_metadata_id, column_encryption_policy)\n        if pk_indexes:\n            routing_key_indexes = pk_indexes\n        else:\n            routing_key_indexes = None\n            first_col = column_metadata[0]\n            ks_meta = cluster_metadata.keyspaces.get(first_col.keyspace_name)\n            if ks_meta:\n                table_meta = ks_meta.tables.get(first_col.table_name)\n                if table_meta:\n                    partition_key_columns = table_meta.partition_key\n                    statement_indexes = dict(((c.name, i) for i, c in enumerate(column_metadata)))\n                    try:\n                        routing_key_indexes = [statement_indexes[c.name] for c in partition_key_columns]\n                    except KeyError:\n                        pass\n        return PreparedStatement(column_metadata, query_id, routing_key_indexes, query, prepared_keyspace, protocol_version, result_metadata, result_metadata_id, column_encryption_policy)\n\n    def bind(self, values):\n        \"\"\"\n        Creates and returns a :class:`BoundStatement` instance using `values`.\n\n        See :meth:`BoundStatement.bind` for rules on input ``values``.\n        \"\"\"\n        return BoundStatement(self).bind(values)\n\n    def is_routing_key_index(self, i):\n        if self._routing_key_index_set is None:\n            self._routing_key_index_set = set(self.routing_key_indexes) if self.routing_key_indexes else set()\n        return i in self._routing_key_index_set\n\n    def __str__(self):\n        consistency = ConsistencyLevel.value_to_name.get(self.consistency_level, 'Not Set')\n        return u'<PreparedStatement query=\"%s\", consistency=%s>' % (self.query_string, consistency)\n    __repr__ = __str__\n\nclass BoundStatement(Statement):\n    \"\"\"\n    A prepared statement that has been bound to a particular set of values.\n    These may be created directly or through :meth:`.PreparedStatement.bind()`.\n    \"\"\"\n    prepared_statement = None\n    '\\n    The :class:`PreparedStatement` instance that this was created from.\\n    '\n    values = None\n    '\\n    The sequence of values that were bound to the prepared statement.\\n    '\n\n    def __init__(self, prepared_statement, retry_policy=None, consistency_level=None, routing_key=None, serial_consistency_level=None, fetch_size=FETCH_SIZE_UNSET, keyspace=None, custom_payload=None):\n        \"\"\"\n        `prepared_statement` should be an instance of :class:`PreparedStatement`.\n\n        See :class:`Statement` attributes for a description of the other parameters.\n        \"\"\"\n        self.prepared_statement = prepared_statement\n        self.retry_policy = prepared_statement.retry_policy\n        self.consistency_level = prepared_statement.consistency_level\n        self.serial_consistency_level = prepared_statement.serial_consistency_level\n        self.fetch_size = prepared_statement.fetch_size\n        self.custom_payload = prepared_statement.custom_payload\n        self.is_idempotent = prepared_statement.is_idempotent\n        self.values = []\n        meta = prepared_statement.column_metadata\n        if meta:\n            self.keyspace = meta[0].keyspace_name\n            self.table = meta[0].table_name\n        Statement.__init__(self, retry_policy, consistency_level, routing_key, serial_consistency_level, fetch_size, keyspace, custom_payload, prepared_statement.is_idempotent)\n\n    def bind(self, values):\n        \"\"\"\n        Binds a sequence of values for the prepared statement parameters\n        and returns this instance.  Note that `values` *must* be:\n\n        * a sequence, even if you are only binding one value, or\n        * a dict that relates 1-to-1 between dict keys and columns\n\n        .. versionchanged:: 2.6.0\n\n            :data:`~.UNSET_VALUE` was introduced. These can be bound as positional parameters\n            in a sequence, or by name in a dict. Additionally, when using protocol v4+:\n\n            * short sequences will be extended to match bind parameters with UNSET_VALUE\n            * names may be omitted from a dict with UNSET_VALUE implied.\n\n        .. versionchanged:: 3.0.0\n\n            method will not throw if extra keys are present in bound dict (PYTHON-178)\n        \"\"\"\n        if values is None:\n            values = ()\n        proto_version = self.prepared_statement.protocol_version\n        col_meta = self.prepared_statement.column_metadata\n        ce_policy = self.prepared_statement.column_encryption_policy\n        if isinstance(values, dict):\n            values_dict = values\n            values = []\n            for col in col_meta:\n                try:\n                    values.append(values_dict[col.name])\n                except KeyError:\n                    if proto_version >= 4:\n                        values.append(UNSET_VALUE)\n                    else:\n                        raise KeyError('Column name `%s` not found in bound dict.' % col.name)\n        value_len = len(values)\n        col_meta_len = len(col_meta)\n        if value_len > col_meta_len:\n            raise ValueError('Too many arguments provided to bind() (got %d, expected %d)' % (len(values), len(col_meta)))\n        if proto_version < 4 and self.prepared_statement.routing_key_indexes and (value_len < len(self.prepared_statement.routing_key_indexes)):\n            raise ValueError('Too few arguments provided to bind() (got %d, required %d for routing key)' % (value_len, len(self.prepared_statement.routing_key_indexes)))\n        self.raw_values = values\n        self.values = []\n        for value, col_spec in zip(values, col_meta):\n            if value is None:\n                self.values.append(None)\n            elif value is UNSET_VALUE:\n                if proto_version >= 4:\n                    self._append_unset_value()\n                else:\n                    raise ValueError('Attempt to bind UNSET_VALUE while using unsuitable protocol version (%d < 4)' % proto_version)\n            else:\n                try:\n                    col_desc = ColDesc(col_spec.keyspace_name, col_spec.table_name, col_spec.name)\n                    uses_ce = ce_policy and ce_policy.contains_column(col_desc)\n                    col_type = ce_policy.column_type(col_desc) if uses_ce else col_spec.type\n                    col_bytes = col_type.serialize(value, proto_version)\n                    if uses_ce:\n                        col_bytes = ce_policy.encrypt(col_desc, col_bytes)\n                    self.values.append(col_bytes)\n                except (TypeError, struct.error) as exc:\n                    actual_type = type(value)\n                    message = 'Received an argument of invalid type for column \"%s\". Expected: %s, Got: %s; (%s)' % (col_spec.name, col_spec.type, actual_type, exc)\n                    raise TypeError(message)\n        if proto_version >= 4:\n            diff = col_meta_len - len(self.values)\n            if diff:\n                for _ in range(diff):\n                    self._append_unset_value()\n        return self\n\n    def _append_unset_value(self):\n        next_index = len(self.values)\n        if self.prepared_statement.is_routing_key_index(next_index):\n            col_meta = self.prepared_statement.column_metadata[next_index]\n            raise ValueError(\"Cannot bind UNSET_VALUE as a part of the routing key '%s'\" % col_meta.name)\n        self.values.append(UNSET_VALUE)\n\n    @property\n    def routing_key(self):\n        if not self.prepared_statement.routing_key_indexes:\n            return None\n        if self._routing_key is not None:\n            return self._routing_key\n        routing_indexes = self.prepared_statement.routing_key_indexes\n        if len(routing_indexes) == 1:\n            self._routing_key = self.values[routing_indexes[0]]\n        else:\n            self._routing_key = b''.join(self._key_parts_packed((self.values[i] for i in routing_indexes)))\n        return self._routing_key\n\n    def __str__(self):\n        consistency = ConsistencyLevel.value_to_name.get(self.consistency_level, 'Not Set')\n        return u'<BoundStatement query=\"%s\", values=%s, consistency=%s>' % (self.prepared_statement.query_string, self.raw_values, consistency)\n    __repr__ = __str__\n\nclass BatchType(object):\n    \"\"\"\n    A BatchType is used with :class:`.BatchStatement` instances to control\n    the atomicity of the batch operation.\n\n    .. versionadded:: 2.0.0\n    \"\"\"\n    LOGGED = None\n    '\\n    Atomic batch operation.\\n    '\n    UNLOGGED = None\n    '\\n    Non-atomic batch operation.\\n    '\n    COUNTER = None\n    '\\n    Batches of counter operations.\\n    '\n\n    def __init__(self, name, value):\n        self.name = name\n        self.value = value\n\n    def __str__(self):\n        return self.name\n\n    def __repr__(self):\n        return 'BatchType.%s' % (self.name,)\nBatchType.LOGGED = BatchType('LOGGED', 0)\nBatchType.UNLOGGED = BatchType('UNLOGGED', 1)\nBatchType.COUNTER = BatchType('COUNTER', 2)\n\nclass BatchStatement(Statement):\n    \"\"\"\n    A protocol-level batch of operations which are applied atomically\n    by default.\n\n    .. versionadded:: 2.0.0\n    \"\"\"\n    batch_type = None\n    '\\n    The :class:`.BatchType` for the batch operation.  Defaults to\\n    :attr:`.BatchType.LOGGED`.\\n    '\n    serial_consistency_level = None\n    '\\n    The same as :attr:`.Statement.serial_consistency_level`, but is only\\n    supported when using protocol version 3 or higher.\\n    '\n    _statements_and_parameters = None\n    _session = None\n\n    def __init__(self, batch_type=BatchType.LOGGED, retry_policy=None, consistency_level=None, serial_consistency_level=None, session=None, custom_payload=None):\n        \"\"\"\n        `batch_type` specifies The :class:`.BatchType` for the batch operation.\n        Defaults to :attr:`.BatchType.LOGGED`.\n\n        `retry_policy` should be a :class:`~.RetryPolicy` instance for\n        controlling retries on the operation.\n\n        `consistency_level` should be a :class:`~.ConsistencyLevel` value\n        to be used for all operations in the batch.\n\n        `custom_payload` is a :ref:`custom_payload` passed to the server.\n        Note: as Statement objects are added to the batch, this map is\n        updated with any values found in their custom payloads. These are\n        only allowed when using protocol version 4 or higher.\n\n        Example usage:\n\n        .. code-block:: python\n\n            insert_user = session.prepare(\"INSERT INTO users (name, age) VALUES (?, ?)\")\n            batch = BatchStatement(consistency_level=ConsistencyLevel.QUORUM)\n\n            for (name, age) in users_to_insert:\n                batch.add(insert_user, (name, age))\n\n            session.execute(batch)\n\n        You can also mix different types of operations within a batch:\n\n        .. code-block:: python\n\n            batch = BatchStatement()\n            batch.add(SimpleStatement(\"INSERT INTO users (name, age) VALUES (%s, %s)\"), (name, age))\n            batch.add(SimpleStatement(\"DELETE FROM pending_users WHERE name=%s\"), (name,))\n            session.execute(batch)\n\n        .. versionadded:: 2.0.0\n\n        .. versionchanged:: 2.1.0\n            Added `serial_consistency_level` as a parameter\n\n        .. versionchanged:: 2.6.0\n            Added `custom_payload` as a parameter\n        \"\"\"\n        self.batch_type = batch_type\n        self._statements_and_parameters = []\n        self._session = session\n        Statement.__init__(self, retry_policy=retry_policy, consistency_level=consistency_level, serial_consistency_level=serial_consistency_level, custom_payload=custom_payload)\n\n    def clear(self):\n        \"\"\"\n        This is a convenience method to clear a batch statement for reuse.\n\n        *Note:* it should not be used concurrently with uncompleted execution futures executing the same\n        ``BatchStatement``.\n        \"\"\"\n        del self._statements_and_parameters[:]\n        self.keyspace = None\n        self.routing_key = None\n        if self.custom_payload:\n            self.custom_payload.clear()\n\n    def add(self, statement, parameters=None):\n        \"\"\"\n        Adds a :class:`.Statement` and optional sequence of parameters\n        to be used with the statement to the batch.\n\n        Like with other statements, parameters must be a sequence, even\n        if there is only one item.\n        \"\"\"\n        if isinstance(statement, str):\n            if parameters:\n                encoder = Encoder() if self._session is None else self._session.encoder\n                statement = bind_params(statement, parameters, encoder)\n            self._add_statement_and_params(False, statement, ())\n        elif isinstance(statement, PreparedStatement):\n            query_id = statement.query_id\n            bound_statement = statement.bind(() if parameters is None else parameters)\n            self._update_state(bound_statement)\n            self._add_statement_and_params(True, query_id, bound_statement.values)\n        elif isinstance(statement, BoundStatement):\n            if parameters:\n                raise ValueError('Parameters cannot be passed with a BoundStatement to BatchStatement.add()')\n            self._update_state(statement)\n            self._add_statement_and_params(True, statement.prepared_statement.query_id, statement.values)\n        else:\n            query_string = statement.query_string\n            if parameters:\n                encoder = Encoder() if self._session is None else self._session.encoder\n                query_string = bind_params(query_string, parameters, encoder)\n            self._update_state(statement)\n            self._add_statement_and_params(False, query_string, ())\n        return self\n\n    def add_all(self, statements, parameters):\n        \"\"\"\n        Adds a sequence of :class:`.Statement` objects and a matching sequence\n        of parameters to the batch. Statement and parameter sequences must be of equal length or\n        one will be truncated. :const:`None` can be used in the parameters position where are needed.\n        \"\"\"\n        for statement, value in zip(statements, parameters):\n            self.add(statement, value)\n\n    def _add_statement_and_params(self, is_prepared, statement, parameters):\n        if len(self._statements_and_parameters) >= 65535:\n            raise ValueError('Batch statement cannot contain more than %d statements.' % 65535)\n        self._statements_and_parameters.append((is_prepared, statement, parameters))\n\n    def _maybe_set_routing_attributes(self, statement):\n        if self.routing_key is None:\n            if statement.keyspace and statement.routing_key:\n                self.routing_key = statement.routing_key\n                self.keyspace = statement.keyspace\n\n    def _update_custom_payload(self, statement):\n        if statement.custom_payload:\n            if self.custom_payload is None:\n                self.custom_payload = {}\n            self.custom_payload.update(statement.custom_payload)\n\n    def _update_state(self, statement):\n        self._maybe_set_routing_attributes(statement)\n        self._update_custom_payload(statement)\n\n    def __len__(self):\n        return len(self._statements_and_parameters)\n\n    def __str__(self):\n        consistency = ConsistencyLevel.value_to_name.get(self.consistency_level, 'Not Set')\n        return u'<BatchStatement type=%s, statements=%d, consistency=%s>' % (self.batch_type, len(self), consistency)\n    __repr__ = __str__\nValueSequence = cassandra.encoder.ValueSequence\n'\\nA wrapper class that is used to specify that a sequence of values should\\nbe treated as a CQL list of values instead of a single column collection when used\\nas part of the `parameters` argument for :meth:`.Session.execute()`.\\n\\nThis is typically needed when supplying a list of keys to select.\\nFor example::\\n\\n    >>> my_user_ids = (\\'alice\\', \\'bob\\', \\'charles\\')\\n    >>> query = \"SELECT * FROM users WHERE user_id IN %s\"\\n    >>> session.execute(query, parameters=[ValueSequence(my_user_ids)])\\n\\n'\n\ndef bind_params(query, params, encoder):\n    if isinstance(params, dict):\n        return query % dict(((k, encoder.cql_encode_all_types(v)) for k, v in params.items()))\n    else:\n        return query % tuple((encoder.cql_encode_all_types(v) for v in params))\n\nclass TraceUnavailable(Exception):\n    \"\"\"\n    Raised when complete trace details cannot be fetched from Cassandra.\n    \"\"\"\n    pass\n\nclass QueryTrace(object):\n    \"\"\"\n    A trace of the duration and events that occurred when executing\n    an operation.\n    \"\"\"\n    trace_id = None\n    '\\n    :class:`uuid.UUID` unique identifier for this tracing session.  Matches\\n    the ``session_id`` column in ``system_traces.sessions`` and\\n    ``system_traces.events``.\\n    '\n    request_type = None\n    '\\n    A string that very generally describes the traced operation.\\n    '\n    duration = None\n    '\\n    A :class:`datetime.timedelta` measure of the duration of the query.\\n    '\n    client = None\n    '\\n    The IP address of the client that issued this request\\n\\n    This is only available when using Cassandra 2.2+\\n    '\n    coordinator = None\n    '\\n    The IP address of the host that acted as coordinator for this request.\\n    '\n    parameters = None\n    '\\n    A :class:`dict` of parameters for the traced operation, such as the\\n    specific query string.\\n    '\n    started_at = None\n    '\\n    A UTC :class:`datetime.datetime` object describing when the operation\\n    was started.\\n    '\n    events = None\n    '\\n    A chronologically sorted list of :class:`.TraceEvent` instances\\n    representing the steps the traced operation went through.  This\\n    corresponds to the rows in ``system_traces.events`` for this tracing\\n    session.\\n    '\n    _session = None\n    _SELECT_SESSIONS_FORMAT = 'SELECT * FROM system_traces.sessions WHERE session_id = %s'\n    _SELECT_EVENTS_FORMAT = 'SELECT * FROM system_traces.events WHERE session_id = %s'\n    _BASE_RETRY_SLEEP = 0.003\n\n    def __init__(self, trace_id, session):\n        self.trace_id = trace_id\n        self._session = session\n\n    def populate(self, max_wait=2.0, wait_for_complete=True, query_cl=None):\n        \"\"\"\n        Retrieves the actual tracing details from Cassandra and populates the\n        attributes of this instance.  Because tracing details are stored\n        asynchronously by Cassandra, this may need to retry the session\n        detail fetch.  If the trace is still not available after `max_wait`\n        seconds, :exc:`.TraceUnavailable` will be raised; if `max_wait` is\n        :const:`None`, this will retry forever.\n\n        `wait_for_complete=False` bypasses the wait for duration to be populated.\n        This can be used to query events from partial sessions.\n\n        `query_cl` specifies a consistency level to use for polling the trace tables,\n        if it should be different than the session default.\n        \"\"\"\n        attempt = 0\n        start = time.time()\n        while True:\n            time_spent = time.time() - start\n            if max_wait is not None and time_spent >= max_wait:\n                raise TraceUnavailable('Trace information was not available within %f seconds. Consider raising Session.max_trace_wait.' % (max_wait,))\n            log.debug('Attempting to fetch trace info for trace ID: %s', self.trace_id)\n            metadata_request_timeout = self._session.cluster.control_connection and self._session.cluster.control_connection._metadata_request_timeout\n            session_results = self._execute(SimpleStatement(maybe_add_timeout_to_query(self._SELECT_SESSIONS_FORMAT, metadata_request_timeout), consistency_level=query_cl), (self.trace_id,), time_spent, max_wait)\n            session_row = session_results.one() if session_results else None\n            is_complete = session_row is not None and session_row.duration is not None and (session_row.started_at is not None)\n            if not session_results or (wait_for_complete and (not is_complete)):\n                time.sleep(self._BASE_RETRY_SLEEP * 2 ** attempt)\n                attempt += 1\n                continue\n            if is_complete:\n                log.debug('Fetched trace info for trace ID: %s', self.trace_id)\n            else:\n                log.debug('Fetching parital trace info for trace ID: %s', self.trace_id)\n            self.request_type = session_row.request\n            self.duration = timedelta(microseconds=session_row.duration) if is_complete else None\n            self.started_at = session_row.started_at\n            self.coordinator = session_row.coordinator\n            self.parameters = session_row.parameters\n            self.client = getattr(session_row, 'client', None)\n            log.debug('Attempting to fetch trace events for trace ID: %s', self.trace_id)\n            time_spent = time.time() - start\n            event_results = self._execute(SimpleStatement(maybe_add_timeout_to_query(self._SELECT_EVENTS_FORMAT, metadata_request_timeout), consistency_level=query_cl), (self.trace_id,), time_spent, max_wait)\n            log.debug('Fetched trace events for trace ID: %s', self.trace_id)\n            self.events = tuple((TraceEvent(r.activity, r.event_id, r.source, r.source_elapsed, r.thread) for r in event_results))\n            break\n\n    def _execute(self, query, parameters, time_spent, max_wait):\n        timeout = max_wait - time_spent if max_wait is not None else None\n        future = self._session._create_response_future(query, parameters, trace=False, custom_payload=None, timeout=timeout)\n        future.row_factory = named_tuple_factory\n        future.send_request()\n        try:\n            return future.result()\n        except OperationTimedOut:\n            raise TraceUnavailable('Trace information was not available within %f seconds' % (max_wait,))\n\n    def __str__(self):\n        return '%s [%s] coordinator: %s, started at: %s, duration: %s, parameters: %s' % (self.request_type, self.trace_id, self.coordinator, self.started_at, self.duration, self.parameters)\n\nclass TraceEvent(object):\n    \"\"\"\n    Representation of a single event within a query trace.\n    \"\"\"\n    description = None\n    '\\n    A brief description of the event.\\n    '\n    datetime = None\n    '\\n    A UTC :class:`datetime.datetime` marking when the event occurred.\\n    '\n    source = None\n    '\\n    The IP address of the node this event occurred on.\\n    '\n    source_elapsed = None\n    '\\n    A :class:`datetime.timedelta` measuring the amount of time until\\n    this event occurred starting from when :attr:`.source` first\\n    received the query.\\n    '\n    thread_name = None\n    '\\n    The name of the thread that this event occurred on.\\n    '\n\n    def __init__(self, description, timeuuid, source, source_elapsed, thread_name):\n        self.description = description\n        self.datetime = datetime.fromtimestamp(unix_time_from_uuid1(timeuuid), tz=timezone.utc)\n        self.source = source\n        if source_elapsed is not None:\n            self.source_elapsed = timedelta(microseconds=source_elapsed)\n        else:\n            self.source_elapsed = None\n        self.thread_name = thread_name\n\n    def __str__(self):\n        return '%s on %s[%s] at %s' % (self.description, self.source, self.thread_name, self.datetime)\n\nclass HostTargetingStatement(object):\n    \"\"\"\n    Wraps any query statement and attaches a target host, making\n    it usable in a targeted LBP without modifying the user's statement.\n    \"\"\"\n\n    def __init__(self, inner_statement, target_host):\n        self.__class__ = type(inner_statement.__class__.__name__, (self.__class__, inner_statement.__class__), {})\n        self.__dict__ = inner_statement.__dict__\n        self.target_host = target_host",
    "cassandra/__init__.py": "from enum import Enum\nimport logging\n\nclass NullHandler(logging.Handler):\n\n    def emit(self, record):\n        pass\nlogging.getLogger('cassandra').addHandler(NullHandler())\n__version_info__ = (3, 28, 0)\n__version__ = '.'.join(map(str, __version_info__))\n\nclass ConsistencyLevel(object):\n    \"\"\"\n    Spcifies how many replicas must respond for an operation to be considered\n    a success.  By default, ``ONE`` is used for all operations.\n    \"\"\"\n    ANY = 0\n    '\\n    Only requires that one replica receives the write *or* the coordinator\\n    stores a hint to replay later. Valid only for writes.\\n    '\n    ONE = 1\n    '\\n    Only one replica needs to respond to consider the operation a success\\n    '\n    TWO = 2\n    '\\n    Two replicas must respond to consider the operation a success\\n    '\n    THREE = 3\n    '\\n    Three replicas must respond to consider the operation a success\\n    '\n    QUORUM = 4\n    '\\n    ``ceil(RF/2) + 1`` replicas must respond to consider the operation a success\\n    '\n    ALL = 5\n    '\\n    All replicas must respond to consider the operation a success\\n    '\n    LOCAL_QUORUM = 6\n    '\\n    Requires a quorum of replicas in the local datacenter\\n    '\n    EACH_QUORUM = 7\n    '\\n    Requires a quorum of replicas in each datacenter\\n    '\n    SERIAL = 8\n    \"\\n    For conditional inserts/updates that utilize Cassandra's lightweight\\n    transactions, this requires consensus among all replicas for the\\n    modified data.\\n    \"\n    LOCAL_SERIAL = 9\n    '\\n    Like :attr:`~ConsistencyLevel.SERIAL`, but only requires consensus\\n    among replicas in the local datacenter.\\n    '\n    LOCAL_ONE = 10\n    '\\n    Sends a request only to replicas in the local datacenter and waits for\\n    one response.\\n    '\n\n    @staticmethod\n    def is_serial(cl):\n        return cl == ConsistencyLevel.SERIAL or cl == ConsistencyLevel.LOCAL_SERIAL\nConsistencyLevel.value_to_name = {ConsistencyLevel.ANY: 'ANY', ConsistencyLevel.ONE: 'ONE', ConsistencyLevel.TWO: 'TWO', ConsistencyLevel.THREE: 'THREE', ConsistencyLevel.QUORUM: 'QUORUM', ConsistencyLevel.ALL: 'ALL', ConsistencyLevel.LOCAL_QUORUM: 'LOCAL_QUORUM', ConsistencyLevel.EACH_QUORUM: 'EACH_QUORUM', ConsistencyLevel.SERIAL: 'SERIAL', ConsistencyLevel.LOCAL_SERIAL: 'LOCAL_SERIAL', ConsistencyLevel.LOCAL_ONE: 'LOCAL_ONE'}\nConsistencyLevel.name_to_value = {'ANY': ConsistencyLevel.ANY, 'ONE': ConsistencyLevel.ONE, 'TWO': ConsistencyLevel.TWO, 'THREE': ConsistencyLevel.THREE, 'QUORUM': ConsistencyLevel.QUORUM, 'ALL': ConsistencyLevel.ALL, 'LOCAL_QUORUM': ConsistencyLevel.LOCAL_QUORUM, 'EACH_QUORUM': ConsistencyLevel.EACH_QUORUM, 'SERIAL': ConsistencyLevel.SERIAL, 'LOCAL_SERIAL': ConsistencyLevel.LOCAL_SERIAL, 'LOCAL_ONE': ConsistencyLevel.LOCAL_ONE}\n\ndef consistency_value_to_name(value):\n    return ConsistencyLevel.value_to_name[value] if value is not None else 'Not Set'\n\nclass ProtocolVersion(object):\n    \"\"\"\n    Defines native protocol versions supported by this driver.\n    \"\"\"\n    V1 = 1\n    '\\n    v1, supported in Cassandra 1.2-->2.2\\n    '\n    V2 = 2\n    '\\n    v2, supported in Cassandra 2.0-->2.2;\\n    added support for lightweight transactions, batch operations, and automatic query paging.\\n    '\n    V3 = 3\n    '\\n    v3, supported in Cassandra 2.1-->3.x+;\\n    added support for protocol-level client-side timestamps (see :attr:`.Session.use_client_timestamp`),\\n    serial consistency levels for :class:`~.BatchStatement`, and an improved connection pool.\\n    '\n    V4 = 4\n    '\\n    v4, supported in Cassandra 2.2-->3.x+;\\n    added a number of new types, server warnings, new failure messages, and custom payloads. Details in the\\n    `project docs <https://github.com/apache/cassandra/blob/trunk/doc/native_protocol_v4.spec>`_\\n    '\n    V5 = 5\n    '\\n    v5, in beta from 3.x+. Finalised in 4.0-beta5\\n    '\n    V6 = 6\n    '\\n    v6, in beta from 4.0-beta5\\n    '\n    DSE_V1 = 65\n    '\\n    DSE private protocol v1, supported in DSE 5.1+\\n    '\n    DSE_V2 = 66\n    '\\n    DSE private protocol v2, supported in DSE 6.0+\\n    '\n    SUPPORTED_VERSIONS = (DSE_V2, DSE_V1, V6, V5, V4, V3, V2, V1)\n    '\\n    A tuple of all supported protocol versions\\n    '\n    BETA_VERSIONS = (V6,)\n    '\\n    A tuple of all beta protocol versions\\n    '\n    MIN_SUPPORTED = min(SUPPORTED_VERSIONS)\n    '\\n    Minimum protocol version supported by this driver.\\n    '\n    MAX_SUPPORTED = max(SUPPORTED_VERSIONS)\n    '\\n    Maximum protocol version supported by this driver.\\n    '\n\n    @classmethod\n    def get_lower_supported(cls, previous_version):\n        \"\"\"\n        Return the lower supported protocol version. Beta versions are omitted.\n        \"\"\"\n        try:\n            version = next((v for v in sorted(ProtocolVersion.SUPPORTED_VERSIONS, reverse=True) if v not in ProtocolVersion.BETA_VERSIONS and v < previous_version))\n        except StopIteration:\n            version = 0\n        return version\n\n    @classmethod\n    def uses_int_query_flags(cls, version):\n        return version >= cls.V5\n\n    @classmethod\n    def uses_prepare_flags(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def uses_prepared_metadata(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def uses_error_code_map(cls, version):\n        return version >= cls.V5\n\n    @classmethod\n    def uses_keyspace_flag(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def has_continuous_paging_support(cls, version):\n        return version >= cls.DSE_V1\n\n    @classmethod\n    def has_continuous_paging_next_pages(cls, version):\n        return version >= cls.DSE_V2\n\n    @classmethod\n    def has_checksumming_support(cls, version):\n        return cls.V5 <= version < cls.DSE_V1\n\nclass WriteType(object):\n    \"\"\"\n    For usage with :class:`.RetryPolicy`, this describe a type\n    of write operation.\n    \"\"\"\n    SIMPLE = 0\n    '\\n    A write to a single partition key. Such writes are guaranteed to be atomic\\n    and isolated.\\n    '\n    BATCH = 1\n    '\\n    A write to multiple partition keys that used the distributed batch log to\\n    ensure atomicity.\\n    '\n    UNLOGGED_BATCH = 2\n    '\\n    A write to multiple partition keys that did not use the distributed batch\\n    log. Atomicity for such writes is not guaranteed.\\n    '\n    COUNTER = 3\n    '\\n    A counter write (for one or multiple partition keys). Such writes should\\n    not be replayed in order to avoid overcount.\\n    '\n    BATCH_LOG = 4\n    '\\n    The initial write to the distributed batch log that Cassandra performs\\n    internally before a BATCH write.\\n    '\n    CAS = 5\n    '\\n    A lighweight-transaction write, such as \"DELETE ... IF EXISTS\".\\n    '\n    VIEW = 6\n    '\\n    This WriteType is only seen in results for requests that were unable to\\n    complete MV operations.\\n    '\n    CDC = 7\n    '\\n    This WriteType is only seen in results for requests that were unable to\\n    complete CDC operations.\\n    '\nWriteType.name_to_value = {'SIMPLE': WriteType.SIMPLE, 'BATCH': WriteType.BATCH, 'UNLOGGED_BATCH': WriteType.UNLOGGED_BATCH, 'COUNTER': WriteType.COUNTER, 'BATCH_LOG': WriteType.BATCH_LOG, 'CAS': WriteType.CAS, 'VIEW': WriteType.VIEW, 'CDC': WriteType.CDC}\nWriteType.value_to_name = {v: k for k, v in WriteType.name_to_value.items()}\n\nclass SchemaChangeType(object):\n    DROPPED = 'DROPPED'\n    CREATED = 'CREATED'\n    UPDATED = 'UPDATED'\n\nclass SchemaTargetType(object):\n    KEYSPACE = 'KEYSPACE'\n    TABLE = 'TABLE'\n    TYPE = 'TYPE'\n    FUNCTION = 'FUNCTION'\n    AGGREGATE = 'AGGREGATE'\n\nclass SignatureDescriptor(object):\n\n    def __init__(self, name, argument_types):\n        self.name = name\n        self.argument_types = argument_types\n\n    @property\n    def signature(self):\n        \"\"\"\n        function signature string in the form 'name([type0[,type1[...]]])'\n\n        can be used to uniquely identify overloaded function names within a keyspace\n        \"\"\"\n        return self.format_signature(self.name, self.argument_types)\n\n    @staticmethod\n    def format_signature(name, argument_types):\n        return '%s(%s)' % (name, ','.join((t for t in argument_types)))\n\n    def __repr__(self):\n        return '%s(%s, %s)' % (self.__class__.__name__, self.name, self.argument_types)\n\nclass UserFunctionDescriptor(SignatureDescriptor):\n    \"\"\"\n    Describes a User function by name and argument signature\n    \"\"\"\n    name = None\n    '\\n    name of the function\\n    '\n    argument_types = None\n    '\\n    Ordered list of CQL argument type names comprising the type signature\\n    '\n\nclass UserAggregateDescriptor(SignatureDescriptor):\n    \"\"\"\n    Describes a User aggregate function by name and argument signature\n    \"\"\"\n    name = None\n    '\\n    name of the aggregate\\n    '\n    argument_types = None\n    '\\n    Ordered list of CQL argument type names comprising the type signature\\n    '\n\nclass DriverException(Exception):\n    \"\"\"\n    Base for all exceptions explicitly raised by the driver.\n    \"\"\"\n    pass\n\nclass RequestExecutionException(DriverException):\n    \"\"\"\n    Base for request execution exceptions returned from the server.\n    \"\"\"\n    pass\n\nclass Unavailable(RequestExecutionException):\n    \"\"\"\n    There were not enough live replicas to satisfy the requested consistency\n    level, so the coordinator node immediately failed the request without\n    forwarding it to any replicas.\n    \"\"\"\n    consistency = None\n    ' The requested :class:`ConsistencyLevel` '\n    required_replicas = None\n    ' The number of replicas that needed to be live to complete the operation '\n    alive_replicas = None\n    ' The number of replicas that were actually alive '\n\n    def __init__(self, summary_message, consistency=None, required_replicas=None, alive_replicas=None):\n        self.consistency = consistency\n        self.required_replicas = required_replicas\n        self.alive_replicas = alive_replicas\n        Exception.__init__(self, summary_message + ' info=' + repr({'consistency': consistency_value_to_name(consistency), 'required_replicas': required_replicas, 'alive_replicas': alive_replicas}))\n\nclass Timeout(RequestExecutionException):\n    \"\"\"\n    Replicas failed to respond to the coordinator node before timing out.\n    \"\"\"\n    consistency = None\n    ' The requested :class:`ConsistencyLevel` '\n    required_responses = None\n    ' The number of required replica responses '\n    received_responses = None\n    '\\n    The number of replicas that responded before the coordinator timed out\\n    the operation\\n    '\n\n    def __init__(self, summary_message, consistency=None, required_responses=None, received_responses=None, **kwargs):\n        self.consistency = consistency\n        self.required_responses = required_responses\n        self.received_responses = received_responses\n        if 'write_type' in kwargs:\n            kwargs['write_type'] = WriteType.value_to_name[kwargs['write_type']]\n        info = {'consistency': consistency_value_to_name(consistency), 'required_responses': required_responses, 'received_responses': received_responses}\n        info.update(kwargs)\n        Exception.__init__(self, summary_message + ' info=' + repr(info))\n\nclass ReadTimeout(Timeout):\n    \"\"\"\n    A subclass of :exc:`Timeout` for read operations.\n\n    This indicates that the replicas failed to respond to the coordinator\n    node before the configured timeout. This timeout is configured in\n    ``cassandra.yaml`` with the ``read_request_timeout_in_ms``\n    and ``range_request_timeout_in_ms`` options.\n    \"\"\"\n    data_retrieved = None\n    '\\n    A boolean indicating whether the requested data was retrieved\\n    by the coordinator from any replicas before it timed out the\\n    operation\\n    '\n\n    def __init__(self, message, data_retrieved=None, **kwargs):\n        Timeout.__init__(self, message, **kwargs)\n        self.data_retrieved = data_retrieved\n\nclass WriteTimeout(Timeout):\n    \"\"\"\n    A subclass of :exc:`Timeout` for write operations.\n\n    This indicates that the replicas failed to respond to the coordinator\n    node before the configured timeout. This timeout is configured in\n    ``cassandra.yaml`` with the ``write_request_timeout_in_ms``\n    option.\n    \"\"\"\n    write_type = None\n    '\\n    The type of write operation, enum on :class:`~cassandra.policies.WriteType`\\n    '\n\n    def __init__(self, message, write_type=None, **kwargs):\n        kwargs['write_type'] = write_type\n        Timeout.__init__(self, message, **kwargs)\n        self.write_type = write_type\n\nclass CDCWriteFailure(RequestExecutionException):\n    \"\"\"\n    Hit limit on data in CDC folder, writes are rejected\n    \"\"\"\n\n    def __init__(self, message):\n        Exception.__init__(self, message)\n\nclass CoordinationFailure(RequestExecutionException):\n    \"\"\"\n    Replicas sent a failure to the coordinator.\n    \"\"\"\n    consistency = None\n    ' The requested :class:`ConsistencyLevel` '\n    required_responses = None\n    ' The number of required replica responses '\n    received_responses = None\n    '\\n    The number of replicas that responded before the coordinator timed out\\n    the operation\\n    '\n    failures = None\n    '\\n    The number of replicas that sent a failure message\\n    '\n    error_code_map = None\n    '\\n    A map of inet addresses to error codes representing replicas that sent\\n    a failure message.  Only set when `protocol_version` is 5 or higher.\\n    '\n\n    def __init__(self, summary_message, consistency=None, required_responses=None, received_responses=None, failures=None, error_code_map=None):\n        self.consistency = consistency\n        self.required_responses = required_responses\n        self.received_responses = received_responses\n        self.failures = failures\n        self.error_code_map = error_code_map\n        info_dict = {'consistency': consistency_value_to_name(consistency), 'required_responses': required_responses, 'received_responses': received_responses, 'failures': failures}\n        if error_code_map is not None:\n            formatted_map = dict(((addr, '0x%04x' % err_code) for addr, err_code in error_code_map.items()))\n            info_dict['error_code_map'] = formatted_map\n        Exception.__init__(self, summary_message + ' info=' + repr(info_dict))\n\nclass ReadFailure(CoordinationFailure):\n    \"\"\"\n    A subclass of :exc:`CoordinationFailure` for read operations.\n\n    This indicates that the replicas sent a failure message to the coordinator.\n    \"\"\"\n    data_retrieved = None\n    '\\n    A boolean indicating whether the requested data was retrieved\\n    by the coordinator from any replicas before it timed out the\\n    operation\\n    '\n\n    def __init__(self, message, data_retrieved=None, **kwargs):\n        CoordinationFailure.__init__(self, message, **kwargs)\n        self.data_retrieved = data_retrieved\n\nclass WriteFailure(CoordinationFailure):\n    \"\"\"\n    A subclass of :exc:`CoordinationFailure` for write operations.\n\n    This indicates that the replicas sent a failure message to the coordinator.\n    \"\"\"\n    write_type = None\n    '\\n    The type of write operation, enum on :class:`~cassandra.policies.WriteType`\\n    '\n\n    def __init__(self, message, write_type=None, **kwargs):\n        CoordinationFailure.__init__(self, message, **kwargs)\n        self.write_type = write_type\n\nclass FunctionFailure(RequestExecutionException):\n    \"\"\"\n    User Defined Function failed during execution\n    \"\"\"\n    keyspace = None\n    '\\n    Keyspace of the function\\n    '\n    function = None\n    '\\n    Name of the function\\n    '\n    arg_types = None\n    '\\n    List of argument type names of the function\\n    '\n\n    def __init__(self, summary_message, keyspace, function, arg_types):\n        self.keyspace = keyspace\n        self.function = function\n        self.arg_types = arg_types\n        Exception.__init__(self, summary_message)\n\nclass RequestValidationException(DriverException):\n    \"\"\"\n    Server request validation failed\n    \"\"\"\n    pass\n\nclass ConfigurationException(RequestValidationException):\n    \"\"\"\n    Server indicated request errro due to current configuration\n    \"\"\"\n    pass\n\nclass AlreadyExists(ConfigurationException):\n    \"\"\"\n    An attempt was made to create a keyspace or table that already exists.\n    \"\"\"\n    keyspace = None\n    '\\n    The name of the keyspace that already exists, or, if an attempt was\\n    made to create a new table, the keyspace that the table is in.\\n    '\n    table = None\n    '\\n    The name of the table that already exists, or, if an attempt was\\n    make to create a keyspace, :const:`None`.\\n    '\n\n    def __init__(self, keyspace=None, table=None):\n        if table:\n            message = \"Table '%s.%s' already exists\" % (keyspace, table)\n        else:\n            message = \"Keyspace '%s' already exists\" % (keyspace,)\n        Exception.__init__(self, message)\n        self.keyspace = keyspace\n        self.table = table\n\nclass InvalidRequest(RequestValidationException):\n    \"\"\"\n    A query was made that was invalid for some reason, such as trying to set\n    the keyspace for a connection to a nonexistent keyspace.\n    \"\"\"\n    pass\n\nclass Unauthorized(RequestValidationException):\n    \"\"\"\n    The current user is not authorized to perform the requested operation.\n    \"\"\"\n    pass\n\nclass AuthenticationFailed(DriverException):\n    \"\"\"\n    Failed to authenticate.\n    \"\"\"\n    pass\n\nclass OperationTimedOut(DriverException):\n    \"\"\"\n    The operation took longer than the specified (client-side) timeout\n    to complete.  This is not an error generated by Cassandra, only\n    the driver.\n    \"\"\"\n    errors = None\n    '\\n    A dict of errors keyed by the :class:`~.Host` against which they occurred.\\n    '\n    last_host = None\n    '\\n    The last :class:`~.Host` this operation was attempted against.\\n    '\n\n    def __init__(self, errors=None, last_host=None):\n        self.errors = errors\n        self.last_host = last_host\n        message = 'errors=%s, last_host=%s' % (self.errors, self.last_host)\n        Exception.__init__(self, message)\n\nclass UnsupportedOperation(DriverException):\n    \"\"\"\n    An attempt was made to use a feature that is not supported by the\n    selected protocol version.  See :attr:`Cluster.protocol_version`\n    for more details.\n    \"\"\"\n    pass\n\nclass UnresolvableContactPoints(DriverException):\n    \"\"\"\n    The driver was unable to resolve any provided hostnames.\n\n    Note that this is *not* raised when a :class:`.Cluster` is created with no\n    contact points, only when lookup fails for all hosts\n    \"\"\"\n    pass\n\nclass OperationType(Enum):\n    Read = 0\n    Write = 1\n\nclass RateLimitReached(ConfigurationException):\n    \"\"\"\n    Rate limit was exceeded for a partition affected by the request.\n    \"\"\"\n    op_type = None\n    rejected_by_coordinator = False\n\n    def __init__(self, op_type=None, rejected_by_coordinator=False):\n        self.op_type = op_type\n        self.rejected_by_coordinator = rejected_by_coordinator\n        message = f'[request_error_rate_limit_reached OpType={op_type.name} RejectedByCoordinator={rejected_by_coordinator}]'\n        Exception.__init__(self, message)",
    "cassandra/policies.py": "import random\nfrom collections import namedtuple\nfrom functools import lru_cache\nfrom itertools import islice, cycle, groupby, repeat\nimport logging\nfrom random import randint, shuffle\nfrom threading import Lock\nimport socket\nimport warnings\nlog = logging.getLogger(__name__)\nfrom cassandra import WriteType as WT\nWriteType = WT\nfrom cassandra import ConsistencyLevel, OperationTimedOut\n\nclass HostDistance(object):\n    \"\"\"\n    A measure of how \"distant\" a node is from the client, which\n    may influence how the load balancer distributes requests\n    and how many connections are opened to the node.\n    \"\"\"\n    IGNORED = -1\n    '\\n    A node with this distance should never be queried or have\\n    connections opened to it.\\n    '\n    LOCAL_RACK = 0\n    '\\n    Nodes with ``LOCAL_RACK`` distance will be preferred for operations\\n    under some load balancing policies (such as :class:`.RackAwareRoundRobinPolicy`)\\n    and will have a greater number of connections opened against\\n    them by default.\\n\\n    This distance is typically used for nodes within the same\\n    datacenter and the same rack as the client.\\n    '\n    LOCAL = 1\n    '\\n    Nodes with ``LOCAL`` distance will be preferred for operations\\n    under some load balancing policies (such as :class:`.DCAwareRoundRobinPolicy`)\\n    and will have a greater number of connections opened against\\n    them by default.\\n\\n    This distance is typically used for nodes within the same\\n    datacenter as the client.\\n    '\n    REMOTE = 2\n    '\\n    Nodes with ``REMOTE`` distance will be treated as a last resort\\n    by some load balancing policies (such as :class:`.DCAwareRoundRobinPolicy`\\n    and :class:`.RackAwareRoundRobinPolicy`)and will have a smaller number of\\n    connections opened against them by default.\\n\\n    This distance is typically used for nodes outside of the\\n    datacenter that the client is running in.\\n    '\n\nclass HostStateListener(object):\n\n    def on_up(self, host):\n        \"\"\" Called when a node is marked up. \"\"\"\n        raise NotImplementedError()\n\n    def on_down(self, host):\n        \"\"\" Called when a node is marked down. \"\"\"\n        raise NotImplementedError()\n\n    def on_add(self, host):\n        \"\"\"\n        Called when a node is added to the cluster.  The newly added node\n        should be considered up.\n        \"\"\"\n        raise NotImplementedError()\n\n    def on_remove(self, host):\n        \"\"\" Called when a node is removed from the cluster. \"\"\"\n        raise NotImplementedError()\n\nclass LoadBalancingPolicy(HostStateListener):\n    \"\"\"\n    Load balancing policies are used to decide how to distribute\n    requests among all possible coordinator nodes in the cluster.\n\n    In particular, they may focus on querying \"near\" nodes (those\n    in a local datacenter) or on querying nodes who happen to\n    be replicas for the requested data.\n\n    You may also use subclasses of :class:`.LoadBalancingPolicy` for\n    custom behavior.\n\n    You should always use immutable collections (e.g., tuples or\n    frozensets) to store information about hosts to prevent accidental\n    modification. When there are changes to the hosts (e.g., a host is\n    down or up), the old collection should be replaced with a new one.\n    \"\"\"\n    _hosts_lock = None\n\n    def __init__(self):\n        self._hosts_lock = Lock()\n\n    def distance(self, host):\n        \"\"\"\n        Returns a measure of how remote a :class:`~.pool.Host` is in\n        terms of the :class:`.HostDistance` enums.\n        \"\"\"\n        raise NotImplementedError()\n\n    def populate(self, cluster, hosts):\n        \"\"\"\n        This method is called to initialize the load balancing\n        policy with a set of :class:`.Host` instances before its\n        first use.  The `cluster` parameter is an instance of\n        :class:`.Cluster`.\n        \"\"\"\n        raise NotImplementedError()\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        \"\"\"\n        Given a :class:`~.query.Statement` instance, return a iterable\n        of :class:`.Host` instances which should be queried in that\n        order.  A generator may work well for custom implementations\n        of this method.\n\n        Note that the `query` argument may be :const:`None` when preparing\n        statements.\n\n        `working_keyspace` should be the string name of the current keyspace,\n        as set through :meth:`.Session.set_keyspace()` or with a ``USE``\n        statement.\n        \"\"\"\n        raise NotImplementedError()\n\n    def check_supported(self):\n        \"\"\"\n        This will be called after the cluster Metadata has been initialized.\n        If the load balancing policy implementation cannot be supported for\n        some reason (such as a missing C extension), this is the point at\n        which it should raise an exception.\n        \"\"\"\n        pass\n\nclass RoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    A subclass of :class:`.LoadBalancingPolicy` which evenly\n    distributes queries across all nodes in the cluster,\n    regardless of what datacenter the nodes may be in.\n    \"\"\"\n    _live_hosts = frozenset(())\n    _position = 0\n\n    def populate(self, cluster, hosts):\n        self._live_hosts = frozenset(hosts)\n        if len(hosts) > 1:\n            self._position = randint(0, len(hosts) - 1)\n\n    def distance(self, host):\n        return HostDistance.LOCAL\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        pos = self._position\n        self._position += 1\n        hosts = self._live_hosts\n        length = len(hosts)\n        if length:\n            pos %= length\n            return islice(cycle(hosts), pos, pos + length)\n        else:\n            return []\n\n    def on_up(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.union((host,))\n\n    def on_down(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.difference((host,))\n\n    def on_add(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.union((host,))\n\n    def on_remove(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.difference((host,))\n\nclass DCAwareRoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    Similar to :class:`.RoundRobinPolicy`, but prefers hosts\n    in the local datacenter and only uses nodes in remote\n    datacenters as a last resort.\n    \"\"\"\n    local_dc = None\n    used_hosts_per_remote_dc = 0\n\n    def __init__(self, local_dc='', used_hosts_per_remote_dc=0):\n        \"\"\"\n        The `local_dc` parameter should be the name of the datacenter\n        (such as is reported by ``nodetool ring``) that should\n        be considered local. If not specified, the driver will choose\n        a local_dc based on the first host among :attr:`.Cluster.contact_points`\n        having a valid DC. If relying on this mechanism, all specified\n        contact points should be nodes in a single, local DC.\n\n        `used_hosts_per_remote_dc` controls how many nodes in\n        each remote datacenter will have connections opened\n        against them. In other words, `used_hosts_per_remote_dc` hosts\n        will be considered :attr:`~.HostDistance.REMOTE` and the\n        rest will be considered :attr:`~.HostDistance.IGNORED`.\n        By default, all remote hosts are ignored.\n        \"\"\"\n        self.local_dc = local_dc\n        self.used_hosts_per_remote_dc = used_hosts_per_remote_dc\n        self._dc_live_hosts = {}\n        self._position = 0\n        self._endpoints = []\n        LoadBalancingPolicy.__init__(self)\n\n    def _dc(self, host):\n        return host.datacenter or self.local_dc\n\n    def populate(self, cluster, hosts):\n        for dc, dc_hosts in groupby(hosts, lambda h: self._dc(h)):\n            self._dc_live_hosts[dc] = tuple(set(dc_hosts))\n        if not self.local_dc:\n            self._endpoints = [endpoint for endpoint in cluster.endpoints_resolved]\n        self._position = randint(0, len(hosts) - 1) if hosts else 0\n\n    def distance(self, host):\n        dc = self._dc(host)\n        if dc == self.local_dc:\n            return HostDistance.LOCAL\n        if not self.used_hosts_per_remote_dc:\n            return HostDistance.IGNORED\n        else:\n            dc_hosts = self._dc_live_hosts.get(dc)\n            if not dc_hosts:\n                return HostDistance.IGNORED\n            if host in list(dc_hosts)[:self.used_hosts_per_remote_dc]:\n                return HostDistance.REMOTE\n            else:\n                return HostDistance.IGNORED\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        pos = self._position\n        self._position += 1\n        local_live = self._dc_live_hosts.get(self.local_dc, ())\n        pos = pos % len(local_live) if local_live else 0\n        for host in islice(cycle(local_live), pos, pos + len(local_live)):\n            yield host\n        other_dcs = [dc for dc in self._dc_live_hosts.copy().keys() if dc != self.local_dc]\n        for dc in other_dcs:\n            remote_live = self._dc_live_hosts.get(dc, ())\n            for host in remote_live[:self.used_hosts_per_remote_dc]:\n                yield host\n\n    def on_up(self, host):\n        if not self.local_dc and host.datacenter:\n            if host.endpoint in self._endpoints:\n                self.local_dc = host.datacenter\n                log.info(\"Using datacenter '%s' for DCAwareRoundRobinPolicy (via host '%s'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\" % (self.local_dc, host.endpoint))\n                del self._endpoints\n        dc = self._dc(host)\n        with self._hosts_lock:\n            current_hosts = self._dc_live_hosts.get(dc, ())\n            if host not in current_hosts:\n                self._dc_live_hosts[dc] = current_hosts + (host,)\n\n    def on_down(self, host):\n        dc = self._dc(host)\n        with self._hosts_lock:\n            current_hosts = self._dc_live_hosts.get(dc, ())\n            if host in current_hosts:\n                hosts = tuple((h for h in current_hosts if h != host))\n                if hosts:\n                    self._dc_live_hosts[dc] = hosts\n                else:\n                    del self._dc_live_hosts[dc]\n\n    def on_add(self, host):\n        self.on_up(host)\n\n    def on_remove(self, host):\n        self.on_down(host)\n\nclass RackAwareRoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    Similar to :class:`.DCAwareRoundRobinPolicy`, but prefers hosts\n    in the local rack, before hosts in the local datacenter but a\n    different rack, before hosts in all other datercentres\n    \"\"\"\n    local_dc = None\n    local_rack = None\n    used_hosts_per_remote_dc = 0\n\n    def __init__(self, local_dc, local_rack, used_hosts_per_remote_dc=0):\n        \"\"\"\n        The `local_dc` and `local_rack` parameters should be the name of the\n        datacenter and rack (such as is reported by ``nodetool ring``) that\n        should be considered local.\n\n        `used_hosts_per_remote_dc` controls how many nodes in\n        each remote datacenter will have connections opened\n        against them. In other words, `used_hosts_per_remote_dc` hosts\n        will be considered :attr:`~.HostDistance.REMOTE` and the\n        rest will be considered :attr:`~.HostDistance.IGNORED`.\n        By default, all remote hosts are ignored.\n        \"\"\"\n        self.local_rack = local_rack\n        self.local_dc = local_dc\n        self.used_hosts_per_remote_dc = used_hosts_per_remote_dc\n        self._live_hosts = {}\n        self._dc_live_hosts = {}\n        self._endpoints = []\n        self._position = 0\n        LoadBalancingPolicy.__init__(self)\n\n    def _rack(self, host):\n        return host.rack or self.local_rack\n\n    def _dc(self, host):\n        return host.datacenter or self.local_dc\n\n    def populate(self, cluster, hosts):\n        for (dc, rack), rack_hosts in groupby(hosts, lambda host: (self._dc(host), self._rack(host))):\n            self._live_hosts[dc, rack] = tuple(set(rack_hosts))\n        for dc, dc_hosts in groupby(hosts, lambda host: self._dc(host)):\n            self._dc_live_hosts[dc] = tuple(set(dc_hosts))\n        self._position = randint(0, len(hosts) - 1) if hosts else 0\n\n    def distance(self, host):\n        rack = self._rack(host)\n        dc = self._dc(host)\n        if rack == self.local_rack and dc == self.local_dc:\n            return HostDistance.LOCAL_RACK\n        if dc == self.local_dc:\n            return HostDistance.LOCAL\n        if not self.used_hosts_per_remote_dc:\n            return HostDistance.IGNORED\n        dc_hosts = self._dc_live_hosts.get(dc, ())\n        if not dc_hosts:\n            return HostDistance.IGNORED\n        if host in dc_hosts and dc_hosts.index(host) < self.used_hosts_per_remote_dc:\n            return HostDistance.REMOTE\n        else:\n            return HostDistance.IGNORED\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        pos = self._position\n        self._position += 1\n        local_rack_live = self._live_hosts.get((self.local_dc, self.local_rack), ())\n        pos = pos % len(local_rack_live) if local_rack_live else 0\n        for host in islice(cycle(local_rack_live), pos, pos + len(local_rack_live)):\n            yield host\n        local_live = [host for host in self._dc_live_hosts.get(self.local_dc, ()) if host.rack != self.local_rack]\n        pos = pos % len(local_live) if local_live else 0\n        for host in islice(cycle(local_live), pos, pos + len(local_live)):\n            yield host\n        for dc, remote_live in self._dc_live_hosts.copy().items():\n            if dc != self.local_dc:\n                for host in remote_live[:self.used_hosts_per_remote_dc]:\n                    yield host\n\n    def on_up(self, host):\n        dc = self._dc(host)\n        rack = self._rack(host)\n        with self._hosts_lock:\n            current_rack_hosts = self._live_hosts.get((dc, rack), ())\n            if host not in current_rack_hosts:\n                self._live_hosts[dc, rack] = current_rack_hosts + (host,)\n            current_dc_hosts = self._dc_live_hosts.get(dc, ())\n            if host not in current_dc_hosts:\n                self._dc_live_hosts[dc] = current_dc_hosts + (host,)\n\n    def on_down(self, host):\n        dc = self._dc(host)\n        rack = self._rack(host)\n        with self._hosts_lock:\n            current_rack_hosts = self._live_hosts.get((dc, rack), ())\n            if host in current_rack_hosts:\n                hosts = tuple((h for h in current_rack_hosts if h != host))\n                if hosts:\n                    self._live_hosts[dc, rack] = hosts\n                else:\n                    del self._live_hosts[dc, rack]\n            current_dc_hosts = self._dc_live_hosts.get(dc, ())\n            if host in current_dc_hosts:\n                hosts = tuple((h for h in current_dc_hosts if h != host))\n                if hosts:\n                    self._dc_live_hosts[dc] = hosts\n                else:\n                    del self._dc_live_hosts[dc]\n\n    def on_add(self, host):\n        self.on_up(host)\n\n    def on_remove(self, host):\n        self.on_down(host)\n\nclass TokenAwarePolicy(LoadBalancingPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` wrapper that adds token awareness to\n    a child policy.\n\n    This alters the child policy's behavior so that it first attempts to\n    send queries to :attr:`~.HostDistance.LOCAL` replicas (as determined\n    by the child policy) based on the :class:`.Statement`'s\n    :attr:`~.Statement.routing_key`. If :attr:`.shuffle_replicas` is\n    truthy, these replicas will be yielded in a random order. Once those\n    hosts are exhausted, the remaining hosts in the child policy's query\n    plan will be used in the order provided by the child policy.\n\n    If no :attr:`~.Statement.routing_key` is set on the query, the child\n    policy's query plan will be used as is.\n    \"\"\"\n    _child_policy = None\n    _cluster_metadata = None\n    _tablets_routing_v1 = False\n    shuffle_replicas = False\n    '\\n    Yield local replicas in a random order.\\n    '\n\n    def __init__(self, child_policy, shuffle_replicas=False):\n        self._child_policy = child_policy\n        self.shuffle_replicas = shuffle_replicas\n\n    def populate(self, cluster, hosts):\n        self._cluster_metadata = cluster.metadata\n        self._tablets_routing_v1 = cluster.control_connection._tablets_routing_v1\n        self._child_policy.populate(cluster, hosts)\n\n    def check_supported(self):\n        if not self._cluster_metadata.can_support_partitioner():\n            raise RuntimeError('%s cannot be used with the cluster partitioner (%s) because the relevant C extension for this driver was not compiled. See the installation instructions for details on building and installing the C extensions.' % (self.__class__.__name__, self._cluster_metadata.partitioner))\n\n    def distance(self, *args, **kwargs):\n        return self._child_policy.distance(*args, **kwargs)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        keyspace = query.keyspace if query and query.keyspace else working_keyspace\n        child = self._child_policy\n        if query is None or query.routing_key is None or keyspace is None:\n            for host in child.make_query_plan(keyspace, query):\n                yield host\n            return\n        replicas = []\n        if self._tablets_routing_v1:\n            tablet = self._cluster_metadata._tablets.get_tablet_for_key(keyspace, query.table, self._cluster_metadata.token_map.token_class.from_key(query.routing_key))\n            if tablet is not None:\n                replicas_mapped = set(map(lambda r: r[0], tablet.replicas))\n                child_plan = child.make_query_plan(keyspace, query)\n                replicas = [host for host in child_plan if host.host_id in replicas_mapped]\n        if not replicas:\n            replicas = self._cluster_metadata.get_replicas(keyspace, query.routing_key)\n        if self.shuffle_replicas:\n            shuffle(replicas)\n        for replica in replicas:\n            if replica.is_up and child.distance(replica) in [HostDistance.LOCAL, HostDistance.LOCAL_RACK]:\n                yield replica\n        for host in child.make_query_plan(keyspace, query):\n            if host not in replicas or child.distance(host) == HostDistance.REMOTE:\n                yield host\n\n    def on_up(self, *args, **kwargs):\n        return self._child_policy.on_up(*args, **kwargs)\n\n    def on_down(self, *args, **kwargs):\n        return self._child_policy.on_down(*args, **kwargs)\n\n    def on_add(self, *args, **kwargs):\n        return self._child_policy.on_add(*args, **kwargs)\n\n    def on_remove(self, *args, **kwargs):\n        return self._child_policy.on_remove(*args, **kwargs)\n\nclass WhiteListRoundRobinPolicy(RoundRobinPolicy):\n    \"\"\"\n    A subclass of :class:`.RoundRobinPolicy` which evenly\n    distributes queries across all nodes in the cluster,\n    regardless of what datacenter the nodes may be in, but\n    only if that node exists in the list of allowed nodes\n\n    This policy is addresses the issue described in\n    https://datastax-oss.atlassian.net/browse/JAVA-145\n    Where connection errors occur when connection\n    attempts are made to private IP addresses remotely\n    \"\"\"\n\n    def __init__(self, hosts):\n        \"\"\"\n        The `hosts` parameter should be a sequence of hosts to permit\n        connections to.\n        \"\"\"\n        self._allowed_hosts = tuple(hosts)\n        self._allowed_hosts_resolved = []\n        for h in self._allowed_hosts:\n            unix_socket_path = getattr(h, '_unix_socket_path', None)\n            if unix_socket_path:\n                self._allowed_hosts_resolved.append(unix_socket_path)\n            else:\n                self._allowed_hosts_resolved.extend([endpoint[4][0] for endpoint in socket.getaddrinfo(h, None, socket.AF_UNSPEC, socket.SOCK_STREAM)])\n        RoundRobinPolicy.__init__(self)\n\n    def populate(self, cluster, hosts):\n        self._live_hosts = frozenset((h for h in hosts if h.address in self._allowed_hosts_resolved))\n        if len(hosts) <= 1:\n            self._position = 0\n        else:\n            self._position = randint(0, len(hosts) - 1)\n\n    def distance(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            return HostDistance.LOCAL\n        else:\n            return HostDistance.IGNORED\n\n    def on_up(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            RoundRobinPolicy.on_up(self, host)\n\n    def on_add(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            RoundRobinPolicy.on_add(self, host)\n\nclass HostFilterPolicy(LoadBalancingPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` subclass configured with a child policy,\n    and a single-argument predicate. This policy defers to the child policy for\n    hosts where ``predicate(host)`` is truthy. Hosts for which\n    ``predicate(host)`` is falsy will be considered :attr:`.IGNORED`, and will\n    not be used in a query plan.\n\n    This can be used in the cases where you need a whitelist or blacklist\n    policy, e.g. to prepare for decommissioning nodes or for testing:\n\n    .. code-block:: python\n\n        def address_is_ignored(host):\n            return host.address in [ignored_address0, ignored_address1]\n\n        blacklist_filter_policy = HostFilterPolicy(\n            child_policy=RoundRobinPolicy(),\n            predicate=address_is_ignored\n        )\n\n        cluster = Cluster(\n            primary_host,\n            load_balancing_policy=blacklist_filter_policy,\n        )\n\n    See the note in the :meth:`.make_query_plan` documentation for a caveat on\n    how wrapping ordering polices (e.g. :class:`.RoundRobinPolicy`) may break\n    desirable properties of the wrapped policy.\n\n    Please note that whitelist and blacklist policies are not recommended for\n    general, day-to-day use. You probably want something like\n    :class:`.DCAwareRoundRobinPolicy`, which prefers a local DC but has\n    fallbacks, over a brute-force method like whitelisting or blacklisting.\n    \"\"\"\n\n    def __init__(self, child_policy, predicate):\n        \"\"\"\n        :param child_policy: an instantiated :class:`.LoadBalancingPolicy`\n                             that this one will defer to.\n        :param predicate: a one-parameter function that takes a :class:`.Host`.\n                          If it returns a falsy value, the :class:`.Host` will\n                          be :attr:`.IGNORED` and not returned in query plans.\n        \"\"\"\n        super(HostFilterPolicy, self).__init__()\n        self._child_policy = child_policy\n        self._predicate = predicate\n\n    def on_up(self, host, *args, **kwargs):\n        return self._child_policy.on_up(host, *args, **kwargs)\n\n    def on_down(self, host, *args, **kwargs):\n        return self._child_policy.on_down(host, *args, **kwargs)\n\n    def on_add(self, host, *args, **kwargs):\n        return self._child_policy.on_add(host, *args, **kwargs)\n\n    def on_remove(self, host, *args, **kwargs):\n        return self._child_policy.on_remove(host, *args, **kwargs)\n\n    @property\n    def predicate(self):\n        \"\"\"\n        A predicate, set on object initialization, that takes a :class:`.Host`\n        and returns a value. If the value is falsy, the :class:`.Host` is\n        :class:`~HostDistance.IGNORED`. If the value is truthy,\n        :class:`.HostFilterPolicy` defers to the child policy to determine the\n        host's distance.\n\n        This is a read-only value set in ``__init__``, implemented as a\n        ``property``.\n        \"\"\"\n        return self._predicate\n\n    def distance(self, host):\n        \"\"\"\n        Checks if ``predicate(host)``, then returns\n        :attr:`~HostDistance.IGNORED` if falsy, and defers to the child policy\n        otherwise.\n        \"\"\"\n        if self.predicate(host):\n            return self._child_policy.distance(host)\n        else:\n            return HostDistance.IGNORED\n\n    def populate(self, cluster, hosts):\n        self._child_policy.populate(cluster=cluster, hosts=hosts)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        \"\"\"\n        Defers to the child policy's\n        :meth:`.LoadBalancingPolicy.make_query_plan` and filters the results.\n\n        Note that this filtering may break desirable properties of the wrapped\n        policy in some cases. For instance, imagine if you configure this\n        policy to filter out ``host2``, and to wrap a round-robin policy that\n        rotates through three hosts in the order ``host1, host2, host3``,\n        ``host2, host3, host1``, ``host3, host1, host2``, repeating. This\n        policy will yield ``host1, host3``, ``host3, host1``, ``host3, host1``,\n        disproportionately favoring ``host3``.\n        \"\"\"\n        child_qp = self._child_policy.make_query_plan(working_keyspace=working_keyspace, query=query)\n        for host in child_qp:\n            if self.predicate(host):\n                yield host\n\n    def check_supported(self):\n        return self._child_policy.check_supported()\n\nclass ConvictionPolicy(object):\n    \"\"\"\n    A policy which decides when hosts should be considered down\n    based on the types of failures and the number of failures.\n\n    If custom behavior is needed, this class may be subclassed.\n    \"\"\"\n\n    def __init__(self, host):\n        \"\"\"\n        `host` is an instance of :class:`.Host`.\n        \"\"\"\n        self.host = host\n\n    def add_failure(self, connection_exc):\n        \"\"\"\n        Implementations should return :const:`True` if the host should be\n        convicted, :const:`False` otherwise.\n        \"\"\"\n        raise NotImplementedError()\n\n    def reset(self):\n        \"\"\"\n        Implementations should clear out any convictions or state regarding\n        the host.\n        \"\"\"\n        raise NotImplementedError()\n\nclass SimpleConvictionPolicy(ConvictionPolicy):\n    \"\"\"\n    The default implementation of :class:`ConvictionPolicy`,\n    which simply marks a host as down after the first failure\n    of any kind.\n    \"\"\"\n\n    def add_failure(self, connection_exc):\n        return not isinstance(connection_exc, OperationTimedOut)\n\n    def reset(self):\n        pass\n\nclass ReconnectionPolicy(object):\n    \"\"\"\n    This class and its subclasses govern how frequently an attempt is made\n    to reconnect to nodes that are marked as dead.\n\n    If custom behavior is needed, this class may be subclassed.\n    \"\"\"\n\n    def new_schedule(self):\n        \"\"\"\n        This should return a finite or infinite iterable of delays (each as a\n        floating point number of seconds) in-between each failed reconnection\n        attempt.  Note that if the iterable is finite, reconnection attempts\n        will cease once the iterable is exhausted.\n        \"\"\"\n        raise NotImplementedError()\n\nclass ConstantReconnectionPolicy(ReconnectionPolicy):\n    \"\"\"\n    A :class:`.ReconnectionPolicy` subclass which sleeps for a fixed delay\n    in-between each reconnection attempt.\n    \"\"\"\n\n    def __init__(self, delay, max_attempts=64):\n        \"\"\"\n        `delay` should be a floating point number of seconds to wait in-between\n        each attempt.\n\n        `max_attempts` should be a total number of attempts to be made before\n        giving up, or :const:`None` to continue reconnection attempts forever.\n        The default is 64.\n        \"\"\"\n        if delay < 0:\n            raise ValueError('delay must not be negative')\n        if max_attempts is not None and max_attempts < 0:\n            raise ValueError('max_attempts must not be negative')\n        self.delay = delay\n        self.max_attempts = max_attempts\n\n    def new_schedule(self):\n        if self.max_attempts:\n            return repeat(self.delay, self.max_attempts)\n        return repeat(self.delay)\n\nclass ExponentialReconnectionPolicy(ReconnectionPolicy):\n    \"\"\"\n    A :class:`.ReconnectionPolicy` subclass which exponentially increases\n    the length of the delay in-between each reconnection attempt up to\n    a set maximum delay.\n\n    A random amount of jitter (+/- 15%) will be added to the pure exponential\n    delay value to avoid the situations where many reconnection handlers are\n    trying to reconnect at exactly the same time.\n    \"\"\"\n\n    def __init__(self, base_delay, max_delay, max_attempts=64):\n        \"\"\"\n        `base_delay` and `max_delay` should be in floating point units of\n        seconds.\n\n        `max_attempts` should be a total number of attempts to be made before\n        giving up, or :const:`None` to continue reconnection attempts forever.\n        The default is 64.\n        \"\"\"\n        if base_delay < 0 or max_delay < 0:\n            raise ValueError('Delays may not be negative')\n        if max_delay < base_delay:\n            raise ValueError('Max delay must be greater than base delay')\n        if max_attempts is not None and max_attempts < 0:\n            raise ValueError('max_attempts must not be negative')\n        self.base_delay = base_delay\n        self.max_delay = max_delay\n        self.max_attempts = max_attempts\n\n    def new_schedule(self):\n        i, overflowed = (0, False)\n        while self.max_attempts is None or i < self.max_attempts:\n            if overflowed:\n                yield self.max_delay\n            else:\n                try:\n                    yield self._add_jitter(min(self.base_delay * 2 ** i, self.max_delay))\n                except OverflowError:\n                    overflowed = True\n                    yield self.max_delay\n            i += 1\n\n    def _add_jitter(self, value):\n        jitter = randint(85, 115)\n        delay = jitter * value / 100\n        return min(max(self.base_delay, delay), self.max_delay)\n\nclass RetryPolicy(object):\n    \"\"\"\n    A policy that describes whether to retry, rethrow, or ignore coordinator\n    timeout and unavailable failures. These are failures reported from the\n    server side. Timeouts are configured by\n    `settings in cassandra.yaml <https://github.com/apache/cassandra/blob/cassandra-2.1.4/conf/cassandra.yaml#L568-L584>`_.\n    Unavailable failures occur when the coordinator cannot achieve the consistency\n    level for a request. For further information see the method descriptions\n    below.\n\n    To specify a default retry policy, set the\n    :attr:`.Cluster.default_retry_policy` attribute to an instance of this\n    class or one of its subclasses.\n\n    To specify a retry policy per query, set the :attr:`.Statement.retry_policy`\n    attribute to an instance of this class or one of its subclasses.\n\n    If custom behavior is needed for retrying certain operations,\n    this class may be subclassed.\n    \"\"\"\n    RETRY = 0\n    '\\n    This should be returned from the below methods if the operation\\n    should be retried on the same connection.\\n    '\n    RETHROW = 1\n    '\\n    This should be returned from the below methods if the failure\\n    should be propagated and no more retries attempted.\\n    '\n    IGNORE = 2\n    '\\n    This should be returned from the below methods if the failure\\n    should be ignored but no more retries should be attempted.\\n    '\n    RETRY_NEXT_HOST = 3\n    '\\n    This should be returned from the below methods if the operation\\n    should be retried on another connection.\\n    '\n\n    def on_read_timeout(self, query, consistency, required_responses, received_responses, data_retrieved, retry_num):\n        \"\"\"\n        This is called when a read operation times out from the coordinator's\n        perspective (i.e. a replica did not respond to the coordinator in time).\n        It should return a tuple with two items: one of the class enums (such\n        as :attr:`.RETRY`) and a :class:`.ConsistencyLevel` to retry the\n        operation at or :const:`None` to keep the same consistency level.\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        The `required_responses` and `received_responses` parameters describe\n        how many replicas needed to respond to meet the requested consistency\n        level and how many actually did respond before the coordinator timed\n        out the request. `data_retrieved` is a boolean indicating whether\n        any of those responses contained data (as opposed to just a digest).\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, operations will be retried at most once, and only if\n        a sufficient number of replicas responded (with data digests).\n        \"\"\"\n        if retry_num != 0:\n            return (self.RETHROW, None)\n        elif received_responses >= required_responses and (not data_retrieved):\n            return (self.RETRY, consistency)\n        else:\n            return (self.RETHROW, None)\n\n    def on_write_timeout(self, query, consistency, write_type, required_responses, received_responses, retry_num):\n        \"\"\"\n        This is called when a write operation times out from the coordinator's\n        perspective (i.e. a replica did not respond to the coordinator in time).\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `write_type` is one of the :class:`.WriteType` enums describing the\n        type of write operation.\n\n        The `required_responses` and `received_responses` parameters describe\n        how many replicas needed to acknowledge the write to meet the requested\n        consistency level and how many replicas actually did acknowledge the\n        write before the coordinator timed out the request.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, failed write operations will retried at most once, and\n        they will only be retried if the `write_type` was\n        :attr:`~.WriteType.BATCH_LOG`.\n        \"\"\"\n        if retry_num != 0:\n            return (self.RETHROW, None)\n        elif write_type == WriteType.BATCH_LOG:\n            return (self.RETRY, consistency)\n        else:\n            return (self.RETHROW, None)\n\n    def on_unavailable(self, query, consistency, required_replicas, alive_replicas, retry_num):\n        \"\"\"\n        This is called when the coordinator node determines that a read or\n        write operation cannot be successful because the number of live\n        replicas are too low to meet the requested :class:`.ConsistencyLevel`.\n        This means that the read or write operation was never forwarded to\n        any replicas.\n\n        `query` is the :class:`.Statement` that failed.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `required_replicas` is the number of replicas that would have needed to\n        acknowledge the operation to meet the requested consistency level.\n        `alive_replicas` is the number of replicas that the coordinator\n        considered alive at the time of the request.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, if this is the first retry, it triggers a retry on the next\n        host in the query plan with the same consistency level. If this is not the\n        first retry, no retries will be attempted and the error will be re-raised.\n        \"\"\"\n        return (self.RETRY_NEXT_HOST, None) if retry_num == 0 else (self.RETHROW, None)\n\n    def on_request_error(self, query, consistency, error, retry_num):\n        \"\"\"\n        This is called when an unexpected error happens. This can be in the\n        following situations:\n\n        * On a connection error\n        * On server errors: overloaded, isBootstrapping, serverError, etc.\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `error` the instance of the exception.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, it triggers a retry on the next host in the query plan\n        with the same consistency level.\n        \"\"\"\n        return (self.RETRY_NEXT_HOST, None)\n\nclass FallthroughRetryPolicy(RetryPolicy):\n    \"\"\"\n    A retry policy that never retries and always propagates failures to\n    the application.\n    \"\"\"\n\n    def on_read_timeout(self, *args, **kwargs):\n        return (self.RETHROW, None)\n\n    def on_write_timeout(self, *args, **kwargs):\n        return (self.RETHROW, None)\n\n    def on_unavailable(self, *args, **kwargs):\n        return (self.RETHROW, None)\n\n    def on_request_error(self, *args, **kwargs):\n        return (self.RETHROW, None)\n\nclass DowngradingConsistencyRetryPolicy(RetryPolicy):\n    \"\"\"\n    *Deprecated:* This retry policy will be removed in the next major release.\n\n    A retry policy that sometimes retries with a lower consistency level than\n    the one initially requested.\n\n    **BEWARE**: This policy may retry queries using a lower consistency\n    level than the one initially requested. By doing so, it may break\n    consistency guarantees. In other words, if you use this retry policy,\n    there are cases (documented below) where a read at :attr:`~.QUORUM`\n    *may not* see a preceding write at :attr:`~.QUORUM`. Do not use this\n    policy unless you have understood the cases where this can happen and\n    are ok with that. It is also recommended to subclass this class so\n    that queries that required a consistency level downgrade can be\n    recorded (so that repairs can be made later, etc).\n\n    This policy implements the same retries as :class:`.RetryPolicy`,\n    but on top of that, it also retries in the following cases:\n\n    * On a read timeout: if the number of replicas that responded is\n      greater than one but lower than is required by the requested\n      consistency level, the operation is retried at a lower consistency\n      level.\n    * On a write timeout: if the operation is an :attr:`~.UNLOGGED_BATCH`\n      and at least one replica acknowledged the write, the operation is\n      retried at a lower consistency level.  Furthermore, for other\n      write types, if at least one replica acknowledged the write, the\n      timeout is ignored.\n    * On an unavailable exception: if at least one replica is alive, the\n      operation is retried at a lower consistency level.\n\n    The reasoning behind this retry policy is as follows: if, based\n    on the information the Cassandra coordinator node returns, retrying the\n    operation with the initially requested consistency has a chance to\n    succeed, do it. Otherwise, if based on that information we know the\n    initially requested consistency level cannot be achieved currently, then:\n\n    * For writes, ignore the exception (thus silently failing the\n      consistency requirement) if we know the write has been persisted on at\n      least one replica.\n    * For reads, try reading at a lower consistency level (thus silently\n      failing the consistency requirement).\n\n    In other words, this policy implements the idea that if the requested\n    consistency level cannot be achieved, the next best thing for writes is\n    to make sure the data is persisted, and that reading something is better\n    than reading nothing, even if there is a risk of reading stale data.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(DowngradingConsistencyRetryPolicy, self).__init__(*args, **kwargs)\n        warnings.warn('DowngradingConsistencyRetryPolicy is deprecated and will be removed in the next major release.', DeprecationWarning)\n\n    def _pick_consistency(self, num_responses):\n        if num_responses >= 3:\n            return (self.RETRY, ConsistencyLevel.THREE)\n        elif num_responses >= 2:\n            return (self.RETRY, ConsistencyLevel.TWO)\n        elif num_responses >= 1:\n            return (self.RETRY, ConsistencyLevel.ONE)\n        else:\n            return (self.RETHROW, None)\n\n    def on_read_timeout(self, query, consistency, required_responses, received_responses, data_retrieved, retry_num):\n        if retry_num != 0:\n            return (self.RETHROW, None)\n        elif ConsistencyLevel.is_serial(consistency):\n            return (self.RETHROW, None)\n        elif received_responses < required_responses:\n            return self._pick_consistency(received_responses)\n        elif not data_retrieved:\n            return (self.RETRY, consistency)\n        else:\n            return (self.RETHROW, None)\n\n    def on_write_timeout(self, query, consistency, write_type, required_responses, received_responses, retry_num):\n        if retry_num != 0:\n            return (self.RETHROW, None)\n        if write_type in (WriteType.SIMPLE, WriteType.BATCH, WriteType.COUNTER):\n            if received_responses > 0:\n                return (self.IGNORE, None)\n            else:\n                return (self.RETHROW, None)\n        elif write_type == WriteType.UNLOGGED_BATCH:\n            return self._pick_consistency(received_responses)\n        elif write_type == WriteType.BATCH_LOG:\n            return (self.RETRY, consistency)\n        return (self.RETHROW, None)\n\n    def on_unavailable(self, query, consistency, required_replicas, alive_replicas, retry_num):\n        if retry_num != 0:\n            return (self.RETHROW, None)\n        elif ConsistencyLevel.is_serial(consistency):\n            return (self.RETRY_NEXT_HOST, None)\n        else:\n            return self._pick_consistency(alive_replicas)\n\nclass ExponentialBackoffRetryPolicy(RetryPolicy):\n    \"\"\"\n    A policy that do retries with exponential backoff\n    \"\"\"\n\n    def __init__(self, max_num_retries: float, min_interval: float=0.1, max_interval: float=10.0, *args, **kwargs):\n        \"\"\"\n        `max_num_retries` counts how many times the operation would be retried,\n        `min_interval` is the initial time in seconds to wait before first retry\n        `max_interval` is the maximum time to wait between retries\n        \"\"\"\n        self.min_interval = min_interval\n        self.max_num_retries = max_num_retries\n        self.max_interval = max_interval\n        super(ExponentialBackoffRetryPolicy).__init__(*args, **kwargs)\n\n    def _calculate_backoff(self, attempt: int):\n        delay = min(self.max_interval, self.min_interval * 2 ** attempt)\n        delay += random.random() * self.min_interval - self.min_interval / 2\n        return delay\n\n    def on_read_timeout(self, query, consistency, required_responses, received_responses, data_retrieved, retry_num):\n        if retry_num < self.max_num_retries and received_responses >= required_responses and (not data_retrieved):\n            return (self.RETRY, consistency, self._calculate_backoff(retry_num))\n        else:\n            return (self.RETHROW, None, None)\n\n    def on_write_timeout(self, query, consistency, write_type, required_responses, received_responses, retry_num):\n        if retry_num < self.max_num_retries and write_type == WriteType.BATCH_LOG:\n            return (self.RETRY, consistency, self._calculate_backoff(retry_num))\n        else:\n            return (self.RETHROW, None, None)\n\n    def on_unavailable(self, query, consistency, required_replicas, alive_replicas, retry_num):\n        if retry_num < self.max_num_retries:\n            return (self.RETRY_NEXT_HOST, None, self._calculate_backoff(retry_num))\n        else:\n            return (self.RETHROW, None, None)\n\n    def on_request_error(self, query, consistency, error, retry_num):\n        if retry_num < self.max_num_retries:\n            return (self.RETRY_NEXT_HOST, None, self._calculate_backoff(retry_num))\n        else:\n            return (self.RETHROW, None, None)\n\nclass AddressTranslator(object):\n    \"\"\"\n    Interface for translating cluster-defined endpoints.\n\n    The driver discovers nodes using server metadata and topology change events. Normally,\n    the endpoint defined by the server is the right way to connect to a node. In some environments,\n    these addresses may not be reachable, or not preferred (public vs. private IPs in cloud environments,\n    suboptimal routing, etc). This interface allows for translating from server defined endpoints to\n    preferred addresses for driver connections.\n\n    *Note:* :attr:`~Cluster.contact_points` provided while creating the :class:`~.Cluster` instance are not\n    translated using this mechanism -- only addresses received from Cassandra nodes are.\n    \"\"\"\n\n    def translate(self, addr):\n        \"\"\"\n        Accepts the node ip address, and returns a translated address to be used connecting to this node.\n        \"\"\"\n        raise NotImplementedError()\n\nclass IdentityTranslator(AddressTranslator):\n    \"\"\"\n    Returns the endpoint with no translation\n    \"\"\"\n\n    def translate(self, addr):\n        return addr\n\nclass EC2MultiRegionTranslator(AddressTranslator):\n    \"\"\"\n    Resolves private ips of the hosts in the same datacenter as the client, and public ips of hosts in other datacenters.\n    \"\"\"\n\n    def translate(self, addr):\n        \"\"\"\n        Reverse DNS the public broadcast_address, then lookup that hostname to get the AWS-resolved IP, which\n        will point to the private IP address within the same datacenter.\n        \"\"\"\n        family = socket.getaddrinfo(addr, 0, socket.AF_UNSPEC, socket.SOCK_STREAM)[0][0]\n        host = socket.getfqdn(addr)\n        for a in socket.getaddrinfo(host, 0, family, socket.SOCK_STREAM):\n            try:\n                return a[4][0]\n            except Exception:\n                pass\n        return addr\n\nclass SpeculativeExecutionPolicy(object):\n    \"\"\"\n    Interface for specifying speculative execution plans\n    \"\"\"\n\n    def new_plan(self, keyspace, statement):\n        \"\"\"\n        Returns\n\n        :param keyspace:\n        :param statement:\n        :return:\n        \"\"\"\n        raise NotImplementedError()\n\nclass SpeculativeExecutionPlan(object):\n\n    def next_execution(self, host):\n        raise NotImplementedError()\n\nclass NoSpeculativeExecutionPlan(SpeculativeExecutionPlan):\n\n    def next_execution(self, host):\n        return -1\n\nclass NoSpeculativeExecutionPolicy(SpeculativeExecutionPolicy):\n\n    def new_plan(self, keyspace, statement):\n        return NoSpeculativeExecutionPlan()\n\nclass ConstantSpeculativeExecutionPolicy(SpeculativeExecutionPolicy):\n    \"\"\"\n    A speculative execution policy that sends a new query every X seconds (**delay**) for a maximum of Y attempts (**max_attempts**).\n    \"\"\"\n\n    def __init__(self, delay, max_attempts):\n        self.delay = delay\n        self.max_attempts = max_attempts\n\n    class ConstantSpeculativeExecutionPlan(SpeculativeExecutionPlan):\n\n        def __init__(self, delay, max_attempts):\n            self.delay = delay\n            self.remaining = max_attempts\n\n        def next_execution(self, host):\n            if self.remaining > 0:\n                self.remaining -= 1\n                return self.delay\n            else:\n                return -1\n\n    def new_plan(self, keyspace, statement):\n        return self.ConstantSpeculativeExecutionPlan(self.delay, self.max_attempts)\n\nclass WrapperPolicy(LoadBalancingPolicy):\n\n    def __init__(self, child_policy):\n        self._child_policy = child_policy\n\n    def distance(self, *args, **kwargs):\n        return self._child_policy.distance(*args, **kwargs)\n\n    def populate(self, cluster, hosts):\n        self._child_policy.populate(cluster, hosts)\n\n    def on_up(self, *args, **kwargs):\n        return self._child_policy.on_up(*args, **kwargs)\n\n    def on_down(self, *args, **kwargs):\n        return self._child_policy.on_down(*args, **kwargs)\n\n    def on_add(self, *args, **kwargs):\n        return self._child_policy.on_add(*args, **kwargs)\n\n    def on_remove(self, *args, **kwargs):\n        return self._child_policy.on_remove(*args, **kwargs)\n\nclass DefaultLoadBalancingPolicy(WrapperPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` wrapper that adds the ability to target a specific host first.\n\n    If no host is set on the query, the child policy's query plan will be used as is.\n    \"\"\"\n    _cluster_metadata = None\n\n    def populate(self, cluster, hosts):\n        self._cluster_metadata = cluster.metadata\n        self._child_policy.populate(cluster, hosts)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        if query and query.keyspace:\n            keyspace = query.keyspace\n        else:\n            keyspace = working_keyspace\n        addr = getattr(query, 'target_host', None) if query else None\n        target_host = self._cluster_metadata.get_host(addr)\n        child = self._child_policy\n        if target_host and target_host.is_up:\n            yield target_host\n            for h in child.make_query_plan(keyspace, query):\n                if h != target_host:\n                    yield h\n        else:\n            for h in child.make_query_plan(keyspace, query):\n                yield h\n\nclass DSELoadBalancingPolicy(DefaultLoadBalancingPolicy):\n    \"\"\"\n    *Deprecated:* This will be removed in the next major release,\n    consider using :class:`.DefaultLoadBalancingPolicy`.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(DSELoadBalancingPolicy, self).__init__(*args, **kwargs)\n        warnings.warn('DSELoadBalancingPolicy will be removed in 4.0. Consider using DefaultLoadBalancingPolicy.', DeprecationWarning)\n\nclass NeverRetryPolicy(RetryPolicy):\n\n    def _rethrow(self, *args, **kwargs):\n        return (self.RETHROW, None)\n    on_read_timeout = _rethrow\n    on_write_timeout = _rethrow\n    on_unavailable = _rethrow\nColDesc = namedtuple('ColDesc', ['ks', 'table', 'col'])\n\nclass ColumnEncryptionPolicy(object):\n    \"\"\"\n    A policy enabling (mostly) transparent encryption and decryption of data before it is\n    sent to the cluster.\n\n    Key materials and other configurations are specified on a per-column basis.  This policy can\n    then be used by driver structures which are aware of the underlying columns involved in their\n    work.  In practice this includes the following cases:\n\n    * Prepared statements - data for columns specified by the cluster's policy will be transparently\n      encrypted before they are sent\n    * Rows returned from any query - data for columns specified by the cluster's policy will be\n      transparently decrypted before they are returned to the user\n\n    To enable this functionality, create an instance of this class (or more likely a subclass)\n    before creating a cluster.  This policy should then be configured and supplied to the Cluster\n    at creation time via the :attr:`.Cluster.column_encryption_policy` attribute.\n    \"\"\"\n\n    def encrypt(self, coldesc, obj_bytes):\n        \"\"\"\n        Encrypt the specified bytes using the cryptography materials for the specified column.\n        Largely used internally, although this could also be used to encrypt values supplied\n        to non-prepared statements in a way that is consistent with this policy.\n        \"\"\"\n        raise NotImplementedError()\n\n    def decrypt(self, coldesc, encrypted_bytes):\n        \"\"\"\n        Decrypt the specified (encrypted) bytes using the cryptography materials for the\n        specified column.  Used internally; could be used externally as well but there's\n        not currently an obvious use case.\n        \"\"\"\n        raise NotImplementedError()\n\n    def add_column(self, coldesc, key):\n        \"\"\"\n        Provide cryptography materials to be used when encrypted and/or decrypting data\n        for the specified column.\n        \"\"\"\n        raise NotImplementedError()\n\n    def contains_column(self, coldesc):\n        \"\"\"\n        Predicate to determine if a specific column is supported by this policy.\n        Currently only used internally.\n        \"\"\"\n        raise NotImplementedError()\n\n    def encode_and_encrypt(self, coldesc, obj):\n        \"\"\"\n        Helper function to enable use of this policy on simple (i.e. non-prepared)\n        statements.\n        \"\"\"\n        raise NotImplementedError()"
  },
  "call_tree": {
    "tests/unit/test_row_factories.py:TestNamedTupleFactory:test_creation_no_warning_on_short_column_list": {
      "cassandra/query.py:named_tuple_factory": {
        "cassandra/query.py:_clean_column_name": {}
      }
    },
    "tests/unit/test_row_factories.py:TestNamedTupleFactory:test_creation_warning_on_long_column_list": {
      "cassandra/query.py:named_tuple_factory": {
        "cassandra/query.py:_clean_column_name": {}
      }
    },
    "/mnt/sfs_turbo/yaxindu/MAC/PackageDataset/app/scylla_driver-image/scylla_driver/tests/unit/test_policies.py:TestRackOrDCAwareRoundRobinPolicy:test_with_remotes": {
      "cassandra/policies.py:DCAwareRoundRobinPolicy:DCAwareRoundRobinPolicy": {},
      "cassandra/policies.py:RackAwareRoundRobinPolicy:RackAwareRoundRobinPolicy": {}
    },
    "/mnt/sfs_turbo/yaxindu/MAC/PackageDataset/app/scylla_driver-image/scylla_driver/tests/unit/test_policies.py:TestRackOrDCAwareRoundRobinPolicy:test_get_distance": {
      "cassandra/policies.py:DCAwareRoundRobinPolicy:DCAwareRoundRobinPolicy": {},
      "cassandra/policies.py:RackAwareRoundRobinPolicy:RackAwareRoundRobinPolicy": {}
    },
    "/mnt/sfs_turbo/yaxindu/MAC/PackageDataset/app/scylla_driver-image/scylla_driver/tests/integration/long/test_loadbalancingpolicies.py:LoadBalancingPolicyTests:test_token_aware_is_used_by_default": {
      "cassandra/policies.py:TokenAwarePolicy:TokenAwarePolicy": {},
      "cassandra/policies.py:DCAwareRoundRobinPolicy:DCAwareRoundRobinPolicy": {}
    },
    "/mnt/sfs_turbo/yaxindu/MAC/PackageDataset/app/scylla_driver-image/scylla_driver/tests/integration/advanced/graph/test_graph.py:GraphTimeoutTests:test_server_timeout_less_then_request": {
      "cassandra/__init__.py:InvalidRequest:InvalidRequest": {},
      "cassandra/__init__.py:OperationTimedOut:OperationTimedOut": {}
    },
    "/mnt/sfs_turbo/yaxindu/MAC/PackageDataset/app/scylla_driver-image/scylla_driver/tests/integration/advanced/graph/test_graph.py:GraphProfileTests:test_graph_profile": {
      "cassandra/__init__.py:InvalidRequest:InvalidRequest": {},
      "cassandra/__init__.py:OperationTimedOut:OperationTimedOut": {}
    }
  }
}