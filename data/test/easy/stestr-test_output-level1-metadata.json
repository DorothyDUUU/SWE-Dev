{
  "dir_path": "/app/stestr",
  "package_name": "stestr",
  "sample_name": "stestr-test_output",
  "src_dir": "stestr/",
  "test_dir": "stestr/tests/",
  "test_file": "modified_testcases/test_output.py",
  "test_code": "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nimport io\n\nfrom stestr import output\nfrom stestr.tests import base\n\n\nclass TestOutput(base.TestCase):\n    def test_output_table(self):\n        table = [\n            [\"Header 1\", \"Header 2\", \"Header 999\"],\n            [1, \"0000000002\", \"foo\"],\n            [\"bar\", 6, \"This is a content.\"],\n        ]\n        expected = (\n            \"Header 1  Header 2    Header 999\\n\"\n            \"--------  ----------  ------------------\\n\"\n            \"1         0000000002  foo\\n\"\n            \"bar       6           This is a content.\\n\"\n        )\n        with io.StringIO() as f:\n            output.output_table(table, f)\n            actual = f.getvalue()\n            self.assertEqual(expected, actual)\n\n    def test_output_tests(self):\n        class Test:\n            def __init__(self, i):\n                self.i = i\n\n            def id(self):\n                return self.i\n\n        tests = [Test(\"a\"), Test(\"b\"), Test(\"foo\")]\n        expected = \"a\\nb\\nfoo\\n\"\n        with io.StringIO() as f:\n            output.output_tests(tests, f)\n            actual = f.getvalue()\n            self.assertEqual(expected, actual)\n\n    def test_output_summary_passed(self):\n        expected = (\n            \"Ran 10 (+5) tests in 1.100s (+0.100s)\\n\"\n            \"PASSED (id=99 (+1), id=100 (+2))\\n\"\n        )\n        with io.StringIO() as f:\n            output.output_summary(\n                successful=True,\n                tests=10,\n                tests_delta=5,\n                time=1.1,\n                time_delta=0.1,\n                values=[(\"id\", 99, 1), (\"id\", \"100\", 2)],\n                output=f,\n            )\n            actual = f.getvalue()\n            self.assertEqual(expected, actual)\n\n    def test_output_summary_failed(self):\n        expected = (\n            \"Ran 10 (+5) tests in 1.100s (+0.100s)\\n\"\n            \"FAILED (id=99 (+1), id=100 (+2))\\n\"\n        )\n        with io.StringIO() as f:\n            output.output_summary(\n                successful=False,\n                tests=10,\n                tests_delta=5,\n                time=1.1,\n                time_delta=0.1,\n                values=[(\"id\", 99, 1), (\"id\", \"100\", 2)],\n                output=f,\n            )\n            actual = f.getvalue()\n            self.assertEqual(expected, actual)\n",
  "GT_file_code": {
    "stestr/output.py": "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nimport io\nimport sys\n\nimport subunit\nimport testtools\n\n\ndef output_table(table, output=sys.stdout):\n    \"\"\"Display a table of information.\n\n    :param table: A list of sets representing each row in the table. Each\n        element in the set represents a column in the table.\n    :param output: The output file object to write the table to. By default\n        this is sys.stdout\n    \"\"\"\n    # stringify\n    contents = []\n    for row in table:\n        new_row = []\n        for column in row:\n            new_row.append(str(column))\n        contents.append(new_row)\n    if not contents:\n        return\n    widths = [0] * len(contents[0])\n    for row in contents:\n        for idx, column in enumerate(row):\n            if widths[idx] < len(column):\n                widths[idx] = len(column)\n    # Show a row\n    outputs = []\n\n    def show_row(row):\n        for idx, column in enumerate(row):\n            outputs.append(column)\n            if idx == len(row) - 1:\n                outputs.append(\"\\n\")\n                return\n            # spacers for the next column\n            outputs.append(\" \" * (widths[idx] - len(column)))\n            outputs.append(\"  \")\n\n    show_row(contents[0])\n    # title spacer\n    for idx, width in enumerate(widths):\n        outputs.append(\"-\" * width)\n        if idx == len(widths) - 1:\n            outputs.append(\"\\n\")\n            continue\n        outputs.append(\"  \")\n    for row in contents[1:]:\n        show_row(row)\n    output.write(\"\".join(outputs))\n\n\ndef output_tests(tests, output=sys.stdout):\n    \"\"\"Display a list of tests.\n\n    :param tests: A list of test objects to output\n    :param output: The output file object to write the list to. By default\n        this is sys.stdout\n    \"\"\"\n\n    for test in tests:\n        id_str = test.id()\n        output.write(str(id_str))\n        output.write(\"\\n\")\n\n\ndef make_result(get_id, output=sys.stdout):\n    serializer = subunit.StreamResultToBytes(output)\n    # By pass user transforms - just forward it all,\n    result = serializer\n    # and interpret everything as success.\n    summary = testtools.StreamSummary()\n    summary.startTestRun()\n    summary.stopTestRun()\n    return result, summary\n\n\ndef output_summary(\n    successful, tests, tests_delta, time, time_delta, values, output=sys.stdout\n):\n    \"\"\"Display a summary view for the test run.\n\n    :param bool successful: Was the test run successful\n    :param int tests: The number of tests that ran\n    :param int tests_delta: The change in the number of tests that ran since\n        the last run\n    :param float time: The number of seconds that it took for the run to\n        execute\n    :param float time_delta: The change in run time since the last run\n    :param values: A list of sets that are used for a breakdown of statuses\n        other than success. Each set is in the format:\n        (status, number of tests, change in number of tests).\n    :param output: The output file object to use. This defaults to stdout\n    \"\"\"\n    summary = []\n    a = summary.append\n    if tests:\n        a(\"Ran {}\".format(tests))\n        if tests_delta:\n            a(\" (%+d)\" % (tests_delta,))\n        a(\" tests\")\n    if time:\n        if not summary:\n            a(\"Ran tests\")\n        a(\" in {:0.3f}s\".format(time))\n        if time_delta:\n            a(\" ({:+0.3f}s)\".format(time_delta))\n    if summary:\n        a(\"\\n\")\n    if successful:\n        a(\"PASSED\")\n    else:\n        a(\"FAILED\")\n    if values:\n        a(\" (\")\n        values_strings = []\n        for name, value, delta in values:\n            value_str = \"{}={}\".format(name, value)\n            if delta:\n                value_str += \" (%+d)\" % (delta,)\n            values_strings.append(value_str)\n        a(\", \".join(values_strings))\n        a(\")\")\n    output.write(\"\".join(summary) + \"\\n\")\n\n\ndef output_stream(stream, output=sys.stdout):\n    _binary_stdout = subunit.make_stream_binary(output)\n    contents = stream.read(65536)\n    assert type(contents) is bytes, \"Bad stream contents %r\" % type(contents)\n    # If there are unflushed bytes in the text wrapper, we need to sync..\n    output.flush()\n    while contents:\n        _binary_stdout.write(contents)\n        contents = stream.read(65536)\n    _binary_stdout.flush()\n\n\nclass ReturnCodeToSubunit:\n    \"\"\"Converts a process return code to a subunit error on the process stdout.\n\n    The ReturnCodeToSubunit object behaves as a read-only stream, supplying\n    the read, readline and readlines methods. If the process exits non-zero a\n    synthetic test is added to the output, making the error accessible to\n    subunit stream consumers. If the process closes its stdout and then does\n    not terminate, reading from the ReturnCodeToSubunit stream will hang.\n\n    :param process: A subprocess.Popen object that is\n        generating subunit.\n    \"\"\"\n\n    def __init__(self, process):\n        \"\"\"Adapt a process to a readable stream.\"\"\"\n        self.proc = process\n        self.done = False\n        self.source = self.proc.stdout\n        self.lastoutput = bytes((b\"\\n\")[0])\n\n    def __del__(self):\n        self.proc.wait()\n\n    def _append_return_code_as_test(self):\n        if self.done is True:\n            return\n        self.source = io.BytesIO()\n        returncode = self.proc.wait()\n        if returncode != 0:\n            if self.lastoutput != bytes((b\"\\n\")[0]):\n                # Subunit V1 is line orientated, it has to start on a fresh\n                # line. V2 needs to start on any fresh utf8 character border\n                # - which is not guaranteed in an arbitrary stream endpoint, so\n                # injecting a \\n gives us such a guarantee.\n                self.source.write(bytes(\"\\n\"))\n            stream = subunit.StreamResultToBytes(self.source)\n            stream.status(\n                test_id=\"process-returncode\",\n                test_status=\"fail\",\n                file_name=\"traceback\",\n                mime_type=\"text/plain;charset=utf8\",\n                file_bytes=(\"returncode %d\" % returncode).encode(\"utf8\"),\n            )\n        self.source.seek(0)\n        self.done = True\n\n    def read(self, count=-1):\n        if count == 0:\n            return \"\"\n        result = self.source.read(count)\n        if result:\n            self.lastoutput = result[-1]\n            return result\n        self._append_return_code_as_test()\n        return self.source.read(count)\n\n    def readline(self):\n        result = self.source.readline()\n        if result:\n            self.lastoutput = result[-1]\n            return result\n        self._append_return_code_as_test()\n        return self.source.readline()\n\n    def readlines(self):\n        result = self.source.readlines()\n        if result:\n            self.lastoutput = result[-1][-1]\n        self._append_return_code_as_test()\n        result.extend(self.source.readlines())\n        return result\n"
  },
  "GT_src_dict": {
    "stestr/output.py": {
      "output_table": {
        "code": "def output_table(table, output=sys.stdout):\n    \"\"\"Display a formatted table of information to the specified output.\n\nParameters:\n- table (list of sets): A collection where each set represents a row in the table, with each element of the set corresponding to a column.\n- output (file-like object, optional): The output destination for the table. Defaults to sys.stdout, allowing the table to be printed to the console.\n\nReturns:\n- None: This function does not return any value; it writes the formatted table directly to the specified output.\n\nNotes:\n- The function converts elements to strings for display and calculates the necessary column widths to align the output.\n- It handles empty tables gracefully by returning early if no rows are provided.\n- Dependencies: The function relies on the sys module for the default output stream, which is set to sys.stdout.\"\"\"\n    'Display a table of information.\\n\\n    :param table: A list of sets representing each row in the table. Each\\n        element in the set represents a column in the table.\\n    :param output: The output file object to write the table to. By default\\n        this is sys.stdout\\n    '\n    contents = []\n    for row in table:\n        new_row = []\n        for column in row:\n            new_row.append(str(column))\n        contents.append(new_row)\n    if not contents:\n        return\n    widths = [0] * len(contents[0])\n    for row in contents:\n        for idx, column in enumerate(row):\n            if widths[idx] < len(column):\n                widths[idx] = len(column)\n    outputs = []\n\n    def show_row(row):\n        for idx, column in enumerate(row):\n            outputs.append(column)\n            if idx == len(row) - 1:\n                outputs.append('\\n')\n                return\n            outputs.append(' ' * (widths[idx] - len(column)))\n            outputs.append('  ')\n    show_row(contents[0])\n    for idx, width in enumerate(widths):\n        outputs.append('-' * width)\n        if idx == len(widths) - 1:\n            outputs.append('\\n')\n            continue\n        outputs.append('  ')\n    for row in contents[1:]:\n        show_row(row)\n    output.write(''.join(outputs))",
        "docstring": "Display a formatted table of information to the specified output.\n\nParameters:\n- table (list of sets): A collection where each set represents a row in the table, with each element of the set corresponding to a column.\n- output (file-like object, optional): The output destination for the table. Defaults to sys.stdout, allowing the table to be printed to the console.\n\nReturns:\n- None: This function does not return any value; it writes the formatted table directly to the specified output.\n\nNotes:\n- The function converts elements to strings for display and calculates the necessary column widths to align the output.\n- It handles empty tables gracefully by returning early if no rows are provided.\n- Dependencies: The function relies on the sys module for the default output stream, which is set to sys.stdout.",
        "signature": "def output_table(table, output=sys.stdout):",
        "type": "Function",
        "class_signature": null
      },
      "output_tests": {
        "code": "def output_tests(tests, output=sys.stdout):\n    \"\"\"Display a list of test identifiers.\n\n:param tests: A list of test objects whose IDs will be output. Each test object is expected to have a method `id()` that returns a string representation of the test's identifier.\n:param output: The output file object where the list of test IDs will be written. The default value is `sys.stdout`, allowing for easy output to the console.\n\nThe function iterates over the provided list of tests, retrieves the identifier for each test, and writes these identifiers to the specified output, separated by newlines. This function is part of a larger testing framework, interacting with classes like `subunit` and `testtools`, which facilitate the management and reporting of test results.\"\"\"\n    'Display a list of tests.\\n\\n    :param tests: A list of test objects to output\\n    :param output: The output file object to write the list to. By default\\n        this is sys.stdout\\n    '\n    for test in tests:\n        id_str = test.id()\n        output.write(str(id_str))\n        output.write('\\n')",
        "docstring": "Display a list of test identifiers.\n\n:param tests: A list of test objects whose IDs will be output. Each test object is expected to have a method `id()` that returns a string representation of the test's identifier.\n:param output: The output file object where the list of test IDs will be written. The default value is `sys.stdout`, allowing for easy output to the console.\n\nThe function iterates over the provided list of tests, retrieves the identifier for each test, and writes these identifiers to the specified output, separated by newlines. This function is part of a larger testing framework, interacting with classes like `subunit` and `testtools`, which facilitate the management and reporting of test results.",
        "signature": "def output_tests(tests, output=sys.stdout):",
        "type": "Function",
        "class_signature": null
      },
      "output_summary": {
        "code": "def output_summary(successful, tests, tests_delta, time, time_delta, values, output=sys.stdout):\n    \"\"\"Display a summary view for the test run, indicating the number of tests executed, their success status, and other relevant metrics.\n\nParameters:\n- successful (bool): Indicates if the test run was successful.\n- tests (int): The total number of tests that were executed.\n- tests_delta (int): The change in the number of tests since the last execution.\n- time (float): The duration (in seconds) that the test run took to complete.\n- time_delta (float): The change in duration since the last execution.\n- values (list of sets): A breakdown of statuses beyond success, where each set contains (status, number of tests, change in number of tests).\n- output (file object, default=sys.stdout): The output destination for the summary, defaulting to standard output.\n\nThis function aggregates the data and provides a formatted summary output to the specified output stream, consolidating various metrics for the test execution and leveraging the default behavior of Python's standard output for display.\"\"\"\n    'Display a summary view for the test run.\\n\\n    :param bool successful: Was the test run successful\\n    :param int tests: The number of tests that ran\\n    :param int tests_delta: The change in the number of tests that ran since\\n        the last run\\n    :param float time: The number of seconds that it took for the run to\\n        execute\\n    :param float time_delta: The change in run time since the last run\\n    :param values: A list of sets that are used for a breakdown of statuses\\n        other than success. Each set is in the format:\\n        (status, number of tests, change in number of tests).\\n    :param output: The output file object to use. This defaults to stdout\\n    '\n    summary = []\n    a = summary.append\n    if tests:\n        a('Ran {}'.format(tests))\n        if tests_delta:\n            a(' (%+d)' % (tests_delta,))\n        a(' tests')\n    if time:\n        if not summary:\n            a('Ran tests')\n        a(' in {:0.3f}s'.format(time))\n        if time_delta:\n            a(' ({:+0.3f}s)'.format(time_delta))\n    if summary:\n        a('\\n')\n    if successful:\n        a('PASSED')\n    else:\n        a('FAILED')\n    if values:\n        a(' (')\n        values_strings = []\n        for name, value, delta in values:\n            value_str = '{}={}'.format(name, value)\n            if delta:\n                value_str += ' (%+d)' % (delta,)\n            values_strings.append(value_str)\n        a(', '.join(values_strings))\n        a(')')\n    output.write(''.join(summary) + '\\n')",
        "docstring": "Display a summary view for the test run, indicating the number of tests executed, their success status, and other relevant metrics.\n\nParameters:\n- successful (bool): Indicates if the test run was successful.\n- tests (int): The total number of tests that were executed.\n- tests_delta (int): The change in the number of tests since the last execution.\n- time (float): The duration (in seconds) that the test run took to complete.\n- time_delta (float): The change in duration since the last execution.\n- values (list of sets): A breakdown of statuses beyond success, where each set contains (status, number of tests, change in number of tests).\n- output (file object, default=sys.stdout): The output destination for the summary, defaulting to standard output.\n\nThis function aggregates the data and provides a formatted summary output to the specified output stream, consolidating various metrics for the test execution and leveraging the default behavior of Python's standard output for display.",
        "signature": "def output_summary(successful, tests, tests_delta, time, time_delta, values, output=sys.stdout):",
        "type": "Function",
        "class_signature": null
      }
    }
  },
  "dependency_dict": {
    "stestr/output.py:output_table": {},
    "stestr/output.py:output_tests": {}
  },
  "PRD": "# PROJECT NAME: stestr-test_output\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 stestr/\n    \u2514\u2500\u2500 output.py\n        \u251c\u2500\u2500 output_summary\n        \u251c\u2500\u2500 output_table\n        \u2514\u2500\u2500 output_tests\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThis module is designed to handle the generation and output of structured data, providing a suite of functions for formatting and displaying tabular information, test results, and execution summaries. It enables rendering of tables, detailed test output, and execution summaries with performance metrics in a consistent and human-readable format, facilitating better readability and consumption of structured data in various contexts. The module simplifies the process of converting raw data into clear, formatted outputs, addressing the need for streamlined communication of test execution details and metrics. By abstracting output formatting, it reduces the burden on developers, ensuring consistent presentation across testing and reporting workflows.\n\n## FILE 1: stestr/output.py\n\n- FUNCTION NAME: output_tests\n  - SIGNATURE: def output_tests(tests, output=sys.stdout):\n  - DOCSTRING: \n```python\n\"\"\"\nDisplay a list of test identifiers.\n\n:param tests: A list of test objects whose IDs will be output. Each test object is expected to have a method `id()` that returns a string representation of the test's identifier.\n:param output: The output file object where the list of test IDs will be written. The default value is `sys.stdout`, allowing for easy output to the console.\n\nThe function iterates over the provided list of tests, retrieves the identifier for each test, and writes these identifiers to the specified output, separated by newlines. This function is part of a larger testing framework, interacting with classes like `subunit` and `testtools`, which facilitate the management and reporting of test results.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - modified_testcases/test_output.py:TestOutput:id\n\n- FUNCTION NAME: output_table\n  - SIGNATURE: def output_table(table, output=sys.stdout):\n  - DOCSTRING: \n```python\n\"\"\"\nDisplay a formatted table of information to the specified output.\n\nParameters:\n- table (list of sets): A collection where each set represents a row in the table, with each element of the set corresponding to a column.\n- output (file-like object, optional): The output destination for the table. Defaults to sys.stdout, allowing the table to be printed to the console.\n\nReturns:\n- None: This function does not return any value; it writes the formatted table directly to the specified output.\n\nNotes:\n- The function converts elements to strings for display and calculates the necessary column widths to align the output.\n- It handles empty tables gracefully by returning early if no rows are provided.\n- Dependencies: The function relies on the sys module for the default output stream, which is set to sys.stdout.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - stestr/output.py:show_row\n\n- FUNCTION NAME: output_summary\n  - SIGNATURE: def output_summary(successful, tests, tests_delta, time, time_delta, values, output=sys.stdout):\n  - DOCSTRING: \n```python\n\"\"\"\nDisplay a summary view for the test run, indicating the number of tests executed, their success status, and other relevant metrics.\n\nParameters:\n- successful (bool): Indicates if the test run was successful.\n- tests (int): The total number of tests that were executed.\n- tests_delta (int): The change in the number of tests since the last execution.\n- time (float): The duration (in seconds) that the test run took to complete.\n- time_delta (float): The change in duration since the last execution.\n- values (list of sets): A breakdown of statuses beyond success, where each set contains (status, number of tests, change in number of tests).\n- output (file object, default=sys.stdout): The output destination for the summary, defaulting to standard output.\n\nThis function aggregates the data and provides a formatted summary output to the specified output stream, consolidating various metrics for the test execution and leveraging the default behavior of Python's standard output for display.\n\"\"\"\n```\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "stestr/output.py": "import io\nimport sys\nimport subunit\nimport testtools\n\ndef make_result(get_id, output=sys.stdout):\n    serializer = subunit.StreamResultToBytes(output)\n    result = serializer\n    summary = testtools.StreamSummary()\n    summary.startTestRun()\n    summary.stopTestRun()\n    return (result, summary)\n\ndef output_stream(stream, output=sys.stdout):\n    _binary_stdout = subunit.make_stream_binary(output)\n    contents = stream.read(65536)\n    assert type(contents) is bytes, 'Bad stream contents %r' % type(contents)\n    output.flush()\n    while contents:\n        _binary_stdout.write(contents)\n        contents = stream.read(65536)\n    _binary_stdout.flush()\n\nclass ReturnCodeToSubunit:\n    \"\"\"Converts a process return code to a subunit error on the process stdout.\n\n    The ReturnCodeToSubunit object behaves as a read-only stream, supplying\n    the read, readline and readlines methods. If the process exits non-zero a\n    synthetic test is added to the output, making the error accessible to\n    subunit stream consumers. If the process closes its stdout and then does\n    not terminate, reading from the ReturnCodeToSubunit stream will hang.\n\n    :param process: A subprocess.Popen object that is\n        generating subunit.\n    \"\"\"\n\n    def __init__(self, process):\n        \"\"\"Adapt a process to a readable stream.\"\"\"\n        self.proc = process\n        self.done = False\n        self.source = self.proc.stdout\n        self.lastoutput = bytes(b'\\n'[0])\n\n    def __del__(self):\n        self.proc.wait()\n\n    def _append_return_code_as_test(self):\n        if self.done is True:\n            return\n        self.source = io.BytesIO()\n        returncode = self.proc.wait()\n        if returncode != 0:\n            if self.lastoutput != bytes(b'\\n'[0]):\n                self.source.write(bytes('\\n'))\n            stream = subunit.StreamResultToBytes(self.source)\n            stream.status(test_id='process-returncode', test_status='fail', file_name='traceback', mime_type='text/plain;charset=utf8', file_bytes=('returncode %d' % returncode).encode('utf8'))\n        self.source.seek(0)\n        self.done = True\n\n    def read(self, count=-1):\n        if count == 0:\n            return ''\n        result = self.source.read(count)\n        if result:\n            self.lastoutput = result[-1]\n            return result\n        self._append_return_code_as_test()\n        return self.source.read(count)\n\n    def readline(self):\n        result = self.source.readline()\n        if result:\n            self.lastoutput = result[-1]\n            return result\n        self._append_return_code_as_test()\n        return self.source.readline()\n\n    def readlines(self):\n        result = self.source.readlines()\n        if result:\n            self.lastoutput = result[-1][-1]\n        self._append_return_code_as_test()\n        result.extend(self.source.readlines())\n        return result"
  },
  "call_tree": {
    "modified_testcases/test_output.py:TestOutput:test_output_summary_failed": {
      "stestr/output.py:output_summary": {}
    },
    "modified_testcases/test_output.py:TestOutput:test_output_summary_passed": {
      "stestr/output.py:output_summary": {}
    },
    "modified_testcases/test_output.py:TestOutput:test_output_table": {
      "stestr/output.py:output_table": {
        "stestr/output.py:show_row": {}
      }
    },
    "modified_testcases/test_output.py:TestOutput:test_output_tests": {
      "modified_testcases/test_output.py:TestOutput:Test": {},
      "modified_testcases/test_output.py:TestOutput:__init__": {},
      "stestr/output.py:output_tests": {
        "modified_testcases/test_output.py:TestOutput:id": {}
      }
    }
  }
}