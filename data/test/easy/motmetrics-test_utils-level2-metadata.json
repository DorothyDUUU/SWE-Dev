{
  "dir_path": "/app/motmetrics",
  "package_name": "motmetrics",
  "sample_name": "motmetrics-test_utils",
  "src_dir": "motmetrics/",
  "test_dir": "motmetrics/tests/",
  "test_file": "modified_testcases/test_utils.py",
  "test_code": "# py-motmetrics - Metrics for multiple object tracker (MOT) benchmarking.\n# https://github.com/cheind/py-motmetrics/\n#\n# MIT License\n# Copyright (c) 2017-2020 Christoph Heindl, Jack Valmadre and others.\n# See LICENSE file for terms.\n\n\"\"\"Tests accumulation of events using utility functions.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\n\nimport numpy as np\nimport pandas as pd\n\nimport motmetrics as mm\n\n\ndef test_annotations_xor_predictions_present():\n    \"\"\"Tests frames that contain only annotations or predictions.\"\"\"\n    _ = None\n    anno_tracks = {\n        1: [0, 2, 4, 6, _, _, _],\n        2: [_, _, 0, 2, 4, _, _],\n    }\n    pred_tracks = {\n        1: [_, _, 3, 5, 7, 7, 7],\n    }\n    anno = _tracks_to_dataframe(anno_tracks)\n    pred = _tracks_to_dataframe(pred_tracks)\n    acc = mm.utils.compare_to_groundtruth(anno, pred, 'euclidean', distfields=['Position'], distth=2)\n    mh = mm.metrics.create()\n    metrics = mh.compute(acc, return_dataframe=False, metrics=[\n        'num_objects', 'num_predictions', 'num_unique_objects',\n    ])\n    np.testing.assert_equal(metrics['num_objects'], 7)\n    np.testing.assert_equal(metrics['num_predictions'], 5)\n    np.testing.assert_equal(metrics['num_unique_objects'], 2)\n\n\ndef _tracks_to_dataframe(tracks):\n    rows = []\n    for track_id, track in tracks.items():\n        for frame_id, position in zip(itertools.count(1), track):\n            if position is None:\n                continue\n            rows.append({\n                'FrameId': frame_id,\n                'Id': track_id,\n                'Position': position,\n            })\n    return pd.DataFrame(rows).set_index(['FrameId', 'Id'])\n",
  "GT_file_code": {
    "motmetrics/mot.py": "# py-motmetrics - Metrics for multiple object tracker (MOT) benchmarking.\n# https://github.com/cheind/py-motmetrics/\n#\n# MIT License\n# Copyright (c) 2017-2020 Christoph Heindl, Jack Valmadre and others.\n# See LICENSE file for terms.\n\n\"\"\"Accumulate tracking events frame by frame.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import OrderedDict\nimport itertools\n\nimport numpy as np\nimport pandas as pd\n\nfrom motmetrics.lap import linear_sum_assignment\n\n_INDEX_FIELDS = ['FrameId', 'Event']\n_EVENT_FIELDS = ['Type', 'OId', 'HId', 'D']\n\n\nclass MOTAccumulator(object):\n    \"\"\"Manage tracking events.\n\n    This class computes per-frame tracking events from a given set of object / hypothesis\n    ids and pairwise distances. Indended usage\n\n        import motmetrics as mm\n        acc = mm.MOTAccumulator()\n        acc.update(['a', 'b'], [0, 1, 2], dists, frameid=0)\n        ...\n        acc.update(['d'], [6,10], other_dists, frameid=76)\n        summary = mm.metrics.summarize(acc)\n        print(mm.io.render_summary(summary))\n\n    Update is called once per frame and takes objects / hypothesis ids and a pairwise distance\n    matrix between those (see distances module for support). Per frame max(len(objects), len(hypothesis))\n    events are generated. Each event type is one of the following\n        - `'MATCH'` a match between a object and hypothesis was found\n        - `'SWITCH'` a match between a object and hypothesis was found but differs from previous assignment (hypothesisid != previous)\n        - `'MISS'` no match for an object was found\n        - `'FP'` no match for an hypothesis was found (spurious detections)\n        - `'RAW'` events corresponding to raw input\n        - `'TRANSFER'` a match between a object and hypothesis was found but differs from previous assignment (objectid != previous)\n        - `'ASCEND'` a match between a object and hypothesis was found but differs from previous assignment  (hypothesisid is new)\n        - `'MIGRATE'` a match between a object and hypothesis was found but differs from previous assignment  (objectid is new)\n\n    Events are tracked in a pandas Dataframe. The dataframe is hierarchically indexed by (`FrameId`, `EventId`),\n    where `FrameId` is either provided during the call to `update` or auto-incremented when `auto_id` is set\n    true during construction of MOTAccumulator. `EventId` is auto-incremented. The dataframe has the following\n    columns\n        - `Type` one of `('MATCH', 'SWITCH', 'MISS', 'FP', 'RAW')`\n        - `OId` object id or np.nan when `'FP'` or `'RAW'` and object is not present\n        - `HId` hypothesis id or np.nan when `'MISS'` or `'RAW'` and hypothesis is not present\n        - `D` distance or np.nan when `'FP'` or `'MISS'` or `'RAW'` and either object/hypothesis is absent\n\n    From the events and associated fields the entire tracking history can be recovered. Once the accumulator\n    has been populated with per-frame data use `metrics.summarize` to compute statistics. See `metrics.compute_metrics`\n    for a list of metrics computed.\n\n    References\n    ----------\n    1. Bernardin, Keni, and Rainer Stiefelhagen. \"Evaluating multiple object tracking performance: the CLEAR MOT metrics.\"\n    EURASIP Journal on Image and Video Processing 2008.1 (2008): 1-10.\n    2. Milan, Anton, et al. \"Mot16: A benchmark for multi-object tracking.\" arXiv preprint arXiv:1603.00831 (2016).\n    3. Li, Yuan, Chang Huang, and Ram Nevatia. \"Learning to associate: Hybridboosted multi-target tracker for crowded scene.\"\n    Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 2009.\n    \"\"\"\n\n    def __init__(self, auto_id=False, max_switch_time=float('inf')):\n        \"\"\"Create a MOTAccumulator.\n\n        Params\n        ------\n        auto_id : bool, optional\n            Whether or not frame indices are auto-incremented or provided upon\n            updating. Defaults to false. Not specifying a frame-id when this value\n            is true results in an error. Specifying a frame-id when this value is\n            false also results in an error.\n\n        max_switch_time : scalar, optional\n            Allows specifying an upper bound on the timespan an unobserved but\n            tracked object is allowed to generate track switch events. Useful if groundtruth\n            objects leaving the field of view keep their ID when they reappear,\n            but your tracker is not capable of recognizing this (resulting in\n            track switch events). The default is that there is no upper bound\n            on the timespan. In units of frame timestamps. When using auto_id\n            in units of count.\n        \"\"\"\n\n        # Parameters of the accumulator.\n        self.auto_id = auto_id\n        self.max_switch_time = max_switch_time\n\n        # Accumulator state.\n        self._events = None\n        self._indices = None\n        self.m = None\n        self.res_m = None\n        self.last_occurrence = None\n        self.last_match = None\n        self.hypHistory = None\n        self.dirty_events = None\n        self.cached_events_df = None\n        self.last_update_frameid = None\n\n        self.reset()\n\n    def reset(self):\n        \"\"\"Reset the accumulator to empty state.\"\"\"\n\n        self._events = {field: [] for field in _EVENT_FIELDS}\n        self._indices = {field: [] for field in _INDEX_FIELDS}\n        self.m = {}  # Pairings up to current timestamp\n        self.res_m = {}  # Result pairings up to now\n        self.last_occurrence = {}  # Tracks most recent occurance of object\n        self.last_match = {}  # Tracks most recent match of object\n        self.hypHistory = {}\n        self.dirty_events = True\n        self.cached_events_df = None\n        self.last_update_frameid = None\n\n    def _append_to_indices(self, frameid, eid):\n        self._indices['FrameId'].append(frameid)\n        self._indices['Event'].append(eid)\n\n    def _append_to_events(self, typestr, oid, hid, distance):\n        self._events['Type'].append(typestr)\n        self._events['OId'].append(oid)\n        self._events['HId'].append(hid)\n        self._events['D'].append(distance)\n\n    def update(self, oids, hids, dists, frameid=None, vf='', similartiy_matrix=None, th=None):\n        \"\"\"Updates the accumulator with frame specific objects/detections.\n\n        This method generates events based on the following algorithm [1]:\n        1. Try to carry forward already established tracks. If any paired object / hypothesis\n        from previous timestamps are still visible in the current frame, create a 'MATCH'\n        event between them.\n        2. For the remaining constellations minimize the total object / hypothesis distance\n        error (Kuhn-Munkres algorithm). If a correspondence made contradicts a previous\n        match create a 'SWITCH' else a 'MATCH' event.\n        3. Create 'MISS' events for all remaining unassigned objects.\n        4. Create 'FP' events for all remaining unassigned hypotheses.\n\n        Params\n        ------\n        oids : N array\n            Array of object ids.\n        hids : M array\n            Array of hypothesis ids.\n        dists: NxM array\n            Distance matrix. np.nan values to signal do-not-pair constellations.\n            See `distances` module for support methods.\n\n        Kwargs\n        ------\n        frameId : id\n            Unique frame id. Optional when MOTAccumulator.auto_id is specified during\n            construction.\n        vf: file to log details\n        Returns\n        -------\n        frame_events : pd.DataFrame\n            Dataframe containing generated events\n\n        References\n        ----------\n        1. Bernardin, Keni, and Rainer Stiefelhagen. \"Evaluating multiple object tracking performance: the CLEAR MOT metrics.\"\n        EURASIP Journal on Image and Video Processing 2008.1 (2008): 1-10.\n        \"\"\"\n        # pylint: disable=too-many-locals, too-many-statements\n\n        self.dirty_events = True\n        oids = np.asarray(oids)\n        oids_masked = np.zeros_like(oids, dtype=np.bool_)\n        hids = np.asarray(hids)\n        hids_masked = np.zeros_like(hids, dtype=np.bool_)\n        dists = np.atleast_2d(dists).astype(float).reshape(oids.shape[0], hids.shape[0]).copy()\n\n        if frameid is None:\n            assert self.auto_id, 'auto-id is not enabled'\n            if len(self._indices['FrameId']) > 0:\n                frameid = self._indices['FrameId'][-1] + 1\n            else:\n                frameid = 0\n        else:\n            assert not self.auto_id, 'Cannot provide frame id when auto-id is enabled'\n\n        eid = itertools.count()\n\n        # 0. Record raw events\n\n        no = len(oids)\n        nh = len(hids)\n\n        # Add a RAW event simply to ensure the frame is counted.\n        self._append_to_indices(frameid, next(eid))\n        self._append_to_events('RAW', np.nan, np.nan, np.nan)\n\n        # Postcompute the distance matrix if necessary. (e.g., HOTA)\n        cost_for_matching = dists.copy()\n        if similartiy_matrix is not None and th is not None:\n            dists = 1 - similartiy_matrix\n            dists = np.where(similartiy_matrix < th - np.finfo(\"float\").eps, np.nan, dists)\n\n        # There must be at least one RAW event per object and hypothesis.\n        # Record all finite distances as RAW events.\n        valid_i, valid_j = np.where(np.isfinite(dists))\n        valid_dists = dists[valid_i, valid_j]\n        for i, j, dist_ij in zip(valid_i, valid_j, valid_dists):\n            self._append_to_indices(frameid, next(eid))\n            self._append_to_events('RAW', oids[i], hids[j], dist_ij)\n        # Add a RAW event for objects and hypotheses that were present but did\n        # not overlap with anything.\n        used_i = np.unique(valid_i)\n        used_j = np.unique(valid_j)\n        unused_i = np.setdiff1d(np.arange(no), used_i)\n        unused_j = np.setdiff1d(np.arange(nh), used_j)\n        for oid in oids[unused_i]:\n            self._append_to_indices(frameid, next(eid))\n            self._append_to_events('RAW', oid, np.nan, np.nan)\n        for hid in hids[unused_j]:\n            self._append_to_indices(frameid, next(eid))\n            self._append_to_events('RAW', np.nan, hid, np.nan)\n\n        if oids.size * hids.size > 0:\n            # 1. Try to re-establish tracks from correspondences in last update\n            #    ignore this if post processing is performed (e.g., HOTA)\n            if similartiy_matrix is None or th is None:\n                for i in range(oids.shape[0]):\n                    # No need to check oids_masked[i] here.\n                    if not (oids[i] in self.m and self.last_match[oids[i]] == self.last_update_frameid):\n                        continue\n\n                    hprev = self.m[oids[i]]\n                    j, = np.where(~hids_masked & (hids == hprev))\n                    if j.shape[0] == 0:\n                        continue\n                    j = j[0]\n\n                    if np.isfinite(dists[i, j]):\n                        o = oids[i]\n                        h = hids[j]\n                        oids_masked[i] = True\n                        hids_masked[j] = True\n                        self.m[oids[i]] = hids[j]\n\n                        self._append_to_indices(frameid, next(eid))\n                        self._append_to_events('MATCH', oids[i], hids[j], dists[i, j])\n                        self.last_match[o] = frameid\n                        self.hypHistory[h] = frameid\n\n            # 2. Try to remaining objects/hypotheses\n            dists[oids_masked, :] = np.nan\n            dists[:, hids_masked] = np.nan\n\n            rids, cids = linear_sum_assignment(cost_for_matching)\n\n            for i, j in zip(rids, cids):\n                if not np.isfinite(dists[i, j]):\n                    continue\n\n                o = oids[i]\n                h = hids[j]\n                ######################################################################\n                # todo - fixed a bug:\n                # is_switch = (o in self.m and\n                #              self.m[o] != h and\n                #              abs(frameid - self.last_occurrence[o]) <= self.max_switch_time)\n                switch_condition = (\n                    o in self.m and\n                    self.m[o] != h and\n                    o in self.last_occurrence and  # Ensure the object ID 'o' is initialized in last_occurrence\n                    abs(frameid - self.last_occurrence[o]) <= self.max_switch_time\n                )\n                is_switch = switch_condition\n                ######################################################################\n                cat1 = 'SWITCH' if is_switch else 'MATCH'\n                if cat1 == 'SWITCH':\n                    if h not in self.hypHistory:\n                        subcat = 'ASCEND'\n                        self._append_to_indices(frameid, next(eid))\n                        self._append_to_events(subcat, oids[i], hids[j], dists[i, j])\n                # ignore the last condition temporarily\n                is_transfer = (h in self.res_m and\n                               self.res_m[h] != o)\n                # is_transfer = (h in self.res_m and\n                #                self.res_m[h] != o and\n                #                abs(frameid - self.last_occurrence[o]) <= self.max_switch_time)\n                cat2 = 'TRANSFER' if is_transfer else 'MATCH'\n                if cat2 == 'TRANSFER':\n                    if o not in self.last_match:\n                        subcat = 'MIGRATE'\n                        self._append_to_indices(frameid, next(eid))\n                        self._append_to_events(subcat, oids[i], hids[j], dists[i, j])\n                    self._append_to_indices(frameid, next(eid))\n                    self._append_to_events(cat2, oids[i], hids[j], dists[i, j])\n                if vf != '' and (cat1 != 'MATCH' or cat2 != 'MATCH'):\n                    if cat1 == 'SWITCH':\n                        vf.write('%s %d %d %d %d %d\\n' % (subcat[:2], o, self.last_match[o], self.m[o], frameid, h))\n                    if cat2 == 'TRANSFER':\n                        vf.write('%s %d %d %d %d %d\\n' % (subcat[:2], h, self.hypHistory[h], self.res_m[h], frameid, o))\n                self.hypHistory[h] = frameid\n                self.last_match[o] = frameid\n                self._append_to_indices(frameid, next(eid))\n                self._append_to_events(cat1, oids[i], hids[j], dists[i, j])\n                oids_masked[i] = True\n                hids_masked[j] = True\n                self.m[o] = h\n                self.res_m[h] = o\n\n        # 3. All remaining objects are missed\n        for o in oids[~oids_masked]:\n            self._append_to_indices(frameid, next(eid))\n            self._append_to_events('MISS', o, np.nan, np.nan)\n            if vf != '':\n                vf.write('FN %d %d\\n' % (frameid, o))\n\n        # 4. All remaining hypotheses are false alarms\n        for h in hids[~hids_masked]:\n            self._append_to_indices(frameid, next(eid))\n            self._append_to_events('FP', np.nan, h, np.nan)\n            if vf != '':\n                vf.write('FP %d %d\\n' % (frameid, h))\n\n        # 5. Update occurance state\n        for o in oids:\n            self.last_occurrence[o] = frameid\n\n        self.last_update_frameid = frameid\n\n        return frameid\n\n    @property\n    def events(self):\n        if self.dirty_events:\n            self.cached_events_df = MOTAccumulator.new_event_dataframe_with_data(self._indices, self._events)\n            self.dirty_events = False\n        return self.cached_events_df\n\n    @property\n    def mot_events(self):\n        df = self.events\n        return df[df.Type != 'RAW']\n\n    @staticmethod\n    def new_event_dataframe():\n        \"\"\"Create a new DataFrame for event tracking.\"\"\"\n        idx = pd.MultiIndex(levels=[[], []], codes=[[], []], names=['FrameId', 'Event'])\n        cats = pd.Categorical([], categories=['RAW', 'FP', 'MISS', 'SWITCH', 'MATCH', 'TRANSFER', 'ASCEND', 'MIGRATE'])\n        df = pd.DataFrame(\n            OrderedDict([\n                ('Type', pd.Series(cats)),          # Type of event. One of FP (false positive), MISS, SWITCH, MATCH\n                ('OId', pd.Series(dtype=float)),      # Object ID or -1 if FP. Using float as missing values will be converted to NaN anyways.\n                ('HId', pd.Series(dtype=float)),      # Hypothesis ID or NaN if MISS. Using float as missing values will be converted to NaN anyways.\n                ('D', pd.Series(dtype=float)),      # Distance or NaN when FP or MISS\n            ]),\n            index=idx\n        )\n        return df\n\n    @staticmethod\n    def new_event_dataframe_with_data(indices, events):\n        \"\"\"Create a new DataFrame filled with data.\n\n        Params\n        ------\n        indices: dict\n            dict of lists with fields 'FrameId' and 'Event'\n        events: dict\n            dict of lists with fields 'Type', 'OId', 'HId', 'D'\n        \"\"\"\n\n        if len(events) == 0:\n            return MOTAccumulator.new_event_dataframe()\n\n        raw_type = pd.Categorical(\n            events['Type'],\n            categories=['RAW', 'FP', 'MISS', 'SWITCH', 'MATCH', 'TRANSFER', 'ASCEND', 'MIGRATE'],\n            ordered=False)\n        series = [\n            pd.Series(raw_type, name='Type'),\n            pd.Series(events['OId'], dtype=float, name='OId'),\n            pd.Series(events['HId'], dtype=float, name='HId'),\n            pd.Series(events['D'], dtype=float, name='D')\n        ]\n\n        idx = pd.MultiIndex.from_arrays(\n            [indices[field] for field in _INDEX_FIELDS],\n            names=_INDEX_FIELDS)\n        df = pd.concat(series, axis=1)\n        df.index = idx\n        return df\n\n    @staticmethod\n    def merge_analysis(anas, infomap):\n        # pylint: disable=missing-function-docstring\n        res = {'hyp': {}, 'obj': {}}\n        mapp = {'hyp': 'hid_map', 'obj': 'oid_map'}\n        for ana, infom in zip(anas, infomap):\n            if ana is None:\n                return None\n            for t in ana.keys():\n                which = mapp[t]\n                if np.nan in infom[which]:\n                    res[t][int(infom[which][np.nan])] = 0\n                if 'nan' in infom[which]:\n                    res[t][int(infom[which]['nan'])] = 0\n                for _id, cnt in ana[t].items():\n                    if _id not in infom[which]:\n                        _id = str(_id)\n                    res[t][int(infom[which][_id])] = cnt\n        return res\n\n    @staticmethod\n    def merge_event_dataframes(dfs, update_frame_indices=True, update_oids=True, update_hids=True, return_mappings=False):\n        \"\"\"Merge dataframes.\n\n        Params\n        ------\n        dfs : list of pandas.DataFrame or MotAccumulator\n            A list of event containers to merge\n\n        Kwargs\n        ------\n        update_frame_indices : boolean, optional\n            Ensure that frame indices are unique in the merged container\n        update_oids : boolean, unique\n            Ensure that object ids are unique in the merged container\n        update_hids : boolean, unique\n            Ensure that hypothesis ids are unique in the merged container\n        return_mappings : boolean, unique\n            Whether or not to return mapping information\n\n        Returns\n        -------\n        df : pandas.DataFrame\n            Merged event data frame\n        \"\"\"\n\n        mapping_infos = []\n        new_oid = itertools.count()\n        new_hid = itertools.count()\n\n        r = MOTAccumulator.new_event_dataframe()\n        for df in dfs:\n\n            if isinstance(df, MOTAccumulator):\n                df = df.events\n\n            copy = df.copy()\n            infos = {}\n\n            # Update index\n            if update_frame_indices:\n                # pylint: disable=cell-var-from-loop\n                next_frame_id = max(r.index.get_level_values(0).max() + 1, r.index.get_level_values(0).unique().shape[0])\n                if np.isnan(next_frame_id):\n                    next_frame_id = 0\n                if not copy.index.empty:\n                    copy.index = copy.index.map(lambda x: (x[0] + next_frame_id, x[1]))\n                infos['frame_offset'] = next_frame_id\n\n            # Update object / hypothesis ids\n            if update_oids:\n                # pylint: disable=cell-var-from-loop\n                oid_map = dict([oid, str(next(new_oid))] for oid in copy['OId'].dropna().unique())\n                copy['OId'] = copy['OId'].map(lambda x: oid_map[x], na_action='ignore')\n                infos['oid_map'] = oid_map\n\n            if update_hids:\n                # pylint: disable=cell-var-from-loop\n                hid_map = dict([hid, str(next(new_hid))] for hid in copy['HId'].dropna().unique())\n                copy['HId'] = copy['HId'].map(lambda x: hid_map[x], na_action='ignore')\n                infos['hid_map'] = hid_map\n\n            r = pd.concat([r, copy])\n            mapping_infos.append(infos)\n\n        if return_mappings:\n            return r, mapping_infos\n        else:\n            return r\n",
    "motmetrics/metrics.py": "# py-motmetrics - Metrics for multiple object tracker (MOT) benchmarking.\n# https://github.com/cheind/py-motmetrics/\n#\n# MIT License\n# Copyright (c) 2017-2020 Christoph Heindl, Jack Valmadre and others.\n# See LICENSE file for terms.\n\n\"\"\"Obtain metrics from event logs.\"\"\"\n\n# pylint: disable=redefined-outer-name\n\nfrom __future__ import absolute_import, division, print_function\n\nimport inspect\nimport logging\nimport time\nfrom collections import OrderedDict\n\nimport numpy as np\nimport pandas as pd\n\nfrom motmetrics import math_util\nfrom motmetrics.lap import linear_sum_assignment\nfrom motmetrics.mot import MOTAccumulator\n\ntry:\n    _getargspec = inspect.getfullargspec\nexcept AttributeError:\n    _getargspec = inspect.getargspec\n\n\nclass MetricsHost:\n    \"\"\"Keeps track of metrics and intra metric dependencies.\"\"\"\n\n    def __init__(self):\n        self.metrics = OrderedDict()\n\n    def register(\n        self,\n        fnc,\n        deps=\"auto\",\n        name=None,\n        helpstr=None,\n        formatter=None,\n        fnc_m=None,\n        deps_m=\"auto\",\n    ):\n        \"\"\"Register a new metric.\n\n        Params\n        ------\n        fnc : Function\n            Function that computes the metric to be registered. The number of arguments\n            is 1 + N, where N is the number of dependencies of the metric to be registered.\n            The order of the argument passed is `df, result_dep1, result_dep2, ...`.\n\n        Kwargs\n        ------\n        deps : string, list of strings or None, optional\n            The dependencies of this metric. Each dependency is evaluated and the result\n            is passed as argument to `fnc` as described above. If None is specified, the\n            function does not have any dependencies. If a list of strings is given, dependencies\n            for these metric strings are registered. If 'auto' is passed, the dependencies\n            are deduced from argument inspection of the method. For this to work the argument\n            names have to be equal to the intended dependencies.\n        name : string or None, optional\n            Name identifier of this metric. If None is passed the name is deduced from\n            function inspection.\n        helpstr : string or None, optional\n            A description of what the metric computes. If no help message is given it\n            is deduced from the docstring of the function.\n        formatter: Format object, optional\n            An optional default formatter when rendering metric results as string. I.e to\n            render the result `0.35` as `35%` one would pass `{:.2%}.format`\n        fnc_m : Function or None, optional\n            Function that merges metric results. The number of arguments\n            is 1 + N, where N is the number of dependencies of the metric to be registered.\n            The order of the argument passed is `df, result_dep1, result_dep2, ...`.\n        \"\"\"\n\n        assert fnc is not None, \"No function given for metric {}\".format(name)\n\n        if deps is None:\n            deps = []\n        elif deps == \"auto\":\n            if _getargspec(fnc).defaults is not None:\n                k = -len(_getargspec(fnc).defaults)\n            else:\n                k = len(_getargspec(fnc).args)\n            deps = _getargspec(fnc).args[1:k]  # assumes dataframe as first argument\n\n        if name is None:\n            name = (\n                fnc.__name__\n            )  # Relies on meaningful function names, i.e don't use for lambdas\n\n        if helpstr is None:\n            helpstr = inspect.getdoc(fnc) if inspect.getdoc(fnc) else \"No description.\"\n            helpstr = \" \".join(helpstr.split())\n        if fnc_m is None and name + \"_m\" in globals():\n            fnc_m = globals()[name + \"_m\"]\n        if fnc_m is not None:\n            if deps_m is None:\n                deps_m = []\n            elif deps_m == \"auto\":\n                if _getargspec(fnc_m).defaults is not None:\n                    k = -len(_getargspec(fnc_m).defaults)\n                else:\n                    k = len(_getargspec(fnc_m).args)\n                deps_m = _getargspec(fnc_m).args[\n                    1:k\n                ]  # assumes dataframe as first argument\n        else:\n            deps_m = None\n\n        self.metrics[name] = {\n            \"name\": name,\n            \"fnc\": fnc,\n            \"fnc_m\": fnc_m,\n            \"deps\": deps,\n            \"deps_m\": deps_m,\n            \"help\": helpstr,\n            \"formatter\": formatter,\n        }\n\n    @property\n    def names(self):\n        \"\"\"Returns the name identifiers of all registered metrics.\"\"\"\n        return [v[\"name\"] for v in self.metrics.values()]\n\n    @property\n    def formatters(self):\n        \"\"\"Returns the formatters for all metrics that have associated formatters.\"\"\"\n        return {\n            k: v[\"formatter\"]\n            for k, v in self.metrics.items()\n            if v[\"formatter\"] is not None\n        }\n\n    def list_metrics(self, include_deps=False):\n        \"\"\"Returns a dataframe containing names, descriptions and optionally dependencies for each metric.\"\"\"\n        cols = [\"Name\", \"Description\", \"Dependencies\"]\n        if include_deps:\n            data = [(m[\"name\"], m[\"help\"], m[\"deps\"]) for m in self.metrics.values()]\n        else:\n            data = [(m[\"name\"], m[\"help\"]) for m in self.metrics.values()]\n            cols = cols[:-1]\n\n        return pd.DataFrame(data, columns=cols)\n\n    def list_metrics_markdown(self, include_deps=False):\n        \"\"\"Returns a markdown ready version of `list_metrics`.\"\"\"\n        df = self.list_metrics(include_deps=include_deps)\n        fmt = [\":---\" for i in range(len(df.columns))]\n        df_fmt = pd.DataFrame([fmt], columns=df.columns)\n        df_formatted = pd.concat([df_fmt, df])\n        return df_formatted.to_csv(sep=\"|\", index=False)\n\n    def compute(\n        self,\n        df,\n        ana=None,\n        metrics=None,\n        return_dataframe=True,\n        return_cached=False,\n        name=None,\n    ):\n        \"\"\"Compute metrics on the dataframe / accumulator.\n\n        Params\n        ------\n        df : MOTAccumulator or pandas.DataFrame\n            The dataframe to compute the metrics on\n\n        Kwargs\n        ------\n        ana: dict or None, optional\n            To cache results for fast computation.\n        metrics : string, list of string or None, optional\n            The identifiers of the metrics to be computed. This method will only\n            compute the minimal set of necessary metrics to fullfill the request.\n            If None is passed all registered metrics are computed.\n        return_dataframe : bool, optional\n            Return the result as pandas.DataFrame (default) or dict.\n        return_cached : bool, optional\n           If true all intermediate metrics required to compute the desired metrics are returned as well.\n        name : string, optional\n            When returning a pandas.DataFrame this is the index of the row containing\n            the computed metric values.\n        \"\"\"\n\n        if isinstance(df, MOTAccumulator):\n            df = df.events\n\n        if metrics is None:\n            metrics = motchallenge_metrics\n        elif isinstance(metrics, str):\n            metrics = [metrics]\n\n        df_map = events_to_df_map(df)\n\n        cache = {}\n        options = {\"ana\": ana}\n        for mname in metrics:\n            cache[mname] = self._compute(\n                df_map, mname, cache, options, parent=\"summarize\"\n            )\n\n        if name is None:\n            name = 0\n\n        if return_cached:\n            data = cache\n        else:\n            data = OrderedDict([(k, cache[k]) for k in metrics])\n\n        ret = pd.DataFrame(data, index=[name]) if return_dataframe else data\n        return ret\n\n    def compute_overall(\n        self,\n        partials,\n        metrics=None,\n        return_dataframe=True,\n        return_cached=False,\n        name=None,\n    ):\n        \"\"\"Compute overall metrics based on multiple results.\n\n        Params\n        ------\n        partials : list of metric results to combine overall\n\n        Kwargs\n        ------\n        metrics : string, list of string or None, optional\n            The identifiers of the metrics to be computed. This method will only\n            compute the minimal set of necessary metrics to fullfill the request.\n            If None is passed all registered metrics are computed.\n        return_dataframe : bool, optional\n            Return the result as pandas.DataFrame (default) or dict.\n        return_cached : bool, optional\n           If true all intermediate metrics required to compute the desired metrics are returned as well.\n        name : string, optional\n            When returning a pandas.DataFrame this is the index of the row containing\n            the computed metric values.\n\n        Returns\n        -------\n        df : pandas.DataFrame\n            A datafrom containing the metrics in columns and names in rows.\n        \"\"\"\n        if metrics is None:\n            metrics = motchallenge_metrics\n        elif isinstance(metrics, str):\n            metrics = [metrics]\n        cache = {}\n\n        for mname in metrics:\n            cache[mname] = self._compute_overall(\n                partials, mname, cache, parent=\"summarize\"\n            )\n\n        if name is None:\n            name = 0\n        if return_cached:\n            data = cache\n        else:\n            data = OrderedDict([(k, cache[k]) for k in metrics])\n        return pd.DataFrame(data, index=[name]) if return_dataframe else data\n\n    def compute_many(\n        self, dfs, anas=None, metrics=None, names=None, generate_overall=False\n    ):\n        \"\"\"Compute metrics on multiple dataframe / accumulators.\n\n        Params\n        ------\n        dfs : list of MOTAccumulator or list of pandas.DataFrame\n            The data to compute metrics on.\n\n        Kwargs\n        ------\n        anas: dict or None, optional\n            To cache results for fast computation.\n        metrics : string, list of string or None, optional\n            The identifiers of the metrics to be computed. This method will only\n            compute the minimal set of necessary metrics to fullfill the request.\n            If None is passed all registered metrics are computed.\n        names : list of string, optional\n            The names of individual rows in the resulting dataframe.\n        generate_overall : boolean, optional\n            If true resulting dataframe will contain a summary row that is computed\n            using the same metrics over an accumulator that is the concatentation of\n            all input containers. In creating this temporary accumulator, care is taken\n            to offset frame indices avoid object id collisions.\n\n        Returns\n        -------\n        df : pandas.DataFrame\n            A datafrom containing the metrics in columns and names in rows.\n        \"\"\"\n        if metrics is None:\n            metrics = motchallenge_metrics\n        elif isinstance(metrics, str):\n            metrics = [metrics]\n\n        assert names is None or len(names) == len(dfs)\n        st = time.time()\n        if names is None:\n            names = list(range(len(dfs)))\n        if anas is None:\n            anas = [None] * len(dfs)\n        partials = [\n            self.compute(\n                acc,\n                ana=analysis,\n                metrics=metrics,\n                name=name,\n                return_cached=True,\n                return_dataframe=False,\n            )\n            for acc, analysis, name in zip(dfs, anas, names)\n        ]\n        logging.info(\"partials: %.3f seconds.\", time.time() - st)\n        details = partials\n        partials = [\n            pd.DataFrame(OrderedDict([(k, i[k]) for k in metrics]), index=[name])\n            for i, name in zip(partials, names)\n        ]\n        if generate_overall:\n            names = \"OVERALL\"\n            # merged, infomap = MOTAccumulator.merge_event_dataframes(dfs, return_mappings = True)\n            # dfs = merged\n            # anas = MOTAccumulator.merge_analysis(anas, infomap)\n            # partials.append(self.compute(dfs, ana=anas, metrics=metrics, name=names)[0])\n            partials.append(self.compute_overall(details, metrics=metrics, name=names))\n        logging.info(\"mergeOverall: %.3f seconds.\", time.time() - st)\n        return pd.concat(partials)\n\n    def _compute(self, df_map, name, cache, options, parent=None):\n        \"\"\"Compute metric and resolve dependencies.\"\"\"\n        assert name in self.metrics, \"Cannot find metric {} required by {}.\".format(\n            name, parent\n        )\n        already = cache.get(name, None)\n        if already is not None:\n            return already\n        minfo = self.metrics[name]\n        vals = []\n        for depname in minfo[\"deps\"]:\n            v = cache.get(depname, None)\n            if v is None:\n                v = cache[depname] = self._compute(\n                    df_map, depname, cache, options, parent=name\n                )\n            vals.append(v)\n        if _getargspec(minfo[\"fnc\"]).defaults is None:\n            return minfo[\"fnc\"](df_map, *vals)\n        else:\n            return minfo[\"fnc\"](df_map, *vals, **options)\n\n    def _compute_overall(self, partials, name, cache, parent=None):\n        assert name in self.metrics, \"Cannot find metric {} required by {}.\".format(\n            name, parent\n        )\n        already = cache.get(name, None)\n        if already is not None:\n            return already\n        minfo = self.metrics[name]\n        vals = []\n        for depname in minfo[\"deps_m\"]:\n            v = cache.get(depname, None)\n            if v is None:\n                v = cache[depname] = self._compute_overall(\n                    partials, depname, cache, parent=name\n                )\n            vals.append(v)\n        assert minfo[\"fnc_m\"] is not None, \"merge function for metric %s is None\" % name\n        return minfo[\"fnc_m\"](partials, *vals)\n\n\nsimple_add_func = []\n\n\ndef num_frames(df):\n    \"\"\"Total number of frames.\"\"\"\n    return df.full.index.get_level_values(0).unique().shape[0]\n\n\nsimple_add_func.append(num_frames)\n\n\ndef obj_frequencies(df):\n    \"\"\"Total number of occurrences of individual objects over all frames.\"\"\"\n    return df.noraw.OId.value_counts()\n\n\ndef pred_frequencies(df):\n    \"\"\"Total number of occurrences of individual predictions over all frames.\"\"\"\n    return df.noraw.HId.value_counts()\n\n\ndef num_unique_objects(df, obj_frequencies):\n    \"\"\"Total number of unique object ids encountered.\"\"\"\n    del df  # unused\n    return len(obj_frequencies)\n\n\nsimple_add_func.append(num_unique_objects)\n\n\ndef num_matches(df):\n    \"\"\"Total number matches.\"\"\"\n    return df.noraw.Type.isin([\"MATCH\"]).sum()\n\n\nsimple_add_func.append(num_matches)\n\n\ndef num_switches(df):\n    \"\"\"Total number of track switches.\"\"\"\n    return df.noraw.Type.isin([\"SWITCH\"]).sum()\n\n\nsimple_add_func.append(num_switches)\n\n\ndef num_transfer(df):\n    \"\"\"Total number of track transfer.\"\"\"\n    return df.extra.Type.isin([\"TRANSFER\"]).sum()\n\n\nsimple_add_func.append(num_transfer)\n\n\ndef num_ascend(df):\n    \"\"\"Total number of track ascend.\"\"\"\n    return df.extra.Type.isin([\"ASCEND\"]).sum()\n\n\nsimple_add_func.append(num_ascend)\n\n\ndef num_migrate(df):\n    \"\"\"Total number of track migrate.\"\"\"\n    return df.extra.Type.isin([\"MIGRATE\"]).sum()\n\n\nsimple_add_func.append(num_migrate)\n\n\ndef num_false_positives(df):\n    \"\"\"Total number of false positives (false-alarms).\"\"\"\n    return df.noraw.Type.isin([\"FP\"]).sum()\n\n\nsimple_add_func.append(num_false_positives)\n\n\ndef num_misses(df):\n    \"\"\"Total number of misses.\"\"\"\n    return df.noraw.Type.isin([\"MISS\"]).sum()\n\n\nsimple_add_func.append(num_misses)\n\n\ndef num_detections(df, num_matches, num_switches):\n    \"\"\"Total number of detected objects including matches and switches.\"\"\"\n    del df  # unused\n    return num_matches + num_switches\n\n\nsimple_add_func.append(num_detections)\n\n\ndef num_objects(df, obj_frequencies):\n    \"\"\"Total number of unique object appearances over all frames.\"\"\"\n    del df  # unused\n    return obj_frequencies.sum()\n\n\nsimple_add_func.append(num_objects)\n\n\ndef num_predictions(df, pred_frequencies):\n    \"\"\"Total number of unique prediction appearances over all frames.\"\"\"\n    del df  # unused\n    return pred_frequencies.sum()\n\n\nsimple_add_func.append(num_predictions)\n\n\ndef num_gt_ids(df):\n    \"\"\"Number of unique gt ids.\"\"\"\n    return df.full[\"OId\"].dropna().unique().shape[0]\n\n\nsimple_add_func.append(num_gt_ids)\n\n\ndef num_dt_ids(df):\n    \"\"\"Number of unique dt ids.\"\"\"\n    return df.full[\"HId\"].dropna().unique().shape[0]\n\n\nsimple_add_func.append(num_dt_ids)\n\n\ndef track_ratios(df, obj_frequencies):\n    \"\"\"Ratio of assigned to total appearance count per unique object id.\"\"\"\n    tracked = df.noraw[df.noraw.Type != \"MISS\"][\"OId\"].value_counts()\n    return tracked.div(obj_frequencies).fillna(0.0)\n\n\ndef mostly_tracked(df, track_ratios):\n    \"\"\"Number of objects tracked for at least 80 percent of lifespan.\"\"\"\n    del df  # unused\n    return track_ratios[track_ratios >= 0.8].count()\n\n\nsimple_add_func.append(mostly_tracked)\n\n\ndef partially_tracked(df, track_ratios):\n    \"\"\"Number of objects tracked between 20 and 80 percent of lifespan.\"\"\"\n    del df  # unused\n    return track_ratios[(track_ratios >= 0.2) & (track_ratios < 0.8)].count()\n\n\nsimple_add_func.append(partially_tracked)\n\n\ndef mostly_lost(df, track_ratios):\n    \"\"\"Number of objects tracked less than 20 percent of lifespan.\"\"\"\n    del df  # unused\n    return track_ratios[track_ratios < 0.2].count()\n\n\nsimple_add_func.append(mostly_lost)\n\n\ndef num_fragmentations(df, obj_frequencies):\n    \"\"\"Total number of switches from tracked to not tracked.\"\"\"\n    fra = 0\n    for o in obj_frequencies.index:\n        # Find first and last time object was not missed (track span). Then count\n        # the number switches from NOT MISS to MISS state.\n        dfo = df.noraw[df.noraw.OId == o]\n        notmiss = dfo[dfo.Type != \"MISS\"]\n        if len(notmiss) == 0:\n            continue\n        first = notmiss.index[0]\n        last = notmiss.index[-1]\n        diffs = dfo.loc[first:last].Type.apply(lambda x: 1 if x == \"MISS\" else 0).diff()\n        fra += diffs[diffs == 1].count()\n    return fra\n\n\nsimple_add_func.append(num_fragmentations)\n\n\ndef motp(df, num_detections):\n    \"\"\"Multiple object tracker precision.\"\"\"\n    return math_util.quiet_divide(df.noraw[\"D\"].sum(), num_detections)\n\n\ndef motp_m(partials, num_detections):\n    res = 0\n    for v in partials:\n        res += v[\"motp\"] * v[\"num_detections\"]\n    return math_util.quiet_divide(res, num_detections)\n\n\ndef mota(df, num_misses, num_switches, num_false_positives, num_objects):\n    \"\"\"Multiple object tracker accuracy.\"\"\"\n    del df  # unused\n    return 1.0 - math_util.quiet_divide(\n        num_misses + num_switches + num_false_positives, num_objects\n    )\n\n\ndef mota_m(partials, num_misses, num_switches, num_false_positives, num_objects):\n    del partials  # unused\n    return 1.0 - math_util.quiet_divide(\n        num_misses + num_switches + num_false_positives, num_objects\n    )\n\n\ndef precision(df, num_detections, num_false_positives):\n    \"\"\"Number of detected objects over sum of detected and false positives.\"\"\"\n    del df  # unused\n    return math_util.quiet_divide(num_detections, num_false_positives + num_detections)\n\n\ndef precision_m(partials, num_detections, num_false_positives):\n    del partials  # unused\n    return math_util.quiet_divide(num_detections, num_false_positives + num_detections)\n\n\ndef recall(df, num_detections, num_objects):\n    \"\"\"Number of detections over number of objects.\"\"\"\n    del df  # unused\n    return math_util.quiet_divide(num_detections, num_objects)\n\n\ndef recall_m(partials, num_detections, num_objects):\n    del partials  # unused\n    return math_util.quiet_divide(num_detections, num_objects)\n\n\ndef deta_alpha(df, num_detections, num_objects, num_false_positives):\n    r\"\"\"DeTA under specific threshold $\\alpha$\n    Source: https://jonathonluiten.medium.com/how-to-evaluate-tracking-with-the-hota-metrics-754036d183e1\n    \"\"\"\n    del df  # unused\n    return math_util.quiet_divide(num_detections, max(1, num_objects + num_false_positives))\n\n\ndef deta_alpha_m(partials):\n    res = 0\n    for v in partials:\n        res += v[\"deta_alpha\"]\n    return math_util.quiet_divide(res, len(partials))\n\n\ndef assa_alpha(df, num_detections, num_gt_ids, num_dt_ids):\n    r\"\"\"AssA under specific threshold $\\alpha$\n    Source: https://github.com/JonathonLuiten/TrackEval/blob/12c8791b303e0a0b50f753af204249e622d0281a/trackeval/metrics/hota.py#L107-L108\n    \"\"\"\n    max_gt_ids = int(df.noraw.OId.max())\n    max_dt_ids = int(df.noraw.HId.max())\n\n    match_count_array = np.zeros((max_gt_ids, max_dt_ids))\n    gt_id_counts = np.zeros((max_gt_ids, 1))\n    tracker_id_counts = np.zeros((1, max_dt_ids))\n    for idx in range(len(df.noraw)):\n        oid, hid = df.noraw.iloc[idx, 1], df.noraw.iloc[idx, 2]\n        if df.noraw.iloc[idx, 0] in [\"SWITCH\", \"MATCH\"]:\n            match_count_array[int(oid) - 1, int(hid) - 1] += 1\n        if oid == oid:  # check non nan\n            gt_id_counts[int(oid) - 1] += 1\n        if hid == hid:\n            tracker_id_counts[0, int(hid) - 1] += 1\n\n    ass_a = match_count_array / np.maximum(1, gt_id_counts + tracker_id_counts - match_count_array)\n    return math_util.quiet_divide((ass_a * match_count_array).sum(), max(1, num_detections))\n\n\ndef assa_alpha_m(partials):\n    res = 0\n    for v in partials:\n        res += v[\"assa_alpha\"]\n    return math_util.quiet_divide(res, len(partials))\n\n\ndef hota_alpha(df, deta_alpha, assa_alpha):\n    r\"\"\"HOTA under specific threshold $\\alpha$\"\"\"\n    del df\n    return (deta_alpha * assa_alpha) ** 0.5\n\n\ndef hota_alpha_m(partials):\n    res = 0\n    for v in partials:\n        res += v[\"hota_alpha\"]\n    return math_util.quiet_divide(res, len(partials))\n\n\nclass DataFrameMap:  # pylint: disable=too-few-public-methods\n    def __init__(self, full, raw, noraw, extra):\n        self.full = full\n        self.raw = raw\n        self.noraw = noraw\n        self.extra = extra\n\n\ndef events_to_df_map(df):\n    raw = df[df.Type == \"RAW\"]\n    noraw = df[\n        (df.Type != \"RAW\")\n        & (df.Type != \"ASCEND\")\n        & (df.Type != \"TRANSFER\")\n        & (df.Type != \"MIGRATE\")\n    ]\n    extra = df[df.Type != \"RAW\"]\n    df_map = DataFrameMap(full=df, raw=raw, noraw=noraw, extra=extra)\n    return df_map\n\n\ndef extract_counts_from_df_map(df):\n    \"\"\"\n    Returns:\n        Tuple (ocs, hcs, tps).\n        ocs: Dict from object id to count.\n        hcs: Dict from hypothesis id to count.\n        tps: Dict from (object id, hypothesis id) to true-positive count.\n        The ids are arbitrary, they might NOT be consecutive integers from 0.\n    \"\"\"\n    oids = df.full[\"OId\"].dropna().unique()\n    hids = df.full[\"HId\"].dropna().unique()\n\n    flat = df.raw.reset_index()\n    # Exclude events that do not belong to either set.\n    flat = flat[flat[\"OId\"].isin(oids) | flat[\"HId\"].isin(hids)]\n    # Count number of frames where each (non-empty) OId and HId appears.\n    ocs = flat.set_index(\"OId\")[\"FrameId\"].groupby(\"OId\").nunique().to_dict()\n    hcs = flat.set_index(\"HId\")[\"FrameId\"].groupby(\"HId\").nunique().to_dict()\n    # Select three columns of interest and index by ('OId', 'HId').\n    dists = flat[[\"OId\", \"HId\", \"D\"]].set_index([\"OId\", \"HId\"]).dropna()\n    # Count events with non-empty distance for each pair.\n    tps = dists.groupby([\"OId\", \"HId\"])[\"D\"].count().to_dict()\n    return ocs, hcs, tps\n\n\ndef id_global_assignment(df, ana=None):\n    \"\"\"ID measures: Global min-cost assignment for ID measures.\"\"\"\n    # pylint: disable=too-many-locals\n    del ana  # unused\n    ocs, hcs, tps = extract_counts_from_df_map(df)\n    oids = sorted(ocs.keys())\n    hids = sorted(hcs.keys())\n    oids_idx = dict((o, i) for i, o in enumerate(oids))\n    hids_idx = dict((h, i) for i, h in enumerate(hids))\n    no = len(ocs)\n    nh = len(hcs)\n\n    fpmatrix = np.full((no + nh, no + nh), 0.0)\n    fnmatrix = np.full((no + nh, no + nh), 0.0)\n    fpmatrix[no:, :nh] = np.nan\n    fnmatrix[:no, nh:] = np.nan\n\n    for oid, oc in ocs.items():\n        r = oids_idx[oid]\n        fnmatrix[r, :nh] = oc\n        fnmatrix[r, nh + r] = oc\n\n    for hid, hc in hcs.items():\n        c = hids_idx[hid]\n        fpmatrix[:no, c] = hc\n        fpmatrix[c + no, c] = hc\n\n    for (oid, hid), ex in tps.items():\n        r = oids_idx[oid]\n        c = hids_idx[hid]\n        fpmatrix[r, c] -= ex\n        fnmatrix[r, c] -= ex\n\n    costs = fpmatrix + fnmatrix\n    rids, cids = linear_sum_assignment(costs)\n\n    return {\n        \"fpmatrix\": fpmatrix,\n        \"fnmatrix\": fnmatrix,\n        \"rids\": rids,\n        \"cids\": cids,\n        \"costs\": costs,\n        \"min_cost\": costs[rids, cids].sum(),\n    }\n\n\ndef idfp(df, id_global_assignment):\n    \"\"\"ID measures: Number of false positive matches after global min-cost matching.\"\"\"\n    del df  # unused\n    rids, cids = id_global_assignment[\"rids\"], id_global_assignment[\"cids\"]\n    return id_global_assignment[\"fpmatrix\"][rids, cids].sum()\n\n\nsimple_add_func.append(idfp)\n\n\ndef idfn(df, id_global_assignment):\n    \"\"\"ID measures: Number of false negatives matches after global min-cost matching.\"\"\"\n    del df  # unused\n    rids, cids = id_global_assignment[\"rids\"], id_global_assignment[\"cids\"]\n    return id_global_assignment[\"fnmatrix\"][rids, cids].sum()\n\n\nsimple_add_func.append(idfn)\n\n\ndef idtp(df, id_global_assignment, num_objects, idfn):\n    \"\"\"ID measures: Number of true positives matches after global min-cost matching.\"\"\"\n    del df, id_global_assignment  # unused\n    return num_objects - idfn\n\n\nsimple_add_func.append(idtp)\n\n\ndef idp(df, idtp, idfp):\n    \"\"\"ID measures: global min-cost precision.\"\"\"\n    del df  # unused\n    return math_util.quiet_divide(idtp, idtp + idfp)\n\n\ndef idp_m(partials, idtp, idfp):\n    del partials  # unused\n    return math_util.quiet_divide(idtp, idtp + idfp)\n\n\ndef idr(df, idtp, idfn):\n    \"\"\"ID measures: global min-cost recall.\"\"\"\n    del df  # unused\n    return math_util.quiet_divide(idtp, idtp + idfn)\n\n\ndef idr_m(partials, idtp, idfn):\n    del partials  # unused\n    return math_util.quiet_divide(idtp, idtp + idfn)\n\n\ndef idf1(df, idtp, num_objects, num_predictions):\n    \"\"\"ID measures: global min-cost F1 score.\"\"\"\n    del df  # unused\n    return math_util.quiet_divide(2 * idtp, num_objects + num_predictions)\n\n\ndef idf1_m(partials, idtp, num_objects, num_predictions):\n    del partials  # unused\n    return math_util.quiet_divide(2 * idtp, num_objects + num_predictions)\n\n\nfor one in simple_add_func:\n    name = one.__name__\n\n    def getSimpleAdd(nm):\n        def simpleAddHolder(partials):\n            res = 0\n            for v in partials:\n                res += v[nm]\n            return res\n\n        return simpleAddHolder\n\n    locals()[name + \"_m\"] = getSimpleAdd(name)\n\n\ndef create():\n    \"\"\"Creates a MetricsHost and populates it with default metrics.\"\"\"\n    m = MetricsHost()\n\n    m.register(num_frames, formatter=\"{:d}\".format)\n    m.register(obj_frequencies, formatter=\"{:d}\".format)\n    m.register(pred_frequencies, formatter=\"{:d}\".format)\n    m.register(num_matches, formatter=\"{:d}\".format)\n    m.register(num_switches, formatter=\"{:d}\".format)\n    m.register(num_transfer, formatter=\"{:d}\".format)\n    m.register(num_ascend, formatter=\"{:d}\".format)\n    m.register(num_migrate, formatter=\"{:d}\".format)\n    m.register(num_false_positives, formatter=\"{:d}\".format)\n    m.register(num_misses, formatter=\"{:d}\".format)\n    m.register(num_detections, formatter=\"{:d}\".format)\n    m.register(num_objects, formatter=\"{:d}\".format)\n    m.register(num_predictions, formatter=\"{:d}\".format)\n    m.register(num_gt_ids, formatter=\"{:d}\".format)\n    m.register(num_dt_ids, formatter=\"{:d}\".format)\n    m.register(num_unique_objects, formatter=\"{:d}\".format)\n    m.register(track_ratios)\n    m.register(mostly_tracked, formatter=\"{:d}\".format)\n    m.register(partially_tracked, formatter=\"{:d}\".format)\n    m.register(mostly_lost, formatter=\"{:d}\".format)\n    m.register(num_fragmentations)\n    m.register(motp, formatter=\"{:.3f}\".format)\n    m.register(mota, formatter=\"{:.1%}\".format)\n    m.register(precision, formatter=\"{:.1%}\".format)\n    m.register(recall, formatter=\"{:.1%}\".format)\n\n    m.register(id_global_assignment)\n    m.register(idfp)\n    m.register(idfn)\n    m.register(idtp)\n    m.register(idp, formatter=\"{:.1%}\".format)\n    m.register(idr, formatter=\"{:.1%}\".format)\n    m.register(idf1, formatter=\"{:.1%}\".format)\n\n    m.register(deta_alpha, formatter=\"{:.1%}\".format)\n    m.register(assa_alpha, formatter=\"{:.1%}\".format)\n    m.register(hota_alpha, formatter=\"{:.1%}\".format)\n\n    return m\n\n\nmotchallenge_metrics = [\n    \"idf1\",\n    \"idp\",\n    \"idr\",\n    \"recall\",\n    \"precision\",\n    \"num_unique_objects\",\n    \"mostly_tracked\",\n    \"partially_tracked\",\n    \"mostly_lost\",\n    \"num_false_positives\",\n    \"num_misses\",\n    \"num_switches\",\n    \"num_fragmentations\",\n    \"mota\",\n    \"motp\",\n    \"num_transfer\",\n    \"num_ascend\",\n    \"num_migrate\",\n]\n\"\"\"A list of all metrics from MOTChallenge.\"\"\"\n",
    "motmetrics/utils.py": "# py-motmetrics - Metrics for multiple object tracker (MOT) benchmarking.\n# https://github.com/cheind/py-motmetrics/\n#\n# MIT License\n# Copyright (c) 2017-2020 Christoph Heindl, Jack Valmadre and others.\n# See LICENSE file for terms.\n\n\"\"\"Functions for populating event accumulators.\"\"\"\n\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\n\nfrom motmetrics.distances import iou_matrix, norm2squared_matrix\nfrom motmetrics.mot import MOTAccumulator\nfrom motmetrics.preprocess import preprocessResult\n\n\ndef compute_global_aligment_score(\n    allframeids, fid_to_fgt, fid_to_fdt, num_gt_id, num_det_id, dist_func\n):\n    \"\"\"Taken from https://github.com/JonathonLuiten/TrackEval/blob/12c8791b303e0a0b50f753af204249e622d0281a/trackeval/metrics/hota.py\"\"\"\n    potential_matches_count = np.zeros((num_gt_id, num_det_id))\n    gt_id_count = np.zeros((num_gt_id, 1))\n    tracker_id_count = np.zeros((1, num_det_id))\n\n    for fid in allframeids:\n        oids = np.empty(0)\n        hids = np.empty(0)\n        if fid in fid_to_fgt:\n            fgt = fid_to_fgt[fid]\n            oids = fgt.index.get_level_values(\"Id\")\n        if fid in fid_to_fdt:\n            fdt = fid_to_fdt[fid]\n            hids = fdt.index.get_level_values(\"Id\")\n        if len(oids) > 0 and len(hids) > 0:\n            gt_ids = np.array(oids.values) - 1\n            dt_ids = np.array(hids.values) - 1\n            similarity = dist_func(fgt.values, fdt.values, return_dist=False)\n\n            sim_iou_denom = (\n                similarity.sum(0)[np.newaxis, :] + similarity.sum(1)[:, np.newaxis] - similarity\n            )\n            sim_iou = np.zeros_like(similarity)\n            sim_iou_mask = sim_iou_denom > 0 + np.finfo(\"float\").eps\n            sim_iou[sim_iou_mask] = similarity[sim_iou_mask] / sim_iou_denom[sim_iou_mask]\n            potential_matches_count[gt_ids[:, np.newaxis], dt_ids[np.newaxis, :]] += sim_iou\n\n            # Calculate the total number of dets for each gt_id and tracker_id.\n            gt_id_count[gt_ids] += 1\n            tracker_id_count[0, dt_ids] += 1\n    global_alignment_score = potential_matches_count / (\n        np.maximum(1, gt_id_count + tracker_id_count - potential_matches_count)\n    )\n    return global_alignment_score\n\n\ndef compare_to_groundtruth_reweighting(gt, dt, dist=\"iou\", distfields=None, distth=(0.5)):\n    \"\"\"Compare groundtruth and detector results with global alignment score.\n\n    This method assumes both results are given in terms of DataFrames with at least the following fields\n     - `FrameId` First level index used for matching ground-truth and test frames.\n     - `Id` Secondary level index marking available object / hypothesis ids\n\n    Depending on the distance to be used relevant distfields need to be specified.\n\n    Params\n    ------\n    gt : pd.DataFrame\n        Dataframe for ground-truth\n    test : pd.DataFrame\n        Dataframe for detector results\n\n    Kwargs\n    ------\n    dist : str, optional\n        String identifying distance to be used. Defaults to intersection over union ('iou'). Euclidean\n        distance ('euclidean') and squared euclidean distance ('seuc') are also supported.\n    distfields: array, optional\n        Fields relevant for extracting distance information. Defaults to ['X', 'Y', 'Width', 'Height']\n    distth: Union(float, array_like), optional\n        Maximum tolerable distance. Pairs exceeding this threshold are marked 'do-not-pair'.\n        If a list of thresholds is given, multiple accumulators are returned.\n    \"\"\"\n    # pylint: disable=too-many-locals\n    if distfields is None:\n        distfields = [\"X\", \"Y\", \"Width\", \"Height\"]\n\n    def compute_iou(a, b, return_dist):\n        return iou_matrix(a, b, max_iou=distth, return_dist=return_dist)\n\n    def compute_euc(a, b, *args, **kwargs):\n        return np.sqrt(norm2squared_matrix(a, b, max_d2=distth**2))\n\n    def compute_seuc(a, b, *args, **kwargs):\n        return norm2squared_matrix(a, b, max_d2=distth)\n\n    if dist.upper() == \"IOU\":\n        compute_dist = compute_iou\n    elif dist.upper() == \"EUC\":\n        compute_dist = compute_euc\n        import warnings\n\n        warnings.warn(\n            f\"'euc' flag changed its behavior. The euclidean distance is now used instead of the squared euclidean distance. Make sure the used threshold (distth={distth}) is not squared. Use 'euclidean' flag to avoid this warning.\"\n        )\n    elif dist.upper() == \"EUCLIDEAN\":\n        compute_dist = compute_euc\n    elif dist.upper() == \"SEUC\":\n        compute_dist = compute_seuc\n    else:\n        raise f'Unknown distance metric {dist}. Use \"IOU\", \"EUCLIDEAN\",  or \"SEUC\"'\n\n    return_single = False\n    if isinstance(distth, float):\n        distth = [distth]\n        return_single = True\n\n    acc_list = [MOTAccumulator() for _ in range(len(distth))]\n\n    num_gt_id = gt.index.get_level_values(\"Id\").max()\n    num_det_id = dt.index.get_level_values(\"Id\").max()\n\n    # We need to account for all frames reported either by ground truth or\n    # detector. In case a frame is missing in GT this will lead to FPs, in\n    # case a frame is missing in detector results this will lead to FNs.\n    allframeids = gt.index.union(dt.index).levels[0]\n\n    gt = gt[distfields]\n    dt = dt[distfields]\n    fid_to_fgt = dict(iter(gt.groupby(\"FrameId\")))\n    fid_to_fdt = dict(iter(dt.groupby(\"FrameId\")))\n\n    global_alignment_score = compute_global_aligment_score(\n        allframeids, fid_to_fgt, fid_to_fdt, num_gt_id, num_det_id, compute_dist\n    )\n\n    for fid in allframeids:\n        oids = np.empty(0)\n        hids = np.empty(0)\n        weighted_dists = np.empty((0, 0))\n        if fid in fid_to_fgt:\n            fgt = fid_to_fgt[fid]\n            oids = fgt.index.get_level_values(\"Id\")\n        if fid in fid_to_fdt:\n            fdt = fid_to_fdt[fid]\n            hids = fdt.index.get_level_values(\"Id\")\n        if len(oids) > 0 and len(hids) > 0:\n            gt_ids = np.array(oids.values) - 1\n            dt_ids = np.array(hids.values) - 1\n            dists = compute_dist(fgt.values, fdt.values, return_dist=False)\n            weighted_dists = (\n                dists * global_alignment_score[gt_ids[:, np.newaxis], dt_ids[np.newaxis, :]]\n            )\n        for acc, th in zip(acc_list, distth):\n            acc.update(oids, hids, 1 - weighted_dists, frameid=fid, similartiy_matrix=dists, th=th)\n    return acc_list[0] if return_single else acc_list\n\n\ndef compare_to_groundtruth(gt, dt, dist='iou', distfields=None, distth=0.5):\n    \"\"\"Compare groundtruth and detector results.\n\n    This method assumes both results are given in terms of DataFrames with at least the following fields\n     - `FrameId` First level index used for matching ground-truth and test frames.\n     - `Id` Secondary level index marking available object / hypothesis ids\n\n    Depending on the distance to be used relevant distfields need to be specified.\n\n    Params\n    ------\n    gt : pd.DataFrame\n        Dataframe for ground-truth\n    test : pd.DataFrame\n        Dataframe for detector results\n\n    Kwargs\n    ------\n    dist : str, optional\n        String identifying distance to be used. Defaults to intersection over union ('iou'). Euclidean\n        distance ('euclidean') and squared euclidean distance ('seuc') are also supported.\n    distfields: array, optional\n        Fields relevant for extracting distance information. Defaults to ['X', 'Y', 'Width', 'Height']\n    distth: float, optional\n        Maximum tolerable distance. Pairs exceeding this threshold are marked 'do-not-pair'.\n    \"\"\"\n    # pylint: disable=too-many-locals\n    if distfields is None:\n        distfields = ['X', 'Y', 'Width', 'Height']\n\n    def compute_iou(a, b):\n        return iou_matrix(a, b, max_iou=distth)\n\n    def compute_euc(a, b):\n        return np.sqrt(norm2squared_matrix(a, b, max_d2=distth**2))\n\n    def compute_seuc(a, b):\n        return norm2squared_matrix(a, b, max_d2=distth)\n\n    if dist.upper() == 'IOU':\n        compute_dist = compute_iou\n    elif dist.upper() == 'EUC':\n        compute_dist = compute_euc\n        import warnings\n        warnings.warn(f\"'euc' flag changed its behavior. The euclidean distance is now used instead of the squared euclidean distance. Make sure the used threshold (distth={distth}) is not squared. Use 'euclidean' flag to avoid this warning.\")\n    elif dist.upper() == 'EUCLIDEAN':\n        compute_dist = compute_euc\n    elif dist.upper() == 'SEUC':\n        compute_dist = compute_seuc\n    else:\n        raise f'Unknown distance metric {dist}. Use \"IOU\", \"EUCLIDEAN\",  or \"SEUC\"'\n\n    acc = MOTAccumulator()\n\n    # We need to account for all frames reported either by ground truth or\n    # detector. In case a frame is missing in GT this will lead to FPs, in\n    # case a frame is missing in detector results this will lead to FNs.\n    allframeids = gt.index.union(dt.index).levels[0]\n\n    gt = gt[distfields]\n    dt = dt[distfields]\n    fid_to_fgt = dict(iter(gt.groupby('FrameId')))\n    fid_to_fdt = dict(iter(dt.groupby('FrameId')))\n\n    for fid in allframeids:\n        oids = np.empty(0)\n        hids = np.empty(0)\n        dists = np.empty((0, 0))\n        if fid in fid_to_fgt:\n            fgt = fid_to_fgt[fid]\n            oids = fgt.index.get_level_values('Id')\n        if fid in fid_to_fdt:\n            fdt = fid_to_fdt[fid]\n            hids = fdt.index.get_level_values('Id')\n        if len(oids) > 0 and len(hids) > 0:\n            dists = compute_dist(fgt.values, fdt.values)\n        acc.update(oids, hids, dists, frameid=fid)\n\n    return acc\n\n\ndef CLEAR_MOT_M(gt, dt, inifile, dist='iou', distfields=None, distth=0.5, include_all=False, vflag=''):\n    \"\"\"Compare groundtruth and detector results.\n\n    This method assumes both results are given in terms of DataFrames with at least the following fields\n     - `FrameId` First level index used for matching ground-truth and test frames.\n     - `Id` Secondary level index marking available object / hypothesis ids\n\n    Depending on the distance to be used relevant distfields need to be specified.\n\n    Params\n    ------\n    gt : pd.DataFrame\n        Dataframe for ground-truth\n    test : pd.DataFrame\n        Dataframe for detector results\n\n    Kwargs\n    ------\n    dist : str, optional\n        String identifying distance to be used. Defaults to intersection over union.\n    distfields: array, optional\n        Fields relevant for extracting distance information. Defaults to ['X', 'Y', 'Width', 'Height']\n    distth: float, optional\n        Maximum tolerable distance. Pairs exceeding this threshold are marked 'do-not-pair'.\n    \"\"\"\n    # pylint: disable=too-many-locals\n    if distfields is None:\n        distfields = ['X', 'Y', 'Width', 'Height']\n\n    def compute_iou(a, b):\n        return iou_matrix(a, b, max_iou=distth)\n\n    def compute_euc(a, b):\n        return norm2squared_matrix(a, b, max_d2=distth)\n\n    compute_dist = compute_iou if dist.upper() == 'IOU' else compute_euc\n\n    acc = MOTAccumulator()\n    dt = preprocessResult(dt, gt, inifile)\n    if include_all:\n        gt = gt[gt['Confidence'] >= 0.99]\n    else:\n        gt = gt[(gt['Confidence'] >= 0.99) & (gt['ClassId'] == 1)]\n    # We need to account for all frames reported either by ground truth or\n    # detector. In case a frame is missing in GT this will lead to FPs, in\n    # case a frame is missing in detector results this will lead to FNs.\n    allframeids = gt.index.union(dt.index).levels[0]\n    analysis = {'hyp': {}, 'obj': {}}\n    for fid in allframeids:\n        oids = np.empty(0)\n        hids = np.empty(0)\n        dists = np.empty((0, 0))\n\n        if fid in gt.index:\n            fgt = gt.loc[fid]\n            oids = fgt.index.values\n            for oid in oids:\n                oid = int(oid)\n                if oid not in analysis['obj']:\n                    analysis['obj'][oid] = 0\n                analysis['obj'][oid] += 1\n\n        if fid in dt.index:\n            fdt = dt.loc[fid]\n            hids = fdt.index.values\n            for hid in hids:\n                hid = int(hid)\n                if hid not in analysis['hyp']:\n                    analysis['hyp'][hid] = 0\n                analysis['hyp'][hid] += 1\n\n        if oids.shape[0] > 0 and hids.shape[0] > 0:\n            dists = compute_dist(fgt[distfields].values, fdt[distfields].values)\n\n        acc.update(oids, hids, dists, frameid=fid, vf=vflag)\n\n    return acc, analysis\n"
  },
  "GT_src_dict": {
    "motmetrics/mot.py": {
      "MOTAccumulator.__init__": {
        "code": "    def __init__(self, auto_id=False, max_switch_time=float('inf')):\n        \"\"\"Initializes a `MOTAccumulator` instance for managing and accumulating tracking events in multi-object tracking scenarios. \n\nParameters\n----------\nauto_id : bool, optional\n    If set to `True`, the frame indices will be auto-incremented. Providing a frame ID during updates will result in an error. Defaults to `False`.\nmax_switch_time : float, optional\n    This defines the maximum time span for which unobserved, tracked objects can generate track switch events. It helps maintain object IDs for objects reappearing after leaving the field of view. Default is set to infinity (no upper bound).\n\nAttributes\n----------\nauto_id : bool\n    Indicates if frame indices are auto-incremented.\nmax_switch_time : float\n    The allowable timespan for track switch events.\n_events : dict\n    Stores event data (like type and IDs).\n_indices : dict\n    Keeps track of indices for events, structured by 'FrameId' and 'Event'.\nm : dict\n    Holds the pairings of objects and hypotheses at the current timestamp.\nres_m : dict\n    Tracks result pairings across all frames.\nlast_occurrence : dict\n    Stores the most recent occurrence of each object.\nlast_match : dict\n    Keeps track of the last match for each object.\nhypHistory : dict\n    Maintains history of hypothesis occurrences.\ndirty_events : bool\n    Indicates if the events need recalculating.\ncached_events_df : pd.DataFrame or None\n    Caches the DataFrame of events to improve performance.\nlast_update_frameid : int or None\n    Records the last updated frame ID.\n\nDependencies\n------------\nThis class interacts with other methods and classes in the `motmetrics` library, particularly with the event summarization and statistics computations.\"\"\"\n        'Create a MOTAccumulator.\\n\\n        Params\\n        ------\\n        auto_id : bool, optional\\n            Whether or not frame indices are auto-incremented or provided upon\\n            updating. Defaults to false. Not specifying a frame-id when this value\\n            is true results in an error. Specifying a frame-id when this value is\\n            false also results in an error.\\n\\n        max_switch_time : scalar, optional\\n            Allows specifying an upper bound on the timespan an unobserved but\\n            tracked object is allowed to generate track switch events. Useful if groundtruth\\n            objects leaving the field of view keep their ID when they reappear,\\n            but your tracker is not capable of recognizing this (resulting in\\n            track switch events). The default is that there is no upper bound\\n            on the timespan. In units of frame timestamps. When using auto_id\\n            in units of count.\\n        '\n        self.auto_id = auto_id\n        self.max_switch_time = max_switch_time\n        self._events = None\n        self._indices = None\n        self.m = None\n        self.res_m = None\n        self.last_occurrence = None\n        self.last_match = None\n        self.hypHistory = None\n        self.dirty_events = None\n        self.cached_events_df = None\n        self.last_update_frameid = None\n        self.reset()",
        "docstring": "Initializes a `MOTAccumulator` instance for managing and accumulating tracking events in multi-object tracking scenarios. \n\nParameters\n----------\nauto_id : bool, optional\n    If set to `True`, the frame indices will be auto-incremented. Providing a frame ID during updates will result in an error. Defaults to `False`.\nmax_switch_time : float, optional\n    This defines the maximum time span for which unobserved, tracked objects can generate track switch events. It helps maintain object IDs for objects reappearing after leaving the field of view. Default is set to infinity (no upper bound).\n\nAttributes\n----------\nauto_id : bool\n    Indicates if frame indices are auto-incremented.\nmax_switch_time : float\n    The allowable timespan for track switch events.\n_events : dict\n    Stores event data (like type and IDs).\n_indices : dict\n    Keeps track of indices for events, structured by 'FrameId' and 'Event'.\nm : dict\n    Holds the pairings of objects and hypotheses at the current timestamp.\nres_m : dict\n    Tracks result pairings across all frames.\nlast_occurrence : dict\n    Stores the most recent occurrence of each object.\nlast_match : dict\n    Keeps track of the last match for each object.\nhypHistory : dict\n    Maintains history of hypothesis occurrences.\ndirty_events : bool\n    Indicates if the events need recalculating.\ncached_events_df : pd.DataFrame or None\n    Caches the DataFrame of events to improve performance.\nlast_update_frameid : int or None\n    Records the last updated frame ID.\n\nDependencies\n------------\nThis class interacts with other methods and classes in the `motmetrics` library, particularly with the event summarization and statistics computations.",
        "signature": "def __init__(self, auto_id=False, max_switch_time=float('inf')):",
        "type": "Method",
        "class_signature": "class MOTAccumulator(object):"
      },
      "MOTAccumulator.update": {
        "code": "    def update(self, oids, hids, dists, frameid=None, vf='', similartiy_matrix=None, th=None):\n        \"\"\"Updates the MOTAccumulator with frame-specific object detections and generates tracking events.\n\nThis method processes arrays of object and hypothesis IDs, along with a distance matrix, to determine the relationships between objects and hypotheses for a given frame. It produces various tracking events including 'MATCH', 'SWITCH', 'MISS', and 'FP' based on defined criteria. The algorithm includes a phase for re-establishing previously tracked objects and minimizing distance errors using the Kuhn-Munkres algorithm.\n\nParameters\n----------\noids : ndarray\n    Array of object IDs for the current frame.\nhids : ndarray\n    Array of hypothesis IDs for the current frame.\ndists : ndarray\n    Distance matrix of shape (N, M), where N is the number of objects and M is the number of hypotheses, indicating distances between each object and hypothesis. NaN values indicate do-not-pair conditions.\nframeid : int, optional\n    Unique identifier for the current frame. Required if `auto_id` is not enabled.\nvf : str, optional\n    File path for logging details of events.\nsimilartiy_matrix : ndarray, optional\n    Similarity matrix used for distance calculation; if provided, will influence distance handling.\nth : float, optional\n    Threshold for the similarity matrix to determine valid pairings.\n\nReturns\n-------\nint\n    The updated frame ID after processing the current frame.\n\nNotes\n-----\nThe method relies on instance variables such as `self.m`, `self.last_match`, `self.last_occurrence`, and others, which maintain state across frames. It generates a pandas DataFrame from collected events that can be accessed via the `events` property. The use of `_append_to_indices` and `_append_to_events` methods is critical for recording event data.\n\nAdditionally, constants defined in the class, such as event types (e.g., 'MATCH', 'SWITCH'), guide the categorization of event types.\"\"\"\n        'Updates the accumulator with frame specific objects/detections.\\n\\n        This method generates events based on the following algorithm [1]:\\n        1. Try to carry forward already established tracks. If any paired object / hypothesis\\n        from previous timestamps are still visible in the current frame, create a \\'MATCH\\'\\n        event between them.\\n        2. For the remaining constellations minimize the total object / hypothesis distance\\n        error (Kuhn-Munkres algorithm). If a correspondence made contradicts a previous\\n        match create a \\'SWITCH\\' else a \\'MATCH\\' event.\\n        3. Create \\'MISS\\' events for all remaining unassigned objects.\\n        4. Create \\'FP\\' events for all remaining unassigned hypotheses.\\n\\n        Params\\n        ------\\n        oids : N array\\n            Array of object ids.\\n        hids : M array\\n            Array of hypothesis ids.\\n        dists: NxM array\\n            Distance matrix. np.nan values to signal do-not-pair constellations.\\n            See `distances` module for support methods.\\n\\n        Kwargs\\n        ------\\n        frameId : id\\n            Unique frame id. Optional when MOTAccumulator.auto_id is specified during\\n            construction.\\n        vf: file to log details\\n        Returns\\n        -------\\n        frame_events : pd.DataFrame\\n            Dataframe containing generated events\\n\\n        References\\n        ----------\\n        1. Bernardin, Keni, and Rainer Stiefelhagen. \"Evaluating multiple object tracking performance: the CLEAR MOT metrics.\"\\n        EURASIP Journal on Image and Video Processing 2008.1 (2008): 1-10.\\n        '\n        self.dirty_events = True\n        oids = np.asarray(oids)\n        oids_masked = np.zeros_like(oids, dtype=np.bool_)\n        hids = np.asarray(hids)\n        hids_masked = np.zeros_like(hids, dtype=np.bool_)\n        dists = np.atleast_2d(dists).astype(float).reshape(oids.shape[0], hids.shape[0]).copy()\n        if frameid is None:\n            assert self.auto_id, 'auto-id is not enabled'\n            if len(self._indices['FrameId']) > 0:\n                frameid = self._indices['FrameId'][-1] + 1\n            else:\n                frameid = 0\n        else:\n            assert not self.auto_id, 'Cannot provide frame id when auto-id is enabled'\n        eid = itertools.count()\n        no = len(oids)\n        nh = len(hids)\n        self._append_to_indices(frameid, next(eid))\n        self._append_to_events('RAW', np.nan, np.nan, np.nan)\n        cost_for_matching = dists.copy()\n        if similartiy_matrix is not None and th is not None:\n            dists = 1 - similartiy_matrix\n            dists = np.where(similartiy_matrix < th - np.finfo('float').eps, np.nan, dists)\n        valid_i, valid_j = np.where(np.isfinite(dists))\n        valid_dists = dists[valid_i, valid_j]\n        for i, j, dist_ij in zip(valid_i, valid_j, valid_dists):\n            self._append_to_indices(frameid, next(eid))\n            self._append_to_events('RAW', oids[i], hids[j], dist_ij)\n        used_i = np.unique(valid_i)\n        used_j = np.unique(valid_j)\n        unused_i = np.setdiff1d(np.arange(no), used_i)\n        unused_j = np.setdiff1d(np.arange(nh), used_j)\n        for oid in oids[unused_i]:\n            self._append_to_indices(frameid, next(eid))\n            self._append_to_events('RAW', oid, np.nan, np.nan)\n        for hid in hids[unused_j]:\n            self._append_to_indices(frameid, next(eid))\n            self._append_to_events('RAW', np.nan, hid, np.nan)\n        if oids.size * hids.size > 0:\n            if similartiy_matrix is None or th is None:\n                for i in range(oids.shape[0]):\n                    if not (oids[i] in self.m and self.last_match[oids[i]] == self.last_update_frameid):\n                        continue\n                    hprev = self.m[oids[i]]\n                    j, = np.where(~hids_masked & (hids == hprev))\n                    if j.shape[0] == 0:\n                        continue\n                    j = j[0]\n                    if np.isfinite(dists[i, j]):\n                        o = oids[i]\n                        h = hids[j]\n                        oids_masked[i] = True\n                        hids_masked[j] = True\n                        self.m[oids[i]] = hids[j]\n                        self._append_to_indices(frameid, next(eid))\n                        self._append_to_events('MATCH', oids[i], hids[j], dists[i, j])\n                        self.last_match[o] = frameid\n                        self.hypHistory[h] = frameid\n            dists[oids_masked, :] = np.nan\n            dists[:, hids_masked] = np.nan\n            rids, cids = linear_sum_assignment(cost_for_matching)\n            for i, j in zip(rids, cids):\n                if not np.isfinite(dists[i, j]):\n                    continue\n                o = oids[i]\n                h = hids[j]\n                switch_condition = o in self.m and self.m[o] != h and (o in self.last_occurrence) and (abs(frameid - self.last_occurrence[o]) <= self.max_switch_time)\n                is_switch = switch_condition\n                cat1 = 'SWITCH' if is_switch else 'MATCH'\n                if cat1 == 'SWITCH':\n                    if h not in self.hypHistory:\n                        subcat = 'ASCEND'\n                        self._append_to_indices(frameid, next(eid))\n                        self._append_to_events(subcat, oids[i], hids[j], dists[i, j])\n                is_transfer = h in self.res_m and self.res_m[h] != o\n                cat2 = 'TRANSFER' if is_transfer else 'MATCH'\n                if cat2 == 'TRANSFER':\n                    if o not in self.last_match:\n                        subcat = 'MIGRATE'\n                        self._append_to_indices(frameid, next(eid))\n                        self._append_to_events(subcat, oids[i], hids[j], dists[i, j])\n                    self._append_to_indices(frameid, next(eid))\n                    self._append_to_events(cat2, oids[i], hids[j], dists[i, j])\n                if vf != '' and (cat1 != 'MATCH' or cat2 != 'MATCH'):\n                    if cat1 == 'SWITCH':\n                        vf.write('%s %d %d %d %d %d\\n' % (subcat[:2], o, self.last_match[o], self.m[o], frameid, h))\n                    if cat2 == 'TRANSFER':\n                        vf.write('%s %d %d %d %d %d\\n' % (subcat[:2], h, self.hypHistory[h], self.res_m[h], frameid, o))\n                self.hypHistory[h] = frameid\n                self.last_match[o] = frameid\n                self._append_to_indices(frameid, next(eid))\n                self._append_to_events(cat1, oids[i], hids[j], dists[i, j])\n                oids_masked[i] = True\n                hids_masked[j] = True\n                self.m[o] = h\n                self.res_m[h] = o\n        for o in oids[~oids_masked]:\n            self._append_to_indices(frameid, next(eid))\n            self._append_to_events('MISS', o, np.nan, np.nan)\n            if vf != '':\n                vf.write('FN %d %d\\n' % (frameid, o))\n        for h in hids[~hids_masked]:\n            self._append_to_indices(frameid, next(eid))\n            self._append_to_events('FP', np.nan, h, np.nan)\n            if vf != '':\n                vf.write('FP %d %d\\n' % (frameid, h))\n        for o in oids:\n            self.last_occurrence[o] = frameid\n        self.last_update_frameid = frameid\n        return frameid",
        "docstring": "Updates the MOTAccumulator with frame-specific object detections and generates tracking events.\n\nThis method processes arrays of object and hypothesis IDs, along with a distance matrix, to determine the relationships between objects and hypotheses for a given frame. It produces various tracking events including 'MATCH', 'SWITCH', 'MISS', and 'FP' based on defined criteria. The algorithm includes a phase for re-establishing previously tracked objects and minimizing distance errors using the Kuhn-Munkres algorithm.\n\nParameters\n----------\noids : ndarray\n    Array of object IDs for the current frame.\nhids : ndarray\n    Array of hypothesis IDs for the current frame.\ndists : ndarray\n    Distance matrix of shape (N, M), where N is the number of objects and M is the number of hypotheses, indicating distances between each object and hypothesis. NaN values indicate do-not-pair conditions.\nframeid : int, optional\n    Unique identifier for the current frame. Required if `auto_id` is not enabled.\nvf : str, optional\n    File path for logging details of events.\nsimilartiy_matrix : ndarray, optional\n    Similarity matrix used for distance calculation; if provided, will influence distance handling.\nth : float, optional\n    Threshold for the similarity matrix to determine valid pairings.\n\nReturns\n-------\nint\n    The updated frame ID after processing the current frame.\n\nNotes\n-----\nThe method relies on instance variables such as `self.m`, `self.last_match`, `self.last_occurrence`, and others, which maintain state across frames. It generates a pandas DataFrame from collected events that can be accessed via the `events` property. The use of `_append_to_indices` and `_append_to_events` methods is critical for recording event data.\n\nAdditionally, constants defined in the class, such as event types (e.g., 'MATCH', 'SWITCH'), guide the categorization of event types.",
        "signature": "def update(self, oids, hids, dists, frameid=None, vf='', similartiy_matrix=None, th=None):",
        "type": "Method",
        "class_signature": "class MOTAccumulator(object):"
      },
      "MOTAccumulator.events": {
        "code": "    def events(self):\n        \"\"\"Return the DataFrame containing tracking events.\n\nThis property retrieves the accumulated tracking events in the form of a pandas DataFrame.\nIf the events data is marked as dirty (i.e., `_dirty_events` is `True`), it invokes the \ncreation of a new DataFrame using `MOTAccumulator.new_event_dataframe_with_data`, \nwhich populates the DataFrame with event data based on the internal state of the accumulator, \nnamely `_indices` and `_events`. The `_indices` dictionary contains frame IDs and event IDs, \nwhile the `_events` dictionary stores details about each event, including type, object ID, \nhypothesis ID, and distance.\n\nReturns\n-------\npd.DataFrame\n    A DataFrame comprising the various tracking events, with hierarchically indexed by \n    `FrameId` and `EventId`, and columns for event type, object ID, hypothesis ID, \n    and distance.\"\"\"\n        if self.dirty_events:\n            self.cached_events_df = MOTAccumulator.new_event_dataframe_with_data(self._indices, self._events)\n            self.dirty_events = False\n        return self.cached_events_df",
        "docstring": "Return the DataFrame containing tracking events.\n\nThis property retrieves the accumulated tracking events in the form of a pandas DataFrame.\nIf the events data is marked as dirty (i.e., `_dirty_events` is `True`), it invokes the \ncreation of a new DataFrame using `MOTAccumulator.new_event_dataframe_with_data`, \nwhich populates the DataFrame with event data based on the internal state of the accumulator, \nnamely `_indices` and `_events`. The `_indices` dictionary contains frame IDs and event IDs, \nwhile the `_events` dictionary stores details about each event, including type, object ID, \nhypothesis ID, and distance.\n\nReturns\n-------\npd.DataFrame\n    A DataFrame comprising the various tracking events, with hierarchically indexed by \n    `FrameId` and `EventId`, and columns for event type, object ID, hypothesis ID, \n    and distance.",
        "signature": "def events(self):",
        "type": "Method",
        "class_signature": "class MOTAccumulator(object):"
      }
    },
    "motmetrics/metrics.py": {
      "MetricsHost.__init__": {
        "code": "    def __init__(self):\n        \"\"\"Initializes an instance of the MetricsHost class, which is responsible for managing metrics and their dependencies within the context of multiple object tracking (MOT) benchmarking.\n\n    Attributes\n    ----------\n    metrics : OrderedDict\n        An ordered dictionary that stores registered metrics. Each metric is represented by a key-value pair where the key is the name of the metric and the value is a dictionary containing its properties, such as the metric function, its dependencies, and formatting options.\n\n    This class interacts with various metric functions defined in the same module, allowing the addition, retrieval, and computation of metrics related to object tracking analysis.\"\"\"\n        self.metrics = OrderedDict()",
        "docstring": "Initializes an instance of the MetricsHost class, which is responsible for managing metrics and their dependencies within the context of multiple object tracking (MOT) benchmarking.\n\nAttributes\n----------\nmetrics : OrderedDict\n    An ordered dictionary that stores registered metrics. Each metric is represented by a key-value pair where the key is the name of the metric and the value is a dictionary containing its properties, such as the metric function, its dependencies, and formatting options.\n\nThis class interacts with various metric functions defined in the same module, allowing the addition, retrieval, and computation of metrics related to object tracking analysis.",
        "signature": "def __init__(self):",
        "type": "Method",
        "class_signature": "class MetricsHost:"
      },
      "MetricsHost.register": {
        "code": "    def register(self, fnc, deps='auto', name=None, helpstr=None, formatter=None, fnc_m=None, deps_m='auto'):\n        \"\"\"Register a new metric for evaluation in the MetricsHost class.\n\nThis method allows the registration of user-defined and built-in metrics that compute various performance measures on tracking data. Each metric is defined by a function which can have dependencies on other metrics, and it can provide a formatted output.\n\nParameters\n----------\nfnc : function\n    The function that computes the metric. It must accept at least one argument (the DataFrame) followed by any dependencies.\ndeps : str or list of str, optional\n    The dependencies of the metric, which can be automatically inferred from the function's arguments, specified as 'auto', or provided explicitly. If None, the metric has no dependencies.\nname : str or None, optional\n    A unique identifier for the metric. If not provided, it defaults to the function name.\nhelpstr : str or None, optional\n    A description of what the metric computes. If not provided, it defaults to the function's docstring.\nformatter : callable, optional\n    A format string for displaying the metric's result, such as \"{:.2%}.format\".\nfnc_m : function or None, optional\n    A merging function for the metric results, if applicable. If not provided and the derived name (name + \"_m\") exists in the global scope, it will be used.\ndeps_m : str or list of str, optional\n    Dependencies for the merging function, handled similarly to the `deps` parameter.\n\nReturns\n-------\nNone\n    The method registers the metric in the metrics dictionary without returning any value.\n\nNotes\n-----\n- The registered metrics can be listed and computed later using other methods in the MetricsHost class.\n- The `motchallenge_metrics` constant is a list of metrics that are predefined for evaluation and can be leveraged for easy access to common tracking performance measures.\"\"\"\n        \"Register a new metric.\\n\\n        Params\\n        ------\\n        fnc : Function\\n            Function that computes the metric to be registered. The number of arguments\\n            is 1 + N, where N is the number of dependencies of the metric to be registered.\\n            The order of the argument passed is `df, result_dep1, result_dep2, ...`.\\n\\n        Kwargs\\n        ------\\n        deps : string, list of strings or None, optional\\n            The dependencies of this metric. Each dependency is evaluated and the result\\n            is passed as argument to `fnc` as described above. If None is specified, the\\n            function does not have any dependencies. If a list of strings is given, dependencies\\n            for these metric strings are registered. If 'auto' is passed, the dependencies\\n            are deduced from argument inspection of the method. For this to work the argument\\n            names have to be equal to the intended dependencies.\\n        name : string or None, optional\\n            Name identifier of this metric. If None is passed the name is deduced from\\n            function inspection.\\n        helpstr : string or None, optional\\n            A description of what the metric computes. If no help message is given it\\n            is deduced from the docstring of the function.\\n        formatter: Format object, optional\\n            An optional default formatter when rendering metric results as string. I.e to\\n            render the result `0.35` as `35%` one would pass `{:.2%}.format`\\n        fnc_m : Function or None, optional\\n            Function that merges metric results. The number of arguments\\n            is 1 + N, where N is the number of dependencies of the metric to be registered.\\n            The order of the argument passed is `df, result_dep1, result_dep2, ...`.\\n        \"\n        assert fnc is not None, 'No function given for metric {}'.format(name)\n        if deps is None:\n            deps = []\n        elif deps == 'auto':\n            if _getargspec(fnc).defaults is not None:\n                k = -len(_getargspec(fnc).defaults)\n            else:\n                k = len(_getargspec(fnc).args)\n            deps = _getargspec(fnc).args[1:k]\n        if name is None:\n            name = fnc.__name__\n        if helpstr is None:\n            helpstr = inspect.getdoc(fnc) if inspect.getdoc(fnc) else 'No description.'\n            helpstr = ' '.join(helpstr.split())\n        if fnc_m is None and name + '_m' in globals():\n            fnc_m = globals()[name + '_m']\n        if fnc_m is not None:\n            if deps_m is None:\n                deps_m = []\n            elif deps_m == 'auto':\n                if _getargspec(fnc_m).defaults is not None:\n                    k = -len(_getargspec(fnc_m).defaults)\n                else:\n                    k = len(_getargspec(fnc_m).args)\n                deps_m = _getargspec(fnc_m).args[1:k]\n        else:\n            deps_m = None\n        self.metrics[name] = {'name': name, 'fnc': fnc, 'fnc_m': fnc_m, 'deps': deps, 'deps_m': deps_m, 'help': helpstr, 'formatter': formatter}",
        "docstring": "Register a new metric for evaluation in the MetricsHost class.\n\nThis method allows the registration of user-defined and built-in metrics that compute various performance measures on tracking data. Each metric is defined by a function which can have dependencies on other metrics, and it can provide a formatted output.\n\nParameters\n----------\nfnc : function\n    The function that computes the metric. It must accept at least one argument (the DataFrame) followed by any dependencies.\ndeps : str or list of str, optional\n    The dependencies of the metric, which can be automatically inferred from the function's arguments, specified as 'auto', or provided explicitly. If None, the metric has no dependencies.\nname : str or None, optional\n    A unique identifier for the metric. If not provided, it defaults to the function name.\nhelpstr : str or None, optional\n    A description of what the metric computes. If not provided, it defaults to the function's docstring.\nformatter : callable, optional\n    A format string for displaying the metric's result, such as \"{:.2%}.format\".\nfnc_m : function or None, optional\n    A merging function for the metric results, if applicable. If not provided and the derived name (name + \"_m\") exists in the global scope, it will be used.\ndeps_m : str or list of str, optional\n    Dependencies for the merging function, handled similarly to the `deps` parameter.\n\nReturns\n-------\nNone\n    The method registers the metric in the metrics dictionary without returning any value.\n\nNotes\n-----\n- The registered metrics can be listed and computed later using other methods in the MetricsHost class.\n- The `motchallenge_metrics` constant is a list of metrics that are predefined for evaluation and can be leveraged for easy access to common tracking performance measures.",
        "signature": "def register(self, fnc, deps='auto', name=None, helpstr=None, formatter=None, fnc_m=None, deps_m='auto'):",
        "type": "Method",
        "class_signature": "class MetricsHost:"
      },
      "MetricsHost.compute": {
        "code": "    def compute(self, df, ana=None, metrics=None, return_dataframe=True, return_cached=False, name=None):\n        \"\"\"Compute metrics on a given dataframe or accumulator.\n\nThis method calculates specified metrics based on input data, which can be a\nMOTAccumulator or a pandas DataFrame. It utilizes registered metric functions,\nevaluating dependencies as needed.\n\nParameters\n----------\ndf : MOTAccumulator or pandas.DataFrame\n    The dataframe or accumulator containing event data for metric computation.\nana : dict or None, optional\n    A dictionary for caching intermediate results to speed up calculations.\nmetrics : string, list of string or None, optional\n    The names of the metrics to be computed. If None, all registered metrics are calculated.\nreturn_dataframe : bool, optional\n    If True, the result is returned as a pandas DataFrame; otherwise, a dictionary is returned.\nreturn_cached : bool, optional\n    If True, all intermediate metric results required for the computed metrics are also included in the output.\nname : string, optional\n    The index for the resulting DataFrame row containing computed metrics; defaults to 0 if None.\n\nReturns\n-------\npd.DataFrame or dict\n    A collection of computed metrics as a DataFrame or dictionary, based on the `return_dataframe` parameter.\n\nDependencies\n------------\nThe method is dependent on the `events_to_df_map` function for converting the input dataframe\ninto a structured format, and it uses the `_compute` helper method to perform the actual calculations.\nIt also references the global constant `motchallenge_metrics` to determine default metrics when none are specified.\"\"\"\n        'Compute metrics on the dataframe / accumulator.\\n\\n        Params\\n        ------\\n        df : MOTAccumulator or pandas.DataFrame\\n            The dataframe to compute the metrics on\\n\\n        Kwargs\\n        ------\\n        ana: dict or None, optional\\n            To cache results for fast computation.\\n        metrics : string, list of string or None, optional\\n            The identifiers of the metrics to be computed. This method will only\\n            compute the minimal set of necessary metrics to fullfill the request.\\n            If None is passed all registered metrics are computed.\\n        return_dataframe : bool, optional\\n            Return the result as pandas.DataFrame (default) or dict.\\n        return_cached : bool, optional\\n           If true all intermediate metrics required to compute the desired metrics are returned as well.\\n        name : string, optional\\n            When returning a pandas.DataFrame this is the index of the row containing\\n            the computed metric values.\\n        '\n        if isinstance(df, MOTAccumulator):\n            df = df.events\n        if metrics is None:\n            metrics = motchallenge_metrics\n        elif isinstance(metrics, str):\n            metrics = [metrics]\n        df_map = events_to_df_map(df)\n        cache = {}\n        options = {'ana': ana}\n        for mname in metrics:\n            cache[mname] = self._compute(df_map, mname, cache, options, parent='summarize')\n        if name is None:\n            name = 0\n        if return_cached:\n            data = cache\n        else:\n            data = OrderedDict([(k, cache[k]) for k in metrics])\n        ret = pd.DataFrame(data, index=[name]) if return_dataframe else data\n        return ret",
        "docstring": "Compute metrics on a given dataframe or accumulator.\n\nThis method calculates specified metrics based on input data, which can be a\nMOTAccumulator or a pandas DataFrame. It utilizes registered metric functions,\nevaluating dependencies as needed.\n\nParameters\n----------\ndf : MOTAccumulator or pandas.DataFrame\n    The dataframe or accumulator containing event data for metric computation.\nana : dict or None, optional\n    A dictionary for caching intermediate results to speed up calculations.\nmetrics : string, list of string or None, optional\n    The names of the metrics to be computed. If None, all registered metrics are calculated.\nreturn_dataframe : bool, optional\n    If True, the result is returned as a pandas DataFrame; otherwise, a dictionary is returned.\nreturn_cached : bool, optional\n    If True, all intermediate metric results required for the computed metrics are also included in the output.\nname : string, optional\n    The index for the resulting DataFrame row containing computed metrics; defaults to 0 if None.\n\nReturns\n-------\npd.DataFrame or dict\n    A collection of computed metrics as a DataFrame or dictionary, based on the `return_dataframe` parameter.\n\nDependencies\n------------\nThe method is dependent on the `events_to_df_map` function for converting the input dataframe\ninto a structured format, and it uses the `_compute` helper method to perform the actual calculations.\nIt also references the global constant `motchallenge_metrics` to determine default metrics when none are specified.",
        "signature": "def compute(self, df, ana=None, metrics=None, return_dataframe=True, return_cached=False, name=None):",
        "type": "Method",
        "class_signature": "class MetricsHost:"
      },
      "MetricsHost._compute": {
        "code": "    def _compute(self, df_map, name, cache, options, parent=None):\n        \"\"\"Compute metrics based on the provided dataframe mappings and resolve any dependencies.\n\nParameters\n----------\ndf_map : DataFrameMap\n    A mapping of different views of the event data (full, raw, noraw, extra) extracted from a pandas DataFrame.\nname : str\n    The name identifier of the metric to compute, which must be registered in `self.metrics`.\ncache : dict\n    A cache for storing computed metric values to avoid redundant calculations.\noptions : dict\n    Additional options that may be required by the metric function.\nparent : str, optional\n    The name of the parent metric calling this computation, useful for error messaging.\n\nReturns\n-------\nThe computed metric value, which could be either a scalar or other data structure depending on the metric function specified in `self.metrics`.\n\nRaises\n------\nAssertionError\n    If the specified metric name is not found in `self.metrics`.\n\nNotes\n-----\n- This method retrieves the required values for any dependencies declared in `self.metrics[name]['deps']` and computes them recursively.\n- `_getargspec` is used to determine if the metric function requires any additional options. If it does, those options will be passed to the metric function during the calculation.\"\"\"\n        'Compute metric and resolve dependencies.'\n        assert name in self.metrics, 'Cannot find metric {} required by {}.'.format(name, parent)\n        already = cache.get(name, None)\n        if already is not None:\n            return already\n        minfo = self.metrics[name]\n        vals = []\n        for depname in minfo['deps']:\n            v = cache.get(depname, None)\n            if v is None:\n                v = cache[depname] = self._compute(df_map, depname, cache, options, parent=name)\n            vals.append(v)\n        if _getargspec(minfo['fnc']).defaults is None:\n            return minfo['fnc'](df_map, *vals)\n        else:\n            return minfo['fnc'](df_map, *vals, **options)",
        "docstring": "Compute metrics based on the provided dataframe mappings and resolve any dependencies.\n\nParameters\n----------\ndf_map : DataFrameMap\n    A mapping of different views of the event data (full, raw, noraw, extra) extracted from a pandas DataFrame.\nname : str\n    The name identifier of the metric to compute, which must be registered in `self.metrics`.\ncache : dict\n    A cache for storing computed metric values to avoid redundant calculations.\noptions : dict\n    Additional options that may be required by the metric function.\nparent : str, optional\n    The name of the parent metric calling this computation, useful for error messaging.\n\nReturns\n-------\nThe computed metric value, which could be either a scalar or other data structure depending on the metric function specified in `self.metrics`.\n\nRaises\n------\nAssertionError\n    If the specified metric name is not found in `self.metrics`.\n\nNotes\n-----\n- This method retrieves the required values for any dependencies declared in `self.metrics[name]['deps']` and computes them recursively.\n- `_getargspec` is used to determine if the metric function requires any additional options. If it does, those options will be passed to the metric function during the calculation.",
        "signature": "def _compute(self, df_map, name, cache, options, parent=None):",
        "type": "Method",
        "class_signature": "class MetricsHost:"
      },
      "events_to_df_map": {
        "code": "def events_to_df_map(df):\n    \"\"\"Create a mapping of different event types from a given DataFrame to facilitate metric computation.\n\nParameters\n----------\ndf : pandas.DataFrame\n    A DataFrame containing tracking event data, with a 'Type' column that classifies events.\n\nReturns\n-------\nDataFrameMap\n    An instance of the DataFrameMap class that contains:\n    - full: The original DataFrame with all events.\n    - raw: A DataFrame containing only RAW events.\n    - noraw: A DataFrame that excludes RAW, ASCEND, TRANSFER, and MIGRATE events, focusing on specific event types for analysis.\n    - extra: A DataFrame that includes all events except RAW.\n\nThe function is used to extract specific subsets of the tracking events from the provided DataFrame, enabling efficient computation of various tracking metrics by organizing the data based on event types.\"\"\"\n    raw = df[df.Type == 'RAW']\n    noraw = df[(df.Type != 'RAW') & (df.Type != 'ASCEND') & (df.Type != 'TRANSFER') & (df.Type != 'MIGRATE')]\n    extra = df[df.Type != 'RAW']\n    df_map = DataFrameMap(full=df, raw=raw, noraw=noraw, extra=extra)\n    return df_map",
        "docstring": "Create a mapping of different event types from a given DataFrame to facilitate metric computation.\n\nParameters\n----------\ndf : pandas.DataFrame\n    A DataFrame containing tracking event data, with a 'Type' column that classifies events.\n\nReturns\n-------\nDataFrameMap\n    An instance of the DataFrameMap class that contains:\n    - full: The original DataFrame with all events.\n    - raw: A DataFrame containing only RAW events.\n    - noraw: A DataFrame that excludes RAW, ASCEND, TRANSFER, and MIGRATE events, focusing on specific event types for analysis.\n    - extra: A DataFrame that includes all events except RAW.\n\nThe function is used to extract specific subsets of the tracking events from the provided DataFrame, enabling efficient computation of various tracking metrics by organizing the data based on event types.",
        "signature": "def events_to_df_map(df):",
        "type": "Function",
        "class_signature": null
      },
      "create": {
        "code": "def create():\n    \"\"\"Creates a MetricsHost instance and populates it with default metrics used for evaluating multiple object tracking (MOT) performance. The metrics registered include frame counts, object frequencies, match statistics, and various tracking accuracy measures.\n\nParameters\n----------\nNone\n\nReturns\n-------\nMetricsHost\n    An instance of the MetricsHost class, populated with specified metrics for tracking evaluations.\n\nDependencies\n------------\nThe function utilizes several metric functions such as `num_frames`, `obj_frequencies`, and `motp`, each defined elsewhere in the code, that compute specific tracking metrics based on input data frames. The `motchallenge_metrics` constant is a list of strings that identifies the relevant metrics for a standard evaluation, ensuring that the appropriate metrics are registered for comprehensive tracking analysis.\"\"\"\n    'Creates a MetricsHost and populates it with default metrics.'\n    m = MetricsHost()\n    m.register(num_frames, formatter='{:d}'.format)\n    m.register(obj_frequencies, formatter='{:d}'.format)\n    m.register(pred_frequencies, formatter='{:d}'.format)\n    m.register(num_matches, formatter='{:d}'.format)\n    m.register(num_switches, formatter='{:d}'.format)\n    m.register(num_transfer, formatter='{:d}'.format)\n    m.register(num_ascend, formatter='{:d}'.format)\n    m.register(num_migrate, formatter='{:d}'.format)\n    m.register(num_false_positives, formatter='{:d}'.format)\n    m.register(num_misses, formatter='{:d}'.format)\n    m.register(num_detections, formatter='{:d}'.format)\n    m.register(num_objects, formatter='{:d}'.format)\n    m.register(num_predictions, formatter='{:d}'.format)\n    m.register(num_gt_ids, formatter='{:d}'.format)\n    m.register(num_dt_ids, formatter='{:d}'.format)\n    m.register(num_unique_objects, formatter='{:d}'.format)\n    m.register(track_ratios)\n    m.register(mostly_tracked, formatter='{:d}'.format)\n    m.register(partially_tracked, formatter='{:d}'.format)\n    m.register(mostly_lost, formatter='{:d}'.format)\n    m.register(num_fragmentations)\n    m.register(motp, formatter='{:.3f}'.format)\n    m.register(mota, formatter='{:.1%}'.format)\n    m.register(precision, formatter='{:.1%}'.format)\n    m.register(recall, formatter='{:.1%}'.format)\n    m.register(id_global_assignment)\n    m.register(idfp)\n    m.register(idfn)\n    m.register(idtp)\n    m.register(idp, formatter='{:.1%}'.format)\n    m.register(idr, formatter='{:.1%}'.format)\n    m.register(idf1, formatter='{:.1%}'.format)\n    m.register(deta_alpha, formatter='{:.1%}'.format)\n    m.register(assa_alpha, formatter='{:.1%}'.format)\n    m.register(hota_alpha, formatter='{:.1%}'.format)\n    return m",
        "docstring": "Creates a MetricsHost instance and populates it with default metrics used for evaluating multiple object tracking (MOT) performance. The metrics registered include frame counts, object frequencies, match statistics, and various tracking accuracy measures.\n\nParameters\n----------\nNone\n\nReturns\n-------\nMetricsHost\n    An instance of the MetricsHost class, populated with specified metrics for tracking evaluations.\n\nDependencies\n------------\nThe function utilizes several metric functions such as `num_frames`, `obj_frequencies`, and `motp`, each defined elsewhere in the code, that compute specific tracking metrics based on input data frames. The `motchallenge_metrics` constant is a list of strings that identifies the relevant metrics for a standard evaluation, ensuring that the appropriate metrics are registered for comprehensive tracking analysis.",
        "signature": "def create():",
        "type": "Function",
        "class_signature": null
      }
    },
    "motmetrics/utils.py": {
      "compare_to_groundtruth": {
        "code": "def compare_to_groundtruth(gt, dt, dist='iou', distfields=None, distth=0.5):\n    \"\"\"Compare ground truth and detector results using a specified distance metric.\n\nThis function takes two pandas DataFrames: one for ground truth (`gt`) and one for detector results (`dt`). It computes distances between the detected objects and ground truth objects based on specified fields, updating a `MOTAccumulator` with the results.\n\nParameters\n----------\ngt : pd.DataFrame\n    DataFrame containing ground truth data with at least 'FrameId' and 'Id' as indices.\ndt : pd.DataFrame\n    DataFrame containing detector results with at least 'FrameId' and 'Id' as indices.\ndist : str, optional\n    Distance metric to use for comparisons. Defaults to 'iou'. Supported options include 'iou', 'euclidean', and 'seuc' (squared euclidean distance).\ndistfields : array, optional\n    Specific fields to be used for calculating distances. Defaults to ['X', 'Y', 'Width', 'Height'].\ndistth : float, optional\n    Maximum tolerable distance for pairs, beyond which they are marked as 'do-not-pair'. Default is 0.5.\n\nReturns\n-------\nMOTAccumulator\n    An accumulator object that contains the results of the comparison, including distances between ground truth and detected objects, along with the corresponding frame IDs.\n\nNotes\n-----\n- The distance calculations leverage helper functions defined within the method, such as `compute_iou`, `compute_euc`, and `compute_seuc`, to obtain necessary distance matrices using imported functions from `motmetrics.distances`.\n- If an unknown distance metric is provided, a RuntimeError is raised with an appropriate message.\n- This function assumes proper formatting of input DataFrames and handles discrepancies in the frame IDs between ground truth and detection results, allowing for comprehensive evaluation of the detection performance.\"\"\"\n    \"Compare groundtruth and detector results.\\n\\n    This method assumes both results are given in terms of DataFrames with at least the following fields\\n     - `FrameId` First level index used for matching ground-truth and test frames.\\n     - `Id` Secondary level index marking available object / hypothesis ids\\n\\n    Depending on the distance to be used relevant distfields need to be specified.\\n\\n    Params\\n    ------\\n    gt : pd.DataFrame\\n        Dataframe for ground-truth\\n    test : pd.DataFrame\\n        Dataframe for detector results\\n\\n    Kwargs\\n    ------\\n    dist : str, optional\\n        String identifying distance to be used. Defaults to intersection over union ('iou'). Euclidean\\n        distance ('euclidean') and squared euclidean distance ('seuc') are also supported.\\n    distfields: array, optional\\n        Fields relevant for extracting distance information. Defaults to ['X', 'Y', 'Width', 'Height']\\n    distth: float, optional\\n        Maximum tolerable distance. Pairs exceeding this threshold are marked 'do-not-pair'.\\n    \"\n    if distfields is None:\n        distfields = ['X', 'Y', 'Width', 'Height']\n\n    def compute_iou(a, b):\n        return iou_matrix(a, b, max_iou=distth)\n\n    def compute_euc(a, b):\n        \"\"\"Calculates the Euclidean distance between two sets of points.\n\nParams\n------\na : ndarray\n    A 2D array representing the first set of points.\nb : ndarray\n    A 2D array representing the second set of points.\n\nReturns\n-------\nndarray\n    A 2D array of distances, where each element represents the Euclidean distance between pairs of points from the two input arrays.\n\nNotes\n-----\nThis function relies on the `norm2squared_matrix` function, which computes the squared Euclidean distance. The calculated distances are then square-rooted to obtain the final Euclidean distances. The parameter `distth`, which is defined outside this function, specifies the maximum tolerable distance and is squared in the call to `norm2squared_matrix`.\"\"\"\n        return np.sqrt(norm2squared_matrix(a, b, max_d2=distth ** 2))\n\n    def compute_seuc(a, b):\n        return norm2squared_matrix(a, b, max_d2=distth)\n    if dist.upper() == 'IOU':\n        compute_dist = compute_iou\n    elif dist.upper() == 'EUC':\n        compute_dist = compute_euc\n        import warnings\n        warnings.warn(f\"'euc' flag changed its behavior. The euclidean distance is now used instead of the squared euclidean distance. Make sure the used threshold (distth={distth}) is not squared. Use 'euclidean' flag to avoid this warning.\")\n    elif dist.upper() == 'EUCLIDEAN':\n        compute_dist = compute_euc\n    elif dist.upper() == 'SEUC':\n        compute_dist = compute_seuc\n    else:\n        raise f'Unknown distance metric {dist}. Use \"IOU\", \"EUCLIDEAN\",  or \"SEUC\"'\n    acc = MOTAccumulator()\n    allframeids = gt.index.union(dt.index).levels[0]\n    gt = gt[distfields]\n    dt = dt[distfields]\n    fid_to_fgt = dict(iter(gt.groupby('FrameId')))\n    fid_to_fdt = dict(iter(dt.groupby('FrameId')))\n    for fid in allframeids:\n        oids = np.empty(0)\n        hids = np.empty(0)\n        dists = np.empty((0, 0))\n        if fid in fid_to_fgt:\n            fgt = fid_to_fgt[fid]\n            oids = fgt.index.get_level_values('Id')\n        if fid in fid_to_fdt:\n            fdt = fid_to_fdt[fid]\n            hids = fdt.index.get_level_values('Id')\n        if len(oids) > 0 and len(hids) > 0:\n            dists = compute_dist(fgt.values, fdt.values)\n        acc.update(oids, hids, dists, frameid=fid)\n    return acc",
        "docstring": "Compare ground truth and detector results using a specified distance metric.\n\nThis function takes two pandas DataFrames: one for ground truth (`gt`) and one for detector results (`dt`). It computes distances between the detected objects and ground truth objects based on specified fields, updating a `MOTAccumulator` with the results.\n\nParameters\n----------\ngt : pd.DataFrame\n    DataFrame containing ground truth data with at least 'FrameId' and 'Id' as indices.\ndt : pd.DataFrame\n    DataFrame containing detector results with at least 'FrameId' and 'Id' as indices.\ndist : str, optional\n    Distance metric to use for comparisons. Defaults to 'iou'. Supported options include 'iou', 'euclidean', and 'seuc' (squared euclidean distance).\ndistfields : array, optional\n    Specific fields to be used for calculating distances. Defaults to ['X', 'Y', 'Width', 'Height'].\ndistth : float, optional\n    Maximum tolerable distance for pairs, beyond which they are marked as 'do-not-pair'. Default is 0.5.\n\nReturns\n-------\nMOTAccumulator\n    An accumulator object that contains the results of the comparison, including distances between ground truth and detected objects, along with the corresponding frame IDs.\n\nNotes\n-----\n- The distance calculations leverage helper functions defined within the method, such as `compute_iou`, `compute_euc`, and `compute_seuc`, to obtain necessary distance matrices using imported functions from `motmetrics.distances`.\n- If an unknown distance metric is provided, a RuntimeError is raised with an appropriate message.\n- This function assumes proper formatting of input DataFrames and handles discrepancies in the frame IDs between ground truth and detection results, allowing for comprehensive evaluation of the detection performance.",
        "signature": "def compare_to_groundtruth(gt, dt, dist='iou', distfields=None, distth=0.5):",
        "type": "Function",
        "class_signature": null
      },
      "compute_euc": {
        "code": "    def compute_euc(a, b):\n        \"\"\"Compute the Euclidean distance between two sets of points.\n\nThis function calculates the squared Euclidean distance between points in arrays `a` and `b`, using the `norm2squared_matrix` function from the `motmetrics.distances` module. The calculation respects the maximum squared distance threshold defined by `max_d2`, which is derived from the `distth` variable (expected to be a float). This function is typically used in the context of evaluating object tracking performance metrics.\n\nParameters\n----------\na : np.ndarray\n    An array representing the first set of points.\nb : np.ndarray\n    An array representing the second set of points.\n\nReturns\n-------\nnp.ndarray\n    A squared distance matrix computed between the specified points in `a` and `b`.\"\"\"\n        return norm2squared_matrix(a, b, max_d2=distth)",
        "docstring": "Compute the Euclidean distance between two sets of points.\n\nThis function calculates the squared Euclidean distance between points in arrays `a` and `b`, using the `norm2squared_matrix` function from the `motmetrics.distances` module. The calculation respects the maximum squared distance threshold defined by `max_d2`, which is derived from the `distth` variable (expected to be a float). This function is typically used in the context of evaluating object tracking performance metrics.\n\nParameters\n----------\na : np.ndarray\n    An array representing the first set of points.\nb : np.ndarray\n    An array representing the second set of points.\n\nReturns\n-------\nnp.ndarray\n    A squared distance matrix computed between the specified points in `a` and `b`.",
        "signature": "def compute_euc(a, b):",
        "type": "Function",
        "class_signature": null
      }
    }
  },
  "dependency_dict": {
    "motmetrics/utils.py:compare_to_groundtruth": {},
    "motmetrics/mot.py:MOTAccumulator:__init__": {
      "motmetrics/mot.py": {
        "MOTAccumulator.reset": {
          "code": "    def reset(self):\n        \"\"\"Reset the accumulator to empty state.\"\"\"\n        self._events = {field: [] for field in _EVENT_FIELDS}\n        self._indices = {field: [] for field in _INDEX_FIELDS}\n        self.m = {}\n        self.res_m = {}\n        self.last_occurrence = {}\n        self.last_match = {}\n        self.hypHistory = {}\n        self.dirty_events = True\n        self.cached_events_df = None\n        self.last_update_frameid = None",
          "docstring": "Reset the accumulator to empty state.",
          "signature": "def reset(self):",
          "type": "Method",
          "class_signature": "class MOTAccumulator(object):"
        }
      }
    },
    "motmetrics/mot.py:MOTAccumulator:update": {
      "motmetrics/mot.py": {
        "MOTAccumulator._append_to_indices": {
          "code": "    def _append_to_indices(self, frameid, eid):\n        self._indices['FrameId'].append(frameid)\n        self._indices['Event'].append(eid)",
          "docstring": "",
          "signature": "def _append_to_indices(self, frameid, eid):",
          "type": "Method",
          "class_signature": "class MOTAccumulator(object):"
        },
        "MOTAccumulator._append_to_events": {
          "code": "    def _append_to_events(self, typestr, oid, hid, distance):\n        self._events['Type'].append(typestr)\n        self._events['OId'].append(oid)\n        self._events['HId'].append(hid)\n        self._events['D'].append(distance)",
          "docstring": "",
          "signature": "def _append_to_events(self, typestr, oid, hid, distance):",
          "type": "Method",
          "class_signature": "class MOTAccumulator(object):"
        }
      },
      "motmetrics/lap.py": {
        "linear_sum_assignment": {
          "code": "def linear_sum_assignment(costs, solver=None):\n    \"\"\"Solve a linear sum assignment problem (LSA).\n\n    For large datasets solving the minimum cost assignment becomes the dominant runtime part.\n    We therefore support various solvers out of the box (currently lapsolver, scipy, ortools, munkres)\n\n    Params\n    ------\n    costs : np.array\n        numpy matrix containing costs. Use NaN/Inf values for unassignable\n        row/column pairs.\n\n    Kwargs\n    ------\n    solver : callable or str, optional\n        When str: name of solver to use.\n        When callable: function to invoke\n        When None: uses first available solver\n    \"\"\"\n    costs = np.asarray(costs)\n    if not costs.size:\n        return np.array([], dtype=int), np.array([], dtype=int)\n\n    solver = solver or default_solver\n\n    if isinstance(solver, str):\n        # Try resolve from string\n        solver = solver_map.get(solver, None)\n\n    assert callable(solver), 'Invalid LAP solver.'\n    rids, cids = solver(costs)\n    rids = np.asarray(rids).astype(int)\n    cids = np.asarray(cids).astype(int)\n    return rids, cids",
          "docstring": "Solve a linear sum assignment problem (LSA).\n\nFor large datasets solving the minimum cost assignment becomes the dominant runtime part.\nWe therefore support various solvers out of the box (currently lapsolver, scipy, ortools, munkres)\n\nParams\n------\ncosts : np.array\n    numpy matrix containing costs. Use NaN/Inf values for unassignable\n    row/column pairs.\n\nKwargs\n------\nsolver : callable or str, optional\n    When str: name of solver to use.\n    When callable: function to invoke\n    When None: uses first available solver",
          "signature": "def linear_sum_assignment(costs, solver=None):",
          "type": "Function",
          "class_signature": null
        }
      }
    },
    "motmetrics/utils.py:compute_euc": {
      "motmetrics/distances.py": {
        "norm2squared_matrix": {
          "code": "def norm2squared_matrix(objs, hyps, max_d2=float('inf')):\n    \"\"\"Computes the squared Euclidean distance matrix between object and hypothesis points.\n\n    Params\n    ------\n    objs : NxM array\n        Object points of dim M in rows\n    hyps : KxM array\n        Hypothesis points of dim M in rows\n\n    Kwargs\n    ------\n    max_d2 : float\n        Maximum tolerable squared Euclidean distance. Object / hypothesis points\n        with larger distance are set to np.nan signalling do-not-pair. Defaults\n        to +inf\n\n    Returns\n    -------\n    C : NxK array\n        Distance matrix containing pairwise distances or np.nan.\n    \"\"\"\n\n    objs = np.atleast_2d(objs).astype(float)\n    hyps = np.atleast_2d(hyps).astype(float)\n\n    if objs.size == 0 or hyps.size == 0:\n        return np.empty((0, 0))\n\n    assert hyps.shape[1] == objs.shape[1], \"Dimension mismatch\"\n\n    delta = objs[:, np.newaxis] - hyps[np.newaxis, :]\n    C = np.sum(delta ** 2, axis=-1)\n\n    C[C > max_d2] = np.nan\n    return C",
          "docstring": "Computes the squared Euclidean distance matrix between object and hypothesis points.\n\nParams\n------\nobjs : NxM array\n    Object points of dim M in rows\nhyps : KxM array\n    Hypothesis points of dim M in rows\n\nKwargs\n------\nmax_d2 : float\n    Maximum tolerable squared Euclidean distance. Object / hypothesis points\n    with larger distance are set to np.nan signalling do-not-pair. Defaults\n    to +inf\n\nReturns\n-------\nC : NxK array\n    Distance matrix containing pairwise distances or np.nan.",
          "signature": "def norm2squared_matrix(objs, hyps, max_d2=float('inf')):",
          "type": "Function",
          "class_signature": null
        }
      }
    },
    "motmetrics/metrics.py:create": {},
    "motmetrics/metrics.py:MetricsHost:__init__": {},
    "motmetrics/metrics.py:MetricsHost:register": {},
    "motmetrics/metrics.py:MetricsHost:compute": {},
    "motmetrics/mot.py:MOTAccumulator:MOTAccumulator": {},
    "motmetrics/mot.py:MOTAccumulator:events": {
      "motmetrics/mot.py": {
        "MOTAccumulator.new_event_dataframe_with_data": {
          "code": "    def new_event_dataframe_with_data(indices, events):\n        \"\"\"Create a new DataFrame filled with data.\n\n        Params\n        ------\n        indices: dict\n            dict of lists with fields 'FrameId' and 'Event'\n        events: dict\n            dict of lists with fields 'Type', 'OId', 'HId', 'D'\n        \"\"\"\n        if len(events) == 0:\n            return MOTAccumulator.new_event_dataframe()\n        raw_type = pd.Categorical(events['Type'], categories=['RAW', 'FP', 'MISS', 'SWITCH', 'MATCH', 'TRANSFER', 'ASCEND', 'MIGRATE'], ordered=False)\n        series = [pd.Series(raw_type, name='Type'), pd.Series(events['OId'], dtype=float, name='OId'), pd.Series(events['HId'], dtype=float, name='HId'), pd.Series(events['D'], dtype=float, name='D')]\n        idx = pd.MultiIndex.from_arrays([indices[field] for field in _INDEX_FIELDS], names=_INDEX_FIELDS)\n        df = pd.concat(series, axis=1)\n        df.index = idx\n        return df",
          "docstring": "Create a new DataFrame filled with data.\n\nParams\n------\nindices: dict\n    dict of lists with fields 'FrameId' and 'Event'\nevents: dict\n    dict of lists with fields 'Type', 'OId', 'HId', 'D'",
          "signature": "def new_event_dataframe_with_data(indices, events):",
          "type": "Method",
          "class_signature": "class MOTAccumulator(object):"
        }
      }
    },
    "motmetrics/metrics.py:events_to_df_map": {
      "motmetrics/metrics.py": {
        "DataFrameMap.__init__": {
          "code": "    def __init__(self, full, raw, noraw, extra):\n        self.full = full\n        self.raw = raw\n        self.noraw = noraw\n        self.extra = extra",
          "docstring": "",
          "signature": "def __init__(self, full, raw, noraw, extra):",
          "type": "Method",
          "class_signature": "class DataFrameMap:"
        }
      }
    },
    "motmetrics/metrics.py:MetricsHost:_compute": {
      "motmetrics/metrics.py": {
        "num_unique_objects": {
          "code": "def num_unique_objects(df, obj_frequencies):\n    \"\"\"Total number of unique object ids encountered.\"\"\"\n    del df\n    return len(obj_frequencies)",
          "docstring": "Total number of unique object ids encountered.",
          "signature": "def num_unique_objects(df, obj_frequencies):",
          "type": "Function",
          "class_signature": null
        },
        "num_objects": {
          "code": "def num_objects(df, obj_frequencies):\n    \"\"\"Total number of unique object appearances over all frames.\"\"\"\n    del df\n    return obj_frequencies.sum()",
          "docstring": "Total number of unique object appearances over all frames.",
          "signature": "def num_objects(df, obj_frequencies):",
          "type": "Function",
          "class_signature": null
        },
        "num_predictions": {
          "code": "def num_predictions(df, pred_frequencies):\n    \"\"\"Total number of unique prediction appearances over all frames.\"\"\"\n    del df\n    return pred_frequencies.sum()",
          "docstring": "Total number of unique prediction appearances over all frames.",
          "signature": "def num_predictions(df, pred_frequencies):",
          "type": "Function",
          "class_signature": null
        }
      }
    }
  },
  "call_tree": {
    "modified_testcases/test_utils.py:test_annotations_xor_predictions_present": {
      "modified_testcases/test_utils.py:_tracks_to_dataframe": {},
      "motmetrics/utils.py:compare_to_groundtruth": {
        "motmetrics/mot.py:MOTAccumulator:__init__": {
          "motmetrics/mot.py:MOTAccumulator:reset": {}
        },
        "motmetrics/mot.py:MOTAccumulator:update": {
          "motmetrics/mot.py:MOTAccumulator:_append_to_indices": {},
          "motmetrics/mot.py:MOTAccumulator:_append_to_events": {},
          "motmetrics/lap.py:linear_sum_assignment": {
            "motmetrics/lap.py:lsa_solve_scipy": {
              "motmetrics/lap.py:add_expensive_edges": {},
              "motmetrics/lap.py:_exclude_missing_edges": {}
            }
          }
        },
        "motmetrics/utils.py:compute_euc": {
          "motmetrics/distances.py:norm2squared_matrix": {}
        }
      },
      "motmetrics/metrics.py:create": {
        "motmetrics/metrics.py:MetricsHost:__init__": {},
        "motmetrics/metrics.py:MetricsHost:register": {}
      },
      "motmetrics/metrics.py:MetricsHost:compute": {
        "motmetrics/mot.py:MOTAccumulator:MOTAccumulator": {},
        "motmetrics/mot.py:MOTAccumulator:events": {
          "motmetrics/mot.py:MOTAccumulator:new_event_dataframe_with_data": {}
        },
        "motmetrics/metrics.py:events_to_df_map": {
          "motmetrics/metrics.py:DataFrameMap:__init__": {}
        },
        "motmetrics/metrics.py:MetricsHost:_compute": {
          "motmetrics/metrics.py:MetricsHost:_compute": {
            "[ignored_or_cut_off]": "..."
          },
          "motmetrics/metrics.py:num_objects": {},
          "motmetrics/metrics.py:num_predictions": {},
          "motmetrics/metrics.py:num_unique_objects": {}
        }
      }
    }
  },
  "PRD": "# PROJECT NAME: motmetrics-test_utils\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 motmetrics/\n    \u251c\u2500\u2500 metrics.py\n    \u2502   \u251c\u2500\u2500 MetricsHost.__init__\n    \u2502   \u251c\u2500\u2500 MetricsHost._compute\n    \u2502   \u251c\u2500\u2500 MetricsHost.compute\n    \u2502   \u251c\u2500\u2500 MetricsHost.register\n    \u2502   \u251c\u2500\u2500 create\n    \u2502   \u2514\u2500\u2500 events_to_df_map\n    \u251c\u2500\u2500 mot.py\n    \u2502   \u251c\u2500\u2500 MOTAccumulator.MOTAccumulator\n    \u2502   \u251c\u2500\u2500 MOTAccumulator.__init__\n    \u2502   \u251c\u2500\u2500 MOTAccumulator.events\n    \u2502   \u2514\u2500\u2500 MOTAccumulator.update\n    \u2514\u2500\u2500 utils.py\n        \u251c\u2500\u2500 compare_to_groundtruth\n        \u2514\u2500\u2500 compute_euc\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThis module provides functionality for benchmarking multiple object tracking (MOT) algorithms by comparing predicted object trajectories with annotated ground truth data. It evaluates the performance of trackers using a set of defined metrics, such as the total number of objects, predictions, and unique objects, across frames. By facilitating the accumulation and analysis of event data, it simplifies performance evaluation and enables consistent, metric-driven comparisons of MOT systems. This solves the challenge of accurately measuring tracker performance, reducing the effort for developers and researchers to validate and fine-tune their solutions in a standardized way.\n\n## FILE 1: motmetrics/mot.py\n\n- CLASS METHOD: MOTAccumulator.events\n  - CLASS SIGNATURE: class MOTAccumulator(object):\n  - SIGNATURE: def events(self):\n  - DOCSTRING: \n```python\n\"\"\"\nReturn the DataFrame containing tracking events.\n\nThis property retrieves the accumulated tracking events in the form of a pandas DataFrame.\nIf the events data is marked as dirty (i.e., `_dirty_events` is `True`), it invokes the \ncreation of a new DataFrame using `MOTAccumulator.new_event_dataframe_with_data`, \nwhich populates the DataFrame with event data based on the internal state of the accumulator, \nnamely `_indices` and `_events`. The `_indices` dictionary contains frame IDs and event IDs, \nwhile the `_events` dictionary stores details about each event, including type, object ID, \nhypothesis ID, and distance.\n\nReturns\n-------\npd.DataFrame\n    A DataFrame comprising the various tracking events, with hierarchically indexed by \n    `FrameId` and `EventId`, and columns for event type, object ID, hypothesis ID, \n    and distance.\n\"\"\"\n```\n\n- CLASS METHOD: MOTAccumulator.update\n  - CLASS SIGNATURE: class MOTAccumulator(object):\n  - SIGNATURE: def update(self, oids, hids, dists, frameid=None, vf='', similartiy_matrix=None, th=None):\n  - DOCSTRING: \n```python\n\"\"\"\nUpdates the MOTAccumulator with frame-specific object detections and generates tracking events.\n\nThis method processes arrays of object and hypothesis IDs, along with a distance matrix, to determine the relationships between objects and hypotheses for a given frame. It produces various tracking events including 'MATCH', 'SWITCH', 'MISS', and 'FP' based on defined criteria. The algorithm includes a phase for re-establishing previously tracked objects and minimizing distance errors using the Kuhn-Munkres algorithm.\n\nParameters\n----------\noids : ndarray\n    Array of object IDs for the current frame.\nhids : ndarray\n    Array of hypothesis IDs for the current frame.\ndists : ndarray\n    Distance matrix of shape (N, M), where N is the number of objects and M is the number of hypotheses, indicating distances between each object and hypothesis. NaN values indicate do-not-pair conditions.\nframeid : int, optional\n    Unique identifier for the current frame. Required if `auto_id` is not enabled.\nvf : str, optional\n    File path for logging details of events.\nsimilartiy_matrix : ndarray, optional\n    Similarity matrix used for distance calculation; if provided, will influence distance handling.\nth : float, optional\n    Threshold for the similarity matrix to determine valid pairings.\n\nReturns\n-------\nint\n    The updated frame ID after processing the current frame.\n\nNotes\n-----\nThe method relies on instance variables such as `self.m`, `self.last_match`, `self.last_occurrence`, and others, which maintain state across frames. It generates a pandas DataFrame from collected events that can be accessed via the `events` property. The use of `_append_to_indices` and `_append_to_events` methods is critical for recording event data.\n\nAdditionally, constants defined in the class, such as event types (e.g., 'MATCH', 'SWITCH'), guide the categorization of event types.\n\"\"\"\n```\n\n- CLASS METHOD: MOTAccumulator.__init__\n  - CLASS SIGNATURE: class MOTAccumulator(object):\n  - SIGNATURE: def __init__(self, auto_id=False, max_switch_time=float('inf')):\n  - DOCSTRING: \n```python\n\"\"\"\nInitializes a `MOTAccumulator` instance for managing and accumulating tracking events in multi-object tracking scenarios. \n\nParameters\n----------\nauto_id : bool, optional\n    If set to `True`, the frame indices will be auto-incremented. Providing a frame ID during updates will result in an error. Defaults to `False`.\nmax_switch_time : float, optional\n    This defines the maximum time span for which unobserved, tracked objects can generate track switch events. It helps maintain object IDs for objects reappearing after leaving the field of view. Default is set to infinity (no upper bound).\n\nAttributes\n----------\nauto_id : bool\n    Indicates if frame indices are auto-incremented.\nmax_switch_time : float\n    The allowable timespan for track switch events.\n_events : dict\n    Stores event data (like type and IDs).\n_indices : dict\n    Keeps track of indices for events, structured by 'FrameId' and 'Event'.\nm : dict\n    Holds the pairings of objects and hypotheses at the current timestamp.\nres_m : dict\n    Tracks result pairings across all frames.\nlast_occurrence : dict\n    Stores the most recent occurrence of each object.\nlast_match : dict\n    Keeps track of the last match for each object.\nhypHistory : dict\n    Maintains history of hypothesis occurrences.\ndirty_events : bool\n    Indicates if the events need recalculating.\ncached_events_df : pd.DataFrame or None\n    Caches the DataFrame of events to improve performance.\nlast_update_frameid : int or None\n    Records the last updated frame ID.\n\nDependencies\n------------\nThis class interacts with other methods and classes in the `motmetrics` library, particularly with the event summarization and statistics computations.\n\"\"\"\n```\n\n## FILE 2: motmetrics/metrics.py\n\n- FUNCTION NAME: events_to_df_map\n  - SIGNATURE: def events_to_df_map(df):\n  - DOCSTRING: \n```python\n\"\"\"\nCreate a mapping of different event types from a given DataFrame to facilitate metric computation.\n\nParameters\n----------\ndf : pandas.DataFrame\n    A DataFrame containing tracking event data, with a 'Type' column that classifies events.\n\nReturns\n-------\nDataFrameMap\n    An instance of the DataFrameMap class that contains:\n    - full: The original DataFrame with all events.\n    - raw: A DataFrame containing only RAW events.\n    - noraw: A DataFrame that excludes RAW, ASCEND, TRANSFER, and MIGRATE events, focusing on specific event types for analysis.\n    - extra: A DataFrame that includes all events except RAW.\n\nThe function is used to extract specific subsets of the tracking events from the provided DataFrame, enabling efficient computation of various tracking metrics by organizing the data based on event types.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - motmetrics/metrics.py:DataFrameMap:__init__\n    - motmetrics/metrics.py:MetricsHost:compute\n\n- CLASS METHOD: MetricsHost.__init__\n  - CLASS SIGNATURE: class MetricsHost:\n  - SIGNATURE: def __init__(self):\n  - DOCSTRING: \n```python\n\"\"\"\nInitializes an instance of the MetricsHost class, which is responsible for managing metrics and their dependencies within the context of multiple object tracking (MOT) benchmarking.\n\nAttributes\n----------\nmetrics : OrderedDict\n    An ordered dictionary that stores registered metrics. Each metric is represented by a key-value pair where the key is the name of the metric and the value is a dictionary containing its properties, such as the metric function, its dependencies, and formatting options.\n\nThis class interacts with various metric functions defined in the same module, allowing the addition, retrieval, and computation of metrics related to object tracking analysis.\n\"\"\"\n```\n\n- CLASS METHOD: MetricsHost.register\n  - CLASS SIGNATURE: class MetricsHost:\n  - SIGNATURE: def register(self, fnc, deps='auto', name=None, helpstr=None, formatter=None, fnc_m=None, deps_m='auto'):\n  - DOCSTRING: \n```python\n\"\"\"\nRegister a new metric for evaluation in the MetricsHost class.\n\nThis method allows the registration of user-defined and built-in metrics that compute various performance measures on tracking data. Each metric is defined by a function which can have dependencies on other metrics, and it can provide a formatted output.\n\nParameters\n----------\nfnc : function\n    The function that computes the metric. It must accept at least one argument (the DataFrame) followed by any dependencies.\ndeps : str or list of str, optional\n    The dependencies of the metric, which can be automatically inferred from the function's arguments, specified as 'auto', or provided explicitly. If None, the metric has no dependencies.\nname : str or None, optional\n    A unique identifier for the metric. If not provided, it defaults to the function name.\nhelpstr : str or None, optional\n    A description of what the metric computes. If not provided, it defaults to the function's docstring.\nformatter : callable, optional\n    A format string for displaying the metric's result, such as \"{:.2%}.format\".\nfnc_m : function or None, optional\n    A merging function for the metric results, if applicable. If not provided and the derived name (name + \"_m\") exists in the global scope, it will be used.\ndeps_m : str or list of str, optional\n    Dependencies for the merging function, handled similarly to the `deps` parameter.\n\nReturns\n-------\nNone\n    The method registers the metric in the metrics dictionary without returning any value.\n\nNotes\n-----\n- The registered metrics can be listed and computed later using other methods in the MetricsHost class.\n- The `motchallenge_metrics` constant is a list of metrics that are predefined for evaluation and can be leveraged for easy access to common tracking performance measures.\n\"\"\"\n```\n\n- CLASS METHOD: MetricsHost._compute\n  - CLASS SIGNATURE: class MetricsHost:\n  - SIGNATURE: def _compute(self, df_map, name, cache, options, parent=None):\n  - DOCSTRING: \n```python\n\"\"\"\nCompute metrics based on the provided dataframe mappings and resolve any dependencies.\n\nParameters\n----------\ndf_map : DataFrameMap\n    A mapping of different views of the event data (full, raw, noraw, extra) extracted from a pandas DataFrame.\nname : str\n    The name identifier of the metric to compute, which must be registered in `self.metrics`.\ncache : dict\n    A cache for storing computed metric values to avoid redundant calculations.\noptions : dict\n    Additional options that may be required by the metric function.\nparent : str, optional\n    The name of the parent metric calling this computation, useful for error messaging.\n\nReturns\n-------\nThe computed metric value, which could be either a scalar or other data structure depending on the metric function specified in `self.metrics`.\n\nRaises\n------\nAssertionError\n    If the specified metric name is not found in `self.metrics`.\n\nNotes\n-----\n- This method retrieves the required values for any dependencies declared in `self.metrics[name]['deps']` and computes them recursively.\n- `_getargspec` is used to determine if the metric function requires any additional options. If it does, those options will be passed to the metric function during the calculation.\n\"\"\"\n```\n\n- FUNCTION NAME: create\n  - SIGNATURE: def create():\n  - DOCSTRING: \n```python\n\"\"\"\nCreates a MetricsHost instance and populates it with default metrics used for evaluating multiple object tracking (MOT) performance. The metrics registered include frame counts, object frequencies, match statistics, and various tracking accuracy measures.\n\nParameters\n----------\nNone\n\nReturns\n-------\nMetricsHost\n    An instance of the MetricsHost class, populated with specified metrics for tracking evaluations.\n\nDependencies\n------------\nThe function utilizes several metric functions such as `num_frames`, `obj_frequencies`, and `motp`, each defined elsewhere in the code, that compute specific tracking metrics based on input data frames. The `motchallenge_metrics` constant is a list of strings that identifies the relevant metrics for a standard evaluation, ensuring that the appropriate metrics are registered for comprehensive tracking analysis.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - motmetrics/metrics.py:MetricsHost:register\n    - motmetrics/metrics.py:MetricsHost:__init__\n\n- CLASS METHOD: MetricsHost.compute\n  - CLASS SIGNATURE: class MetricsHost:\n  - SIGNATURE: def compute(self, df, ana=None, metrics=None, return_dataframe=True, return_cached=False, name=None):\n  - DOCSTRING: \n```python\n\"\"\"\nCompute metrics on a given dataframe or accumulator.\n\nThis method calculates specified metrics based on input data, which can be a\nMOTAccumulator or a pandas DataFrame. It utilizes registered metric functions,\nevaluating dependencies as needed.\n\nParameters\n----------\ndf : MOTAccumulator or pandas.DataFrame\n    The dataframe or accumulator containing event data for metric computation.\nana : dict or None, optional\n    A dictionary for caching intermediate results to speed up calculations.\nmetrics : string, list of string or None, optional\n    The names of the metrics to be computed. If None, all registered metrics are calculated.\nreturn_dataframe : bool, optional\n    If True, the result is returned as a pandas DataFrame; otherwise, a dictionary is returned.\nreturn_cached : bool, optional\n    If True, all intermediate metric results required for the computed metrics are also included in the output.\nname : string, optional\n    The index for the resulting DataFrame row containing computed metrics; defaults to 0 if None.\n\nReturns\n-------\npd.DataFrame or dict\n    A collection of computed metrics as a DataFrame or dictionary, based on the `return_dataframe` parameter.\n\nDependencies\n------------\nThe method is dependent on the `events_to_df_map` function for converting the input dataframe\ninto a structured format, and it uses the `_compute` helper method to perform the actual calculations.\nIt also references the global constant `motchallenge_metrics` to determine default metrics when none are specified.\n\"\"\"\n```\n\n## FILE 3: motmetrics/utils.py\n\n- FUNCTION NAME: compare_to_groundtruth\n  - SIGNATURE: def compare_to_groundtruth(gt, dt, dist='iou', distfields=None, distth=0.5):\n  - DOCSTRING: \n```python\n\"\"\"\nCompare ground truth and detector results using a specified distance metric.\n\nThis function takes two pandas DataFrames: one for ground truth (`gt`) and one for detector results (`dt`). It computes distances between the detected objects and ground truth objects based on specified fields, updating a `MOTAccumulator` with the results.\n\nParameters\n----------\ngt : pd.DataFrame\n    DataFrame containing ground truth data with at least 'FrameId' and 'Id' as indices.\ndt : pd.DataFrame\n    DataFrame containing detector results with at least 'FrameId' and 'Id' as indices.\ndist : str, optional\n    Distance metric to use for comparisons. Defaults to 'iou'. Supported options include 'iou', 'euclidean', and 'seuc' (squared euclidean distance).\ndistfields : array, optional\n    Specific fields to be used for calculating distances. Defaults to ['X', 'Y', 'Width', 'Height'].\ndistth : float, optional\n    Maximum tolerable distance for pairs, beyond which they are marked as 'do-not-pair'. Default is 0.5.\n\nReturns\n-------\nMOTAccumulator\n    An accumulator object that contains the results of the comparison, including distances between ground truth and detected objects, along with the corresponding frame IDs.\n\nNotes\n-----\n- The distance calculations leverage helper functions defined within the method, such as `compute_iou`, `compute_euc`, and `compute_seuc`, to obtain necessary distance matrices using imported functions from `motmetrics.distances`.\n- If an unknown distance metric is provided, a RuntimeError is raised with an appropriate message.\n- This function assumes proper formatting of input DataFrames and handles discrepancies in the frame IDs between ground truth and detection results, allowing for comprehensive evaluation of the detection performance.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - motmetrics/mot.py:MOTAccumulator:__init__\n    - motmetrics/utils.py:compute_euc\n    - motmetrics/mot.py:MOTAccumulator:update\n\n- FUNCTION NAME: compute_euc\n  - SIGNATURE: def compute_euc(a, b):\n  - DOCSTRING: \n```python\n\"\"\"\nCompute the Euclidean distance between two sets of points.\n\nThis function calculates the squared Euclidean distance between points in arrays `a` and `b`, using the `norm2squared_matrix` function from the `motmetrics.distances` module. The calculation respects the maximum squared distance threshold defined by `max_d2`, which is derived from the `distth` variable (expected to be a float). This function is typically used in the context of evaluating object tracking performance metrics.\n\nParameters\n----------\na : np.ndarray\n    An array representing the first set of points.\nb : np.ndarray\n    An array representing the second set of points.\n\nReturns\n-------\nnp.ndarray\n    A squared distance matrix computed between the specified points in `a` and `b`.\n\"\"\"\n```\n  - DEPENDENCIES:\n    - motmetrics/distances.py:norm2squared_matrix\n    - motmetrics/utils.py:compare_to_groundtruth\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "motmetrics/mot.py": "\"\"\"Accumulate tracking events frame by frame.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom collections import OrderedDict\nimport itertools\nimport numpy as np\nimport pandas as pd\nfrom motmetrics.lap import linear_sum_assignment\n_INDEX_FIELDS = ['FrameId', 'Event']\n_EVENT_FIELDS = ['Type', 'OId', 'HId', 'D']\n\nclass MOTAccumulator(object):\n    \"\"\"Manage tracking events.\n\n    This class computes per-frame tracking events from a given set of object / hypothesis\n    ids and pairwise distances. Indended usage\n\n        import motmetrics as mm\n        acc = mm.MOTAccumulator()\n        acc.update(['a', 'b'], [0, 1, 2], dists, frameid=0)\n        ...\n        acc.update(['d'], [6,10], other_dists, frameid=76)\n        summary = mm.metrics.summarize(acc)\n        print(mm.io.render_summary(summary))\n\n    Update is called once per frame and takes objects / hypothesis ids and a pairwise distance\n    matrix between those (see distances module for support). Per frame max(len(objects), len(hypothesis))\n    events are generated. Each event type is one of the following\n        - `'MATCH'` a match between a object and hypothesis was found\n        - `'SWITCH'` a match between a object and hypothesis was found but differs from previous assignment (hypothesisid != previous)\n        - `'MISS'` no match for an object was found\n        - `'FP'` no match for an hypothesis was found (spurious detections)\n        - `'RAW'` events corresponding to raw input\n        - `'TRANSFER'` a match between a object and hypothesis was found but differs from previous assignment (objectid != previous)\n        - `'ASCEND'` a match between a object and hypothesis was found but differs from previous assignment  (hypothesisid is new)\n        - `'MIGRATE'` a match between a object and hypothesis was found but differs from previous assignment  (objectid is new)\n\n    Events are tracked in a pandas Dataframe. The dataframe is hierarchically indexed by (`FrameId`, `EventId`),\n    where `FrameId` is either provided during the call to `update` or auto-incremented when `auto_id` is set\n    true during construction of MOTAccumulator. `EventId` is auto-incremented. The dataframe has the following\n    columns\n        - `Type` one of `('MATCH', 'SWITCH', 'MISS', 'FP', 'RAW')`\n        - `OId` object id or np.nan when `'FP'` or `'RAW'` and object is not present\n        - `HId` hypothesis id or np.nan when `'MISS'` or `'RAW'` and hypothesis is not present\n        - `D` distance or np.nan when `'FP'` or `'MISS'` or `'RAW'` and either object/hypothesis is absent\n\n    From the events and associated fields the entire tracking history can be recovered. Once the accumulator\n    has been populated with per-frame data use `metrics.summarize` to compute statistics. See `metrics.compute_metrics`\n    for a list of metrics computed.\n\n    References\n    ----------\n    1. Bernardin, Keni, and Rainer Stiefelhagen. \"Evaluating multiple object tracking performance: the CLEAR MOT metrics.\"\n    EURASIP Journal on Image and Video Processing 2008.1 (2008): 1-10.\n    2. Milan, Anton, et al. \"Mot16: A benchmark for multi-object tracking.\" arXiv preprint arXiv:1603.00831 (2016).\n    3. Li, Yuan, Chang Huang, and Ram Nevatia. \"Learning to associate: Hybridboosted multi-target tracker for crowded scene.\"\n    Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 2009.\n    \"\"\"\n\n    def reset(self):\n        \"\"\"Reset the accumulator to empty state.\"\"\"\n        self._events = {field: [] for field in _EVENT_FIELDS}\n        self._indices = {field: [] for field in _INDEX_FIELDS}\n        self.m = {}\n        self.res_m = {}\n        self.last_occurrence = {}\n        self.last_match = {}\n        self.hypHistory = {}\n        self.dirty_events = True\n        self.cached_events_df = None\n        self.last_update_frameid = None\n\n    def _append_to_indices(self, frameid, eid):\n        self._indices['FrameId'].append(frameid)\n        self._indices['Event'].append(eid)\n\n    def _append_to_events(self, typestr, oid, hid, distance):\n        self._events['Type'].append(typestr)\n        self._events['OId'].append(oid)\n        self._events['HId'].append(hid)\n        self._events['D'].append(distance)\n\n    @property\n    def mot_events(self):\n        df = self.events\n        return df[df.Type != 'RAW']\n\n    @staticmethod\n    def new_event_dataframe():\n        \"\"\"Create a new DataFrame for event tracking.\"\"\"\n        idx = pd.MultiIndex(levels=[[], []], codes=[[], []], names=['FrameId', 'Event'])\n        cats = pd.Categorical([], categories=['RAW', 'FP', 'MISS', 'SWITCH', 'MATCH', 'TRANSFER', 'ASCEND', 'MIGRATE'])\n        df = pd.DataFrame(OrderedDict([('Type', pd.Series(cats)), ('OId', pd.Series(dtype=float)), ('HId', pd.Series(dtype=float)), ('D', pd.Series(dtype=float))]), index=idx)\n        return df\n\n    @staticmethod\n    def new_event_dataframe_with_data(indices, events):\n        \"\"\"Create a new DataFrame filled with data.\n\n        Params\n        ------\n        indices: dict\n            dict of lists with fields 'FrameId' and 'Event'\n        events: dict\n            dict of lists with fields 'Type', 'OId', 'HId', 'D'\n        \"\"\"\n        if len(events) == 0:\n            return MOTAccumulator.new_event_dataframe()\n        raw_type = pd.Categorical(events['Type'], categories=['RAW', 'FP', 'MISS', 'SWITCH', 'MATCH', 'TRANSFER', 'ASCEND', 'MIGRATE'], ordered=False)\n        series = [pd.Series(raw_type, name='Type'), pd.Series(events['OId'], dtype=float, name='OId'), pd.Series(events['HId'], dtype=float, name='HId'), pd.Series(events['D'], dtype=float, name='D')]\n        idx = pd.MultiIndex.from_arrays([indices[field] for field in _INDEX_FIELDS], names=_INDEX_FIELDS)\n        df = pd.concat(series, axis=1)\n        df.index = idx\n        return df\n\n    @staticmethod\n    def merge_analysis(anas, infomap):\n        res = {'hyp': {}, 'obj': {}}\n        mapp = {'hyp': 'hid_map', 'obj': 'oid_map'}\n        for ana, infom in zip(anas, infomap):\n            if ana is None:\n                return None\n            for t in ana.keys():\n                which = mapp[t]\n                if np.nan in infom[which]:\n                    res[t][int(infom[which][np.nan])] = 0\n                if 'nan' in infom[which]:\n                    res[t][int(infom[which]['nan'])] = 0\n                for _id, cnt in ana[t].items():\n                    if _id not in infom[which]:\n                        _id = str(_id)\n                    res[t][int(infom[which][_id])] = cnt\n        return res\n\n    @staticmethod\n    def merge_event_dataframes(dfs, update_frame_indices=True, update_oids=True, update_hids=True, return_mappings=False):\n        \"\"\"Merge dataframes.\n\n        Params\n        ------\n        dfs : list of pandas.DataFrame or MotAccumulator\n            A list of event containers to merge\n\n        Kwargs\n        ------\n        update_frame_indices : boolean, optional\n            Ensure that frame indices are unique in the merged container\n        update_oids : boolean, unique\n            Ensure that object ids are unique in the merged container\n        update_hids : boolean, unique\n            Ensure that hypothesis ids are unique in the merged container\n        return_mappings : boolean, unique\n            Whether or not to return mapping information\n\n        Returns\n        -------\n        df : pandas.DataFrame\n            Merged event data frame\n        \"\"\"\n        mapping_infos = []\n        new_oid = itertools.count()\n        new_hid = itertools.count()\n        r = MOTAccumulator.new_event_dataframe()\n        for df in dfs:\n            if isinstance(df, MOTAccumulator):\n                df = df.events\n            copy = df.copy()\n            infos = {}\n            if update_frame_indices:\n                next_frame_id = max(r.index.get_level_values(0).max() + 1, r.index.get_level_values(0).unique().shape[0])\n                if np.isnan(next_frame_id):\n                    next_frame_id = 0\n                if not copy.index.empty:\n                    copy.index = copy.index.map(lambda x: (x[0] + next_frame_id, x[1]))\n                infos['frame_offset'] = next_frame_id\n            if update_oids:\n                oid_map = dict(([oid, str(next(new_oid))] for oid in copy['OId'].dropna().unique()))\n                copy['OId'] = copy['OId'].map(lambda x: oid_map[x], na_action='ignore')\n                infos['oid_map'] = oid_map\n            if update_hids:\n                hid_map = dict(([hid, str(next(new_hid))] for hid in copy['HId'].dropna().unique()))\n                copy['HId'] = copy['HId'].map(lambda x: hid_map[x], na_action='ignore')\n                infos['hid_map'] = hid_map\n            r = pd.concat([r, copy])\n            mapping_infos.append(infos)\n        if return_mappings:\n            return (r, mapping_infos)\n        else:\n            return r",
    "motmetrics/metrics.py": "\"\"\"Obtain metrics from event logs.\"\"\"\nfrom __future__ import absolute_import, division, print_function\nimport inspect\nimport logging\nimport time\nfrom collections import OrderedDict\nimport numpy as np\nimport pandas as pd\nfrom motmetrics import math_util\nfrom motmetrics.lap import linear_sum_assignment\nfrom motmetrics.mot import MOTAccumulator\ntry:\n    _getargspec = inspect.getfullargspec\nexcept AttributeError:\n    _getargspec = inspect.getargspec\n\nclass MetricsHost:\n    \"\"\"Keeps track of metrics and intra metric dependencies.\"\"\"\n\n    @property\n    def names(self):\n        \"\"\"Returns the name identifiers of all registered metrics.\"\"\"\n        return [v['name'] for v in self.metrics.values()]\n\n    @property\n    def formatters(self):\n        \"\"\"Returns the formatters for all metrics that have associated formatters.\"\"\"\n        return {k: v['formatter'] for k, v in self.metrics.items() if v['formatter'] is not None}\n\n    def list_metrics(self, include_deps=False):\n        \"\"\"Returns a dataframe containing names, descriptions and optionally dependencies for each metric.\"\"\"\n        cols = ['Name', 'Description', 'Dependencies']\n        if include_deps:\n            data = [(m['name'], m['help'], m['deps']) for m in self.metrics.values()]\n        else:\n            data = [(m['name'], m['help']) for m in self.metrics.values()]\n            cols = cols[:-1]\n        return pd.DataFrame(data, columns=cols)\n\n    def list_metrics_markdown(self, include_deps=False):\n        \"\"\"Returns a markdown ready version of `list_metrics`.\"\"\"\n        df = self.list_metrics(include_deps=include_deps)\n        fmt = [':---' for i in range(len(df.columns))]\n        df_fmt = pd.DataFrame([fmt], columns=df.columns)\n        df_formatted = pd.concat([df_fmt, df])\n        return df_formatted.to_csv(sep='|', index=False)\n\n    def compute_overall(self, partials, metrics=None, return_dataframe=True, return_cached=False, name=None):\n        \"\"\"Compute overall metrics based on multiple results.\n\n        Params\n        ------\n        partials : list of metric results to combine overall\n\n        Kwargs\n        ------\n        metrics : string, list of string or None, optional\n            The identifiers of the metrics to be computed. This method will only\n            compute the minimal set of necessary metrics to fullfill the request.\n            If None is passed all registered metrics are computed.\n        return_dataframe : bool, optional\n            Return the result as pandas.DataFrame (default) or dict.\n        return_cached : bool, optional\n           If true all intermediate metrics required to compute the desired metrics are returned as well.\n        name : string, optional\n            When returning a pandas.DataFrame this is the index of the row containing\n            the computed metric values.\n\n        Returns\n        -------\n        df : pandas.DataFrame\n            A datafrom containing the metrics in columns and names in rows.\n        \"\"\"\n        if metrics is None:\n            metrics = motchallenge_metrics\n        elif isinstance(metrics, str):\n            metrics = [metrics]\n        cache = {}\n        for mname in metrics:\n            cache[mname] = self._compute_overall(partials, mname, cache, parent='summarize')\n        if name is None:\n            name = 0\n        if return_cached:\n            data = cache\n        else:\n            data = OrderedDict([(k, cache[k]) for k in metrics])\n        return pd.DataFrame(data, index=[name]) if return_dataframe else data\n\n    def compute_many(self, dfs, anas=None, metrics=None, names=None, generate_overall=False):\n        \"\"\"Compute metrics on multiple dataframe / accumulators.\n\n        Params\n        ------\n        dfs : list of MOTAccumulator or list of pandas.DataFrame\n            The data to compute metrics on.\n\n        Kwargs\n        ------\n        anas: dict or None, optional\n            To cache results for fast computation.\n        metrics : string, list of string or None, optional\n            The identifiers of the metrics to be computed. This method will only\n            compute the minimal set of necessary metrics to fullfill the request.\n            If None is passed all registered metrics are computed.\n        names : list of string, optional\n            The names of individual rows in the resulting dataframe.\n        generate_overall : boolean, optional\n            If true resulting dataframe will contain a summary row that is computed\n            using the same metrics over an accumulator that is the concatentation of\n            all input containers. In creating this temporary accumulator, care is taken\n            to offset frame indices avoid object id collisions.\n\n        Returns\n        -------\n        df : pandas.DataFrame\n            A datafrom containing the metrics in columns and names in rows.\n        \"\"\"\n        if metrics is None:\n            metrics = motchallenge_metrics\n        elif isinstance(metrics, str):\n            metrics = [metrics]\n        assert names is None or len(names) == len(dfs)\n        st = time.time()\n        if names is None:\n            names = list(range(len(dfs)))\n        if anas is None:\n            anas = [None] * len(dfs)\n        partials = [self.compute(acc, ana=analysis, metrics=metrics, name=name, return_cached=True, return_dataframe=False) for acc, analysis, name in zip(dfs, anas, names)]\n        logging.info('partials: %.3f seconds.', time.time() - st)\n        details = partials\n        partials = [pd.DataFrame(OrderedDict([(k, i[k]) for k in metrics]), index=[name]) for i, name in zip(partials, names)]\n        if generate_overall:\n            names = 'OVERALL'\n            partials.append(self.compute_overall(details, metrics=metrics, name=names))\n        logging.info('mergeOverall: %.3f seconds.', time.time() - st)\n        return pd.concat(partials)\n\n    def _compute_overall(self, partials, name, cache, parent=None):\n        assert name in self.metrics, 'Cannot find metric {} required by {}.'.format(name, parent)\n        already = cache.get(name, None)\n        if already is not None:\n            return already\n        minfo = self.metrics[name]\n        vals = []\n        for depname in minfo['deps_m']:\n            v = cache.get(depname, None)\n            if v is None:\n                v = cache[depname] = self._compute_overall(partials, depname, cache, parent=name)\n            vals.append(v)\n        assert minfo['fnc_m'] is not None, 'merge function for metric %s is None' % name\n        return minfo['fnc_m'](partials, *vals)\nsimple_add_func = []\n\ndef num_frames(df):\n    \"\"\"Total number of frames.\"\"\"\n    return df.full.index.get_level_values(0).unique().shape[0]\nsimple_add_func.append(num_frames)\n\ndef obj_frequencies(df):\n    \"\"\"Total number of occurrences of individual objects over all frames.\"\"\"\n    return df.noraw.OId.value_counts()\n\ndef pred_frequencies(df):\n    \"\"\"Total number of occurrences of individual predictions over all frames.\"\"\"\n    return df.noraw.HId.value_counts()\n\ndef num_unique_objects(df, obj_frequencies):\n    \"\"\"Total number of unique object ids encountered.\"\"\"\n    del df\n    return len(obj_frequencies)\nsimple_add_func.append(num_unique_objects)\n\ndef num_matches(df):\n    \"\"\"Total number matches.\"\"\"\n    return df.noraw.Type.isin(['MATCH']).sum()\nsimple_add_func.append(num_matches)\n\ndef num_switches(df):\n    \"\"\"Total number of track switches.\"\"\"\n    return df.noraw.Type.isin(['SWITCH']).sum()\nsimple_add_func.append(num_switches)\n\ndef num_transfer(df):\n    \"\"\"Total number of track transfer.\"\"\"\n    return df.extra.Type.isin(['TRANSFER']).sum()\nsimple_add_func.append(num_transfer)\n\ndef num_ascend(df):\n    \"\"\"Total number of track ascend.\"\"\"\n    return df.extra.Type.isin(['ASCEND']).sum()\nsimple_add_func.append(num_ascend)\n\ndef num_migrate(df):\n    \"\"\"Total number of track migrate.\"\"\"\n    return df.extra.Type.isin(['MIGRATE']).sum()\nsimple_add_func.append(num_migrate)\n\ndef num_false_positives(df):\n    \"\"\"Total number of false positives (false-alarms).\"\"\"\n    return df.noraw.Type.isin(['FP']).sum()\nsimple_add_func.append(num_false_positives)\n\ndef num_misses(df):\n    \"\"\"Total number of misses.\"\"\"\n    return df.noraw.Type.isin(['MISS']).sum()\nsimple_add_func.append(num_misses)\n\ndef num_detections(df, num_matches, num_switches):\n    \"\"\"Total number of detected objects including matches and switches.\"\"\"\n    del df\n    return num_matches + num_switches\nsimple_add_func.append(num_detections)\n\ndef num_objects(df, obj_frequencies):\n    \"\"\"Total number of unique object appearances over all frames.\"\"\"\n    del df\n    return obj_frequencies.sum()\nsimple_add_func.append(num_objects)\n\ndef num_predictions(df, pred_frequencies):\n    \"\"\"Total number of unique prediction appearances over all frames.\"\"\"\n    del df\n    return pred_frequencies.sum()\nsimple_add_func.append(num_predictions)\n\ndef num_gt_ids(df):\n    \"\"\"Number of unique gt ids.\"\"\"\n    return df.full['OId'].dropna().unique().shape[0]\nsimple_add_func.append(num_gt_ids)\n\ndef num_dt_ids(df):\n    \"\"\"Number of unique dt ids.\"\"\"\n    return df.full['HId'].dropna().unique().shape[0]\nsimple_add_func.append(num_dt_ids)\n\ndef track_ratios(df, obj_frequencies):\n    \"\"\"Ratio of assigned to total appearance count per unique object id.\"\"\"\n    tracked = df.noraw[df.noraw.Type != 'MISS']['OId'].value_counts()\n    return tracked.div(obj_frequencies).fillna(0.0)\n\ndef mostly_tracked(df, track_ratios):\n    \"\"\"Number of objects tracked for at least 80 percent of lifespan.\"\"\"\n    del df\n    return track_ratios[track_ratios >= 0.8].count()\nsimple_add_func.append(mostly_tracked)\n\ndef partially_tracked(df, track_ratios):\n    \"\"\"Number of objects tracked between 20 and 80 percent of lifespan.\"\"\"\n    del df\n    return track_ratios[(track_ratios >= 0.2) & (track_ratios < 0.8)].count()\nsimple_add_func.append(partially_tracked)\n\ndef mostly_lost(df, track_ratios):\n    \"\"\"Number of objects tracked less than 20 percent of lifespan.\"\"\"\n    del df\n    return track_ratios[track_ratios < 0.2].count()\nsimple_add_func.append(mostly_lost)\n\ndef num_fragmentations(df, obj_frequencies):\n    \"\"\"Total number of switches from tracked to not tracked.\"\"\"\n    fra = 0\n    for o in obj_frequencies.index:\n        dfo = df.noraw[df.noraw.OId == o]\n        notmiss = dfo[dfo.Type != 'MISS']\n        if len(notmiss) == 0:\n            continue\n        first = notmiss.index[0]\n        last = notmiss.index[-1]\n        diffs = dfo.loc[first:last].Type.apply(lambda x: 1 if x == 'MISS' else 0).diff()\n        fra += diffs[diffs == 1].count()\n    return fra\nsimple_add_func.append(num_fragmentations)\n\ndef motp(df, num_detections):\n    \"\"\"Multiple object tracker precision.\"\"\"\n    return math_util.quiet_divide(df.noraw['D'].sum(), num_detections)\n\ndef motp_m(partials, num_detections):\n    res = 0\n    for v in partials:\n        res += v['motp'] * v['num_detections']\n    return math_util.quiet_divide(res, num_detections)\n\ndef mota(df, num_misses, num_switches, num_false_positives, num_objects):\n    \"\"\"Multiple object tracker accuracy.\"\"\"\n    del df\n    return 1.0 - math_util.quiet_divide(num_misses + num_switches + num_false_positives, num_objects)\n\ndef mota_m(partials, num_misses, num_switches, num_false_positives, num_objects):\n    del partials\n    return 1.0 - math_util.quiet_divide(num_misses + num_switches + num_false_positives, num_objects)\n\ndef precision(df, num_detections, num_false_positives):\n    \"\"\"Number of detected objects over sum of detected and false positives.\"\"\"\n    del df\n    return math_util.quiet_divide(num_detections, num_false_positives + num_detections)\n\ndef precision_m(partials, num_detections, num_false_positives):\n    del partials\n    return math_util.quiet_divide(num_detections, num_false_positives + num_detections)\n\ndef recall(df, num_detections, num_objects):\n    \"\"\"Number of detections over number of objects.\"\"\"\n    del df\n    return math_util.quiet_divide(num_detections, num_objects)\n\ndef recall_m(partials, num_detections, num_objects):\n    del partials\n    return math_util.quiet_divide(num_detections, num_objects)\n\ndef deta_alpha(df, num_detections, num_objects, num_false_positives):\n    \"\"\"DeTA under specific threshold $\\\\alpha$\n    Source: https://jonathonluiten.medium.com/how-to-evaluate-tracking-with-the-hota-metrics-754036d183e1\n    \"\"\"\n    del df\n    return math_util.quiet_divide(num_detections, max(1, num_objects + num_false_positives))\n\ndef deta_alpha_m(partials):\n    res = 0\n    for v in partials:\n        res += v['deta_alpha']\n    return math_util.quiet_divide(res, len(partials))\n\ndef assa_alpha(df, num_detections, num_gt_ids, num_dt_ids):\n    \"\"\"AssA under specific threshold $\\\\alpha$\n    Source: https://github.com/JonathonLuiten/TrackEval/blob/12c8791b303e0a0b50f753af204249e622d0281a/trackeval/metrics/hota.py#L107-L108\n    \"\"\"\n    max_gt_ids = int(df.noraw.OId.max())\n    max_dt_ids = int(df.noraw.HId.max())\n    match_count_array = np.zeros((max_gt_ids, max_dt_ids))\n    gt_id_counts = np.zeros((max_gt_ids, 1))\n    tracker_id_counts = np.zeros((1, max_dt_ids))\n    for idx in range(len(df.noraw)):\n        oid, hid = (df.noraw.iloc[idx, 1], df.noraw.iloc[idx, 2])\n        if df.noraw.iloc[idx, 0] in ['SWITCH', 'MATCH']:\n            match_count_array[int(oid) - 1, int(hid) - 1] += 1\n        if oid == oid:\n            gt_id_counts[int(oid) - 1] += 1\n        if hid == hid:\n            tracker_id_counts[0, int(hid) - 1] += 1\n    ass_a = match_count_array / np.maximum(1, gt_id_counts + tracker_id_counts - match_count_array)\n    return math_util.quiet_divide((ass_a * match_count_array).sum(), max(1, num_detections))\n\ndef assa_alpha_m(partials):\n    res = 0\n    for v in partials:\n        res += v['assa_alpha']\n    return math_util.quiet_divide(res, len(partials))\n\ndef hota_alpha(df, deta_alpha, assa_alpha):\n    \"\"\"HOTA under specific threshold $\\\\alpha$\"\"\"\n    del df\n    return (deta_alpha * assa_alpha) ** 0.5\n\ndef hota_alpha_m(partials):\n    res = 0\n    for v in partials:\n        res += v['hota_alpha']\n    return math_util.quiet_divide(res, len(partials))\n\nclass DataFrameMap:\n\n    def __init__(self, full, raw, noraw, extra):\n        self.full = full\n        self.raw = raw\n        self.noraw = noraw\n        self.extra = extra\n\ndef extract_counts_from_df_map(df):\n    \"\"\"\n    Returns:\n        Tuple (ocs, hcs, tps).\n        ocs: Dict from object id to count.\n        hcs: Dict from hypothesis id to count.\n        tps: Dict from (object id, hypothesis id) to true-positive count.\n        The ids are arbitrary, they might NOT be consecutive integers from 0.\n    \"\"\"\n    oids = df.full['OId'].dropna().unique()\n    hids = df.full['HId'].dropna().unique()\n    flat = df.raw.reset_index()\n    flat = flat[flat['OId'].isin(oids) | flat['HId'].isin(hids)]\n    ocs = flat.set_index('OId')['FrameId'].groupby('OId').nunique().to_dict()\n    hcs = flat.set_index('HId')['FrameId'].groupby('HId').nunique().to_dict()\n    dists = flat[['OId', 'HId', 'D']].set_index(['OId', 'HId']).dropna()\n    tps = dists.groupby(['OId', 'HId'])['D'].count().to_dict()\n    return (ocs, hcs, tps)\n\ndef id_global_assignment(df, ana=None):\n    \"\"\"ID measures: Global min-cost assignment for ID measures.\"\"\"\n    del ana\n    ocs, hcs, tps = extract_counts_from_df_map(df)\n    oids = sorted(ocs.keys())\n    hids = sorted(hcs.keys())\n    oids_idx = dict(((o, i) for i, o in enumerate(oids)))\n    hids_idx = dict(((h, i) for i, h in enumerate(hids)))\n    no = len(ocs)\n    nh = len(hcs)\n    fpmatrix = np.full((no + nh, no + nh), 0.0)\n    fnmatrix = np.full((no + nh, no + nh), 0.0)\n    fpmatrix[no:, :nh] = np.nan\n    fnmatrix[:no, nh:] = np.nan\n    for oid, oc in ocs.items():\n        r = oids_idx[oid]\n        fnmatrix[r, :nh] = oc\n        fnmatrix[r, nh + r] = oc\n    for hid, hc in hcs.items():\n        c = hids_idx[hid]\n        fpmatrix[:no, c] = hc\n        fpmatrix[c + no, c] = hc\n    for (oid, hid), ex in tps.items():\n        r = oids_idx[oid]\n        c = hids_idx[hid]\n        fpmatrix[r, c] -= ex\n        fnmatrix[r, c] -= ex\n    costs = fpmatrix + fnmatrix\n    rids, cids = linear_sum_assignment(costs)\n    return {'fpmatrix': fpmatrix, 'fnmatrix': fnmatrix, 'rids': rids, 'cids': cids, 'costs': costs, 'min_cost': costs[rids, cids].sum()}\n\ndef idfp(df, id_global_assignment):\n    \"\"\"ID measures: Number of false positive matches after global min-cost matching.\"\"\"\n    del df\n    rids, cids = (id_global_assignment['rids'], id_global_assignment['cids'])\n    return id_global_assignment['fpmatrix'][rids, cids].sum()\nsimple_add_func.append(idfp)\n\ndef idfn(df, id_global_assignment):\n    \"\"\"ID measures: Number of false negatives matches after global min-cost matching.\"\"\"\n    del df\n    rids, cids = (id_global_assignment['rids'], id_global_assignment['cids'])\n    return id_global_assignment['fnmatrix'][rids, cids].sum()\nsimple_add_func.append(idfn)\n\ndef idtp(df, id_global_assignment, num_objects, idfn):\n    \"\"\"ID measures: Number of true positives matches after global min-cost matching.\"\"\"\n    del df, id_global_assignment\n    return num_objects - idfn\nsimple_add_func.append(idtp)\n\ndef idp(df, idtp, idfp):\n    \"\"\"ID measures: global min-cost precision.\"\"\"\n    del df\n    return math_util.quiet_divide(idtp, idtp + idfp)\n\ndef idp_m(partials, idtp, idfp):\n    del partials\n    return math_util.quiet_divide(idtp, idtp + idfp)\n\ndef idr(df, idtp, idfn):\n    \"\"\"ID measures: global min-cost recall.\"\"\"\n    del df\n    return math_util.quiet_divide(idtp, idtp + idfn)\n\ndef idr_m(partials, idtp, idfn):\n    del partials\n    return math_util.quiet_divide(idtp, idtp + idfn)\n\ndef idf1(df, idtp, num_objects, num_predictions):\n    \"\"\"ID measures: global min-cost F1 score.\"\"\"\n    del df\n    return math_util.quiet_divide(2 * idtp, num_objects + num_predictions)\n\ndef idf1_m(partials, idtp, num_objects, num_predictions):\n    del partials\n    return math_util.quiet_divide(2 * idtp, num_objects + num_predictions)\nfor one in simple_add_func:\n    name = one.__name__\n\n    def getSimpleAdd(nm):\n\n        def simpleAddHolder(partials):\n            res = 0\n            for v in partials:\n                res += v[nm]\n            return res\n        return simpleAddHolder\n    locals()[name + '_m'] = getSimpleAdd(name)\nmotchallenge_metrics = ['idf1', 'idp', 'idr', 'recall', 'precision', 'num_unique_objects', 'mostly_tracked', 'partially_tracked', 'mostly_lost', 'num_false_positives', 'num_misses', 'num_switches', 'num_fragmentations', 'mota', 'motp', 'num_transfer', 'num_ascend', 'num_migrate']\n'A list of all metrics from MOTChallenge.'",
    "motmetrics/utils.py": "\"\"\"Functions for populating event accumulators.\"\"\"\nfrom __future__ import absolute_import, division, print_function\nimport numpy as np\nfrom motmetrics.distances import iou_matrix, norm2squared_matrix\nfrom motmetrics.mot import MOTAccumulator\nfrom motmetrics.preprocess import preprocessResult\n\ndef compute_global_aligment_score(allframeids, fid_to_fgt, fid_to_fdt, num_gt_id, num_det_id, dist_func):\n    \"\"\"Taken from https://github.com/JonathonLuiten/TrackEval/blob/12c8791b303e0a0b50f753af204249e622d0281a/trackeval/metrics/hota.py\"\"\"\n    potential_matches_count = np.zeros((num_gt_id, num_det_id))\n    gt_id_count = np.zeros((num_gt_id, 1))\n    tracker_id_count = np.zeros((1, num_det_id))\n    for fid in allframeids:\n        oids = np.empty(0)\n        hids = np.empty(0)\n        if fid in fid_to_fgt:\n            fgt = fid_to_fgt[fid]\n            oids = fgt.index.get_level_values('Id')\n        if fid in fid_to_fdt:\n            fdt = fid_to_fdt[fid]\n            hids = fdt.index.get_level_values('Id')\n        if len(oids) > 0 and len(hids) > 0:\n            gt_ids = np.array(oids.values) - 1\n            dt_ids = np.array(hids.values) - 1\n            similarity = dist_func(fgt.values, fdt.values, return_dist=False)\n            sim_iou_denom = similarity.sum(0)[np.newaxis, :] + similarity.sum(1)[:, np.newaxis] - similarity\n            sim_iou = np.zeros_like(similarity)\n            sim_iou_mask = sim_iou_denom > 0 + np.finfo('float').eps\n            sim_iou[sim_iou_mask] = similarity[sim_iou_mask] / sim_iou_denom[sim_iou_mask]\n            potential_matches_count[gt_ids[:, np.newaxis], dt_ids[np.newaxis, :]] += sim_iou\n            gt_id_count[gt_ids] += 1\n            tracker_id_count[0, dt_ids] += 1\n    global_alignment_score = potential_matches_count / np.maximum(1, gt_id_count + tracker_id_count - potential_matches_count)\n    return global_alignment_score\n\ndef compare_to_groundtruth_reweighting(gt, dt, dist='iou', distfields=None, distth=0.5):\n    \"\"\"Compare groundtruth and detector results with global alignment score.\n\n    This method assumes both results are given in terms of DataFrames with at least the following fields\n     - `FrameId` First level index used for matching ground-truth and test frames.\n     - `Id` Secondary level index marking available object / hypothesis ids\n\n    Depending on the distance to be used relevant distfields need to be specified.\n\n    Params\n    ------\n    gt : pd.DataFrame\n        Dataframe for ground-truth\n    test : pd.DataFrame\n        Dataframe for detector results\n\n    Kwargs\n    ------\n    dist : str, optional\n        String identifying distance to be used. Defaults to intersection over union ('iou'). Euclidean\n        distance ('euclidean') and squared euclidean distance ('seuc') are also supported.\n    distfields: array, optional\n        Fields relevant for extracting distance information. Defaults to ['X', 'Y', 'Width', 'Height']\n    distth: Union(float, array_like), optional\n        Maximum tolerable distance. Pairs exceeding this threshold are marked 'do-not-pair'.\n        If a list of thresholds is given, multiple accumulators are returned.\n    \"\"\"\n    if distfields is None:\n        distfields = ['X', 'Y', 'Width', 'Height']\n\n    def compute_iou(a, b, return_dist):\n        return iou_matrix(a, b, max_iou=distth, return_dist=return_dist)\n\n    def compute_seuc(a, b, *args, **kwargs):\n        return norm2squared_matrix(a, b, max_d2=distth)\n    if dist.upper() == 'IOU':\n        compute_dist = compute_iou\n    elif dist.upper() == 'EUC':\n        compute_dist = compute_euc\n        import warnings\n        warnings.warn(f\"'euc' flag changed its behavior. The euclidean distance is now used instead of the squared euclidean distance. Make sure the used threshold (distth={distth}) is not squared. Use 'euclidean' flag to avoid this warning.\")\n    elif dist.upper() == 'EUCLIDEAN':\n        compute_dist = compute_euc\n    elif dist.upper() == 'SEUC':\n        compute_dist = compute_seuc\n    else:\n        raise f'Unknown distance metric {dist}. Use \"IOU\", \"EUCLIDEAN\",  or \"SEUC\"'\n    return_single = False\n    if isinstance(distth, float):\n        distth = [distth]\n        return_single = True\n    acc_list = [MOTAccumulator() for _ in range(len(distth))]\n    num_gt_id = gt.index.get_level_values('Id').max()\n    num_det_id = dt.index.get_level_values('Id').max()\n    allframeids = gt.index.union(dt.index).levels[0]\n    gt = gt[distfields]\n    dt = dt[distfields]\n    fid_to_fgt = dict(iter(gt.groupby('FrameId')))\n    fid_to_fdt = dict(iter(dt.groupby('FrameId')))\n    global_alignment_score = compute_global_aligment_score(allframeids, fid_to_fgt, fid_to_fdt, num_gt_id, num_det_id, compute_dist)\n    for fid in allframeids:\n        oids = np.empty(0)\n        hids = np.empty(0)\n        weighted_dists = np.empty((0, 0))\n        if fid in fid_to_fgt:\n            fgt = fid_to_fgt[fid]\n            oids = fgt.index.get_level_values('Id')\n        if fid in fid_to_fdt:\n            fdt = fid_to_fdt[fid]\n            hids = fdt.index.get_level_values('Id')\n        if len(oids) > 0 and len(hids) > 0:\n            gt_ids = np.array(oids.values) - 1\n            dt_ids = np.array(hids.values) - 1\n            dists = compute_dist(fgt.values, fdt.values, return_dist=False)\n            weighted_dists = dists * global_alignment_score[gt_ids[:, np.newaxis], dt_ids[np.newaxis, :]]\n        for acc, th in zip(acc_list, distth):\n            acc.update(oids, hids, 1 - weighted_dists, frameid=fid, similartiy_matrix=dists, th=th)\n    return acc_list[0] if return_single else acc_list\n\ndef CLEAR_MOT_M(gt, dt, inifile, dist='iou', distfields=None, distth=0.5, include_all=False, vflag=''):\n    \"\"\"Compare groundtruth and detector results.\n\n    This method assumes both results are given in terms of DataFrames with at least the following fields\n     - `FrameId` First level index used for matching ground-truth and test frames.\n     - `Id` Secondary level index marking available object / hypothesis ids\n\n    Depending on the distance to be used relevant distfields need to be specified.\n\n    Params\n    ------\n    gt : pd.DataFrame\n        Dataframe for ground-truth\n    test : pd.DataFrame\n        Dataframe for detector results\n\n    Kwargs\n    ------\n    dist : str, optional\n        String identifying distance to be used. Defaults to intersection over union.\n    distfields: array, optional\n        Fields relevant for extracting distance information. Defaults to ['X', 'Y', 'Width', 'Height']\n    distth: float, optional\n        Maximum tolerable distance. Pairs exceeding this threshold are marked 'do-not-pair'.\n    \"\"\"\n    if distfields is None:\n        distfields = ['X', 'Y', 'Width', 'Height']\n\n    def compute_iou(a, b):\n        return iou_matrix(a, b, max_iou=distth)\n    compute_dist = compute_iou if dist.upper() == 'IOU' else compute_euc\n    acc = MOTAccumulator()\n    dt = preprocessResult(dt, gt, inifile)\n    if include_all:\n        gt = gt[gt['Confidence'] >= 0.99]\n    else:\n        gt = gt[(gt['Confidence'] >= 0.99) & (gt['ClassId'] == 1)]\n    allframeids = gt.index.union(dt.index).levels[0]\n    analysis = {'hyp': {}, 'obj': {}}\n    for fid in allframeids:\n        oids = np.empty(0)\n        hids = np.empty(0)\n        dists = np.empty((0, 0))\n        if fid in gt.index:\n            fgt = gt.loc[fid]\n            oids = fgt.index.values\n            for oid in oids:\n                oid = int(oid)\n                if oid not in analysis['obj']:\n                    analysis['obj'][oid] = 0\n                analysis['obj'][oid] += 1\n        if fid in dt.index:\n            fdt = dt.loc[fid]\n            hids = fdt.index.values\n            for hid in hids:\n                hid = int(hid)\n                if hid not in analysis['hyp']:\n                    analysis['hyp'][hid] = 0\n                analysis['hyp'][hid] += 1\n        if oids.shape[0] > 0 and hids.shape[0] > 0:\n            dists = compute_dist(fgt[distfields].values, fdt[distfields].values)\n        acc.update(oids, hids, dists, frameid=fid, vf=vflag)\n    return (acc, analysis)"
  }
}