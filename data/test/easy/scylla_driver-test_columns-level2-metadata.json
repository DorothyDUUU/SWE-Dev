{
  "dir_path": "/app/scylla_driver",
  "package_name": "scylla_driver",
  "sample_name": "scylla_driver-test_columns",
  "src_dir": "cassandra/",
  "test_dir": "tests/",
  "test_file": "tests/unit/cqlengine/test_columns.py",
  "test_code": "# Copyright DataStax, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport unittest\n\nfrom cassandra.cqlengine.columns import Column\n\n\nclass ColumnTest(unittest.TestCase):\n\n    def test_comparisons(self):\n        c0 = Column()\n        c1 = Column()\n        self.assertEqual(c1.position - c0.position, 1)\n\n        # __ne__\n        self.assertNotEqual(c0, c1)\n        self.assertNotEqual(c0, object())\n\n        # __eq__\n        self.assertEqual(c0, c0)\n        self.assertFalse(c0 == object())\n\n        # __lt__\n        self.assertLess(c0, c1)\n        try:\n            c0 < object()  # this raises for Python 3\n        except TypeError:\n            pass\n\n        # __le__\n        self.assertLessEqual(c0, c1)\n        self.assertLessEqual(c0, c0)\n        try:\n            c0 <= object()  # this raises for Python 3\n        except TypeError:\n            pass\n\n        # __gt__\n        self.assertGreater(c1, c0)\n        try:\n            c1 > object()  # this raises for Python 3\n        except TypeError:\n            pass\n\n        # __ge__\n        self.assertGreaterEqual(c1, c0)\n        self.assertGreaterEqual(c1, c1)\n        try:\n            c1 >= object()  # this raises for Python 3\n        except TypeError:\n            pass\n\n    def test_hash(self):\n        c0 = Column()\n        self.assertEqual(id(c0), c0.__hash__())\n\n",
  "GT_file_code": {
    "cassandra/cqlengine/columns.py": "# Copyright DataStax, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom copy import deepcopy, copy\nfrom datetime import date, datetime, timedelta\nimport logging\nfrom uuid import UUID as _UUID\n\nfrom cassandra import util\nfrom cassandra.cqltypes import SimpleDateType, _cqltypes, UserType\nfrom cassandra.cqlengine import ValidationError\nfrom cassandra.cqlengine.functions import get_total_seconds\nfrom cassandra.util import Duration as _Duration\n\nlog = logging.getLogger(__name__)\n\n\nclass BaseValueManager(object):\n\n    def __init__(self, instance, column, value):\n        self.instance = instance\n        self.column = column\n        self.value = value\n        self.previous_value = None\n        self.explicit = False\n\n    @property\n    def deleted(self):\n        return self.column._val_is_null(self.value) and (self.explicit or not self.column._val_is_null(self.previous_value))\n\n    @property\n    def changed(self):\n        \"\"\"\n        Indicates whether or not this value has changed.\n\n        :rtype: boolean\n\n        \"\"\"\n        if self.explicit:\n            return self.value != self.previous_value\n\n        if isinstance(self.column, BaseContainerColumn):\n            default_value = self.column.get_default()\n            if self.column._val_is_null(default_value):\n                return not self.column._val_is_null(self.value) and self.value != self.previous_value\n            elif self.previous_value is None:\n                return self.value != default_value\n\n            return self.value != self.previous_value\n\n        return False\n\n    def reset_previous_value(self):\n        self.previous_value = deepcopy(self.value)\n\n    def getval(self):\n        return self.value\n\n    def setval(self, val):\n        self.value = val\n        self.explicit = True\n\n    def delval(self):\n        self.value = None\n\n    def get_property(self):\n        _get = lambda slf: self.getval()\n        _set = lambda slf, val: self.setval(val)\n        _del = lambda slf: self.delval()\n\n        if self.column.can_delete:\n            return property(_get, _set, _del)\n        else:\n            return property(_get, _set)\n\n\nclass Column(object):\n\n    # the cassandra type this column maps to\n    db_type = None\n    value_manager = BaseValueManager\n\n    instance_counter = 0\n\n    _python_type_hashable = True\n\n    primary_key = False\n    \"\"\"\n    bool flag, indicates this column is a primary key. The first primary key defined\n    on a model is the partition key (unless partition keys are set), all others are cluster keys\n    \"\"\"\n\n    partition_key = False\n\n    \"\"\"\n    indicates that this column should be the partition key, defining\n    more than one partition key column creates a compound partition key\n    \"\"\"\n\n    index = False\n    \"\"\"\n    bool flag, indicates an index should be created for this column\n    \"\"\"\n\n    custom_index = False\n    \"\"\"\n    bool flag, indicates an index is managed outside of cqlengine. This is\n    useful if you want to do filter queries on fields that have custom\n    indexes.\n    \"\"\"\n\n    db_field = None\n    \"\"\"\n    the fieldname this field will map to in the database\n    \"\"\"\n\n    default = None\n    \"\"\"\n    the default value, can be a value or a callable (no args)\n    \"\"\"\n\n    required = False\n    \"\"\"\n    boolean, is the field required? Model validation will raise and\n    exception if required is set to True and there is a None value assigned\n    \"\"\"\n\n    clustering_order = None\n    \"\"\"\n    only applicable on clustering keys (primary keys that are not partition keys)\n    determines the order that the clustering keys are sorted on disk\n    \"\"\"\n\n    discriminator_column = False\n    \"\"\"\n    boolean, if set to True, this column will be used for discriminating records\n    of inherited models.\n\n    Should only be set on a column of an abstract model being used for inheritance.\n\n    There may only be one discriminator column per model. See :attr:`~.__discriminator_value__`\n    for how to specify the value of this column on specialized models.\n    \"\"\"\n\n    static = False\n    \"\"\"\n    boolean, if set to True, this is a static column, with a single value per partition\n    \"\"\"\n\n    def __init__(self,\n                 primary_key=False,\n                 partition_key=False,\n                 index=False,\n                 db_field=None,\n                 default=None,\n                 required=False,\n                 clustering_order=None,\n                 discriminator_column=False,\n                 static=False,\n                 custom_index=False):\n        self.partition_key = partition_key\n        self.primary_key = partition_key or primary_key\n        self.index = index\n        self.custom_index = custom_index\n        self.db_field = db_field\n        self.default = default\n        self.required = required\n        self.clustering_order = clustering_order\n        self.discriminator_column = discriminator_column\n\n        # the column name in the model definition\n        self.column_name = None\n        self._partition_key_index = None\n        self.static = static\n\n        self.value = None\n\n        # keep track of instantiation order\n        self.position = Column.instance_counter\n        Column.instance_counter += 1\n\n    def __ne__(self, other):\n        if isinstance(other, Column):\n            return self.position != other.position\n        return NotImplemented\n\n    def __eq__(self, other):\n        if isinstance(other, Column):\n            return self.position == other.position\n        return NotImplemented\n\n    def __lt__(self, other):\n        if isinstance(other, Column):\n            return self.position < other.position\n        return NotImplemented\n\n    def __le__(self, other):\n        if isinstance(other, Column):\n            return self.position <= other.position\n        return NotImplemented\n\n    def __gt__(self, other):\n        if isinstance(other, Column):\n            return self.position > other.position\n        return NotImplemented\n\n    def __ge__(self, other):\n        if isinstance(other, Column):\n            return self.position >= other.position\n        return NotImplemented\n\n    def __hash__(self):\n        return id(self)\n\n    def validate(self, value):\n        \"\"\"\n        Returns a cleaned and validated value. Raises a ValidationError\n        if there's a problem\n        \"\"\"\n        if value is None:\n            if self.required:\n                raise ValidationError('{0} - None values are not allowed'.format(self.column_name or self.db_field))\n        return value\n\n    def to_python(self, value):\n        \"\"\"\n        Converts data from the database into python values\n        raises a ValidationError if the value can't be converted\n        \"\"\"\n        return value\n\n    def to_database(self, value):\n        \"\"\"\n        Converts python value into database value\n        \"\"\"\n        return value\n\n    @property\n    def has_default(self):\n        return self.default is not None\n\n    @property\n    def is_primary_key(self):\n        return self.primary_key\n\n    @property\n    def can_delete(self):\n        return not self.primary_key\n\n    def get_default(self):\n        if self.has_default:\n            if callable(self.default):\n                return self.default()\n            else:\n                return self.default\n\n    def get_column_def(self):\n        \"\"\"\n        Returns a column definition for CQL table definition\n        \"\"\"\n        static = \"static\" if self.static else \"\"\n        return '{0} {1} {2}'.format(self.cql, self.db_type, static)\n\n    # TODO: make columns use cqltypes under the hood\n    # until then, this bridges the gap in using types along with cassandra.metadata for CQL generation\n    def cql_parameterized_type(self):\n        return self.db_type\n\n    def set_column_name(self, name):\n        \"\"\"\n        Sets the column name during document class construction\n        This value will be ignored if db_field is set in __init__\n        \"\"\"\n        self.column_name = name\n\n    @property\n    def db_field_name(self):\n        \"\"\" Returns the name of the cql name of this column \"\"\"\n        return self.db_field if self.db_field is not None else self.column_name\n\n    @property\n    def db_index_name(self):\n        \"\"\" Returns the name of the cql index \"\"\"\n        return 'index_{0}'.format(self.db_field_name)\n\n    @property\n    def has_index(self):\n        return self.index or self.custom_index\n\n    @property\n    def cql(self):\n        return self.get_cql()\n\n    def get_cql(self):\n        return '\"{0}\"'.format(self.db_field_name)\n\n    def _val_is_null(self, val):\n        \"\"\" determines if the given value equates to a null value for the given column type \"\"\"\n        return val is None\n\n    @property\n    def sub_types(self):\n        return []\n\n    @property\n    def cql_type(self):\n        return _cqltypes[self.db_type]\n\n\nclass Blob(Column):\n    \"\"\"\n    Stores a raw binary value\n    \"\"\"\n    db_type = 'blob'\n\n    def to_database(self, value):\n\n        if not isinstance(value, (bytes, bytearray)):\n            raise Exception(\"expecting a binary, got a %s\" % type(value))\n\n        val = super(Bytes, self).to_database(value)\n        return bytearray(val)\n\n\nBytes = Blob\n\n\nclass Inet(Column):\n    \"\"\"\n    Stores an IP address in IPv4 or IPv6 format\n    \"\"\"\n    db_type = 'inet'\n\n\nclass Text(Column):\n    \"\"\"\n    Stores a UTF-8 encoded string\n    \"\"\"\n    db_type = 'text'\n\n    def __init__(self, min_length=None, max_length=None, **kwargs):\n        \"\"\"\n        :param int min_length: Sets the minimum length of this string, for validation purposes.\n            Defaults to 1 if this is a ``required`` column. Otherwise, None.\n        :param int max_length: Sets the maximum length of this string, for validation purposes.\n        \"\"\"\n        self.min_length = (\n            1 if min_length is None and kwargs.get('required', False)\n            else min_length)\n        self.max_length = max_length\n\n        if self.min_length is not None:\n            if self.min_length < 0:\n                raise ValueError(\n                    'Minimum length is not allowed to be negative.')\n\n        if self.max_length is not None:\n            if self.max_length < 0:\n                raise ValueError(\n                    'Maximum length is not allowed to be negative.')\n\n        if self.min_length is not None and self.max_length is not None:\n            if self.max_length < self.min_length:\n                raise ValueError(\n                    'Maximum length must be greater or equal '\n                    'to minimum length.')\n\n        super(Text, self).__init__(**kwargs)\n\n    def validate(self, value):\n        value = super(Text, self).validate(value)\n        if not isinstance(value, (str, bytearray)) and value is not None:\n            raise ValidationError('{0} {1} is not a string'.format(self.column_name, type(value)))\n        if self.max_length is not None:\n            if value and len(value) > self.max_length:\n                raise ValidationError('{0} is longer than {1} characters'.format(self.column_name, self.max_length))\n        if self.min_length:\n            if (self.min_length and not value) or len(value) < self.min_length:\n                raise ValidationError('{0} is shorter than {1} characters'.format(self.column_name, self.min_length))\n        return value\n\n\nclass Ascii(Text):\n    \"\"\"\n    Stores a US-ASCII character string\n    \"\"\"\n    db_type = 'ascii'\n\n    def validate(self, value):\n        \"\"\" Only allow ASCII and None values.\n\n        Check against US-ASCII, a.k.a. 7-bit ASCII, a.k.a. ISO646-US, a.k.a.\n        the Basic Latin block of the Unicode character set.\n\n        Source: https://github.com/apache/cassandra/blob\n        /3dcbe90e02440e6ee534f643c7603d50ca08482b/src/java/org/apache/cassandra\n        /serializers/AsciiSerializer.java#L29\n        \"\"\"\n        value = super(Ascii, self).validate(value)\n        if value:\n            charset = value if isinstance(\n                value, (bytearray, )) else map(ord, value)\n            if not set(range(128)).issuperset(charset):\n                raise ValidationError(\n                    '{!r} is not an ASCII string.'.format(value))\n        return value\n\n\nclass Integer(Column):\n    \"\"\"\n    Stores a 32-bit signed integer value\n    \"\"\"\n\n    db_type = 'int'\n\n    def validate(self, value):\n        val = super(Integer, self).validate(value)\n        if val is None:\n            return\n        try:\n            return int(val)\n        except (TypeError, ValueError):\n            raise ValidationError(\"{0} {1} can't be converted to integral value\".format(self.column_name, value))\n\n    def to_python(self, value):\n        return self.validate(value)\n\n    def to_database(self, value):\n        return self.validate(value)\n\n\nclass TinyInt(Integer):\n    \"\"\"\n    Stores an 8-bit signed integer value\n\n    .. versionadded:: 2.6.0\n\n    requires C* 2.2+ and protocol v4+\n    \"\"\"\n    db_type = 'tinyint'\n\n\nclass SmallInt(Integer):\n    \"\"\"\n    Stores a 16-bit signed integer value\n\n    .. versionadded:: 2.6.0\n\n    requires C* 2.2+ and protocol v4+\n    \"\"\"\n    db_type = 'smallint'\n\n\nclass BigInt(Integer):\n    \"\"\"\n    Stores a 64-bit signed integer value\n    \"\"\"\n    db_type = 'bigint'\n\n\nclass VarInt(Column):\n    \"\"\"\n    Stores an arbitrary-precision integer\n    \"\"\"\n    db_type = 'varint'\n\n    def validate(self, value):\n        val = super(VarInt, self).validate(value)\n        if val is None:\n            return\n        try:\n            return int(val)\n        except (TypeError, ValueError):\n            raise ValidationError(\n                \"{0} {1} can't be converted to integral value\".format(self.column_name, value))\n\n    def to_python(self, value):\n        return self.validate(value)\n\n    def to_database(self, value):\n        return self.validate(value)\n\n\nclass CounterValueManager(BaseValueManager):\n    def __init__(self, instance, column, value):\n        super(CounterValueManager, self).__init__(instance, column, value)\n        self.value = self.value or 0\n        self.previous_value = self.previous_value or 0\n\n\nclass Counter(Integer):\n    \"\"\"\n    Stores a counter that can be incremented and decremented\n    \"\"\"\n    db_type = 'counter'\n\n    value_manager = CounterValueManager\n\n    def __init__(self,\n                 index=False,\n                 db_field=None,\n                 required=False):\n        super(Counter, self).__init__(\n            primary_key=False,\n            partition_key=False,\n            index=index,\n            db_field=db_field,\n            default=0,\n            required=required,\n        )\n\n\nclass DateTime(Column):\n    \"\"\"\n    Stores a datetime value\n    \"\"\"\n    db_type = 'timestamp'\n\n    truncate_microseconds = False\n    \"\"\"\n    Set this ``True`` to have model instances truncate the date, quantizing it in the same way it will be in the database.\n    This allows equality comparison between assigned values and values read back from the database::\n\n        DateTime.truncate_microseconds = True\n        assert Model.create(id=0, d=datetime.utcnow()) == Model.objects(id=0).first()\n\n    Defaults to ``False`` to preserve legacy behavior. May change in the future.\n    \"\"\"\n\n    def to_python(self, value):\n        if value is None:\n            return\n        if isinstance(value, datetime):\n            if DateTime.truncate_microseconds:\n                us = value.microsecond\n                truncated_us = us // 1000 * 1000\n                return value - timedelta(microseconds=us - truncated_us)\n            else:\n                return value\n        elif isinstance(value, date):\n            return datetime(*(value.timetuple()[:6]))\n\n        return datetime.utcfromtimestamp(value)\n\n    def to_database(self, value):\n        value = super(DateTime, self).to_database(value)\n        if value is None:\n            return\n        if not isinstance(value, datetime):\n            if isinstance(value, date):\n                value = datetime(value.year, value.month, value.day)\n            else:\n                raise ValidationError(\"{0} '{1}' is not a datetime object\".format(self.column_name, value))\n        epoch = datetime(1970, 1, 1, tzinfo=value.tzinfo)\n        offset = get_total_seconds(epoch.tzinfo.utcoffset(epoch)) if epoch.tzinfo else 0\n\n        return int((get_total_seconds(value - epoch) - offset) * 1000)\n\n\nclass Date(Column):\n    \"\"\"\n    Stores a simple date, with no time-of-day\n\n    .. versionchanged:: 2.6.0\n\n        removed overload of Date and DateTime. DateTime is a drop-in replacement for legacy models\n\n    requires C* 2.2+ and protocol v4+\n    \"\"\"\n    db_type = 'date'\n\n    def to_database(self, value):\n        if value is None:\n            return\n\n        # need to translate to int version because some dates are not representable in\n        # string form (datetime limitation)\n        d = value if isinstance(value, util.Date) else util.Date(value)\n        return d.days_from_epoch + SimpleDateType.EPOCH_OFFSET_DAYS\n\n    def to_python(self, value):\n        if value is None:\n            return\n        if isinstance(value, util.Date):\n            return value\n        if isinstance(value, datetime):\n            value = value.date()\n        return util.Date(value)\n\nclass Time(Column):\n    \"\"\"\n    Stores a timezone-naive time-of-day, with nanosecond precision\n\n    .. versionadded:: 2.6.0\n\n    requires C* 2.2+ and protocol v4+\n    \"\"\"\n    db_type = 'time'\n\n    def to_database(self, value):\n        value = super(Time, self).to_database(value)\n        if value is None:\n            return\n        # str(util.Time) yields desired CQL encoding\n        return value if isinstance(value, util.Time) else util.Time(value)\n\n    def to_python(self, value):\n        value = super(Time, self).to_database(value)\n        if value is None:\n            return\n        if isinstance(value, util.Time):\n            return value\n        return util.Time(value)\n\nclass Duration(Column):\n    \"\"\"\n    Stores a duration (months, days, nanoseconds)\n\n    .. versionadded:: 3.10.0\n\n    requires C* 3.10+ and protocol v4+\n    \"\"\"\n    db_type = 'duration'\n\n    def validate(self, value):\n        val = super(Duration, self).validate(value)\n        if val is None:\n            return\n        if not isinstance(val, _Duration):\n            raise TypeError('{0} {1} is not a valid Duration.'.format(self.column_name, value))\n        return val\n\n\nclass UUID(Column):\n    \"\"\"\n    Stores a type 1 or 4 UUID\n    \"\"\"\n    db_type = 'uuid'\n\n    def validate(self, value):\n        val = super(UUID, self).validate(value)\n        if val is None:\n            return\n        if isinstance(val, _UUID):\n            return val\n        if isinstance(val, str):\n            try:\n                return _UUID(val)\n            except ValueError:\n                # fall-through to error\n                pass\n        raise ValidationError(\"{0} {1} is not a valid uuid\".format(\n            self.column_name, value))\n\n    def to_python(self, value):\n        return self.validate(value)\n\n    def to_database(self, value):\n        return self.validate(value)\n\n\nclass TimeUUID(UUID):\n    \"\"\"\n    UUID containing timestamp\n    \"\"\"\n\n    db_type = 'timeuuid'\n\n\nclass Boolean(Column):\n    \"\"\"\n    Stores a boolean True or False value\n    \"\"\"\n    db_type = 'boolean'\n\n    def validate(self, value):\n        \"\"\" Always returns a Python boolean. \"\"\"\n        value = super(Boolean, self).validate(value)\n\n        if value is not None:\n            value = bool(value)\n\n        return value\n\n    def to_python(self, value):\n        return self.validate(value)\n\n\nclass BaseFloat(Column):\n    def validate(self, value):\n        value = super(BaseFloat, self).validate(value)\n        if value is None:\n            return\n        try:\n            return float(value)\n        except (TypeError, ValueError):\n            raise ValidationError(\"{0} {1} is not a valid float\".format(self.column_name, value))\n\n    def to_python(self, value):\n        return self.validate(value)\n\n    def to_database(self, value):\n        return self.validate(value)\n\n\nclass Float(BaseFloat):\n    \"\"\"\n    Stores a single-precision floating-point value\n    \"\"\"\n    db_type = 'float'\n\n\nclass Double(BaseFloat):\n    \"\"\"\n    Stores a double-precision floating-point value\n    \"\"\"\n    db_type = 'double'\n\n\nclass Decimal(Column):\n    \"\"\"\n    Stores a variable precision decimal value\n    \"\"\"\n    db_type = 'decimal'\n\n    def validate(self, value):\n        from decimal import Decimal as _Decimal\n        from decimal import InvalidOperation\n        val = super(Decimal, self).validate(value)\n        if val is None:\n            return\n        try:\n            return _Decimal(repr(val)) if isinstance(val, float) else _Decimal(val)\n        except InvalidOperation:\n            raise ValidationError(\"{0} '{1}' can't be coerced to decimal\".format(self.column_name, val))\n\n    def to_python(self, value):\n        return self.validate(value)\n\n    def to_database(self, value):\n        return self.validate(value)\n\n\nclass BaseCollectionColumn(Column):\n    \"\"\"\n    Base Container type for collection-like columns.\n\n    http://cassandra.apache.org/doc/cql3/CQL-3.0.html#collections\n    \"\"\"\n    def __init__(self, types, **kwargs):\n        \"\"\"\n        :param types: a sequence of sub types in this collection\n        \"\"\"\n        instances = []\n        for t in types:\n            inheritance_comparator = issubclass if isinstance(t, type) else isinstance\n            if not inheritance_comparator(t, Column):\n                raise ValidationError(\"%s is not a column class\" % (t,))\n            if t.db_type is None:\n                raise ValidationError(\"%s is an abstract type\" % (t,))\n            inst = t() if isinstance(t, type) else t\n            if isinstance(t, BaseCollectionColumn):\n                inst._freeze_db_type()\n            instances.append(inst)\n\n        self.types = instances\n        super(BaseCollectionColumn, self).__init__(**kwargs)\n\n    def validate(self, value):\n        value = super(BaseCollectionColumn, self).validate(value)\n        # It is dangerous to let collections have more than 65535.\n        # See: https://issues.apache.org/jira/browse/CASSANDRA-5428\n        if value is not None and len(value) > 65535:\n            raise ValidationError(\"{0} Collection can't have more than 65535 elements.\".format(self.column_name))\n        return value\n\n    def _val_is_null(self, val):\n        return not val\n\n    def _freeze_db_type(self):\n        if not self.db_type.startswith('frozen'):\n            self.db_type = \"frozen<%s>\" % (self.db_type,)\n\n    @property\n    def sub_types(self):\n        return self.types\n\n    @property\n    def cql_type(self):\n        return _cqltypes[self.__class__.__name__.lower()].apply_parameters([c.cql_type for c in self.types])\n\n\nclass Tuple(BaseCollectionColumn):\n    \"\"\"\n    Stores a fixed-length set of positional values\n\n    http://docs.datastax.com/en/cql/3.1/cql/cql_reference/tupleType.html\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        :param args: column types representing tuple composition\n        \"\"\"\n        if not args:\n            raise ValueError(\"Tuple must specify at least one inner type\")\n        super(Tuple, self).__init__(args, **kwargs)\n        self.db_type = 'tuple<{0}>'.format(', '.join(typ.db_type for typ in self.types))\n\n    def validate(self, value):\n        val = super(Tuple, self).validate(value)\n        if val is None:\n            return\n        if len(val) > len(self.types):\n            raise ValidationError(\"Value %r has more fields than tuple definition (%s)\" %\n                                  (val, ', '.join(t for t in self.types)))\n        return tuple(t.validate(v) for t, v in zip(self.types, val))\n\n    def to_python(self, value):\n        if value is None:\n            return tuple()\n        return tuple(t.to_python(v) for t, v in zip(self.types, value))\n\n    def to_database(self, value):\n        if value is None:\n            return\n        return tuple(t.to_database(v) for t, v in zip(self.types, value))\n\n\nclass BaseContainerColumn(BaseCollectionColumn):\n    pass\n\n\nclass Set(BaseContainerColumn):\n    \"\"\"\n    Stores a set of unordered, unique values\n\n    http://www.datastax.com/documentation/cql/3.1/cql/cql_using/use_set_t.html\n    \"\"\"\n\n    _python_type_hashable = False\n\n    def __init__(self, value_type, strict=True, default=set, **kwargs):\n        \"\"\"\n        :param value_type: a column class indicating the types of the value\n        :param strict: sets whether non set values will be coerced to set\n            type on validation, or raise a validation error, defaults to True\n        \"\"\"\n        self.strict = strict\n        super(Set, self).__init__((value_type,), default=default, **kwargs)\n        self.value_col = self.types[0]\n        if not self.value_col._python_type_hashable:\n            raise ValidationError(\"Cannot create a Set with unhashable value type (see PYTHON-494)\")\n        self.db_type = 'set<{0}>'.format(self.value_col.db_type)\n\n    def validate(self, value):\n        val = super(Set, self).validate(value)\n        if val is None:\n            return\n        types = (set, util.SortedSet) if self.strict else (set, util.SortedSet, list, tuple)\n        if not isinstance(val, types):\n            if self.strict:\n                raise ValidationError('{0} {1} is not a set object'.format(self.column_name, val))\n            else:\n                raise ValidationError('{0} {1} cannot be coerced to a set object'.format(self.column_name, val))\n\n        if None in val:\n            raise ValidationError(\"{0} None not allowed in a set\".format(self.column_name))\n        # TODO: stop doing this conversion because it doesn't support non-hashable collections as keys (cassandra does)\n        # will need to start using the cassandra.util types in the next major rev (PYTHON-494)\n        return set(self.value_col.validate(v) for v in val)\n\n    def to_python(self, value):\n        if value is None:\n            return set()\n        return set(self.value_col.to_python(v) for v in value)\n\n    def to_database(self, value):\n        if value is None:\n            return None\n        return set(self.value_col.to_database(v) for v in value)\n\n\nclass List(BaseContainerColumn):\n    \"\"\"\n    Stores a list of ordered values\n\n    http://www.datastax.com/documentation/cql/3.1/cql/cql_using/use_list_t.html\n    \"\"\"\n\n    _python_type_hashable = False\n\n    def __init__(self, value_type, default=list, **kwargs):\n        \"\"\"\n        :param value_type: a column class indicating the types of the value\n        \"\"\"\n        super(List, self).__init__((value_type,), default=default, **kwargs)\n        self.value_col = self.types[0]\n        self.db_type = 'list<{0}>'.format(self.value_col.db_type)\n\n    def validate(self, value):\n        val = super(List, self).validate(value)\n        if val is None:\n            return\n        if not isinstance(val, (set, list, tuple)):\n            raise ValidationError('{0} {1} is not a list object'.format(self.column_name, val))\n        if None in val:\n            raise ValidationError(\"{0} None is not allowed in a list\".format(self.column_name))\n        return [self.value_col.validate(v) for v in val]\n\n    def to_python(self, value):\n        if value is None:\n            return []\n        return [self.value_col.to_python(v) for v in value]\n\n    def to_database(self, value):\n        if value is None:\n            return None\n        return [self.value_col.to_database(v) for v in value]\n\n\nclass Map(BaseContainerColumn):\n    \"\"\"\n    Stores a key -> value map (dictionary)\n\n    https://docs.datastax.com/en/dse/6.7/cql/cql/cql_using/useMap.html\n    \"\"\"\n\n    _python_type_hashable = False\n\n    def __init__(self, key_type, value_type, default=dict, **kwargs):\n        \"\"\"\n        :param key_type: a column class indicating the types of the key\n        :param value_type: a column class indicating the types of the value\n        \"\"\"\n        super(Map, self).__init__((key_type, value_type), default=default, **kwargs)\n        self.key_col = self.types[0]\n        self.value_col = self.types[1]\n\n        if not self.key_col._python_type_hashable:\n            raise ValidationError(\"Cannot create a Map with unhashable key type (see PYTHON-494)\")\n\n        self.db_type = 'map<{0}, {1}>'.format(self.key_col.db_type, self.value_col.db_type)\n\n    def validate(self, value):\n        val = super(Map, self).validate(value)\n        if val is None:\n            return\n        if not isinstance(val, (dict, util.OrderedMap)):\n            raise ValidationError('{0} {1} is not a dict object'.format(self.column_name, val))\n        if None in val:\n            raise ValidationError(\"{0} None is not allowed in a map\".format(self.column_name))\n        # TODO: stop doing this conversion because it doesn't support non-hashable collections as keys (cassandra does)\n        # will need to start using the cassandra.util types in the next major rev (PYTHON-494)\n        return dict((self.key_col.validate(k), self.value_col.validate(v)) for k, v in val.items())\n\n    def to_python(self, value):\n        if value is None:\n            return {}\n        if value is not None:\n            return dict((self.key_col.to_python(k), self.value_col.to_python(v)) for k, v in value.items())\n\n    def to_database(self, value):\n        if value is None:\n            return None\n        return dict((self.key_col.to_database(k), self.value_col.to_database(v)) for k, v in value.items())\n\n\nclass UDTValueManager(BaseValueManager):\n    @property\n    def changed(self):\n        if self.explicit:\n            return self.value != self.previous_value\n\n        default_value = self.column.get_default()\n        if not self.column._val_is_null(default_value):\n            return self.value != default_value\n        elif self.previous_value is None:\n            return not self.column._val_is_null(self.value) and self.value.has_changed_fields()\n\n        return False\n\n    def reset_previous_value(self):\n        if self.value is not None:\n            self.value.reset_changed_fields()\n        self.previous_value = copy(self.value)\n\n\nclass UserDefinedType(Column):\n    \"\"\"\n    User Defined Type column\n\n    http://www.datastax.com/documentation/cql/3.1/cql/cql_using/cqlUseUDT.html\n\n    These columns are represented by a specialization of :class:`cassandra.cqlengine.usertype.UserType`.\n\n    Please see :ref:`user_types` for examples and discussion.\n    \"\"\"\n\n    value_manager = UDTValueManager\n\n    def __init__(self, user_type, **kwargs):\n        \"\"\"\n        :param type user_type: specifies the :class:`~.cqlengine.usertype.UserType` model of the column\n        \"\"\"\n        self.user_type = user_type\n        self.db_type = \"frozen<%s>\" % user_type.type_name()\n        super(UserDefinedType, self).__init__(**kwargs)\n\n    @property\n    def sub_types(self):\n        return list(self.user_type._fields.values())\n\n    @property\n    def cql_type(self):\n        return UserType.make_udt_class(keyspace='', udt_name=self.user_type.type_name(),\n                                       field_names=[c.db_field_name for c in self.user_type._fields.values()],\n                                       field_types=[c.cql_type for c in self.user_type._fields.values()])\n\n    def validate(self, value):\n        val = super(UserDefinedType, self).validate(value)\n        if val is None:\n            return\n        val.validate()\n        return val\n\n    def to_python(self, value):\n        if value is None:\n            return\n\n        for name, field in self.user_type._fields.items():\n            if value[name] is not None or isinstance(field, BaseContainerColumn):\n                value[name] = field.to_python(value[name])\n\n        return value\n\n    def to_database(self, value):\n        if value is None:\n            return\n\n        copied_value = deepcopy(value)\n        for name, field in self.user_type._fields.items():\n            if copied_value[name] is not None or isinstance(field, BaseContainerColumn):\n                copied_value[name] = field.to_database(copied_value[name])\n\n        return copied_value\n\n\ndef resolve_udts(col_def, out_list):\n    for col in col_def.sub_types:\n        resolve_udts(col, out_list)\n    if isinstance(col_def, UserDefinedType):\n        out_list.append(col_def.user_type)\n\n\nclass _PartitionKeysToken(Column):\n    \"\"\"\n    virtual column representing token of partition columns.\n    Used by filter(pk__token=Token(...)) filters\n    \"\"\"\n\n    def __init__(self, model):\n        self.partition_columns = list(model._partition_keys.values())\n        super(_PartitionKeysToken, self).__init__(partition_key=True)\n\n    @property\n    def db_field_name(self):\n        return 'token({0})'.format(', '.join(['\"{0}\"'.format(c.db_field_name) for c in self.partition_columns]))\n",
    "cassandra/policies.py": "# Copyright DataStax, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport random\n\nfrom collections import namedtuple\nfrom functools import lru_cache\nfrom itertools import islice, cycle, groupby, repeat\nimport logging\nfrom random import randint, shuffle\nfrom threading import Lock\nimport socket\nimport warnings\n\nlog = logging.getLogger(__name__)\n\nfrom cassandra import WriteType as WT\n\n\n# This is done this way because WriteType was originally\n# defined here and in order not to break the API.\n# It may removed in the next mayor.\nWriteType = WT\n\nfrom cassandra import ConsistencyLevel, OperationTimedOut\n\nclass HostDistance(object):\n    \"\"\"\n    A measure of how \"distant\" a node is from the client, which\n    may influence how the load balancer distributes requests\n    and how many connections are opened to the node.\n    \"\"\"\n\n    IGNORED = -1\n    \"\"\"\n    A node with this distance should never be queried or have\n    connections opened to it.\n    \"\"\"\n\n    LOCAL_RACK = 0\n    \"\"\"\n    Nodes with ``LOCAL_RACK`` distance will be preferred for operations\n    under some load balancing policies (such as :class:`.RackAwareRoundRobinPolicy`)\n    and will have a greater number of connections opened against\n    them by default.\n\n    This distance is typically used for nodes within the same\n    datacenter and the same rack as the client.\n    \"\"\"\n\n    LOCAL = 1\n    \"\"\"\n    Nodes with ``LOCAL`` distance will be preferred for operations\n    under some load balancing policies (such as :class:`.DCAwareRoundRobinPolicy`)\n    and will have a greater number of connections opened against\n    them by default.\n\n    This distance is typically used for nodes within the same\n    datacenter as the client.\n    \"\"\"\n\n    REMOTE = 2\n    \"\"\"\n    Nodes with ``REMOTE`` distance will be treated as a last resort\n    by some load balancing policies (such as :class:`.DCAwareRoundRobinPolicy`\n    and :class:`.RackAwareRoundRobinPolicy`)and will have a smaller number of\n    connections opened against them by default.\n\n    This distance is typically used for nodes outside of the\n    datacenter that the client is running in.\n    \"\"\"\n\n\nclass HostStateListener(object):\n\n    def on_up(self, host):\n        \"\"\" Called when a node is marked up. \"\"\"\n        raise NotImplementedError()\n\n    def on_down(self, host):\n        \"\"\" Called when a node is marked down. \"\"\"\n        raise NotImplementedError()\n\n    def on_add(self, host):\n        \"\"\"\n        Called when a node is added to the cluster.  The newly added node\n        should be considered up.\n        \"\"\"\n        raise NotImplementedError()\n\n    def on_remove(self, host):\n        \"\"\" Called when a node is removed from the cluster. \"\"\"\n        raise NotImplementedError()\n\n\nclass LoadBalancingPolicy(HostStateListener):\n    \"\"\"\n    Load balancing policies are used to decide how to distribute\n    requests among all possible coordinator nodes in the cluster.\n\n    In particular, they may focus on querying \"near\" nodes (those\n    in a local datacenter) or on querying nodes who happen to\n    be replicas for the requested data.\n\n    You may also use subclasses of :class:`.LoadBalancingPolicy` for\n    custom behavior.\n\n    You should always use immutable collections (e.g., tuples or\n    frozensets) to store information about hosts to prevent accidental\n    modification. When there are changes to the hosts (e.g., a host is\n    down or up), the old collection should be replaced with a new one.\n    \"\"\"\n\n    _hosts_lock = None\n\n    def __init__(self):\n        self._hosts_lock = Lock()\n\n    def distance(self, host):\n        \"\"\"\n        Returns a measure of how remote a :class:`~.pool.Host` is in\n        terms of the :class:`.HostDistance` enums.\n        \"\"\"\n        raise NotImplementedError()\n\n    def populate(self, cluster, hosts):\n        \"\"\"\n        This method is called to initialize the load balancing\n        policy with a set of :class:`.Host` instances before its\n        first use.  The `cluster` parameter is an instance of\n        :class:`.Cluster`.\n        \"\"\"\n        raise NotImplementedError()\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        \"\"\"\n        Given a :class:`~.query.Statement` instance, return a iterable\n        of :class:`.Host` instances which should be queried in that\n        order.  A generator may work well for custom implementations\n        of this method.\n\n        Note that the `query` argument may be :const:`None` when preparing\n        statements.\n\n        `working_keyspace` should be the string name of the current keyspace,\n        as set through :meth:`.Session.set_keyspace()` or with a ``USE``\n        statement.\n        \"\"\"\n        raise NotImplementedError()\n\n    def check_supported(self):\n        \"\"\"\n        This will be called after the cluster Metadata has been initialized.\n        If the load balancing policy implementation cannot be supported for\n        some reason (such as a missing C extension), this is the point at\n        which it should raise an exception.\n        \"\"\"\n        pass\n\n\nclass RoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    A subclass of :class:`.LoadBalancingPolicy` which evenly\n    distributes queries across all nodes in the cluster,\n    regardless of what datacenter the nodes may be in.\n    \"\"\"\n    _live_hosts = frozenset(())\n    _position = 0\n\n    def populate(self, cluster, hosts):\n        self._live_hosts = frozenset(hosts)\n        if len(hosts) > 1:\n            self._position = randint(0, len(hosts) - 1)\n\n    def distance(self, host):\n        return HostDistance.LOCAL\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        # not thread-safe, but we don't care much about lost increments\n        # for the purposes of load balancing\n        pos = self._position\n        self._position += 1\n\n        hosts = self._live_hosts\n        length = len(hosts)\n        if length:\n            pos %= length\n            return islice(cycle(hosts), pos, pos + length)\n        else:\n            return []\n\n    def on_up(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.union((host, ))\n\n    def on_down(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.difference((host, ))\n\n    def on_add(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.union((host, ))\n\n    def on_remove(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.difference((host, ))\n\n\nclass DCAwareRoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    Similar to :class:`.RoundRobinPolicy`, but prefers hosts\n    in the local datacenter and only uses nodes in remote\n    datacenters as a last resort.\n    \"\"\"\n\n    local_dc = None\n    used_hosts_per_remote_dc = 0\n\n    def __init__(self, local_dc='', used_hosts_per_remote_dc=0):\n        \"\"\"\n        The `local_dc` parameter should be the name of the datacenter\n        (such as is reported by ``nodetool ring``) that should\n        be considered local. If not specified, the driver will choose\n        a local_dc based on the first host among :attr:`.Cluster.contact_points`\n        having a valid DC. If relying on this mechanism, all specified\n        contact points should be nodes in a single, local DC.\n\n        `used_hosts_per_remote_dc` controls how many nodes in\n        each remote datacenter will have connections opened\n        against them. In other words, `used_hosts_per_remote_dc` hosts\n        will be considered :attr:`~.HostDistance.REMOTE` and the\n        rest will be considered :attr:`~.HostDistance.IGNORED`.\n        By default, all remote hosts are ignored.\n        \"\"\"\n        self.local_dc = local_dc\n        self.used_hosts_per_remote_dc = used_hosts_per_remote_dc\n        self._dc_live_hosts = {}\n        self._position = 0\n        self._endpoints = []\n        LoadBalancingPolicy.__init__(self)\n\n    def _dc(self, host):\n        return host.datacenter or self.local_dc\n\n    def populate(self, cluster, hosts):\n        for dc, dc_hosts in groupby(hosts, lambda h: self._dc(h)):\n            self._dc_live_hosts[dc] = tuple(set(dc_hosts))\n\n        if not self.local_dc:\n            self._endpoints = [\n                endpoint\n                for endpoint in cluster.endpoints_resolved]\n\n        self._position = randint(0, len(hosts) - 1) if hosts else 0\n\n    def distance(self, host):\n        dc = self._dc(host)\n        if dc == self.local_dc:\n            return HostDistance.LOCAL\n\n        if not self.used_hosts_per_remote_dc:\n            return HostDistance.IGNORED\n        else:\n            dc_hosts = self._dc_live_hosts.get(dc)\n            if not dc_hosts:\n                return HostDistance.IGNORED\n\n            if host in list(dc_hosts)[:self.used_hosts_per_remote_dc]:\n                return HostDistance.REMOTE\n            else:\n                return HostDistance.IGNORED\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        # not thread-safe, but we don't care much about lost increments\n        # for the purposes of load balancing\n        pos = self._position\n        self._position += 1\n\n        local_live = self._dc_live_hosts.get(self.local_dc, ())\n        pos = (pos % len(local_live)) if local_live else 0\n        for host in islice(cycle(local_live), pos, pos + len(local_live)):\n            yield host\n\n        # the dict can change, so get candidate DCs iterating over keys of a copy\n        other_dcs = [dc for dc in self._dc_live_hosts.copy().keys() if dc != self.local_dc]\n        for dc in other_dcs:\n            remote_live = self._dc_live_hosts.get(dc, ())\n            for host in remote_live[:self.used_hosts_per_remote_dc]:\n                yield host\n\n    def on_up(self, host):\n        # not worrying about threads because this will happen during\n        # control connection startup/refresh\n        if not self.local_dc and host.datacenter:\n            if host.endpoint in self._endpoints:\n                self.local_dc = host.datacenter\n                log.info(\"Using datacenter '%s' for DCAwareRoundRobinPolicy (via host '%s'); \"\n                         \"if incorrect, please specify a local_dc to the constructor, \"\n                         \"or limit contact points to local cluster nodes\" %\n                         (self.local_dc, host.endpoint))\n                del self._endpoints\n\n        dc = self._dc(host)\n        with self._hosts_lock:\n            current_hosts = self._dc_live_hosts.get(dc, ())\n            if host not in current_hosts:\n                self._dc_live_hosts[dc] = current_hosts + (host, )\n\n    def on_down(self, host):\n        dc = self._dc(host)\n        with self._hosts_lock:\n            current_hosts = self._dc_live_hosts.get(dc, ())\n            if host in current_hosts:\n                hosts = tuple(h for h in current_hosts if h != host)\n                if hosts:\n                    self._dc_live_hosts[dc] = hosts\n                else:\n                    del self._dc_live_hosts[dc]\n\n    def on_add(self, host):\n        self.on_up(host)\n\n    def on_remove(self, host):\n        self.on_down(host)\n\nclass RackAwareRoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    Similar to :class:`.DCAwareRoundRobinPolicy`, but prefers hosts\n    in the local rack, before hosts in the local datacenter but a\n    different rack, before hosts in all other datercentres\n    \"\"\"\n\n    local_dc = None\n    local_rack = None\n    used_hosts_per_remote_dc = 0\n\n    def __init__(self, local_dc, local_rack, used_hosts_per_remote_dc=0):\n        \"\"\"\n        The `local_dc` and `local_rack` parameters should be the name of the\n        datacenter and rack (such as is reported by ``nodetool ring``) that\n        should be considered local.\n\n        `used_hosts_per_remote_dc` controls how many nodes in\n        each remote datacenter will have connections opened\n        against them. In other words, `used_hosts_per_remote_dc` hosts\n        will be considered :attr:`~.HostDistance.REMOTE` and the\n        rest will be considered :attr:`~.HostDistance.IGNORED`.\n        By default, all remote hosts are ignored.\n        \"\"\"\n        self.local_rack = local_rack\n        self.local_dc = local_dc\n        self.used_hosts_per_remote_dc = used_hosts_per_remote_dc\n        self._live_hosts = {}\n        self._dc_live_hosts = {}\n        self._endpoints = []\n        self._position = 0\n        LoadBalancingPolicy.__init__(self)\n\n    def _rack(self, host):\n        return host.rack or self.local_rack\n\n    def _dc(self, host):\n        return host.datacenter or self.local_dc\n\n    def populate(self, cluster, hosts):\n        for (dc, rack), rack_hosts in groupby(hosts, lambda host: (self._dc(host), self._rack(host))):\n            self._live_hosts[(dc, rack)] = tuple(set(rack_hosts))\n        for dc, dc_hosts in groupby(hosts, lambda host: self._dc(host)):\n            self._dc_live_hosts[dc] = tuple(set(dc_hosts))\n\n        self._position = randint(0, len(hosts) - 1) if hosts else 0\n\n    def distance(self, host):\n        rack = self._rack(host)\n        dc = self._dc(host)\n        if rack == self.local_rack and dc == self.local_dc:\n            return HostDistance.LOCAL_RACK\n\n        if dc == self.local_dc:\n            return HostDistance.LOCAL\n\n        if not self.used_hosts_per_remote_dc:\n            return HostDistance.IGNORED\n\n        dc_hosts = self._dc_live_hosts.get(dc, ())\n        if not dc_hosts:\n            return HostDistance.IGNORED\n        if host in dc_hosts and dc_hosts.index(host) < self.used_hosts_per_remote_dc:\n            return HostDistance.REMOTE\n        else:\n            return HostDistance.IGNORED\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        pos = self._position\n        self._position += 1\n\n        local_rack_live = self._live_hosts.get((self.local_dc, self.local_rack), ())\n        pos = (pos % len(local_rack_live)) if local_rack_live else 0\n        # Slice the cyclic iterator to start from pos and include the next len(local_live) elements\n        # This ensures we get exactly one full cycle starting from pos\n        for host in islice(cycle(local_rack_live), pos, pos + len(local_rack_live)):\n            yield host\n\n        local_live = [host for host in self._dc_live_hosts.get(self.local_dc, ()) if host.rack != self.local_rack]\n        pos = (pos % len(local_live)) if local_live else 0\n        for host in islice(cycle(local_live), pos, pos + len(local_live)):\n            yield host\n\n        # the dict can change, so get candidate DCs iterating over keys of a copy\n        for dc, remote_live in self._dc_live_hosts.copy().items():\n            if dc != self.local_dc:\n                for host in remote_live[:self.used_hosts_per_remote_dc]:\n                    yield host\n\n    def on_up(self, host):\n        dc = self._dc(host)\n        rack = self._rack(host)\n        with self._hosts_lock:\n            current_rack_hosts = self._live_hosts.get((dc, rack), ())\n            if host not in current_rack_hosts:\n                self._live_hosts[(dc, rack)] = current_rack_hosts + (host, )\n            current_dc_hosts = self._dc_live_hosts.get(dc, ())\n            if host not in current_dc_hosts:\n                self._dc_live_hosts[dc] = current_dc_hosts + (host, )\n\n    def on_down(self, host):\n        dc = self._dc(host)\n        rack = self._rack(host)\n        with self._hosts_lock:\n            current_rack_hosts = self._live_hosts.get((dc, rack), ())\n            if host in current_rack_hosts:\n                hosts = tuple(h for h in current_rack_hosts if h != host)\n                if hosts:\n                    self._live_hosts[(dc, rack)] = hosts\n                else:\n                    del self._live_hosts[(dc, rack)]\n            current_dc_hosts = self._dc_live_hosts.get(dc, ())\n            if host in current_dc_hosts:\n                hosts = tuple(h for h in current_dc_hosts if h != host)\n                if hosts:\n                    self._dc_live_hosts[dc] = hosts\n                else:\n                    del self._dc_live_hosts[dc]\n\n    def on_add(self, host):\n        self.on_up(host)\n\n    def on_remove(self, host):\n        self.on_down(host)\n\nclass TokenAwarePolicy(LoadBalancingPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` wrapper that adds token awareness to\n    a child policy.\n\n    This alters the child policy's behavior so that it first attempts to\n    send queries to :attr:`~.HostDistance.LOCAL` replicas (as determined\n    by the child policy) based on the :class:`.Statement`'s\n    :attr:`~.Statement.routing_key`. If :attr:`.shuffle_replicas` is\n    truthy, these replicas will be yielded in a random order. Once those\n    hosts are exhausted, the remaining hosts in the child policy's query\n    plan will be used in the order provided by the child policy.\n\n    If no :attr:`~.Statement.routing_key` is set on the query, the child\n    policy's query plan will be used as is.\n    \"\"\"\n\n    _child_policy = None\n    _cluster_metadata = None\n    _tablets_routing_v1 = False\n    shuffle_replicas = False\n    \"\"\"\n    Yield local replicas in a random order.\n    \"\"\"\n\n    def __init__(self, child_policy, shuffle_replicas=False):\n        self._child_policy = child_policy\n        self.shuffle_replicas = shuffle_replicas\n\n    def populate(self, cluster, hosts):\n        self._cluster_metadata = cluster.metadata\n        self._tablets_routing_v1 = cluster.control_connection._tablets_routing_v1\n        self._child_policy.populate(cluster, hosts)\n\n    def check_supported(self):\n        if not self._cluster_metadata.can_support_partitioner():\n            raise RuntimeError(\n                '%s cannot be used with the cluster partitioner (%s) because '\n                'the relevant C extension for this driver was not compiled. '\n                'See the installation instructions for details on building '\n                'and installing the C extensions.' %\n                (self.__class__.__name__, self._cluster_metadata.partitioner))\n\n    def distance(self, *args, **kwargs):\n        return self._child_policy.distance(*args, **kwargs)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        keyspace = query.keyspace if query and query.keyspace else working_keyspace\n\n        child = self._child_policy\n        if query is None or query.routing_key is None or keyspace is None:\n            for host in child.make_query_plan(keyspace, query):\n                yield host\n            return\n\n        replicas = []\n        if self._tablets_routing_v1:\n            tablet = self._cluster_metadata._tablets.get_tablet_for_key(\n                keyspace, query.table, self._cluster_metadata.token_map.token_class.from_key(query.routing_key))\n\n            if tablet is not None:\n                replicas_mapped = set(map(lambda r: r[0], tablet.replicas))\n                child_plan = child.make_query_plan(keyspace, query)\n\n                replicas = [host for host in child_plan if host.host_id in replicas_mapped]\n\n        if not replicas:\n            replicas = self._cluster_metadata.get_replicas(keyspace, query.routing_key)\n\n        if self.shuffle_replicas:\n            shuffle(replicas)\n\n        for replica in replicas:\n            if replica.is_up and child.distance(replica) in [HostDistance.LOCAL, HostDistance.LOCAL_RACK]:\n                yield replica\n\n        for host in child.make_query_plan(keyspace, query):\n            # skip if we've already listed this host\n            if host not in replicas or child.distance(host) == HostDistance.REMOTE:\n                yield host\n\n    def on_up(self, *args, **kwargs):\n        return self._child_policy.on_up(*args, **kwargs)\n\n    def on_down(self, *args, **kwargs):\n        return self._child_policy.on_down(*args, **kwargs)\n\n    def on_add(self, *args, **kwargs):\n        return self._child_policy.on_add(*args, **kwargs)\n\n    def on_remove(self, *args, **kwargs):\n        return self._child_policy.on_remove(*args, **kwargs)\n\n\nclass WhiteListRoundRobinPolicy(RoundRobinPolicy):\n    \"\"\"\n    A subclass of :class:`.RoundRobinPolicy` which evenly\n    distributes queries across all nodes in the cluster,\n    regardless of what datacenter the nodes may be in, but\n    only if that node exists in the list of allowed nodes\n\n    This policy is addresses the issue described in\n    https://datastax-oss.atlassian.net/browse/JAVA-145\n    Where connection errors occur when connection\n    attempts are made to private IP addresses remotely\n    \"\"\"\n\n    def __init__(self, hosts):\n        \"\"\"\n        The `hosts` parameter should be a sequence of hosts to permit\n        connections to.\n        \"\"\"\n        self._allowed_hosts = tuple(hosts)\n        self._allowed_hosts_resolved = []\n        for h in self._allowed_hosts:\n            unix_socket_path = getattr(h, \"_unix_socket_path\", None)\n            if unix_socket_path:\n                self._allowed_hosts_resolved.append(unix_socket_path)\n            else:\n                self._allowed_hosts_resolved.extend([endpoint[4][0]\n                                        for endpoint in socket.getaddrinfo(h, None, socket.AF_UNSPEC, socket.SOCK_STREAM)])\n\n        RoundRobinPolicy.__init__(self)\n\n    def populate(self, cluster, hosts):\n        self._live_hosts = frozenset(h for h in hosts if h.address in self._allowed_hosts_resolved)\n\n        if len(hosts) <= 1:\n            self._position = 0\n        else:\n            self._position = randint(0, len(hosts) - 1)\n\n    def distance(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            return HostDistance.LOCAL\n        else:\n            return HostDistance.IGNORED\n\n    def on_up(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            RoundRobinPolicy.on_up(self, host)\n\n    def on_add(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            RoundRobinPolicy.on_add(self, host)\n\n\nclass HostFilterPolicy(LoadBalancingPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` subclass configured with a child policy,\n    and a single-argument predicate. This policy defers to the child policy for\n    hosts where ``predicate(host)`` is truthy. Hosts for which\n    ``predicate(host)`` is falsy will be considered :attr:`.IGNORED`, and will\n    not be used in a query plan.\n\n    This can be used in the cases where you need a whitelist or blacklist\n    policy, e.g. to prepare for decommissioning nodes or for testing:\n\n    .. code-block:: python\n\n        def address_is_ignored(host):\n            return host.address in [ignored_address0, ignored_address1]\n\n        blacklist_filter_policy = HostFilterPolicy(\n            child_policy=RoundRobinPolicy(),\n            predicate=address_is_ignored\n        )\n\n        cluster = Cluster(\n            primary_host,\n            load_balancing_policy=blacklist_filter_policy,\n        )\n\n    See the note in the :meth:`.make_query_plan` documentation for a caveat on\n    how wrapping ordering polices (e.g. :class:`.RoundRobinPolicy`) may break\n    desirable properties of the wrapped policy.\n\n    Please note that whitelist and blacklist policies are not recommended for\n    general, day-to-day use. You probably want something like\n    :class:`.DCAwareRoundRobinPolicy`, which prefers a local DC but has\n    fallbacks, over a brute-force method like whitelisting or blacklisting.\n    \"\"\"\n\n    def __init__(self, child_policy, predicate):\n        \"\"\"\n        :param child_policy: an instantiated :class:`.LoadBalancingPolicy`\n                             that this one will defer to.\n        :param predicate: a one-parameter function that takes a :class:`.Host`.\n                          If it returns a falsy value, the :class:`.Host` will\n                          be :attr:`.IGNORED` and not returned in query plans.\n        \"\"\"\n        super(HostFilterPolicy, self).__init__()\n        self._child_policy = child_policy\n        self._predicate = predicate\n\n    def on_up(self, host, *args, **kwargs):\n        return self._child_policy.on_up(host, *args, **kwargs)\n\n    def on_down(self, host, *args, **kwargs):\n        return self._child_policy.on_down(host, *args, **kwargs)\n\n    def on_add(self, host, *args, **kwargs):\n        return self._child_policy.on_add(host, *args, **kwargs)\n\n    def on_remove(self, host, *args, **kwargs):\n        return self._child_policy.on_remove(host, *args, **kwargs)\n\n    @property\n    def predicate(self):\n        \"\"\"\n        A predicate, set on object initialization, that takes a :class:`.Host`\n        and returns a value. If the value is falsy, the :class:`.Host` is\n        :class:`~HostDistance.IGNORED`. If the value is truthy,\n        :class:`.HostFilterPolicy` defers to the child policy to determine the\n        host's distance.\n\n        This is a read-only value set in ``__init__``, implemented as a\n        ``property``.\n        \"\"\"\n        return self._predicate\n\n    def distance(self, host):\n        \"\"\"\n        Checks if ``predicate(host)``, then returns\n        :attr:`~HostDistance.IGNORED` if falsy, and defers to the child policy\n        otherwise.\n        \"\"\"\n        if self.predicate(host):\n            return self._child_policy.distance(host)\n        else:\n            return HostDistance.IGNORED\n\n    def populate(self, cluster, hosts):\n        self._child_policy.populate(cluster=cluster, hosts=hosts)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        \"\"\"\n        Defers to the child policy's\n        :meth:`.LoadBalancingPolicy.make_query_plan` and filters the results.\n\n        Note that this filtering may break desirable properties of the wrapped\n        policy in some cases. For instance, imagine if you configure this\n        policy to filter out ``host2``, and to wrap a round-robin policy that\n        rotates through three hosts in the order ``host1, host2, host3``,\n        ``host2, host3, host1``, ``host3, host1, host2``, repeating. This\n        policy will yield ``host1, host3``, ``host3, host1``, ``host3, host1``,\n        disproportionately favoring ``host3``.\n        \"\"\"\n        child_qp = self._child_policy.make_query_plan(\n            working_keyspace=working_keyspace, query=query\n        )\n        for host in child_qp:\n            if self.predicate(host):\n                yield host\n\n    def check_supported(self):\n        return self._child_policy.check_supported()\n\n\nclass ConvictionPolicy(object):\n    \"\"\"\n    A policy which decides when hosts should be considered down\n    based on the types of failures and the number of failures.\n\n    If custom behavior is needed, this class may be subclassed.\n    \"\"\"\n\n    def __init__(self, host):\n        \"\"\"\n        `host` is an instance of :class:`.Host`.\n        \"\"\"\n        self.host = host\n\n    def add_failure(self, connection_exc):\n        \"\"\"\n        Implementations should return :const:`True` if the host should be\n        convicted, :const:`False` otherwise.\n        \"\"\"\n        raise NotImplementedError()\n\n    def reset(self):\n        \"\"\"\n        Implementations should clear out any convictions or state regarding\n        the host.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass SimpleConvictionPolicy(ConvictionPolicy):\n    \"\"\"\n    The default implementation of :class:`ConvictionPolicy`,\n    which simply marks a host as down after the first failure\n    of any kind.\n    \"\"\"\n\n    def add_failure(self, connection_exc):\n        return not isinstance(connection_exc, OperationTimedOut)\n\n    def reset(self):\n        pass\n\n\nclass ReconnectionPolicy(object):\n    \"\"\"\n    This class and its subclasses govern how frequently an attempt is made\n    to reconnect to nodes that are marked as dead.\n\n    If custom behavior is needed, this class may be subclassed.\n    \"\"\"\n\n    def new_schedule(self):\n        \"\"\"\n        This should return a finite or infinite iterable of delays (each as a\n        floating point number of seconds) in-between each failed reconnection\n        attempt.  Note that if the iterable is finite, reconnection attempts\n        will cease once the iterable is exhausted.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass ConstantReconnectionPolicy(ReconnectionPolicy):\n    \"\"\"\n    A :class:`.ReconnectionPolicy` subclass which sleeps for a fixed delay\n    in-between each reconnection attempt.\n    \"\"\"\n\n    def __init__(self, delay, max_attempts=64):\n        \"\"\"\n        `delay` should be a floating point number of seconds to wait in-between\n        each attempt.\n\n        `max_attempts` should be a total number of attempts to be made before\n        giving up, or :const:`None` to continue reconnection attempts forever.\n        The default is 64.\n        \"\"\"\n        if delay < 0:\n            raise ValueError(\"delay must not be negative\")\n        if max_attempts is not None and max_attempts < 0:\n            raise ValueError(\"max_attempts must not be negative\")\n\n        self.delay = delay\n        self.max_attempts = max_attempts\n\n    def new_schedule(self):\n        if self.max_attempts:\n            return repeat(self.delay, self.max_attempts)\n        return repeat(self.delay)\n\n\nclass ExponentialReconnectionPolicy(ReconnectionPolicy):\n    \"\"\"\n    A :class:`.ReconnectionPolicy` subclass which exponentially increases\n    the length of the delay in-between each reconnection attempt up to\n    a set maximum delay.\n\n    A random amount of jitter (+/- 15%) will be added to the pure exponential\n    delay value to avoid the situations where many reconnection handlers are\n    trying to reconnect at exactly the same time.\n    \"\"\"\n\n    # TODO: max_attempts is 64 to preserve legacy default behavior\n    # consider changing to None in major release to prevent the policy\n    # giving up forever\n    def __init__(self, base_delay, max_delay, max_attempts=64):\n        \"\"\"\n        `base_delay` and `max_delay` should be in floating point units of\n        seconds.\n\n        `max_attempts` should be a total number of attempts to be made before\n        giving up, or :const:`None` to continue reconnection attempts forever.\n        The default is 64.\n        \"\"\"\n        if base_delay < 0 or max_delay < 0:\n            raise ValueError(\"Delays may not be negative\")\n\n        if max_delay < base_delay:\n            raise ValueError(\"Max delay must be greater than base delay\")\n\n        if max_attempts is not None and max_attempts < 0:\n            raise ValueError(\"max_attempts must not be negative\")\n\n        self.base_delay = base_delay\n        self.max_delay = max_delay\n        self.max_attempts = max_attempts\n\n    def new_schedule(self):\n        i, overflowed = 0, False\n        while self.max_attempts is None or i < self.max_attempts:\n            if overflowed:\n                yield self.max_delay\n            else:\n                try:\n                    yield self._add_jitter(min(self.base_delay * (2 ** i), self.max_delay))\n                except OverflowError:\n                    overflowed = True\n                    yield self.max_delay\n\n            i += 1\n\n    # Adds -+ 15% to the delay provided\n    def _add_jitter(self, value):\n        jitter = randint(85, 115)\n        delay = (jitter * value) / 100\n        return min(max(self.base_delay, delay), self.max_delay)\n\n\nclass RetryPolicy(object):\n    \"\"\"\n    A policy that describes whether to retry, rethrow, or ignore coordinator\n    timeout and unavailable failures. These are failures reported from the\n    server side. Timeouts are configured by\n    `settings in cassandra.yaml <https://github.com/apache/cassandra/blob/cassandra-2.1.4/conf/cassandra.yaml#L568-L584>`_.\n    Unavailable failures occur when the coordinator cannot achieve the consistency\n    level for a request. For further information see the method descriptions\n    below.\n\n    To specify a default retry policy, set the\n    :attr:`.Cluster.default_retry_policy` attribute to an instance of this\n    class or one of its subclasses.\n\n    To specify a retry policy per query, set the :attr:`.Statement.retry_policy`\n    attribute to an instance of this class or one of its subclasses.\n\n    If custom behavior is needed for retrying certain operations,\n    this class may be subclassed.\n    \"\"\"\n\n    RETRY = 0\n    \"\"\"\n    This should be returned from the below methods if the operation\n    should be retried on the same connection.\n    \"\"\"\n\n    RETHROW = 1\n    \"\"\"\n    This should be returned from the below methods if the failure\n    should be propagated and no more retries attempted.\n    \"\"\"\n\n    IGNORE = 2\n    \"\"\"\n    This should be returned from the below methods if the failure\n    should be ignored but no more retries should be attempted.\n    \"\"\"\n\n    RETRY_NEXT_HOST = 3\n    \"\"\"\n    This should be returned from the below methods if the operation\n    should be retried on another connection.\n    \"\"\"\n\n    def on_read_timeout(self, query, consistency, required_responses,\n                        received_responses, data_retrieved, retry_num):\n        \"\"\"\n        This is called when a read operation times out from the coordinator's\n        perspective (i.e. a replica did not respond to the coordinator in time).\n        It should return a tuple with two items: one of the class enums (such\n        as :attr:`.RETRY`) and a :class:`.ConsistencyLevel` to retry the\n        operation at or :const:`None` to keep the same consistency level.\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        The `required_responses` and `received_responses` parameters describe\n        how many replicas needed to respond to meet the requested consistency\n        level and how many actually did respond before the coordinator timed\n        out the request. `data_retrieved` is a boolean indicating whether\n        any of those responses contained data (as opposed to just a digest).\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, operations will be retried at most once, and only if\n        a sufficient number of replicas responded (with data digests).\n        \"\"\"\n        if retry_num != 0:\n            return self.RETHROW, None\n        elif received_responses >= required_responses and not data_retrieved:\n            return self.RETRY, consistency\n        else:\n            return self.RETHROW, None\n\n    def on_write_timeout(self, query, consistency, write_type,\n                         required_responses, received_responses, retry_num):\n        \"\"\"\n        This is called when a write operation times out from the coordinator's\n        perspective (i.e. a replica did not respond to the coordinator in time).\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `write_type` is one of the :class:`.WriteType` enums describing the\n        type of write operation.\n\n        The `required_responses` and `received_responses` parameters describe\n        how many replicas needed to acknowledge the write to meet the requested\n        consistency level and how many replicas actually did acknowledge the\n        write before the coordinator timed out the request.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, failed write operations will retried at most once, and\n        they will only be retried if the `write_type` was\n        :attr:`~.WriteType.BATCH_LOG`.\n        \"\"\"\n        if retry_num != 0:\n            return self.RETHROW, None\n        elif write_type == WriteType.BATCH_LOG:\n            return self.RETRY, consistency\n        else:\n            return self.RETHROW, None\n\n    def on_unavailable(self, query, consistency, required_replicas, alive_replicas, retry_num):\n        \"\"\"\n        This is called when the coordinator node determines that a read or\n        write operation cannot be successful because the number of live\n        replicas are too low to meet the requested :class:`.ConsistencyLevel`.\n        This means that the read or write operation was never forwarded to\n        any replicas.\n\n        `query` is the :class:`.Statement` that failed.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `required_replicas` is the number of replicas that would have needed to\n        acknowledge the operation to meet the requested consistency level.\n        `alive_replicas` is the number of replicas that the coordinator\n        considered alive at the time of the request.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, if this is the first retry, it triggers a retry on the next\n        host in the query plan with the same consistency level. If this is not the\n        first retry, no retries will be attempted and the error will be re-raised.\n        \"\"\"\n        return (self.RETRY_NEXT_HOST, None) if retry_num == 0 else (self.RETHROW, None)\n\n    def on_request_error(self, query, consistency, error, retry_num):\n        \"\"\"\n        This is called when an unexpected error happens. This can be in the\n        following situations:\n\n        * On a connection error\n        * On server errors: overloaded, isBootstrapping, serverError, etc.\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `error` the instance of the exception.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, it triggers a retry on the next host in the query plan\n        with the same consistency level.\n        \"\"\"\n        # TODO revisit this for the next major\n        # To preserve the same behavior than before, we don't take retry_num into account\n        return self.RETRY_NEXT_HOST, None\n\n\nclass FallthroughRetryPolicy(RetryPolicy):\n    \"\"\"\n    A retry policy that never retries and always propagates failures to\n    the application.\n    \"\"\"\n\n    def on_read_timeout(self, *args, **kwargs):\n        return self.RETHROW, None\n\n    def on_write_timeout(self, *args, **kwargs):\n        return self.RETHROW, None\n\n    def on_unavailable(self, *args, **kwargs):\n        return self.RETHROW, None\n\n    def on_request_error(self, *args, **kwargs):\n        return self.RETHROW, None\n\n\nclass DowngradingConsistencyRetryPolicy(RetryPolicy):\n    \"\"\"\n    *Deprecated:* This retry policy will be removed in the next major release.\n\n    A retry policy that sometimes retries with a lower consistency level than\n    the one initially requested.\n\n    **BEWARE**: This policy may retry queries using a lower consistency\n    level than the one initially requested. By doing so, it may break\n    consistency guarantees. In other words, if you use this retry policy,\n    there are cases (documented below) where a read at :attr:`~.QUORUM`\n    *may not* see a preceding write at :attr:`~.QUORUM`. Do not use this\n    policy unless you have understood the cases where this can happen and\n    are ok with that. It is also recommended to subclass this class so\n    that queries that required a consistency level downgrade can be\n    recorded (so that repairs can be made later, etc).\n\n    This policy implements the same retries as :class:`.RetryPolicy`,\n    but on top of that, it also retries in the following cases:\n\n    * On a read timeout: if the number of replicas that responded is\n      greater than one but lower than is required by the requested\n      consistency level, the operation is retried at a lower consistency\n      level.\n    * On a write timeout: if the operation is an :attr:`~.UNLOGGED_BATCH`\n      and at least one replica acknowledged the write, the operation is\n      retried at a lower consistency level.  Furthermore, for other\n      write types, if at least one replica acknowledged the write, the\n      timeout is ignored.\n    * On an unavailable exception: if at least one replica is alive, the\n      operation is retried at a lower consistency level.\n\n    The reasoning behind this retry policy is as follows: if, based\n    on the information the Cassandra coordinator node returns, retrying the\n    operation with the initially requested consistency has a chance to\n    succeed, do it. Otherwise, if based on that information we know the\n    initially requested consistency level cannot be achieved currently, then:\n\n    * For writes, ignore the exception (thus silently failing the\n      consistency requirement) if we know the write has been persisted on at\n      least one replica.\n    * For reads, try reading at a lower consistency level (thus silently\n      failing the consistency requirement).\n\n    In other words, this policy implements the idea that if the requested\n    consistency level cannot be achieved, the next best thing for writes is\n    to make sure the data is persisted, and that reading something is better\n    than reading nothing, even if there is a risk of reading stale data.\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(DowngradingConsistencyRetryPolicy, self).__init__(*args, **kwargs)\n        warnings.warn('DowngradingConsistencyRetryPolicy is deprecated '\n                      'and will be removed in the next major release.',\n                      DeprecationWarning)\n\n    def _pick_consistency(self, num_responses):\n        if num_responses >= 3:\n            return self.RETRY, ConsistencyLevel.THREE\n        elif num_responses >= 2:\n            return self.RETRY, ConsistencyLevel.TWO\n        elif num_responses >= 1:\n            return self.RETRY, ConsistencyLevel.ONE\n        else:\n            return self.RETHROW, None\n\n    def on_read_timeout(self, query, consistency, required_responses,\n                        received_responses, data_retrieved, retry_num):\n        if retry_num != 0:\n            return self.RETHROW, None\n        elif ConsistencyLevel.is_serial(consistency):\n            # Downgrading does not make sense for a CAS read query\n            return self.RETHROW, None\n        elif received_responses < required_responses:\n            return self._pick_consistency(received_responses)\n        elif not data_retrieved:\n            return self.RETRY, consistency\n        else:\n            return self.RETHROW, None\n\n    def on_write_timeout(self, query, consistency, write_type,\n                         required_responses, received_responses, retry_num):\n        if retry_num != 0:\n            return self.RETHROW, None\n\n        if write_type in (WriteType.SIMPLE, WriteType.BATCH, WriteType.COUNTER):\n            if received_responses > 0:\n                # persisted on at least one replica\n                return self.IGNORE, None\n            else:\n                return self.RETHROW, None\n        elif write_type == WriteType.UNLOGGED_BATCH:\n            return self._pick_consistency(received_responses)\n        elif write_type == WriteType.BATCH_LOG:\n            return self.RETRY, consistency\n\n        return self.RETHROW, None\n\n    def on_unavailable(self, query, consistency, required_replicas, alive_replicas, retry_num):\n        if retry_num != 0:\n            return self.RETHROW, None\n        elif ConsistencyLevel.is_serial(consistency):\n            # failed at the paxos phase of a LWT, retry on the next host\n            return self.RETRY_NEXT_HOST, None\n        else:\n            return self._pick_consistency(alive_replicas)\n\n\nclass ExponentialBackoffRetryPolicy(RetryPolicy):\n    \"\"\"\n    A policy that do retries with exponential backoff\n    \"\"\"\n\n    def __init__(self, max_num_retries: float, min_interval: float = 0.1, max_interval: float = 10.0,\n                 *args, **kwargs):\n        \"\"\"\n        `max_num_retries` counts how many times the operation would be retried,\n        `min_interval` is the initial time in seconds to wait before first retry\n        `max_interval` is the maximum time to wait between retries\n        \"\"\"\n        self.min_interval = min_interval\n        self.max_num_retries = max_num_retries\n        self.max_interval = max_interval\n        super(ExponentialBackoffRetryPolicy).__init__(*args, **kwargs)\n\n    def _calculate_backoff(self, attempt: int):\n        delay = min(self.max_interval, self.min_interval * 2 ** attempt)\n        # add some jitter\n        delay += random.random() * self.min_interval - (self.min_interval / 2)\n        return delay\n\n    def on_read_timeout(self, query, consistency, required_responses,\n                        received_responses, data_retrieved, retry_num):\n        if retry_num < self.max_num_retries and received_responses >= required_responses and not data_retrieved:\n            return self.RETRY, consistency, self._calculate_backoff(retry_num)\n        else:\n            return self.RETHROW, None, None\n\n    def on_write_timeout(self, query, consistency, write_type,\n                         required_responses, received_responses, retry_num):\n        if retry_num < self.max_num_retries and write_type == WriteType.BATCH_LOG:\n            return self.RETRY, consistency, self._calculate_backoff(retry_num)\n        else:\n            return self.RETHROW, None, None\n\n    def on_unavailable(self, query, consistency, required_replicas,\n                       alive_replicas, retry_num):\n        if retry_num < self.max_num_retries:\n            return self.RETRY_NEXT_HOST, None, self._calculate_backoff(retry_num)\n        else:\n            return self.RETHROW, None, None\n\n    def on_request_error(self, query, consistency, error, retry_num):\n        if retry_num < self.max_num_retries:\n            return self.RETRY_NEXT_HOST, None, self._calculate_backoff(retry_num)\n        else:\n            return self.RETHROW, None, None\n\n\nclass AddressTranslator(object):\n    \"\"\"\n    Interface for translating cluster-defined endpoints.\n\n    The driver discovers nodes using server metadata and topology change events. Normally,\n    the endpoint defined by the server is the right way to connect to a node. In some environments,\n    these addresses may not be reachable, or not preferred (public vs. private IPs in cloud environments,\n    suboptimal routing, etc). This interface allows for translating from server defined endpoints to\n    preferred addresses for driver connections.\n\n    *Note:* :attr:`~Cluster.contact_points` provided while creating the :class:`~.Cluster` instance are not\n    translated using this mechanism -- only addresses received from Cassandra nodes are.\n    \"\"\"\n    def translate(self, addr):\n        \"\"\"\n        Accepts the node ip address, and returns a translated address to be used connecting to this node.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass IdentityTranslator(AddressTranslator):\n    \"\"\"\n    Returns the endpoint with no translation\n    \"\"\"\n    def translate(self, addr):\n        return addr\n\n\nclass EC2MultiRegionTranslator(AddressTranslator):\n    \"\"\"\n    Resolves private ips of the hosts in the same datacenter as the client, and public ips of hosts in other datacenters.\n    \"\"\"\n    def translate(self, addr):\n        \"\"\"\n        Reverse DNS the public broadcast_address, then lookup that hostname to get the AWS-resolved IP, which\n        will point to the private IP address within the same datacenter.\n        \"\"\"\n        # get family of this address so we translate to the same\n        family = socket.getaddrinfo(addr, 0, socket.AF_UNSPEC, socket.SOCK_STREAM)[0][0]\n        host = socket.getfqdn(addr)\n        for a in socket.getaddrinfo(host, 0, family, socket.SOCK_STREAM):\n            try:\n                return a[4][0]\n            except Exception:\n                pass\n        return addr\n\n\nclass SpeculativeExecutionPolicy(object):\n    \"\"\"\n    Interface for specifying speculative execution plans\n    \"\"\"\n\n    def new_plan(self, keyspace, statement):\n        \"\"\"\n        Returns\n\n        :param keyspace:\n        :param statement:\n        :return:\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass SpeculativeExecutionPlan(object):\n    def next_execution(self, host):\n        raise NotImplementedError()\n\n\nclass NoSpeculativeExecutionPlan(SpeculativeExecutionPlan):\n    def next_execution(self, host):\n        return -1\n\n\nclass NoSpeculativeExecutionPolicy(SpeculativeExecutionPolicy):\n\n    def new_plan(self, keyspace, statement):\n        return NoSpeculativeExecutionPlan()\n\n\nclass ConstantSpeculativeExecutionPolicy(SpeculativeExecutionPolicy):\n    \"\"\"\n    A speculative execution policy that sends a new query every X seconds (**delay**) for a maximum of Y attempts (**max_attempts**).\n    \"\"\"\n\n    def __init__(self, delay, max_attempts):\n        self.delay = delay\n        self.max_attempts = max_attempts\n\n    class ConstantSpeculativeExecutionPlan(SpeculativeExecutionPlan):\n        def __init__(self, delay, max_attempts):\n            self.delay = delay\n            self.remaining = max_attempts\n\n        def next_execution(self, host):\n            if self.remaining > 0:\n                self.remaining -= 1\n                return self.delay\n            else:\n                return -1\n\n    def new_plan(self, keyspace, statement):\n        return self.ConstantSpeculativeExecutionPlan(self.delay, self.max_attempts)\n\n\nclass WrapperPolicy(LoadBalancingPolicy):\n\n    def __init__(self, child_policy):\n        self._child_policy = child_policy\n\n    def distance(self, *args, **kwargs):\n        return self._child_policy.distance(*args, **kwargs)\n\n    def populate(self, cluster, hosts):\n        self._child_policy.populate(cluster, hosts)\n\n    def on_up(self, *args, **kwargs):\n        return self._child_policy.on_up(*args, **kwargs)\n\n    def on_down(self, *args, **kwargs):\n        return self._child_policy.on_down(*args, **kwargs)\n\n    def on_add(self, *args, **kwargs):\n        return self._child_policy.on_add(*args, **kwargs)\n\n    def on_remove(self, *args, **kwargs):\n        return self._child_policy.on_remove(*args, **kwargs)\n\n\nclass DefaultLoadBalancingPolicy(WrapperPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` wrapper that adds the ability to target a specific host first.\n\n    If no host is set on the query, the child policy's query plan will be used as is.\n    \"\"\"\n\n    _cluster_metadata = None\n\n    def populate(self, cluster, hosts):\n        self._cluster_metadata = cluster.metadata\n        self._child_policy.populate(cluster, hosts)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        if query and query.keyspace:\n            keyspace = query.keyspace\n        else:\n            keyspace = working_keyspace\n\n        # TODO remove next major since execute(..., host=XXX) is now available\n        addr = getattr(query, 'target_host', None) if query else None\n        target_host = self._cluster_metadata.get_host(addr)\n\n        child = self._child_policy\n        if target_host and target_host.is_up:\n            yield target_host\n            for h in child.make_query_plan(keyspace, query):\n                if h != target_host:\n                    yield h\n        else:\n            for h in child.make_query_plan(keyspace, query):\n                yield h\n\n\n# TODO for backward compatibility, remove in next major\nclass DSELoadBalancingPolicy(DefaultLoadBalancingPolicy):\n    \"\"\"\n    *Deprecated:* This will be removed in the next major release,\n    consider using :class:`.DefaultLoadBalancingPolicy`.\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(DSELoadBalancingPolicy, self).__init__(*args, **kwargs)\n        warnings.warn(\"DSELoadBalancingPolicy will be removed in 4.0. Consider using \"\n                      \"DefaultLoadBalancingPolicy.\", DeprecationWarning)\n\n\nclass NeverRetryPolicy(RetryPolicy):\n    def _rethrow(self, *args, **kwargs):\n        return self.RETHROW, None\n\n    on_read_timeout = _rethrow\n    on_write_timeout = _rethrow\n    on_unavailable = _rethrow\n\n\nColDesc = namedtuple('ColDesc', ['ks', 'table', 'col'])\n\nclass ColumnEncryptionPolicy(object):\n    \"\"\"\n    A policy enabling (mostly) transparent encryption and decryption of data before it is\n    sent to the cluster.\n\n    Key materials and other configurations are specified on a per-column basis.  This policy can\n    then be used by driver structures which are aware of the underlying columns involved in their\n    work.  In practice this includes the following cases:\n\n    * Prepared statements - data for columns specified by the cluster's policy will be transparently\n      encrypted before they are sent\n    * Rows returned from any query - data for columns specified by the cluster's policy will be\n      transparently decrypted before they are returned to the user\n\n    To enable this functionality, create an instance of this class (or more likely a subclass)\n    before creating a cluster.  This policy should then be configured and supplied to the Cluster\n    at creation time via the :attr:`.Cluster.column_encryption_policy` attribute.\n    \"\"\"\n\n    def encrypt(self, coldesc, obj_bytes):\n        \"\"\"\n        Encrypt the specified bytes using the cryptography materials for the specified column.\n        Largely used internally, although this could also be used to encrypt values supplied\n        to non-prepared statements in a way that is consistent with this policy.\n        \"\"\"\n        raise NotImplementedError()\n\n    def decrypt(self, coldesc, encrypted_bytes):\n        \"\"\"\n        Decrypt the specified (encrypted) bytes using the cryptography materials for the\n        specified column.  Used internally; could be used externally as well but there's\n        not currently an obvious use case.\n        \"\"\"\n        raise NotImplementedError()\n\n    def add_column(self, coldesc, key):\n        \"\"\"\n        Provide cryptography materials to be used when encrypted and/or decrypting data\n        for the specified column.\n        \"\"\"\n        raise NotImplementedError()\n\n    def contains_column(self, coldesc):\n        \"\"\"\n        Predicate to determine if a specific column is supported by this policy.\n        Currently only used internally.\n        \"\"\"\n        raise NotImplementedError()\n\n    def encode_and_encrypt(self, coldesc, obj):\n        \"\"\"\n        Helper function to enable use of this policy on simple (i.e. non-prepared)\n        statements.\n        \"\"\"\n        raise NotImplementedError()\n",
    "cassandra/__init__.py": "# Copyright DataStax, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom enum import Enum\nimport logging\n\n\nclass NullHandler(logging.Handler):\n\n    def emit(self, record):\n        pass\n\nlogging.getLogger('cassandra').addHandler(NullHandler())\n\n__version_info__ = (3, 28, 0)\n__version__ = '.'.join(map(str, __version_info__))\n\n\nclass ConsistencyLevel(object):\n    \"\"\"\n    Spcifies how many replicas must respond for an operation to be considered\n    a success.  By default, ``ONE`` is used for all operations.\n    \"\"\"\n\n    ANY = 0\n    \"\"\"\n    Only requires that one replica receives the write *or* the coordinator\n    stores a hint to replay later. Valid only for writes.\n    \"\"\"\n\n    ONE = 1\n    \"\"\"\n    Only one replica needs to respond to consider the operation a success\n    \"\"\"\n\n    TWO = 2\n    \"\"\"\n    Two replicas must respond to consider the operation a success\n    \"\"\"\n\n    THREE = 3\n    \"\"\"\n    Three replicas must respond to consider the operation a success\n    \"\"\"\n\n    QUORUM = 4\n    \"\"\"\n    ``ceil(RF/2) + 1`` replicas must respond to consider the operation a success\n    \"\"\"\n\n    ALL = 5\n    \"\"\"\n    All replicas must respond to consider the operation a success\n    \"\"\"\n\n    LOCAL_QUORUM = 6\n    \"\"\"\n    Requires a quorum of replicas in the local datacenter\n    \"\"\"\n\n    EACH_QUORUM = 7\n    \"\"\"\n    Requires a quorum of replicas in each datacenter\n    \"\"\"\n\n    SERIAL = 8\n    \"\"\"\n    For conditional inserts/updates that utilize Cassandra's lightweight\n    transactions, this requires consensus among all replicas for the\n    modified data.\n    \"\"\"\n\n    LOCAL_SERIAL = 9\n    \"\"\"\n    Like :attr:`~ConsistencyLevel.SERIAL`, but only requires consensus\n    among replicas in the local datacenter.\n    \"\"\"\n\n    LOCAL_ONE = 10\n    \"\"\"\n    Sends a request only to replicas in the local datacenter and waits for\n    one response.\n    \"\"\"\n\n    @staticmethod\n    def is_serial(cl):\n        return cl == ConsistencyLevel.SERIAL or cl == ConsistencyLevel.LOCAL_SERIAL\n\n\nConsistencyLevel.value_to_name = {\n    ConsistencyLevel.ANY: 'ANY',\n    ConsistencyLevel.ONE: 'ONE',\n    ConsistencyLevel.TWO: 'TWO',\n    ConsistencyLevel.THREE: 'THREE',\n    ConsistencyLevel.QUORUM: 'QUORUM',\n    ConsistencyLevel.ALL: 'ALL',\n    ConsistencyLevel.LOCAL_QUORUM: 'LOCAL_QUORUM',\n    ConsistencyLevel.EACH_QUORUM: 'EACH_QUORUM',\n    ConsistencyLevel.SERIAL: 'SERIAL',\n    ConsistencyLevel.LOCAL_SERIAL: 'LOCAL_SERIAL',\n    ConsistencyLevel.LOCAL_ONE: 'LOCAL_ONE'\n}\n\nConsistencyLevel.name_to_value = {\n    'ANY': ConsistencyLevel.ANY,\n    'ONE': ConsistencyLevel.ONE,\n    'TWO': ConsistencyLevel.TWO,\n    'THREE': ConsistencyLevel.THREE,\n    'QUORUM': ConsistencyLevel.QUORUM,\n    'ALL': ConsistencyLevel.ALL,\n    'LOCAL_QUORUM': ConsistencyLevel.LOCAL_QUORUM,\n    'EACH_QUORUM': ConsistencyLevel.EACH_QUORUM,\n    'SERIAL': ConsistencyLevel.SERIAL,\n    'LOCAL_SERIAL': ConsistencyLevel.LOCAL_SERIAL,\n    'LOCAL_ONE': ConsistencyLevel.LOCAL_ONE\n}\n\n\ndef consistency_value_to_name(value):\n    return ConsistencyLevel.value_to_name[value] if value is not None else \"Not Set\"\n\n\nclass ProtocolVersion(object):\n    \"\"\"\n    Defines native protocol versions supported by this driver.\n    \"\"\"\n    V1 = 1\n    \"\"\"\n    v1, supported in Cassandra 1.2-->2.2\n    \"\"\"\n\n    V2 = 2\n    \"\"\"\n    v2, supported in Cassandra 2.0-->2.2;\n    added support for lightweight transactions, batch operations, and automatic query paging.\n    \"\"\"\n\n    V3 = 3\n    \"\"\"\n    v3, supported in Cassandra 2.1-->3.x+;\n    added support for protocol-level client-side timestamps (see :attr:`.Session.use_client_timestamp`),\n    serial consistency levels for :class:`~.BatchStatement`, and an improved connection pool.\n    \"\"\"\n\n    V4 = 4\n    \"\"\"\n    v4, supported in Cassandra 2.2-->3.x+;\n    added a number of new types, server warnings, new failure messages, and custom payloads. Details in the\n    `project docs <https://github.com/apache/cassandra/blob/trunk/doc/native_protocol_v4.spec>`_\n    \"\"\"\n\n    V5 = 5\n    \"\"\"\n    v5, in beta from 3.x+. Finalised in 4.0-beta5\n    \"\"\"\n\n    V6 = 6\n    \"\"\"\n    v6, in beta from 4.0-beta5\n    \"\"\"\n\n    DSE_V1 = 0x41\n    \"\"\"\n    DSE private protocol v1, supported in DSE 5.1+\n    \"\"\"\n\n    DSE_V2 = 0x42\n    \"\"\"\n    DSE private protocol v2, supported in DSE 6.0+\n    \"\"\"\n\n    SUPPORTED_VERSIONS = (DSE_V2, DSE_V1, V6, V5, V4, V3, V2, V1)\n    \"\"\"\n    A tuple of all supported protocol versions\n    \"\"\"\n\n    BETA_VERSIONS = (V6,)\n    \"\"\"\n    A tuple of all beta protocol versions\n    \"\"\"\n\n    MIN_SUPPORTED = min(SUPPORTED_VERSIONS)\n    \"\"\"\n    Minimum protocol version supported by this driver.\n    \"\"\"\n\n    MAX_SUPPORTED = max(SUPPORTED_VERSIONS)\n    \"\"\"\n    Maximum protocol version supported by this driver.\n    \"\"\"\n\n    @classmethod\n    def get_lower_supported(cls, previous_version):\n        \"\"\"\n        Return the lower supported protocol version. Beta versions are omitted.\n        \"\"\"\n        try:\n            version = next(v for v in sorted(ProtocolVersion.SUPPORTED_VERSIONS, reverse=True) if\n                           v not in ProtocolVersion.BETA_VERSIONS and v < previous_version)\n        except StopIteration:\n            version = 0\n\n        return version\n\n    @classmethod\n    def uses_int_query_flags(cls, version):\n        return version >= cls.V5\n\n    @classmethod\n    def uses_prepare_flags(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def uses_prepared_metadata(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def uses_error_code_map(cls, version):\n        return version >= cls.V5\n\n    @classmethod\n    def uses_keyspace_flag(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def has_continuous_paging_support(cls, version):\n        return version >= cls.DSE_V1\n\n    @classmethod\n    def has_continuous_paging_next_pages(cls, version):\n        return version >= cls.DSE_V2\n\n    @classmethod\n    def has_checksumming_support(cls, version):\n        return cls.V5 <= version < cls.DSE_V1\n\n\nclass WriteType(object):\n    \"\"\"\n    For usage with :class:`.RetryPolicy`, this describe a type\n    of write operation.\n    \"\"\"\n\n    SIMPLE = 0\n    \"\"\"\n    A write to a single partition key. Such writes are guaranteed to be atomic\n    and isolated.\n    \"\"\"\n\n    BATCH = 1\n    \"\"\"\n    A write to multiple partition keys that used the distributed batch log to\n    ensure atomicity.\n    \"\"\"\n\n    UNLOGGED_BATCH = 2\n    \"\"\"\n    A write to multiple partition keys that did not use the distributed batch\n    log. Atomicity for such writes is not guaranteed.\n    \"\"\"\n\n    COUNTER = 3\n    \"\"\"\n    A counter write (for one or multiple partition keys). Such writes should\n    not be replayed in order to avoid overcount.\n    \"\"\"\n\n    BATCH_LOG = 4\n    \"\"\"\n    The initial write to the distributed batch log that Cassandra performs\n    internally before a BATCH write.\n    \"\"\"\n\n    CAS = 5\n    \"\"\"\n    A lighweight-transaction write, such as \"DELETE ... IF EXISTS\".\n    \"\"\"\n\n    VIEW = 6\n    \"\"\"\n    This WriteType is only seen in results for requests that were unable to\n    complete MV operations.\n    \"\"\"\n\n    CDC = 7\n    \"\"\"\n    This WriteType is only seen in results for requests that were unable to\n    complete CDC operations.\n    \"\"\"\n\n\nWriteType.name_to_value = {\n    'SIMPLE': WriteType.SIMPLE,\n    'BATCH': WriteType.BATCH,\n    'UNLOGGED_BATCH': WriteType.UNLOGGED_BATCH,\n    'COUNTER': WriteType.COUNTER,\n    'BATCH_LOG': WriteType.BATCH_LOG,\n    'CAS': WriteType.CAS,\n    'VIEW': WriteType.VIEW,\n    'CDC': WriteType.CDC\n}\n\n\nWriteType.value_to_name = {v: k for k, v in WriteType.name_to_value.items()}\n\n\nclass SchemaChangeType(object):\n    DROPPED = 'DROPPED'\n    CREATED = 'CREATED'\n    UPDATED = 'UPDATED'\n\n\nclass SchemaTargetType(object):\n    KEYSPACE = 'KEYSPACE'\n    TABLE = 'TABLE'\n    TYPE = 'TYPE'\n    FUNCTION = 'FUNCTION'\n    AGGREGATE = 'AGGREGATE'\n\n\nclass SignatureDescriptor(object):\n\n    def __init__(self, name, argument_types):\n        self.name = name\n        self.argument_types = argument_types\n\n    @property\n    def signature(self):\n        \"\"\"\n        function signature string in the form 'name([type0[,type1[...]]])'\n\n        can be used to uniquely identify overloaded function names within a keyspace\n        \"\"\"\n        return self.format_signature(self.name, self.argument_types)\n\n    @staticmethod\n    def format_signature(name, argument_types):\n        return \"%s(%s)\" % (name, ','.join(t for t in argument_types))\n\n    def __repr__(self):\n        return \"%s(%s, %s)\" % (self.__class__.__name__, self.name, self.argument_types)\n\n\nclass UserFunctionDescriptor(SignatureDescriptor):\n    \"\"\"\n    Describes a User function by name and argument signature\n    \"\"\"\n\n    name = None\n    \"\"\"\n    name of the function\n    \"\"\"\n\n    argument_types = None\n    \"\"\"\n    Ordered list of CQL argument type names comprising the type signature\n    \"\"\"\n\n\nclass UserAggregateDescriptor(SignatureDescriptor):\n    \"\"\"\n    Describes a User aggregate function by name and argument signature\n    \"\"\"\n\n    name = None\n    \"\"\"\n    name of the aggregate\n    \"\"\"\n\n    argument_types = None\n    \"\"\"\n    Ordered list of CQL argument type names comprising the type signature\n    \"\"\"\n\n\nclass DriverException(Exception):\n    \"\"\"\n    Base for all exceptions explicitly raised by the driver.\n    \"\"\"\n    pass\n\n\nclass RequestExecutionException(DriverException):\n    \"\"\"\n    Base for request execution exceptions returned from the server.\n    \"\"\"\n    pass\n\n\nclass Unavailable(RequestExecutionException):\n    \"\"\"\n    There were not enough live replicas to satisfy the requested consistency\n    level, so the coordinator node immediately failed the request without\n    forwarding it to any replicas.\n    \"\"\"\n\n    consistency = None\n    \"\"\" The requested :class:`ConsistencyLevel` \"\"\"\n\n    required_replicas = None\n    \"\"\" The number of replicas that needed to be live to complete the operation \"\"\"\n\n    alive_replicas = None\n    \"\"\" The number of replicas that were actually alive \"\"\"\n\n    def __init__(self, summary_message, consistency=None, required_replicas=None, alive_replicas=None):\n        self.consistency = consistency\n        self.required_replicas = required_replicas\n        self.alive_replicas = alive_replicas\n        Exception.__init__(self, summary_message + ' info=' +\n                           repr({'consistency': consistency_value_to_name(consistency),\n                                 'required_replicas': required_replicas,\n                                 'alive_replicas': alive_replicas}))\n\n\nclass Timeout(RequestExecutionException):\n    \"\"\"\n    Replicas failed to respond to the coordinator node before timing out.\n    \"\"\"\n\n    consistency = None\n    \"\"\" The requested :class:`ConsistencyLevel` \"\"\"\n\n    required_responses = None\n    \"\"\" The number of required replica responses \"\"\"\n\n    received_responses = None\n    \"\"\"\n    The number of replicas that responded before the coordinator timed out\n    the operation\n    \"\"\"\n\n    def __init__(self, summary_message, consistency=None, required_responses=None,\n                 received_responses=None, **kwargs):\n        self.consistency = consistency\n        self.required_responses = required_responses\n        self.received_responses = received_responses\n\n        if \"write_type\" in kwargs:\n            kwargs[\"write_type\"] = WriteType.value_to_name[kwargs[\"write_type\"]]\n\n        info = {'consistency': consistency_value_to_name(consistency),\n                'required_responses': required_responses,\n                'received_responses': received_responses}\n        info.update(kwargs)\n\n        Exception.__init__(self, summary_message + ' info=' + repr(info))\n\n\nclass ReadTimeout(Timeout):\n    \"\"\"\n    A subclass of :exc:`Timeout` for read operations.\n\n    This indicates that the replicas failed to respond to the coordinator\n    node before the configured timeout. This timeout is configured in\n    ``cassandra.yaml`` with the ``read_request_timeout_in_ms``\n    and ``range_request_timeout_in_ms`` options.\n    \"\"\"\n\n    data_retrieved = None\n    \"\"\"\n    A boolean indicating whether the requested data was retrieved\n    by the coordinator from any replicas before it timed out the\n    operation\n    \"\"\"\n\n    def __init__(self, message, data_retrieved=None, **kwargs):\n        Timeout.__init__(self, message, **kwargs)\n        self.data_retrieved = data_retrieved\n\n\nclass WriteTimeout(Timeout):\n    \"\"\"\n    A subclass of :exc:`Timeout` for write operations.\n\n    This indicates that the replicas failed to respond to the coordinator\n    node before the configured timeout. This timeout is configured in\n    ``cassandra.yaml`` with the ``write_request_timeout_in_ms``\n    option.\n    \"\"\"\n\n    write_type = None\n    \"\"\"\n    The type of write operation, enum on :class:`~cassandra.policies.WriteType`\n    \"\"\"\n\n    def __init__(self, message, write_type=None, **kwargs):\n        kwargs[\"write_type\"] = write_type\n        Timeout.__init__(self, message, **kwargs)\n        self.write_type = write_type\n\n\nclass CDCWriteFailure(RequestExecutionException):\n    \"\"\"\n    Hit limit on data in CDC folder, writes are rejected\n    \"\"\"\n    def __init__(self, message):\n        Exception.__init__(self, message)\n\n\nclass CoordinationFailure(RequestExecutionException):\n    \"\"\"\n    Replicas sent a failure to the coordinator.\n    \"\"\"\n\n    consistency = None\n    \"\"\" The requested :class:`ConsistencyLevel` \"\"\"\n\n    required_responses = None\n    \"\"\" The number of required replica responses \"\"\"\n\n    received_responses = None\n    \"\"\"\n    The number of replicas that responded before the coordinator timed out\n    the operation\n    \"\"\"\n\n    failures = None\n    \"\"\"\n    The number of replicas that sent a failure message\n    \"\"\"\n\n    error_code_map = None\n    \"\"\"\n    A map of inet addresses to error codes representing replicas that sent\n    a failure message.  Only set when `protocol_version` is 5 or higher.\n    \"\"\"\n\n    def __init__(self, summary_message, consistency=None, required_responses=None,\n                 received_responses=None, failures=None, error_code_map=None):\n        self.consistency = consistency\n        self.required_responses = required_responses\n        self.received_responses = received_responses\n        self.failures = failures\n        self.error_code_map = error_code_map\n\n        info_dict = {\n            'consistency': consistency_value_to_name(consistency),\n            'required_responses': required_responses,\n            'received_responses': received_responses,\n            'failures': failures\n        }\n\n        if error_code_map is not None:\n            # make error codes look like \"0x002a\"\n            formatted_map = dict((addr, '0x%04x' % err_code)\n                                 for (addr, err_code) in error_code_map.items())\n            info_dict['error_code_map'] = formatted_map\n\n        Exception.__init__(self, summary_message + ' info=' + repr(info_dict))\n\n\nclass ReadFailure(CoordinationFailure):\n    \"\"\"\n    A subclass of :exc:`CoordinationFailure` for read operations.\n\n    This indicates that the replicas sent a failure message to the coordinator.\n    \"\"\"\n\n    data_retrieved = None\n    \"\"\"\n    A boolean indicating whether the requested data was retrieved\n    by the coordinator from any replicas before it timed out the\n    operation\n    \"\"\"\n\n    def __init__(self, message, data_retrieved=None, **kwargs):\n        CoordinationFailure.__init__(self, message, **kwargs)\n        self.data_retrieved = data_retrieved\n\n\nclass WriteFailure(CoordinationFailure):\n    \"\"\"\n    A subclass of :exc:`CoordinationFailure` for write operations.\n\n    This indicates that the replicas sent a failure message to the coordinator.\n    \"\"\"\n\n    write_type = None\n    \"\"\"\n    The type of write operation, enum on :class:`~cassandra.policies.WriteType`\n    \"\"\"\n\n    def __init__(self, message, write_type=None, **kwargs):\n        CoordinationFailure.__init__(self, message, **kwargs)\n        self.write_type = write_type\n\n\nclass FunctionFailure(RequestExecutionException):\n    \"\"\"\n    User Defined Function failed during execution\n    \"\"\"\n\n    keyspace = None\n    \"\"\"\n    Keyspace of the function\n    \"\"\"\n\n    function = None\n    \"\"\"\n    Name of the function\n    \"\"\"\n\n    arg_types = None\n    \"\"\"\n    List of argument type names of the function\n    \"\"\"\n\n    def __init__(self, summary_message, keyspace, function, arg_types):\n        self.keyspace = keyspace\n        self.function = function\n        self.arg_types = arg_types\n        Exception.__init__(self, summary_message)\n\n\nclass RequestValidationException(DriverException):\n    \"\"\"\n    Server request validation failed\n    \"\"\"\n    pass\n\n\nclass ConfigurationException(RequestValidationException):\n    \"\"\"\n    Server indicated request errro due to current configuration\n    \"\"\"\n    pass\n\n\nclass AlreadyExists(ConfigurationException):\n    \"\"\"\n    An attempt was made to create a keyspace or table that already exists.\n    \"\"\"\n\n    keyspace = None\n    \"\"\"\n    The name of the keyspace that already exists, or, if an attempt was\n    made to create a new table, the keyspace that the table is in.\n    \"\"\"\n\n    table = None\n    \"\"\"\n    The name of the table that already exists, or, if an attempt was\n    make to create a keyspace, :const:`None`.\n    \"\"\"\n\n    def __init__(self, keyspace=None, table=None):\n        if table:\n            message = \"Table '%s.%s' already exists\" % (keyspace, table)\n        else:\n            message = \"Keyspace '%s' already exists\" % (keyspace,)\n\n        Exception.__init__(self, message)\n        self.keyspace = keyspace\n        self.table = table\n\n\nclass InvalidRequest(RequestValidationException):\n    \"\"\"\n    A query was made that was invalid for some reason, such as trying to set\n    the keyspace for a connection to a nonexistent keyspace.\n    \"\"\"\n    pass\n\n\nclass Unauthorized(RequestValidationException):\n    \"\"\"\n    The current user is not authorized to perform the requested operation.\n    \"\"\"\n    pass\n\n\nclass AuthenticationFailed(DriverException):\n    \"\"\"\n    Failed to authenticate.\n    \"\"\"\n    pass\n\n\nclass OperationTimedOut(DriverException):\n    \"\"\"\n    The operation took longer than the specified (client-side) timeout\n    to complete.  This is not an error generated by Cassandra, only\n    the driver.\n    \"\"\"\n\n    errors = None\n    \"\"\"\n    A dict of errors keyed by the :class:`~.Host` against which they occurred.\n    \"\"\"\n\n    last_host = None\n    \"\"\"\n    The last :class:`~.Host` this operation was attempted against.\n    \"\"\"\n\n    def __init__(self, errors=None, last_host=None):\n        self.errors = errors\n        self.last_host = last_host\n        message = \"errors=%s, last_host=%s\" % (self.errors, self.last_host)\n        Exception.__init__(self, message)\n\n\nclass UnsupportedOperation(DriverException):\n    \"\"\"\n    An attempt was made to use a feature that is not supported by the\n    selected protocol version.  See :attr:`Cluster.protocol_version`\n    for more details.\n    \"\"\"\n    pass\n\n\nclass UnresolvableContactPoints(DriverException):\n    \"\"\"\n    The driver was unable to resolve any provided hostnames.\n\n    Note that this is *not* raised when a :class:`.Cluster` is created with no\n    contact points, only when lookup fails for all hosts\n    \"\"\"\n    pass\n\n\nclass OperationType(Enum):\n    Read = 0\n    Write = 1\n\nclass RateLimitReached(ConfigurationException):\n    '''\n    Rate limit was exceeded for a partition affected by the request.\n    '''\n    op_type = None\n    rejected_by_coordinator = False\n\n    def __init__(self, op_type=None, rejected_by_coordinator=False):\n        self.op_type = op_type\n        self.rejected_by_coordinator = rejected_by_coordinator\n        message = f\"[request_error_rate_limit_reached OpType={op_type.name} RejectedByCoordinator={rejected_by_coordinator}]\"\n        Exception.__init__(self, message)\n"
  },
  "GT_src_dict": {
    "cassandra/cqlengine/columns.py": {
      "Column.__init__": {
        "code": "    def __init__(self, primary_key=False, partition_key=False, index=False, db_field=None, default=None, required=False, clustering_order=None, discriminator_column=False, static=False, custom_index=False):\n        \"\"\"Initializes a Column object representing a database column within a Cassandra model.\n\nParameters:\n- primary_key (bool): Indicates if the column is a primary key. Defaults to False.\n- partition_key (bool): Indicates if the column is a partition key. Defaults to False.\n- index (bool): Flag for creating an index on this column. Defaults to False.\n- db_field (str): The database field name this column maps to. Defaults to None.\n- default (any): The default value for the column, which can either be a fixed value or a callable (no arguments). Defaults to None.\n- required (bool): Indicates if the column is mandatory (i.e., cannot be None). Defaults to False.\n- clustering_order (any): Specifies the order of clustering keys. Defaults to None.\n- discriminator_column (bool): Indicates if this column is used for model inheritance discrimination. Defaults to False.\n- static (bool): Indicates if this column is static, having a single value per partition. Defaults to False.\n- custom_index (bool): Indicates if the index is managed outside of cqlengine. Defaults to False.\n\nAttributes:\n- column_name (str): The name of the column in the model definition. Initialized to None.\n- _partition_key_index (int or None): An internal index to manage partition key positions. Initialized to None.\n- static (bool): Reflects if the column is defined as static.\n- value (any): Holds the column's value; initially set to None.\n- position (int): Tracks instantiation order, incremented with each new instantiation.\n\nSide Effects: Each instantiation increments the class-level instance_counter, ensuring each column instance has a unique position identifier.\"\"\"\n        self.partition_key = partition_key\n        self.primary_key = partition_key or primary_key\n        self.index = index\n        self.custom_index = custom_index\n        self.db_field = db_field\n        self.default = default\n        self.required = required\n        self.clustering_order = clustering_order\n        self.discriminator_column = discriminator_column\n        self.column_name = None\n        self._partition_key_index = None\n        self.static = static\n        self.value = None\n        self.position = Column.instance_counter\n        Column.instance_counter += 1",
        "docstring": "Initializes a Column object representing a database column within a Cassandra model.\n\nParameters:\n- primary_key (bool): Indicates if the column is a primary key. Defaults to False.\n- partition_key (bool): Indicates if the column is a partition key. Defaults to False.\n- index (bool): Flag for creating an index on this column. Defaults to False.\n- db_field (str): The database field name this column maps to. Defaults to None.\n- default (any): The default value for the column, which can either be a fixed value or a callable (no arguments). Defaults to None.\n- required (bool): Indicates if the column is mandatory (i.e., cannot be None). Defaults to False.\n- clustering_order (any): Specifies the order of clustering keys. Defaults to None.\n- discriminator_column (bool): Indicates if this column is used for model inheritance discrimination. Defaults to False.\n- static (bool): Indicates if this column is static, having a single value per partition. Defaults to False.\n- custom_index (bool): Indicates if the index is managed outside of cqlengine. Defaults to False.\n\nAttributes:\n- column_name (str): The name of the column in the model definition. Initialized to None.\n- _partition_key_index (int or None): An internal index to manage partition key positions. Initialized to None.\n- static (bool): Reflects if the column is defined as static.\n- value (any): Holds the column's value; initially set to None.\n- position (int): Tracks instantiation order, incremented with each new instantiation.\n\nSide Effects: Each instantiation increments the class-level instance_counter, ensuring each column instance has a unique position identifier.",
        "signature": "def __init__(self, primary_key=False, partition_key=False, index=False, db_field=None, default=None, required=False, clustering_order=None, discriminator_column=False, static=False, custom_index=False):",
        "type": "Method",
        "class_signature": "class Column(object):"
      },
      "Column.__ne__": {
        "code": "    def __ne__(self, other):\n        \"\"\"Compares this Column instance with another for inequality.\n\nThis method implements the \"not equal to\" operation (`!=`) for Column instances by comparing their instantiation order. It checks if the `other` parameter is an instance of the Column class, and if so, compares their `position` attributes. The `position` attribute is an integer that represents the order in which the column instances are created; it is assigned during initialization and helps determine the uniqueness of each Column instance.\n\nParameters:\n- other (Column or any): The value to compare against this Column instance.\n\nReturns:\n- bool: True if the two Column instances are not equal (i.e., their positions differ), False otherwise. Returns NotImplemented if `other` is not an instance of Column.\n\nDependencies:\n- The method relies on the `position` attribute, which is defined in the Column class's `__init__` method as a unique identifier for column instances. It increments each time a new Column instance is created.\"\"\"\n        if isinstance(other, Column):\n            return self.position != other.position\n        return NotImplemented",
        "docstring": "Compares this Column instance with another for inequality.\n\nThis method implements the \"not equal to\" operation (`!=`) for Column instances by comparing their instantiation order. It checks if the `other` parameter is an instance of the Column class, and if so, compares their `position` attributes. The `position` attribute is an integer that represents the order in which the column instances are created; it is assigned during initialization and helps determine the uniqueness of each Column instance.\n\nParameters:\n- other (Column or any): The value to compare against this Column instance.\n\nReturns:\n- bool: True if the two Column instances are not equal (i.e., their positions differ), False otherwise. Returns NotImplemented if `other` is not an instance of Column.\n\nDependencies:\n- The method relies on the `position` attribute, which is defined in the Column class's `__init__` method as a unique identifier for column instances. It increments each time a new Column instance is created.",
        "signature": "def __ne__(self, other):",
        "type": "Method",
        "class_signature": "class Column(object):"
      },
      "Column.__eq__": {
        "code": "    def __eq__(self, other):\n        \"\"\"Checks for equality between two instances of the Column class based on their instantiation order.\n\nParameters:\n- other (Column): The other Column instance to compare against.\n\nReturns:\n- bool: True if the positions of the two Column instances are identical, indicating they are considered equal; False otherwise.\n\nNotes:\n- This method relies on the `position` attribute, which is assigned during the initialization of the Column class. \n- The `position` attribute is used to keep track of the order of instantiation among Column instances, allowing for comparisons that are sensitive to the order of declaration in model definitions.\n- If `other` is not an instance of Column, NotImplemented is returned, signaling that the comparison cannot be performed.\"\"\"\n        if isinstance(other, Column):\n            return self.position == other.position\n        return NotImplemented",
        "docstring": "Checks for equality between two instances of the Column class based on their instantiation order.\n\nParameters:\n- other (Column): The other Column instance to compare against.\n\nReturns:\n- bool: True if the positions of the two Column instances are identical, indicating they are considered equal; False otherwise.\n\nNotes:\n- This method relies on the `position` attribute, which is assigned during the initialization of the Column class. \n- The `position` attribute is used to keep track of the order of instantiation among Column instances, allowing for comparisons that are sensitive to the order of declaration in model definitions.\n- If `other` is not an instance of Column, NotImplemented is returned, signaling that the comparison cannot be performed.",
        "signature": "def __eq__(self, other):",
        "type": "Method",
        "class_signature": "class Column(object):"
      },
      "Column.__lt__": {
        "code": "    def __lt__(self, other):\n        \"\"\"Compares the current Column instance with another Column instance for ordering.\n\nParameters:\n    other (Column): The Column instance to compare against.\n\nReturns:\n    bool: True if the current instance's position is less than the position of the other instance, otherwise False. If 'other' is not a Column instance, NotImplemented is returned.\n\nThe 'position' attribute, which is an integer representing the order of instantiation of the Column, is used for comparison. This attribute is defined in the Column class and is incremented for each new instance created, thus allowing for a natural ordering among Column instances based on their creation order.\"\"\"\n        if isinstance(other, Column):\n            return self.position < other.position\n        return NotImplemented",
        "docstring": "Compares the current Column instance with another Column instance for ordering.\n\nParameters:\n    other (Column): The Column instance to compare against.\n\nReturns:\n    bool: True if the current instance's position is less than the position of the other instance, otherwise False. If 'other' is not a Column instance, NotImplemented is returned.\n\nThe 'position' attribute, which is an integer representing the order of instantiation of the Column, is used for comparison. This attribute is defined in the Column class and is incremented for each new instance created, thus allowing for a natural ordering among Column instances based on their creation order.",
        "signature": "def __lt__(self, other):",
        "type": "Method",
        "class_signature": "class Column(object):"
      },
      "Column.__le__": {
        "code": "    def __le__(self, other):\n        \"\"\"Defines the less-than-or-equal-to comparison operator for instances of the Column class.\n\nParameters:\n    other (Column): Another instance of the Column class to compare against.\n\nReturns:\n    bool: True if the current instance's position is less than or equal to the other instance's position; otherwise, False.\n\nNotes:\n    The 'position' attribute is an integer that tracks the order of instantiation of Column instances, with lower values indicating earlier creation. This method enables comparison between Column instances based on their instantiation order.\"\"\"\n        if isinstance(other, Column):\n            return self.position <= other.position\n        return NotImplemented",
        "docstring": "Defines the less-than-or-equal-to comparison operator for instances of the Column class.\n\nParameters:\n    other (Column): Another instance of the Column class to compare against.\n\nReturns:\n    bool: True if the current instance's position is less than or equal to the other instance's position; otherwise, False.\n\nNotes:\n    The 'position' attribute is an integer that tracks the order of instantiation of Column instances, with lower values indicating earlier creation. This method enables comparison between Column instances based on their instantiation order.",
        "signature": "def __le__(self, other):",
        "type": "Method",
        "class_signature": "class Column(object):"
      },
      "Column.__gt__": {
        "code": "    def __gt__(self, other):\n        \"\"\"Determines if the current Column instance is greater than another Column instance based on their instantiation order.\n\nParameters:\n- other (Column): An instance of the Column class to compare against.\n\nReturns:\n- bool: Returns True if the position of the current Column instance is greater than the position of the other Column instance. If 'other' is not an instance of Column, NotImplemented is returned.\n\nThis method relies on the `position` attribute, which is assigned during the initialization of a Column instance to maintain the order of instantiation. The `position` is utilized to establish the ordering of columns in data models.\"\"\"\n        if isinstance(other, Column):\n            return self.position > other.position\n        return NotImplemented",
        "docstring": "Determines if the current Column instance is greater than another Column instance based on their instantiation order.\n\nParameters:\n- other (Column): An instance of the Column class to compare against.\n\nReturns:\n- bool: Returns True if the position of the current Column instance is greater than the position of the other Column instance. If 'other' is not an instance of Column, NotImplemented is returned.\n\nThis method relies on the `position` attribute, which is assigned during the initialization of a Column instance to maintain the order of instantiation. The `position` is utilized to establish the ordering of columns in data models.",
        "signature": "def __gt__(self, other):",
        "type": "Method",
        "class_signature": "class Column(object):"
      },
      "Column.__ge__": {
        "code": "    def __ge__(self, other):\n        \"\"\"Compares the position of two Column instances to determine if the current column is greater than or equal to the other.\n\n    :param other: The other Column instance to compare against.\n    :type other: Column\n    :return: True if the current column's position is greater than or equal to the other column's position; otherwise, False.\n    :rtype: boolean\n\n    The position attribute is an integer that keeps track of the instantiation order of columns. It is defined as \n    a class-level attribute (`instance_counter`) in the Column class, which increments every time a new Column \n    instance is created. This method allows for ordering and comparison operations within the Column hierarchy.\"\"\"\n        if isinstance(other, Column):\n            return self.position >= other.position\n        return NotImplemented",
        "docstring": "Compares the position of two Column instances to determine if the current column is greater than or equal to the other.\n\n:param other: The other Column instance to compare against.\n:type other: Column\n:return: True if the current column's position is greater than or equal to the other column's position; otherwise, False.\n:rtype: boolean\n\nThe position attribute is an integer that keeps track of the instantiation order of columns. It is defined as \na class-level attribute (`instance_counter`) in the Column class, which increments every time a new Column \ninstance is created. This method allows for ordering and comparison operations within the Column hierarchy.",
        "signature": "def __ge__(self, other):",
        "type": "Method",
        "class_signature": "class Column(object):"
      },
      "Column.__hash__": {
        "code": "    def __hash__(self):\n        \"\"\"Computes the hash value for the Column instance using its unique identifier.\n\nThis method is essential for using Column instances in hash-based collections such as sets and dictionaries, ensuring that each instance can be uniquely identified based on its memory address. It does not take any parameters and will always return an integer representing the hash value.\n\nReturns:\n    int: The memory address of the Column instance, which serves as its hash value.\"\"\"\n        return id(self)",
        "docstring": "Computes the hash value for the Column instance using its unique identifier.\n\nThis method is essential for using Column instances in hash-based collections such as sets and dictionaries, ensuring that each instance can be uniquely identified based on its memory address. It does not take any parameters and will always return an integer representing the hash value.\n\nReturns:\n    int: The memory address of the Column instance, which serves as its hash value.",
        "signature": "def __hash__(self):",
        "type": "Method",
        "class_signature": "class Column(object):"
      }
    },
    "cassandra/policies.py": {},
    "cassandra/__init__.py": {}
  },
  "dependency_dict": {
    "cassandra/cqlengine/columns.py:Column:__ne__": {},
    "cassandra/cqlengine/columns.py:Column:Column": {},
    "cassandra/cqlengine/columns.py:Column:__eq__": {},
    "cassandra/cqlengine/columns.py:Column:__lt__": {},
    "cassandra/cqlengine/columns.py:Column:__le__": {},
    "cassandra/cqlengine/columns.py:Column:__gt__": {},
    "cassandra/cqlengine/columns.py:Column:__ge__": {}
  },
  "call_tree": {
    "tests/unit/cqlengine/test_columns.py:ColumnTest:test_comparisons": {
      "cassandra/cqlengine/columns.py:Column:__init__": {},
      "cassandra/cqlengine/columns.py:Column:__ne__": {
        "cassandra/cqlengine/columns.py:Column:Column": {}
      },
      "cassandra/cqlengine/columns.py:Column:__eq__": {
        "cassandra/cqlengine/columns.py:Column:Column": {}
      },
      "cassandra/cqlengine/columns.py:Column:__lt__": {
        "cassandra/cqlengine/columns.py:Column:Column": {}
      },
      "cassandra/cqlengine/columns.py:Column:__le__": {
        "cassandra/cqlengine/columns.py:Column:Column": {}
      },
      "cassandra/cqlengine/columns.py:Column:__gt__": {
        "cassandra/cqlengine/columns.py:Column:Column": {}
      },
      "cassandra/cqlengine/columns.py:Column:__ge__": {
        "cassandra/cqlengine/columns.py:Column:Column": {}
      }
    },
    "tests/unit/cqlengine/test_columns.py:ColumnTest:test_hash": {
      "cassandra/cqlengine/columns.py:Column:__init__": {},
      "cassandra/cqlengine/columns.py:Column:__hash__": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/scylla_driver-image-test_columns/scylla_driver-test_columns/tests/unit/test_policies.py:TestRackOrDCAwareRoundRobinPolicy:test_with_remotes": {
      "cassandra/policies.py:DCAwareRoundRobinPolicy:DCAwareRoundRobinPolicy": {},
      "cassandra/policies.py:RackAwareRoundRobinPolicy:RackAwareRoundRobinPolicy": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/scylla_driver-image-test_columns/scylla_driver-test_columns/tests/unit/test_policies.py:TestRackOrDCAwareRoundRobinPolicy:test_get_distance": {
      "cassandra/policies.py:DCAwareRoundRobinPolicy:DCAwareRoundRobinPolicy": {},
      "cassandra/policies.py:RackAwareRoundRobinPolicy:RackAwareRoundRobinPolicy": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/scylla_driver-image-test_columns/scylla_driver-test_columns/tests/integration/cqlengine/columns/test_validation.py:TestTimeUUIDFromDatetime:test_conversion_specific_date": {
      "cassandra/cqlengine/columns.py:UUID:UUID": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/scylla_driver-image-test_columns/scylla_driver-test_columns/tests/integration/long/test_loadbalancingpolicies.py:LoadBalancingPolicyTests:test_token_aware_is_used_by_default": {
      "cassandra/policies.py:TokenAwarePolicy:TokenAwarePolicy": {},
      "cassandra/policies.py:DCAwareRoundRobinPolicy:DCAwareRoundRobinPolicy": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/scylla_driver-image-test_columns/scylla_driver-test_columns/tests/integration/advanced/graph/test_graph.py:GraphTimeoutTests:test_server_timeout_less_then_request": {
      "cassandra/__init__.py:InvalidRequest:InvalidRequest": {},
      "cassandra/__init__.py:OperationTimedOut:OperationTimedOut": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/scylla_driver-image-test_columns/scylla_driver-test_columns/tests/integration/advanced/graph/test_graph.py:GraphProfileTests:test_graph_profile": {
      "cassandra/__init__.py:InvalidRequest:InvalidRequest": {},
      "cassandra/__init__.py:OperationTimedOut:OperationTimedOut": {}
    }
  },
  "PRD": "# PROJECT NAME: scylla_driver-test_columns\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 cassandra/\n    \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 InvalidRequest.InvalidRequest\n    \u2502   \u2514\u2500\u2500 OperationTimedOut.OperationTimedOut\n    \u251c\u2500\u2500 cqlengine/\n    \u2502   \u2514\u2500\u2500 columns.py\n    \u2502       \u251c\u2500\u2500 Column.Column\n    \u2502       \u251c\u2500\u2500 Column.__eq__\n    \u2502       \u251c\u2500\u2500 Column.__ge__\n    \u2502       \u251c\u2500\u2500 Column.__gt__\n    \u2502       \u251c\u2500\u2500 Column.__hash__\n    \u2502       \u251c\u2500\u2500 Column.__init__\n    \u2502       \u251c\u2500\u2500 Column.__le__\n    \u2502       \u251c\u2500\u2500 Column.__lt__\n    \u2502       \u251c\u2500\u2500 Column.__ne__\n    \u2502       \u2514\u2500\u2500 UUID.UUID\n    \u2514\u2500\u2500 policies.py\n        \u251c\u2500\u2500 DCAwareRoundRobinPolicy.DCAwareRoundRobinPolicy\n        \u251c\u2500\u2500 RackAwareRoundRobinPolicy.RackAwareRoundRobinPolicy\n        \u2514\u2500\u2500 TokenAwarePolicy.TokenAwarePolicy\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThe module is designed to test the functionality and behavior of the `Column` class within the Cassandra Object Mapper (CQL Engine) framework. It validates core features such as equality, comparison operations, hashing, and object positioning to ensure consistent behavior when `Column` objects are used in database schemas or application logic. By automating these tests, the module ensures the reliability and integrity of the `Column` class's functions, particularly in scenarios involving sorting, hashing, or comparisons. This helps developers prevent errors, maintain compatibility across Python versions, and build robust data models for Cassandra-based applications.\n\n## FILE 1: cassandra/cqlengine/columns.py\n\n- CLASS METHOD: Column.__gt__\n  - CLASS SIGNATURE: class Column(object):\n  - SIGNATURE: def __gt__(self, other):\n  - DOCSTRING: \n```python\n\"\"\"\nDetermines if the current Column instance is greater than another Column instance based on their instantiation order.\n\nParameters:\n- other (Column): An instance of the Column class to compare against.\n\nReturns:\n- bool: Returns True if the position of the current Column instance is greater than the position of the other Column instance. If 'other' is not an instance of Column, NotImplemented is returned.\n\nThis method relies on the `position` attribute, which is assigned during the initialization of a Column instance to maintain the order of instantiation. The `position` is utilized to establish the ordering of columns in data models.\n\"\"\"\n```\n\n- CLASS METHOD: Column.__ge__\n  - CLASS SIGNATURE: class Column(object):\n  - SIGNATURE: def __ge__(self, other):\n  - DOCSTRING: \n```python\n\"\"\"\nCompares the position of two Column instances to determine if the current column is greater than or equal to the other.\n\n:param other: The other Column instance to compare against.\n:type other: Column\n:return: True if the current column's position is greater than or equal to the other column's position; otherwise, False.\n:rtype: boolean\n\nThe position attribute is an integer that keeps track of the instantiation order of columns. It is defined as \na class-level attribute (`instance_counter`) in the Column class, which increments every time a new Column \ninstance is created. This method allows for ordering and comparison operations within the Column hierarchy.\n\"\"\"\n```\n\n- CLASS METHOD: Column.__init__\n  - CLASS SIGNATURE: class Column(object):\n  - SIGNATURE: def __init__(self, primary_key=False, partition_key=False, index=False, db_field=None, default=None, required=False, clustering_order=None, discriminator_column=False, static=False, custom_index=False):\n  - DOCSTRING: \n```python\n\"\"\"\nInitializes a Column object representing a database column within a Cassandra model.\n\nParameters:\n- primary_key (bool): Indicates if the column is a primary key. Defaults to False.\n- partition_key (bool): Indicates if the column is a partition key. Defaults to False.\n- index (bool): Flag for creating an index on this column. Defaults to False.\n- db_field (str): The database field name this column maps to. Defaults to None.\n- default (any): The default value for the column, which can either be a fixed value or a callable (no arguments). Defaults to None.\n- required (bool): Indicates if the column is mandatory (i.e., cannot be None). Defaults to False.\n- clustering_order (any): Specifies the order of clustering keys. Defaults to None.\n- discriminator_column (bool): Indicates if this column is used for model inheritance discrimination. Defaults to False.\n- static (bool): Indicates if this column is static, having a single value per partition. Defaults to False.\n- custom_index (bool): Indicates if the index is managed outside of cqlengine. Defaults to False.\n\nAttributes:\n- column_name (str): The name of the column in the model definition. Initialized to None.\n- _partition_key_index (int or None): An internal index to manage partition key positions. Initialized to None.\n- static (bool): Reflects if the column is defined as static.\n- value (any): Holds the column's value; initially set to None.\n- position (int): Tracks instantiation order, incremented with each new instantiation.\n\nSide Effects: Each instantiation increments the class-level instance_counter, ensuring each column instance has a unique position identifier.\n\"\"\"\n```\n\n- CLASS METHOD: Column.__ne__\n  - CLASS SIGNATURE: class Column(object):\n  - SIGNATURE: def __ne__(self, other):\n  - DOCSTRING: \n```python\n\"\"\"\nCompares this Column instance with another for inequality.\n\nThis method implements the \"not equal to\" operation (`!=`) for Column instances by comparing their instantiation order. It checks if the `other` parameter is an instance of the Column class, and if so, compares their `position` attributes. The `position` attribute is an integer that represents the order in which the column instances are created; it is assigned during initialization and helps determine the uniqueness of each Column instance.\n\nParameters:\n- other (Column or any): The value to compare against this Column instance.\n\nReturns:\n- bool: True if the two Column instances are not equal (i.e., their positions differ), False otherwise. Returns NotImplemented if `other` is not an instance of Column.\n\nDependencies:\n- The method relies on the `position` attribute, which is defined in the Column class's `__init__` method as a unique identifier for column instances. It increments each time a new Column instance is created.\n\"\"\"\n```\n\n- CLASS METHOD: Column.__lt__\n  - CLASS SIGNATURE: class Column(object):\n  - SIGNATURE: def __lt__(self, other):\n  - DOCSTRING: \n```python\n\"\"\"\nCompares the current Column instance with another Column instance for ordering.\n\nParameters:\n    other (Column): The Column instance to compare against.\n\nReturns:\n    bool: True if the current instance's position is less than the position of the other instance, otherwise False. If 'other' is not a Column instance, NotImplemented is returned.\n\nThe 'position' attribute, which is an integer representing the order of instantiation of the Column, is used for comparison. This attribute is defined in the Column class and is incremented for each new instance created, thus allowing for a natural ordering among Column instances based on their creation order.\n\"\"\"\n```\n\n- CLASS METHOD: Column.__hash__\n  - CLASS SIGNATURE: class Column(object):\n  - SIGNATURE: def __hash__(self):\n  - DOCSTRING: \n```python\n\"\"\"\nComputes the hash value for the Column instance using its unique identifier.\n\nThis method is essential for using Column instances in hash-based collections such as sets and dictionaries, ensuring that each instance can be uniquely identified based on its memory address. It does not take any parameters and will always return an integer representing the hash value.\n\nReturns:\n    int: The memory address of the Column instance, which serves as its hash value.\n\"\"\"\n```\n\n- CLASS METHOD: Column.__le__\n  - CLASS SIGNATURE: class Column(object):\n  - SIGNATURE: def __le__(self, other):\n  - DOCSTRING: \n```python\n\"\"\"\nDefines the less-than-or-equal-to comparison operator for instances of the Column class.\n\nParameters:\n    other (Column): Another instance of the Column class to compare against.\n\nReturns:\n    bool: True if the current instance's position is less than or equal to the other instance's position; otherwise, False.\n\nNotes:\n    The 'position' attribute is an integer that tracks the order of instantiation of Column instances, with lower values indicating earlier creation. This method enables comparison between Column instances based on their instantiation order.\n\"\"\"\n```\n\n- CLASS METHOD: Column.__eq__\n  - CLASS SIGNATURE: class Column(object):\n  - SIGNATURE: def __eq__(self, other):\n  - DOCSTRING: \n```python\n\"\"\"\nChecks for equality between two instances of the Column class based on their instantiation order.\n\nParameters:\n- other (Column): The other Column instance to compare against.\n\nReturns:\n- bool: True if the positions of the two Column instances are identical, indicating they are considered equal; False otherwise.\n\nNotes:\n- This method relies on the `position` attribute, which is assigned during the initialization of the Column class. \n- The `position` attribute is used to keep track of the order of instantiation among Column instances, allowing for comparisons that are sensitive to the order of declaration in model definitions.\n- If `other` is not an instance of Column, NotImplemented is returned, signaling that the comparison cannot be performed.\n\"\"\"\n```\n\n## FILE 2: cassandra/policies.py\n\n## FILE 3: cassandra/__init__.py\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "cassandra/cqlengine/columns.py": "from copy import deepcopy, copy\nfrom datetime import date, datetime, timedelta\nimport logging\nfrom uuid import UUID as _UUID\nfrom cassandra import util\nfrom cassandra.cqltypes import SimpleDateType, _cqltypes, UserType\nfrom cassandra.cqlengine import ValidationError\nfrom cassandra.cqlengine.functions import get_total_seconds\nfrom cassandra.util import Duration as _Duration\nlog = logging.getLogger(__name__)\n\nclass BaseValueManager(object):\n\n    def __init__(self, instance, column, value):\n        self.instance = instance\n        self.column = column\n        self.value = value\n        self.previous_value = None\n        self.explicit = False\n\n    @property\n    def deleted(self):\n        return self.column._val_is_null(self.value) and (self.explicit or not self.column._val_is_null(self.previous_value))\n\n    @property\n    def changed(self):\n        \"\"\"\n        Indicates whether or not this value has changed.\n\n        :rtype: boolean\n\n        \"\"\"\n        if self.explicit:\n            return self.value != self.previous_value\n        if isinstance(self.column, BaseContainerColumn):\n            default_value = self.column.get_default()\n            if self.column._val_is_null(default_value):\n                return not self.column._val_is_null(self.value) and self.value != self.previous_value\n            elif self.previous_value is None:\n                return self.value != default_value\n            return self.value != self.previous_value\n        return False\n\n    def reset_previous_value(self):\n        self.previous_value = deepcopy(self.value)\n\n    def getval(self):\n        return self.value\n\n    def setval(self, val):\n        self.value = val\n        self.explicit = True\n\n    def delval(self):\n        self.value = None\n\n    def get_property(self):\n        _get = lambda slf: self.getval()\n        _set = lambda slf, val: self.setval(val)\n        _del = lambda slf: self.delval()\n        if self.column.can_delete:\n            return property(_get, _set, _del)\n        else:\n            return property(_get, _set)\n\nclass Column(object):\n    db_type = None\n    value_manager = BaseValueManager\n    instance_counter = 0\n    _python_type_hashable = True\n    primary_key = False\n    '\\n    bool flag, indicates this column is a primary key. The first primary key defined\\n    on a model is the partition key (unless partition keys are set), all others are cluster keys\\n    '\n    partition_key = False\n    '\\n    indicates that this column should be the partition key, defining\\n    more than one partition key column creates a compound partition key\\n    '\n    index = False\n    '\\n    bool flag, indicates an index should be created for this column\\n    '\n    custom_index = False\n    '\\n    bool flag, indicates an index is managed outside of cqlengine. This is\\n    useful if you want to do filter queries on fields that have custom\\n    indexes.\\n    '\n    db_field = None\n    '\\n    the fieldname this field will map to in the database\\n    '\n    default = None\n    '\\n    the default value, can be a value or a callable (no args)\\n    '\n    required = False\n    '\\n    boolean, is the field required? Model validation will raise and\\n    exception if required is set to True and there is a None value assigned\\n    '\n    clustering_order = None\n    '\\n    only applicable on clustering keys (primary keys that are not partition keys)\\n    determines the order that the clustering keys are sorted on disk\\n    '\n    discriminator_column = False\n    '\\n    boolean, if set to True, this column will be used for discriminating records\\n    of inherited models.\\n\\n    Should only be set on a column of an abstract model being used for inheritance.\\n\\n    There may only be one discriminator column per model. See :attr:`~.__discriminator_value__`\\n    for how to specify the value of this column on specialized models.\\n    '\n    static = False\n    '\\n    boolean, if set to True, this is a static column, with a single value per partition\\n    '\n\n    def validate(self, value):\n        \"\"\"\n        Returns a cleaned and validated value. Raises a ValidationError\n        if there's a problem\n        \"\"\"\n        if value is None:\n            if self.required:\n                raise ValidationError('{0} - None values are not allowed'.format(self.column_name or self.db_field))\n        return value\n\n    def to_python(self, value):\n        \"\"\"\n        Converts data from the database into python values\n        raises a ValidationError if the value can't be converted\n        \"\"\"\n        return value\n\n    def to_database(self, value):\n        \"\"\"\n        Converts python value into database value\n        \"\"\"\n        return value\n\n    @property\n    def has_default(self):\n        return self.default is not None\n\n    @property\n    def is_primary_key(self):\n        return self.primary_key\n\n    @property\n    def can_delete(self):\n        return not self.primary_key\n\n    def get_default(self):\n        if self.has_default:\n            if callable(self.default):\n                return self.default()\n            else:\n                return self.default\n\n    def get_column_def(self):\n        \"\"\"\n        Returns a column definition for CQL table definition\n        \"\"\"\n        static = 'static' if self.static else ''\n        return '{0} {1} {2}'.format(self.cql, self.db_type, static)\n\n    def cql_parameterized_type(self):\n        return self.db_type\n\n    def set_column_name(self, name):\n        \"\"\"\n        Sets the column name during document class construction\n        This value will be ignored if db_field is set in __init__\n        \"\"\"\n        self.column_name = name\n\n    @property\n    def db_field_name(self):\n        \"\"\" Returns the name of the cql name of this column \"\"\"\n        return self.db_field if self.db_field is not None else self.column_name\n\n    @property\n    def db_index_name(self):\n        \"\"\" Returns the name of the cql index \"\"\"\n        return 'index_{0}'.format(self.db_field_name)\n\n    @property\n    def has_index(self):\n        return self.index or self.custom_index\n\n    @property\n    def cql(self):\n        return self.get_cql()\n\n    def get_cql(self):\n        return '\"{0}\"'.format(self.db_field_name)\n\n    def _val_is_null(self, val):\n        \"\"\" determines if the given value equates to a null value for the given column type \"\"\"\n        return val is None\n\n    @property\n    def sub_types(self):\n        return []\n\n    @property\n    def cql_type(self):\n        return _cqltypes[self.db_type]\n\nclass Blob(Column):\n    \"\"\"\n    Stores a raw binary value\n    \"\"\"\n    db_type = 'blob'\n\n    def to_database(self, value):\n        if not isinstance(value, (bytes, bytearray)):\n            raise Exception('expecting a binary, got a %s' % type(value))\n        val = super(Bytes, self).to_database(value)\n        return bytearray(val)\nBytes = Blob\n\nclass Inet(Column):\n    \"\"\"\n    Stores an IP address in IPv4 or IPv6 format\n    \"\"\"\n    db_type = 'inet'\n\nclass Text(Column):\n    \"\"\"\n    Stores a UTF-8 encoded string\n    \"\"\"\n    db_type = 'text'\n\n    def __init__(self, min_length=None, max_length=None, **kwargs):\n        \"\"\"\n        :param int min_length: Sets the minimum length of this string, for validation purposes.\n            Defaults to 1 if this is a ``required`` column. Otherwise, None.\n        :param int max_length: Sets the maximum length of this string, for validation purposes.\n        \"\"\"\n        self.min_length = 1 if min_length is None and kwargs.get('required', False) else min_length\n        self.max_length = max_length\n        if self.min_length is not None:\n            if self.min_length < 0:\n                raise ValueError('Minimum length is not allowed to be negative.')\n        if self.max_length is not None:\n            if self.max_length < 0:\n                raise ValueError('Maximum length is not allowed to be negative.')\n        if self.min_length is not None and self.max_length is not None:\n            if self.max_length < self.min_length:\n                raise ValueError('Maximum length must be greater or equal to minimum length.')\n        super(Text, self).__init__(**kwargs)\n\n    def validate(self, value):\n        value = super(Text, self).validate(value)\n        if not isinstance(value, (str, bytearray)) and value is not None:\n            raise ValidationError('{0} {1} is not a string'.format(self.column_name, type(value)))\n        if self.max_length is not None:\n            if value and len(value) > self.max_length:\n                raise ValidationError('{0} is longer than {1} characters'.format(self.column_name, self.max_length))\n        if self.min_length:\n            if self.min_length and (not value) or len(value) < self.min_length:\n                raise ValidationError('{0} is shorter than {1} characters'.format(self.column_name, self.min_length))\n        return value\n\nclass Ascii(Text):\n    \"\"\"\n    Stores a US-ASCII character string\n    \"\"\"\n    db_type = 'ascii'\n\n    def validate(self, value):\n        \"\"\" Only allow ASCII and None values.\n\n        Check against US-ASCII, a.k.a. 7-bit ASCII, a.k.a. ISO646-US, a.k.a.\n        the Basic Latin block of the Unicode character set.\n\n        Source: https://github.com/apache/cassandra/blob\n        /3dcbe90e02440e6ee534f643c7603d50ca08482b/src/java/org/apache/cassandra\n        /serializers/AsciiSerializer.java#L29\n        \"\"\"\n        value = super(Ascii, self).validate(value)\n        if value:\n            charset = value if isinstance(value, (bytearray,)) else map(ord, value)\n            if not set(range(128)).issuperset(charset):\n                raise ValidationError('{!r} is not an ASCII string.'.format(value))\n        return value\n\nclass Integer(Column):\n    \"\"\"\n    Stores a 32-bit signed integer value\n    \"\"\"\n    db_type = 'int'\n\n    def validate(self, value):\n        val = super(Integer, self).validate(value)\n        if val is None:\n            return\n        try:\n            return int(val)\n        except (TypeError, ValueError):\n            raise ValidationError(\"{0} {1} can't be converted to integral value\".format(self.column_name, value))\n\n    def to_python(self, value):\n        return self.validate(value)\n\n    def to_database(self, value):\n        return self.validate(value)\n\nclass TinyInt(Integer):\n    \"\"\"\n    Stores an 8-bit signed integer value\n\n    .. versionadded:: 2.6.0\n\n    requires C* 2.2+ and protocol v4+\n    \"\"\"\n    db_type = 'tinyint'\n\nclass SmallInt(Integer):\n    \"\"\"\n    Stores a 16-bit signed integer value\n\n    .. versionadded:: 2.6.0\n\n    requires C* 2.2+ and protocol v4+\n    \"\"\"\n    db_type = 'smallint'\n\nclass BigInt(Integer):\n    \"\"\"\n    Stores a 64-bit signed integer value\n    \"\"\"\n    db_type = 'bigint'\n\nclass VarInt(Column):\n    \"\"\"\n    Stores an arbitrary-precision integer\n    \"\"\"\n    db_type = 'varint'\n\n    def validate(self, value):\n        val = super(VarInt, self).validate(value)\n        if val is None:\n            return\n        try:\n            return int(val)\n        except (TypeError, ValueError):\n            raise ValidationError(\"{0} {1} can't be converted to integral value\".format(self.column_name, value))\n\n    def to_python(self, value):\n        return self.validate(value)\n\n    def to_database(self, value):\n        return self.validate(value)\n\nclass CounterValueManager(BaseValueManager):\n\n    def __init__(self, instance, column, value):\n        super(CounterValueManager, self).__init__(instance, column, value)\n        self.value = self.value or 0\n        self.previous_value = self.previous_value or 0\n\nclass Counter(Integer):\n    \"\"\"\n    Stores a counter that can be incremented and decremented\n    \"\"\"\n    db_type = 'counter'\n    value_manager = CounterValueManager\n\n    def __init__(self, index=False, db_field=None, required=False):\n        super(Counter, self).__init__(primary_key=False, partition_key=False, index=index, db_field=db_field, default=0, required=required)\n\nclass DateTime(Column):\n    \"\"\"\n    Stores a datetime value\n    \"\"\"\n    db_type = 'timestamp'\n    truncate_microseconds = False\n    '\\n    Set this ``True`` to have model instances truncate the date, quantizing it in the same way it will be in the database.\\n    This allows equality comparison between assigned values and values read back from the database::\\n\\n        DateTime.truncate_microseconds = True\\n        assert Model.create(id=0, d=datetime.utcnow()) == Model.objects(id=0).first()\\n\\n    Defaults to ``False`` to preserve legacy behavior. May change in the future.\\n    '\n\n    def to_python(self, value):\n        if value is None:\n            return\n        if isinstance(value, datetime):\n            if DateTime.truncate_microseconds:\n                us = value.microsecond\n                truncated_us = us // 1000 * 1000\n                return value - timedelta(microseconds=us - truncated_us)\n            else:\n                return value\n        elif isinstance(value, date):\n            return datetime(*value.timetuple()[:6])\n        return datetime.utcfromtimestamp(value)\n\n    def to_database(self, value):\n        value = super(DateTime, self).to_database(value)\n        if value is None:\n            return\n        if not isinstance(value, datetime):\n            if isinstance(value, date):\n                value = datetime(value.year, value.month, value.day)\n            else:\n                raise ValidationError(\"{0} '{1}' is not a datetime object\".format(self.column_name, value))\n        epoch = datetime(1970, 1, 1, tzinfo=value.tzinfo)\n        offset = get_total_seconds(epoch.tzinfo.utcoffset(epoch)) if epoch.tzinfo else 0\n        return int((get_total_seconds(value - epoch) - offset) * 1000)\n\nclass Date(Column):\n    \"\"\"\n    Stores a simple date, with no time-of-day\n\n    .. versionchanged:: 2.6.0\n\n        removed overload of Date and DateTime. DateTime is a drop-in replacement for legacy models\n\n    requires C* 2.2+ and protocol v4+\n    \"\"\"\n    db_type = 'date'\n\n    def to_database(self, value):\n        if value is None:\n            return\n        d = value if isinstance(value, util.Date) else util.Date(value)\n        return d.days_from_epoch + SimpleDateType.EPOCH_OFFSET_DAYS\n\n    def to_python(self, value):\n        if value is None:\n            return\n        if isinstance(value, util.Date):\n            return value\n        if isinstance(value, datetime):\n            value = value.date()\n        return util.Date(value)\n\nclass Time(Column):\n    \"\"\"\n    Stores a timezone-naive time-of-day, with nanosecond precision\n\n    .. versionadded:: 2.6.0\n\n    requires C* 2.2+ and protocol v4+\n    \"\"\"\n    db_type = 'time'\n\n    def to_database(self, value):\n        value = super(Time, self).to_database(value)\n        if value is None:\n            return\n        return value if isinstance(value, util.Time) else util.Time(value)\n\n    def to_python(self, value):\n        value = super(Time, self).to_database(value)\n        if value is None:\n            return\n        if isinstance(value, util.Time):\n            return value\n        return util.Time(value)\n\nclass Duration(Column):\n    \"\"\"\n    Stores a duration (months, days, nanoseconds)\n\n    .. versionadded:: 3.10.0\n\n    requires C* 3.10+ and protocol v4+\n    \"\"\"\n    db_type = 'duration'\n\n    def validate(self, value):\n        val = super(Duration, self).validate(value)\n        if val is None:\n            return\n        if not isinstance(val, _Duration):\n            raise TypeError('{0} {1} is not a valid Duration.'.format(self.column_name, value))\n        return val\n\nclass UUID(Column):\n    \"\"\"\n    Stores a type 1 or 4 UUID\n    \"\"\"\n    db_type = 'uuid'\n\n    def validate(self, value):\n        val = super(UUID, self).validate(value)\n        if val is None:\n            return\n        if isinstance(val, _UUID):\n            return val\n        if isinstance(val, str):\n            try:\n                return _UUID(val)\n            except ValueError:\n                pass\n        raise ValidationError('{0} {1} is not a valid uuid'.format(self.column_name, value))\n\n    def to_python(self, value):\n        return self.validate(value)\n\n    def to_database(self, value):\n        return self.validate(value)\n\nclass TimeUUID(UUID):\n    \"\"\"\n    UUID containing timestamp\n    \"\"\"\n    db_type = 'timeuuid'\n\nclass Boolean(Column):\n    \"\"\"\n    Stores a boolean True or False value\n    \"\"\"\n    db_type = 'boolean'\n\n    def validate(self, value):\n        \"\"\" Always returns a Python boolean. \"\"\"\n        value = super(Boolean, self).validate(value)\n        if value is not None:\n            value = bool(value)\n        return value\n\n    def to_python(self, value):\n        return self.validate(value)\n\nclass BaseFloat(Column):\n\n    def validate(self, value):\n        value = super(BaseFloat, self).validate(value)\n        if value is None:\n            return\n        try:\n            return float(value)\n        except (TypeError, ValueError):\n            raise ValidationError('{0} {1} is not a valid float'.format(self.column_name, value))\n\n    def to_python(self, value):\n        return self.validate(value)\n\n    def to_database(self, value):\n        return self.validate(value)\n\nclass Float(BaseFloat):\n    \"\"\"\n    Stores a single-precision floating-point value\n    \"\"\"\n    db_type = 'float'\n\nclass Double(BaseFloat):\n    \"\"\"\n    Stores a double-precision floating-point value\n    \"\"\"\n    db_type = 'double'\n\nclass Decimal(Column):\n    \"\"\"\n    Stores a variable precision decimal value\n    \"\"\"\n    db_type = 'decimal'\n\n    def validate(self, value):\n        from decimal import Decimal as _Decimal\n        from decimal import InvalidOperation\n        val = super(Decimal, self).validate(value)\n        if val is None:\n            return\n        try:\n            return _Decimal(repr(val)) if isinstance(val, float) else _Decimal(val)\n        except InvalidOperation:\n            raise ValidationError(\"{0} '{1}' can't be coerced to decimal\".format(self.column_name, val))\n\n    def to_python(self, value):\n        return self.validate(value)\n\n    def to_database(self, value):\n        return self.validate(value)\n\nclass BaseCollectionColumn(Column):\n    \"\"\"\n    Base Container type for collection-like columns.\n\n    http://cassandra.apache.org/doc/cql3/CQL-3.0.html#collections\n    \"\"\"\n\n    def __init__(self, types, **kwargs):\n        \"\"\"\n        :param types: a sequence of sub types in this collection\n        \"\"\"\n        instances = []\n        for t in types:\n            inheritance_comparator = issubclass if isinstance(t, type) else isinstance\n            if not inheritance_comparator(t, Column):\n                raise ValidationError('%s is not a column class' % (t,))\n            if t.db_type is None:\n                raise ValidationError('%s is an abstract type' % (t,))\n            inst = t() if isinstance(t, type) else t\n            if isinstance(t, BaseCollectionColumn):\n                inst._freeze_db_type()\n            instances.append(inst)\n        self.types = instances\n        super(BaseCollectionColumn, self).__init__(**kwargs)\n\n    def validate(self, value):\n        value = super(BaseCollectionColumn, self).validate(value)\n        if value is not None and len(value) > 65535:\n            raise ValidationError(\"{0} Collection can't have more than 65535 elements.\".format(self.column_name))\n        return value\n\n    def _val_is_null(self, val):\n        return not val\n\n    def _freeze_db_type(self):\n        if not self.db_type.startswith('frozen'):\n            self.db_type = 'frozen<%s>' % (self.db_type,)\n\n    @property\n    def sub_types(self):\n        return self.types\n\n    @property\n    def cql_type(self):\n        return _cqltypes[self.__class__.__name__.lower()].apply_parameters([c.cql_type for c in self.types])\n\nclass Tuple(BaseCollectionColumn):\n    \"\"\"\n    Stores a fixed-length set of positional values\n\n    http://docs.datastax.com/en/cql/3.1/cql/cql_reference/tupleType.html\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        :param args: column types representing tuple composition\n        \"\"\"\n        if not args:\n            raise ValueError('Tuple must specify at least one inner type')\n        super(Tuple, self).__init__(args, **kwargs)\n        self.db_type = 'tuple<{0}>'.format(', '.join((typ.db_type for typ in self.types)))\n\n    def validate(self, value):\n        val = super(Tuple, self).validate(value)\n        if val is None:\n            return\n        if len(val) > len(self.types):\n            raise ValidationError('Value %r has more fields than tuple definition (%s)' % (val, ', '.join((t for t in self.types))))\n        return tuple((t.validate(v) for t, v in zip(self.types, val)))\n\n    def to_python(self, value):\n        if value is None:\n            return tuple()\n        return tuple((t.to_python(v) for t, v in zip(self.types, value)))\n\n    def to_database(self, value):\n        if value is None:\n            return\n        return tuple((t.to_database(v) for t, v in zip(self.types, value)))\n\nclass BaseContainerColumn(BaseCollectionColumn):\n    pass\n\nclass Set(BaseContainerColumn):\n    \"\"\"\n    Stores a set of unordered, unique values\n\n    http://www.datastax.com/documentation/cql/3.1/cql/cql_using/use_set_t.html\n    \"\"\"\n    _python_type_hashable = False\n\n    def __init__(self, value_type, strict=True, default=set, **kwargs):\n        \"\"\"\n        :param value_type: a column class indicating the types of the value\n        :param strict: sets whether non set values will be coerced to set\n            type on validation, or raise a validation error, defaults to True\n        \"\"\"\n        self.strict = strict\n        super(Set, self).__init__((value_type,), default=default, **kwargs)\n        self.value_col = self.types[0]\n        if not self.value_col._python_type_hashable:\n            raise ValidationError('Cannot create a Set with unhashable value type (see PYTHON-494)')\n        self.db_type = 'set<{0}>'.format(self.value_col.db_type)\n\n    def validate(self, value):\n        val = super(Set, self).validate(value)\n        if val is None:\n            return\n        types = (set, util.SortedSet) if self.strict else (set, util.SortedSet, list, tuple)\n        if not isinstance(val, types):\n            if self.strict:\n                raise ValidationError('{0} {1} is not a set object'.format(self.column_name, val))\n            else:\n                raise ValidationError('{0} {1} cannot be coerced to a set object'.format(self.column_name, val))\n        if None in val:\n            raise ValidationError('{0} None not allowed in a set'.format(self.column_name))\n        return set((self.value_col.validate(v) for v in val))\n\n    def to_python(self, value):\n        if value is None:\n            return set()\n        return set((self.value_col.to_python(v) for v in value))\n\n    def to_database(self, value):\n        if value is None:\n            return None\n        return set((self.value_col.to_database(v) for v in value))\n\nclass List(BaseContainerColumn):\n    \"\"\"\n    Stores a list of ordered values\n\n    http://www.datastax.com/documentation/cql/3.1/cql/cql_using/use_list_t.html\n    \"\"\"\n    _python_type_hashable = False\n\n    def __init__(self, value_type, default=list, **kwargs):\n        \"\"\"\n        :param value_type: a column class indicating the types of the value\n        \"\"\"\n        super(List, self).__init__((value_type,), default=default, **kwargs)\n        self.value_col = self.types[0]\n        self.db_type = 'list<{0}>'.format(self.value_col.db_type)\n\n    def validate(self, value):\n        val = super(List, self).validate(value)\n        if val is None:\n            return\n        if not isinstance(val, (set, list, tuple)):\n            raise ValidationError('{0} {1} is not a list object'.format(self.column_name, val))\n        if None in val:\n            raise ValidationError('{0} None is not allowed in a list'.format(self.column_name))\n        return [self.value_col.validate(v) for v in val]\n\n    def to_python(self, value):\n        if value is None:\n            return []\n        return [self.value_col.to_python(v) for v in value]\n\n    def to_database(self, value):\n        if value is None:\n            return None\n        return [self.value_col.to_database(v) for v in value]\n\nclass Map(BaseContainerColumn):\n    \"\"\"\n    Stores a key -> value map (dictionary)\n\n    https://docs.datastax.com/en/dse/6.7/cql/cql/cql_using/useMap.html\n    \"\"\"\n    _python_type_hashable = False\n\n    def __init__(self, key_type, value_type, default=dict, **kwargs):\n        \"\"\"\n        :param key_type: a column class indicating the types of the key\n        :param value_type: a column class indicating the types of the value\n        \"\"\"\n        super(Map, self).__init__((key_type, value_type), default=default, **kwargs)\n        self.key_col = self.types[0]\n        self.value_col = self.types[1]\n        if not self.key_col._python_type_hashable:\n            raise ValidationError('Cannot create a Map with unhashable key type (see PYTHON-494)')\n        self.db_type = 'map<{0}, {1}>'.format(self.key_col.db_type, self.value_col.db_type)\n\n    def validate(self, value):\n        val = super(Map, self).validate(value)\n        if val is None:\n            return\n        if not isinstance(val, (dict, util.OrderedMap)):\n            raise ValidationError('{0} {1} is not a dict object'.format(self.column_name, val))\n        if None in val:\n            raise ValidationError('{0} None is not allowed in a map'.format(self.column_name))\n        return dict(((self.key_col.validate(k), self.value_col.validate(v)) for k, v in val.items()))\n\n    def to_python(self, value):\n        if value is None:\n            return {}\n        if value is not None:\n            return dict(((self.key_col.to_python(k), self.value_col.to_python(v)) for k, v in value.items()))\n\n    def to_database(self, value):\n        if value is None:\n            return None\n        return dict(((self.key_col.to_database(k), self.value_col.to_database(v)) for k, v in value.items()))\n\nclass UDTValueManager(BaseValueManager):\n\n    @property\n    def changed(self):\n        if self.explicit:\n            return self.value != self.previous_value\n        default_value = self.column.get_default()\n        if not self.column._val_is_null(default_value):\n            return self.value != default_value\n        elif self.previous_value is None:\n            return not self.column._val_is_null(self.value) and self.value.has_changed_fields()\n        return False\n\n    def reset_previous_value(self):\n        if self.value is not None:\n            self.value.reset_changed_fields()\n        self.previous_value = copy(self.value)\n\nclass UserDefinedType(Column):\n    \"\"\"\n    User Defined Type column\n\n    http://www.datastax.com/documentation/cql/3.1/cql/cql_using/cqlUseUDT.html\n\n    These columns are represented by a specialization of :class:`cassandra.cqlengine.usertype.UserType`.\n\n    Please see :ref:`user_types` for examples and discussion.\n    \"\"\"\n    value_manager = UDTValueManager\n\n    def __init__(self, user_type, **kwargs):\n        \"\"\"\n        :param type user_type: specifies the :class:`~.cqlengine.usertype.UserType` model of the column\n        \"\"\"\n        self.user_type = user_type\n        self.db_type = 'frozen<%s>' % user_type.type_name()\n        super(UserDefinedType, self).__init__(**kwargs)\n\n    @property\n    def sub_types(self):\n        return list(self.user_type._fields.values())\n\n    @property\n    def cql_type(self):\n        return UserType.make_udt_class(keyspace='', udt_name=self.user_type.type_name(), field_names=[c.db_field_name for c in self.user_type._fields.values()], field_types=[c.cql_type for c in self.user_type._fields.values()])\n\n    def validate(self, value):\n        val = super(UserDefinedType, self).validate(value)\n        if val is None:\n            return\n        val.validate()\n        return val\n\n    def to_python(self, value):\n        if value is None:\n            return\n        for name, field in self.user_type._fields.items():\n            if value[name] is not None or isinstance(field, BaseContainerColumn):\n                value[name] = field.to_python(value[name])\n        return value\n\n    def to_database(self, value):\n        if value is None:\n            return\n        copied_value = deepcopy(value)\n        for name, field in self.user_type._fields.items():\n            if copied_value[name] is not None or isinstance(field, BaseContainerColumn):\n                copied_value[name] = field.to_database(copied_value[name])\n        return copied_value\n\ndef resolve_udts(col_def, out_list):\n    for col in col_def.sub_types:\n        resolve_udts(col, out_list)\n    if isinstance(col_def, UserDefinedType):\n        out_list.append(col_def.user_type)\n\nclass _PartitionKeysToken(Column):\n    \"\"\"\n    virtual column representing token of partition columns.\n    Used by filter(pk__token=Token(...)) filters\n    \"\"\"\n\n    def __init__(self, model):\n        self.partition_columns = list(model._partition_keys.values())\n        super(_PartitionKeysToken, self).__init__(partition_key=True)\n\n    @property\n    def db_field_name(self):\n        return 'token({0})'.format(', '.join(['\"{0}\"'.format(c.db_field_name) for c in self.partition_columns]))",
    "cassandra/policies.py": "import random\nfrom collections import namedtuple\nfrom functools import lru_cache\nfrom itertools import islice, cycle, groupby, repeat\nimport logging\nfrom random import randint, shuffle\nfrom threading import Lock\nimport socket\nimport warnings\nlog = logging.getLogger(__name__)\nfrom cassandra import WriteType as WT\nWriteType = WT\nfrom cassandra import ConsistencyLevel, OperationTimedOut\n\nclass HostDistance(object):\n    \"\"\"\n    A measure of how \"distant\" a node is from the client, which\n    may influence how the load balancer distributes requests\n    and how many connections are opened to the node.\n    \"\"\"\n    IGNORED = -1\n    '\\n    A node with this distance should never be queried or have\\n    connections opened to it.\\n    '\n    LOCAL_RACK = 0\n    '\\n    Nodes with ``LOCAL_RACK`` distance will be preferred for operations\\n    under some load balancing policies (such as :class:`.RackAwareRoundRobinPolicy`)\\n    and will have a greater number of connections opened against\\n    them by default.\\n\\n    This distance is typically used for nodes within the same\\n    datacenter and the same rack as the client.\\n    '\n    LOCAL = 1\n    '\\n    Nodes with ``LOCAL`` distance will be preferred for operations\\n    under some load balancing policies (such as :class:`.DCAwareRoundRobinPolicy`)\\n    and will have a greater number of connections opened against\\n    them by default.\\n\\n    This distance is typically used for nodes within the same\\n    datacenter as the client.\\n    '\n    REMOTE = 2\n    '\\n    Nodes with ``REMOTE`` distance will be treated as a last resort\\n    by some load balancing policies (such as :class:`.DCAwareRoundRobinPolicy`\\n    and :class:`.RackAwareRoundRobinPolicy`)and will have a smaller number of\\n    connections opened against them by default.\\n\\n    This distance is typically used for nodes outside of the\\n    datacenter that the client is running in.\\n    '\n\nclass HostStateListener(object):\n\n    def on_up(self, host):\n        \"\"\" Called when a node is marked up. \"\"\"\n        raise NotImplementedError()\n\n    def on_down(self, host):\n        \"\"\" Called when a node is marked down. \"\"\"\n        raise NotImplementedError()\n\n    def on_add(self, host):\n        \"\"\"\n        Called when a node is added to the cluster.  The newly added node\n        should be considered up.\n        \"\"\"\n        raise NotImplementedError()\n\n    def on_remove(self, host):\n        \"\"\" Called when a node is removed from the cluster. \"\"\"\n        raise NotImplementedError()\n\nclass LoadBalancingPolicy(HostStateListener):\n    \"\"\"\n    Load balancing policies are used to decide how to distribute\n    requests among all possible coordinator nodes in the cluster.\n\n    In particular, they may focus on querying \"near\" nodes (those\n    in a local datacenter) or on querying nodes who happen to\n    be replicas for the requested data.\n\n    You may also use subclasses of :class:`.LoadBalancingPolicy` for\n    custom behavior.\n\n    You should always use immutable collections (e.g., tuples or\n    frozensets) to store information about hosts to prevent accidental\n    modification. When there are changes to the hosts (e.g., a host is\n    down or up), the old collection should be replaced with a new one.\n    \"\"\"\n    _hosts_lock = None\n\n    def __init__(self):\n        self._hosts_lock = Lock()\n\n    def distance(self, host):\n        \"\"\"\n        Returns a measure of how remote a :class:`~.pool.Host` is in\n        terms of the :class:`.HostDistance` enums.\n        \"\"\"\n        raise NotImplementedError()\n\n    def populate(self, cluster, hosts):\n        \"\"\"\n        This method is called to initialize the load balancing\n        policy with a set of :class:`.Host` instances before its\n        first use.  The `cluster` parameter is an instance of\n        :class:`.Cluster`.\n        \"\"\"\n        raise NotImplementedError()\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        \"\"\"\n        Given a :class:`~.query.Statement` instance, return a iterable\n        of :class:`.Host` instances which should be queried in that\n        order.  A generator may work well for custom implementations\n        of this method.\n\n        Note that the `query` argument may be :const:`None` when preparing\n        statements.\n\n        `working_keyspace` should be the string name of the current keyspace,\n        as set through :meth:`.Session.set_keyspace()` or with a ``USE``\n        statement.\n        \"\"\"\n        raise NotImplementedError()\n\n    def check_supported(self):\n        \"\"\"\n        This will be called after the cluster Metadata has been initialized.\n        If the load balancing policy implementation cannot be supported for\n        some reason (such as a missing C extension), this is the point at\n        which it should raise an exception.\n        \"\"\"\n        pass\n\nclass RoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    A subclass of :class:`.LoadBalancingPolicy` which evenly\n    distributes queries across all nodes in the cluster,\n    regardless of what datacenter the nodes may be in.\n    \"\"\"\n    _live_hosts = frozenset(())\n    _position = 0\n\n    def populate(self, cluster, hosts):\n        self._live_hosts = frozenset(hosts)\n        if len(hosts) > 1:\n            self._position = randint(0, len(hosts) - 1)\n\n    def distance(self, host):\n        return HostDistance.LOCAL\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        pos = self._position\n        self._position += 1\n        hosts = self._live_hosts\n        length = len(hosts)\n        if length:\n            pos %= length\n            return islice(cycle(hosts), pos, pos + length)\n        else:\n            return []\n\n    def on_up(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.union((host,))\n\n    def on_down(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.difference((host,))\n\n    def on_add(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.union((host,))\n\n    def on_remove(self, host):\n        with self._hosts_lock:\n            self._live_hosts = self._live_hosts.difference((host,))\n\nclass DCAwareRoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    Similar to :class:`.RoundRobinPolicy`, but prefers hosts\n    in the local datacenter and only uses nodes in remote\n    datacenters as a last resort.\n    \"\"\"\n    local_dc = None\n    used_hosts_per_remote_dc = 0\n\n    def __init__(self, local_dc='', used_hosts_per_remote_dc=0):\n        \"\"\"\n        The `local_dc` parameter should be the name of the datacenter\n        (such as is reported by ``nodetool ring``) that should\n        be considered local. If not specified, the driver will choose\n        a local_dc based on the first host among :attr:`.Cluster.contact_points`\n        having a valid DC. If relying on this mechanism, all specified\n        contact points should be nodes in a single, local DC.\n\n        `used_hosts_per_remote_dc` controls how many nodes in\n        each remote datacenter will have connections opened\n        against them. In other words, `used_hosts_per_remote_dc` hosts\n        will be considered :attr:`~.HostDistance.REMOTE` and the\n        rest will be considered :attr:`~.HostDistance.IGNORED`.\n        By default, all remote hosts are ignored.\n        \"\"\"\n        self.local_dc = local_dc\n        self.used_hosts_per_remote_dc = used_hosts_per_remote_dc\n        self._dc_live_hosts = {}\n        self._position = 0\n        self._endpoints = []\n        LoadBalancingPolicy.__init__(self)\n\n    def _dc(self, host):\n        return host.datacenter or self.local_dc\n\n    def populate(self, cluster, hosts):\n        for dc, dc_hosts in groupby(hosts, lambda h: self._dc(h)):\n            self._dc_live_hosts[dc] = tuple(set(dc_hosts))\n        if not self.local_dc:\n            self._endpoints = [endpoint for endpoint in cluster.endpoints_resolved]\n        self._position = randint(0, len(hosts) - 1) if hosts else 0\n\n    def distance(self, host):\n        dc = self._dc(host)\n        if dc == self.local_dc:\n            return HostDistance.LOCAL\n        if not self.used_hosts_per_remote_dc:\n            return HostDistance.IGNORED\n        else:\n            dc_hosts = self._dc_live_hosts.get(dc)\n            if not dc_hosts:\n                return HostDistance.IGNORED\n            if host in list(dc_hosts)[:self.used_hosts_per_remote_dc]:\n                return HostDistance.REMOTE\n            else:\n                return HostDistance.IGNORED\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        pos = self._position\n        self._position += 1\n        local_live = self._dc_live_hosts.get(self.local_dc, ())\n        pos = pos % len(local_live) if local_live else 0\n        for host in islice(cycle(local_live), pos, pos + len(local_live)):\n            yield host\n        other_dcs = [dc for dc in self._dc_live_hosts.copy().keys() if dc != self.local_dc]\n        for dc in other_dcs:\n            remote_live = self._dc_live_hosts.get(dc, ())\n            for host in remote_live[:self.used_hosts_per_remote_dc]:\n                yield host\n\n    def on_up(self, host):\n        if not self.local_dc and host.datacenter:\n            if host.endpoint in self._endpoints:\n                self.local_dc = host.datacenter\n                log.info(\"Using datacenter '%s' for DCAwareRoundRobinPolicy (via host '%s'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\" % (self.local_dc, host.endpoint))\n                del self._endpoints\n        dc = self._dc(host)\n        with self._hosts_lock:\n            current_hosts = self._dc_live_hosts.get(dc, ())\n            if host not in current_hosts:\n                self._dc_live_hosts[dc] = current_hosts + (host,)\n\n    def on_down(self, host):\n        dc = self._dc(host)\n        with self._hosts_lock:\n            current_hosts = self._dc_live_hosts.get(dc, ())\n            if host in current_hosts:\n                hosts = tuple((h for h in current_hosts if h != host))\n                if hosts:\n                    self._dc_live_hosts[dc] = hosts\n                else:\n                    del self._dc_live_hosts[dc]\n\n    def on_add(self, host):\n        self.on_up(host)\n\n    def on_remove(self, host):\n        self.on_down(host)\n\nclass RackAwareRoundRobinPolicy(LoadBalancingPolicy):\n    \"\"\"\n    Similar to :class:`.DCAwareRoundRobinPolicy`, but prefers hosts\n    in the local rack, before hosts in the local datacenter but a\n    different rack, before hosts in all other datercentres\n    \"\"\"\n    local_dc = None\n    local_rack = None\n    used_hosts_per_remote_dc = 0\n\n    def __init__(self, local_dc, local_rack, used_hosts_per_remote_dc=0):\n        \"\"\"\n        The `local_dc` and `local_rack` parameters should be the name of the\n        datacenter and rack (such as is reported by ``nodetool ring``) that\n        should be considered local.\n\n        `used_hosts_per_remote_dc` controls how many nodes in\n        each remote datacenter will have connections opened\n        against them. In other words, `used_hosts_per_remote_dc` hosts\n        will be considered :attr:`~.HostDistance.REMOTE` and the\n        rest will be considered :attr:`~.HostDistance.IGNORED`.\n        By default, all remote hosts are ignored.\n        \"\"\"\n        self.local_rack = local_rack\n        self.local_dc = local_dc\n        self.used_hosts_per_remote_dc = used_hosts_per_remote_dc\n        self._live_hosts = {}\n        self._dc_live_hosts = {}\n        self._endpoints = []\n        self._position = 0\n        LoadBalancingPolicy.__init__(self)\n\n    def _rack(self, host):\n        return host.rack or self.local_rack\n\n    def _dc(self, host):\n        return host.datacenter or self.local_dc\n\n    def populate(self, cluster, hosts):\n        for (dc, rack), rack_hosts in groupby(hosts, lambda host: (self._dc(host), self._rack(host))):\n            self._live_hosts[dc, rack] = tuple(set(rack_hosts))\n        for dc, dc_hosts in groupby(hosts, lambda host: self._dc(host)):\n            self._dc_live_hosts[dc] = tuple(set(dc_hosts))\n        self._position = randint(0, len(hosts) - 1) if hosts else 0\n\n    def distance(self, host):\n        rack = self._rack(host)\n        dc = self._dc(host)\n        if rack == self.local_rack and dc == self.local_dc:\n            return HostDistance.LOCAL_RACK\n        if dc == self.local_dc:\n            return HostDistance.LOCAL\n        if not self.used_hosts_per_remote_dc:\n            return HostDistance.IGNORED\n        dc_hosts = self._dc_live_hosts.get(dc, ())\n        if not dc_hosts:\n            return HostDistance.IGNORED\n        if host in dc_hosts and dc_hosts.index(host) < self.used_hosts_per_remote_dc:\n            return HostDistance.REMOTE\n        else:\n            return HostDistance.IGNORED\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        pos = self._position\n        self._position += 1\n        local_rack_live = self._live_hosts.get((self.local_dc, self.local_rack), ())\n        pos = pos % len(local_rack_live) if local_rack_live else 0\n        for host in islice(cycle(local_rack_live), pos, pos + len(local_rack_live)):\n            yield host\n        local_live = [host for host in self._dc_live_hosts.get(self.local_dc, ()) if host.rack != self.local_rack]\n        pos = pos % len(local_live) if local_live else 0\n        for host in islice(cycle(local_live), pos, pos + len(local_live)):\n            yield host\n        for dc, remote_live in self._dc_live_hosts.copy().items():\n            if dc != self.local_dc:\n                for host in remote_live[:self.used_hosts_per_remote_dc]:\n                    yield host\n\n    def on_up(self, host):\n        dc = self._dc(host)\n        rack = self._rack(host)\n        with self._hosts_lock:\n            current_rack_hosts = self._live_hosts.get((dc, rack), ())\n            if host not in current_rack_hosts:\n                self._live_hosts[dc, rack] = current_rack_hosts + (host,)\n            current_dc_hosts = self._dc_live_hosts.get(dc, ())\n            if host not in current_dc_hosts:\n                self._dc_live_hosts[dc] = current_dc_hosts + (host,)\n\n    def on_down(self, host):\n        dc = self._dc(host)\n        rack = self._rack(host)\n        with self._hosts_lock:\n            current_rack_hosts = self._live_hosts.get((dc, rack), ())\n            if host in current_rack_hosts:\n                hosts = tuple((h for h in current_rack_hosts if h != host))\n                if hosts:\n                    self._live_hosts[dc, rack] = hosts\n                else:\n                    del self._live_hosts[dc, rack]\n            current_dc_hosts = self._dc_live_hosts.get(dc, ())\n            if host in current_dc_hosts:\n                hosts = tuple((h for h in current_dc_hosts if h != host))\n                if hosts:\n                    self._dc_live_hosts[dc] = hosts\n                else:\n                    del self._dc_live_hosts[dc]\n\n    def on_add(self, host):\n        self.on_up(host)\n\n    def on_remove(self, host):\n        self.on_down(host)\n\nclass TokenAwarePolicy(LoadBalancingPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` wrapper that adds token awareness to\n    a child policy.\n\n    This alters the child policy's behavior so that it first attempts to\n    send queries to :attr:`~.HostDistance.LOCAL` replicas (as determined\n    by the child policy) based on the :class:`.Statement`'s\n    :attr:`~.Statement.routing_key`. If :attr:`.shuffle_replicas` is\n    truthy, these replicas will be yielded in a random order. Once those\n    hosts are exhausted, the remaining hosts in the child policy's query\n    plan will be used in the order provided by the child policy.\n\n    If no :attr:`~.Statement.routing_key` is set on the query, the child\n    policy's query plan will be used as is.\n    \"\"\"\n    _child_policy = None\n    _cluster_metadata = None\n    _tablets_routing_v1 = False\n    shuffle_replicas = False\n    '\\n    Yield local replicas in a random order.\\n    '\n\n    def __init__(self, child_policy, shuffle_replicas=False):\n        self._child_policy = child_policy\n        self.shuffle_replicas = shuffle_replicas\n\n    def populate(self, cluster, hosts):\n        self._cluster_metadata = cluster.metadata\n        self._tablets_routing_v1 = cluster.control_connection._tablets_routing_v1\n        self._child_policy.populate(cluster, hosts)\n\n    def check_supported(self):\n        if not self._cluster_metadata.can_support_partitioner():\n            raise RuntimeError('%s cannot be used with the cluster partitioner (%s) because the relevant C extension for this driver was not compiled. See the installation instructions for details on building and installing the C extensions.' % (self.__class__.__name__, self._cluster_metadata.partitioner))\n\n    def distance(self, *args, **kwargs):\n        return self._child_policy.distance(*args, **kwargs)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        keyspace = query.keyspace if query and query.keyspace else working_keyspace\n        child = self._child_policy\n        if query is None or query.routing_key is None or keyspace is None:\n            for host in child.make_query_plan(keyspace, query):\n                yield host\n            return\n        replicas = []\n        if self._tablets_routing_v1:\n            tablet = self._cluster_metadata._tablets.get_tablet_for_key(keyspace, query.table, self._cluster_metadata.token_map.token_class.from_key(query.routing_key))\n            if tablet is not None:\n                replicas_mapped = set(map(lambda r: r[0], tablet.replicas))\n                child_plan = child.make_query_plan(keyspace, query)\n                replicas = [host for host in child_plan if host.host_id in replicas_mapped]\n        if not replicas:\n            replicas = self._cluster_metadata.get_replicas(keyspace, query.routing_key)\n        if self.shuffle_replicas:\n            shuffle(replicas)\n        for replica in replicas:\n            if replica.is_up and child.distance(replica) in [HostDistance.LOCAL, HostDistance.LOCAL_RACK]:\n                yield replica\n        for host in child.make_query_plan(keyspace, query):\n            if host not in replicas or child.distance(host) == HostDistance.REMOTE:\n                yield host\n\n    def on_up(self, *args, **kwargs):\n        return self._child_policy.on_up(*args, **kwargs)\n\n    def on_down(self, *args, **kwargs):\n        return self._child_policy.on_down(*args, **kwargs)\n\n    def on_add(self, *args, **kwargs):\n        return self._child_policy.on_add(*args, **kwargs)\n\n    def on_remove(self, *args, **kwargs):\n        return self._child_policy.on_remove(*args, **kwargs)\n\nclass WhiteListRoundRobinPolicy(RoundRobinPolicy):\n    \"\"\"\n    A subclass of :class:`.RoundRobinPolicy` which evenly\n    distributes queries across all nodes in the cluster,\n    regardless of what datacenter the nodes may be in, but\n    only if that node exists in the list of allowed nodes\n\n    This policy is addresses the issue described in\n    https://datastax-oss.atlassian.net/browse/JAVA-145\n    Where connection errors occur when connection\n    attempts are made to private IP addresses remotely\n    \"\"\"\n\n    def __init__(self, hosts):\n        \"\"\"\n        The `hosts` parameter should be a sequence of hosts to permit\n        connections to.\n        \"\"\"\n        self._allowed_hosts = tuple(hosts)\n        self._allowed_hosts_resolved = []\n        for h in self._allowed_hosts:\n            unix_socket_path = getattr(h, '_unix_socket_path', None)\n            if unix_socket_path:\n                self._allowed_hosts_resolved.append(unix_socket_path)\n            else:\n                self._allowed_hosts_resolved.extend([endpoint[4][0] for endpoint in socket.getaddrinfo(h, None, socket.AF_UNSPEC, socket.SOCK_STREAM)])\n        RoundRobinPolicy.__init__(self)\n\n    def populate(self, cluster, hosts):\n        self._live_hosts = frozenset((h for h in hosts if h.address in self._allowed_hosts_resolved))\n        if len(hosts) <= 1:\n            self._position = 0\n        else:\n            self._position = randint(0, len(hosts) - 1)\n\n    def distance(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            return HostDistance.LOCAL\n        else:\n            return HostDistance.IGNORED\n\n    def on_up(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            RoundRobinPolicy.on_up(self, host)\n\n    def on_add(self, host):\n        if host.address in self._allowed_hosts_resolved:\n            RoundRobinPolicy.on_add(self, host)\n\nclass HostFilterPolicy(LoadBalancingPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` subclass configured with a child policy,\n    and a single-argument predicate. This policy defers to the child policy for\n    hosts where ``predicate(host)`` is truthy. Hosts for which\n    ``predicate(host)`` is falsy will be considered :attr:`.IGNORED`, and will\n    not be used in a query plan.\n\n    This can be used in the cases where you need a whitelist or blacklist\n    policy, e.g. to prepare for decommissioning nodes or for testing:\n\n    .. code-block:: python\n\n        def address_is_ignored(host):\n            return host.address in [ignored_address0, ignored_address1]\n\n        blacklist_filter_policy = HostFilterPolicy(\n            child_policy=RoundRobinPolicy(),\n            predicate=address_is_ignored\n        )\n\n        cluster = Cluster(\n            primary_host,\n            load_balancing_policy=blacklist_filter_policy,\n        )\n\n    See the note in the :meth:`.make_query_plan` documentation for a caveat on\n    how wrapping ordering polices (e.g. :class:`.RoundRobinPolicy`) may break\n    desirable properties of the wrapped policy.\n\n    Please note that whitelist and blacklist policies are not recommended for\n    general, day-to-day use. You probably want something like\n    :class:`.DCAwareRoundRobinPolicy`, which prefers a local DC but has\n    fallbacks, over a brute-force method like whitelisting or blacklisting.\n    \"\"\"\n\n    def __init__(self, child_policy, predicate):\n        \"\"\"\n        :param child_policy: an instantiated :class:`.LoadBalancingPolicy`\n                             that this one will defer to.\n        :param predicate: a one-parameter function that takes a :class:`.Host`.\n                          If it returns a falsy value, the :class:`.Host` will\n                          be :attr:`.IGNORED` and not returned in query plans.\n        \"\"\"\n        super(HostFilterPolicy, self).__init__()\n        self._child_policy = child_policy\n        self._predicate = predicate\n\n    def on_up(self, host, *args, **kwargs):\n        return self._child_policy.on_up(host, *args, **kwargs)\n\n    def on_down(self, host, *args, **kwargs):\n        return self._child_policy.on_down(host, *args, **kwargs)\n\n    def on_add(self, host, *args, **kwargs):\n        return self._child_policy.on_add(host, *args, **kwargs)\n\n    def on_remove(self, host, *args, **kwargs):\n        return self._child_policy.on_remove(host, *args, **kwargs)\n\n    @property\n    def predicate(self):\n        \"\"\"\n        A predicate, set on object initialization, that takes a :class:`.Host`\n        and returns a value. If the value is falsy, the :class:`.Host` is\n        :class:`~HostDistance.IGNORED`. If the value is truthy,\n        :class:`.HostFilterPolicy` defers to the child policy to determine the\n        host's distance.\n\n        This is a read-only value set in ``__init__``, implemented as a\n        ``property``.\n        \"\"\"\n        return self._predicate\n\n    def distance(self, host):\n        \"\"\"\n        Checks if ``predicate(host)``, then returns\n        :attr:`~HostDistance.IGNORED` if falsy, and defers to the child policy\n        otherwise.\n        \"\"\"\n        if self.predicate(host):\n            return self._child_policy.distance(host)\n        else:\n            return HostDistance.IGNORED\n\n    def populate(self, cluster, hosts):\n        self._child_policy.populate(cluster=cluster, hosts=hosts)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        \"\"\"\n        Defers to the child policy's\n        :meth:`.LoadBalancingPolicy.make_query_plan` and filters the results.\n\n        Note that this filtering may break desirable properties of the wrapped\n        policy in some cases. For instance, imagine if you configure this\n        policy to filter out ``host2``, and to wrap a round-robin policy that\n        rotates through three hosts in the order ``host1, host2, host3``,\n        ``host2, host3, host1``, ``host3, host1, host2``, repeating. This\n        policy will yield ``host1, host3``, ``host3, host1``, ``host3, host1``,\n        disproportionately favoring ``host3``.\n        \"\"\"\n        child_qp = self._child_policy.make_query_plan(working_keyspace=working_keyspace, query=query)\n        for host in child_qp:\n            if self.predicate(host):\n                yield host\n\n    def check_supported(self):\n        return self._child_policy.check_supported()\n\nclass ConvictionPolicy(object):\n    \"\"\"\n    A policy which decides when hosts should be considered down\n    based on the types of failures and the number of failures.\n\n    If custom behavior is needed, this class may be subclassed.\n    \"\"\"\n\n    def __init__(self, host):\n        \"\"\"\n        `host` is an instance of :class:`.Host`.\n        \"\"\"\n        self.host = host\n\n    def add_failure(self, connection_exc):\n        \"\"\"\n        Implementations should return :const:`True` if the host should be\n        convicted, :const:`False` otherwise.\n        \"\"\"\n        raise NotImplementedError()\n\n    def reset(self):\n        \"\"\"\n        Implementations should clear out any convictions or state regarding\n        the host.\n        \"\"\"\n        raise NotImplementedError()\n\nclass SimpleConvictionPolicy(ConvictionPolicy):\n    \"\"\"\n    The default implementation of :class:`ConvictionPolicy`,\n    which simply marks a host as down after the first failure\n    of any kind.\n    \"\"\"\n\n    def add_failure(self, connection_exc):\n        return not isinstance(connection_exc, OperationTimedOut)\n\n    def reset(self):\n        pass\n\nclass ReconnectionPolicy(object):\n    \"\"\"\n    This class and its subclasses govern how frequently an attempt is made\n    to reconnect to nodes that are marked as dead.\n\n    If custom behavior is needed, this class may be subclassed.\n    \"\"\"\n\n    def new_schedule(self):\n        \"\"\"\n        This should return a finite or infinite iterable of delays (each as a\n        floating point number of seconds) in-between each failed reconnection\n        attempt.  Note that if the iterable is finite, reconnection attempts\n        will cease once the iterable is exhausted.\n        \"\"\"\n        raise NotImplementedError()\n\nclass ConstantReconnectionPolicy(ReconnectionPolicy):\n    \"\"\"\n    A :class:`.ReconnectionPolicy` subclass which sleeps for a fixed delay\n    in-between each reconnection attempt.\n    \"\"\"\n\n    def __init__(self, delay, max_attempts=64):\n        \"\"\"\n        `delay` should be a floating point number of seconds to wait in-between\n        each attempt.\n\n        `max_attempts` should be a total number of attempts to be made before\n        giving up, or :const:`None` to continue reconnection attempts forever.\n        The default is 64.\n        \"\"\"\n        if delay < 0:\n            raise ValueError('delay must not be negative')\n        if max_attempts is not None and max_attempts < 0:\n            raise ValueError('max_attempts must not be negative')\n        self.delay = delay\n        self.max_attempts = max_attempts\n\n    def new_schedule(self):\n        if self.max_attempts:\n            return repeat(self.delay, self.max_attempts)\n        return repeat(self.delay)\n\nclass ExponentialReconnectionPolicy(ReconnectionPolicy):\n    \"\"\"\n    A :class:`.ReconnectionPolicy` subclass which exponentially increases\n    the length of the delay in-between each reconnection attempt up to\n    a set maximum delay.\n\n    A random amount of jitter (+/- 15%) will be added to the pure exponential\n    delay value to avoid the situations where many reconnection handlers are\n    trying to reconnect at exactly the same time.\n    \"\"\"\n\n    def __init__(self, base_delay, max_delay, max_attempts=64):\n        \"\"\"\n        `base_delay` and `max_delay` should be in floating point units of\n        seconds.\n\n        `max_attempts` should be a total number of attempts to be made before\n        giving up, or :const:`None` to continue reconnection attempts forever.\n        The default is 64.\n        \"\"\"\n        if base_delay < 0 or max_delay < 0:\n            raise ValueError('Delays may not be negative')\n        if max_delay < base_delay:\n            raise ValueError('Max delay must be greater than base delay')\n        if max_attempts is not None and max_attempts < 0:\n            raise ValueError('max_attempts must not be negative')\n        self.base_delay = base_delay\n        self.max_delay = max_delay\n        self.max_attempts = max_attempts\n\n    def new_schedule(self):\n        i, overflowed = (0, False)\n        while self.max_attempts is None or i < self.max_attempts:\n            if overflowed:\n                yield self.max_delay\n            else:\n                try:\n                    yield self._add_jitter(min(self.base_delay * 2 ** i, self.max_delay))\n                except OverflowError:\n                    overflowed = True\n                    yield self.max_delay\n            i += 1\n\n    def _add_jitter(self, value):\n        jitter = randint(85, 115)\n        delay = jitter * value / 100\n        return min(max(self.base_delay, delay), self.max_delay)\n\nclass RetryPolicy(object):\n    \"\"\"\n    A policy that describes whether to retry, rethrow, or ignore coordinator\n    timeout and unavailable failures. These are failures reported from the\n    server side. Timeouts are configured by\n    `settings in cassandra.yaml <https://github.com/apache/cassandra/blob/cassandra-2.1.4/conf/cassandra.yaml#L568-L584>`_.\n    Unavailable failures occur when the coordinator cannot achieve the consistency\n    level for a request. For further information see the method descriptions\n    below.\n\n    To specify a default retry policy, set the\n    :attr:`.Cluster.default_retry_policy` attribute to an instance of this\n    class or one of its subclasses.\n\n    To specify a retry policy per query, set the :attr:`.Statement.retry_policy`\n    attribute to an instance of this class or one of its subclasses.\n\n    If custom behavior is needed for retrying certain operations,\n    this class may be subclassed.\n    \"\"\"\n    RETRY = 0\n    '\\n    This should be returned from the below methods if the operation\\n    should be retried on the same connection.\\n    '\n    RETHROW = 1\n    '\\n    This should be returned from the below methods if the failure\\n    should be propagated and no more retries attempted.\\n    '\n    IGNORE = 2\n    '\\n    This should be returned from the below methods if the failure\\n    should be ignored but no more retries should be attempted.\\n    '\n    RETRY_NEXT_HOST = 3\n    '\\n    This should be returned from the below methods if the operation\\n    should be retried on another connection.\\n    '\n\n    def on_read_timeout(self, query, consistency, required_responses, received_responses, data_retrieved, retry_num):\n        \"\"\"\n        This is called when a read operation times out from the coordinator's\n        perspective (i.e. a replica did not respond to the coordinator in time).\n        It should return a tuple with two items: one of the class enums (such\n        as :attr:`.RETRY`) and a :class:`.ConsistencyLevel` to retry the\n        operation at or :const:`None` to keep the same consistency level.\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        The `required_responses` and `received_responses` parameters describe\n        how many replicas needed to respond to meet the requested consistency\n        level and how many actually did respond before the coordinator timed\n        out the request. `data_retrieved` is a boolean indicating whether\n        any of those responses contained data (as opposed to just a digest).\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, operations will be retried at most once, and only if\n        a sufficient number of replicas responded (with data digests).\n        \"\"\"\n        if retry_num != 0:\n            return (self.RETHROW, None)\n        elif received_responses >= required_responses and (not data_retrieved):\n            return (self.RETRY, consistency)\n        else:\n            return (self.RETHROW, None)\n\n    def on_write_timeout(self, query, consistency, write_type, required_responses, received_responses, retry_num):\n        \"\"\"\n        This is called when a write operation times out from the coordinator's\n        perspective (i.e. a replica did not respond to the coordinator in time).\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `write_type` is one of the :class:`.WriteType` enums describing the\n        type of write operation.\n\n        The `required_responses` and `received_responses` parameters describe\n        how many replicas needed to acknowledge the write to meet the requested\n        consistency level and how many replicas actually did acknowledge the\n        write before the coordinator timed out the request.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, failed write operations will retried at most once, and\n        they will only be retried if the `write_type` was\n        :attr:`~.WriteType.BATCH_LOG`.\n        \"\"\"\n        if retry_num != 0:\n            return (self.RETHROW, None)\n        elif write_type == WriteType.BATCH_LOG:\n            return (self.RETRY, consistency)\n        else:\n            return (self.RETHROW, None)\n\n    def on_unavailable(self, query, consistency, required_replicas, alive_replicas, retry_num):\n        \"\"\"\n        This is called when the coordinator node determines that a read or\n        write operation cannot be successful because the number of live\n        replicas are too low to meet the requested :class:`.ConsistencyLevel`.\n        This means that the read or write operation was never forwarded to\n        any replicas.\n\n        `query` is the :class:`.Statement` that failed.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `required_replicas` is the number of replicas that would have needed to\n        acknowledge the operation to meet the requested consistency level.\n        `alive_replicas` is the number of replicas that the coordinator\n        considered alive at the time of the request.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, if this is the first retry, it triggers a retry on the next\n        host in the query plan with the same consistency level. If this is not the\n        first retry, no retries will be attempted and the error will be re-raised.\n        \"\"\"\n        return (self.RETRY_NEXT_HOST, None) if retry_num == 0 else (self.RETHROW, None)\n\n    def on_request_error(self, query, consistency, error, retry_num):\n        \"\"\"\n        This is called when an unexpected error happens. This can be in the\n        following situations:\n\n        * On a connection error\n        * On server errors: overloaded, isBootstrapping, serverError, etc.\n\n        `query` is the :class:`.Statement` that timed out.\n\n        `consistency` is the :class:`.ConsistencyLevel` that the operation was\n        attempted at.\n\n        `error` the instance of the exception.\n\n        `retry_num` counts how many times the operation has been retried, so\n        the first time this method is called, `retry_num` will be 0.\n\n        By default, it triggers a retry on the next host in the query plan\n        with the same consistency level.\n        \"\"\"\n        return (self.RETRY_NEXT_HOST, None)\n\nclass FallthroughRetryPolicy(RetryPolicy):\n    \"\"\"\n    A retry policy that never retries and always propagates failures to\n    the application.\n    \"\"\"\n\n    def on_read_timeout(self, *args, **kwargs):\n        return (self.RETHROW, None)\n\n    def on_write_timeout(self, *args, **kwargs):\n        return (self.RETHROW, None)\n\n    def on_unavailable(self, *args, **kwargs):\n        return (self.RETHROW, None)\n\n    def on_request_error(self, *args, **kwargs):\n        return (self.RETHROW, None)\n\nclass DowngradingConsistencyRetryPolicy(RetryPolicy):\n    \"\"\"\n    *Deprecated:* This retry policy will be removed in the next major release.\n\n    A retry policy that sometimes retries with a lower consistency level than\n    the one initially requested.\n\n    **BEWARE**: This policy may retry queries using a lower consistency\n    level than the one initially requested. By doing so, it may break\n    consistency guarantees. In other words, if you use this retry policy,\n    there are cases (documented below) where a read at :attr:`~.QUORUM`\n    *may not* see a preceding write at :attr:`~.QUORUM`. Do not use this\n    policy unless you have understood the cases where this can happen and\n    are ok with that. It is also recommended to subclass this class so\n    that queries that required a consistency level downgrade can be\n    recorded (so that repairs can be made later, etc).\n\n    This policy implements the same retries as :class:`.RetryPolicy`,\n    but on top of that, it also retries in the following cases:\n\n    * On a read timeout: if the number of replicas that responded is\n      greater than one but lower than is required by the requested\n      consistency level, the operation is retried at a lower consistency\n      level.\n    * On a write timeout: if the operation is an :attr:`~.UNLOGGED_BATCH`\n      and at least one replica acknowledged the write, the operation is\n      retried at a lower consistency level.  Furthermore, for other\n      write types, if at least one replica acknowledged the write, the\n      timeout is ignored.\n    * On an unavailable exception: if at least one replica is alive, the\n      operation is retried at a lower consistency level.\n\n    The reasoning behind this retry policy is as follows: if, based\n    on the information the Cassandra coordinator node returns, retrying the\n    operation with the initially requested consistency has a chance to\n    succeed, do it. Otherwise, if based on that information we know the\n    initially requested consistency level cannot be achieved currently, then:\n\n    * For writes, ignore the exception (thus silently failing the\n      consistency requirement) if we know the write has been persisted on at\n      least one replica.\n    * For reads, try reading at a lower consistency level (thus silently\n      failing the consistency requirement).\n\n    In other words, this policy implements the idea that if the requested\n    consistency level cannot be achieved, the next best thing for writes is\n    to make sure the data is persisted, and that reading something is better\n    than reading nothing, even if there is a risk of reading stale data.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(DowngradingConsistencyRetryPolicy, self).__init__(*args, **kwargs)\n        warnings.warn('DowngradingConsistencyRetryPolicy is deprecated and will be removed in the next major release.', DeprecationWarning)\n\n    def _pick_consistency(self, num_responses):\n        if num_responses >= 3:\n            return (self.RETRY, ConsistencyLevel.THREE)\n        elif num_responses >= 2:\n            return (self.RETRY, ConsistencyLevel.TWO)\n        elif num_responses >= 1:\n            return (self.RETRY, ConsistencyLevel.ONE)\n        else:\n            return (self.RETHROW, None)\n\n    def on_read_timeout(self, query, consistency, required_responses, received_responses, data_retrieved, retry_num):\n        if retry_num != 0:\n            return (self.RETHROW, None)\n        elif ConsistencyLevel.is_serial(consistency):\n            return (self.RETHROW, None)\n        elif received_responses < required_responses:\n            return self._pick_consistency(received_responses)\n        elif not data_retrieved:\n            return (self.RETRY, consistency)\n        else:\n            return (self.RETHROW, None)\n\n    def on_write_timeout(self, query, consistency, write_type, required_responses, received_responses, retry_num):\n        if retry_num != 0:\n            return (self.RETHROW, None)\n        if write_type in (WriteType.SIMPLE, WriteType.BATCH, WriteType.COUNTER):\n            if received_responses > 0:\n                return (self.IGNORE, None)\n            else:\n                return (self.RETHROW, None)\n        elif write_type == WriteType.UNLOGGED_BATCH:\n            return self._pick_consistency(received_responses)\n        elif write_type == WriteType.BATCH_LOG:\n            return (self.RETRY, consistency)\n        return (self.RETHROW, None)\n\n    def on_unavailable(self, query, consistency, required_replicas, alive_replicas, retry_num):\n        if retry_num != 0:\n            return (self.RETHROW, None)\n        elif ConsistencyLevel.is_serial(consistency):\n            return (self.RETRY_NEXT_HOST, None)\n        else:\n            return self._pick_consistency(alive_replicas)\n\nclass ExponentialBackoffRetryPolicy(RetryPolicy):\n    \"\"\"\n    A policy that do retries with exponential backoff\n    \"\"\"\n\n    def __init__(self, max_num_retries: float, min_interval: float=0.1, max_interval: float=10.0, *args, **kwargs):\n        \"\"\"\n        `max_num_retries` counts how many times the operation would be retried,\n        `min_interval` is the initial time in seconds to wait before first retry\n        `max_interval` is the maximum time to wait between retries\n        \"\"\"\n        self.min_interval = min_interval\n        self.max_num_retries = max_num_retries\n        self.max_interval = max_interval\n        super(ExponentialBackoffRetryPolicy).__init__(*args, **kwargs)\n\n    def _calculate_backoff(self, attempt: int):\n        delay = min(self.max_interval, self.min_interval * 2 ** attempt)\n        delay += random.random() * self.min_interval - self.min_interval / 2\n        return delay\n\n    def on_read_timeout(self, query, consistency, required_responses, received_responses, data_retrieved, retry_num):\n        if retry_num < self.max_num_retries and received_responses >= required_responses and (not data_retrieved):\n            return (self.RETRY, consistency, self._calculate_backoff(retry_num))\n        else:\n            return (self.RETHROW, None, None)\n\n    def on_write_timeout(self, query, consistency, write_type, required_responses, received_responses, retry_num):\n        if retry_num < self.max_num_retries and write_type == WriteType.BATCH_LOG:\n            return (self.RETRY, consistency, self._calculate_backoff(retry_num))\n        else:\n            return (self.RETHROW, None, None)\n\n    def on_unavailable(self, query, consistency, required_replicas, alive_replicas, retry_num):\n        if retry_num < self.max_num_retries:\n            return (self.RETRY_NEXT_HOST, None, self._calculate_backoff(retry_num))\n        else:\n            return (self.RETHROW, None, None)\n\n    def on_request_error(self, query, consistency, error, retry_num):\n        if retry_num < self.max_num_retries:\n            return (self.RETRY_NEXT_HOST, None, self._calculate_backoff(retry_num))\n        else:\n            return (self.RETHROW, None, None)\n\nclass AddressTranslator(object):\n    \"\"\"\n    Interface for translating cluster-defined endpoints.\n\n    The driver discovers nodes using server metadata and topology change events. Normally,\n    the endpoint defined by the server is the right way to connect to a node. In some environments,\n    these addresses may not be reachable, or not preferred (public vs. private IPs in cloud environments,\n    suboptimal routing, etc). This interface allows for translating from server defined endpoints to\n    preferred addresses for driver connections.\n\n    *Note:* :attr:`~Cluster.contact_points` provided while creating the :class:`~.Cluster` instance are not\n    translated using this mechanism -- only addresses received from Cassandra nodes are.\n    \"\"\"\n\n    def translate(self, addr):\n        \"\"\"\n        Accepts the node ip address, and returns a translated address to be used connecting to this node.\n        \"\"\"\n        raise NotImplementedError()\n\nclass IdentityTranslator(AddressTranslator):\n    \"\"\"\n    Returns the endpoint with no translation\n    \"\"\"\n\n    def translate(self, addr):\n        return addr\n\nclass EC2MultiRegionTranslator(AddressTranslator):\n    \"\"\"\n    Resolves private ips of the hosts in the same datacenter as the client, and public ips of hosts in other datacenters.\n    \"\"\"\n\n    def translate(self, addr):\n        \"\"\"\n        Reverse DNS the public broadcast_address, then lookup that hostname to get the AWS-resolved IP, which\n        will point to the private IP address within the same datacenter.\n        \"\"\"\n        family = socket.getaddrinfo(addr, 0, socket.AF_UNSPEC, socket.SOCK_STREAM)[0][0]\n        host = socket.getfqdn(addr)\n        for a in socket.getaddrinfo(host, 0, family, socket.SOCK_STREAM):\n            try:\n                return a[4][0]\n            except Exception:\n                pass\n        return addr\n\nclass SpeculativeExecutionPolicy(object):\n    \"\"\"\n    Interface for specifying speculative execution plans\n    \"\"\"\n\n    def new_plan(self, keyspace, statement):\n        \"\"\"\n        Returns\n\n        :param keyspace:\n        :param statement:\n        :return:\n        \"\"\"\n        raise NotImplementedError()\n\nclass SpeculativeExecutionPlan(object):\n\n    def next_execution(self, host):\n        raise NotImplementedError()\n\nclass NoSpeculativeExecutionPlan(SpeculativeExecutionPlan):\n\n    def next_execution(self, host):\n        return -1\n\nclass NoSpeculativeExecutionPolicy(SpeculativeExecutionPolicy):\n\n    def new_plan(self, keyspace, statement):\n        return NoSpeculativeExecutionPlan()\n\nclass ConstantSpeculativeExecutionPolicy(SpeculativeExecutionPolicy):\n    \"\"\"\n    A speculative execution policy that sends a new query every X seconds (**delay**) for a maximum of Y attempts (**max_attempts**).\n    \"\"\"\n\n    def __init__(self, delay, max_attempts):\n        self.delay = delay\n        self.max_attempts = max_attempts\n\n    class ConstantSpeculativeExecutionPlan(SpeculativeExecutionPlan):\n\n        def __init__(self, delay, max_attempts):\n            self.delay = delay\n            self.remaining = max_attempts\n\n        def next_execution(self, host):\n            if self.remaining > 0:\n                self.remaining -= 1\n                return self.delay\n            else:\n                return -1\n\n    def new_plan(self, keyspace, statement):\n        return self.ConstantSpeculativeExecutionPlan(self.delay, self.max_attempts)\n\nclass WrapperPolicy(LoadBalancingPolicy):\n\n    def __init__(self, child_policy):\n        self._child_policy = child_policy\n\n    def distance(self, *args, **kwargs):\n        return self._child_policy.distance(*args, **kwargs)\n\n    def populate(self, cluster, hosts):\n        self._child_policy.populate(cluster, hosts)\n\n    def on_up(self, *args, **kwargs):\n        return self._child_policy.on_up(*args, **kwargs)\n\n    def on_down(self, *args, **kwargs):\n        return self._child_policy.on_down(*args, **kwargs)\n\n    def on_add(self, *args, **kwargs):\n        return self._child_policy.on_add(*args, **kwargs)\n\n    def on_remove(self, *args, **kwargs):\n        return self._child_policy.on_remove(*args, **kwargs)\n\nclass DefaultLoadBalancingPolicy(WrapperPolicy):\n    \"\"\"\n    A :class:`.LoadBalancingPolicy` wrapper that adds the ability to target a specific host first.\n\n    If no host is set on the query, the child policy's query plan will be used as is.\n    \"\"\"\n    _cluster_metadata = None\n\n    def populate(self, cluster, hosts):\n        self._cluster_metadata = cluster.metadata\n        self._child_policy.populate(cluster, hosts)\n\n    def make_query_plan(self, working_keyspace=None, query=None):\n        if query and query.keyspace:\n            keyspace = query.keyspace\n        else:\n            keyspace = working_keyspace\n        addr = getattr(query, 'target_host', None) if query else None\n        target_host = self._cluster_metadata.get_host(addr)\n        child = self._child_policy\n        if target_host and target_host.is_up:\n            yield target_host\n            for h in child.make_query_plan(keyspace, query):\n                if h != target_host:\n                    yield h\n        else:\n            for h in child.make_query_plan(keyspace, query):\n                yield h\n\nclass DSELoadBalancingPolicy(DefaultLoadBalancingPolicy):\n    \"\"\"\n    *Deprecated:* This will be removed in the next major release,\n    consider using :class:`.DefaultLoadBalancingPolicy`.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super(DSELoadBalancingPolicy, self).__init__(*args, **kwargs)\n        warnings.warn('DSELoadBalancingPolicy will be removed in 4.0. Consider using DefaultLoadBalancingPolicy.', DeprecationWarning)\n\nclass NeverRetryPolicy(RetryPolicy):\n\n    def _rethrow(self, *args, **kwargs):\n        return (self.RETHROW, None)\n    on_read_timeout = _rethrow\n    on_write_timeout = _rethrow\n    on_unavailable = _rethrow\nColDesc = namedtuple('ColDesc', ['ks', 'table', 'col'])\n\nclass ColumnEncryptionPolicy(object):\n    \"\"\"\n    A policy enabling (mostly) transparent encryption and decryption of data before it is\n    sent to the cluster.\n\n    Key materials and other configurations are specified on a per-column basis.  This policy can\n    then be used by driver structures which are aware of the underlying columns involved in their\n    work.  In practice this includes the following cases:\n\n    * Prepared statements - data for columns specified by the cluster's policy will be transparently\n      encrypted before they are sent\n    * Rows returned from any query - data for columns specified by the cluster's policy will be\n      transparently decrypted before they are returned to the user\n\n    To enable this functionality, create an instance of this class (or more likely a subclass)\n    before creating a cluster.  This policy should then be configured and supplied to the Cluster\n    at creation time via the :attr:`.Cluster.column_encryption_policy` attribute.\n    \"\"\"\n\n    def encrypt(self, coldesc, obj_bytes):\n        \"\"\"\n        Encrypt the specified bytes using the cryptography materials for the specified column.\n        Largely used internally, although this could also be used to encrypt values supplied\n        to non-prepared statements in a way that is consistent with this policy.\n        \"\"\"\n        raise NotImplementedError()\n\n    def decrypt(self, coldesc, encrypted_bytes):\n        \"\"\"\n        Decrypt the specified (encrypted) bytes using the cryptography materials for the\n        specified column.  Used internally; could be used externally as well but there's\n        not currently an obvious use case.\n        \"\"\"\n        raise NotImplementedError()\n\n    def add_column(self, coldesc, key):\n        \"\"\"\n        Provide cryptography materials to be used when encrypted and/or decrypting data\n        for the specified column.\n        \"\"\"\n        raise NotImplementedError()\n\n    def contains_column(self, coldesc):\n        \"\"\"\n        Predicate to determine if a specific column is supported by this policy.\n        Currently only used internally.\n        \"\"\"\n        raise NotImplementedError()\n\n    def encode_and_encrypt(self, coldesc, obj):\n        \"\"\"\n        Helper function to enable use of this policy on simple (i.e. non-prepared)\n        statements.\n        \"\"\"\n        raise NotImplementedError()",
    "cassandra/__init__.py": "from enum import Enum\nimport logging\n\nclass NullHandler(logging.Handler):\n\n    def emit(self, record):\n        pass\nlogging.getLogger('cassandra').addHandler(NullHandler())\n__version_info__ = (3, 28, 0)\n__version__ = '.'.join(map(str, __version_info__))\n\nclass ConsistencyLevel(object):\n    \"\"\"\n    Spcifies how many replicas must respond for an operation to be considered\n    a success.  By default, ``ONE`` is used for all operations.\n    \"\"\"\n    ANY = 0\n    '\\n    Only requires that one replica receives the write *or* the coordinator\\n    stores a hint to replay later. Valid only for writes.\\n    '\n    ONE = 1\n    '\\n    Only one replica needs to respond to consider the operation a success\\n    '\n    TWO = 2\n    '\\n    Two replicas must respond to consider the operation a success\\n    '\n    THREE = 3\n    '\\n    Three replicas must respond to consider the operation a success\\n    '\n    QUORUM = 4\n    '\\n    ``ceil(RF/2) + 1`` replicas must respond to consider the operation a success\\n    '\n    ALL = 5\n    '\\n    All replicas must respond to consider the operation a success\\n    '\n    LOCAL_QUORUM = 6\n    '\\n    Requires a quorum of replicas in the local datacenter\\n    '\n    EACH_QUORUM = 7\n    '\\n    Requires a quorum of replicas in each datacenter\\n    '\n    SERIAL = 8\n    \"\\n    For conditional inserts/updates that utilize Cassandra's lightweight\\n    transactions, this requires consensus among all replicas for the\\n    modified data.\\n    \"\n    LOCAL_SERIAL = 9\n    '\\n    Like :attr:`~ConsistencyLevel.SERIAL`, but only requires consensus\\n    among replicas in the local datacenter.\\n    '\n    LOCAL_ONE = 10\n    '\\n    Sends a request only to replicas in the local datacenter and waits for\\n    one response.\\n    '\n\n    @staticmethod\n    def is_serial(cl):\n        return cl == ConsistencyLevel.SERIAL or cl == ConsistencyLevel.LOCAL_SERIAL\nConsistencyLevel.value_to_name = {ConsistencyLevel.ANY: 'ANY', ConsistencyLevel.ONE: 'ONE', ConsistencyLevel.TWO: 'TWO', ConsistencyLevel.THREE: 'THREE', ConsistencyLevel.QUORUM: 'QUORUM', ConsistencyLevel.ALL: 'ALL', ConsistencyLevel.LOCAL_QUORUM: 'LOCAL_QUORUM', ConsistencyLevel.EACH_QUORUM: 'EACH_QUORUM', ConsistencyLevel.SERIAL: 'SERIAL', ConsistencyLevel.LOCAL_SERIAL: 'LOCAL_SERIAL', ConsistencyLevel.LOCAL_ONE: 'LOCAL_ONE'}\nConsistencyLevel.name_to_value = {'ANY': ConsistencyLevel.ANY, 'ONE': ConsistencyLevel.ONE, 'TWO': ConsistencyLevel.TWO, 'THREE': ConsistencyLevel.THREE, 'QUORUM': ConsistencyLevel.QUORUM, 'ALL': ConsistencyLevel.ALL, 'LOCAL_QUORUM': ConsistencyLevel.LOCAL_QUORUM, 'EACH_QUORUM': ConsistencyLevel.EACH_QUORUM, 'SERIAL': ConsistencyLevel.SERIAL, 'LOCAL_SERIAL': ConsistencyLevel.LOCAL_SERIAL, 'LOCAL_ONE': ConsistencyLevel.LOCAL_ONE}\n\ndef consistency_value_to_name(value):\n    return ConsistencyLevel.value_to_name[value] if value is not None else 'Not Set'\n\nclass ProtocolVersion(object):\n    \"\"\"\n    Defines native protocol versions supported by this driver.\n    \"\"\"\n    V1 = 1\n    '\\n    v1, supported in Cassandra 1.2-->2.2\\n    '\n    V2 = 2\n    '\\n    v2, supported in Cassandra 2.0-->2.2;\\n    added support for lightweight transactions, batch operations, and automatic query paging.\\n    '\n    V3 = 3\n    '\\n    v3, supported in Cassandra 2.1-->3.x+;\\n    added support for protocol-level client-side timestamps (see :attr:`.Session.use_client_timestamp`),\\n    serial consistency levels for :class:`~.BatchStatement`, and an improved connection pool.\\n    '\n    V4 = 4\n    '\\n    v4, supported in Cassandra 2.2-->3.x+;\\n    added a number of new types, server warnings, new failure messages, and custom payloads. Details in the\\n    `project docs <https://github.com/apache/cassandra/blob/trunk/doc/native_protocol_v4.spec>`_\\n    '\n    V5 = 5\n    '\\n    v5, in beta from 3.x+. Finalised in 4.0-beta5\\n    '\n    V6 = 6\n    '\\n    v6, in beta from 4.0-beta5\\n    '\n    DSE_V1 = 65\n    '\\n    DSE private protocol v1, supported in DSE 5.1+\\n    '\n    DSE_V2 = 66\n    '\\n    DSE private protocol v2, supported in DSE 6.0+\\n    '\n    SUPPORTED_VERSIONS = (DSE_V2, DSE_V1, V6, V5, V4, V3, V2, V1)\n    '\\n    A tuple of all supported protocol versions\\n    '\n    BETA_VERSIONS = (V6,)\n    '\\n    A tuple of all beta protocol versions\\n    '\n    MIN_SUPPORTED = min(SUPPORTED_VERSIONS)\n    '\\n    Minimum protocol version supported by this driver.\\n    '\n    MAX_SUPPORTED = max(SUPPORTED_VERSIONS)\n    '\\n    Maximum protocol version supported by this driver.\\n    '\n\n    @classmethod\n    def get_lower_supported(cls, previous_version):\n        \"\"\"\n        Return the lower supported protocol version. Beta versions are omitted.\n        \"\"\"\n        try:\n            version = next((v for v in sorted(ProtocolVersion.SUPPORTED_VERSIONS, reverse=True) if v not in ProtocolVersion.BETA_VERSIONS and v < previous_version))\n        except StopIteration:\n            version = 0\n        return version\n\n    @classmethod\n    def uses_int_query_flags(cls, version):\n        return version >= cls.V5\n\n    @classmethod\n    def uses_prepare_flags(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def uses_prepared_metadata(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def uses_error_code_map(cls, version):\n        return version >= cls.V5\n\n    @classmethod\n    def uses_keyspace_flag(cls, version):\n        return version >= cls.V5 and version != cls.DSE_V1\n\n    @classmethod\n    def has_continuous_paging_support(cls, version):\n        return version >= cls.DSE_V1\n\n    @classmethod\n    def has_continuous_paging_next_pages(cls, version):\n        return version >= cls.DSE_V2\n\n    @classmethod\n    def has_checksumming_support(cls, version):\n        return cls.V5 <= version < cls.DSE_V1\n\nclass WriteType(object):\n    \"\"\"\n    For usage with :class:`.RetryPolicy`, this describe a type\n    of write operation.\n    \"\"\"\n    SIMPLE = 0\n    '\\n    A write to a single partition key. Such writes are guaranteed to be atomic\\n    and isolated.\\n    '\n    BATCH = 1\n    '\\n    A write to multiple partition keys that used the distributed batch log to\\n    ensure atomicity.\\n    '\n    UNLOGGED_BATCH = 2\n    '\\n    A write to multiple partition keys that did not use the distributed batch\\n    log. Atomicity for such writes is not guaranteed.\\n    '\n    COUNTER = 3\n    '\\n    A counter write (for one or multiple partition keys). Such writes should\\n    not be replayed in order to avoid overcount.\\n    '\n    BATCH_LOG = 4\n    '\\n    The initial write to the distributed batch log that Cassandra performs\\n    internally before a BATCH write.\\n    '\n    CAS = 5\n    '\\n    A lighweight-transaction write, such as \"DELETE ... IF EXISTS\".\\n    '\n    VIEW = 6\n    '\\n    This WriteType is only seen in results for requests that were unable to\\n    complete MV operations.\\n    '\n    CDC = 7\n    '\\n    This WriteType is only seen in results for requests that were unable to\\n    complete CDC operations.\\n    '\nWriteType.name_to_value = {'SIMPLE': WriteType.SIMPLE, 'BATCH': WriteType.BATCH, 'UNLOGGED_BATCH': WriteType.UNLOGGED_BATCH, 'COUNTER': WriteType.COUNTER, 'BATCH_LOG': WriteType.BATCH_LOG, 'CAS': WriteType.CAS, 'VIEW': WriteType.VIEW, 'CDC': WriteType.CDC}\nWriteType.value_to_name = {v: k for k, v in WriteType.name_to_value.items()}\n\nclass SchemaChangeType(object):\n    DROPPED = 'DROPPED'\n    CREATED = 'CREATED'\n    UPDATED = 'UPDATED'\n\nclass SchemaTargetType(object):\n    KEYSPACE = 'KEYSPACE'\n    TABLE = 'TABLE'\n    TYPE = 'TYPE'\n    FUNCTION = 'FUNCTION'\n    AGGREGATE = 'AGGREGATE'\n\nclass SignatureDescriptor(object):\n\n    def __init__(self, name, argument_types):\n        self.name = name\n        self.argument_types = argument_types\n\n    @property\n    def signature(self):\n        \"\"\"\n        function signature string in the form 'name([type0[,type1[...]]])'\n\n        can be used to uniquely identify overloaded function names within a keyspace\n        \"\"\"\n        return self.format_signature(self.name, self.argument_types)\n\n    @staticmethod\n    def format_signature(name, argument_types):\n        return '%s(%s)' % (name, ','.join((t for t in argument_types)))\n\n    def __repr__(self):\n        return '%s(%s, %s)' % (self.__class__.__name__, self.name, self.argument_types)\n\nclass UserFunctionDescriptor(SignatureDescriptor):\n    \"\"\"\n    Describes a User function by name and argument signature\n    \"\"\"\n    name = None\n    '\\n    name of the function\\n    '\n    argument_types = None\n    '\\n    Ordered list of CQL argument type names comprising the type signature\\n    '\n\nclass UserAggregateDescriptor(SignatureDescriptor):\n    \"\"\"\n    Describes a User aggregate function by name and argument signature\n    \"\"\"\n    name = None\n    '\\n    name of the aggregate\\n    '\n    argument_types = None\n    '\\n    Ordered list of CQL argument type names comprising the type signature\\n    '\n\nclass DriverException(Exception):\n    \"\"\"\n    Base for all exceptions explicitly raised by the driver.\n    \"\"\"\n    pass\n\nclass RequestExecutionException(DriverException):\n    \"\"\"\n    Base for request execution exceptions returned from the server.\n    \"\"\"\n    pass\n\nclass Unavailable(RequestExecutionException):\n    \"\"\"\n    There were not enough live replicas to satisfy the requested consistency\n    level, so the coordinator node immediately failed the request without\n    forwarding it to any replicas.\n    \"\"\"\n    consistency = None\n    ' The requested :class:`ConsistencyLevel` '\n    required_replicas = None\n    ' The number of replicas that needed to be live to complete the operation '\n    alive_replicas = None\n    ' The number of replicas that were actually alive '\n\n    def __init__(self, summary_message, consistency=None, required_replicas=None, alive_replicas=None):\n        self.consistency = consistency\n        self.required_replicas = required_replicas\n        self.alive_replicas = alive_replicas\n        Exception.__init__(self, summary_message + ' info=' + repr({'consistency': consistency_value_to_name(consistency), 'required_replicas': required_replicas, 'alive_replicas': alive_replicas}))\n\nclass Timeout(RequestExecutionException):\n    \"\"\"\n    Replicas failed to respond to the coordinator node before timing out.\n    \"\"\"\n    consistency = None\n    ' The requested :class:`ConsistencyLevel` '\n    required_responses = None\n    ' The number of required replica responses '\n    received_responses = None\n    '\\n    The number of replicas that responded before the coordinator timed out\\n    the operation\\n    '\n\n    def __init__(self, summary_message, consistency=None, required_responses=None, received_responses=None, **kwargs):\n        self.consistency = consistency\n        self.required_responses = required_responses\n        self.received_responses = received_responses\n        if 'write_type' in kwargs:\n            kwargs['write_type'] = WriteType.value_to_name[kwargs['write_type']]\n        info = {'consistency': consistency_value_to_name(consistency), 'required_responses': required_responses, 'received_responses': received_responses}\n        info.update(kwargs)\n        Exception.__init__(self, summary_message + ' info=' + repr(info))\n\nclass ReadTimeout(Timeout):\n    \"\"\"\n    A subclass of :exc:`Timeout` for read operations.\n\n    This indicates that the replicas failed to respond to the coordinator\n    node before the configured timeout. This timeout is configured in\n    ``cassandra.yaml`` with the ``read_request_timeout_in_ms``\n    and ``range_request_timeout_in_ms`` options.\n    \"\"\"\n    data_retrieved = None\n    '\\n    A boolean indicating whether the requested data was retrieved\\n    by the coordinator from any replicas before it timed out the\\n    operation\\n    '\n\n    def __init__(self, message, data_retrieved=None, **kwargs):\n        Timeout.__init__(self, message, **kwargs)\n        self.data_retrieved = data_retrieved\n\nclass WriteTimeout(Timeout):\n    \"\"\"\n    A subclass of :exc:`Timeout` for write operations.\n\n    This indicates that the replicas failed to respond to the coordinator\n    node before the configured timeout. This timeout is configured in\n    ``cassandra.yaml`` with the ``write_request_timeout_in_ms``\n    option.\n    \"\"\"\n    write_type = None\n    '\\n    The type of write operation, enum on :class:`~cassandra.policies.WriteType`\\n    '\n\n    def __init__(self, message, write_type=None, **kwargs):\n        kwargs['write_type'] = write_type\n        Timeout.__init__(self, message, **kwargs)\n        self.write_type = write_type\n\nclass CDCWriteFailure(RequestExecutionException):\n    \"\"\"\n    Hit limit on data in CDC folder, writes are rejected\n    \"\"\"\n\n    def __init__(self, message):\n        Exception.__init__(self, message)\n\nclass CoordinationFailure(RequestExecutionException):\n    \"\"\"\n    Replicas sent a failure to the coordinator.\n    \"\"\"\n    consistency = None\n    ' The requested :class:`ConsistencyLevel` '\n    required_responses = None\n    ' The number of required replica responses '\n    received_responses = None\n    '\\n    The number of replicas that responded before the coordinator timed out\\n    the operation\\n    '\n    failures = None\n    '\\n    The number of replicas that sent a failure message\\n    '\n    error_code_map = None\n    '\\n    A map of inet addresses to error codes representing replicas that sent\\n    a failure message.  Only set when `protocol_version` is 5 or higher.\\n    '\n\n    def __init__(self, summary_message, consistency=None, required_responses=None, received_responses=None, failures=None, error_code_map=None):\n        self.consistency = consistency\n        self.required_responses = required_responses\n        self.received_responses = received_responses\n        self.failures = failures\n        self.error_code_map = error_code_map\n        info_dict = {'consistency': consistency_value_to_name(consistency), 'required_responses': required_responses, 'received_responses': received_responses, 'failures': failures}\n        if error_code_map is not None:\n            formatted_map = dict(((addr, '0x%04x' % err_code) for addr, err_code in error_code_map.items()))\n            info_dict['error_code_map'] = formatted_map\n        Exception.__init__(self, summary_message + ' info=' + repr(info_dict))\n\nclass ReadFailure(CoordinationFailure):\n    \"\"\"\n    A subclass of :exc:`CoordinationFailure` for read operations.\n\n    This indicates that the replicas sent a failure message to the coordinator.\n    \"\"\"\n    data_retrieved = None\n    '\\n    A boolean indicating whether the requested data was retrieved\\n    by the coordinator from any replicas before it timed out the\\n    operation\\n    '\n\n    def __init__(self, message, data_retrieved=None, **kwargs):\n        CoordinationFailure.__init__(self, message, **kwargs)\n        self.data_retrieved = data_retrieved\n\nclass WriteFailure(CoordinationFailure):\n    \"\"\"\n    A subclass of :exc:`CoordinationFailure` for write operations.\n\n    This indicates that the replicas sent a failure message to the coordinator.\n    \"\"\"\n    write_type = None\n    '\\n    The type of write operation, enum on :class:`~cassandra.policies.WriteType`\\n    '\n\n    def __init__(self, message, write_type=None, **kwargs):\n        CoordinationFailure.__init__(self, message, **kwargs)\n        self.write_type = write_type\n\nclass FunctionFailure(RequestExecutionException):\n    \"\"\"\n    User Defined Function failed during execution\n    \"\"\"\n    keyspace = None\n    '\\n    Keyspace of the function\\n    '\n    function = None\n    '\\n    Name of the function\\n    '\n    arg_types = None\n    '\\n    List of argument type names of the function\\n    '\n\n    def __init__(self, summary_message, keyspace, function, arg_types):\n        self.keyspace = keyspace\n        self.function = function\n        self.arg_types = arg_types\n        Exception.__init__(self, summary_message)\n\nclass RequestValidationException(DriverException):\n    \"\"\"\n    Server request validation failed\n    \"\"\"\n    pass\n\nclass ConfigurationException(RequestValidationException):\n    \"\"\"\n    Server indicated request errro due to current configuration\n    \"\"\"\n    pass\n\nclass AlreadyExists(ConfigurationException):\n    \"\"\"\n    An attempt was made to create a keyspace or table that already exists.\n    \"\"\"\n    keyspace = None\n    '\\n    The name of the keyspace that already exists, or, if an attempt was\\n    made to create a new table, the keyspace that the table is in.\\n    '\n    table = None\n    '\\n    The name of the table that already exists, or, if an attempt was\\n    make to create a keyspace, :const:`None`.\\n    '\n\n    def __init__(self, keyspace=None, table=None):\n        if table:\n            message = \"Table '%s.%s' already exists\" % (keyspace, table)\n        else:\n            message = \"Keyspace '%s' already exists\" % (keyspace,)\n        Exception.__init__(self, message)\n        self.keyspace = keyspace\n        self.table = table\n\nclass InvalidRequest(RequestValidationException):\n    \"\"\"\n    A query was made that was invalid for some reason, such as trying to set\n    the keyspace for a connection to a nonexistent keyspace.\n    \"\"\"\n    pass\n\nclass Unauthorized(RequestValidationException):\n    \"\"\"\n    The current user is not authorized to perform the requested operation.\n    \"\"\"\n    pass\n\nclass AuthenticationFailed(DriverException):\n    \"\"\"\n    Failed to authenticate.\n    \"\"\"\n    pass\n\nclass OperationTimedOut(DriverException):\n    \"\"\"\n    The operation took longer than the specified (client-side) timeout\n    to complete.  This is not an error generated by Cassandra, only\n    the driver.\n    \"\"\"\n    errors = None\n    '\\n    A dict of errors keyed by the :class:`~.Host` against which they occurred.\\n    '\n    last_host = None\n    '\\n    The last :class:`~.Host` this operation was attempted against.\\n    '\n\n    def __init__(self, errors=None, last_host=None):\n        self.errors = errors\n        self.last_host = last_host\n        message = 'errors=%s, last_host=%s' % (self.errors, self.last_host)\n        Exception.__init__(self, message)\n\nclass UnsupportedOperation(DriverException):\n    \"\"\"\n    An attempt was made to use a feature that is not supported by the\n    selected protocol version.  See :attr:`Cluster.protocol_version`\n    for more details.\n    \"\"\"\n    pass\n\nclass UnresolvableContactPoints(DriverException):\n    \"\"\"\n    The driver was unable to resolve any provided hostnames.\n\n    Note that this is *not* raised when a :class:`.Cluster` is created with no\n    contact points, only when lookup fails for all hosts\n    \"\"\"\n    pass\n\nclass OperationType(Enum):\n    Read = 0\n    Write = 1\n\nclass RateLimitReached(ConfigurationException):\n    \"\"\"\n    Rate limit was exceeded for a partition affected by the request.\n    \"\"\"\n    op_type = None\n    rejected_by_coordinator = False\n\n    def __init__(self, op_type=None, rejected_by_coordinator=False):\n        self.op_type = op_type\n        self.rejected_by_coordinator = rejected_by_coordinator\n        message = f'[request_error_rate_limit_reached OpType={op_type.name} RejectedByCoordinator={rejected_by_coordinator}]'\n        Exception.__init__(self, message)"
  }
}