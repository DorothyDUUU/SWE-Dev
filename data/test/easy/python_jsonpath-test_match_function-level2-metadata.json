{
  "dir_path": "/app/python_jsonpath",
  "package_name": "python_jsonpath",
  "sample_name": "python_jsonpath-test_match_function",
  "src_dir": "jsonpath/",
  "test_dir": "tests/",
  "test_file": "tests/test_match_function.py",
  "test_code": "import asyncio\nimport dataclasses\nimport operator\nfrom typing import Any\nfrom typing import List\nfrom typing import Mapping\nfrom typing import Sequence\nfrom typing import Union\n\nimport pytest\n\nfrom jsonpath import JSONPathEnvironment\n\n\n@dataclasses.dataclass\nclass Case:\n    description: str\n    path: str\n    data: Union[Sequence[Any], Mapping[str, Any]]\n    want: Union[Sequence[Any], Mapping[str, Any]]\n\n\nTEST_CASES = [\n    Case(\n        description=\"match a regex\",\n        path=\"$.some[?match(@.thing, 'fo[a-z]')]\",\n        data={\"some\": [{\"thing\": \"foo\"}]},\n        want=[{\"thing\": \"foo\"}],\n    ),\n    Case(\n        description=\"regex with no match\",\n        path=\"$.some[?match(@.thing, 'fo[a-z]')]\",\n        data={\"some\": [{\"thing\": \"foO\"}]},\n        want=[],\n    ),\n]\n\n\n@pytest.fixture()\ndef env() -> JSONPathEnvironment:\n    return JSONPathEnvironment()\n\n\n@pytest.mark.parametrize(\"case\", TEST_CASES, ids=operator.attrgetter(\"description\"))\ndef test_match_function(env: JSONPathEnvironment, case: Case) -> None:\n    path = env.compile(case.path)\n    assert path.findall(case.data) == case.want\n\n\n@pytest.mark.parametrize(\"case\", TEST_CASES, ids=operator.attrgetter(\"description\"))\ndef test_match_function_async(env: JSONPathEnvironment, case: Case) -> None:\n    path = env.compile(case.path)\n\n    async def coro() -> List[object]:\n        return await path.findall_async(case.data)\n\n    assert asyncio.run(coro()) == case.want\n\n\n# TODO: test error conditions\n",
  "GT_file_code": {
    "jsonpath/env.py": "\"\"\"Core JSONPath configuration object.\"\"\"\n\nfrom __future__ import annotations\n\nimport re\nfrom decimal import Decimal\nfrom operator import getitem\nfrom typing import TYPE_CHECKING\nfrom typing import Any\nfrom typing import AsyncIterable\nfrom typing import Callable\nfrom typing import Dict\nfrom typing import Iterable\nfrom typing import List\nfrom typing import Mapping\nfrom typing import Optional\nfrom typing import Sequence\nfrom typing import Type\nfrom typing import Union\n\nfrom . import function_extensions\nfrom .exceptions import JSONPathNameError\nfrom .exceptions import JSONPathSyntaxError\nfrom .exceptions import JSONPathTypeError\nfrom .filter import UNDEFINED\nfrom .filter import VALUE_TYPE_EXPRESSIONS\nfrom .filter import FilterExpression\nfrom .filter import FunctionExtension\nfrom .filter import InfixExpression\nfrom .filter import Path\nfrom .fluent_api import Query\nfrom .function_extensions import ExpressionType\nfrom .function_extensions import FilterFunction\nfrom .function_extensions import validate\nfrom .lex import Lexer\nfrom .match import JSONPathMatch\nfrom .match import NodeList\nfrom .parse import Parser\nfrom .path import CompoundJSONPath\nfrom .path import JSONPath\nfrom .stream import TokenStream\nfrom .token import TOKEN_EOF\nfrom .token import TOKEN_FAKE_ROOT\nfrom .token import TOKEN_INTERSECTION\nfrom .token import TOKEN_UNION\nfrom .token import Token\n\nif TYPE_CHECKING:\n    from io import IOBase\n\n    from .match import FilterContextVars\n\n\nclass JSONPathEnvironment:\n    \"\"\"JSONPath configuration.\n\n    This class contains settings for path tokenization, parsing and resolution\n    behavior, plus convenience methods for matching an unparsed path to some\n    data.\n\n    Most applications will want to create a single `JSONPathEnvironment`, or\n    use `jsonpath.compile()`, `jsonpath.findall()`, etc. from the package-level\n    default environment.\n\n    ## Environment customization\n\n    Environment customization is achieved by subclassing `JSONPathEnvironment`\n    and overriding class attributes and/or methods. Some of these\n    customizations include:\n\n    - Changing the root (`$`), self (`@`) or filter context (`_`) token with\n      class attributes `root_token`, `self_token` and `filter_context_token`.\n    - Registering a custom lexer or parser with the class attributes\n      `lexer_class` or `parser_class`. `lexer_class` must be a subclass of\n      [`Lexer`]() and `parser_class` must be a subclass of [`Parser`]().\n    - Setup built-in function extensions by overriding\n      `setup_function_extensions()`\n    - Hook in to mapping and sequence item getting by overriding `getitem()`.\n    - Change filter comparison operator behavior by overriding `compare()`.\n\n    Arguments:\n        filter_caching (bool): If `True`, filter expressions will be cached\n            where possible.\n        unicode_escape: If `True`, decode UTF-16 escape sequences found in\n            JSONPath string literals.\n        well_typed: Control well-typedness checks on filter function expressions.\n            If `True` (the default), JSONPath expressions are checked for\n            well-typedness as compile time.\n\n            **New in version 0.10.0**\n\n    ## Class attributes\n\n    Attributes:\n        fake_root_token (str): The pattern used to select a \"fake\" root node, one level\n            above the real root node.\n        filter_context_token (str): The pattern used to select extra filter context\n            data. Defaults to `\"_\"`.\n        intersection_token (str): The pattern used as the intersection operator.\n            Defaults to `\"&\"`.\n        key_token (str): The pattern used to identify the current key or index when\n            filtering a, mapping or sequence. Defaults to `\"#\"`.\n        keys_selector_token (str): The pattern used as the \"keys\" selector. Defaults to\n            `\"~\"`.\n        lexer_class: The lexer to use when tokenizing path strings.\n        max_int_index (int): The maximum integer allowed when selecting array items by\n            index. Defaults to `(2**53) - 1`.\n        min_int_index (int): The minimum integer allowed when selecting array items by\n            index. Defaults to `-(2**53) + 1`.\n        parser_class: The parser to use when parsing tokens from the lexer.\n        root_token (str): The pattern used to select the root node in a JSON document.\n            Defaults to `\"$\"`.\n        self_token (str): The pattern used to select the current node in a JSON\n            document. Defaults to `\"@\"`\n        union_token (str): The pattern used as the union operator. Defaults to `\"|\"`.\n    \"\"\"\n\n    # These should be unescaped strings. `re.escape` will be called\n    # on them automatically when compiling lexer rules.\n    fake_root_token = \"^\"\n    filter_context_token = \"_\"\n    intersection_token = \"&\"\n    key_token = \"#\"\n    keys_selector_token = \"~\"\n    root_token = \"$\"\n    self_token = \"@\"\n    union_token = \"|\"\n\n    max_int_index = (2**53) - 1\n    min_int_index = -(2**53) + 1\n\n    # Override these to customize path tokenization and parsing.\n    lexer_class: Type[Lexer] = Lexer\n    parser_class: Type[Parser] = Parser\n    match_class: Type[JSONPathMatch] = JSONPathMatch\n\n    def __init__(\n        self,\n        *,\n        filter_caching: bool = True,\n        unicode_escape: bool = True,\n        well_typed: bool = True,\n    ) -> None:\n        self.filter_caching: bool = filter_caching\n        \"\"\"Enable or disable filter expression caching.\"\"\"\n\n        self.unicode_escape: bool = unicode_escape\n        \"\"\"Enable or disable decoding of UTF-16 escape sequences found in\n        JSONPath string literals.\"\"\"\n\n        self.well_typed: bool = well_typed\n        \"\"\"Control well-typedness checks on filter function expressions.\"\"\"\n\n        self.lexer: Lexer = self.lexer_class(env=self)\n        \"\"\"The lexer bound to this environment.\"\"\"\n\n        self.parser: Parser = self.parser_class(env=self)\n        \"\"\"The parser bound to this environment.\"\"\"\n\n        self.function_extensions: Dict[str, Callable[..., Any]] = {}\n        \"\"\"A list of function extensions available to filters.\"\"\"\n\n        self.setup_function_extensions()\n\n    def compile(self, path: str) -> Union[JSONPath, CompoundJSONPath]:  # noqa: A003\n        \"\"\"Prepare a path string ready for repeated matching against different data.\n\n        Arguments:\n            path: A JSONPath as a string.\n\n        Returns:\n            A `JSONPath` or `CompoundJSONPath`, ready to match against some data.\n                Expect a `CompoundJSONPath` if the path string uses the _union_ or\n                _intersection_ operators.\n\n        Raises:\n            JSONPathSyntaxError: If _path_ is invalid.\n            JSONPathTypeError: If filter functions are given arguments of an\n                unacceptable type.\n        \"\"\"\n        tokens = self.lexer.tokenize(path)\n        stream = TokenStream(tokens)\n        fake_root = stream.current.kind == TOKEN_FAKE_ROOT\n        _path: Union[JSONPath, CompoundJSONPath] = JSONPath(\n            env=self, selectors=self.parser.parse(stream), fake_root=fake_root\n        )\n\n        if stream.current.kind != TOKEN_EOF:\n            _path = CompoundJSONPath(env=self, path=_path)\n            while stream.current.kind != TOKEN_EOF:\n                if stream.peek.kind == TOKEN_EOF:\n                    # trailing union or intersection\n                    raise JSONPathSyntaxError(\n                        f\"expected a path after {stream.current.value!r}\",\n                        token=stream.current,\n                    )\n\n                if stream.current.kind == TOKEN_UNION:\n                    stream.next_token()\n                    fake_root = stream.current.kind == TOKEN_FAKE_ROOT\n                    _path = _path.union(\n                        JSONPath(\n                            env=self,\n                            selectors=self.parser.parse(stream),\n                            fake_root=fake_root,\n                        )\n                    )\n                elif stream.current.kind == TOKEN_INTERSECTION:\n                    stream.next_token()\n                    fake_root = stream.current.kind == TOKEN_FAKE_ROOT\n                    _path = _path.intersection(\n                        JSONPath(\n                            env=self,\n                            selectors=self.parser.parse(stream),\n                            fake_root=fake_root,\n                        )\n                    )\n                else:  # pragma: no cover\n                    # Parser.parse catches this too\n                    raise JSONPathSyntaxError(  # noqa: TRY003\n                        f\"unexpected token {stream.current.value!r}\",\n                        token=stream.current,\n                    )\n\n        return _path\n\n    def findall(\n        self,\n        path: str,\n        data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]],\n        *,\n        filter_context: Optional[FilterContextVars] = None,\n    ) -> List[object]:\n        \"\"\"Find all objects in _data_ matching the JSONPath _path_.\n\n        If _data_ is a string or a file-like objects, it will be loaded\n        using `json.loads()` and the default `JSONDecoder`.\n\n        Arguments:\n            path: The JSONPath as a string.\n            data: A JSON document or Python object implementing the `Sequence`\n                or `Mapping` interfaces.\n            filter_context: Arbitrary data made available to filters using\n                the _filter context_ selector.\n\n        Returns:\n            A list of matched objects. If there are no matches, the list will\n                be empty.\n\n        Raises:\n            JSONPathSyntaxError: If the path is invalid.\n            JSONPathTypeError: If a filter expression attempts to use types in\n                an incompatible way.\n        \"\"\"\n        return self.compile(path).findall(data, filter_context=filter_context)\n\n    def finditer(\n        self,\n        path: str,\n        data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]],\n        *,\n        filter_context: Optional[FilterContextVars] = None,\n    ) -> Iterable[JSONPathMatch]:\n        \"\"\"Generate `JSONPathMatch` objects for each match of _path_ in _data_.\n\n        If _data_ is a string or a file-like objects, it will be loaded using\n        `json.loads()` and the default `JSONDecoder`.\n\n        Arguments:\n            path: The JSONPath as a string.\n            data: A JSON document or Python object implementing the `Sequence`\n                or `Mapping` interfaces.\n            filter_context: Arbitrary data made available to filters using\n                the _filter context_ selector.\n\n        Returns:\n            An iterator yielding `JSONPathMatch` objects for each match.\n\n        Raises:\n            JSONPathSyntaxError: If the path is invalid.\n            JSONPathTypeError: If a filter expression attempts to use types in\n                an incompatible way.\n        \"\"\"\n        return self.compile(path).finditer(data, filter_context=filter_context)\n\n    def match(\n        self,\n        path: str,\n        data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]],\n        *,\n        filter_context: Optional[FilterContextVars] = None,\n    ) -> Union[JSONPathMatch, None]:\n        \"\"\"Return a `JSONPathMatch` instance for the first object found in _data_.\n\n        `None` is returned if there are no matches.\n\n        Arguments:\n            path: The JSONPath as a string.\n            data: A JSON document or Python object implementing the `Sequence`\n                or `Mapping` interfaces.\n            filter_context: Arbitrary data made available to filters using\n                the _filter context_ selector.\n\n        Returns:\n            A `JSONPathMatch` object for the first match, or `None` if there were\n                no matches.\n\n        Raises:\n            JSONPathSyntaxError: If the path is invalid.\n            JSONPathTypeError: If a filter expression attempts to use types in\n                an incompatible way.\n        \"\"\"\n        return self.compile(path).match(data, filter_context=filter_context)\n\n    def query(\n        self,\n        path: str,\n        data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]],\n        filter_context: Optional[FilterContextVars] = None,\n    ) -> Query:\n        \"\"\"Return a `Query` iterator over matches found by applying _path_ to _data_.\n\n        `Query` objects are iterable.\n\n        ```\n        for match in jsonpath.query(\"$.foo..bar\", data):\n            ...\n        ```\n\n        You can skip and limit results with `Query.skip()` and `Query.limit()`.\n\n        ```\n        matches = (\n            jsonpath.query(\"$.foo..bar\", data)\n            .skip(5)\n            .limit(10)\n        )\n\n        for match in matches\n            ...\n        ```\n\n        `Query.tail()` will get the last _n_ results.\n\n        ```\n        for match in jsonpath.query(\"$.foo..bar\", data).tail(5):\n            ...\n        ```\n\n        Get values for each match using `Query.values()`.\n\n        ```\n        for obj in jsonpath.query(\"$.foo..bar\", data).limit(5).values():\n            ...\n        ```\n\n        Arguments:\n            path: The JSONPath as a string.\n            data: A JSON document or Python object implementing the `Sequence`\n                or `Mapping` interfaces.\n            filter_context: Arbitrary data made available to filters using\n                the _filter context_ selector.\n\n        Returns:\n            A query iterator.\n\n        Raises:\n            JSONPathSyntaxError: If the path is invalid.\n            JSONPathTypeError: If a filter expression attempts to use types in\n                an incompatible way.\n        \"\"\"\n        return Query(self.finditer(path, data, filter_context=filter_context), self)\n\n    async def findall_async(\n        self,\n        path: str,\n        data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]],\n        *,\n        filter_context: Optional[FilterContextVars] = None,\n    ) -> List[object]:\n        \"\"\"An async version of `findall()`.\"\"\"\n        return await self.compile(path).findall_async(\n            data, filter_context=filter_context\n        )\n\n    async def finditer_async(\n        self,\n        path: str,\n        data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]],\n        *,\n        filter_context: Optional[FilterContextVars] = None,\n    ) -> AsyncIterable[JSONPathMatch]:\n        \"\"\"An async version of `finditer()`.\"\"\"\n        return await self.compile(path).finditer_async(\n            data, filter_context=filter_context\n        )\n\n    def setup_function_extensions(self) -> None:\n        \"\"\"Initialize function extensions.\"\"\"\n        self.function_extensions[\"length\"] = function_extensions.Length()\n        self.function_extensions[\"count\"] = function_extensions.Count()\n        self.function_extensions[\"match\"] = function_extensions.Match()\n        self.function_extensions[\"search\"] = function_extensions.Search()\n        self.function_extensions[\"value\"] = function_extensions.Value()\n        self.function_extensions[\"isinstance\"] = function_extensions.IsInstance()\n        self.function_extensions[\"is\"] = self.function_extensions[\"isinstance\"]\n        self.function_extensions[\"typeof\"] = function_extensions.TypeOf()\n        self.function_extensions[\"type\"] = self.function_extensions[\"typeof\"]\n\n    def validate_function_extension_signature(\n        self, token: Token, args: List[Any]\n    ) -> List[Any]:\n        \"\"\"Compile-time validation of function extension arguments.\n\n        RFC 9535 requires us to reject paths that use filter functions with\n        too many or too few arguments.\n        \"\"\"\n        try:\n            func = self.function_extensions[token.value]\n        except KeyError as err:\n            raise JSONPathNameError(\n                f\"function {token.value!r} is not defined\", token=token\n            ) from err\n\n        # Type-aware function extensions use the spec's type system.\n        if self.well_typed and isinstance(func, FilterFunction):\n            self.check_well_typedness(token, func, args)\n            return args\n\n        # A callable with a `validate` method?\n        if hasattr(func, \"validate\"):\n            args = func.validate(self, args, token)\n            assert isinstance(args, list)\n            return args\n\n        # Generic validation using introspection.\n        return validate(self, func, args, token)\n\n    def check_well_typedness(\n        self,\n        token: Token,\n        func: FilterFunction,\n        args: List[FilterExpression],\n    ) -> None:\n        \"\"\"Check the well-typedness of a function's arguments at compile-time.\"\"\"\n        # Correct number of arguments?\n        if len(args) != len(func.arg_types):\n            raise JSONPathTypeError(\n                f\"{token.value!r}() requires {len(func.arg_types)} arguments\",\n                token=token,\n            )\n\n        # Argument types\n        for idx, typ in enumerate(func.arg_types):\n            arg = args[idx]\n            if typ == ExpressionType.VALUE:\n                if not (\n                    isinstance(arg, VALUE_TYPE_EXPRESSIONS)\n                    or (isinstance(arg, Path) and arg.path.singular_query())\n                    or (self._function_return_type(arg) == ExpressionType.VALUE)\n                ):\n                    raise JSONPathTypeError(\n                        f\"{token.value}() argument {idx} must be of ValueType\",\n                        token=token,\n                    )\n            elif typ == ExpressionType.LOGICAL:\n                if not isinstance(arg, (Path, InfixExpression)):\n                    raise JSONPathTypeError(\n                        f\"{token.value}() argument {idx} must be of LogicalType\",\n                        token=token,\n                    )\n            elif typ == ExpressionType.NODES and not (\n                isinstance(arg, Path)\n                or self._function_return_type(arg) == ExpressionType.NODES\n            ):\n                raise JSONPathTypeError(\n                    f\"{token.value}() argument {idx} must be of NodesType\",\n                    token=token,\n                )\n\n    def _function_return_type(self, expr: FilterExpression) -> Optional[ExpressionType]:\n        \"\"\"Return the type returned from a filter function.\n\n        If _expr_ is not a `FunctionExtension` or the registered function definition is\n        not type-aware, return `None`.\n        \"\"\"\n        if not isinstance(expr, FunctionExtension):\n            return None\n        func = self.function_extensions.get(expr.name)\n        if isinstance(func, FilterFunction):\n            return func.return_type\n        return None\n\n    def getitem(self, obj: Any, key: Any) -> Any:\n        \"\"\"Sequence and mapping item getter used throughout JSONPath resolution.\n\n        The default implementation of `getitem` simply calls `operators.getitem()`\n        from Python's standard library. Same as `obj[key]`.\n\n        Arguments:\n            obj: A mapping or sequence that might contain _key_.\n            key: A mapping key, sequence index or sequence slice.\n        \"\"\"\n        return getitem(obj, key)\n\n    async def getitem_async(self, obj: Any, key: object) -> Any:\n        \"\"\"An async sequence and mapping item getter.\"\"\"\n        if hasattr(obj, \"__getitem_async__\"):\n            return await obj.__getitem_async__(key)\n        return getitem(obj, key)\n\n    def is_truthy(self, obj: object) -> bool:\n        \"\"\"Test for truthiness when evaluating JSONPath filter expressions.\n\n        In some cases, RFC 9535 requires us to test for existence rather than\n        truthiness. So the default implementation returns `True` for empty\n        collections and `None`. The special `UNDEFINED` object means that\n        _obj_ was missing, as opposed to an explicit `None`.\n\n        Arguments:\n            obj: Any object.\n\n        Returns:\n            `True` if the object exists and is not `False` or `0`.\n        \"\"\"\n        if isinstance(obj, NodeList) and len(obj) == 0:\n            return False\n        if obj is UNDEFINED:\n            return False\n        if obj is None:\n            return True\n        return bool(obj)\n\n    def compare(  # noqa: PLR0911\n        self, left: object, operator: str, right: object\n    ) -> bool:\n        \"\"\"Object comparison within JSONPath filters.\n\n        Override this to customize filter expression comparison operator\n        behavior.\n\n        Args:\n            left: The left hand side of the comparison expression.\n            operator: The comparison expression's operator.\n            right: The right hand side of the comparison expression.\n\n        Returns:\n            `True` if the comparison between _left_ and _right_, with the\n            given _operator_, is truthy. `False` otherwise.\n        \"\"\"\n        if operator == \"&&\":\n            return self.is_truthy(left) and self.is_truthy(right)\n        if operator == \"||\":\n            return self.is_truthy(left) or self.is_truthy(right)\n        if operator == \"==\":\n            return self._eq(left, right)\n        if operator == \"!=\":\n            return not self._eq(left, right)\n        if operator == \"<\":\n            return self._lt(left, right)\n        if operator == \">\":\n            return self._lt(right, left)\n        if operator == \">=\":\n            return self._lt(right, left) or self._eq(left, right)\n        if operator == \"<=\":\n            return self._lt(left, right) or self._eq(left, right)\n        if operator == \"in\" and isinstance(right, (Mapping, Sequence)):\n            return left in right\n        if operator == \"contains\" and isinstance(left, (Mapping, Sequence)):\n            return right in left\n        if operator == \"=~\" and isinstance(right, re.Pattern) and isinstance(left, str):\n            return bool(right.fullmatch(left))\n        return False\n\n    def _eq(self, left: object, right: object) -> bool:  # noqa: PLR0911\n        if isinstance(right, NodeList):\n            left, right = right, left\n\n        if isinstance(left, NodeList):\n            if isinstance(right, NodeList):\n                return left == right\n            if left.empty():\n                return right is UNDEFINED\n            if len(left) == 1:\n                return left[0] == right\n            return False\n\n        if left is UNDEFINED and right is UNDEFINED:\n            return True\n\n        # Remember 1 == True and 0 == False in Python\n        if isinstance(right, bool):\n            left, right = right, left\n\n        if isinstance(left, bool):\n            return isinstance(right, bool) and left == right\n\n        return left == right\n\n    def _lt(self, left: object, right: object) -> bool:\n        if isinstance(left, str) and isinstance(right, str):\n            return left < right\n\n        if isinstance(left, (int, float, Decimal)) and isinstance(\n            right, (int, float, Decimal)\n        ):\n            return left < right\n\n        return False\n",
    "jsonpath/path.py": "# noqa: D100\nfrom __future__ import annotations\n\nimport itertools\nfrom typing import TYPE_CHECKING\nfrom typing import Any\nfrom typing import AsyncIterable\nfrom typing import Iterable\nfrom typing import List\nfrom typing import Mapping\nfrom typing import Optional\nfrom typing import Sequence\nfrom typing import Tuple\nfrom typing import TypeVar\nfrom typing import Union\n\nfrom jsonpath._data import load_data\nfrom jsonpath.fluent_api import Query\nfrom jsonpath.match import FilterContextVars\nfrom jsonpath.match import JSONPathMatch\nfrom jsonpath.selectors import IndexSelector\nfrom jsonpath.selectors import ListSelector\nfrom jsonpath.selectors import PropertySelector\n\nif TYPE_CHECKING:\n    from io import IOBase\n\n    from .env import JSONPathEnvironment\n    from .selectors import JSONPathSelector\n\n\nclass JSONPath:\n    \"\"\"A compiled JSONPath ready to be applied to a JSON string or Python object.\n\n    Arguments:\n        env: The `JSONPathEnvironment` this path is bound to.\n        selectors: An iterable of `JSONPathSelector` objects, as generated by\n            a `Parser`.\n        fake_root: Indicates if target JSON values should be wrapped in a single-\n            element array, so as to make the target root value selectable.\n\n\n    Attributes:\n        env: The `JSONPathEnvironment` this path is bound to.\n        selectors: The `JSONPathSelector` instances that make up this path.\n    \"\"\"\n\n    __slots__ = (\"env\", \"fake_root\", \"selectors\")\n\n    def __init__(\n        self,\n        *,\n        env: JSONPathEnvironment,\n        selectors: Iterable[JSONPathSelector],\n        fake_root: bool = False,\n    ) -> None:\n        self.env = env\n        self.selectors = tuple(selectors)\n        self.fake_root = fake_root\n\n    def __str__(self) -> str:\n        return self.env.root_token + \"\".join(\n            str(selector) for selector in self.selectors\n        )\n\n    def __eq__(self, __value: object) -> bool:\n        return isinstance(__value, JSONPath) and self.selectors == __value.selectors\n\n    def __hash__(self) -> int:\n        return hash(self.selectors)\n\n    def findall(\n        self,\n        data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]],\n        *,\n        filter_context: Optional[FilterContextVars] = None,\n    ) -> List[object]:\n        \"\"\"Find all objects in `data` matching the given JSONPath `path`.\n\n        If `data` is a string or a file-like objects, it will be loaded\n        using `json.loads()` and the default `JSONDecoder`.\n\n        Arguments:\n            data: A JSON document or Python object implementing the `Sequence`\n                or `Mapping` interfaces.\n            filter_context: Arbitrary data made available to filters using\n                the _filter context_ selector.\n\n        Returns:\n            A list of matched objects. If there are no matches, the list will\n            be empty.\n\n        Raises:\n            JSONPathSyntaxError: If the path is invalid.\n            JSONPathTypeError: If a filter expression attempts to use types in\n                an incompatible way.\n        \"\"\"\n        return [\n            match.obj for match in self.finditer(data, filter_context=filter_context)\n        ]\n\n    def finditer(\n        self,\n        data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]],\n        *,\n        filter_context: Optional[FilterContextVars] = None,\n    ) -> Iterable[JSONPathMatch]:\n        \"\"\"Generate `JSONPathMatch` objects for each match.\n\n        If `data` is a string or a file-like objects, it will be loaded\n        using `json.loads()` and the default `JSONDecoder`.\n\n        Arguments:\n            data: A JSON document or Python object implementing the `Sequence`\n                or `Mapping` interfaces.\n            filter_context: Arbitrary data made available to filters using\n                the _filter context_ selector.\n\n        Returns:\n            An iterator yielding `JSONPathMatch` objects for each match.\n\n        Raises:\n            JSONPathSyntaxError: If the path is invalid.\n            JSONPathTypeError: If a filter expression attempts to use types in\n                an incompatible way.\n        \"\"\"\n        _data = load_data(data)\n        matches: Iterable[JSONPathMatch] = [\n            JSONPathMatch(\n                filter_context=filter_context or {},\n                obj=[_data] if self.fake_root else _data,\n                parent=None,\n                path=self.env.root_token,\n                parts=(),\n                root=_data,\n            )\n        ]\n\n        for selector in self.selectors:\n            matches = selector.resolve(matches)\n\n        return matches\n\n    async def findall_async(\n        self,\n        data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]],\n        *,\n        filter_context: Optional[FilterContextVars] = None,\n    ) -> List[object]:\n        \"\"\"An async version of `findall()`.\"\"\"\n        return [\n            match.obj\n            async for match in await self.finditer_async(\n                data, filter_context=filter_context\n            )\n        ]\n\n    async def finditer_async(\n        self,\n        data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]],\n        *,\n        filter_context: Optional[FilterContextVars] = None,\n    ) -> AsyncIterable[JSONPathMatch]:\n        \"\"\"An async version of `finditer()`.\"\"\"\n        _data = load_data(data)\n\n        async def root_iter() -> AsyncIterable[JSONPathMatch]:\n            yield self.env.match_class(\n                filter_context=filter_context or {},\n                obj=[_data] if self.fake_root else _data,\n                parent=None,\n                path=self.env.root_token,\n                parts=(),\n                root=_data,\n            )\n\n        matches: AsyncIterable[JSONPathMatch] = root_iter()\n\n        for selector in self.selectors:\n            matches = selector.resolve_async(matches)\n\n        return matches\n\n    def match(\n        self,\n        data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]],\n        *,\n        filter_context: Optional[FilterContextVars] = None,\n    ) -> Union[JSONPathMatch, None]:\n        \"\"\"Return a `JSONPathMatch` instance for the first object found in _data_.\n\n        `None` is returned if there are no matches.\n\n        Arguments:\n            data: A JSON document or Python object implementing the `Sequence`\n                or `Mapping` interfaces.\n            filter_context: Arbitrary data made available to filters using\n                the _filter context_ selector.\n\n        Returns:\n            A `JSONPathMatch` object for the first match, or `None` if there were\n                no matches.\n\n        Raises:\n            JSONPathSyntaxError: If the path is invalid.\n            JSONPathTypeError: If a filter expression attempts to use types in\n                an incompatible way.\n        \"\"\"\n        try:\n            return next(iter(self.finditer(data, filter_context=filter_context)))\n        except StopIteration:\n            return None\n\n    def query(\n        self,\n        data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]],\n        *,\n        filter_context: Optional[FilterContextVars] = None,\n    ) -> Query:\n        \"\"\"Return a `Query` iterator over matches found by applying this path to _data_.\n\n        Arguments:\n            data: A JSON document or Python object implementing the `Sequence`\n                or `Mapping` interfaces.\n            filter_context: Arbitrary data made available to filters using\n                the _filter context_ selector.\n\n        Returns:\n            A query iterator.\n\n        Raises:\n            JSONPathSyntaxError: If the path is invalid.\n            JSONPathTypeError: If a filter expression attempts to use types in\n                an incompatible way.\n        \"\"\"\n        return Query(self.finditer(data, filter_context=filter_context), self.env)\n\n    def empty(self) -> bool:\n        \"\"\"Return `True` if this path has no selectors.\"\"\"\n        return not bool(self.selectors)\n\n    def singular_query(self) -> bool:\n        \"\"\"Return `True` if this JSONPath query is a singular query.\"\"\"\n        for selector in self.selectors:\n            if isinstance(selector, (PropertySelector, IndexSelector)):\n                continue\n            if (\n                isinstance(selector, ListSelector)\n                and len(selector.items) == 1\n                and isinstance(selector.items[0], (PropertySelector, IndexSelector))\n            ):\n                continue\n            return False\n        return True\n\n\nclass CompoundJSONPath:\n    \"\"\"Multiple `JSONPath`s combined.\"\"\"\n\n    __slots__ = (\"env\", \"path\", \"paths\")\n\n    def __init__(\n        self,\n        *,\n        env: JSONPathEnvironment,\n        path: Union[JSONPath, CompoundJSONPath],\n        paths: Iterable[Tuple[str, JSONPath]] = (),\n    ) -> None:\n        self.env = env\n        self.path = path\n        self.paths = tuple(paths)\n\n    def __str__(self) -> str:\n        buf: List[str] = [str(self.path)]\n        for op, path in self.paths:\n            buf.append(f\" {op} \")\n            buf.append(str(path))\n        return \"\".join(buf)\n\n    def __eq__(self, __value: object) -> bool:\n        return (\n            isinstance(__value, CompoundJSONPath)\n            and self.path == __value.path\n            and self.paths == __value.paths\n        )\n\n    def __hash__(self) -> int:\n        return hash((self.path, self.paths))\n\n    def findall(\n        self,\n        data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]],\n        *,\n        filter_context: Optional[FilterContextVars] = None,\n    ) -> List[object]:\n        \"\"\"Find all objects in `data` matching the given JSONPath `path`.\n\n        If `data` is a string or a file-like objects, it will be loaded\n        using `json.loads()` and the default `JSONDecoder`.\n\n        Arguments:\n            data: A JSON document or Python object implementing the `Sequence`\n                or `Mapping` interfaces.\n            filter_context: Arbitrary data made available to filters using\n                the _filter context_ selector.\n\n        Returns:\n            A list of matched objects. If there are no matches, the list will\n                be empty.\n\n        Raises:\n            JSONPathSyntaxError: If the path is invalid.\n            JSONPathTypeError: If a filter expression attempts to use types in\n                an incompatible way.\n        \"\"\"\n        objs = self.path.findall(data, filter_context=filter_context)\n\n        for op, path in self.paths:\n            _objs = path.findall(data, filter_context=filter_context)\n            if op == self.env.union_token:\n                objs.extend(_objs)\n            else:\n                assert op == self.env.intersection_token, op\n                objs = [obj for obj in objs if obj in _objs]\n\n        return objs\n\n    def finditer(\n        self,\n        data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]],\n        *,\n        filter_context: Optional[FilterContextVars] = None,\n    ) -> Iterable[JSONPathMatch]:\n        \"\"\"Generate `JSONPathMatch` objects for each match.\n\n        If `data` is a string or a file-like objects, it will be loaded\n        using `json.loads()` and the default `JSONDecoder`.\n\n        Arguments:\n            data: A JSON document or Python object implementing the `Sequence`\n                or `Mapping` interfaces.\n            filter_context: Arbitrary data made available to filters using\n                the _filter context_ selector.\n\n        Returns:\n            An iterator yielding `JSONPathMatch` objects for each match.\n\n        Raises:\n            JSONPathSyntaxError: If the path is invalid.\n            JSONPathTypeError: If a filter expression attempts to use types in\n                an incompatible way.\n        \"\"\"\n        matches = self.path.finditer(data, filter_context=filter_context)\n\n        for op, path in self.paths:\n            _matches = path.finditer(data, filter_context=filter_context)\n            if op == self.env.union_token:\n                matches = itertools.chain(matches, _matches)\n            else:\n                assert op == self.env.intersection_token\n                _objs = [match.obj for match in _matches]\n                matches = (match for match in matches if match.obj in _objs)\n\n        return matches\n\n    def match(\n        self,\n        data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]],\n        *,\n        filter_context: Optional[FilterContextVars] = None,\n    ) -> Union[JSONPathMatch, None]:\n        \"\"\"Return a `JSONPathMatch` instance for the first object found in _data_.\n\n        `None` is returned if there are no matches.\n\n        Arguments:\n            data: A JSON document or Python object implementing the `Sequence`\n                or `Mapping` interfaces.\n            filter_context: Arbitrary data made available to filters using\n                the _filter context_ selector.\n\n        Returns:\n            A `JSONPathMatch` object for the first match, or `None` if there were\n                no matches.\n\n        Raises:\n            JSONPathSyntaxError: If the path is invalid.\n            JSONPathTypeError: If a filter expression attempts to use types in\n                an incompatible way.\n        \"\"\"\n        try:\n            return next(iter(self.finditer(data, filter_context=filter_context)))\n        except StopIteration:\n            return None\n\n    async def findall_async(\n        self,\n        data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]],\n        *,\n        filter_context: Optional[FilterContextVars] = None,\n    ) -> List[object]:\n        \"\"\"An async version of `findall()`.\"\"\"\n        objs = await self.path.findall_async(data, filter_context=filter_context)\n\n        for op, path in self.paths:\n            _objs = await path.findall_async(data, filter_context=filter_context)\n            if op == self.env.union_token:\n                objs.extend(_objs)\n            else:\n                assert op == self.env.intersection_token\n                objs = [obj for obj in objs if obj in _objs]\n\n        return objs\n\n    async def finditer_async(\n        self,\n        data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]],\n        *,\n        filter_context: Optional[FilterContextVars] = None,\n    ) -> AsyncIterable[JSONPathMatch]:\n        \"\"\"An async version of `finditer()`.\"\"\"\n        matches = await self.path.finditer_async(data, filter_context=filter_context)\n\n        for op, path in self.paths:\n            _matches = await path.finditer_async(data, filter_context=filter_context)\n            if op == self.env.union_token:\n                matches = _achain(matches, _matches)\n            else:\n                assert op == self.env.intersection_token\n                _objs = [match.obj async for match in _matches]\n                matches = (match async for match in matches if match.obj in _objs)\n\n        return matches\n\n    def query(\n        self,\n        data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]],\n        *,\n        filter_context: Optional[FilterContextVars] = None,\n    ) -> Query:\n        \"\"\"Return a `Query` iterator over matches found by applying this path to _data_.\n\n        Arguments:\n            data: A JSON document or Python object implementing the `Sequence`\n                or `Mapping` interfaces.\n            filter_context: Arbitrary data made available to filters using\n                the _filter context_ selector.\n\n        Returns:\n            A query iterator.\n\n        Raises:\n            JSONPathSyntaxError: If the path is invalid.\n            JSONPathTypeError: If a filter expression attempts to use types in\n                an incompatible way.\n        \"\"\"\n        return Query(self.finditer(data, filter_context=filter_context), self.env)\n\n    def union(self, path: JSONPath) -> CompoundJSONPath:\n        \"\"\"Union of this path and another path.\"\"\"\n        return self.__class__(\n            env=self.env,\n            path=self.path,\n            paths=self.paths + ((self.env.union_token, path),),\n        )\n\n    def intersection(self, path: JSONPath) -> CompoundJSONPath:\n        \"\"\"Intersection of this path and another path.\"\"\"\n        return self.__class__(\n            env=self.env,\n            path=self.path,\n            paths=self.paths + ((self.env.intersection_token, path),),\n        )\n\n\nT = TypeVar(\"T\")\n\n\nasync def _achain(*iterables: AsyncIterable[T]) -> AsyncIterable[T]:\n    for it in iterables:\n        async for element in it:\n            yield element\n",
    "jsonpath/filter.py": "\"\"\"Filter expression nodes.\"\"\"\n\nfrom __future__ import annotations\n\nimport copy\nimport json\nimport re\nfrom abc import ABC\nfrom abc import abstractmethod\nfrom typing import TYPE_CHECKING\nfrom typing import Any\nfrom typing import Callable\nfrom typing import Generic\nfrom typing import Iterable\nfrom typing import List\nfrom typing import Mapping\nfrom typing import Pattern\nfrom typing import Sequence\nfrom typing import TypeVar\n\nfrom jsonpath.function_extensions.filter_function import ExpressionType\n\nfrom .exceptions import JSONPathTypeError\nfrom .function_extensions import FilterFunction\nfrom .match import NodeList\nfrom .selectors import Filter as FilterSelector\nfrom .selectors import ListSelector\n\nif TYPE_CHECKING:\n    from .path import JSONPath\n    from .selectors import FilterContext\n\n# ruff: noqa: D102\n\n\nclass FilterExpression(ABC):\n    \"\"\"Base class for all filter expression nodes.\"\"\"\n\n    __slots__ = (\"volatile\",)\n\n    FORCE_CACHE = False\n\n    def __init__(self) -> None:\n        self.volatile: bool = any(child.volatile for child in self.children())\n\n    @abstractmethod\n    def evaluate(self, context: FilterContext) -> object:\n        \"\"\"Resolve the filter expression in the given _context_.\n\n        Arguments:\n            context: Contextual information the expression might choose\n                use during evaluation.\n\n        Returns:\n            The result of evaluating the expression.\n        \"\"\"\n\n    @abstractmethod\n    async def evaluate_async(self, context: FilterContext) -> object:\n        \"\"\"An async version of `evaluate`.\"\"\"\n\n    @abstractmethod\n    def children(self) -> List[FilterExpression]:\n        \"\"\"Return a list of direct child expressions.\"\"\"\n\n    @abstractmethod\n    def set_children(self, children: List[FilterExpression]) -> None:  # noqa: ARG002\n        \"\"\"Update this expression's child expressions.\n\n        _children_ is assumed to have the same number of items as is returned\n        by _self.children_, and in the same order.\n        \"\"\"\n\n\nclass Nil(FilterExpression):\n    \"\"\"The constant `nil`.\n\n    Also aliased as `null` and `None`, sometimes.\n    \"\"\"\n\n    __slots__ = ()\n\n    def __eq__(self, other: object) -> bool:\n        return other is None or isinstance(other, Nil)\n\n    def __repr__(self) -> str:  # pragma: no cover\n        return \"NIL()\"\n\n    def __str__(self) -> str:  # pragma: no cover\n        return \"nil\"\n\n    def evaluate(self, _: FilterContext) -> None:\n        return None\n\n    async def evaluate_async(self, _: FilterContext) -> None:\n        return None\n\n    def children(self) -> List[FilterExpression]:\n        return []\n\n    def set_children(self, children: List[FilterExpression]) -> None:  # noqa: ARG002\n        return\n\n\nNIL = Nil()\n\n\nclass _Undefined:\n    __slots__ = ()\n\n    def __eq__(self, other: object) -> bool:\n        return (\n            other is UNDEFINED_LITERAL\n            or other is UNDEFINED\n            or (isinstance(other, NodeList) and other.empty())\n        )\n\n    def __str__(self) -> str:\n        return \"<UNDEFINED>\"\n\n    def __repr__(self) -> str:\n        return \"<UNDEFINED>\"\n\n\n# This is equivalent to the spec's special `Nothing` value.\nUNDEFINED = _Undefined()\n\n\nclass Undefined(FilterExpression):\n    \"\"\"The constant `undefined`.\"\"\"\n\n    __slots__ = ()\n\n    def __eq__(self, other: object) -> bool:\n        return (\n            isinstance(other, Undefined)\n            or other is UNDEFINED\n            or (isinstance(other, NodeList) and len(other) == 0)\n        )\n\n    def __str__(self) -> str:\n        return \"undefined\"\n\n    def evaluate(self, _: FilterContext) -> object:\n        return UNDEFINED\n\n    async def evaluate_async(self, _: FilterContext) -> object:\n        return UNDEFINED\n\n    def children(self) -> List[FilterExpression]:\n        return []\n\n    def set_children(self, children: List[FilterExpression]) -> None:  # noqa: ARG002\n        return\n\n\nUNDEFINED_LITERAL = Undefined()\n\nLITERAL_EXPRESSION_T = TypeVar(\"LITERAL_EXPRESSION_T\")\n\n\nclass Literal(FilterExpression, Generic[LITERAL_EXPRESSION_T]):\n    \"\"\"Base class for filter expression literals.\"\"\"\n\n    __slots__ = (\"value\",)\n\n    def __init__(self, *, value: LITERAL_EXPRESSION_T) -> None:\n        self.value = value\n        super().__init__()\n\n    def __str__(self) -> str:\n        return repr(self.value).lower()\n\n    def __eq__(self, other: object) -> bool:\n        return self.value == other\n\n    def __hash__(self) -> int:\n        return hash(self.value)\n\n    def evaluate(self, _: FilterContext) -> LITERAL_EXPRESSION_T:\n        return self.value\n\n    async def evaluate_async(self, _: FilterContext) -> LITERAL_EXPRESSION_T:\n        return self.value\n\n    def children(self) -> List[FilterExpression]:\n        return []\n\n    def set_children(self, children: List[FilterExpression]) -> None:  # noqa: ARG002\n        return\n\n\nclass BooleanLiteral(Literal[bool]):\n    \"\"\"A Boolean `True` or `False`.\"\"\"\n\n    __slots__ = ()\n\n\nTRUE = BooleanLiteral(value=True)\n\n\nFALSE = BooleanLiteral(value=False)\n\n\nclass StringLiteral(Literal[str]):\n    \"\"\"A string literal.\"\"\"\n\n    __slots__ = ()\n\n    def __str__(self) -> str:\n        return json.dumps(self.value)\n\n\nclass IntegerLiteral(Literal[int]):\n    \"\"\"An integer literal.\"\"\"\n\n    __slots__ = ()\n\n\nclass FloatLiteral(Literal[float]):\n    \"\"\"A float literal.\"\"\"\n\n    __slots__ = ()\n\n\nclass RegexLiteral(Literal[Pattern[str]]):\n    \"\"\"A regex literal.\"\"\"\n\n    __slots__ = ()\n\n    RE_FLAG_MAP = {\n        re.A: \"a\",\n        re.I: \"i\",\n        re.M: \"m\",\n        re.S: \"s\",\n    }\n\n    RE_UNESCAPE = re.compile(r\"\\\\(.)\")\n\n    def __str__(self) -> str:\n        flags: List[str] = []\n        for flag, ch in self.RE_FLAG_MAP.items():\n            if self.value.flags & flag:\n                flags.append(ch)\n\n        pattern = re.sub(r\"\\\\(.)\", r\"\\1\", self.value.pattern)\n        return f\"/{pattern}/{''.join(flags)}\"\n\n\nclass ListLiteral(FilterExpression):\n    \"\"\"A list literal.\"\"\"\n\n    __slots__ = (\"items\",)\n\n    def __init__(self, items: List[FilterExpression]) -> None:\n        self.items = items\n        super().__init__()\n\n    def __str__(self) -> str:\n        items = \", \".join(str(item) for item in self.items)\n        return f\"[{items}]\"\n\n    def __eq__(self, other: object) -> bool:\n        return isinstance(other, ListLiteral) and self.items == other.items\n\n    def evaluate(self, context: FilterContext) -> object:\n        return [item.evaluate(context) for item in self.items]\n\n    async def evaluate_async(self, context: FilterContext) -> object:\n        return [await item.evaluate_async(context) for item in self.items]\n\n    def children(self) -> List[FilterExpression]:\n        return self.items\n\n    def set_children(self, children: List[FilterExpression]) -> None:  # noqa: ARG002\n        self.items = children\n\n\nclass PrefixExpression(FilterExpression):\n    \"\"\"An expression composed of a prefix operator and another expression.\"\"\"\n\n    __slots__ = (\"operator\", \"right\")\n\n    def __init__(self, operator: str, right: FilterExpression):\n        self.operator = operator\n        self.right = right\n        super().__init__()\n\n    def __str__(self) -> str:\n        return f\"{self.operator}{self.right}\"\n\n    def __eq__(self, other: object) -> bool:\n        return (\n            isinstance(other, PrefixExpression)\n            and self.operator == other.operator\n            and self.right == other.right\n        )\n\n    def _evaluate(self, context: FilterContext, right: object) -> object:\n        if self.operator == \"!\":\n            return not context.env.is_truthy(right)\n        raise JSONPathTypeError(f\"unknown operator {self.operator} {self.right}\")\n\n    def evaluate(self, context: FilterContext) -> object:\n        return self._evaluate(context, self.right.evaluate(context))\n\n    async def evaluate_async(self, context: FilterContext) -> object:\n        return self._evaluate(context, await self.right.evaluate_async(context))\n\n    def children(self) -> List[FilterExpression]:\n        return [self.right]\n\n    def set_children(self, children: List[FilterExpression]) -> None:\n        assert len(children) == 1\n        self.right = children[0]\n\n\nclass InfixExpression(FilterExpression):\n    \"\"\"A pair of expressions and a comparison or logical operator.\"\"\"\n\n    __slots__ = (\"left\", \"operator\", \"right\", \"logical\")\n\n    def __init__(\n        self,\n        left: FilterExpression,\n        operator: str,\n        right: FilterExpression,\n    ):\n        self.left = left\n        self.operator = operator\n        self.right = right\n        self.logical = operator in (\"&&\", \"||\")\n        super().__init__()\n\n    def __str__(self) -> str:\n        if self.logical:\n            return f\"({self.left} {self.operator} {self.right})\"\n        return f\"{self.left} {self.operator} {self.right}\"\n\n    def __eq__(self, other: object) -> bool:\n        return (\n            isinstance(other, InfixExpression)\n            and self.left == other.left\n            and self.operator == other.operator\n            and self.right == other.right\n        )\n\n    def evaluate(self, context: FilterContext) -> bool:\n        left = self.left.evaluate(context)\n        if not self.logical and isinstance(left, NodeList) and len(left) == 1:\n            left = left[0].obj\n\n        right = self.right.evaluate(context)\n        if not self.logical and isinstance(right, NodeList) and len(right) == 1:\n            right = right[0].obj\n\n        return context.env.compare(left, self.operator, right)\n\n    async def evaluate_async(self, context: FilterContext) -> bool:\n        left = await self.left.evaluate_async(context)\n        if not self.logical and isinstance(left, NodeList) and len(left) == 1:\n            left = left[0].obj\n\n        right = await self.right.evaluate_async(context)\n        if not self.logical and isinstance(right, NodeList) and len(right) == 1:\n            right = right[0].obj\n\n        return context.env.compare(left, self.operator, right)\n\n    def children(self) -> List[FilterExpression]:\n        return [self.left, self.right]\n\n    def set_children(self, children: List[FilterExpression]) -> None:\n        assert len(children) == 2  # noqa: PLR2004\n        self.left = children[0]\n        self.right = children[1]\n\n\nclass BooleanExpression(FilterExpression):\n    \"\"\"An expression that always evaluates to `True` or `False`.\"\"\"\n\n    __slots__ = (\"expression\",)\n\n    def __init__(self, expression: FilterExpression):\n        self.expression = expression\n        super().__init__()\n\n    def cache_tree(self) -> BooleanExpression:\n        \"\"\"Return a copy of _self.expression_ augmented with caching nodes.\"\"\"\n\n        def _cache_tree(expr: FilterExpression) -> FilterExpression:\n            children = expr.children()\n            if expr.volatile:\n                _expr = copy.copy(expr)\n            elif not expr.FORCE_CACHE and len(children) == 0:\n                _expr = expr\n            else:\n                _expr = CachingFilterExpression(copy.copy(expr))\n            _expr.set_children([_cache_tree(child) for child in children])\n            return _expr\n\n        return BooleanExpression(_cache_tree(copy.copy(self.expression)))\n\n    def cacheable_nodes(self) -> bool:\n        \"\"\"Return `True` if there are any cacheable nodes in this expression tree.\"\"\"\n        return any(\n            isinstance(node, CachingFilterExpression)\n            for node in walk(self.cache_tree())\n        )\n\n    def __str__(self) -> str:\n        return str(self.expression)\n\n    def __eq__(self, other: object) -> bool:\n        return (\n            isinstance(other, BooleanExpression) and self.expression == other.expression\n        )\n\n    def evaluate(self, context: FilterContext) -> bool:\n        return context.env.is_truthy(self.expression.evaluate(context))\n\n    async def evaluate_async(self, context: FilterContext) -> bool:\n        return context.env.is_truthy(await self.expression.evaluate_async(context))\n\n    def children(self) -> List[FilterExpression]:\n        return [self.expression]\n\n    def set_children(self, children: List[FilterExpression]) -> None:\n        assert len(children) == 1\n        self.expression = children[0]\n\n\nclass CachingFilterExpression(FilterExpression):\n    \"\"\"A FilterExpression wrapper that caches the result.\"\"\"\n\n    __slots__ = (\n        \"_cached\",\n        \"_expr\",\n    )\n\n    _UNSET = object()\n\n    def __init__(self, expression: FilterExpression):\n        self.volatile = False\n        self._expr = expression\n        self._cached: object = self._UNSET\n\n    def evaluate(self, context: FilterContext) -> object:\n        if self._cached is self._UNSET:\n            self._cached = self._expr.evaluate(context)\n        return self._cached\n\n    async def evaluate_async(self, context: FilterContext) -> object:\n        if self._cached is self._UNSET:\n            self._cached = await self._expr.evaluate_async(context)\n        return self._cached\n\n    def children(self) -> List[FilterExpression]:\n        return self._expr.children()\n\n    def set_children(self, children: List[FilterExpression]) -> None:\n        self._expr.set_children(children)\n\n\nclass Path(FilterExpression, ABC):\n    \"\"\"Base expression for all _sub paths_ found in filter expressions.\"\"\"\n\n    __slots__ = (\"path\",)\n\n    def __init__(self, path: JSONPath) -> None:\n        self.path = path\n        super().__init__()\n\n    def __eq__(self, other: object) -> bool:\n        return isinstance(other, Path) and str(self) == str(other)\n\n    def children(self) -> List[FilterExpression]:\n        _children: List[FilterExpression] = []\n        for segment in self.path.selectors:\n            if isinstance(segment, ListSelector):\n                _children.extend(\n                    selector.expression\n                    for selector in segment.items\n                    if isinstance(selector, FilterSelector)\n                )\n        return _children\n\n    def set_children(self, children: List[FilterExpression]) -> None:  # noqa: ARG002\n        # self.path has its own cache\n        return\n\n\nclass SelfPath(Path):\n    \"\"\"A JSONPath starting at the current node.\"\"\"\n\n    __slots__ = ()\n\n    def __init__(self, path: JSONPath) -> None:\n        super().__init__(path)\n        self.volatile = True\n\n    def __str__(self) -> str:\n        return \"@\" + str(self.path)[1:]\n\n    def evaluate(self, context: FilterContext) -> object:\n        if isinstance(context.current, str):  # TODO: refactor\n            if self.path.empty():\n                return context.current\n            return NodeList()\n        if not isinstance(context.current, (Sequence, Mapping)):\n            if self.path.empty():\n                return context.current\n            return NodeList()\n\n        return NodeList(self.path.finditer(context.current))\n\n    async def evaluate_async(self, context: FilterContext) -> object:\n        if isinstance(context.current, str):  # TODO: refactor\n            if self.path.empty():\n                return context.current\n            return NodeList()\n        if not isinstance(context.current, (Sequence, Mapping)):\n            if self.path.empty():\n                return context.current\n            return NodeList()\n\n        return NodeList(\n            [match async for match in await self.path.finditer_async(context.current)]\n        )\n\n\nclass RootPath(Path):\n    \"\"\"A JSONPath starting at the root node.\"\"\"\n\n    __slots__ = ()\n\n    FORCE_CACHE = True\n\n    def __init__(self, path: JSONPath) -> None:\n        super().__init__(path)\n        self.volatile = False\n\n    def __str__(self) -> str:\n        return str(self.path)\n\n    def evaluate(self, context: FilterContext) -> object:\n        return NodeList(self.path.finditer(context.root))\n\n    async def evaluate_async(self, context: FilterContext) -> object:\n        return NodeList(\n            [match async for match in await self.path.finditer_async(context.root)]\n        )\n\n\nclass FilterContextPath(Path):\n    \"\"\"A JSONPath starting at the root of any extra context data.\"\"\"\n\n    __slots__ = ()\n\n    FORCE_CACHE = True\n\n    def __init__(self, path: JSONPath) -> None:\n        super().__init__(path)\n        self.volatile = False\n\n    def __str__(self) -> str:\n        path_repr = str(self.path)\n        return \"_\" + path_repr[1:]\n\n    def evaluate(self, context: FilterContext) -> object:\n        return NodeList(self.path.finditer(context.extra_context))\n\n    async def evaluate_async(self, context: FilterContext) -> object:\n        return NodeList(\n            [\n                match\n                async for match in await self.path.finditer_async(context.extra_context)\n            ]\n        )\n\n\nclass FunctionExtension(FilterExpression):\n    \"\"\"A filter function.\"\"\"\n\n    __slots__ = (\"name\", \"args\")\n\n    def __init__(self, name: str, args: Sequence[FilterExpression]) -> None:\n        self.name = name\n        self.args = args\n        super().__init__()\n\n    def __str__(self) -> str:\n        args = [str(arg) for arg in self.args]\n        return f\"{self.name}({', '.join(args)})\"\n\n    def __eq__(self, other: object) -> bool:\n        return (\n            isinstance(other, FunctionExtension)\n            and other.name == self.name\n            and other.args == self.args\n        )\n\n    def evaluate(self, context: FilterContext) -> object:\n        try:\n            func = context.env.function_extensions[self.name]\n        except KeyError:\n            return UNDEFINED  # TODO: should probably raise an exception\n        args = [arg.evaluate(context) for arg in self.args]\n        return func(*self._unpack_node_lists(func, args))\n\n    async def evaluate_async(self, context: FilterContext) -> object:\n        try:\n            func = context.env.function_extensions[self.name]\n        except KeyError:\n            return UNDEFINED  # TODO: should probably raise an exception\n        args = [await arg.evaluate_async(context) for arg in self.args]\n        return func(*self._unpack_node_lists(func, args))\n\n    def _unpack_node_lists(\n        self, func: Callable[..., Any], args: List[object]\n    ) -> List[object]:\n        if isinstance(func, FilterFunction):\n            _args: List[object] = []\n            for idx, arg in enumerate(args):\n                if func.arg_types[idx] != ExpressionType.NODES and isinstance(\n                    arg, NodeList\n                ):\n                    if len(arg) == 0:\n                        # If the query results in an empty nodelist, the\n                        # argument is the special result Nothing.\n                        _args.append(UNDEFINED)\n                    elif len(arg) == 1:\n                        # If the query results in a nodelist consisting of a\n                        # single node, the argument is the value of the node\n                        _args.append(arg[0].obj)\n                    else:\n                        # This should not be possible as a non-singular query\n                        # would have been rejected when checking function\n                        # well-typedness.\n                        _args.append(arg)\n                else:\n                    _args.append(arg)\n            return _args\n\n        # Legacy way to indicate that a filter function wants node lists as arguments.\n        if getattr(func, \"with_node_lists\", False):\n            return args\n\n        return [\n            obj.values_or_singular() if isinstance(obj, NodeList) else obj\n            for obj in args\n        ]\n\n    def children(self) -> List[FilterExpression]:\n        return list(self.args)\n\n    def set_children(self, children: List[FilterExpression]) -> None:\n        assert len(children) == len(self.args)\n        self.args = children\n\n\nclass CurrentKey(FilterExpression):\n    \"\"\"The key/property or index associated with the current object.\"\"\"\n\n    __slots__ = ()\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.volatile = True\n\n    def __str__(self) -> str:\n        return \"#\"\n\n    def __eq__(self, other: object) -> bool:\n        return isinstance(other, CurrentKey)\n\n    def evaluate(self, context: FilterContext) -> object:\n        if context.current_key is None:\n            return UNDEFINED\n        return context.current_key\n\n    async def evaluate_async(self, context: FilterContext) -> object:\n        return self.evaluate(context)\n\n    def children(self) -> List[FilterExpression]:\n        return []\n\n    def set_children(self, children: List[FilterExpression]) -> None:  # noqa: ARG002\n        return\n\n\nCURRENT_KEY = CurrentKey()\n\n\ndef walk(expr: FilterExpression) -> Iterable[FilterExpression]:\n    \"\"\"Walk the filter expression tree starting at _expr_.\"\"\"\n    yield expr\n    for child in expr.children():\n        yield from walk(child)\n\n\nVALUE_TYPE_EXPRESSIONS = (\n    Nil,\n    Undefined,\n    Literal,\n    ListLiteral,\n    CurrentKey,\n)\n",
    "jsonpath/stream.py": "# noqa: D100\nfrom __future__ import annotations\n\nfrom collections import deque\nfrom typing import Deque\nfrom typing import Iterator\n\nfrom .exceptions import JSONPathSyntaxError\nfrom .token import TOKEN_EOF\nfrom .token import Token\n\n# ruff: noqa: D102\n\n\nclass TokenStream:\n    \"\"\"Step through or iterate a stream of tokens.\"\"\"\n\n    def __init__(self, token_iter: Iterator[Token]):\n        self.iter = token_iter\n        self._pushed: Deque[Token] = deque()\n        self.current = Token(\"\", \"\", -1, \"\")\n        next(self)\n\n    class TokenStreamIterator:\n        \"\"\"An iterable token stream.\"\"\"\n\n        def __init__(self, stream: TokenStream):\n            self.stream = stream\n\n        def __iter__(self) -> Iterator[Token]:\n            return self\n\n        def __next__(self) -> Token:\n            tok = self.stream.current\n            if tok.kind is TOKEN_EOF:\n                self.stream.close()\n                raise StopIteration\n            next(self.stream)\n            return tok\n\n    def __iter__(self) -> Iterator[Token]:\n        return self.TokenStreamIterator(self)\n\n    def __next__(self) -> Token:\n        tok = self.current\n        if self._pushed:\n            self.current = self._pushed.popleft()\n        elif self.current.kind is not TOKEN_EOF:\n            try:\n                self.current = next(self.iter)\n            except StopIteration:\n                self.close()\n        return tok\n\n    def __str__(self) -> str:  # pragma: no cover\n        return f\"current: {self.current}\\nnext: {self.peek}\"\n\n    def next_token(self) -> Token:\n        \"\"\"Return the next token from the stream.\"\"\"\n        return next(self)\n\n    @property\n    def peek(self) -> Token:\n        \"\"\"Look at the next token.\"\"\"\n        current = next(self)\n        result = self.current\n        self.push(current)\n        return result\n\n    def push(self, tok: Token) -> None:\n        \"\"\"Push a token back to the stream.\"\"\"\n        self._pushed.append(self.current)\n        self.current = tok\n\n    def close(self) -> None:\n        \"\"\"Close the stream.\"\"\"\n        self.current = Token(TOKEN_EOF, \"\", -1, \"\")\n\n    def expect(self, *typ: str) -> None:\n        if self.current.kind not in typ:\n            if len(typ) == 1:\n                _typ = repr(typ[0])\n            else:\n                _typ = f\"one of {typ!r}\"\n            raise JSONPathSyntaxError(\n                f\"expected {_typ}, found {self.current.kind!r}\",\n                token=self.current,\n            )\n\n    def expect_peek(self, *typ: str) -> None:\n        if self.peek.kind not in typ:\n            if len(typ) == 1:\n                _typ = repr(typ[0])\n            else:\n                _typ = f\"one of {typ!r}\"\n            raise JSONPathSyntaxError(\n                f\"expected {_typ}, found {self.peek.kind!r}\",\n                token=self.peek,\n            )\n",
    "jsonpath/match.py": "\"\"\"The JSONPath match object, as returned from `JSONPath.finditer()`.\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Any\nfrom typing import List\nfrom typing import Mapping\nfrom typing import Optional\nfrom typing import Sequence\nfrom typing import Tuple\nfrom typing import Union\n\nfrom .pointer import JSONPointer\n\nFilterContextVars = Mapping[str, Any]\nPathPart = Union[int, str]\n\n\nclass JSONPathMatch:\n    \"\"\"A matched object with a concrete path.\n\n    Attributes:\n        children: Matched child nodes. This will only be populated after\n            all children have been visited, usually by using `findall()`\n            or `list(finditer())`.\n        obj: The matched object.\n        parent: The immediate parent to this match in the JSON document.\n            If this is the root node, _parent_ will be `None`.\n        path: The canonical string representation of the path to this match.\n        parts: The keys, indices and/or slices that make up the path to this\n            match.\n        root: A reference to the root node in the JSON document.\n    \"\"\"\n\n    __slots__ = (\n        \"_filter_context\",\n        \"children\",\n        \"obj\",\n        \"parent\",\n        \"parts\",\n        \"path\",\n        \"root\",\n    )\n\n    pointer_class = JSONPointer\n\n    def __init__(\n        self,\n        *,\n        filter_context: FilterContextVars,\n        obj: object,\n        parent: Optional[JSONPathMatch],\n        path: str,\n        parts: Tuple[PathPart, ...],\n        root: Union[Sequence[Any], Mapping[str, Any]],\n    ) -> None:\n        self._filter_context = filter_context\n        self.children: List[JSONPathMatch] = []\n        self.obj: object = obj\n        self.parent: Optional[JSONPathMatch] = parent\n        self.parts: Tuple[PathPart, ...] = parts\n        self.path: str = path\n        self.root: Union[Sequence[Any], Mapping[str, Any]] = root\n\n    def __str__(self) -> str:\n        return f\"{_truncate(str(self.obj), 5)!r} @ {_truncate(self.path, 5)}\"\n\n    def add_child(self, *children: JSONPathMatch) -> None:\n        \"\"\"Append one or more children to this match.\"\"\"\n        self.children.extend(children)\n\n    def filter_context(self) -> FilterContextVars:\n        \"\"\"Return filter context data for this match.\"\"\"\n        return self._filter_context\n\n    def pointer(self) -> JSONPointer:\n        \"\"\"Return a `JSONPointer` pointing to this match's path.\"\"\"\n        return JSONPointer.from_match(self)\n\n    @property\n    def value(self) -> object:\n        \"\"\"Return the value associated with this match/node.\"\"\"\n        return self.obj\n\n\ndef _truncate(val: str, num: int, end: str = \"...\") -> str:\n    # Replaces consecutive whitespace with a single newline.\n    # Treats quoted whitespace the same as unquoted whitespace.\n    words = val.split()\n    if len(words) < num:\n        return \" \".join(words)\n    return \" \".join(words[:num]) + end\n\n\nclass NodeList(List[JSONPathMatch]):\n    \"\"\"List of JSONPathMatch objects, analogous to the spec's nodelist.\"\"\"\n\n    def values(self) -> List[object]:\n        \"\"\"Return the values from this node list.\"\"\"\n        return [match.obj for match in self]\n\n    def values_or_singular(self) -> object:\n        \"\"\"Return the values from this node list.\"\"\"\n        if len(self) == 1:\n            return self[0].obj\n        return [match.obj for match in self]\n\n    def empty(self) -> bool:\n        \"\"\"Return `True` if this node list is empty.\"\"\"\n        return not bool(self)\n\n    def __str__(self) -> str:\n        return f\"NodeList{super().__str__()}\"\n",
    "jsonpath/selectors.py": "\"\"\"JSONPath segments and selectors, as returned from `Parser.parse`.\"\"\"\nfrom __future__ import annotations\n\nfrom abc import ABC\nfrom abc import abstractmethod\nfrom collections.abc import Mapping\nfrom collections.abc import Sequence\nfrom contextlib import suppress\nfrom typing import TYPE_CHECKING\nfrom typing import Any\nfrom typing import AsyncIterable\nfrom typing import Iterable\nfrom typing import List\nfrom typing import Optional\nfrom typing import TypeVar\nfrom typing import Union\n\nfrom .exceptions import JSONPathIndexError\nfrom .exceptions import JSONPathTypeError\n\nif TYPE_CHECKING:\n    from .env import JSONPathEnvironment\n    from .filter import BooleanExpression\n    from .match import JSONPathMatch\n    from .token import Token\n\n# ruff: noqa: D102\n\n\nclass JSONPathSelector(ABC):\n    \"\"\"Base class for all JSONPath segments and selectors.\"\"\"\n\n    __slots__ = (\"env\", \"token\")\n\n    def __init__(self, *, env: JSONPathEnvironment, token: Token) -> None:\n        self.env = env\n        self.token = token\n\n    @abstractmethod\n    def resolve(self, matches: Iterable[JSONPathMatch]) -> Iterable[JSONPathMatch]:\n        \"\"\"Apply the segment/selector to each node in _matches_.\n\n        Arguments:\n            matches: Nodes matched by preceding segments/selectors. This is like\n                a lazy _NodeList_, as described in RFC 9535, but each match carries\n                more than the node's value and location.\n\n        Returns:\n            The `JSONPathMatch` instances created by applying this selector to each\n            preceding node.\n        \"\"\"\n\n    @abstractmethod\n    def resolve_async(\n        self, matches: AsyncIterable[JSONPathMatch]\n    ) -> AsyncIterable[JSONPathMatch]:\n        \"\"\"An async version of `resolve`.\"\"\"\n\n\nclass PropertySelector(JSONPathSelector):\n    \"\"\"A shorthand or bracketed property selector.\"\"\"\n\n    __slots__ = (\"name\", \"shorthand\")\n\n    def __init__(\n        self,\n        *,\n        env: JSONPathEnvironment,\n        token: Token,\n        name: str,\n        shorthand: bool,\n    ) -> None:\n        super().__init__(env=env, token=token)\n        self.name = name\n        self.shorthand = shorthand\n\n    def __str__(self) -> str:\n        return f\"['{self.name}']\" if self.shorthand else f\"'{self.name}'\"\n\n    def __eq__(self, __value: object) -> bool:\n        return (\n            isinstance(__value, PropertySelector)\n            and self.name == __value.name\n            and self.token == __value.token\n        )\n\n    def __hash__(self) -> int:\n        return hash((self.name, self.token))\n\n    def resolve(self, matches: Iterable[JSONPathMatch]) -> Iterable[JSONPathMatch]:\n        for match in matches:\n            if not isinstance(match.obj, Mapping):\n                continue\n\n            with suppress(KeyError):\n                _match = self.env.match_class(\n                    filter_context=match.filter_context(),\n                    obj=self.env.getitem(match.obj, self.name),\n                    parent=match,\n                    parts=match.parts + (self.name,),\n                    path=match.path + f\"['{self.name}']\",\n                    root=match.root,\n                )\n                match.add_child(_match)\n                yield _match\n\n    async def resolve_async(\n        self, matches: AsyncIterable[JSONPathMatch]\n    ) -> AsyncIterable[JSONPathMatch]:\n        async for match in matches:\n            if not isinstance(match.obj, Mapping):\n                continue\n\n            with suppress(KeyError):\n                _match = self.env.match_class(\n                    filter_context=match.filter_context(),\n                    obj=await self.env.getitem_async(match.obj, self.name),\n                    parent=match,\n                    parts=match.parts + (self.name,),\n                    path=match.path + f\"['{self.name}']\",\n                    root=match.root,\n                )\n                match.add_child(_match)\n                yield _match\n\n\nclass IndexSelector(JSONPathSelector):\n    \"\"\"Select an element from an array by index.\n\n    Considering we don't require mapping (JSON object) keys/properties to\n    be quoted, and that we support mappings with numeric keys, we also check\n    to see if the \"index\" is a mapping key, which is non-standard.\n    \"\"\"\n\n    __slots__ = (\"index\", \"_as_key\")\n\n    def __init__(\n        self,\n        *,\n        env: JSONPathEnvironment,\n        token: Token,\n        index: int,\n    ) -> None:\n        if index < env.min_int_index or index > env.max_int_index:\n            raise JSONPathIndexError(\"index out of range\", token=token)\n\n        super().__init__(env=env, token=token)\n        self.index = index\n        self._as_key = str(self.index)\n\n    def __str__(self) -> str:\n        return str(self.index)\n\n    def __eq__(self, __value: object) -> bool:\n        return (\n            isinstance(__value, IndexSelector)\n            and self.index == __value.index\n            and self.token == __value.token\n        )\n\n    def __hash__(self) -> int:\n        return hash((self.index, self.token))\n\n    def _normalized_index(self, obj: Sequence[object]) -> int:\n        if self.index < 0 and len(obj) >= abs(self.index):\n            return len(obj) + self.index\n        return self.index\n\n    def resolve(self, matches: Iterable[JSONPathMatch]) -> Iterable[JSONPathMatch]:\n        for match in matches:\n            if isinstance(match.obj, Mapping):\n                # Try the string representation of the index as a key.\n                with suppress(KeyError):\n                    _match = self.env.match_class(\n                        filter_context=match.filter_context(),\n                        obj=self.env.getitem(match.obj, self._as_key),\n                        parent=match,\n                        parts=match.parts + (self._as_key,),\n                        path=f\"{match.path}['{self.index}']\",\n                        root=match.root,\n                    )\n                    match.add_child(_match)\n                    yield _match\n            elif isinstance(match.obj, Sequence) and not isinstance(match.obj, str):\n                norm_index = self._normalized_index(match.obj)\n                with suppress(IndexError):\n                    _match = self.env.match_class(\n                        filter_context=match.filter_context(),\n                        obj=self.env.getitem(match.obj, self.index),\n                        parent=match,\n                        parts=match.parts + (norm_index,),\n                        path=match.path + f\"[{norm_index}]\",\n                        root=match.root,\n                    )\n                    match.add_child(_match)\n                    yield _match\n\n    async def resolve_async(\n        self, matches: AsyncIterable[JSONPathMatch]\n    ) -> AsyncIterable[JSONPathMatch]:\n        async for match in matches:\n            if isinstance(match.obj, Mapping):\n                # Try the string representation of the index as a key.\n                with suppress(KeyError):\n                    _match = self.env.match_class(\n                        filter_context=match.filter_context(),\n                        obj=await self.env.getitem_async(match.obj, self._as_key),\n                        parent=match,\n                        parts=match.parts + (self._as_key,),\n                        path=f\"{match.path}['{self.index}']\",\n                        root=match.root,\n                    )\n                    match.add_child(_match)\n                    yield _match\n            elif isinstance(match.obj, Sequence) and not isinstance(match.obj, str):\n                norm_index = self._normalized_index(match.obj)\n                with suppress(IndexError):\n                    _match = self.env.match_class(\n                        filter_context=match.filter_context(),\n                        obj=await self.env.getitem_async(match.obj, self.index),\n                        parent=match,\n                        parts=match.parts + (norm_index,),\n                        path=match.path + f\"[{norm_index}]\",\n                        root=match.root,\n                    )\n                    match.add_child(_match)\n                    yield _match\n\n\nclass KeysSelector(JSONPathSelector):\n    \"\"\"Select mapping/object keys/properties.\n\n    NOTE: This is a non-standard selector.\n    \"\"\"\n\n    __slots__ = (\"shorthand\",)\n\n    def __init__(\n        self, *, env: JSONPathEnvironment, token: Token, shorthand: bool\n    ) -> None:\n        super().__init__(env=env, token=token)\n        self.shorthand = shorthand\n\n    def __str__(self) -> str:\n        return (\n            f\"[{self.env.keys_selector_token}]\"\n            if self.shorthand\n            else self.env.keys_selector_token\n        )\n\n    def __eq__(self, __value: object) -> bool:\n        return isinstance(__value, KeysSelector) and self.token == __value.token\n\n    def __hash__(self) -> int:\n        return hash(self.token)\n\n    def _keys(self, match: JSONPathMatch) -> Iterable[JSONPathMatch]:\n        if isinstance(match.obj, Mapping):\n            for i, key in enumerate(match.obj.keys()):\n                _match = self.env.match_class(\n                    filter_context=match.filter_context(),\n                    obj=key,\n                    parent=match,\n                    parts=match.parts + (f\"{self.env.keys_selector_token}{key}\",),\n                    path=f\"{match.path}[{self.env.keys_selector_token}][{i}]\",\n                    root=match.root,\n                )\n                match.add_child(_match)\n                yield _match\n\n    def resolve(self, matches: Iterable[JSONPathMatch]) -> Iterable[JSONPathMatch]:\n        for match in matches:\n            yield from self._keys(match)\n\n    async def resolve_async(\n        self, matches: AsyncIterable[JSONPathMatch]\n    ) -> AsyncIterable[JSONPathMatch]:\n        async for match in matches:\n            for _match in self._keys(match):\n                yield _match\n\n\nclass SliceSelector(JSONPathSelector):\n    \"\"\"Sequence slicing selector.\"\"\"\n\n    __slots__ = (\"slice\",)\n\n    def __init__(\n        self,\n        *,\n        env: JSONPathEnvironment,\n        token: Token,\n        start: Optional[int] = None,\n        stop: Optional[int] = None,\n        step: Optional[int] = None,\n    ) -> None:\n        super().__init__(env=env, token=token)\n        self._check_range(start, stop, step)\n        self.slice = slice(start, stop, step)\n\n    def __str__(self) -> str:\n        stop = self.slice.stop if self.slice.stop is not None else \"\"\n        start = self.slice.start if self.slice.start is not None else \"\"\n        step = self.slice.step if self.slice.step is not None else \"1\"\n        return f\"{start}:{stop}:{step}\"\n\n    def __eq__(self, __value: object) -> bool:\n        return (\n            isinstance(__value, SliceSelector)\n            and self.slice == __value.slice\n            and self.token == __value.token\n        )\n\n    def __hash__(self) -> int:\n        return hash((str(self), self.token))\n\n    def _check_range(self, *indices: Optional[int]) -> None:\n        for i in indices:\n            if i is not None and (\n                i < self.env.min_int_index or i > self.env.max_int_index\n            ):\n                raise JSONPathIndexError(\"index out of range\", token=self.token)\n\n    def _normalized_index(self, obj: Sequence[object], index: int) -> int:\n        if index < 0 and len(obj) >= abs(index):\n            return len(obj) + index\n        return index\n\n    def resolve(self, matches: Iterable[JSONPathMatch]) -> Iterable[JSONPathMatch]:\n        for match in matches:\n            if not isinstance(match.obj, Sequence) or self.slice.step == 0:\n                continue\n\n            idx = self.slice.start or 0\n            step = self.slice.step or 1\n            for obj in self.env.getitem(match.obj, self.slice):\n                norm_index = self._normalized_index(match.obj, idx)\n                _match = self.env.match_class(\n                    filter_context=match.filter_context(),\n                    obj=obj,\n                    parent=match,\n                    parts=match.parts + (norm_index,),\n                    path=f\"{match.path}[{norm_index}]\",\n                    root=match.root,\n                )\n                match.add_child(_match)\n                yield _match\n                idx += step\n\n    async def resolve_async(\n        self, matches: AsyncIterable[JSONPathMatch]\n    ) -> AsyncIterable[JSONPathMatch]:\n        async for match in matches:\n            if not isinstance(match.obj, Sequence) or self.slice.step == 0:\n                continue\n\n            idx = self.slice.start or 0\n            step = self.slice.step or 1\n            for obj in await self.env.getitem_async(match.obj, self.slice):\n                norm_index = self._normalized_index(match.obj, idx)\n                _match = self.env.match_class(\n                    filter_context=match.filter_context(),\n                    obj=obj,\n                    parent=match,\n                    parts=match.parts + (norm_index,),\n                    path=f\"{match.path}[{norm_index}]\",\n                    root=match.root,\n                )\n                match.add_child(_match)\n                yield _match\n                idx += step\n\n\nclass WildSelector(JSONPathSelector):\n    \"\"\"Select all items from a sequence/array or values from a mapping/object.\"\"\"\n\n    __slots__ = (\"shorthand\",)\n\n    def __init__(\n        self, *, env: JSONPathEnvironment, token: Token, shorthand: bool\n    ) -> None:\n        super().__init__(env=env, token=token)\n        self.shorthand = shorthand\n\n    def __str__(self) -> str:\n        return \"[*]\" if self.shorthand else \"*\"\n\n    def __eq__(self, __value: object) -> bool:\n        return isinstance(__value, WildSelector) and self.token == __value.token\n\n    def __hash__(self) -> int:\n        return hash(self.token)\n\n    def resolve(self, matches: Iterable[JSONPathMatch]) -> Iterable[JSONPathMatch]:\n        for match in matches:\n            if isinstance(match.obj, str):\n                continue\n            if isinstance(match.obj, Mapping):\n                for key, val in match.obj.items():\n                    _match = self.env.match_class(\n                        filter_context=match.filter_context(),\n                        obj=val,\n                        parent=match,\n                        parts=match.parts + (key,),\n                        path=match.path + f\"['{key}']\",\n                        root=match.root,\n                    )\n                    match.add_child(_match)\n                    yield _match\n            elif isinstance(match.obj, Sequence):\n                for i, val in enumerate(match.obj):\n                    _match = self.env.match_class(\n                        filter_context=match.filter_context(),\n                        obj=val,\n                        parent=match,\n                        parts=match.parts + (i,),\n                        path=f\"{match.path}[{i}]\",\n                        root=match.root,\n                    )\n                    match.add_child(_match)\n                    yield _match\n\n    async def resolve_async(\n        self, matches: AsyncIterable[JSONPathMatch]\n    ) -> AsyncIterable[JSONPathMatch]:\n        async for match in matches:\n            if isinstance(match.obj, Mapping):\n                for key, val in match.obj.items():\n                    _match = self.env.match_class(\n                        filter_context=match.filter_context(),\n                        obj=val,\n                        parent=match,\n                        parts=match.parts + (key,),\n                        path=match.path + f\"['{key}']\",\n                        root=match.root,\n                    )\n                    match.add_child(_match)\n                    yield _match\n            elif isinstance(match.obj, Sequence):\n                for i, val in enumerate(match.obj):\n                    _match = self.env.match_class(\n                        filter_context=match.filter_context(),\n                        obj=val,\n                        parent=match,\n                        parts=match.parts + (i,),\n                        path=f\"{match.path}[{i}]\",\n                        root=match.root,\n                    )\n                    match.add_child(_match)\n                    yield _match\n\n\nclass RecursiveDescentSelector(JSONPathSelector):\n    \"\"\"A JSONPath selector that visits all nodes recursively.\n\n    NOTE: Strictly this is a \"segment\", not a \"selector\".\n    \"\"\"\n\n    def __str__(self) -> str:\n        return \"..\"\n\n    def __eq__(self, __value: object) -> bool:\n        return (\n            isinstance(__value, RecursiveDescentSelector)\n            and self.token == __value.token\n        )\n\n    def __hash__(self) -> int:\n        return hash(self.token)\n\n    def _expand(self, match: JSONPathMatch) -> Iterable[JSONPathMatch]:\n        if isinstance(match.obj, Mapping):\n            for key, val in match.obj.items():\n                if isinstance(val, str):\n                    pass\n                elif isinstance(val, (Mapping, Sequence)):\n                    _match = self.env.match_class(\n                        filter_context=match.filter_context(),\n                        obj=val,\n                        parent=match,\n                        parts=match.parts + (key,),\n                        path=match.path + f\"['{key}']\",\n                        root=match.root,\n                    )\n                    match.add_child(_match)\n                    yield _match\n                    yield from self._expand(_match)\n        elif isinstance(match.obj, Sequence) and not isinstance(match.obj, str):\n            for i, val in enumerate(match.obj):\n                if isinstance(val, str):\n                    pass\n                elif isinstance(val, (Mapping, Sequence)):\n                    _match = self.env.match_class(\n                        filter_context=match.filter_context(),\n                        obj=val,\n                        parent=match,\n                        parts=match.parts + (i,),\n                        path=f\"{match.path}[{i}]\",\n                        root=match.root,\n                    )\n                    match.add_child(_match)\n                    yield _match\n                    yield from self._expand(_match)\n\n    def resolve(self, matches: Iterable[JSONPathMatch]) -> Iterable[JSONPathMatch]:\n        for match in matches:\n            yield match\n            yield from self._expand(match)\n\n    async def resolve_async(\n        self, matches: AsyncIterable[JSONPathMatch]\n    ) -> AsyncIterable[JSONPathMatch]:\n        async for match in matches:\n            yield match\n            for _match in self._expand(match):\n                yield _match\n\n\nT = TypeVar(\"T\")\n\n\nasync def _alist(it: List[T]) -> AsyncIterable[T]:\n    for item in it:\n        yield item\n\n\nclass ListSelector(JSONPathSelector):\n    \"\"\"A bracketed list of selectors, the results of which are concatenated together.\n\n    NOTE: Strictly this is a \"segment\", not a \"selector\".\n    \"\"\"\n\n    __slots__ = (\"items\",)\n\n    def __init__(\n        self,\n        *,\n        env: JSONPathEnvironment,\n        token: Token,\n        items: List[\n            Union[\n                SliceSelector,\n                KeysSelector,\n                IndexSelector,\n                PropertySelector,\n                WildSelector,\n                Filter,\n            ]\n        ],\n    ) -> None:\n        super().__init__(env=env, token=token)\n        self.items = tuple(items)\n\n    def __str__(self) -> str:\n        return f\"[{', '.join(str(itm) for itm in self.items)}]\"\n\n    def __eq__(self, __value: object) -> bool:\n        return (\n            isinstance(__value, ListSelector)\n            and self.items == __value.items\n            and self.token == __value.token\n        )\n\n    def __hash__(self) -> int:\n        return hash((self.items, self.token))\n\n    def resolve(self, matches: Iterable[JSONPathMatch]) -> Iterable[JSONPathMatch]:\n        for match_ in matches:\n            for item in self.items:\n                yield from item.resolve([match_])\n\n    async def resolve_async(\n        self, matches: AsyncIterable[JSONPathMatch]\n    ) -> AsyncIterable[JSONPathMatch]:\n        async for match_ in matches:\n            for item in self.items:\n                async for m in item.resolve_async(_alist([match_])):\n                    yield m\n\n\nclass Filter(JSONPathSelector):\n    \"\"\"Filter sequence/array items or mapping/object values with a filter expression.\"\"\"\n\n    __slots__ = (\"expression\", \"cacheable_nodes\")\n\n    def __init__(\n        self,\n        *,\n        env: JSONPathEnvironment,\n        token: Token,\n        expression: BooleanExpression,\n    ) -> None:\n        super().__init__(env=env, token=token)\n        self.expression = expression\n        # Compile-time check for cacheable nodes.\n        self.cacheable_nodes = self.expression.cacheable_nodes()\n\n    def __str__(self) -> str:\n        return f\"?{self.expression}\"\n\n    def __eq__(self, __value: object) -> bool:\n        return (\n            isinstance(__value, Filter)\n            and self.expression == __value.expression\n            and self.token == __value.token\n        )\n\n    def __hash__(self) -> int:\n        return hash((str(self.expression), self.token))\n\n    def resolve(  # noqa: PLR0912\n        self, matches: Iterable[JSONPathMatch]\n    ) -> Iterable[JSONPathMatch]:\n        if self.cacheable_nodes and self.env.filter_caching:\n            expr = self.expression.cache_tree()\n        else:\n            expr = self.expression\n\n        for match in matches:\n            if isinstance(match.obj, Mapping):\n                for key, val in match.obj.items():\n                    context = FilterContext(\n                        env=self.env,\n                        current=val,\n                        root=match.root,\n                        extra_context=match.filter_context(),\n                        current_key=key,\n                    )\n                    try:\n                        if expr.evaluate(context):\n                            _match = self.env.match_class(\n                                filter_context=match.filter_context(),\n                                obj=val,\n                                parent=match,\n                                parts=match.parts + (key,),\n                                path=match.path + f\"['{key}']\",\n                                root=match.root,\n                            )\n                            match.add_child(_match)\n                            yield _match\n                    except JSONPathTypeError as err:\n                        if not err.token:\n                            err.token = self.token\n                        raise\n\n            elif isinstance(match.obj, Sequence) and not isinstance(match.obj, str):\n                for i, obj in enumerate(match.obj):\n                    context = FilterContext(\n                        env=self.env,\n                        current=obj,\n                        root=match.root,\n                        extra_context=match.filter_context(),\n                        current_key=i,\n                    )\n                    try:\n                        if expr.evaluate(context):\n                            _match = self.env.match_class(\n                                filter_context=match.filter_context(),\n                                obj=obj,\n                                parent=match,\n                                parts=match.parts + (i,),\n                                path=f\"{match.path}[{i}]\",\n                                root=match.root,\n                            )\n                            match.add_child(_match)\n                            yield _match\n                    except JSONPathTypeError as err:\n                        if not err.token:\n                            err.token = self.token\n                        raise\n\n    async def resolve_async(  # noqa: PLR0912\n        self, matches: AsyncIterable[JSONPathMatch]\n    ) -> AsyncIterable[JSONPathMatch]:\n        if self.cacheable_nodes and self.env.filter_caching:\n            expr = self.expression.cache_tree()\n        else:\n            expr = self.expression\n\n        async for match in matches:\n            if isinstance(match.obj, Mapping):\n                for key, val in match.obj.items():\n                    context = FilterContext(\n                        env=self.env,\n                        current=val,\n                        root=match.root,\n                        extra_context=match.filter_context(),\n                        current_key=key,\n                    )\n\n                    try:\n                        result = await expr.evaluate_async(context)\n                    except JSONPathTypeError as err:\n                        if not err.token:\n                            err.token = self.token\n                        raise\n\n                    if result:\n                        _match = self.env.match_class(\n                            filter_context=match.filter_context(),\n                            obj=val,\n                            parent=match,\n                            parts=match.parts + (key,),\n                            path=match.path + f\"['{key}']\",\n                            root=match.root,\n                        )\n                        match.add_child(_match)\n                        yield _match\n\n            elif isinstance(match.obj, Sequence) and not isinstance(match.obj, str):\n                for i, obj in enumerate(match.obj):\n                    context = FilterContext(\n                        env=self.env,\n                        current=obj,\n                        root=match.root,\n                        extra_context=match.filter_context(),\n                        current_key=i,\n                    )\n\n                    try:\n                        result = await expr.evaluate_async(context)\n                    except JSONPathTypeError as err:\n                        if not err.token:\n                            err.token = self.token\n                        raise\n                    if result:\n                        _match = self.env.match_class(\n                            filter_context=match.filter_context(),\n                            obj=obj,\n                            parent=match,\n                            parts=match.parts + (i,),\n                            path=f\"{match.path}[{i}]\",\n                            root=match.root,\n                        )\n                        match.add_child(_match)\n                        yield _match\n\n\nclass FilterContext:\n    \"\"\"Contextual information and data for evaluating a filter expression.\"\"\"\n\n    __slots__ = (\n        \"current_key\",\n        \"current\",\n        \"env\",\n        \"extra_context\",\n        \"root\",\n    )\n\n    def __init__(\n        self,\n        *,\n        env: JSONPathEnvironment,\n        current: object,\n        root: Union[Sequence[Any], Mapping[str, Any]],\n        extra_context: Optional[Mapping[str, Any]] = None,\n        current_key: Union[str, int, None] = None,\n    ) -> None:\n        self.env = env\n        self.current = current\n        self.root = root\n        self.extra_context = extra_context or {}\n        self.current_key = current_key\n\n    def __str__(self) -> str:\n        return (\n            f\"FilterContext(current={self.current}, \"\n            f\"extra_context={self.extra_context!r})\"\n        )\n",
    "jsonpath/lex.py": "\"\"\"JSONPath tokenization.\"\"\"\n\nfrom __future__ import annotations\n\nimport re\nfrom functools import partial\nfrom typing import TYPE_CHECKING\nfrom typing import Iterator\nfrom typing import Pattern\n\nfrom .exceptions import JSONPathSyntaxError\nfrom .token import TOKEN_AND\nfrom .token import TOKEN_BARE_PROPERTY\nfrom .token import TOKEN_COMMA\nfrom .token import TOKEN_CONTAINS\nfrom .token import TOKEN_DDOT\nfrom .token import TOKEN_DOT_PROPERTY\nfrom .token import TOKEN_DOUBLE_QUOTE_STRING\nfrom .token import TOKEN_EQ\nfrom .token import TOKEN_FAKE_ROOT\nfrom .token import TOKEN_FALSE\nfrom .token import TOKEN_FILTER\nfrom .token import TOKEN_FILTER_CONTEXT\nfrom .token import TOKEN_FLOAT\nfrom .token import TOKEN_FUNCTION\nfrom .token import TOKEN_GE\nfrom .token import TOKEN_GT\nfrom .token import TOKEN_ILLEGAL\nfrom .token import TOKEN_IN\nfrom .token import TOKEN_INT\nfrom .token import TOKEN_INTERSECTION\nfrom .token import TOKEN_KEY\nfrom .token import TOKEN_KEYS\nfrom .token import TOKEN_LE\nfrom .token import TOKEN_LG\nfrom .token import TOKEN_LIST_SLICE\nfrom .token import TOKEN_LIST_START\nfrom .token import TOKEN_LPAREN\nfrom .token import TOKEN_LT\nfrom .token import TOKEN_MISSING\nfrom .token import TOKEN_NE\nfrom .token import TOKEN_NIL\nfrom .token import TOKEN_NONE\nfrom .token import TOKEN_NOT\nfrom .token import TOKEN_NULL\nfrom .token import TOKEN_OR\nfrom .token import TOKEN_PROPERTY\nfrom .token import TOKEN_RBRACKET\nfrom .token import TOKEN_RE\nfrom .token import TOKEN_RE_FLAGS\nfrom .token import TOKEN_RE_PATTERN\nfrom .token import TOKEN_ROOT\nfrom .token import TOKEN_RPAREN\nfrom .token import TOKEN_SELF\nfrom .token import TOKEN_SINGLE_QUOTE_STRING\nfrom .token import TOKEN_SKIP\nfrom .token import TOKEN_SLICE_START\nfrom .token import TOKEN_SLICE_STEP\nfrom .token import TOKEN_SLICE_STOP\nfrom .token import TOKEN_TRUE\nfrom .token import TOKEN_UNDEFINED\nfrom .token import TOKEN_UNION\nfrom .token import TOKEN_WILD\nfrom .token import Token\n\nif TYPE_CHECKING:\n    from . import JSONPathEnvironment\n\n\nclass Lexer:\n    \"\"\"Tokenize a JSONPath string.\n\n    Some customization can be achieved by subclassing _Lexer_ and setting\n    class attributes. Then setting `lexer_class` on a `JSONPathEnvironment`.\n\n    Attributes:\n        key_pattern: The regular expression pattern used to match mapping\n            keys/properties.\n        logical_not_pattern: The regular expression pattern used to match\n            logical negation tokens. By default, `not` and `!` are\n            equivalent.\n        logical_and_pattern: The regular expression pattern used to match\n            logical _and_ tokens. By default, `and` and `&&` are equivalent.\n        logical_or_pattern: The regular expression pattern used to match\n            logical _or_ tokens. By default, `or` and `||` are equivalent.\n    \"\"\"\n\n    key_pattern = r\"[\\u0080-\\uFFFFa-zA-Z_][\\u0080-\\uFFFFa-zA-Z0-9_-]*\"\n\n    # `not` or !\n    logical_not_pattern = r\"(?:not|!)\"\n\n    # && or `and`\n    logical_and_pattern = r\"(?:&&|and)\"\n\n    # || or `or`\n    logical_or_pattern = r\"(?:\\|\\||or)\"\n\n    def __init__(self, *, env: JSONPathEnvironment) -> None:\n        self.env = env\n\n        self.double_quote_pattern = r'\"(?P<G_DQUOTE>(?:(?!(?<!\\\\)\").)*)\"'\n        self.single_quote_pattern = r\"'(?P<G_SQUOTE>(?:(?!(?<!\\\\)').)*)'\"\n\n        # .thing\n        self.dot_property_pattern = rf\"\\.(?P<G_PROP>{self.key_pattern})\"\n\n        self.slice_list_pattern = (\n            r\"(?P<G_LSLICE_START>\\-?\\d*)\\s*\"\n            r\":\\s*(?P<G_LSLICE_STOP>\\-?\\d*)\\s*\"\n            r\"(?::\\s*(?P<G_LSLICE_STEP>\\-?\\d*))?\"\n        )\n\n        # /pattern/ or /pattern/flags\n        self.re_pattern = r\"/(?P<G_RE>.+?)/(?P<G_RE_FLAGS>[aims]*)\"\n\n        # func(\n        self.function_pattern = r\"(?P<G_FUNC>[a-z][a-z_0-9]+)\\(\\s*\"\n\n        self.rules = self.compile_rules()\n\n    def compile_rules(self) -> Pattern[str]:\n        \"\"\"Prepare regular expression rules.\"\"\"\n        env_tokens = [\n            (TOKEN_ROOT, self.env.root_token),\n            (TOKEN_FAKE_ROOT, self.env.fake_root_token),\n            (TOKEN_SELF, self.env.self_token),\n            (TOKEN_KEY, self.env.key_token),\n            (TOKEN_UNION, self.env.union_token),\n            (TOKEN_INTERSECTION, self.env.intersection_token),\n            (TOKEN_FILTER_CONTEXT, self.env.filter_context_token),\n            (TOKEN_KEYS, self.env.keys_selector_token),\n        ]\n\n        rules = [\n            (TOKEN_DOUBLE_QUOTE_STRING, self.double_quote_pattern),\n            (TOKEN_SINGLE_QUOTE_STRING, self.single_quote_pattern),\n            (TOKEN_RE_PATTERN, self.re_pattern),\n            (TOKEN_LIST_SLICE, self.slice_list_pattern),\n            (TOKEN_FUNCTION, self.function_pattern),\n            (TOKEN_DOT_PROPERTY, self.dot_property_pattern),\n            (TOKEN_FLOAT, r\"-?\\d+\\.\\d*(?:[eE][+-]?\\d+)?\"),\n            (TOKEN_INT, r\"-?\\d+(?P<G_EXP>[eE][+\\-]?\\d+)?\\b\"),\n            (TOKEN_DDOT, r\"\\.\\.\"),\n            (TOKEN_AND, self.logical_and_pattern),\n            (TOKEN_OR, self.logical_or_pattern),\n            *[\n                (token, re.escape(pattern))\n                for token, pattern in sorted(\n                    env_tokens, key=lambda x: len(x[1]), reverse=True\n                )\n                if pattern\n            ],\n            (TOKEN_WILD, r\"\\*\"),\n            (TOKEN_FILTER, r\"\\?\"),\n            (TOKEN_IN, r\"in\"),\n            (TOKEN_TRUE, r\"[Tt]rue\"),\n            (TOKEN_FALSE, r\"[Ff]alse\"),\n            (TOKEN_NIL, r\"[Nn]il\"),\n            (TOKEN_NULL, r\"[Nn]ull\"),\n            (TOKEN_NONE, r\"[Nn]one\"),\n            (TOKEN_CONTAINS, r\"contains\"),\n            (TOKEN_UNDEFINED, r\"undefined\"),\n            (TOKEN_MISSING, r\"missing\"),\n            (TOKEN_LIST_START, r\"\\[\"),\n            (TOKEN_RBRACKET, r\"]\"),\n            (TOKEN_COMMA, r\",\"),\n            (TOKEN_EQ, r\"==\"),\n            (TOKEN_NE, r\"!=\"),\n            (TOKEN_LG, r\"<>\"),\n            (TOKEN_LE, r\"<=\"),\n            (TOKEN_GE, r\">=\"),\n            (TOKEN_RE, r\"=~\"),\n            (TOKEN_LT, r\"<\"),\n            (TOKEN_GT, r\">\"),\n            (TOKEN_NOT, self.logical_not_pattern),\n            (TOKEN_BARE_PROPERTY, self.key_pattern),\n            (TOKEN_LPAREN, r\"\\(\"),\n            (TOKEN_RPAREN, r\"\\)\"),\n            (TOKEN_SKIP, r\"[ \\n\\t\\r\\.]+\"),\n            (TOKEN_ILLEGAL, r\".\"),\n        ]\n\n        return re.compile(\n            \"|\".join(f\"(?P<{token}>{pattern})\" for token, pattern in rules),\n            re.DOTALL,\n        )\n\n    def tokenize(self, path: str) -> Iterator[Token]:  # noqa PLR0912\n        \"\"\"Generate a sequence of tokens from a JSONPath string.\"\"\"\n        _token = partial(Token, path=path)\n\n        for match in self.rules.finditer(path):\n            kind = match.lastgroup\n            assert kind is not None\n\n            if kind == TOKEN_DOT_PROPERTY:\n                yield _token(\n                    kind=TOKEN_PROPERTY,\n                    value=match.group(\"G_PROP\"),\n                    index=match.start(\"G_PROP\"),\n                )\n            elif kind == TOKEN_BARE_PROPERTY:\n                yield _token(\n                    kind=TOKEN_BARE_PROPERTY,\n                    value=match.group(),\n                    index=match.start(),\n                )\n            elif kind == TOKEN_LIST_SLICE:\n                yield _token(\n                    kind=TOKEN_SLICE_START,\n                    value=match.group(\"G_LSLICE_START\"),\n                    index=match.start(\"G_LSLICE_START\"),\n                )\n                yield _token(\n                    kind=TOKEN_SLICE_STOP,\n                    value=match.group(\"G_LSLICE_STOP\"),\n                    index=match.start(\"G_LSLICE_STOP\"),\n                )\n                yield _token(\n                    kind=TOKEN_SLICE_STEP,\n                    value=match.group(\"G_LSLICE_STEP\") or \"\",\n                    index=match.start(\"G_LSLICE_STEP\"),\n                )\n            elif kind == TOKEN_DOUBLE_QUOTE_STRING:\n                yield _token(\n                    kind=TOKEN_DOUBLE_QUOTE_STRING,\n                    value=match.group(\"G_DQUOTE\"),\n                    index=match.start(\"G_DQUOTE\"),\n                )\n            elif kind == TOKEN_SINGLE_QUOTE_STRING:\n                yield _token(\n                    kind=TOKEN_SINGLE_QUOTE_STRING,\n                    value=match.group(\"G_SQUOTE\"),\n                    index=match.start(\"G_SQUOTE\"),\n                )\n            elif kind == TOKEN_INT:\n                if match.group(\"G_EXP\") and match.group(\"G_EXP\")[1] == \"-\":\n                    yield _token(\n                        kind=TOKEN_FLOAT,\n                        value=match.group(),\n                        index=match.start(),\n                    )\n                else:\n                    yield _token(\n                        kind=TOKEN_INT,\n                        value=match.group(),\n                        index=match.start(),\n                    )\n            elif kind == TOKEN_RE_PATTERN:\n                yield _token(\n                    kind=TOKEN_RE_PATTERN,\n                    value=match.group(\"G_RE\"),\n                    index=match.start(\"G_RE\"),\n                )\n                yield _token(\n                    TOKEN_RE_FLAGS,\n                    value=match.group(\"G_RE_FLAGS\"),\n                    index=match.start(\"G_RE_FLAGS\"),\n                )\n            elif kind in (TOKEN_NONE, TOKEN_NULL):\n                yield _token(\n                    kind=TOKEN_NIL,\n                    value=match.group(),\n                    index=match.start(),\n                )\n            elif kind == TOKEN_FUNCTION:\n                yield _token(\n                    kind=TOKEN_FUNCTION,\n                    value=match.group(\"G_FUNC\"),\n                    index=match.start(\"G_FUNC\"),\n                )\n            elif kind == TOKEN_SKIP:\n                continue\n            elif kind == TOKEN_ILLEGAL:\n                raise JSONPathSyntaxError(\n                    f\"unexpected token {match.group()!r}\",\n                    token=_token(\n                        TOKEN_ILLEGAL,\n                        value=match.group(),\n                        index=match.start(),\n                    ),\n                )\n            else:\n                yield _token(\n                    kind=kind,\n                    value=match.group(),\n                    index=match.start(),\n                )\n",
    "jsonpath/parse.py": "\"\"\"The default JSONPath parser.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport re\nfrom typing import TYPE_CHECKING\nfrom typing import Callable\nfrom typing import Dict\nfrom typing import Iterable\nfrom typing import List\nfrom typing import Optional\nfrom typing import Union\n\nfrom jsonpath.function_extensions.filter_function import ExpressionType\nfrom jsonpath.function_extensions.filter_function import FilterFunction\n\nfrom .exceptions import JSONPathSyntaxError\nfrom .exceptions import JSONPathTypeError\nfrom .filter import CURRENT_KEY\nfrom .filter import FALSE\nfrom .filter import NIL\nfrom .filter import TRUE\nfrom .filter import UNDEFINED_LITERAL\nfrom .filter import BooleanExpression\nfrom .filter import FilterContextPath\nfrom .filter import FilterExpression\nfrom .filter import FloatLiteral\nfrom .filter import FunctionExtension\nfrom .filter import InfixExpression\nfrom .filter import IntegerLiteral\nfrom .filter import ListLiteral\nfrom .filter import Literal\nfrom .filter import Nil\nfrom .filter import Path\nfrom .filter import PrefixExpression\nfrom .filter import RegexLiteral\nfrom .filter import RootPath\nfrom .filter import SelfPath\nfrom .filter import StringLiteral\nfrom .path import JSONPath\nfrom .selectors import Filter\nfrom .selectors import IndexSelector\nfrom .selectors import JSONPathSelector\nfrom .selectors import KeysSelector\nfrom .selectors import ListSelector\nfrom .selectors import PropertySelector\nfrom .selectors import RecursiveDescentSelector\nfrom .selectors import SliceSelector\nfrom .selectors import WildSelector\nfrom .token import TOKEN_AND\nfrom .token import TOKEN_BARE_PROPERTY\nfrom .token import TOKEN_COMMA\nfrom .token import TOKEN_CONTAINS\nfrom .token import TOKEN_DDOT\nfrom .token import TOKEN_DOUBLE_QUOTE_STRING\nfrom .token import TOKEN_EOF\nfrom .token import TOKEN_EQ\nfrom .token import TOKEN_FAKE_ROOT\nfrom .token import TOKEN_FALSE\nfrom .token import TOKEN_FILTER\nfrom .token import TOKEN_FILTER_CONTEXT\nfrom .token import TOKEN_FLOAT\nfrom .token import TOKEN_FUNCTION\nfrom .token import TOKEN_GE\nfrom .token import TOKEN_GT\nfrom .token import TOKEN_IN\nfrom .token import TOKEN_INT\nfrom .token import TOKEN_INTERSECTION\nfrom .token import TOKEN_KEY\nfrom .token import TOKEN_KEYS\nfrom .token import TOKEN_LE\nfrom .token import TOKEN_LG\nfrom .token import TOKEN_LIST_START\nfrom .token import TOKEN_LPAREN\nfrom .token import TOKEN_LT\nfrom .token import TOKEN_MISSING\nfrom .token import TOKEN_NE\nfrom .token import TOKEN_NIL\nfrom .token import TOKEN_NONE\nfrom .token import TOKEN_NOT\nfrom .token import TOKEN_NULL\nfrom .token import TOKEN_OR\nfrom .token import TOKEN_PROPERTY\nfrom .token import TOKEN_RBRACKET\nfrom .token import TOKEN_RE\nfrom .token import TOKEN_RE_FLAGS\nfrom .token import TOKEN_RE_PATTERN\nfrom .token import TOKEN_ROOT\nfrom .token import TOKEN_RPAREN\nfrom .token import TOKEN_SELF\nfrom .token import TOKEN_SINGLE_QUOTE_STRING\nfrom .token import TOKEN_SLICE_START\nfrom .token import TOKEN_SLICE_STEP\nfrom .token import TOKEN_SLICE_STOP\nfrom .token import TOKEN_TRUE\nfrom .token import TOKEN_UNDEFINED\nfrom .token import TOKEN_UNION\nfrom .token import TOKEN_WILD\nfrom .token import Token\n\nif TYPE_CHECKING:\n    from .env import JSONPathEnvironment\n    from .stream import TokenStream\n\n# ruff: noqa: D102\n\nINVALID_NAME_SELECTOR_CHARS = [\n    \"\\x00\",\n    \"\\x01\",\n    \"\\x02\",\n    \"\\x03\",\n    \"\\x04\",\n    \"\\x05\",\n    \"\\x06\",\n    \"\\x07\",\n    \"\\x08\",\n    \"\\t\",\n    \"\\n\",\n    \"\\x0b\",\n    \"\\x0c\",\n    \"\\r\",\n    \"\\x0e\",\n    \"\\x0f\",\n    \"\\x10\",\n    \"\\x11\",\n    \"\\x12\",\n    \"\\x13\",\n    \"\\x14\",\n    \"\\x15\",\n    \"\\x16\",\n    \"\\x17\",\n    \"\\x18\",\n    \"\\x19\",\n    \"\\x1a\",\n    \"\\x1b\",\n    \"\\x1c\",\n    \"\\x1d\",\n    \"\\x1e\",\n    \"\\x1f\",\n]\n\n\nclass Parser:\n    \"\"\"A JSONPath parser bound to a JSONPathEnvironment.\"\"\"\n\n    PRECEDENCE_LOWEST = 1\n    PRECEDENCE_LOGICALRIGHT = 2\n    PRECEDENCE_LOGICAL_OR = 3\n    PRECEDENCE_LOGICAL_AND = 4\n    PRECEDENCE_RELATIONAL = 5\n    PRECEDENCE_MEMBERSHIP = 6\n    PRECEDENCE_PREFIX = 7\n\n    PRECEDENCES = {\n        TOKEN_AND: PRECEDENCE_LOGICAL_AND,\n        TOKEN_CONTAINS: PRECEDENCE_MEMBERSHIP,\n        TOKEN_EQ: PRECEDENCE_RELATIONAL,\n        TOKEN_GE: PRECEDENCE_RELATIONAL,\n        TOKEN_GT: PRECEDENCE_RELATIONAL,\n        TOKEN_IN: PRECEDENCE_MEMBERSHIP,\n        TOKEN_LE: PRECEDENCE_RELATIONAL,\n        TOKEN_LG: PRECEDENCE_RELATIONAL,\n        TOKEN_LT: PRECEDENCE_RELATIONAL,\n        TOKEN_NE: PRECEDENCE_RELATIONAL,\n        TOKEN_NOT: PRECEDENCE_PREFIX,\n        TOKEN_OR: PRECEDENCE_LOGICAL_OR,\n        TOKEN_RE: PRECEDENCE_RELATIONAL,\n        TOKEN_RPAREN: PRECEDENCE_LOWEST,\n    }\n\n    # Mapping of operator token to canonical string.\n    BINARY_OPERATORS = {\n        TOKEN_AND: \"&&\",\n        TOKEN_CONTAINS: \"contains\",\n        TOKEN_EQ: \"==\",\n        TOKEN_GE: \">=\",\n        TOKEN_GT: \">\",\n        TOKEN_IN: \"in\",\n        TOKEN_LE: \"<=\",\n        TOKEN_LG: \"<>\",\n        TOKEN_LT: \"<\",\n        TOKEN_NE: \"!=\",\n        TOKEN_OR: \"||\",\n        TOKEN_RE: \"=~\",\n    }\n\n    COMPARISON_OPERATORS = frozenset(\n        [\n            \"==\",\n            \">=\",\n            \">\",\n            \"<=\",\n            \"<\",\n            \"!=\",\n            \"=~\",\n        ]\n    )\n\n    # Infix operators that accept filter expression literals.\n    INFIX_LITERAL_OPERATORS = frozenset(\n        [\n            \"==\",\n            \">=\",\n            \">\",\n            \"<=\",\n            \"<\",\n            \"!=\",\n            \"<>\",\n            \"=~\",\n            \"in\",\n            \"contains\",\n        ]\n    )\n\n    PREFIX_OPERATORS = frozenset(\n        [\n            TOKEN_NOT,\n        ]\n    )\n\n    RE_FLAG_MAP = {\n        \"a\": re.A,\n        \"i\": re.I,\n        \"m\": re.M,\n        \"s\": re.S,\n    }\n\n    _INVALID_NAME_SELECTOR_CHARS = f\"[{''.join(INVALID_NAME_SELECTOR_CHARS)}]\"\n    RE_INVALID_NAME_SELECTOR = re.compile(\n        rf'(?:(?!(?<!\\\\)\"){_INVALID_NAME_SELECTOR_CHARS})'\n    )\n\n    def __init__(self, *, env: JSONPathEnvironment) -> None:\n        self.env = env\n\n        self.token_map: Dict[str, Callable[[TokenStream], FilterExpression]] = {\n            TOKEN_DOUBLE_QUOTE_STRING: self.parse_string_literal,\n            TOKEN_FAKE_ROOT: self.parse_root_path,\n            TOKEN_FALSE: self.parse_boolean,\n            TOKEN_FILTER_CONTEXT: self.parse_filter_context_path,\n            TOKEN_FLOAT: self.parse_float_literal,\n            TOKEN_FUNCTION: self.parse_function_extension,\n            TOKEN_INT: self.parse_integer_literal,\n            TOKEN_KEY: self.parse_current_key,\n            TOKEN_LIST_START: self.parse_list_literal,\n            TOKEN_LPAREN: self.parse_grouped_expression,\n            TOKEN_MISSING: self.parse_undefined,\n            TOKEN_NIL: self.parse_nil,\n            TOKEN_NONE: self.parse_nil,\n            TOKEN_NOT: self.parse_prefix_expression,\n            TOKEN_NULL: self.parse_nil,\n            TOKEN_RE_PATTERN: self.parse_regex,\n            TOKEN_ROOT: self.parse_root_path,\n            TOKEN_SELF: self.parse_self_path,\n            TOKEN_SINGLE_QUOTE_STRING: self.parse_string_literal,\n            TOKEN_TRUE: self.parse_boolean,\n            TOKEN_UNDEFINED: self.parse_undefined,\n        }\n\n        self.list_item_map: Dict[str, Callable[[TokenStream], FilterExpression]] = {\n            TOKEN_FALSE: self.parse_boolean,\n            TOKEN_FLOAT: self.parse_float_literal,\n            TOKEN_INT: self.parse_integer_literal,\n            TOKEN_NIL: self.parse_nil,\n            TOKEN_NONE: self.parse_nil,\n            TOKEN_NULL: self.parse_nil,\n            TOKEN_DOUBLE_QUOTE_STRING: self.parse_string_literal,\n            TOKEN_SINGLE_QUOTE_STRING: self.parse_string_literal,\n            TOKEN_TRUE: self.parse_boolean,\n        }\n\n        self.function_argument_map: Dict[\n            str, Callable[[TokenStream], FilterExpression]\n        ] = {\n            TOKEN_DOUBLE_QUOTE_STRING: self.parse_string_literal,\n            TOKEN_FAKE_ROOT: self.parse_root_path,\n            TOKEN_FALSE: self.parse_boolean,\n            TOKEN_FILTER_CONTEXT: self.parse_filter_context_path,\n            TOKEN_FLOAT: self.parse_float_literal,\n            TOKEN_FUNCTION: self.parse_function_extension,\n            TOKEN_INT: self.parse_integer_literal,\n            TOKEN_KEY: self.parse_current_key,\n            TOKEN_NIL: self.parse_nil,\n            TOKEN_NONE: self.parse_nil,\n            TOKEN_NULL: self.parse_nil,\n            TOKEN_ROOT: self.parse_root_path,\n            TOKEN_SELF: self.parse_self_path,\n            TOKEN_SINGLE_QUOTE_STRING: self.parse_string_literal,\n            TOKEN_TRUE: self.parse_boolean,\n        }\n\n    def parse(self, stream: TokenStream) -> Iterable[JSONPathSelector]:\n        \"\"\"Parse a JSONPath from a stream of tokens.\"\"\"\n        if stream.current.kind in {TOKEN_ROOT, TOKEN_FAKE_ROOT}:\n            stream.next_token()\n        yield from self.parse_path(stream, in_filter=False)\n\n        if stream.current.kind not in (TOKEN_EOF, TOKEN_INTERSECTION, TOKEN_UNION):\n            raise JSONPathSyntaxError(\n                f\"unexpected token {stream.current.value!r}\",\n                token=stream.current,\n            )\n\n    def parse_path(\n        self,\n        stream: TokenStream,\n        *,\n        in_filter: bool = False,\n    ) -> Iterable[JSONPathSelector]:\n        \"\"\"Parse a top-level JSONPath, or one that is nested in a filter.\"\"\"\n        while True:\n            if stream.current.kind in (TOKEN_PROPERTY, TOKEN_BARE_PROPERTY):\n                yield PropertySelector(\n                    env=self.env,\n                    token=stream.current,\n                    name=stream.current.value,\n                    shorthand=True,\n                )\n            elif stream.current.kind == TOKEN_SLICE_START:\n                yield self.parse_slice(stream)\n            elif stream.current.kind == TOKEN_WILD:\n                yield WildSelector(\n                    env=self.env,\n                    token=stream.current,\n                    shorthand=True,\n                )\n            elif stream.current.kind == TOKEN_KEYS:\n                yield KeysSelector(\n                    env=self.env,\n                    token=stream.current,\n                    shorthand=True,\n                )\n            elif stream.current.kind == TOKEN_DDOT:\n                yield RecursiveDescentSelector(\n                    env=self.env,\n                    token=stream.current,\n                )\n            elif stream.current.kind == TOKEN_LIST_START:\n                yield self.parse_selector_list(stream)\n            else:\n                if in_filter:\n                    stream.push(stream.current)\n                break\n\n            stream.next_token()\n\n    def parse_slice(self, stream: TokenStream) -> SliceSelector:\n        \"\"\"Parse a slice JSONPath expression from a stream of tokens.\"\"\"\n        start_token = stream.next_token()\n        stream.expect(TOKEN_SLICE_STOP)\n        stop_token = stream.next_token()\n        stream.expect(TOKEN_SLICE_STEP)\n        step_token = stream.current\n\n        if not start_token.value:\n            start: Optional[int] = None\n        else:\n            start = int(start_token.value)\n\n        if not stop_token.value:\n            stop: Optional[int] = None\n        else:\n            stop = int(stop_token.value)\n\n        if not step_token.value:\n            step: Optional[int] = None\n        else:\n            step = int(step_token.value)\n\n        return SliceSelector(\n            env=self.env,\n            token=start_token,\n            start=start,\n            stop=stop,\n            step=step,\n        )\n\n    def parse_selector_list(self, stream: TokenStream) -> ListSelector:  # noqa: PLR0912\n        \"\"\"Parse a comma separated list JSONPath selectors from a stream of tokens.\"\"\"\n        tok = stream.next_token()\n        list_items: List[\n            Union[\n                IndexSelector,\n                KeysSelector,\n                PropertySelector,\n                SliceSelector,\n                WildSelector,\n                Filter,\n            ]\n        ] = []\n\n        while stream.current.kind != TOKEN_RBRACKET:\n            if stream.current.kind == TOKEN_INT:\n                if (\n                    len(stream.current.value) > 1\n                    and stream.current.value.startswith(\"0\")\n                ) or stream.current.value.startswith(\"-0\"):\n                    raise JSONPathSyntaxError(\n                        \"leading zero in index selector\", token=stream.current\n                    )\n                list_items.append(\n                    IndexSelector(\n                        env=self.env,\n                        token=stream.current,\n                        index=int(stream.current.value),\n                    )\n                )\n            elif stream.current.kind == TOKEN_BARE_PROPERTY:\n                list_items.append(\n                    PropertySelector(\n                        env=self.env,\n                        token=stream.current,\n                        name=stream.current.value,\n                        shorthand=False,\n                    ),\n                )\n            elif stream.current.kind == TOKEN_KEYS:\n                list_items.append(\n                    KeysSelector(\n                        env=self.env,\n                        token=stream.current,\n                        shorthand=False,\n                    )\n                )\n            elif stream.current.kind in (\n                TOKEN_DOUBLE_QUOTE_STRING,\n                TOKEN_SINGLE_QUOTE_STRING,\n            ):\n                if self.RE_INVALID_NAME_SELECTOR.search(stream.current.value):\n                    raise JSONPathSyntaxError(\n                        f\"invalid name selector {stream.current.value!r}\",\n                        token=stream.current,\n                    )\n\n                list_items.append(\n                    PropertySelector(\n                        env=self.env,\n                        token=stream.current,\n                        name=self._decode_string_literal(stream.current),\n                        shorthand=False,\n                    ),\n                )\n            elif stream.current.kind == TOKEN_SLICE_START:\n                list_items.append(self.parse_slice(stream))\n            elif stream.current.kind == TOKEN_WILD:\n                list_items.append(\n                    WildSelector(\n                        env=self.env,\n                        token=stream.current,\n                        shorthand=False,\n                    )\n                )\n            elif stream.current.kind == TOKEN_FILTER:\n                list_items.append(self.parse_filter(stream))\n            elif stream.current.kind == TOKEN_EOF:\n                raise JSONPathSyntaxError(\n                    \"unexpected end of query\", token=stream.current\n                )\n            else:\n                raise JSONPathSyntaxError(\n                    f\"unexpected token in bracketed selection {stream.current.kind!r}\",\n                    token=stream.current,\n                )\n\n            if stream.peek.kind == TOKEN_EOF:\n                raise JSONPathSyntaxError(\n                    \"unexpected end of selector list\",\n                    token=stream.current,\n                )\n\n            if stream.peek.kind != TOKEN_RBRACKET:\n                # TODO: error message .. expected a comma or logical operator\n                stream.expect_peek(TOKEN_COMMA)\n                stream.next_token()\n\n                if stream.peek.kind == TOKEN_RBRACKET:\n                    raise JSONPathSyntaxError(\n                        \"unexpected trailing comma\",\n                        token=stream.peek,\n                    )\n\n            stream.next_token()\n\n        if not list_items:\n            raise JSONPathSyntaxError(\"empty bracketed segment\", token=tok)\n\n        return ListSelector(env=self.env, token=tok, items=list_items)\n\n    def parse_filter(self, stream: TokenStream) -> Filter:\n        tok = stream.next_token()\n        expr = self.parse_filter_selector(stream)\n\n        if self.env.well_typed and isinstance(expr, FunctionExtension):\n            func = self.env.function_extensions.get(expr.name)\n            if (\n                func\n                and isinstance(func, FilterFunction)\n                and func.return_type == ExpressionType.VALUE\n            ):\n                raise JSONPathTypeError(\n                    f\"result of {expr.name}() must be compared\", token=tok\n                )\n\n        if isinstance(expr, (Literal, Nil)):\n            raise JSONPathSyntaxError(\n                \"filter expression literals outside of \"\n                \"function expressions must be compared\",\n                token=tok,\n            )\n\n        return Filter(env=self.env, token=tok, expression=BooleanExpression(expr))\n\n    def parse_boolean(self, stream: TokenStream) -> FilterExpression:\n        if stream.current.kind == TOKEN_TRUE:\n            return TRUE\n        return FALSE\n\n    def parse_nil(self, _: TokenStream) -> FilterExpression:\n        return NIL\n\n    def parse_undefined(self, _: TokenStream) -> FilterExpression:\n        return UNDEFINED_LITERAL\n\n    def parse_string_literal(self, stream: TokenStream) -> FilterExpression:\n        return StringLiteral(value=self._decode_string_literal(stream.current))\n\n    def parse_integer_literal(self, stream: TokenStream) -> FilterExpression:\n        # Convert to float first to handle scientific notation.\n        return IntegerLiteral(value=int(float(stream.current.value)))\n\n    def parse_float_literal(self, stream: TokenStream) -> FilterExpression:\n        return FloatLiteral(value=float(stream.current.value))\n\n    def parse_prefix_expression(self, stream: TokenStream) -> FilterExpression:\n        tok = stream.next_token()\n        assert tok.kind == TOKEN_NOT\n        return PrefixExpression(\n            operator=\"!\",\n            right=self.parse_filter_selector(stream, precedence=self.PRECEDENCE_PREFIX),\n        )\n\n    def parse_infix_expression(\n        self, stream: TokenStream, left: FilterExpression\n    ) -> FilterExpression:\n        tok = stream.next_token()\n        precedence = self.PRECEDENCES.get(tok.kind, self.PRECEDENCE_LOWEST)\n        right = self.parse_filter_selector(stream, precedence)\n        operator = self.BINARY_OPERATORS[tok.kind]\n\n        if self.env.well_typed and operator in self.COMPARISON_OPERATORS:\n            self._raise_for_non_comparable_function(left, tok)\n            self._raise_for_non_comparable_function(right, tok)\n\n        if operator not in self.INFIX_LITERAL_OPERATORS:\n            if isinstance(left, (Literal, Nil)):\n                raise JSONPathSyntaxError(\n                    \"filter expression literals outside of \"\n                    \"function expressions must be compared\",\n                    token=tok,\n                )\n            if isinstance(right, (Literal, Nil)):\n                raise JSONPathSyntaxError(\n                    \"filter expression literals outside of \"\n                    \"function expressions must be compared\",\n                    token=tok,\n                )\n\n        return InfixExpression(left, operator, right)\n\n    def parse_grouped_expression(self, stream: TokenStream) -> FilterExpression:\n        stream.next_token()\n        expr = self.parse_filter_selector(stream)\n        stream.next_token()\n\n        while stream.current.kind != TOKEN_RPAREN:\n            if stream.current.kind == TOKEN_EOF:\n                raise JSONPathSyntaxError(\n                    \"unbalanced parentheses\", token=stream.current\n                )\n\n            if stream.current.kind not in self.BINARY_OPERATORS:\n                raise JSONPathSyntaxError(\n                    f\"expected an expression, found '{stream.current.value}'\",\n                    token=stream.current,\n                )\n\n            expr = self.parse_infix_expression(stream, expr)\n\n        stream.expect(TOKEN_RPAREN)\n        return expr\n\n    def parse_root_path(self, stream: TokenStream) -> FilterExpression:\n        root = stream.next_token()\n        return RootPath(\n            JSONPath(\n                env=self.env,\n                selectors=self.parse_path(stream, in_filter=True),\n                fake_root=root.kind == TOKEN_FAKE_ROOT,\n            )\n        )\n\n    def parse_self_path(self, stream: TokenStream) -> FilterExpression:\n        stream.next_token()\n        return SelfPath(\n            JSONPath(env=self.env, selectors=self.parse_path(stream, in_filter=True))\n        )\n\n    def parse_current_key(self, _: TokenStream) -> FilterExpression:\n        return CURRENT_KEY\n\n    def parse_filter_context_path(self, stream: TokenStream) -> FilterExpression:\n        stream.next_token()\n        return FilterContextPath(\n            JSONPath(env=self.env, selectors=self.parse_path(stream, in_filter=True))\n        )\n\n    def parse_regex(self, stream: TokenStream) -> FilterExpression:\n        pattern = stream.current.value\n        flags = 0\n        if stream.peek.kind == TOKEN_RE_FLAGS:\n            stream.next_token()\n            for flag in set(stream.current.value):\n                flags |= self.RE_FLAG_MAP[flag]\n        return RegexLiteral(value=re.compile(pattern, flags))\n\n    def parse_list_literal(self, stream: TokenStream) -> FilterExpression:\n        stream.next_token()\n        list_items: List[FilterExpression] = []\n\n        while stream.current.kind != TOKEN_RBRACKET:\n            try:\n                list_items.append(self.list_item_map[stream.current.kind](stream))\n            except KeyError as err:\n                raise JSONPathSyntaxError(\n                    f\"unexpected {stream.current.value!r}\",\n                    token=stream.current,\n                ) from err\n\n            if stream.peek.kind != TOKEN_RBRACKET:\n                stream.expect_peek(TOKEN_COMMA)\n                stream.next_token()\n\n            stream.next_token()\n\n        return ListLiteral(list_items)\n\n    def parse_function_extension(self, stream: TokenStream) -> FilterExpression:\n        function_arguments: List[FilterExpression] = []\n        tok = stream.next_token()\n\n        while stream.current.kind != TOKEN_RPAREN:\n            try:\n                func = self.function_argument_map[stream.current.kind]\n            except KeyError as err:\n                raise JSONPathSyntaxError(\n                    f\"unexpected {stream.current.value!r}\",\n                    token=stream.current,\n                ) from err\n\n            expr = func(stream)\n\n            # The argument could be a comparison or logical expression\n            peek_kind = stream.peek.kind\n            while peek_kind in self.BINARY_OPERATORS:\n                stream.next_token()\n                expr = self.parse_infix_expression(stream, expr)\n                peek_kind = stream.peek.kind\n\n            function_arguments.append(expr)\n\n            if stream.peek.kind != TOKEN_RPAREN:\n                stream.expect_peek(TOKEN_COMMA)\n                stream.next_token()\n\n            stream.next_token()\n\n        return FunctionExtension(\n            tok.value,\n            self.env.validate_function_extension_signature(tok, function_arguments),\n        )\n\n    def parse_filter_selector(\n        self, stream: TokenStream, precedence: int = PRECEDENCE_LOWEST\n    ) -> FilterExpression:\n        try:\n            left = self.token_map[stream.current.kind](stream)\n        except KeyError as err:\n            if stream.current.kind in (TOKEN_EOF, TOKEN_RBRACKET):\n                msg = \"end of expression\"\n            else:\n                msg = repr(stream.current.value)\n            raise JSONPathSyntaxError(\n                f\"unexpected {msg}\", token=stream.current\n            ) from err\n\n        while True:\n            peek_kind = stream.peek.kind\n            if (\n                peek_kind in (TOKEN_EOF, TOKEN_RBRACKET)\n                or self.PRECEDENCES.get(peek_kind, self.PRECEDENCE_LOWEST) < precedence\n            ):\n                break\n\n            if peek_kind not in self.BINARY_OPERATORS:\n                return left\n\n            stream.next_token()\n            left = self.parse_infix_expression(stream, left)\n\n        return left\n\n    def _decode_string_literal(self, token: Token) -> str:\n        if self.env.unicode_escape:\n            if token.kind == TOKEN_SINGLE_QUOTE_STRING:\n                value = token.value.replace('\"', '\\\\\"').replace(\"\\\\'\", \"'\")\n            else:\n                value = token.value\n            try:\n                rv = json.loads(f'\"{value}\"')\n                assert isinstance(rv, str)\n                return rv\n            except json.JSONDecodeError as err:\n                raise JSONPathSyntaxError(str(err).split(\":\")[1], token=token) from None\n\n        return token.value\n\n    def _raise_for_non_comparable_function(\n        self, expr: FilterExpression, token: Token\n    ) -> None:\n        if isinstance(expr, Path) and not expr.path.singular_query():\n            raise JSONPathTypeError(\"non-singular query is not comparable\", token=token)\n\n        if isinstance(expr, FunctionExtension):\n            func = self.env.function_extensions.get(expr.name)\n            if (\n                isinstance(func, FilterFunction)\n                and func.return_type != ExpressionType.VALUE\n            ):\n                raise JSONPathTypeError(\n                    f\"result of {expr.name}() is not comparable\", token\n                )\n"
  },
  "GT_src_dict": {
    "jsonpath/env.py": {
      "JSONPathEnvironment.__init__": {
        "code": "    def __init__(self, *, filter_caching: bool=True, unicode_escape: bool=True, well_typed: bool=True) -> None:\n        \"\"\"Initialize a new instance of the JSONPathEnvironment class, which configures the settings for JSONPath tokenization, parsing, and resolution behavior. This constructor sets up the lexer, parser, and function extensions based on provided arguments that control filter expression caching, UTF-16 escape sequence decoding, and well-typedness checks on filter function expressions.\n\nParameters:\n    filter_caching (bool): If True, enable caching of filter expressions for performance. Defaults to True.\n    unicode_escape (bool): If True, enable decoding of UTF-16 escape sequences found in JSONPath string literals. Defaults to True.\n    well_typed (bool): If True, enable checks for well-typedness of filter function expressions. Defaults to True.\n\nAttributes:\n    lexer (Lexer): An instance of the lexer defined by lexer_class, used for tokenizing JSONPath strings.\n    parser (Parser): An instance of the parser defined by parser_class, used for parsing tokens.\n    function_extensions (Dict[str, Callable[..., Any]]): A dictionary to store function extensions available for filters, initialized by calling `setup_function_extensions()`.\n\nConstants:\n    lexer_class (Type[Lexer]): A class used to create the lexer, defaulting to the Lexer implementation.\n    parser_class (Type[Parser]): A class used to create the parser, defaulting to the Parser implementation.\"\"\"\n        self.filter_caching: bool = filter_caching\n        'Enable or disable filter expression caching.'\n        self.unicode_escape: bool = unicode_escape\n        'Enable or disable decoding of UTF-16 escape sequences found in\\n        JSONPath string literals.'\n        self.well_typed: bool = well_typed\n        'Control well-typedness checks on filter function expressions.'\n        self.lexer: Lexer = self.lexer_class(env=self)\n        'The lexer bound to this environment.'\n        self.parser: Parser = self.parser_class(env=self)\n        'The parser bound to this environment.'\n        self.function_extensions: Dict[str, Callable[..., Any]] = {}\n        'A list of function extensions available to filters.'\n        self.setup_function_extensions()",
        "docstring": "Initialize a new instance of the JSONPathEnvironment class, which configures the settings for JSONPath tokenization, parsing, and resolution behavior. This constructor sets up the lexer, parser, and function extensions based on provided arguments that control filter expression caching, UTF-16 escape sequence decoding, and well-typedness checks on filter function expressions.\n\nParameters:\n    filter_caching (bool): If True, enable caching of filter expressions for performance. Defaults to True.\n    unicode_escape (bool): If True, enable decoding of UTF-16 escape sequences found in JSONPath string literals. Defaults to True.\n    well_typed (bool): If True, enable checks for well-typedness of filter function expressions. Defaults to True.\n\nAttributes:\n    lexer (Lexer): An instance of the lexer defined by lexer_class, used for tokenizing JSONPath strings.\n    parser (Parser): An instance of the parser defined by parser_class, used for parsing tokens.\n    function_extensions (Dict[str, Callable[..., Any]]): A dictionary to store function extensions available for filters, initialized by calling `setup_function_extensions()`.\n\nConstants:\n    lexer_class (Type[Lexer]): A class used to create the lexer, defaulting to the Lexer implementation.\n    parser_class (Type[Parser]): A class used to create the parser, defaulting to the Parser implementation.",
        "signature": "def __init__(self, *, filter_caching: bool=True, unicode_escape: bool=True, well_typed: bool=True) -> None:",
        "type": "Method",
        "class_signature": "class JSONPathEnvironment:"
      },
      "JSONPathEnvironment.compile": {
        "code": "    def compile(self, path: str) -> Union[JSONPath, CompoundJSONPath]:\n        \"\"\"Compile and prepare a JSONPath expression for repeated matching against different data.\n\nThis method takes a JSONPath string, tokenizes it using the lexer, and processes the tokens with the parser to generate either a `JSONPath` or `CompoundJSONPath` object. These objects are designed to facilitate efficient querying of JSON data structures. If the path includes union (`|`) or intersection (`&`) operators, a `CompoundJSONPath` is returned.\n\nParameters:\n    path (str): A JSONPath expression as a string that specifies the desired data selection criteria.\n\nReturns:\n    Union[JSONPath, CompoundJSONPath]: A compiled path object ready for future data matching. \n\nRaises:\n    JSONPathSyntaxError: If the provided path contains syntax errors.\n    JSONPathTypeError: If the filter functions in the path are given arguments of inappropriate types.\n\nConstants Used:\n    TOKEN_EOF: Represents the end of the token stream, defined in the token module.\n    TOKEN_FAKE_ROOT: Indicates a temporary root node, used to manage certain JSONPath evaluations.\n    TOKEN_UNION and TOKEN_INTERSECTION: Used to identify union and intersection operations in the path.\n\nThe function interacts closely with self.lexer, which is an instance of the lexer class responsible for converting the path string into tokens, and self.parser, which processes those tokens to facilitate the creation of the appropriate JSONPath object.\"\"\"\n        'Prepare a path string ready for repeated matching against different data.\\n\\n        Arguments:\\n            path: A JSONPath as a string.\\n\\n        Returns:\\n            A `JSONPath` or `CompoundJSONPath`, ready to match against some data.\\n                Expect a `CompoundJSONPath` if the path string uses the _union_ or\\n                _intersection_ operators.\\n\\n        Raises:\\n            JSONPathSyntaxError: If _path_ is invalid.\\n            JSONPathTypeError: If filter functions are given arguments of an\\n                unacceptable type.\\n        '\n        tokens = self.lexer.tokenize(path)\n        stream = TokenStream(tokens)\n        fake_root = stream.current.kind == TOKEN_FAKE_ROOT\n        _path: Union[JSONPath, CompoundJSONPath] = JSONPath(env=self, selectors=self.parser.parse(stream), fake_root=fake_root)\n        if stream.current.kind != TOKEN_EOF:\n            _path = CompoundJSONPath(env=self, path=_path)\n            while stream.current.kind != TOKEN_EOF:\n                if stream.peek.kind == TOKEN_EOF:\n                    raise JSONPathSyntaxError(f'expected a path after {stream.current.value!r}', token=stream.current)\n                if stream.current.kind == TOKEN_UNION:\n                    stream.next_token()\n                    fake_root = stream.current.kind == TOKEN_FAKE_ROOT\n                    _path = _path.union(JSONPath(env=self, selectors=self.parser.parse(stream), fake_root=fake_root))\n                elif stream.current.kind == TOKEN_INTERSECTION:\n                    stream.next_token()\n                    fake_root = stream.current.kind == TOKEN_FAKE_ROOT\n                    _path = _path.intersection(JSONPath(env=self, selectors=self.parser.parse(stream), fake_root=fake_root))\n                else:\n                    raise JSONPathSyntaxError(f'unexpected token {stream.current.value!r}', token=stream.current)\n        return _path",
        "docstring": "Compile and prepare a JSONPath expression for repeated matching against different data.\n\nThis method takes a JSONPath string, tokenizes it using the lexer, and processes the tokens with the parser to generate either a `JSONPath` or `CompoundJSONPath` object. These objects are designed to facilitate efficient querying of JSON data structures. If the path includes union (`|`) or intersection (`&`) operators, a `CompoundJSONPath` is returned.\n\nParameters:\n    path (str): A JSONPath expression as a string that specifies the desired data selection criteria.\n\nReturns:\n    Union[JSONPath, CompoundJSONPath]: A compiled path object ready for future data matching. \n\nRaises:\n    JSONPathSyntaxError: If the provided path contains syntax errors.\n    JSONPathTypeError: If the filter functions in the path are given arguments of inappropriate types.\n\nConstants Used:\n    TOKEN_EOF: Represents the end of the token stream, defined in the token module.\n    TOKEN_FAKE_ROOT: Indicates a temporary root node, used to manage certain JSONPath evaluations.\n    TOKEN_UNION and TOKEN_INTERSECTION: Used to identify union and intersection operations in the path.\n\nThe function interacts closely with self.lexer, which is an instance of the lexer class responsible for converting the path string into tokens, and self.parser, which processes those tokens to facilitate the creation of the appropriate JSONPath object.",
        "signature": "def compile(self, path: str) -> Union[JSONPath, CompoundJSONPath]:",
        "type": "Method",
        "class_signature": "class JSONPathEnvironment:"
      },
      "JSONPathEnvironment.setup_function_extensions": {
        "code": "    def setup_function_extensions(self) -> None:\n        \"\"\"Initialize function extensions for JSONPath filtering.\n\nThis method configures a set of built-in function extensions that can be used within JSONPath filter expressions. It sets up various functions, such as `length`, `count`, `match`, `search`, and type-checking functions like `isinstance`, `is`, `typeof`, and `type`, making them available for use in JSONPath queries.\n\n### Side Effects\nThe function modifies the `function_extensions` attribute of the `JSONPathEnvironment` instance by adding key-value pairs, where keys are string identifiers of the function and values are instances of their respective function classes imported from the `function_extensions` module.\n\n### Dependencies\nThis method relies on classes defined in the `function_extensions` module and assumes that these classes adhere to a defined interface for function extensions within JSONPath.\"\"\"\n        'Initialize function extensions.'\n        self.function_extensions['length'] = function_extensions.Length()\n        self.function_extensions['count'] = function_extensions.Count()\n        self.function_extensions['match'] = function_extensions.Match()\n        self.function_extensions['search'] = function_extensions.Search()\n        self.function_extensions['value'] = function_extensions.Value()\n        self.function_extensions['isinstance'] = function_extensions.IsInstance()\n        self.function_extensions['is'] = self.function_extensions['isinstance']\n        self.function_extensions['typeof'] = function_extensions.TypeOf()\n        self.function_extensions['type'] = self.function_extensions['typeof']",
        "docstring": "Initialize function extensions for JSONPath filtering.\n\nThis method configures a set of built-in function extensions that can be used within JSONPath filter expressions. It sets up various functions, such as `length`, `count`, `match`, `search`, and type-checking functions like `isinstance`, `is`, `typeof`, and `type`, making them available for use in JSONPath queries.\n\n### Side Effects\nThe function modifies the `function_extensions` attribute of the `JSONPathEnvironment` instance by adding key-value pairs, where keys are string identifiers of the function and values are instances of their respective function classes imported from the `function_extensions` module.\n\n### Dependencies\nThis method relies on classes defined in the `function_extensions` module and assumes that these classes adhere to a defined interface for function extensions within JSONPath.",
        "signature": "def setup_function_extensions(self) -> None:",
        "type": "Method",
        "class_signature": "class JSONPathEnvironment:"
      }
    },
    "jsonpath/path.py": {
      "JSONPath.__init__": {
        "code": "    def __init__(self, *, env: JSONPathEnvironment, selectors: Iterable[JSONPathSelector], fake_root: bool=False) -> None:\n        \"\"\"Initialize a `JSONPath` instance, which represents a compiled JSONPath ready for querying JSON documents.\n\nParameters:\n- `env` (JSONPathEnvironment): The environment this JSONPath is associated with, which helps define how the path should be applied in terms of syntax and behavior.\n- `selectors` (Iterable[JSONPathSelector]): An iterable of selector objects that dictate the specific path elements to be matched in a JSON document.\n- `fake_root` (bool, optional): If set to `True`, indicates that the target JSON values should be wrapped in a single-element array, allowing the root value to be selected. Defaults to `False`.\n\nAttributes:\n- `self.env`: Stores the `JSONPathEnvironment`, influencing the path evaluation.\n- `self.selectors`: A tuple representing the sequence of selectors that comprise the path for querying.\n- `self.fake_root`: A flag determining if the target values are treated as if they are within a root array.\n\nThis initializer provides the foundational setup necessary for utilizing the methods of the `JSONPath` class, enabling structured querying of JSON data.\"\"\"\n        self.env = env\n        self.selectors = tuple(selectors)\n        self.fake_root = fake_root",
        "docstring": "Initialize a `JSONPath` instance, which represents a compiled JSONPath ready for querying JSON documents.\n\nParameters:\n- `env` (JSONPathEnvironment): The environment this JSONPath is associated with, which helps define how the path should be applied in terms of syntax and behavior.\n- `selectors` (Iterable[JSONPathSelector]): An iterable of selector objects that dictate the specific path elements to be matched in a JSON document.\n- `fake_root` (bool, optional): If set to `True`, indicates that the target JSON values should be wrapped in a single-element array, allowing the root value to be selected. Defaults to `False`.\n\nAttributes:\n- `self.env`: Stores the `JSONPathEnvironment`, influencing the path evaluation.\n- `self.selectors`: A tuple representing the sequence of selectors that comprise the path for querying.\n- `self.fake_root`: A flag determining if the target values are treated as if they are within a root array.\n\nThis initializer provides the foundational setup necessary for utilizing the methods of the `JSONPath` class, enabling structured querying of JSON data.",
        "signature": "def __init__(self, *, env: JSONPathEnvironment, selectors: Iterable[JSONPathSelector], fake_root: bool=False) -> None:",
        "type": "Method",
        "class_signature": "class JSONPath:"
      },
      "JSONPath.findall": {
        "code": "    def findall(self, data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]], *, filter_context: Optional[FilterContextVars]=None) -> List[object]:\n        \"\"\"Find all objects in the provided data that match the given JSONPath.\n\nThis method processes the input `data`, which can either be a JSON document in string format, a file-like object, or a Python object (implementing `Sequence` or `Mapping`). It utilizes the `finditer` method to generate matches, returning a list containing the matched objects. If no matches are found, an empty list is returned.\n\nParameters:\n- data: A JSON document (str), a file-like object (IOBase), or a Python object (Sequence or Mapping) that can be queried.\n- filter_context: Optional mapping that provides arbitrary data to filters, accessible via the _filter context_ selector. Defaults to None.\n\nReturns:\n- A list of matched objects. If there are no matches, the list will be empty.\n\nRaises:\n- JSONPathSyntaxError: If the JSONPath expression is invalid.\n- JSONPathTypeError: If a filter expression uses types in an incompatible manner.\n\nThis method interacts with the `finditer` method to perform the actual matching and relies on the `load_data` function to process string or file inputs into JSON-compatible Python objects.\"\"\"\n        'Find all objects in `data` matching the given JSONPath `path`.\\n\\n        If `data` is a string or a file-like objects, it will be loaded\\n        using `json.loads()` and the default `JSONDecoder`.\\n\\n        Arguments:\\n            data: A JSON document or Python object implementing the `Sequence`\\n                or `Mapping` interfaces.\\n            filter_context: Arbitrary data made available to filters using\\n                the _filter context_ selector.\\n\\n        Returns:\\n            A list of matched objects. If there are no matches, the list will\\n            be empty.\\n\\n        Raises:\\n            JSONPathSyntaxError: If the path is invalid.\\n            JSONPathTypeError: If a filter expression attempts to use types in\\n                an incompatible way.\\n        '\n        return [match.obj for match in self.finditer(data, filter_context=filter_context)]",
        "docstring": "Find all objects in the provided data that match the given JSONPath.\n\nThis method processes the input `data`, which can either be a JSON document in string format, a file-like object, or a Python object (implementing `Sequence` or `Mapping`). It utilizes the `finditer` method to generate matches, returning a list containing the matched objects. If no matches are found, an empty list is returned.\n\nParameters:\n- data: A JSON document (str), a file-like object (IOBase), or a Python object (Sequence or Mapping) that can be queried.\n- filter_context: Optional mapping that provides arbitrary data to filters, accessible via the _filter context_ selector. Defaults to None.\n\nReturns:\n- A list of matched objects. If there are no matches, the list will be empty.\n\nRaises:\n- JSONPathSyntaxError: If the JSONPath expression is invalid.\n- JSONPathTypeError: If a filter expression uses types in an incompatible manner.\n\nThis method interacts with the `finditer` method to perform the actual matching and relies on the `load_data` function to process string or file inputs into JSON-compatible Python objects.",
        "signature": "def findall(self, data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]], *, filter_context: Optional[FilterContextVars]=None) -> List[object]:",
        "type": "Method",
        "class_signature": "class JSONPath:"
      },
      "JSONPath.finditer": {
        "code": "    def finditer(self, data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]], *, filter_context: Optional[FilterContextVars]=None) -> Iterable[JSONPathMatch]:\n        \"\"\"Generate `JSONPathMatch` objects for each match found using this JSONPath instance.\n\nThis method takes in a JSON document or Python object and yields matches based on the selectors defined in the JSONPath instance. The input data can be a string (representing JSON), file-like object, sequence, or mapping. If the data is a string or file-like object, it will be loaded using the `load_data` function from the `jsonpath._data` module, which handles the conversion to a Python object.\n\nParameters:\n- data (Union[str, IOBase, Sequence[Any], Mapping[str, Any]]): A JSON document or a Python object implementing the `Sequence` or `Mapping` interfaces to search for matches.\n- filter_context (Optional[FilterContextVars]): Arbitrary data made available to filters, which can be used in the selectors for context-specific filtering.\n\nReturns:\n- Iterable[JSONPathMatch]: An iterator yielding `JSONPathMatch` objects for each match found.\n\nRaises:\n- JSONPathSyntaxError: If the path defined by the selectors is invalid.\n- JSONPathTypeError: If a filter expression uses types in an incompatible way with the data.\n\nThe `_data` variable holds the processed data ready for matching. The method starts with a root match object and iteratively applies each selector to find additional matches, returning an iterable of results.\"\"\"\n        'Generate `JSONPathMatch` objects for each match.\\n\\n        If `data` is a string or a file-like objects, it will be loaded\\n        using `json.loads()` and the default `JSONDecoder`.\\n\\n        Arguments:\\n            data: A JSON document or Python object implementing the `Sequence`\\n                or `Mapping` interfaces.\\n            filter_context: Arbitrary data made available to filters using\\n                the _filter context_ selector.\\n\\n        Returns:\\n            An iterator yielding `JSONPathMatch` objects for each match.\\n\\n        Raises:\\n            JSONPathSyntaxError: If the path is invalid.\\n            JSONPathTypeError: If a filter expression attempts to use types in\\n                an incompatible way.\\n        '\n        _data = load_data(data)\n        matches: Iterable[JSONPathMatch] = [JSONPathMatch(filter_context=filter_context or {}, obj=[_data] if self.fake_root else _data, parent=None, path=self.env.root_token, parts=(), root=_data)]\n        for selector in self.selectors:\n            matches = selector.resolve(matches)\n        return matches",
        "docstring": "Generate `JSONPathMatch` objects for each match found using this JSONPath instance.\n\nThis method takes in a JSON document or Python object and yields matches based on the selectors defined in the JSONPath instance. The input data can be a string (representing JSON), file-like object, sequence, or mapping. If the data is a string or file-like object, it will be loaded using the `load_data` function from the `jsonpath._data` module, which handles the conversion to a Python object.\n\nParameters:\n- data (Union[str, IOBase, Sequence[Any], Mapping[str, Any]]): A JSON document or a Python object implementing the `Sequence` or `Mapping` interfaces to search for matches.\n- filter_context (Optional[FilterContextVars]): Arbitrary data made available to filters, which can be used in the selectors for context-specific filtering.\n\nReturns:\n- Iterable[JSONPathMatch]: An iterator yielding `JSONPathMatch` objects for each match found.\n\nRaises:\n- JSONPathSyntaxError: If the path defined by the selectors is invalid.\n- JSONPathTypeError: If a filter expression uses types in an incompatible way with the data.\n\nThe `_data` variable holds the processed data ready for matching. The method starts with a root match object and iteratively applies each selector to find additional matches, returning an iterable of results.",
        "signature": "def finditer(self, data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]], *, filter_context: Optional[FilterContextVars]=None) -> Iterable[JSONPathMatch]:",
        "type": "Method",
        "class_signature": "class JSONPath:"
      }
    },
    "jsonpath/filter.py": {},
    "jsonpath/stream.py": {
      "TokenStreamIterator.__init__": {
        "code": "        def __init__(self, stream: TokenStream):\n            \"\"\"Initialize a TokenStreamIterator instance for stepping through a TokenStream.\n\nParameters:\n- stream (TokenStream): An instance of the TokenStream class, which provides an iterable stream of tokens.\n\nThis constructor stores the provided TokenStream instance for use in the TokenStreamIterator's iteration methods. It does not return any value and has no side effects aside from establishing the relationship between the iterator and the token stream.\"\"\"\n            self.stream = stream",
        "docstring": "Initialize a TokenStreamIterator instance for stepping through a TokenStream.\n\nParameters:\n- stream (TokenStream): An instance of the TokenStream class, which provides an iterable stream of tokens.\n\nThis constructor stores the provided TokenStream instance for use in the TokenStreamIterator's iteration methods. It does not return any value and has no side effects aside from establishing the relationship between the iterator and the token stream.",
        "signature": "def __init__(self, stream: TokenStream):",
        "type": "Method",
        "class_signature": "class TokenStreamIterator:"
      }
    },
    "jsonpath/match.py": {},
    "jsonpath/selectors.py": {
      "ListSelector.resolve": {
        "code": "    def resolve(self, matches: Iterable[JSONPathMatch]) -> Iterable[JSONPathMatch]:\n        \"\"\"Resolve the list of selectors in the context of the provided matches.\n\nParameters:\n- matches (Iterable[JSONPathMatch]): An iterable of JSONPathMatch instances representing the nodes matched by preceding selectors. Each match carries information about its value, location, and parent in a JSON structure.\n\nReturns:\n- Iterable[JSONPathMatch]: An iterable of JSONPathMatch instances created by applying each selector in the items list to the provided matches. The results from each selector are concatenated and yielded.\n\nThis method interacts with the `items` attribute, which is a tuple of selectors (such as `SliceSelector`, `KeysSelector`, `IndexSelector`, etc.). Each item in the `items` list is expected to have its own `resolve` method, which is called with the current match, leading to further matches being yielded or manipulated based on the specific selector logic.\"\"\"\n        for match_ in matches:\n            for item in self.items:\n                yield from item.resolve([match_])",
        "docstring": "Resolve the list of selectors in the context of the provided matches.\n\nParameters:\n- matches (Iterable[JSONPathMatch]): An iterable of JSONPathMatch instances representing the nodes matched by preceding selectors. Each match carries information about its value, location, and parent in a JSON structure.\n\nReturns:\n- Iterable[JSONPathMatch]: An iterable of JSONPathMatch instances created by applying each selector in the items list to the provided matches. The results from each selector are concatenated and yielded.\n\nThis method interacts with the `items` attribute, which is a tuple of selectors (such as `SliceSelector`, `KeysSelector`, `IndexSelector`, etc.). Each item in the `items` list is expected to have its own `resolve` method, which is called with the current match, leading to further matches being yielded or manipulated based on the specific selector logic.",
        "signature": "def resolve(self, matches: Iterable[JSONPathMatch]) -> Iterable[JSONPathMatch]:",
        "type": "Method",
        "class_signature": "class ListSelector(JSONPathSelector):"
      }
    },
    "jsonpath/lex.py": {
      "Lexer.__init__": {
        "code": "    def __init__(self, *, env: JSONPathEnvironment) -> None:\n        \"\"\"Initialize a Lexer instance for tokenizing JSONPath strings.\n\nThis constructor configures various regular expression patterns used to\nidentify components of a JSONPath expression. It requires a single parameter,\n`env`, which is an instance of `JSONPathEnvironment`. This environment provides\naccess to specific tokens used within the lexer.\n\nAttributes:\n    env: An instance of `JSONPathEnvironment` that defines specific token \n         attributes for root, fake root, self, union, intersection, and filter \n         context within JSONPath.\n    double_quote_pattern: A regex pattern for matching double-quoted strings.\n    single_quote_pattern: A regex pattern for matching single-quoted strings.\n    dot_property_pattern: A regex pattern for matching properties prefixed with a dot.\n    slice_list_pattern: A regex pattern for matching list slicing syntax.\n    re_pattern: A regex pattern for matching regular expression patterns and their flags.\n    function_pattern: A regex pattern for matching function calls.\n    rules: A compiled regex pattern encompassing all token matching rules.\n\nThe `compile_rules` method is called to aggregate and compile all the defined\npatterns into a single regex, which will be used for tokenization during parsing.\"\"\"\n        self.env = env\n        self.double_quote_pattern = '\"(?P<G_DQUOTE>(?:(?!(?<!\\\\\\\\)\").)*)\"'\n        self.single_quote_pattern = \"'(?P<G_SQUOTE>(?:(?!(?<!\\\\\\\\)').)*)'\"\n        self.dot_property_pattern = f'\\\\.(?P<G_PROP>{self.key_pattern})'\n        self.slice_list_pattern = '(?P<G_LSLICE_START>\\\\-?\\\\d*)\\\\s*:\\\\s*(?P<G_LSLICE_STOP>\\\\-?\\\\d*)\\\\s*(?::\\\\s*(?P<G_LSLICE_STEP>\\\\-?\\\\d*))?'\n        self.re_pattern = '/(?P<G_RE>.+?)/(?P<G_RE_FLAGS>[aims]*)'\n        self.function_pattern = '(?P<G_FUNC>[a-z][a-z_0-9]+)\\\\(\\\\s*'\n        self.rules = self.compile_rules()",
        "docstring": "Initialize a Lexer instance for tokenizing JSONPath strings.\n\nThis constructor configures various regular expression patterns used to\nidentify components of a JSONPath expression. It requires a single parameter,\n`env`, which is an instance of `JSONPathEnvironment`. This environment provides\naccess to specific tokens used within the lexer.\n\nAttributes:\n    env: An instance of `JSONPathEnvironment` that defines specific token \n         attributes for root, fake root, self, union, intersection, and filter \n         context within JSONPath.\n    double_quote_pattern: A regex pattern for matching double-quoted strings.\n    single_quote_pattern: A regex pattern for matching single-quoted strings.\n    dot_property_pattern: A regex pattern for matching properties prefixed with a dot.\n    slice_list_pattern: A regex pattern for matching list slicing syntax.\n    re_pattern: A regex pattern for matching regular expression patterns and their flags.\n    function_pattern: A regex pattern for matching function calls.\n    rules: A compiled regex pattern encompassing all token matching rules.\n\nThe `compile_rules` method is called to aggregate and compile all the defined\npatterns into a single regex, which will be used for tokenization during parsing.",
        "signature": "def __init__(self, *, env: JSONPathEnvironment) -> None:",
        "type": "Method",
        "class_signature": "class Lexer:"
      }
    },
    "jsonpath/parse.py": {
      "Parser.__init__": {
        "code": "    def __init__(self, *, env: JSONPathEnvironment) -> None:\n        \"\"\"Initializes a Parser instance for interpreting JSONPath expressions within a given environment.\n\n    Parameters:\n        env (JSONPathEnvironment): The environment in which the parser operates, providing context and settings necessary for parsing JSONPath expressions.\n\n    Attributes:\n        token_map (Dict[str, Callable[[TokenStream], FilterExpression]]): A mapping from token types to their corresponding parsing methods, enabling the parser to process various components of JSONPath expressions.\n        list_item_map (Dict[str, Callable[[TokenStream], FilterExpression]]): A specialized mapping for parsing elements within list literals, ensuring appropriate handling of different token types.\n        function_argument_map (Dict[str, Callable[[TokenStream], FilterExpression]]): A mapping for parsing function arguments, allowing the parser to correctly interpret and validate function calls within JSONPath expressions.\n\n    Constants used:\n        - TOKEN_* constants (e.g., TOKEN_DOUBLE_QUOTE_STRING, TOKEN_FUNCTION) represent various token types recognized by the parser, facilitating the identification and processing of different elements in the JSONPath syntax. These constants are imported from the .token module, ensuring consistency in token recognition across the parsing process.\"\"\"\n        self.env = env\n        self.token_map: Dict[str, Callable[[TokenStream], FilterExpression]] = {TOKEN_DOUBLE_QUOTE_STRING: self.parse_string_literal, TOKEN_FAKE_ROOT: self.parse_root_path, TOKEN_FALSE: self.parse_boolean, TOKEN_FILTER_CONTEXT: self.parse_filter_context_path, TOKEN_FLOAT: self.parse_float_literal, TOKEN_FUNCTION: self.parse_function_extension, TOKEN_INT: self.parse_integer_literal, TOKEN_KEY: self.parse_current_key, TOKEN_LIST_START: self.parse_list_literal, TOKEN_LPAREN: self.parse_grouped_expression, TOKEN_MISSING: self.parse_undefined, TOKEN_NIL: self.parse_nil, TOKEN_NONE: self.parse_nil, TOKEN_NOT: self.parse_prefix_expression, TOKEN_NULL: self.parse_nil, TOKEN_RE_PATTERN: self.parse_regex, TOKEN_ROOT: self.parse_root_path, TOKEN_SELF: self.parse_self_path, TOKEN_SINGLE_QUOTE_STRING: self.parse_string_literal, TOKEN_TRUE: self.parse_boolean, TOKEN_UNDEFINED: self.parse_undefined}\n        self.list_item_map: Dict[str, Callable[[TokenStream], FilterExpression]] = {TOKEN_FALSE: self.parse_boolean, TOKEN_FLOAT: self.parse_float_literal, TOKEN_INT: self.parse_integer_literal, TOKEN_NIL: self.parse_nil, TOKEN_NONE: self.parse_nil, TOKEN_NULL: self.parse_nil, TOKEN_DOUBLE_QUOTE_STRING: self.parse_string_literal, TOKEN_SINGLE_QUOTE_STRING: self.parse_string_literal, TOKEN_TRUE: self.parse_boolean}\n        self.function_argument_map: Dict[str, Callable[[TokenStream], FilterExpression]] = {TOKEN_DOUBLE_QUOTE_STRING: self.parse_string_literal, TOKEN_FAKE_ROOT: self.parse_root_path, TOKEN_FALSE: self.parse_boolean, TOKEN_FILTER_CONTEXT: self.parse_filter_context_path, TOKEN_FLOAT: self.parse_float_literal, TOKEN_FUNCTION: self.parse_function_extension, TOKEN_INT: self.parse_integer_literal, TOKEN_KEY: self.parse_current_key, TOKEN_NIL: self.parse_nil, TOKEN_NONE: self.parse_nil, TOKEN_NULL: self.parse_nil, TOKEN_ROOT: self.parse_root_path, TOKEN_SELF: self.parse_self_path, TOKEN_SINGLE_QUOTE_STRING: self.parse_string_literal, TOKEN_TRUE: self.parse_boolean}",
        "docstring": "Initializes a Parser instance for interpreting JSONPath expressions within a given environment.\n\nParameters:\n    env (JSONPathEnvironment): The environment in which the parser operates, providing context and settings necessary for parsing JSONPath expressions.\n\nAttributes:\n    token_map (Dict[str, Callable[[TokenStream], FilterExpression]]): A mapping from token types to their corresponding parsing methods, enabling the parser to process various components of JSONPath expressions.\n    list_item_map (Dict[str, Callable[[TokenStream], FilterExpression]]): A specialized mapping for parsing elements within list literals, ensuring appropriate handling of different token types.\n    function_argument_map (Dict[str, Callable[[TokenStream], FilterExpression]]): A mapping for parsing function arguments, allowing the parser to correctly interpret and validate function calls within JSONPath expressions.\n\nConstants used:\n    - TOKEN_* constants (e.g., TOKEN_DOUBLE_QUOTE_STRING, TOKEN_FUNCTION) represent various token types recognized by the parser, facilitating the identification and processing of different elements in the JSONPath syntax. These constants are imported from the .token module, ensuring consistency in token recognition across the parsing process.",
        "signature": "def __init__(self, *, env: JSONPathEnvironment) -> None:",
        "type": "Method",
        "class_signature": "class Parser:"
      }
    }
  },
  "dependency_dict": {
    "jsonpath/env.py:JSONPathEnvironment:__init__": {},
    "jsonpath/lex.py:Lexer:__init__": {
      "jsonpath/lex.py": {
        "Lexer.compile_rules": {
          "code": "    def compile_rules(self) -> Pattern[str]:\n        \"\"\"Prepare regular expression rules.\"\"\"\n        env_tokens = [(TOKEN_ROOT, self.env.root_token), (TOKEN_FAKE_ROOT, self.env.fake_root_token), (TOKEN_SELF, self.env.self_token), (TOKEN_KEY, self.env.key_token), (TOKEN_UNION, self.env.union_token), (TOKEN_INTERSECTION, self.env.intersection_token), (TOKEN_FILTER_CONTEXT, self.env.filter_context_token), (TOKEN_KEYS, self.env.keys_selector_token)]\n        rules = [(TOKEN_DOUBLE_QUOTE_STRING, self.double_quote_pattern), (TOKEN_SINGLE_QUOTE_STRING, self.single_quote_pattern), (TOKEN_RE_PATTERN, self.re_pattern), (TOKEN_LIST_SLICE, self.slice_list_pattern), (TOKEN_FUNCTION, self.function_pattern), (TOKEN_DOT_PROPERTY, self.dot_property_pattern), (TOKEN_FLOAT, '-?\\\\d+\\\\.\\\\d*(?:[eE][+-]?\\\\d+)?'), (TOKEN_INT, '-?\\\\d+(?P<G_EXP>[eE][+\\\\-]?\\\\d+)?\\\\b'), (TOKEN_DDOT, '\\\\.\\\\.'), (TOKEN_AND, self.logical_and_pattern), (TOKEN_OR, self.logical_or_pattern), *[(token, re.escape(pattern)) for token, pattern in sorted(env_tokens, key=lambda x: len(x[1]), reverse=True) if pattern], (TOKEN_WILD, '\\\\*'), (TOKEN_FILTER, '\\\\?'), (TOKEN_IN, 'in'), (TOKEN_TRUE, '[Tt]rue'), (TOKEN_FALSE, '[Ff]alse'), (TOKEN_NIL, '[Nn]il'), (TOKEN_NULL, '[Nn]ull'), (TOKEN_NONE, '[Nn]one'), (TOKEN_CONTAINS, 'contains'), (TOKEN_UNDEFINED, 'undefined'), (TOKEN_MISSING, 'missing'), (TOKEN_LIST_START, '\\\\['), (TOKEN_RBRACKET, ']'), (TOKEN_COMMA, ','), (TOKEN_EQ, '=='), (TOKEN_NE, '!='), (TOKEN_LG, '<>'), (TOKEN_LE, '<='), (TOKEN_GE, '>='), (TOKEN_RE, '=~'), (TOKEN_LT, '<'), (TOKEN_GT, '>'), (TOKEN_NOT, self.logical_not_pattern), (TOKEN_BARE_PROPERTY, self.key_pattern), (TOKEN_LPAREN, '\\\\('), (TOKEN_RPAREN, '\\\\)'), (TOKEN_SKIP, '[ \\\\n\\\\t\\\\r\\\\.]+'), (TOKEN_ILLEGAL, '.')]\n        return re.compile('|'.join((f'(?P<{token}>{pattern})' for token, pattern in rules)), re.DOTALL)",
          "docstring": "Prepare regular expression rules.",
          "signature": "def compile_rules(self) -> Pattern[str]:",
          "type": "Method",
          "class_signature": "class Lexer:"
        }
      }
    },
    "jsonpath/parse.py:Parser:__init__": {},
    "jsonpath/env.py:JSONPathEnvironment:setup_function_extensions": {
      "jsonpath/function_extensions/typeof.py": {
        "TypeOf.__init__": {
          "code": "    def __init__(self, *, single_number_type: bool = True) -> None:\n        self.single_number_type = single_number_type",
          "docstring": "",
          "signature": "def __init__(self, *, single_number_type: bool=True) -> None:",
          "type": "Method",
          "class_signature": "class TypeOf(FilterFunction):"
        }
      }
    },
    "jsonpath/env.py:JSONPathEnvironment:compile": {},
    "jsonpath/stream.py:TokenStreamIterator:__init__": {
      "jsonpath/token.py": {
        "Token.__init__": {
          "code": "    def __init__(\n        self,\n        kind: str,\n        value: str,\n        index: int,\n        path: str,\n    ) -> None:\n        self.kind = kind\n        self.value = value\n        self.index = index\n        self.path = path",
          "docstring": "",
          "signature": "def __init__(self, kind: str, value: str, index: int, path: str) -> None:",
          "type": "Method",
          "class_signature": "class Token:"
        }
      },
      "jsonpath/stream.py": {
        "TokenStreamIterator.__next__": {
          "code": "        def __next__(self) -> Token:\n            tok = self.stream.current\n            if tok.kind is TOKEN_EOF:\n                self.stream.close()\n                raise StopIteration\n            next(self.stream)\n            return tok",
          "docstring": "",
          "signature": "def __next__(self) -> Token:",
          "type": "Method",
          "class_signature": "class TokenStreamIterator:"
        }
      }
    },
    "jsonpath/path.py:JSONPath:__init__": {
      "jsonpath/parse.py": {
        "Parser.parse": {
          "code": "    def parse(self, stream: TokenStream) -> Iterable[JSONPathSelector]:\n        \"\"\"Parse a JSONPath from a stream of tokens.\"\"\"\n        if stream.current.kind in {TOKEN_ROOT, TOKEN_FAKE_ROOT}:\n            stream.next_token()\n        yield from self.parse_path(stream, in_filter=False)\n        if stream.current.kind not in (TOKEN_EOF, TOKEN_INTERSECTION, TOKEN_UNION):\n            raise JSONPathSyntaxError(f'unexpected token {stream.current.value!r}', token=stream.current)",
          "docstring": "Parse a JSONPath from a stream of tokens.",
          "signature": "def parse(self, stream: TokenStream) -> Iterable[JSONPathSelector]:",
          "type": "Method",
          "class_signature": "class Parser:"
        }
      }
    },
    "jsonpath/path.py:JSONPath:findall": {},
    "jsonpath/path.py:JSONPath:finditer": {
      "jsonpath/_data.py": {
        "load_data": {
          "code": "def load_data(data: object) -> Any:\n    if isinstance(data, str):\n        try:\n            return json.loads(data)\n        except json.JSONDecodeError:\n            # Overly simple way to detect a malformed JSON document vs a\n            # top-level string only document\n            if _RE_PROBABLY_MALFORMED.search(data):\n                raise\n            return data\n    if isinstance(data, IOBase):\n        return json.loads(data.read())\n    return data",
          "docstring": "",
          "signature": "def load_data(data: object) -> Any:",
          "type": "Function",
          "class_signature": null
        }
      },
      "jsonpath/match.py": {
        "JSONPathMatch.__init__": {
          "code": "    def __init__(self, *, filter_context: FilterContextVars, obj: object, parent: Optional[JSONPathMatch], path: str, parts: Tuple[PathPart, ...], root: Union[Sequence[Any], Mapping[str, Any]]) -> None:\n        self._filter_context = filter_context\n        self.children: List[JSONPathMatch] = []\n        self.obj: object = obj\n        self.parent: Optional[JSONPathMatch] = parent\n        self.parts: Tuple[PathPart, ...] = parts\n        self.path: str = path\n        self.root: Union[Sequence[Any], Mapping[str, Any]] = root",
          "docstring": "",
          "signature": "def __init__(self, *, filter_context: FilterContextVars, obj: object, parent: Optional[JSONPathMatch], path: str, parts: Tuple[PathPart, ...], root: Union[Sequence[Any], Mapping[str, Any]]) -> None:",
          "type": "Method",
          "class_signature": "class JSONPathMatch:"
        }
      }
    },
    "jsonpath/selectors.py:ListSelector:resolve": {
      "jsonpath/selectors.py": {
        "PropertySelector.resolve": {
          "code": "    def resolve(self, matches: Iterable[JSONPathMatch]) -> Iterable[JSONPathMatch]:\n        for match in matches:\n            if not isinstance(match.obj, Mapping):\n                continue\n            with suppress(KeyError):\n                _match = self.env.match_class(filter_context=match.filter_context(), obj=self.env.getitem(match.obj, self.name), parent=match, parts=match.parts + (self.name,), path=match.path + f\"['{self.name}']\", root=match.root)\n                match.add_child(_match)\n                yield _match",
          "docstring": "",
          "signature": "def resolve(self, matches: Iterable[JSONPathMatch]) -> Iterable[JSONPathMatch]:",
          "type": "Method",
          "class_signature": "class PropertySelector(JSONPathSelector):"
        },
        "Filter.resolve": {
          "code": "    def resolve(self, matches: Iterable[JSONPathMatch]) -> Iterable[JSONPathMatch]:\n        if self.cacheable_nodes and self.env.filter_caching:\n            expr = self.expression.cache_tree()\n        else:\n            expr = self.expression\n        for match in matches:\n            if isinstance(match.obj, Mapping):\n                for key, val in match.obj.items():\n                    context = FilterContext(env=self.env, current=val, root=match.root, extra_context=match.filter_context(), current_key=key)\n                    try:\n                        if expr.evaluate(context):\n                            _match = self.env.match_class(filter_context=match.filter_context(), obj=val, parent=match, parts=match.parts + (key,), path=match.path + f\"['{key}']\", root=match.root)\n                            match.add_child(_match)\n                            yield _match\n                    except JSONPathTypeError as err:\n                        if not err.token:\n                            err.token = self.token\n                        raise\n            elif isinstance(match.obj, Sequence) and (not isinstance(match.obj, str)):\n                for i, obj in enumerate(match.obj):\n                    context = FilterContext(env=self.env, current=obj, root=match.root, extra_context=match.filter_context(), current_key=i)\n                    try:\n                        if expr.evaluate(context):\n                            _match = self.env.match_class(filter_context=match.filter_context(), obj=obj, parent=match, parts=match.parts + (i,), path=f'{match.path}[{i}]', root=match.root)\n                            match.add_child(_match)\n                            yield _match\n                    except JSONPathTypeError as err:\n                        if not err.token:\n                            err.token = self.token\n                        raise",
          "docstring": "",
          "signature": "def resolve(self, matches: Iterable[JSONPathMatch]) -> Iterable[JSONPathMatch]:",
          "type": "Method",
          "class_signature": "class Filter(JSONPathSelector):"
        }
      }
    },
    "jsonpath/path.py:JSONPath:findall_async": {},
    "jsonpath/path.py:JSONPath:finditer_async": {
      "jsonpath/_data.py": {
        "load_data": {
          "code": "def load_data(data: object) -> Any:\n    if isinstance(data, str):\n        try:\n            return json.loads(data)\n        except json.JSONDecodeError:\n            # Overly simple way to detect a malformed JSON document vs a\n            # top-level string only document\n            if _RE_PROBABLY_MALFORMED.search(data):\n                raise\n            return data\n    if isinstance(data, IOBase):\n        return json.loads(data.read())\n    return data",
          "docstring": "",
          "signature": "def load_data(data: object) -> Any:",
          "type": "Function",
          "class_signature": null
        }
      }
    },
    "jsonpath/selectors.py:ListSelector:resolve_async": {}
  },
  "call_tree": {
    "tests/test_match_function.py:env": {
      "jsonpath/env.py:JSONPathEnvironment:__init__": {
        "jsonpath/lex.py:Lexer:__init__": {
          "jsonpath/lex.py:Lexer:compile_rules": {}
        },
        "jsonpath/parse.py:Parser:__init__": {},
        "jsonpath/env.py:JSONPathEnvironment:setup_function_extensions": {
          "jsonpath/function_extensions/typeof.py:TypeOf:__init__": {}
        }
      }
    },
    "tests/test_match_function.py:test_match_function": {
      "jsonpath/env.py:JSONPathEnvironment:compile": {
        "jsonpath/stream.py:TokenStreamIterator:__init__": {
          "jsonpath/token.py:Token:__init__": {},
          "jsonpath/stream.py:TokenStreamIterator:__next__": {
            "jsonpath/lex.py:Lexer:tokenize": {
              "jsonpath/token.py:Token:__init__": {}
            }
          }
        },
        "jsonpath/path.py:JSONPath:__init__": {
          "jsonpath/parse.py:Parser:parse": {
            "jsonpath/stream.py:TokenStreamIterator:next_token": {
              "jsonpath/stream.py:TokenStreamIterator:__next__": {
                "jsonpath/lex.py:Lexer:tokenize": {
                  "jsonpath/token.py:Token:__init__": {}
                }
              }
            },
            "jsonpath/parse.py:Parser:parse_path": {
              "jsonpath/selectors.py:PropertySelector:__init__": {
                "jsonpath/selectors.py:JSONPathSelector:__init__": {}
              },
              "jsonpath/stream.py:TokenStreamIterator:next_token": {
                "jsonpath/stream.py:TokenStreamIterator:__next__": {
                  "jsonpath/lex.py:Lexer:tokenize": {
                    "jsonpath/token.py:Token:__init__": {}
                  },
                  "jsonpath/stream.py:TokenStreamIterator:close": {
                    "jsonpath/token.py:Token:__init__": {}
                  }
                }
              },
              "jsonpath/parse.py:Parser:parse_selector_list": {
                "jsonpath/stream.py:TokenStreamIterator:next_token": {
                  "jsonpath/stream.py:TokenStreamIterator:__next__": {
                    "jsonpath/lex.py:Lexer:tokenize": {
                      "jsonpath/token.py:Token:__init__": {}
                    }
                  }
                },
                "jsonpath/parse.py:Parser:parse_filter": {
                  "jsonpath/filter.py:FunctionExtension:FunctionExtension": {},
                  "jsonpath/function_extensions/filter_function.py:FilterFunction:FilterFunction": {},
                  "jsonpath/stream.py:TokenStreamIterator:next_token": {
                    "jsonpath/stream.py:TokenStreamIterator:__next__": {
                      "jsonpath/lex.py:Lexer:tokenize": {}
                    }
                  },
                  "jsonpath/parse.py:Parser:parse_filter_selector": {
                    "jsonpath/parse.py:Parser:parse_function_extension": {
                      "jsonpath/stream.py:TokenStreamIterator:next_token": {},
                      "jsonpath/parse.py:Parser:parse_self_path": {},
                      "jsonpath/stream.py:TokenStreamIterator:peek": {},
                      "jsonpath/stream.py:TokenStreamIterator:expect_peek": {},
                      "jsonpath/parse.py:Parser:parse_string_literal": {},
                      "jsonpath/env.py:JSONPathEnvironment:validate_function_extension_signature": {
                        "jsonpath/function_extensions/filter_function.py:FilterFunction:FilterFunction": {}
                      },
                      "jsonpath/filter.py:FunctionExtension:__init__": {}
                    },
                    "jsonpath/stream.py:TokenStreamIterator:peek": {
                      "jsonpath/stream.py:TokenStreamIterator:__next__": {},
                      "jsonpath/stream.py:TokenStreamIterator:push": {}
                    }
                  },
                  "jsonpath/filter.py:BooleanExpression:__init__": {
                    "jsonpath/filter.py:FilterExpression:__init__": {
                      "jsonpath/filter.py:BooleanExpression:children": {}
                    }
                  },
                  "jsonpath/selectors.py:Filter:__init__": {
                    "jsonpath/selectors.py:JSONPathSelector:__init__": {},
                    "jsonpath/filter.py:BooleanExpression:cacheable_nodes": {
                      "jsonpath/filter.py:CachingFilterExpression:CachingFilterExpression": {},
                      "jsonpath/filter.py:BooleanExpression:cache_tree": {}
                    }
                  }
                },
                "jsonpath/stream.py:TokenStreamIterator:peek": {
                  "jsonpath/stream.py:TokenStreamIterator:__next__": {},
                  "jsonpath/stream.py:TokenStreamIterator:push": {}
                },
                "jsonpath/selectors.py:ListSelector:__init__": {
                  "jsonpath/selectors.py:JSONPathSelector:__init__": {}
                }
              }
            }
          }
        }
      },
      "jsonpath/path.py:JSONPath:findall": {
        "jsonpath/path.py:JSONPath:finditer": {
          "jsonpath/_data.py:load_data": {},
          "jsonpath/match.py:JSONPathMatch:__init__": {}
        },
        "jsonpath/selectors.py:ListSelector:resolve": {
          "jsonpath/selectors.py:PropertySelector:resolve": {
            "jsonpath/match.py:JSONPathMatch:filter_context": {},
            "jsonpath/env.py:JSONPathEnvironment:getitem": {},
            "jsonpath/match.py:JSONPathMatch:__init__": {},
            "jsonpath/match.py:JSONPathMatch:add_child": {}
          },
          "jsonpath/selectors.py:Filter:resolve": {
            "jsonpath/match.py:JSONPathMatch:filter_context": {},
            "jsonpath/selectors.py:FilterContext:__init__": {},
            "jsonpath/filter.py:BooleanExpression:evaluate": {
              "jsonpath/filter.py:FunctionExtension:evaluate": {
                "jsonpath/filter.py:SelfPath:evaluate": {
                  "jsonpath/path.py:JSONPath:finditer": {
                    "jsonpath/_data.py:load_data": {},
                    "jsonpath/match.py:JSONPathMatch:__init__": {}
                  },
                  "jsonpath/selectors.py:PropertySelector:resolve": {
                    "jsonpath/match.py:JSONPathMatch:filter_context": {},
                    "jsonpath/env.py:JSONPathEnvironment:getitem": {},
                    "jsonpath/match.py:JSONPathMatch:__init__": {},
                    "jsonpath/match.py:JSONPathMatch:add_child": {}
                  }
                },
                "jsonpath/filter.py:Literal:evaluate": {},
                "jsonpath/filter.py:FunctionExtension:_unpack_node_lists": {
                  "jsonpath/function_extensions/filter_function.py:FilterFunction:FilterFunction": {},
                  "jsonpath/match.py:NodeList:NodeList": {}
                },
                "jsonpath/function_extensions/match.py:Match:__call__": {}
              },
              "jsonpath/env.py:JSONPathEnvironment:is_truthy": {
                "jsonpath/match.py:NodeList:NodeList": {}
              }
            },
            "jsonpath/match.py:JSONPathMatch:__init__": {},
            "jsonpath/match.py:JSONPathMatch:add_child": {}
          }
        }
      }
    },
    "tests/test_match_function.py:test_match_function_async": {
      "jsonpath/env.py:JSONPathEnvironment:compile": {
        "jsonpath/stream.py:TokenStreamIterator:__init__": {
          "jsonpath/token.py:Token:__init__": {},
          "jsonpath/stream.py:TokenStreamIterator:__next__": {
            "jsonpath/lex.py:Lexer:tokenize": {
              "jsonpath/token.py:Token:__init__": {}
            }
          }
        },
        "jsonpath/path.py:JSONPath:__init__": {
          "jsonpath/parse.py:Parser:parse": {
            "jsonpath/stream.py:TokenStreamIterator:next_token": {
              "jsonpath/stream.py:TokenStreamIterator:__next__": {
                "jsonpath/lex.py:Lexer:tokenize": {
                  "jsonpath/token.py:Token:__init__": {}
                }
              }
            },
            "jsonpath/parse.py:Parser:parse_path": {
              "jsonpath/selectors.py:PropertySelector:__init__": {
                "jsonpath/selectors.py:JSONPathSelector:__init__": {}
              },
              "jsonpath/stream.py:TokenStreamIterator:next_token": {
                "jsonpath/stream.py:TokenStreamIterator:__next__": {
                  "jsonpath/lex.py:Lexer:tokenize": {
                    "jsonpath/token.py:Token:__init__": {}
                  },
                  "jsonpath/stream.py:TokenStreamIterator:close": {
                    "jsonpath/token.py:Token:__init__": {}
                  }
                }
              },
              "jsonpath/parse.py:Parser:parse_selector_list": {
                "jsonpath/stream.py:TokenStreamIterator:next_token": {
                  "jsonpath/stream.py:TokenStreamIterator:__next__": {
                    "jsonpath/lex.py:Lexer:tokenize": {
                      "jsonpath/token.py:Token:__init__": {}
                    }
                  }
                },
                "jsonpath/parse.py:Parser:parse_filter": {
                  "jsonpath/stream.py:TokenStreamIterator:next_token": {
                    "jsonpath/stream.py:TokenStreamIterator:__next__": {
                      "jsonpath/lex.py:Lexer:tokenize": {}
                    }
                  },
                  "jsonpath/parse.py:Parser:parse_filter_selector": {
                    "jsonpath/parse.py:Parser:parse_function_extension": {
                      "jsonpath/stream.py:TokenStreamIterator:next_token": {},
                      "jsonpath/parse.py:Parser:parse_self_path": {},
                      "jsonpath/stream.py:TokenStreamIterator:peek": {},
                      "jsonpath/stream.py:TokenStreamIterator:expect_peek": {},
                      "jsonpath/parse.py:Parser:parse_string_literal": {},
                      "jsonpath/env.py:JSONPathEnvironment:validate_function_extension_signature": {},
                      "jsonpath/filter.py:FunctionExtension:__init__": {}
                    },
                    "jsonpath/stream.py:TokenStreamIterator:peek": {
                      "jsonpath/stream.py:TokenStreamIterator:__next__": {},
                      "jsonpath/stream.py:TokenStreamIterator:push": {}
                    }
                  },
                  "jsonpath/filter.py:BooleanExpression:__init__": {
                    "jsonpath/filter.py:FilterExpression:__init__": {
                      "jsonpath/filter.py:BooleanExpression:children": {}
                    }
                  },
                  "jsonpath/selectors.py:Filter:__init__": {
                    "jsonpath/selectors.py:JSONPathSelector:__init__": {},
                    "jsonpath/filter.py:BooleanExpression:cacheable_nodes": {
                      "jsonpath/filter.py:BooleanExpression:cache_tree": {}
                    }
                  }
                },
                "jsonpath/stream.py:TokenStreamIterator:peek": {
                  "jsonpath/stream.py:TokenStreamIterator:__next__": {},
                  "jsonpath/stream.py:TokenStreamIterator:push": {}
                },
                "jsonpath/selectors.py:ListSelector:__init__": {
                  "jsonpath/selectors.py:JSONPathSelector:__init__": {}
                }
              }
            }
          }
        }
      },
      "tests/test_match_function.py:coro": {
        "jsonpath/path.py:JSONPath:findall_async": {
          "jsonpath/path.py:JSONPath:finditer_async": {
            "jsonpath/_data.py:load_data": {}
          },
          "jsonpath/selectors.py:ListSelector:resolve_async": {
            "jsonpath/selectors.py:PropertySelector:resolve_async": {
              "jsonpath/path.py:JSONPath:root_iter": {
                "jsonpath/match.py:JSONPathMatch:__init__": {}
              },
              "jsonpath/match.py:JSONPathMatch:filter_context": {},
              "jsonpath/env.py:JSONPathEnvironment:getitem_async": {},
              "jsonpath/match.py:JSONPathMatch:__init__": {},
              "jsonpath/match.py:JSONPathMatch:add_child": {}
            },
            "jsonpath/selectors.py:Filter:resolve_async": {
              "jsonpath/selectors.py:_alist": {},
              "jsonpath/match.py:JSONPathMatch:filter_context": {},
              "jsonpath/selectors.py:FilterContext:__init__": {},
              "jsonpath/filter.py:BooleanExpression:evaluate_async": {
                "jsonpath/filter.py:FunctionExtension:evaluate_async": {
                  "jsonpath/filter.py:SelfPath:evaluate_async": {
                    "jsonpath/path.py:JSONPath:finditer_async": {
                      "jsonpath/_data.py:load_data": {}
                    },
                    "jsonpath/selectors.py:PropertySelector:resolve_async": {
                      "jsonpath/path.py:JSONPath:root_iter": {},
                      "jsonpath/match.py:JSONPathMatch:filter_context": {},
                      "jsonpath/env.py:JSONPathEnvironment:getitem_async": {},
                      "jsonpath/match.py:JSONPathMatch:__init__": {},
                      "jsonpath/match.py:JSONPathMatch:add_child": {}
                    }
                  },
                  "jsonpath/filter.py:Literal:evaluate_async": {},
                  "jsonpath/filter.py:FunctionExtension:_unpack_node_lists": {},
                  "jsonpath/function_extensions/match.py:Match:__call__": {}
                },
                "jsonpath/env.py:JSONPathEnvironment:is_truthy": {}
              },
              "jsonpath/match.py:JSONPathMatch:__init__": {},
              "jsonpath/match.py:JSONPathMatch:add_child": {}
            }
          }
        }
      }
    },
    "tests/test_match_function.py:coro": {
      "jsonpath/path.py:JSONPath:findall_async": {
        "jsonpath/selectors.py:ListSelector:resolve_async": {
          "jsonpath/selectors.py:Filter:resolve_async": {
            "jsonpath/filter.py:BooleanExpression:evaluate_async": {
              "jsonpath/filter.py:FunctionExtension:evaluate_async": {
                "jsonpath/filter.py:SelfPath:evaluate_async": {
                  "jsonpath/selectors.py:PropertySelector:resolve_async": {
                    "jsonpath/path.py:JSONPath:root_iter": {
                      "jsonpath/match.py:JSONPathMatch:__init__": {}
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "/mnt/sfs_turbo/yaxindu/tmp/python_jsonpath-image-test_match_function/python_jsonpath-test_match_function/tests/test_filter_expression_caching.py:test_cache_root_path": {
      "jsonpath/path.py:JSONPath:JSONPath": {},
      "jsonpath/selectors.py:ListSelector:ListSelector": {},
      "jsonpath/filter.py:BooleanExpression:BooleanExpression": {},
      "jsonpath/filter.py:InfixExpression:InfixExpression": {},
      "jsonpath/filter.py:SelfPath:SelfPath": {},
      "jsonpath/filter.py:RootPath:RootPath": {},
      "jsonpath/filter.py:CachingFilterExpression:CachingFilterExpression": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/python_jsonpath-image-test_match_function/python_jsonpath-test_match_function/tests/test_filter_expression_caching.py:test_cache_context_path": {
      "jsonpath/path.py:JSONPath:JSONPath": {},
      "jsonpath/selectors.py:ListSelector:ListSelector": {},
      "jsonpath/filter.py:BooleanExpression:BooleanExpression": {},
      "jsonpath/filter.py:InfixExpression:InfixExpression": {},
      "jsonpath/filter.py:FilterContextPath:FilterContextPath": {},
      "jsonpath/filter.py:SelfPath:SelfPath": {},
      "jsonpath/filter.py:CachingFilterExpression:CachingFilterExpression": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/python_jsonpath-image-test_match_function/python_jsonpath-test_match_function/tests/test_filter_expression_caching.py:test_uncacheable_filter": {
      "jsonpath/path.py:JSONPath:JSONPath": {},
      "jsonpath/selectors.py:ListSelector:ListSelector": {},
      "jsonpath/filter.py:BooleanExpression:BooleanExpression": {},
      "jsonpath/filter.py:InfixExpression:InfixExpression": {},
      "jsonpath/filter.py:SelfPath:SelfPath": {},
      "jsonpath/filter.py:IntegerLiteral:IntegerLiteral": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/python_jsonpath-image-test_match_function/python_jsonpath-test_match_function/tests/test_fluent_api.py:test_query_first_one": {
      "jsonpath/match.py:JSONPathMatch:JSONPathMatch": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/python_jsonpath-image-test_match_function/python_jsonpath-test_match_function/tests/test_fluent_api.py:test_query_one": {
      "jsonpath/match.py:JSONPathMatch:JSONPathMatch": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/python_jsonpath-image-test_match_function/python_jsonpath-test_match_function/tests/test_fluent_api.py:test_query_last_one": {
      "jsonpath/match.py:JSONPathMatch:JSONPathMatch": {}
    },
    "/mnt/sfs_turbo/yaxindu/tmp/python_jsonpath-image-test_match_function/python_jsonpath-test_match_function/tests/test_walk_filter_expression_tree.py:test_is_volatile": {
      "jsonpath/selectors.py:ListSelector:ListSelector": {}
    }
  },
  "PRD": "# PROJECT NAME: python_jsonpath-test_match_function\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 jsonpath/\n    \u251c\u2500\u2500 env.py\n    \u2502   \u251c\u2500\u2500 JSONPathEnvironment.__init__\n    \u2502   \u251c\u2500\u2500 JSONPathEnvironment.compile\n    \u2502   \u2514\u2500\u2500 JSONPathEnvironment.setup_function_extensions\n    \u251c\u2500\u2500 filter.py\n    \u2502   \u251c\u2500\u2500 BooleanExpression.BooleanExpression\n    \u2502   \u251c\u2500\u2500 CachingFilterExpression.CachingFilterExpression\n    \u2502   \u251c\u2500\u2500 FilterContextPath.FilterContextPath\n    \u2502   \u251c\u2500\u2500 InfixExpression.InfixExpression\n    \u2502   \u251c\u2500\u2500 IntegerLiteral.IntegerLiteral\n    \u2502   \u251c\u2500\u2500 RootPath.RootPath\n    \u2502   \u2514\u2500\u2500 SelfPath.SelfPath\n    \u251c\u2500\u2500 lex.py\n    \u2502   \u2514\u2500\u2500 Lexer.__init__\n    \u251c\u2500\u2500 match.py\n    \u2502   \u2514\u2500\u2500 JSONPathMatch.JSONPathMatch\n    \u251c\u2500\u2500 parse.py\n    \u2502   \u2514\u2500\u2500 Parser.__init__\n    \u251c\u2500\u2500 path.py\n    \u2502   \u251c\u2500\u2500 JSONPath.JSONPath\n    \u2502   \u251c\u2500\u2500 JSONPath.__init__\n    \u2502   \u251c\u2500\u2500 JSONPath.findall\n    \u2502   \u251c\u2500\u2500 JSONPath.findall_async\n    \u2502   \u251c\u2500\u2500 JSONPath.finditer\n    \u2502   \u2514\u2500\u2500 JSONPath.finditer_async\n    \u251c\u2500\u2500 selectors.py\n    \u2502   \u251c\u2500\u2500 ListSelector.ListSelector\n    \u2502   \u251c\u2500\u2500 ListSelector.resolve\n    \u2502   \u2514\u2500\u2500 ListSelector.resolve_async\n    \u2514\u2500\u2500 stream.py\n        \u2514\u2500\u2500 TokenStreamIterator.__init__\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThis module facilitates the validation and execution of JSONPath expressions by providing capabilities for both synchronous and asynchronous evaluation of query paths against structured JSON data. It enables users to define and test queries with advanced matching conditions, such as regular expressions, to extract specific data substructures from JSON inputs. By supporting parameterized test cases and seamless integration with testing libraries, the module ensures consistent, efficient verification of JSONPath functionality, solving the challenge of accurately querying and manipulating nested JSON data in dynamic and complex scenarios.\n\n## FILE 1: jsonpath/env.py\n\n- CLASS METHOD: JSONPathEnvironment.compile\n  - CLASS SIGNATURE: class JSONPathEnvironment:\n  - SIGNATURE: def compile(self, path: str) -> Union[JSONPath, CompoundJSONPath]:\n  - DOCSTRING: \n```python\n\"\"\"\nCompile and prepare a JSONPath expression for repeated matching against different data.\n\nThis method takes a JSONPath string, tokenizes it using the lexer, and processes the tokens with the parser to generate either a `JSONPath` or `CompoundJSONPath` object. These objects are designed to facilitate efficient querying of JSON data structures. If the path includes union (`|`) or intersection (`&`) operators, a `CompoundJSONPath` is returned.\n\nParameters:\n    path (str): A JSONPath expression as a string that specifies the desired data selection criteria.\n\nReturns:\n    Union[JSONPath, CompoundJSONPath]: A compiled path object ready for future data matching. \n\nRaises:\n    JSONPathSyntaxError: If the provided path contains syntax errors.\n    JSONPathTypeError: If the filter functions in the path are given arguments of inappropriate types.\n\nConstants Used:\n    TOKEN_EOF: Represents the end of the token stream, defined in the token module.\n    TOKEN_FAKE_ROOT: Indicates a temporary root node, used to manage certain JSONPath evaluations.\n    TOKEN_UNION and TOKEN_INTERSECTION: Used to identify union and intersection operations in the path.\n\nThe function interacts closely with self.lexer, which is an instance of the lexer class responsible for converting the path string into tokens, and self.parser, which processes those tokens to facilitate the creation of the appropriate JSONPath object.\n\"\"\"\n```\n\n- CLASS METHOD: JSONPathEnvironment.setup_function_extensions\n  - CLASS SIGNATURE: class JSONPathEnvironment:\n  - SIGNATURE: def setup_function_extensions(self) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nInitialize function extensions for JSONPath filtering.\n\nThis method configures a set of built-in function extensions that can be used within JSONPath filter expressions. It sets up various functions, such as `length`, `count`, `match`, `search`, and type-checking functions like `isinstance`, `is`, `typeof`, and `type`, making them available for use in JSONPath queries.\n\n### Side Effects\nThe function modifies the `function_extensions` attribute of the `JSONPathEnvironment` instance by adding key-value pairs, where keys are string identifiers of the function and values are instances of their respective function classes imported from the `function_extensions` module.\n\n### Dependencies\nThis method relies on classes defined in the `function_extensions` module and assumes that these classes adhere to a defined interface for function extensions within JSONPath.\n\"\"\"\n```\n\n- CLASS METHOD: JSONPathEnvironment.__init__\n  - CLASS SIGNATURE: class JSONPathEnvironment:\n  - SIGNATURE: def __init__(self, *, filter_caching: bool=True, unicode_escape: bool=True, well_typed: bool=True) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nInitialize a new instance of the JSONPathEnvironment class, which configures the settings for JSONPath tokenization, parsing, and resolution behavior. This constructor sets up the lexer, parser, and function extensions based on provided arguments that control filter expression caching, UTF-16 escape sequence decoding, and well-typedness checks on filter function expressions.\n\nParameters:\n    filter_caching (bool): If True, enable caching of filter expressions for performance. Defaults to True.\n    unicode_escape (bool): If True, enable decoding of UTF-16 escape sequences found in JSONPath string literals. Defaults to True.\n    well_typed (bool): If True, enable checks for well-typedness of filter function expressions. Defaults to True.\n\nAttributes:\n    lexer (Lexer): An instance of the lexer defined by lexer_class, used for tokenizing JSONPath strings.\n    parser (Parser): An instance of the parser defined by parser_class, used for parsing tokens.\n    function_extensions (Dict[str, Callable[..., Any]]): A dictionary to store function extensions available for filters, initialized by calling `setup_function_extensions()`.\n\nConstants:\n    lexer_class (Type[Lexer]): A class used to create the lexer, defaulting to the Lexer implementation.\n    parser_class (Type[Parser]): A class used to create the parser, defaulting to the Parser implementation.\n\"\"\"\n```\n\n## FILE 2: jsonpath/path.py\n\n- CLASS METHOD: JSONPath.__init__\n  - CLASS SIGNATURE: class JSONPath:\n  - SIGNATURE: def __init__(self, *, env: JSONPathEnvironment, selectors: Iterable[JSONPathSelector], fake_root: bool=False) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nInitialize a `JSONPath` instance, which represents a compiled JSONPath ready for querying JSON documents.\n\nParameters:\n- `env` (JSONPathEnvironment): The environment this JSONPath is associated with, which helps define how the path should be applied in terms of syntax and behavior.\n- `selectors` (Iterable[JSONPathSelector]): An iterable of selector objects that dictate the specific path elements to be matched in a JSON document.\n- `fake_root` (bool, optional): If set to `True`, indicates that the target JSON values should be wrapped in a single-element array, allowing the root value to be selected. Defaults to `False`.\n\nAttributes:\n- `self.env`: Stores the `JSONPathEnvironment`, influencing the path evaluation.\n- `self.selectors`: A tuple representing the sequence of selectors that comprise the path for querying.\n- `self.fake_root`: A flag determining if the target values are treated as if they are within a root array.\n\nThis initializer provides the foundational setup necessary for utilizing the methods of the `JSONPath` class, enabling structured querying of JSON data.\n\"\"\"\n```\n\n- CLASS METHOD: JSONPath.findall\n  - CLASS SIGNATURE: class JSONPath:\n  - SIGNATURE: def findall(self, data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]], *, filter_context: Optional[FilterContextVars]=None) -> List[object]:\n  - DOCSTRING: \n```python\n\"\"\"\nFind all objects in the provided data that match the given JSONPath.\n\nThis method processes the input `data`, which can either be a JSON document in string format, a file-like object, or a Python object (implementing `Sequence` or `Mapping`). It utilizes the `finditer` method to generate matches, returning a list containing the matched objects. If no matches are found, an empty list is returned.\n\nParameters:\n- data: A JSON document (str), a file-like object (IOBase), or a Python object (Sequence or Mapping) that can be queried.\n- filter_context: Optional mapping that provides arbitrary data to filters, accessible via the _filter context_ selector. Defaults to None.\n\nReturns:\n- A list of matched objects. If there are no matches, the list will be empty.\n\nRaises:\n- JSONPathSyntaxError: If the JSONPath expression is invalid.\n- JSONPathTypeError: If a filter expression uses types in an incompatible manner.\n\nThis method interacts with the `finditer` method to perform the actual matching and relies on the `load_data` function to process string or file inputs into JSON-compatible Python objects.\n\"\"\"\n```\n\n- CLASS METHOD: JSONPath.finditer\n  - CLASS SIGNATURE: class JSONPath:\n  - SIGNATURE: def finditer(self, data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]], *, filter_context: Optional[FilterContextVars]=None) -> Iterable[JSONPathMatch]:\n  - DOCSTRING: \n```python\n\"\"\"\nGenerate `JSONPathMatch` objects for each match found using this JSONPath instance.\n\nThis method takes in a JSON document or Python object and yields matches based on the selectors defined in the JSONPath instance. The input data can be a string (representing JSON), file-like object, sequence, or mapping. If the data is a string or file-like object, it will be loaded using the `load_data` function from the `jsonpath._data` module, which handles the conversion to a Python object.\n\nParameters:\n- data (Union[str, IOBase, Sequence[Any], Mapping[str, Any]]): A JSON document or a Python object implementing the `Sequence` or `Mapping` interfaces to search for matches.\n- filter_context (Optional[FilterContextVars]): Arbitrary data made available to filters, which can be used in the selectors for context-specific filtering.\n\nReturns:\n- Iterable[JSONPathMatch]: An iterator yielding `JSONPathMatch` objects for each match found.\n\nRaises:\n- JSONPathSyntaxError: If the path defined by the selectors is invalid.\n- JSONPathTypeError: If a filter expression uses types in an incompatible way with the data.\n\nThe `_data` variable holds the processed data ready for matching. The method starts with a root match object and iteratively applies each selector to find additional matches, returning an iterable of results.\n\"\"\"\n```\n\n## FILE 3: jsonpath/filter.py\n\n## FILE 4: jsonpath/stream.py\n\n- CLASS METHOD: TokenStreamIterator.__init__\n  - CLASS SIGNATURE: class TokenStreamIterator:\n  - SIGNATURE: def __init__(self, stream: TokenStream):\n  - DOCSTRING: \n```python\n\"\"\"\nInitialize a TokenStreamIterator instance for stepping through a TokenStream.\n\nParameters:\n- stream (TokenStream): An instance of the TokenStream class, which provides an iterable stream of tokens.\n\nThis constructor stores the provided TokenStream instance for use in the TokenStreamIterator's iteration methods. It does not return any value and has no side effects aside from establishing the relationship between the iterator and the token stream.\n\"\"\"\n```\n\n## FILE 5: jsonpath/match.py\n\n## FILE 6: jsonpath/selectors.py\n\n- CLASS METHOD: ListSelector.resolve\n  - CLASS SIGNATURE: class ListSelector(JSONPathSelector):\n  - SIGNATURE: def resolve(self, matches: Iterable[JSONPathMatch]) -> Iterable[JSONPathMatch]:\n  - DOCSTRING: \n```python\n\"\"\"\nResolve the list of selectors in the context of the provided matches.\n\nParameters:\n- matches (Iterable[JSONPathMatch]): An iterable of JSONPathMatch instances representing the nodes matched by preceding selectors. Each match carries information about its value, location, and parent in a JSON structure.\n\nReturns:\n- Iterable[JSONPathMatch]: An iterable of JSONPathMatch instances created by applying each selector in the items list to the provided matches. The results from each selector are concatenated and yielded.\n\nThis method interacts with the `items` attribute, which is a tuple of selectors (such as `SliceSelector`, `KeysSelector`, `IndexSelector`, etc.). Each item in the `items` list is expected to have its own `resolve` method, which is called with the current match, leading to further matches being yielded or manipulated based on the specific selector logic.\n\"\"\"\n```\n\n## FILE 7: jsonpath/lex.py\n\n- CLASS METHOD: Lexer.__init__\n  - CLASS SIGNATURE: class Lexer:\n  - SIGNATURE: def __init__(self, *, env: JSONPathEnvironment) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nInitialize a Lexer instance for tokenizing JSONPath strings.\n\nThis constructor configures various regular expression patterns used to\nidentify components of a JSONPath expression. It requires a single parameter,\n`env`, which is an instance of `JSONPathEnvironment`. This environment provides\naccess to specific tokens used within the lexer.\n\nAttributes:\n    env: An instance of `JSONPathEnvironment` that defines specific token \n         attributes for root, fake root, self, union, intersection, and filter \n         context within JSONPath.\n    double_quote_pattern: A regex pattern for matching double-quoted strings.\n    single_quote_pattern: A regex pattern for matching single-quoted strings.\n    dot_property_pattern: A regex pattern for matching properties prefixed with a dot.\n    slice_list_pattern: A regex pattern for matching list slicing syntax.\n    re_pattern: A regex pattern for matching regular expression patterns and their flags.\n    function_pattern: A regex pattern for matching function calls.\n    rules: A compiled regex pattern encompassing all token matching rules.\n\nThe `compile_rules` method is called to aggregate and compile all the defined\npatterns into a single regex, which will be used for tokenization during parsing.\n\"\"\"\n```\n\n## FILE 8: jsonpath/parse.py\n\n- CLASS METHOD: Parser.__init__\n  - CLASS SIGNATURE: class Parser:\n  - SIGNATURE: def __init__(self, *, env: JSONPathEnvironment) -> None:\n  - DOCSTRING: \n```python\n\"\"\"\nInitializes a Parser instance for interpreting JSONPath expressions within a given environment.\n\nParameters:\n    env (JSONPathEnvironment): The environment in which the parser operates, providing context and settings necessary for parsing JSONPath expressions.\n\nAttributes:\n    token_map (Dict[str, Callable[[TokenStream], FilterExpression]]): A mapping from token types to their corresponding parsing methods, enabling the parser to process various components of JSONPath expressions.\n    list_item_map (Dict[str, Callable[[TokenStream], FilterExpression]]): A specialized mapping for parsing elements within list literals, ensuring appropriate handling of different token types.\n    function_argument_map (Dict[str, Callable[[TokenStream], FilterExpression]]): A mapping for parsing function arguments, allowing the parser to correctly interpret and validate function calls within JSONPath expressions.\n\nConstants used:\n    - TOKEN_* constants (e.g., TOKEN_DOUBLE_QUOTE_STRING, TOKEN_FUNCTION) represent various token types recognized by the parser, facilitating the identification and processing of different elements in the JSONPath syntax. These constants are imported from the .token module, ensuring consistency in token recognition across the parsing process.\n\"\"\"\n```\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "jsonpath/env.py": "\"\"\"Core JSONPath configuration object.\"\"\"\nfrom __future__ import annotations\nimport re\nfrom decimal import Decimal\nfrom operator import getitem\nfrom typing import TYPE_CHECKING\nfrom typing import Any\nfrom typing import AsyncIterable\nfrom typing import Callable\nfrom typing import Dict\nfrom typing import Iterable\nfrom typing import List\nfrom typing import Mapping\nfrom typing import Optional\nfrom typing import Sequence\nfrom typing import Type\nfrom typing import Union\nfrom . import function_extensions\nfrom .exceptions import JSONPathNameError\nfrom .exceptions import JSONPathSyntaxError\nfrom .exceptions import JSONPathTypeError\nfrom .filter import UNDEFINED\nfrom .filter import VALUE_TYPE_EXPRESSIONS\nfrom .filter import FilterExpression\nfrom .filter import FunctionExtension\nfrom .filter import InfixExpression\nfrom .filter import Path\nfrom .fluent_api import Query\nfrom .function_extensions import ExpressionType\nfrom .function_extensions import FilterFunction\nfrom .function_extensions import validate\nfrom .lex import Lexer\nfrom .match import JSONPathMatch\nfrom .match import NodeList\nfrom .parse import Parser\nfrom .path import CompoundJSONPath\nfrom .path import JSONPath\nfrom .stream import TokenStream\nfrom .token import TOKEN_EOF\nfrom .token import TOKEN_FAKE_ROOT\nfrom .token import TOKEN_INTERSECTION\nfrom .token import TOKEN_UNION\nfrom .token import Token\nif TYPE_CHECKING:\n    from io import IOBase\n    from .match import FilterContextVars\n\nclass JSONPathEnvironment:\n    \"\"\"JSONPath configuration.\n\n    This class contains settings for path tokenization, parsing and resolution\n    behavior, plus convenience methods for matching an unparsed path to some\n    data.\n\n    Most applications will want to create a single `JSONPathEnvironment`, or\n    use `jsonpath.compile()`, `jsonpath.findall()`, etc. from the package-level\n    default environment.\n\n    ## Environment customization\n\n    Environment customization is achieved by subclassing `JSONPathEnvironment`\n    and overriding class attributes and/or methods. Some of these\n    customizations include:\n\n    - Changing the root (`$`), self (`@`) or filter context (`_`) token with\n      class attributes `root_token`, `self_token` and `filter_context_token`.\n    - Registering a custom lexer or parser with the class attributes\n      `lexer_class` or `parser_class`. `lexer_class` must be a subclass of\n      [`Lexer`]() and `parser_class` must be a subclass of [`Parser`]().\n    - Setup built-in function extensions by overriding\n      `setup_function_extensions()`\n    - Hook in to mapping and sequence item getting by overriding `getitem()`.\n    - Change filter comparison operator behavior by overriding `compare()`.\n\n    Arguments:\n        filter_caching (bool): If `True`, filter expressions will be cached\n            where possible.\n        unicode_escape: If `True`, decode UTF-16 escape sequences found in\n            JSONPath string literals.\n        well_typed: Control well-typedness checks on filter function expressions.\n            If `True` (the default), JSONPath expressions are checked for\n            well-typedness as compile time.\n\n            **New in version 0.10.0**\n\n    ## Class attributes\n\n    Attributes:\n        fake_root_token (str): The pattern used to select a \"fake\" root node, one level\n            above the real root node.\n        filter_context_token (str): The pattern used to select extra filter context\n            data. Defaults to `\"_\"`.\n        intersection_token (str): The pattern used as the intersection operator.\n            Defaults to `\"&\"`.\n        key_token (str): The pattern used to identify the current key or index when\n            filtering a, mapping or sequence. Defaults to `\"#\"`.\n        keys_selector_token (str): The pattern used as the \"keys\" selector. Defaults to\n            `\"~\"`.\n        lexer_class: The lexer to use when tokenizing path strings.\n        max_int_index (int): The maximum integer allowed when selecting array items by\n            index. Defaults to `(2**53) - 1`.\n        min_int_index (int): The minimum integer allowed when selecting array items by\n            index. Defaults to `-(2**53) + 1`.\n        parser_class: The parser to use when parsing tokens from the lexer.\n        root_token (str): The pattern used to select the root node in a JSON document.\n            Defaults to `\"$\"`.\n        self_token (str): The pattern used to select the current node in a JSON\n            document. Defaults to `\"@\"`\n        union_token (str): The pattern used as the union operator. Defaults to `\"|\"`.\n    \"\"\"\n    fake_root_token = '^'\n    filter_context_token = '_'\n    intersection_token = '&'\n    key_token = '#'\n    keys_selector_token = '~'\n    root_token = '$'\n    self_token = '@'\n    union_token = '|'\n    max_int_index = 2 ** 53 - 1\n    min_int_index = -2 ** 53 + 1\n    lexer_class: Type[Lexer] = Lexer\n    parser_class: Type[Parser] = Parser\n    match_class: Type[JSONPathMatch] = JSONPathMatch\n\n    def findall(self, path: str, data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]], *, filter_context: Optional[FilterContextVars]=None) -> List[object]:\n        \"\"\"Find all objects in _data_ matching the JSONPath _path_.\n\n        If _data_ is a string or a file-like objects, it will be loaded\n        using `json.loads()` and the default `JSONDecoder`.\n\n        Arguments:\n            path: The JSONPath as a string.\n            data: A JSON document or Python object implementing the `Sequence`\n                or `Mapping` interfaces.\n            filter_context: Arbitrary data made available to filters using\n                the _filter context_ selector.\n\n        Returns:\n            A list of matched objects. If there are no matches, the list will\n                be empty.\n\n        Raises:\n            JSONPathSyntaxError: If the path is invalid.\n            JSONPathTypeError: If a filter expression attempts to use types in\n                an incompatible way.\n        \"\"\"\n        return self.compile(path).findall(data, filter_context=filter_context)\n\n    def finditer(self, path: str, data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]], *, filter_context: Optional[FilterContextVars]=None) -> Iterable[JSONPathMatch]:\n        \"\"\"Generate `JSONPathMatch` objects for each match of _path_ in _data_.\n\n        If _data_ is a string or a file-like objects, it will be loaded using\n        `json.loads()` and the default `JSONDecoder`.\n\n        Arguments:\n            path: The JSONPath as a string.\n            data: A JSON document or Python object implementing the `Sequence`\n                or `Mapping` interfaces.\n            filter_context: Arbitrary data made available to filters using\n                the _filter context_ selector.\n\n        Returns:\n            An iterator yielding `JSONPathMatch` objects for each match.\n\n        Raises:\n            JSONPathSyntaxError: If the path is invalid.\n            JSONPathTypeError: If a filter expression attempts to use types in\n                an incompatible way.\n        \"\"\"\n        return self.compile(path).finditer(data, filter_context=filter_context)\n\n    def match(self, path: str, data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]], *, filter_context: Optional[FilterContextVars]=None) -> Union[JSONPathMatch, None]:\n        \"\"\"Return a `JSONPathMatch` instance for the first object found in _data_.\n\n        `None` is returned if there are no matches.\n\n        Arguments:\n            path: The JSONPath as a string.\n            data: A JSON document or Python object implementing the `Sequence`\n                or `Mapping` interfaces.\n            filter_context: Arbitrary data made available to filters using\n                the _filter context_ selector.\n\n        Returns:\n            A `JSONPathMatch` object for the first match, or `None` if there were\n                no matches.\n\n        Raises:\n            JSONPathSyntaxError: If the path is invalid.\n            JSONPathTypeError: If a filter expression attempts to use types in\n                an incompatible way.\n        \"\"\"\n        return self.compile(path).match(data, filter_context=filter_context)\n\n    def query(self, path: str, data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]], filter_context: Optional[FilterContextVars]=None) -> Query:\n        \"\"\"Return a `Query` iterator over matches found by applying _path_ to _data_.\n\n        `Query` objects are iterable.\n\n        ```\n        for match in jsonpath.query(\"$.foo..bar\", data):\n            ...\n        ```\n\n        You can skip and limit results with `Query.skip()` and `Query.limit()`.\n\n        ```\n        matches = (\n            jsonpath.query(\"$.foo..bar\", data)\n            .skip(5)\n            .limit(10)\n        )\n\n        for match in matches\n            ...\n        ```\n\n        `Query.tail()` will get the last _n_ results.\n\n        ```\n        for match in jsonpath.query(\"$.foo..bar\", data).tail(5):\n            ...\n        ```\n\n        Get values for each match using `Query.values()`.\n\n        ```\n        for obj in jsonpath.query(\"$.foo..bar\", data).limit(5).values():\n            ...\n        ```\n\n        Arguments:\n            path: The JSONPath as a string.\n            data: A JSON document or Python object implementing the `Sequence`\n                or `Mapping` interfaces.\n            filter_context: Arbitrary data made available to filters using\n                the _filter context_ selector.\n\n        Returns:\n            A query iterator.\n\n        Raises:\n            JSONPathSyntaxError: If the path is invalid.\n            JSONPathTypeError: If a filter expression attempts to use types in\n                an incompatible way.\n        \"\"\"\n        return Query(self.finditer(path, data, filter_context=filter_context), self)\n\n    async def findall_async(self, path: str, data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]], *, filter_context: Optional[FilterContextVars]=None) -> List[object]:\n        \"\"\"An async version of `findall()`.\"\"\"\n        return await self.compile(path).findall_async(data, filter_context=filter_context)\n\n    async def finditer_async(self, path: str, data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]], *, filter_context: Optional[FilterContextVars]=None) -> AsyncIterable[JSONPathMatch]:\n        \"\"\"An async version of `finditer()`.\"\"\"\n        return await self.compile(path).finditer_async(data, filter_context=filter_context)\n\n    def validate_function_extension_signature(self, token: Token, args: List[Any]) -> List[Any]:\n        \"\"\"Compile-time validation of function extension arguments.\n\n        RFC 9535 requires us to reject paths that use filter functions with\n        too many or too few arguments.\n        \"\"\"\n        try:\n            func = self.function_extensions[token.value]\n        except KeyError as err:\n            raise JSONPathNameError(f'function {token.value!r} is not defined', token=token) from err\n        if self.well_typed and isinstance(func, FilterFunction):\n            self.check_well_typedness(token, func, args)\n            return args\n        if hasattr(func, 'validate'):\n            args = func.validate(self, args, token)\n            assert isinstance(args, list)\n            return args\n        return validate(self, func, args, token)\n\n    def check_well_typedness(self, token: Token, func: FilterFunction, args: List[FilterExpression]) -> None:\n        \"\"\"Check the well-typedness of a function's arguments at compile-time.\"\"\"\n        if len(args) != len(func.arg_types):\n            raise JSONPathTypeError(f'{token.value!r}() requires {len(func.arg_types)} arguments', token=token)\n        for idx, typ in enumerate(func.arg_types):\n            arg = args[idx]\n            if typ == ExpressionType.VALUE:\n                if not (isinstance(arg, VALUE_TYPE_EXPRESSIONS) or (isinstance(arg, Path) and arg.path.singular_query()) or self._function_return_type(arg) == ExpressionType.VALUE):\n                    raise JSONPathTypeError(f'{token.value}() argument {idx} must be of ValueType', token=token)\n            elif typ == ExpressionType.LOGICAL:\n                if not isinstance(arg, (Path, InfixExpression)):\n                    raise JSONPathTypeError(f'{token.value}() argument {idx} must be of LogicalType', token=token)\n            elif typ == ExpressionType.NODES and (not (isinstance(arg, Path) or self._function_return_type(arg) == ExpressionType.NODES)):\n                raise JSONPathTypeError(f'{token.value}() argument {idx} must be of NodesType', token=token)\n\n    def _function_return_type(self, expr: FilterExpression) -> Optional[ExpressionType]:\n        \"\"\"Return the type returned from a filter function.\n\n        If _expr_ is not a `FunctionExtension` or the registered function definition is\n        not type-aware, return `None`.\n        \"\"\"\n        if not isinstance(expr, FunctionExtension):\n            return None\n        func = self.function_extensions.get(expr.name)\n        if isinstance(func, FilterFunction):\n            return func.return_type\n        return None\n\n    def getitem(self, obj: Any, key: Any) -> Any:\n        \"\"\"Sequence and mapping item getter used throughout JSONPath resolution.\n\n        The default implementation of `getitem` simply calls `operators.getitem()`\n        from Python's standard library. Same as `obj[key]`.\n\n        Arguments:\n            obj: A mapping or sequence that might contain _key_.\n            key: A mapping key, sequence index or sequence slice.\n        \"\"\"\n        return getitem(obj, key)\n\n    async def getitem_async(self, obj: Any, key: object) -> Any:\n        \"\"\"An async sequence and mapping item getter.\"\"\"\n        if hasattr(obj, '__getitem_async__'):\n            return await obj.__getitem_async__(key)\n        return getitem(obj, key)\n\n    def is_truthy(self, obj: object) -> bool:\n        \"\"\"Test for truthiness when evaluating JSONPath filter expressions.\n\n        In some cases, RFC 9535 requires us to test for existence rather than\n        truthiness. So the default implementation returns `True` for empty\n        collections and `None`. The special `UNDEFINED` object means that\n        _obj_ was missing, as opposed to an explicit `None`.\n\n        Arguments:\n            obj: Any object.\n\n        Returns:\n            `True` if the object exists and is not `False` or `0`.\n        \"\"\"\n        if isinstance(obj, NodeList) and len(obj) == 0:\n            return False\n        if obj is UNDEFINED:\n            return False\n        if obj is None:\n            return True\n        return bool(obj)\n\n    def compare(self, left: object, operator: str, right: object) -> bool:\n        \"\"\"Object comparison within JSONPath filters.\n\n        Override this to customize filter expression comparison operator\n        behavior.\n\n        Args:\n            left: The left hand side of the comparison expression.\n            operator: The comparison expression's operator.\n            right: The right hand side of the comparison expression.\n\n        Returns:\n            `True` if the comparison between _left_ and _right_, with the\n            given _operator_, is truthy. `False` otherwise.\n        \"\"\"\n        if operator == '&&':\n            return self.is_truthy(left) and self.is_truthy(right)\n        if operator == '||':\n            return self.is_truthy(left) or self.is_truthy(right)\n        if operator == '==':\n            return self._eq(left, right)\n        if operator == '!=':\n            return not self._eq(left, right)\n        if operator == '<':\n            return self._lt(left, right)\n        if operator == '>':\n            return self._lt(right, left)\n        if operator == '>=':\n            return self._lt(right, left) or self._eq(left, right)\n        if operator == '<=':\n            return self._lt(left, right) or self._eq(left, right)\n        if operator == 'in' and isinstance(right, (Mapping, Sequence)):\n            return left in right\n        if operator == 'contains' and isinstance(left, (Mapping, Sequence)):\n            return right in left\n        if operator == '=~' and isinstance(right, re.Pattern) and isinstance(left, str):\n            return bool(right.fullmatch(left))\n        return False\n\n    def _eq(self, left: object, right: object) -> bool:\n        if isinstance(right, NodeList):\n            left, right = (right, left)\n        if isinstance(left, NodeList):\n            if isinstance(right, NodeList):\n                return left == right\n            if left.empty():\n                return right is UNDEFINED\n            if len(left) == 1:\n                return left[0] == right\n            return False\n        if left is UNDEFINED and right is UNDEFINED:\n            return True\n        if isinstance(right, bool):\n            left, right = (right, left)\n        if isinstance(left, bool):\n            return isinstance(right, bool) and left == right\n        return left == right\n\n    def _lt(self, left: object, right: object) -> bool:\n        if isinstance(left, str) and isinstance(right, str):\n            return left < right\n        if isinstance(left, (int, float, Decimal)) and isinstance(right, (int, float, Decimal)):\n            return left < right\n        return False",
    "jsonpath/path.py": "from __future__ import annotations\nimport itertools\nfrom typing import TYPE_CHECKING\nfrom typing import Any\nfrom typing import AsyncIterable\nfrom typing import Iterable\nfrom typing import List\nfrom typing import Mapping\nfrom typing import Optional\nfrom typing import Sequence\nfrom typing import Tuple\nfrom typing import TypeVar\nfrom typing import Union\nfrom jsonpath._data import load_data\nfrom jsonpath.fluent_api import Query\nfrom jsonpath.match import FilterContextVars\nfrom jsonpath.match import JSONPathMatch\nfrom jsonpath.selectors import IndexSelector\nfrom jsonpath.selectors import ListSelector\nfrom jsonpath.selectors import PropertySelector\nif TYPE_CHECKING:\n    from io import IOBase\n    from .env import JSONPathEnvironment\n    from .selectors import JSONPathSelector\n\nclass JSONPath:\n    \"\"\"A compiled JSONPath ready to be applied to a JSON string or Python object.\n\n    Arguments:\n        env: The `JSONPathEnvironment` this path is bound to.\n        selectors: An iterable of `JSONPathSelector` objects, as generated by\n            a `Parser`.\n        fake_root: Indicates if target JSON values should be wrapped in a single-\n            element array, so as to make the target root value selectable.\n\n\n    Attributes:\n        env: The `JSONPathEnvironment` this path is bound to.\n        selectors: The `JSONPathSelector` instances that make up this path.\n    \"\"\"\n    __slots__ = ('env', 'fake_root', 'selectors')\n\n    def __str__(self) -> str:\n        return self.env.root_token + ''.join((str(selector) for selector in self.selectors))\n\n    def __eq__(self, __value: object) -> bool:\n        return isinstance(__value, JSONPath) and self.selectors == __value.selectors\n\n    def __hash__(self) -> int:\n        return hash(self.selectors)\n\n    def match(self, data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]], *, filter_context: Optional[FilterContextVars]=None) -> Union[JSONPathMatch, None]:\n        \"\"\"Return a `JSONPathMatch` instance for the first object found in _data_.\n\n        `None` is returned if there are no matches.\n\n        Arguments:\n            data: A JSON document or Python object implementing the `Sequence`\n                or `Mapping` interfaces.\n            filter_context: Arbitrary data made available to filters using\n                the _filter context_ selector.\n\n        Returns:\n            A `JSONPathMatch` object for the first match, or `None` if there were\n                no matches.\n\n        Raises:\n            JSONPathSyntaxError: If the path is invalid.\n            JSONPathTypeError: If a filter expression attempts to use types in\n                an incompatible way.\n        \"\"\"\n        try:\n            return next(iter(self.finditer(data, filter_context=filter_context)))\n        except StopIteration:\n            return None\n\n    def query(self, data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]], *, filter_context: Optional[FilterContextVars]=None) -> Query:\n        \"\"\"Return a `Query` iterator over matches found by applying this path to _data_.\n\n        Arguments:\n            data: A JSON document or Python object implementing the `Sequence`\n                or `Mapping` interfaces.\n            filter_context: Arbitrary data made available to filters using\n                the _filter context_ selector.\n\n        Returns:\n            A query iterator.\n\n        Raises:\n            JSONPathSyntaxError: If the path is invalid.\n            JSONPathTypeError: If a filter expression attempts to use types in\n                an incompatible way.\n        \"\"\"\n        return Query(self.finditer(data, filter_context=filter_context), self.env)\n\n    def empty(self) -> bool:\n        \"\"\"Return `True` if this path has no selectors.\"\"\"\n        return not bool(self.selectors)\n\n    def singular_query(self) -> bool:\n        \"\"\"Return `True` if this JSONPath query is a singular query.\"\"\"\n        for selector in self.selectors:\n            if isinstance(selector, (PropertySelector, IndexSelector)):\n                continue\n            if isinstance(selector, ListSelector) and len(selector.items) == 1 and isinstance(selector.items[0], (PropertySelector, IndexSelector)):\n                continue\n            return False\n        return True\n\nclass CompoundJSONPath:\n    \"\"\"Multiple `JSONPath`s combined.\"\"\"\n    __slots__ = ('env', 'path', 'paths')\n\n    def __init__(self, *, env: JSONPathEnvironment, path: Union[JSONPath, CompoundJSONPath], paths: Iterable[Tuple[str, JSONPath]]=()) -> None:\n        self.env = env\n        self.path = path\n        self.paths = tuple(paths)\n\n    def __str__(self) -> str:\n        buf: List[str] = [str(self.path)]\n        for op, path in self.paths:\n            buf.append(f' {op} ')\n            buf.append(str(path))\n        return ''.join(buf)\n\n    def __eq__(self, __value: object) -> bool:\n        return isinstance(__value, CompoundJSONPath) and self.path == __value.path and (self.paths == __value.paths)\n\n    def __hash__(self) -> int:\n        return hash((self.path, self.paths))\n\n    def findall(self, data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]], *, filter_context: Optional[FilterContextVars]=None) -> List[object]:\n        \"\"\"Find all objects in `data` matching the given JSONPath `path`.\n\n        If `data` is a string or a file-like objects, it will be loaded\n        using `json.loads()` and the default `JSONDecoder`.\n\n        Arguments:\n            data: A JSON document or Python object implementing the `Sequence`\n                or `Mapping` interfaces.\n            filter_context: Arbitrary data made available to filters using\n                the _filter context_ selector.\n\n        Returns:\n            A list of matched objects. If there are no matches, the list will\n                be empty.\n\n        Raises:\n            JSONPathSyntaxError: If the path is invalid.\n            JSONPathTypeError: If a filter expression attempts to use types in\n                an incompatible way.\n        \"\"\"\n        objs = self.path.findall(data, filter_context=filter_context)\n        for op, path in self.paths:\n            _objs = path.findall(data, filter_context=filter_context)\n            if op == self.env.union_token:\n                objs.extend(_objs)\n            else:\n                assert op == self.env.intersection_token, op\n                objs = [obj for obj in objs if obj in _objs]\n        return objs\n\n    def finditer(self, data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]], *, filter_context: Optional[FilterContextVars]=None) -> Iterable[JSONPathMatch]:\n        \"\"\"Generate `JSONPathMatch` objects for each match.\n\n        If `data` is a string or a file-like objects, it will be loaded\n        using `json.loads()` and the default `JSONDecoder`.\n\n        Arguments:\n            data: A JSON document or Python object implementing the `Sequence`\n                or `Mapping` interfaces.\n            filter_context: Arbitrary data made available to filters using\n                the _filter context_ selector.\n\n        Returns:\n            An iterator yielding `JSONPathMatch` objects for each match.\n\n        Raises:\n            JSONPathSyntaxError: If the path is invalid.\n            JSONPathTypeError: If a filter expression attempts to use types in\n                an incompatible way.\n        \"\"\"\n        matches = self.path.finditer(data, filter_context=filter_context)\n        for op, path in self.paths:\n            _matches = path.finditer(data, filter_context=filter_context)\n            if op == self.env.union_token:\n                matches = itertools.chain(matches, _matches)\n            else:\n                assert op == self.env.intersection_token\n                _objs = [match.obj for match in _matches]\n                matches = (match for match in matches if match.obj in _objs)\n        return matches\n\n    def match(self, data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]], *, filter_context: Optional[FilterContextVars]=None) -> Union[JSONPathMatch, None]:\n        \"\"\"Return a `JSONPathMatch` instance for the first object found in _data_.\n\n        `None` is returned if there are no matches.\n\n        Arguments:\n            data: A JSON document or Python object implementing the `Sequence`\n                or `Mapping` interfaces.\n            filter_context: Arbitrary data made available to filters using\n                the _filter context_ selector.\n\n        Returns:\n            A `JSONPathMatch` object for the first match, or `None` if there were\n                no matches.\n\n        Raises:\n            JSONPathSyntaxError: If the path is invalid.\n            JSONPathTypeError: If a filter expression attempts to use types in\n                an incompatible way.\n        \"\"\"\n        try:\n            return next(iter(self.finditer(data, filter_context=filter_context)))\n        except StopIteration:\n            return None\n\n    async def findall_async(self, data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]], *, filter_context: Optional[FilterContextVars]=None) -> List[object]:\n        \"\"\"An async version of `findall()`.\"\"\"\n        objs = await self.path.findall_async(data, filter_context=filter_context)\n        for op, path in self.paths:\n            _objs = await path.findall_async(data, filter_context=filter_context)\n            if op == self.env.union_token:\n                objs.extend(_objs)\n            else:\n                assert op == self.env.intersection_token\n                objs = [obj for obj in objs if obj in _objs]\n        return objs\n\n    async def finditer_async(self, data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]], *, filter_context: Optional[FilterContextVars]=None) -> AsyncIterable[JSONPathMatch]:\n        \"\"\"An async version of `finditer()`.\"\"\"\n        matches = await self.path.finditer_async(data, filter_context=filter_context)\n        for op, path in self.paths:\n            _matches = await path.finditer_async(data, filter_context=filter_context)\n            if op == self.env.union_token:\n                matches = _achain(matches, _matches)\n            else:\n                assert op == self.env.intersection_token\n                _objs = [match.obj async for match in _matches]\n                matches = (match async for match in matches if match.obj in _objs)\n        return matches\n\n    def query(self, data: Union[str, IOBase, Sequence[Any], Mapping[str, Any]], *, filter_context: Optional[FilterContextVars]=None) -> Query:\n        \"\"\"Return a `Query` iterator over matches found by applying this path to _data_.\n\n        Arguments:\n            data: A JSON document or Python object implementing the `Sequence`\n                or `Mapping` interfaces.\n            filter_context: Arbitrary data made available to filters using\n                the _filter context_ selector.\n\n        Returns:\n            A query iterator.\n\n        Raises:\n            JSONPathSyntaxError: If the path is invalid.\n            JSONPathTypeError: If a filter expression attempts to use types in\n                an incompatible way.\n        \"\"\"\n        return Query(self.finditer(data, filter_context=filter_context), self.env)\n\n    def union(self, path: JSONPath) -> CompoundJSONPath:\n        \"\"\"Union of this path and another path.\"\"\"\n        return self.__class__(env=self.env, path=self.path, paths=self.paths + ((self.env.union_token, path),))\n\n    def intersection(self, path: JSONPath) -> CompoundJSONPath:\n        \"\"\"Intersection of this path and another path.\"\"\"\n        return self.__class__(env=self.env, path=self.path, paths=self.paths + ((self.env.intersection_token, path),))\nT = TypeVar('T')\n\nasync def _achain(*iterables: AsyncIterable[T]) -> AsyncIterable[T]:\n    for it in iterables:\n        async for element in it:\n            yield element",
    "jsonpath/filter.py": "\"\"\"Filter expression nodes.\"\"\"\nfrom __future__ import annotations\nimport copy\nimport json\nimport re\nfrom abc import ABC\nfrom abc import abstractmethod\nfrom typing import TYPE_CHECKING\nfrom typing import Any\nfrom typing import Callable\nfrom typing import Generic\nfrom typing import Iterable\nfrom typing import List\nfrom typing import Mapping\nfrom typing import Pattern\nfrom typing import Sequence\nfrom typing import TypeVar\nfrom jsonpath.function_extensions.filter_function import ExpressionType\nfrom .exceptions import JSONPathTypeError\nfrom .function_extensions import FilterFunction\nfrom .match import NodeList\nfrom .selectors import Filter as FilterSelector\nfrom .selectors import ListSelector\nif TYPE_CHECKING:\n    from .path import JSONPath\n    from .selectors import FilterContext\n\nclass FilterExpression(ABC):\n    \"\"\"Base class for all filter expression nodes.\"\"\"\n    __slots__ = ('volatile',)\n    FORCE_CACHE = False\n\n    def __init__(self) -> None:\n        self.volatile: bool = any((child.volatile for child in self.children()))\n\n    @abstractmethod\n    def evaluate(self, context: FilterContext) -> object:\n        \"\"\"Resolve the filter expression in the given _context_.\n\n        Arguments:\n            context: Contextual information the expression might choose\n                use during evaluation.\n\n        Returns:\n            The result of evaluating the expression.\n        \"\"\"\n\n    @abstractmethod\n    async def evaluate_async(self, context: FilterContext) -> object:\n        \"\"\"An async version of `evaluate`.\"\"\"\n\n    @abstractmethod\n    def children(self) -> List[FilterExpression]:\n        \"\"\"Return a list of direct child expressions.\"\"\"\n\n    @abstractmethod\n    def set_children(self, children: List[FilterExpression]) -> None:\n        \"\"\"Update this expression's child expressions.\n\n        _children_ is assumed to have the same number of items as is returned\n        by _self.children_, and in the same order.\n        \"\"\"\n\nclass Nil(FilterExpression):\n    \"\"\"The constant `nil`.\n\n    Also aliased as `null` and `None`, sometimes.\n    \"\"\"\n    __slots__ = ()\n\n    def __eq__(self, other: object) -> bool:\n        return other is None or isinstance(other, Nil)\n\n    def __repr__(self) -> str:\n        return 'NIL()'\n\n    def __str__(self) -> str:\n        return 'nil'\n\n    def evaluate(self, _: FilterContext) -> None:\n        return None\n\n    async def evaluate_async(self, _: FilterContext) -> None:\n        return None\n\n    def children(self) -> List[FilterExpression]:\n        return []\n\n    def set_children(self, children: List[FilterExpression]) -> None:\n        return\nNIL = Nil()\n\nclass _Undefined:\n    __slots__ = ()\n\n    def __eq__(self, other: object) -> bool:\n        return other is UNDEFINED_LITERAL or other is UNDEFINED or (isinstance(other, NodeList) and other.empty())\n\n    def __str__(self) -> str:\n        return '<UNDEFINED>'\n\n    def __repr__(self) -> str:\n        return '<UNDEFINED>'\nUNDEFINED = _Undefined()\n\nclass Undefined(FilterExpression):\n    \"\"\"The constant `undefined`.\"\"\"\n    __slots__ = ()\n\n    def __eq__(self, other: object) -> bool:\n        return isinstance(other, Undefined) or other is UNDEFINED or (isinstance(other, NodeList) and len(other) == 0)\n\n    def __str__(self) -> str:\n        return 'undefined'\n\n    def evaluate(self, _: FilterContext) -> object:\n        return UNDEFINED\n\n    async def evaluate_async(self, _: FilterContext) -> object:\n        return UNDEFINED\n\n    def children(self) -> List[FilterExpression]:\n        return []\n\n    def set_children(self, children: List[FilterExpression]) -> None:\n        return\nUNDEFINED_LITERAL = Undefined()\nLITERAL_EXPRESSION_T = TypeVar('LITERAL_EXPRESSION_T')\n\nclass Literal(FilterExpression, Generic[LITERAL_EXPRESSION_T]):\n    \"\"\"Base class for filter expression literals.\"\"\"\n    __slots__ = ('value',)\n\n    def __init__(self, *, value: LITERAL_EXPRESSION_T) -> None:\n        self.value = value\n        super().__init__()\n\n    def __str__(self) -> str:\n        return repr(self.value).lower()\n\n    def __eq__(self, other: object) -> bool:\n        return self.value == other\n\n    def __hash__(self) -> int:\n        return hash(self.value)\n\n    def evaluate(self, _: FilterContext) -> LITERAL_EXPRESSION_T:\n        return self.value\n\n    async def evaluate_async(self, _: FilterContext) -> LITERAL_EXPRESSION_T:\n        return self.value\n\n    def children(self) -> List[FilterExpression]:\n        return []\n\n    def set_children(self, children: List[FilterExpression]) -> None:\n        return\n\nclass BooleanLiteral(Literal[bool]):\n    \"\"\"A Boolean `True` or `False`.\"\"\"\n    __slots__ = ()\nTRUE = BooleanLiteral(value=True)\nFALSE = BooleanLiteral(value=False)\n\nclass StringLiteral(Literal[str]):\n    \"\"\"A string literal.\"\"\"\n    __slots__ = ()\n\n    def __str__(self) -> str:\n        return json.dumps(self.value)\n\nclass IntegerLiteral(Literal[int]):\n    \"\"\"An integer literal.\"\"\"\n    __slots__ = ()\n\nclass FloatLiteral(Literal[float]):\n    \"\"\"A float literal.\"\"\"\n    __slots__ = ()\n\nclass RegexLiteral(Literal[Pattern[str]]):\n    \"\"\"A regex literal.\"\"\"\n    __slots__ = ()\n    RE_FLAG_MAP = {re.A: 'a', re.I: 'i', re.M: 'm', re.S: 's'}\n    RE_UNESCAPE = re.compile('\\\\\\\\(.)')\n\n    def __str__(self) -> str:\n        flags: List[str] = []\n        for flag, ch in self.RE_FLAG_MAP.items():\n            if self.value.flags & flag:\n                flags.append(ch)\n        pattern = re.sub('\\\\\\\\(.)', '\\\\1', self.value.pattern)\n        return f'/{pattern}/{''.join(flags)}'\n\nclass ListLiteral(FilterExpression):\n    \"\"\"A list literal.\"\"\"\n    __slots__ = ('items',)\n\n    def __init__(self, items: List[FilterExpression]) -> None:\n        self.items = items\n        super().__init__()\n\n    def __str__(self) -> str:\n        items = ', '.join((str(item) for item in self.items))\n        return f'[{items}]'\n\n    def __eq__(self, other: object) -> bool:\n        return isinstance(other, ListLiteral) and self.items == other.items\n\n    def evaluate(self, context: FilterContext) -> object:\n        return [item.evaluate(context) for item in self.items]\n\n    async def evaluate_async(self, context: FilterContext) -> object:\n        return [await item.evaluate_async(context) for item in self.items]\n\n    def children(self) -> List[FilterExpression]:\n        return self.items\n\n    def set_children(self, children: List[FilterExpression]) -> None:\n        self.items = children\n\nclass PrefixExpression(FilterExpression):\n    \"\"\"An expression composed of a prefix operator and another expression.\"\"\"\n    __slots__ = ('operator', 'right')\n\n    def __init__(self, operator: str, right: FilterExpression):\n        self.operator = operator\n        self.right = right\n        super().__init__()\n\n    def __str__(self) -> str:\n        return f'{self.operator}{self.right}'\n\n    def __eq__(self, other: object) -> bool:\n        return isinstance(other, PrefixExpression) and self.operator == other.operator and (self.right == other.right)\n\n    def _evaluate(self, context: FilterContext, right: object) -> object:\n        if self.operator == '!':\n            return not context.env.is_truthy(right)\n        raise JSONPathTypeError(f'unknown operator {self.operator} {self.right}')\n\n    def evaluate(self, context: FilterContext) -> object:\n        return self._evaluate(context, self.right.evaluate(context))\n\n    async def evaluate_async(self, context: FilterContext) -> object:\n        return self._evaluate(context, await self.right.evaluate_async(context))\n\n    def children(self) -> List[FilterExpression]:\n        return [self.right]\n\n    def set_children(self, children: List[FilterExpression]) -> None:\n        assert len(children) == 1\n        self.right = children[0]\n\nclass InfixExpression(FilterExpression):\n    \"\"\"A pair of expressions and a comparison or logical operator.\"\"\"\n    __slots__ = ('left', 'operator', 'right', 'logical')\n\n    def __init__(self, left: FilterExpression, operator: str, right: FilterExpression):\n        self.left = left\n        self.operator = operator\n        self.right = right\n        self.logical = operator in ('&&', '||')\n        super().__init__()\n\n    def __str__(self) -> str:\n        if self.logical:\n            return f'({self.left} {self.operator} {self.right})'\n        return f'{self.left} {self.operator} {self.right}'\n\n    def __eq__(self, other: object) -> bool:\n        return isinstance(other, InfixExpression) and self.left == other.left and (self.operator == other.operator) and (self.right == other.right)\n\n    def evaluate(self, context: FilterContext) -> bool:\n        left = self.left.evaluate(context)\n        if not self.logical and isinstance(left, NodeList) and (len(left) == 1):\n            left = left[0].obj\n        right = self.right.evaluate(context)\n        if not self.logical and isinstance(right, NodeList) and (len(right) == 1):\n            right = right[0].obj\n        return context.env.compare(left, self.operator, right)\n\n    async def evaluate_async(self, context: FilterContext) -> bool:\n        left = await self.left.evaluate_async(context)\n        if not self.logical and isinstance(left, NodeList) and (len(left) == 1):\n            left = left[0].obj\n        right = await self.right.evaluate_async(context)\n        if not self.logical and isinstance(right, NodeList) and (len(right) == 1):\n            right = right[0].obj\n        return context.env.compare(left, self.operator, right)\n\n    def children(self) -> List[FilterExpression]:\n        return [self.left, self.right]\n\n    def set_children(self, children: List[FilterExpression]) -> None:\n        assert len(children) == 2\n        self.left = children[0]\n        self.right = children[1]\n\nclass BooleanExpression(FilterExpression):\n    \"\"\"An expression that always evaluates to `True` or `False`.\"\"\"\n    __slots__ = ('expression',)\n\n    def __init__(self, expression: FilterExpression):\n        self.expression = expression\n        super().__init__()\n\n    def cache_tree(self) -> BooleanExpression:\n        \"\"\"Return a copy of _self.expression_ augmented with caching nodes.\"\"\"\n\n        def _cache_tree(expr: FilterExpression) -> FilterExpression:\n            children = expr.children()\n            if expr.volatile:\n                _expr = copy.copy(expr)\n            elif not expr.FORCE_CACHE and len(children) == 0:\n                _expr = expr\n            else:\n                _expr = CachingFilterExpression(copy.copy(expr))\n            _expr.set_children([_cache_tree(child) for child in children])\n            return _expr\n        return BooleanExpression(_cache_tree(copy.copy(self.expression)))\n\n    def cacheable_nodes(self) -> bool:\n        \"\"\"Return `True` if there are any cacheable nodes in this expression tree.\"\"\"\n        return any((isinstance(node, CachingFilterExpression) for node in walk(self.cache_tree())))\n\n    def __str__(self) -> str:\n        return str(self.expression)\n\n    def __eq__(self, other: object) -> bool:\n        return isinstance(other, BooleanExpression) and self.expression == other.expression\n\n    def evaluate(self, context: FilterContext) -> bool:\n        return context.env.is_truthy(self.expression.evaluate(context))\n\n    async def evaluate_async(self, context: FilterContext) -> bool:\n        return context.env.is_truthy(await self.expression.evaluate_async(context))\n\n    def children(self) -> List[FilterExpression]:\n        return [self.expression]\n\n    def set_children(self, children: List[FilterExpression]) -> None:\n        assert len(children) == 1\n        self.expression = children[0]\n\nclass CachingFilterExpression(FilterExpression):\n    \"\"\"A FilterExpression wrapper that caches the result.\"\"\"\n    __slots__ = ('_cached', '_expr')\n    _UNSET = object()\n\n    def __init__(self, expression: FilterExpression):\n        self.volatile = False\n        self._expr = expression\n        self._cached: object = self._UNSET\n\n    def evaluate(self, context: FilterContext) -> object:\n        if self._cached is self._UNSET:\n            self._cached = self._expr.evaluate(context)\n        return self._cached\n\n    async def evaluate_async(self, context: FilterContext) -> object:\n        if self._cached is self._UNSET:\n            self._cached = await self._expr.evaluate_async(context)\n        return self._cached\n\n    def children(self) -> List[FilterExpression]:\n        return self._expr.children()\n\n    def set_children(self, children: List[FilterExpression]) -> None:\n        self._expr.set_children(children)\n\nclass Path(FilterExpression, ABC):\n    \"\"\"Base expression for all _sub paths_ found in filter expressions.\"\"\"\n    __slots__ = ('path',)\n\n    def __init__(self, path: JSONPath) -> None:\n        self.path = path\n        super().__init__()\n\n    def __eq__(self, other: object) -> bool:\n        return isinstance(other, Path) and str(self) == str(other)\n\n    def children(self) -> List[FilterExpression]:\n        _children: List[FilterExpression] = []\n        for segment in self.path.selectors:\n            if isinstance(segment, ListSelector):\n                _children.extend((selector.expression for selector in segment.items if isinstance(selector, FilterSelector)))\n        return _children\n\n    def set_children(self, children: List[FilterExpression]) -> None:\n        return\n\nclass SelfPath(Path):\n    \"\"\"A JSONPath starting at the current node.\"\"\"\n    __slots__ = ()\n\n    def __init__(self, path: JSONPath) -> None:\n        super().__init__(path)\n        self.volatile = True\n\n    def __str__(self) -> str:\n        return '@' + str(self.path)[1:]\n\n    def evaluate(self, context: FilterContext) -> object:\n        if isinstance(context.current, str):\n            if self.path.empty():\n                return context.current\n            return NodeList()\n        if not isinstance(context.current, (Sequence, Mapping)):\n            if self.path.empty():\n                return context.current\n            return NodeList()\n        return NodeList(self.path.finditer(context.current))\n\n    async def evaluate_async(self, context: FilterContext) -> object:\n        if isinstance(context.current, str):\n            if self.path.empty():\n                return context.current\n            return NodeList()\n        if not isinstance(context.current, (Sequence, Mapping)):\n            if self.path.empty():\n                return context.current\n            return NodeList()\n        return NodeList([match async for match in await self.path.finditer_async(context.current)])\n\nclass RootPath(Path):\n    \"\"\"A JSONPath starting at the root node.\"\"\"\n    __slots__ = ()\n    FORCE_CACHE = True\n\n    def __init__(self, path: JSONPath) -> None:\n        super().__init__(path)\n        self.volatile = False\n\n    def __str__(self) -> str:\n        return str(self.path)\n\n    def evaluate(self, context: FilterContext) -> object:\n        return NodeList(self.path.finditer(context.root))\n\n    async def evaluate_async(self, context: FilterContext) -> object:\n        return NodeList([match async for match in await self.path.finditer_async(context.root)])\n\nclass FilterContextPath(Path):\n    \"\"\"A JSONPath starting at the root of any extra context data.\"\"\"\n    __slots__ = ()\n    FORCE_CACHE = True\n\n    def __init__(self, path: JSONPath) -> None:\n        super().__init__(path)\n        self.volatile = False\n\n    def __str__(self) -> str:\n        path_repr = str(self.path)\n        return '_' + path_repr[1:]\n\n    def evaluate(self, context: FilterContext) -> object:\n        return NodeList(self.path.finditer(context.extra_context))\n\n    async def evaluate_async(self, context: FilterContext) -> object:\n        return NodeList([match async for match in await self.path.finditer_async(context.extra_context)])\n\nclass FunctionExtension(FilterExpression):\n    \"\"\"A filter function.\"\"\"\n    __slots__ = ('name', 'args')\n\n    def __init__(self, name: str, args: Sequence[FilterExpression]) -> None:\n        self.name = name\n        self.args = args\n        super().__init__()\n\n    def __str__(self) -> str:\n        args = [str(arg) for arg in self.args]\n        return f'{self.name}({', '.join(args)})'\n\n    def __eq__(self, other: object) -> bool:\n        return isinstance(other, FunctionExtension) and other.name == self.name and (other.args == self.args)\n\n    def evaluate(self, context: FilterContext) -> object:\n        try:\n            func = context.env.function_extensions[self.name]\n        except KeyError:\n            return UNDEFINED\n        args = [arg.evaluate(context) for arg in self.args]\n        return func(*self._unpack_node_lists(func, args))\n\n    async def evaluate_async(self, context: FilterContext) -> object:\n        try:\n            func = context.env.function_extensions[self.name]\n        except KeyError:\n            return UNDEFINED\n        args = [await arg.evaluate_async(context) for arg in self.args]\n        return func(*self._unpack_node_lists(func, args))\n\n    def _unpack_node_lists(self, func: Callable[..., Any], args: List[object]) -> List[object]:\n        if isinstance(func, FilterFunction):\n            _args: List[object] = []\n            for idx, arg in enumerate(args):\n                if func.arg_types[idx] != ExpressionType.NODES and isinstance(arg, NodeList):\n                    if len(arg) == 0:\n                        _args.append(UNDEFINED)\n                    elif len(arg) == 1:\n                        _args.append(arg[0].obj)\n                    else:\n                        _args.append(arg)\n                else:\n                    _args.append(arg)\n            return _args\n        if getattr(func, 'with_node_lists', False):\n            return args\n        return [obj.values_or_singular() if isinstance(obj, NodeList) else obj for obj in args]\n\n    def children(self) -> List[FilterExpression]:\n        return list(self.args)\n\n    def set_children(self, children: List[FilterExpression]) -> None:\n        assert len(children) == len(self.args)\n        self.args = children\n\nclass CurrentKey(FilterExpression):\n    \"\"\"The key/property or index associated with the current object.\"\"\"\n    __slots__ = ()\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.volatile = True\n\n    def __str__(self) -> str:\n        return '#'\n\n    def __eq__(self, other: object) -> bool:\n        return isinstance(other, CurrentKey)\n\n    def evaluate(self, context: FilterContext) -> object:\n        if context.current_key is None:\n            return UNDEFINED\n        return context.current_key\n\n    async def evaluate_async(self, context: FilterContext) -> object:\n        return self.evaluate(context)\n\n    def children(self) -> List[FilterExpression]:\n        return []\n\n    def set_children(self, children: List[FilterExpression]) -> None:\n        return\nCURRENT_KEY = CurrentKey()\n\ndef walk(expr: FilterExpression) -> Iterable[FilterExpression]:\n    \"\"\"Walk the filter expression tree starting at _expr_.\"\"\"\n    yield expr\n    for child in expr.children():\n        yield from walk(child)\nVALUE_TYPE_EXPRESSIONS = (Nil, Undefined, Literal, ListLiteral, CurrentKey)",
    "jsonpath/stream.py": "from __future__ import annotations\nfrom collections import deque\nfrom typing import Deque\nfrom typing import Iterator\nfrom .exceptions import JSONPathSyntaxError\nfrom .token import TOKEN_EOF\nfrom .token import Token\n\nclass TokenStream:\n    \"\"\"Step through or iterate a stream of tokens.\"\"\"\n\n    def __init__(self, token_iter: Iterator[Token]):\n        self.iter = token_iter\n        self._pushed: Deque[Token] = deque()\n        self.current = Token('', '', -1, '')\n        next(self)\n\n    class TokenStreamIterator:\n        \"\"\"An iterable token stream.\"\"\"\n\n        def __iter__(self) -> Iterator[Token]:\n            return self\n\n        def __next__(self) -> Token:\n            tok = self.stream.current\n            if tok.kind is TOKEN_EOF:\n                self.stream.close()\n                raise StopIteration\n            next(self.stream)\n            return tok\n\n    def __iter__(self) -> Iterator[Token]:\n        return self.TokenStreamIterator(self)\n\n    def __next__(self) -> Token:\n        tok = self.current\n        if self._pushed:\n            self.current = self._pushed.popleft()\n        elif self.current.kind is not TOKEN_EOF:\n            try:\n                self.current = next(self.iter)\n            except StopIteration:\n                self.close()\n        return tok\n\n    def __str__(self) -> str:\n        return f'current: {self.current}\\nnext: {self.peek}'\n\n    def next_token(self) -> Token:\n        \"\"\"Return the next token from the stream.\"\"\"\n        return next(self)\n\n    @property\n    def peek(self) -> Token:\n        \"\"\"Look at the next token.\"\"\"\n        current = next(self)\n        result = self.current\n        self.push(current)\n        return result\n\n    def push(self, tok: Token) -> None:\n        \"\"\"Push a token back to the stream.\"\"\"\n        self._pushed.append(self.current)\n        self.current = tok\n\n    def close(self) -> None:\n        \"\"\"Close the stream.\"\"\"\n        self.current = Token(TOKEN_EOF, '', -1, '')\n\n    def expect(self, *typ: str) -> None:\n        if self.current.kind not in typ:\n            if len(typ) == 1:\n                _typ = repr(typ[0])\n            else:\n                _typ = f'one of {typ!r}'\n            raise JSONPathSyntaxError(f'expected {_typ}, found {self.current.kind!r}', token=self.current)\n\n    def expect_peek(self, *typ: str) -> None:\n        if self.peek.kind not in typ:\n            if len(typ) == 1:\n                _typ = repr(typ[0])\n            else:\n                _typ = f'one of {typ!r}'\n            raise JSONPathSyntaxError(f'expected {_typ}, found {self.peek.kind!r}', token=self.peek)",
    "jsonpath/match.py": "\"\"\"The JSONPath match object, as returned from `JSONPath.finditer()`.\"\"\"\nfrom __future__ import annotations\nfrom typing import Any\nfrom typing import List\nfrom typing import Mapping\nfrom typing import Optional\nfrom typing import Sequence\nfrom typing import Tuple\nfrom typing import Union\nfrom .pointer import JSONPointer\nFilterContextVars = Mapping[str, Any]\nPathPart = Union[int, str]\n\nclass JSONPathMatch:\n    \"\"\"A matched object with a concrete path.\n\n    Attributes:\n        children: Matched child nodes. This will only be populated after\n            all children have been visited, usually by using `findall()`\n            or `list(finditer())`.\n        obj: The matched object.\n        parent: The immediate parent to this match in the JSON document.\n            If this is the root node, _parent_ will be `None`.\n        path: The canonical string representation of the path to this match.\n        parts: The keys, indices and/or slices that make up the path to this\n            match.\n        root: A reference to the root node in the JSON document.\n    \"\"\"\n    __slots__ = ('_filter_context', 'children', 'obj', 'parent', 'parts', 'path', 'root')\n    pointer_class = JSONPointer\n\n    def __init__(self, *, filter_context: FilterContextVars, obj: object, parent: Optional[JSONPathMatch], path: str, parts: Tuple[PathPart, ...], root: Union[Sequence[Any], Mapping[str, Any]]) -> None:\n        self._filter_context = filter_context\n        self.children: List[JSONPathMatch] = []\n        self.obj: object = obj\n        self.parent: Optional[JSONPathMatch] = parent\n        self.parts: Tuple[PathPart, ...] = parts\n        self.path: str = path\n        self.root: Union[Sequence[Any], Mapping[str, Any]] = root\n\n    def __str__(self) -> str:\n        return f'{_truncate(str(self.obj), 5)!r} @ {_truncate(self.path, 5)}'\n\n    def add_child(self, *children: JSONPathMatch) -> None:\n        \"\"\"Append one or more children to this match.\"\"\"\n        self.children.extend(children)\n\n    def filter_context(self) -> FilterContextVars:\n        \"\"\"Return filter context data for this match.\"\"\"\n        return self._filter_context\n\n    def pointer(self) -> JSONPointer:\n        \"\"\"Return a `JSONPointer` pointing to this match's path.\"\"\"\n        return JSONPointer.from_match(self)\n\n    @property\n    def value(self) -> object:\n        \"\"\"Return the value associated with this match/node.\"\"\"\n        return self.obj\n\ndef _truncate(val: str, num: int, end: str='...') -> str:\n    words = val.split()\n    if len(words) < num:\n        return ' '.join(words)\n    return ' '.join(words[:num]) + end\n\nclass NodeList(List[JSONPathMatch]):\n    \"\"\"List of JSONPathMatch objects, analogous to the spec's nodelist.\"\"\"\n\n    def values(self) -> List[object]:\n        \"\"\"Return the values from this node list.\"\"\"\n        return [match.obj for match in self]\n\n    def values_or_singular(self) -> object:\n        \"\"\"Return the values from this node list.\"\"\"\n        if len(self) == 1:\n            return self[0].obj\n        return [match.obj for match in self]\n\n    def empty(self) -> bool:\n        \"\"\"Return `True` if this node list is empty.\"\"\"\n        return not bool(self)\n\n    def __str__(self) -> str:\n        return f'NodeList{super().__str__()}'",
    "jsonpath/selectors.py": "\"\"\"JSONPath segments and selectors, as returned from `Parser.parse`.\"\"\"\nfrom __future__ import annotations\nfrom abc import ABC\nfrom abc import abstractmethod\nfrom collections.abc import Mapping\nfrom collections.abc import Sequence\nfrom contextlib import suppress\nfrom typing import TYPE_CHECKING\nfrom typing import Any\nfrom typing import AsyncIterable\nfrom typing import Iterable\nfrom typing import List\nfrom typing import Optional\nfrom typing import TypeVar\nfrom typing import Union\nfrom .exceptions import JSONPathIndexError\nfrom .exceptions import JSONPathTypeError\nif TYPE_CHECKING:\n    from .env import JSONPathEnvironment\n    from .filter import BooleanExpression\n    from .match import JSONPathMatch\n    from .token import Token\n\nclass JSONPathSelector(ABC):\n    \"\"\"Base class for all JSONPath segments and selectors.\"\"\"\n    __slots__ = ('env', 'token')\n\n    def __init__(self, *, env: JSONPathEnvironment, token: Token) -> None:\n        self.env = env\n        self.token = token\n\n    @abstractmethod\n    def resolve(self, matches: Iterable[JSONPathMatch]) -> Iterable[JSONPathMatch]:\n        \"\"\"Apply the segment/selector to each node in _matches_.\n\n        Arguments:\n            matches: Nodes matched by preceding segments/selectors. This is like\n                a lazy _NodeList_, as described in RFC 9535, but each match carries\n                more than the node's value and location.\n\n        Returns:\n            The `JSONPathMatch` instances created by applying this selector to each\n            preceding node.\n        \"\"\"\n\n    @abstractmethod\n    def resolve_async(self, matches: AsyncIterable[JSONPathMatch]) -> AsyncIterable[JSONPathMatch]:\n        \"\"\"An async version of `resolve`.\"\"\"\n\nclass PropertySelector(JSONPathSelector):\n    \"\"\"A shorthand or bracketed property selector.\"\"\"\n    __slots__ = ('name', 'shorthand')\n\n    def __init__(self, *, env: JSONPathEnvironment, token: Token, name: str, shorthand: bool) -> None:\n        super().__init__(env=env, token=token)\n        self.name = name\n        self.shorthand = shorthand\n\n    def __str__(self) -> str:\n        return f\"['{self.name}']\" if self.shorthand else f\"'{self.name}'\"\n\n    def __eq__(self, __value: object) -> bool:\n        return isinstance(__value, PropertySelector) and self.name == __value.name and (self.token == __value.token)\n\n    def __hash__(self) -> int:\n        return hash((self.name, self.token))\n\n    def resolve(self, matches: Iterable[JSONPathMatch]) -> Iterable[JSONPathMatch]:\n        for match in matches:\n            if not isinstance(match.obj, Mapping):\n                continue\n            with suppress(KeyError):\n                _match = self.env.match_class(filter_context=match.filter_context(), obj=self.env.getitem(match.obj, self.name), parent=match, parts=match.parts + (self.name,), path=match.path + f\"['{self.name}']\", root=match.root)\n                match.add_child(_match)\n                yield _match\n\n    async def resolve_async(self, matches: AsyncIterable[JSONPathMatch]) -> AsyncIterable[JSONPathMatch]:\n        async for match in matches:\n            if not isinstance(match.obj, Mapping):\n                continue\n            with suppress(KeyError):\n                _match = self.env.match_class(filter_context=match.filter_context(), obj=await self.env.getitem_async(match.obj, self.name), parent=match, parts=match.parts + (self.name,), path=match.path + f\"['{self.name}']\", root=match.root)\n                match.add_child(_match)\n                yield _match\n\nclass IndexSelector(JSONPathSelector):\n    \"\"\"Select an element from an array by index.\n\n    Considering we don't require mapping (JSON object) keys/properties to\n    be quoted, and that we support mappings with numeric keys, we also check\n    to see if the \"index\" is a mapping key, which is non-standard.\n    \"\"\"\n    __slots__ = ('index', '_as_key')\n\n    def __init__(self, *, env: JSONPathEnvironment, token: Token, index: int) -> None:\n        if index < env.min_int_index or index > env.max_int_index:\n            raise JSONPathIndexError('index out of range', token=token)\n        super().__init__(env=env, token=token)\n        self.index = index\n        self._as_key = str(self.index)\n\n    def __str__(self) -> str:\n        return str(self.index)\n\n    def __eq__(self, __value: object) -> bool:\n        return isinstance(__value, IndexSelector) and self.index == __value.index and (self.token == __value.token)\n\n    def __hash__(self) -> int:\n        return hash((self.index, self.token))\n\n    def _normalized_index(self, obj: Sequence[object]) -> int:\n        if self.index < 0 and len(obj) >= abs(self.index):\n            return len(obj) + self.index\n        return self.index\n\n    def resolve(self, matches: Iterable[JSONPathMatch]) -> Iterable[JSONPathMatch]:\n        for match in matches:\n            if isinstance(match.obj, Mapping):\n                with suppress(KeyError):\n                    _match = self.env.match_class(filter_context=match.filter_context(), obj=self.env.getitem(match.obj, self._as_key), parent=match, parts=match.parts + (self._as_key,), path=f\"{match.path}['{self.index}']\", root=match.root)\n                    match.add_child(_match)\n                    yield _match\n            elif isinstance(match.obj, Sequence) and (not isinstance(match.obj, str)):\n                norm_index = self._normalized_index(match.obj)\n                with suppress(IndexError):\n                    _match = self.env.match_class(filter_context=match.filter_context(), obj=self.env.getitem(match.obj, self.index), parent=match, parts=match.parts + (norm_index,), path=match.path + f'[{norm_index}]', root=match.root)\n                    match.add_child(_match)\n                    yield _match\n\n    async def resolve_async(self, matches: AsyncIterable[JSONPathMatch]) -> AsyncIterable[JSONPathMatch]:\n        async for match in matches:\n            if isinstance(match.obj, Mapping):\n                with suppress(KeyError):\n                    _match = self.env.match_class(filter_context=match.filter_context(), obj=await self.env.getitem_async(match.obj, self._as_key), parent=match, parts=match.parts + (self._as_key,), path=f\"{match.path}['{self.index}']\", root=match.root)\n                    match.add_child(_match)\n                    yield _match\n            elif isinstance(match.obj, Sequence) and (not isinstance(match.obj, str)):\n                norm_index = self._normalized_index(match.obj)\n                with suppress(IndexError):\n                    _match = self.env.match_class(filter_context=match.filter_context(), obj=await self.env.getitem_async(match.obj, self.index), parent=match, parts=match.parts + (norm_index,), path=match.path + f'[{norm_index}]', root=match.root)\n                    match.add_child(_match)\n                    yield _match\n\nclass KeysSelector(JSONPathSelector):\n    \"\"\"Select mapping/object keys/properties.\n\n    NOTE: This is a non-standard selector.\n    \"\"\"\n    __slots__ = ('shorthand',)\n\n    def __init__(self, *, env: JSONPathEnvironment, token: Token, shorthand: bool) -> None:\n        super().__init__(env=env, token=token)\n        self.shorthand = shorthand\n\n    def __str__(self) -> str:\n        return f'[{self.env.keys_selector_token}]' if self.shorthand else self.env.keys_selector_token\n\n    def __eq__(self, __value: object) -> bool:\n        return isinstance(__value, KeysSelector) and self.token == __value.token\n\n    def __hash__(self) -> int:\n        return hash(self.token)\n\n    def _keys(self, match: JSONPathMatch) -> Iterable[JSONPathMatch]:\n        if isinstance(match.obj, Mapping):\n            for i, key in enumerate(match.obj.keys()):\n                _match = self.env.match_class(filter_context=match.filter_context(), obj=key, parent=match, parts=match.parts + (f'{self.env.keys_selector_token}{key}',), path=f'{match.path}[{self.env.keys_selector_token}][{i}]', root=match.root)\n                match.add_child(_match)\n                yield _match\n\n    def resolve(self, matches: Iterable[JSONPathMatch]) -> Iterable[JSONPathMatch]:\n        for match in matches:\n            yield from self._keys(match)\n\n    async def resolve_async(self, matches: AsyncIterable[JSONPathMatch]) -> AsyncIterable[JSONPathMatch]:\n        async for match in matches:\n            for _match in self._keys(match):\n                yield _match\n\nclass SliceSelector(JSONPathSelector):\n    \"\"\"Sequence slicing selector.\"\"\"\n    __slots__ = ('slice',)\n\n    def __init__(self, *, env: JSONPathEnvironment, token: Token, start: Optional[int]=None, stop: Optional[int]=None, step: Optional[int]=None) -> None:\n        super().__init__(env=env, token=token)\n        self._check_range(start, stop, step)\n        self.slice = slice(start, stop, step)\n\n    def __str__(self) -> str:\n        stop = self.slice.stop if self.slice.stop is not None else ''\n        start = self.slice.start if self.slice.start is not None else ''\n        step = self.slice.step if self.slice.step is not None else '1'\n        return f'{start}:{stop}:{step}'\n\n    def __eq__(self, __value: object) -> bool:\n        return isinstance(__value, SliceSelector) and self.slice == __value.slice and (self.token == __value.token)\n\n    def __hash__(self) -> int:\n        return hash((str(self), self.token))\n\n    def _check_range(self, *indices: Optional[int]) -> None:\n        for i in indices:\n            if i is not None and (i < self.env.min_int_index or i > self.env.max_int_index):\n                raise JSONPathIndexError('index out of range', token=self.token)\n\n    def _normalized_index(self, obj: Sequence[object], index: int) -> int:\n        if index < 0 and len(obj) >= abs(index):\n            return len(obj) + index\n        return index\n\n    def resolve(self, matches: Iterable[JSONPathMatch]) -> Iterable[JSONPathMatch]:\n        for match in matches:\n            if not isinstance(match.obj, Sequence) or self.slice.step == 0:\n                continue\n            idx = self.slice.start or 0\n            step = self.slice.step or 1\n            for obj in self.env.getitem(match.obj, self.slice):\n                norm_index = self._normalized_index(match.obj, idx)\n                _match = self.env.match_class(filter_context=match.filter_context(), obj=obj, parent=match, parts=match.parts + (norm_index,), path=f'{match.path}[{norm_index}]', root=match.root)\n                match.add_child(_match)\n                yield _match\n                idx += step\n\n    async def resolve_async(self, matches: AsyncIterable[JSONPathMatch]) -> AsyncIterable[JSONPathMatch]:\n        async for match in matches:\n            if not isinstance(match.obj, Sequence) or self.slice.step == 0:\n                continue\n            idx = self.slice.start or 0\n            step = self.slice.step or 1\n            for obj in await self.env.getitem_async(match.obj, self.slice):\n                norm_index = self._normalized_index(match.obj, idx)\n                _match = self.env.match_class(filter_context=match.filter_context(), obj=obj, parent=match, parts=match.parts + (norm_index,), path=f'{match.path}[{norm_index}]', root=match.root)\n                match.add_child(_match)\n                yield _match\n                idx += step\n\nclass WildSelector(JSONPathSelector):\n    \"\"\"Select all items from a sequence/array or values from a mapping/object.\"\"\"\n    __slots__ = ('shorthand',)\n\n    def __init__(self, *, env: JSONPathEnvironment, token: Token, shorthand: bool) -> None:\n        super().__init__(env=env, token=token)\n        self.shorthand = shorthand\n\n    def __str__(self) -> str:\n        return '[*]' if self.shorthand else '*'\n\n    def __eq__(self, __value: object) -> bool:\n        return isinstance(__value, WildSelector) and self.token == __value.token\n\n    def __hash__(self) -> int:\n        return hash(self.token)\n\n    def resolve(self, matches: Iterable[JSONPathMatch]) -> Iterable[JSONPathMatch]:\n        for match in matches:\n            if isinstance(match.obj, str):\n                continue\n            if isinstance(match.obj, Mapping):\n                for key, val in match.obj.items():\n                    _match = self.env.match_class(filter_context=match.filter_context(), obj=val, parent=match, parts=match.parts + (key,), path=match.path + f\"['{key}']\", root=match.root)\n                    match.add_child(_match)\n                    yield _match\n            elif isinstance(match.obj, Sequence):\n                for i, val in enumerate(match.obj):\n                    _match = self.env.match_class(filter_context=match.filter_context(), obj=val, parent=match, parts=match.parts + (i,), path=f'{match.path}[{i}]', root=match.root)\n                    match.add_child(_match)\n                    yield _match\n\n    async def resolve_async(self, matches: AsyncIterable[JSONPathMatch]) -> AsyncIterable[JSONPathMatch]:\n        async for match in matches:\n            if isinstance(match.obj, Mapping):\n                for key, val in match.obj.items():\n                    _match = self.env.match_class(filter_context=match.filter_context(), obj=val, parent=match, parts=match.parts + (key,), path=match.path + f\"['{key}']\", root=match.root)\n                    match.add_child(_match)\n                    yield _match\n            elif isinstance(match.obj, Sequence):\n                for i, val in enumerate(match.obj):\n                    _match = self.env.match_class(filter_context=match.filter_context(), obj=val, parent=match, parts=match.parts + (i,), path=f'{match.path}[{i}]', root=match.root)\n                    match.add_child(_match)\n                    yield _match\n\nclass RecursiveDescentSelector(JSONPathSelector):\n    \"\"\"A JSONPath selector that visits all nodes recursively.\n\n    NOTE: Strictly this is a \"segment\", not a \"selector\".\n    \"\"\"\n\n    def __str__(self) -> str:\n        return '..'\n\n    def __eq__(self, __value: object) -> bool:\n        return isinstance(__value, RecursiveDescentSelector) and self.token == __value.token\n\n    def __hash__(self) -> int:\n        return hash(self.token)\n\n    def _expand(self, match: JSONPathMatch) -> Iterable[JSONPathMatch]:\n        if isinstance(match.obj, Mapping):\n            for key, val in match.obj.items():\n                if isinstance(val, str):\n                    pass\n                elif isinstance(val, (Mapping, Sequence)):\n                    _match = self.env.match_class(filter_context=match.filter_context(), obj=val, parent=match, parts=match.parts + (key,), path=match.path + f\"['{key}']\", root=match.root)\n                    match.add_child(_match)\n                    yield _match\n                    yield from self._expand(_match)\n        elif isinstance(match.obj, Sequence) and (not isinstance(match.obj, str)):\n            for i, val in enumerate(match.obj):\n                if isinstance(val, str):\n                    pass\n                elif isinstance(val, (Mapping, Sequence)):\n                    _match = self.env.match_class(filter_context=match.filter_context(), obj=val, parent=match, parts=match.parts + (i,), path=f'{match.path}[{i}]', root=match.root)\n                    match.add_child(_match)\n                    yield _match\n                    yield from self._expand(_match)\n\n    def resolve(self, matches: Iterable[JSONPathMatch]) -> Iterable[JSONPathMatch]:\n        for match in matches:\n            yield match\n            yield from self._expand(match)\n\n    async def resolve_async(self, matches: AsyncIterable[JSONPathMatch]) -> AsyncIterable[JSONPathMatch]:\n        async for match in matches:\n            yield match\n            for _match in self._expand(match):\n                yield _match\nT = TypeVar('T')\n\nasync def _alist(it: List[T]) -> AsyncIterable[T]:\n    for item in it:\n        yield item\n\nclass ListSelector(JSONPathSelector):\n    \"\"\"A bracketed list of selectors, the results of which are concatenated together.\n\n    NOTE: Strictly this is a \"segment\", not a \"selector\".\n    \"\"\"\n    __slots__ = ('items',)\n\n    def __init__(self, *, env: JSONPathEnvironment, token: Token, items: List[Union[SliceSelector, KeysSelector, IndexSelector, PropertySelector, WildSelector, Filter]]) -> None:\n        super().__init__(env=env, token=token)\n        self.items = tuple(items)\n\n    def __str__(self) -> str:\n        return f'[{', '.join((str(itm) for itm in self.items))}]'\n\n    def __eq__(self, __value: object) -> bool:\n        return isinstance(__value, ListSelector) and self.items == __value.items and (self.token == __value.token)\n\n    def __hash__(self) -> int:\n        return hash((self.items, self.token))\n\nclass Filter(JSONPathSelector):\n    \"\"\"Filter sequence/array items or mapping/object values with a filter expression.\"\"\"\n    __slots__ = ('expression', 'cacheable_nodes')\n\n    def __init__(self, *, env: JSONPathEnvironment, token: Token, expression: BooleanExpression) -> None:\n        super().__init__(env=env, token=token)\n        self.expression = expression\n        self.cacheable_nodes = self.expression.cacheable_nodes()\n\n    def __str__(self) -> str:\n        return f'?{self.expression}'\n\n    def __eq__(self, __value: object) -> bool:\n        return isinstance(__value, Filter) and self.expression == __value.expression and (self.token == __value.token)\n\n    def __hash__(self) -> int:\n        return hash((str(self.expression), self.token))\n\n    def resolve(self, matches: Iterable[JSONPathMatch]) -> Iterable[JSONPathMatch]:\n        if self.cacheable_nodes and self.env.filter_caching:\n            expr = self.expression.cache_tree()\n        else:\n            expr = self.expression\n        for match in matches:\n            if isinstance(match.obj, Mapping):\n                for key, val in match.obj.items():\n                    context = FilterContext(env=self.env, current=val, root=match.root, extra_context=match.filter_context(), current_key=key)\n                    try:\n                        if expr.evaluate(context):\n                            _match = self.env.match_class(filter_context=match.filter_context(), obj=val, parent=match, parts=match.parts + (key,), path=match.path + f\"['{key}']\", root=match.root)\n                            match.add_child(_match)\n                            yield _match\n                    except JSONPathTypeError as err:\n                        if not err.token:\n                            err.token = self.token\n                        raise\n            elif isinstance(match.obj, Sequence) and (not isinstance(match.obj, str)):\n                for i, obj in enumerate(match.obj):\n                    context = FilterContext(env=self.env, current=obj, root=match.root, extra_context=match.filter_context(), current_key=i)\n                    try:\n                        if expr.evaluate(context):\n                            _match = self.env.match_class(filter_context=match.filter_context(), obj=obj, parent=match, parts=match.parts + (i,), path=f'{match.path}[{i}]', root=match.root)\n                            match.add_child(_match)\n                            yield _match\n                    except JSONPathTypeError as err:\n                        if not err.token:\n                            err.token = self.token\n                        raise\n\n    async def resolve_async(self, matches: AsyncIterable[JSONPathMatch]) -> AsyncIterable[JSONPathMatch]:\n        if self.cacheable_nodes and self.env.filter_caching:\n            expr = self.expression.cache_tree()\n        else:\n            expr = self.expression\n        async for match in matches:\n            if isinstance(match.obj, Mapping):\n                for key, val in match.obj.items():\n                    context = FilterContext(env=self.env, current=val, root=match.root, extra_context=match.filter_context(), current_key=key)\n                    try:\n                        result = await expr.evaluate_async(context)\n                    except JSONPathTypeError as err:\n                        if not err.token:\n                            err.token = self.token\n                        raise\n                    if result:\n                        _match = self.env.match_class(filter_context=match.filter_context(), obj=val, parent=match, parts=match.parts + (key,), path=match.path + f\"['{key}']\", root=match.root)\n                        match.add_child(_match)\n                        yield _match\n            elif isinstance(match.obj, Sequence) and (not isinstance(match.obj, str)):\n                for i, obj in enumerate(match.obj):\n                    context = FilterContext(env=self.env, current=obj, root=match.root, extra_context=match.filter_context(), current_key=i)\n                    try:\n                        result = await expr.evaluate_async(context)\n                    except JSONPathTypeError as err:\n                        if not err.token:\n                            err.token = self.token\n                        raise\n                    if result:\n                        _match = self.env.match_class(filter_context=match.filter_context(), obj=obj, parent=match, parts=match.parts + (i,), path=f'{match.path}[{i}]', root=match.root)\n                        match.add_child(_match)\n                        yield _match\n\nclass FilterContext:\n    \"\"\"Contextual information and data for evaluating a filter expression.\"\"\"\n    __slots__ = ('current_key', 'current', 'env', 'extra_context', 'root')\n\n    def __init__(self, *, env: JSONPathEnvironment, current: object, root: Union[Sequence[Any], Mapping[str, Any]], extra_context: Optional[Mapping[str, Any]]=None, current_key: Union[str, int, None]=None) -> None:\n        self.env = env\n        self.current = current\n        self.root = root\n        self.extra_context = extra_context or {}\n        self.current_key = current_key\n\n    def __str__(self) -> str:\n        return f'FilterContext(current={self.current}, extra_context={self.extra_context!r})'",
    "jsonpath/lex.py": "\"\"\"JSONPath tokenization.\"\"\"\nfrom __future__ import annotations\nimport re\nfrom functools import partial\nfrom typing import TYPE_CHECKING\nfrom typing import Iterator\nfrom typing import Pattern\nfrom .exceptions import JSONPathSyntaxError\nfrom .token import TOKEN_AND\nfrom .token import TOKEN_BARE_PROPERTY\nfrom .token import TOKEN_COMMA\nfrom .token import TOKEN_CONTAINS\nfrom .token import TOKEN_DDOT\nfrom .token import TOKEN_DOT_PROPERTY\nfrom .token import TOKEN_DOUBLE_QUOTE_STRING\nfrom .token import TOKEN_EQ\nfrom .token import TOKEN_FAKE_ROOT\nfrom .token import TOKEN_FALSE\nfrom .token import TOKEN_FILTER\nfrom .token import TOKEN_FILTER_CONTEXT\nfrom .token import TOKEN_FLOAT\nfrom .token import TOKEN_FUNCTION\nfrom .token import TOKEN_GE\nfrom .token import TOKEN_GT\nfrom .token import TOKEN_ILLEGAL\nfrom .token import TOKEN_IN\nfrom .token import TOKEN_INT\nfrom .token import TOKEN_INTERSECTION\nfrom .token import TOKEN_KEY\nfrom .token import TOKEN_KEYS\nfrom .token import TOKEN_LE\nfrom .token import TOKEN_LG\nfrom .token import TOKEN_LIST_SLICE\nfrom .token import TOKEN_LIST_START\nfrom .token import TOKEN_LPAREN\nfrom .token import TOKEN_LT\nfrom .token import TOKEN_MISSING\nfrom .token import TOKEN_NE\nfrom .token import TOKEN_NIL\nfrom .token import TOKEN_NONE\nfrom .token import TOKEN_NOT\nfrom .token import TOKEN_NULL\nfrom .token import TOKEN_OR\nfrom .token import TOKEN_PROPERTY\nfrom .token import TOKEN_RBRACKET\nfrom .token import TOKEN_RE\nfrom .token import TOKEN_RE_FLAGS\nfrom .token import TOKEN_RE_PATTERN\nfrom .token import TOKEN_ROOT\nfrom .token import TOKEN_RPAREN\nfrom .token import TOKEN_SELF\nfrom .token import TOKEN_SINGLE_QUOTE_STRING\nfrom .token import TOKEN_SKIP\nfrom .token import TOKEN_SLICE_START\nfrom .token import TOKEN_SLICE_STEP\nfrom .token import TOKEN_SLICE_STOP\nfrom .token import TOKEN_TRUE\nfrom .token import TOKEN_UNDEFINED\nfrom .token import TOKEN_UNION\nfrom .token import TOKEN_WILD\nfrom .token import Token\nif TYPE_CHECKING:\n    from . import JSONPathEnvironment\n\nclass Lexer:\n    \"\"\"Tokenize a JSONPath string.\n\n    Some customization can be achieved by subclassing _Lexer_ and setting\n    class attributes. Then setting `lexer_class` on a `JSONPathEnvironment`.\n\n    Attributes:\n        key_pattern: The regular expression pattern used to match mapping\n            keys/properties.\n        logical_not_pattern: The regular expression pattern used to match\n            logical negation tokens. By default, `not` and `!` are\n            equivalent.\n        logical_and_pattern: The regular expression pattern used to match\n            logical _and_ tokens. By default, `and` and `&&` are equivalent.\n        logical_or_pattern: The regular expression pattern used to match\n            logical _or_ tokens. By default, `or` and `||` are equivalent.\n    \"\"\"\n    key_pattern = '[\\\\u0080-\\\\uFFFFa-zA-Z_][\\\\u0080-\\\\uFFFFa-zA-Z0-9_-]*'\n    logical_not_pattern = '(?:not|!)'\n    logical_and_pattern = '(?:&&|and)'\n    logical_or_pattern = '(?:\\\\|\\\\||or)'\n\n    def compile_rules(self) -> Pattern[str]:\n        \"\"\"Prepare regular expression rules.\"\"\"\n        env_tokens = [(TOKEN_ROOT, self.env.root_token), (TOKEN_FAKE_ROOT, self.env.fake_root_token), (TOKEN_SELF, self.env.self_token), (TOKEN_KEY, self.env.key_token), (TOKEN_UNION, self.env.union_token), (TOKEN_INTERSECTION, self.env.intersection_token), (TOKEN_FILTER_CONTEXT, self.env.filter_context_token), (TOKEN_KEYS, self.env.keys_selector_token)]\n        rules = [(TOKEN_DOUBLE_QUOTE_STRING, self.double_quote_pattern), (TOKEN_SINGLE_QUOTE_STRING, self.single_quote_pattern), (TOKEN_RE_PATTERN, self.re_pattern), (TOKEN_LIST_SLICE, self.slice_list_pattern), (TOKEN_FUNCTION, self.function_pattern), (TOKEN_DOT_PROPERTY, self.dot_property_pattern), (TOKEN_FLOAT, '-?\\\\d+\\\\.\\\\d*(?:[eE][+-]?\\\\d+)?'), (TOKEN_INT, '-?\\\\d+(?P<G_EXP>[eE][+\\\\-]?\\\\d+)?\\\\b'), (TOKEN_DDOT, '\\\\.\\\\.'), (TOKEN_AND, self.logical_and_pattern), (TOKEN_OR, self.logical_or_pattern), *[(token, re.escape(pattern)) for token, pattern in sorted(env_tokens, key=lambda x: len(x[1]), reverse=True) if pattern], (TOKEN_WILD, '\\\\*'), (TOKEN_FILTER, '\\\\?'), (TOKEN_IN, 'in'), (TOKEN_TRUE, '[Tt]rue'), (TOKEN_FALSE, '[Ff]alse'), (TOKEN_NIL, '[Nn]il'), (TOKEN_NULL, '[Nn]ull'), (TOKEN_NONE, '[Nn]one'), (TOKEN_CONTAINS, 'contains'), (TOKEN_UNDEFINED, 'undefined'), (TOKEN_MISSING, 'missing'), (TOKEN_LIST_START, '\\\\['), (TOKEN_RBRACKET, ']'), (TOKEN_COMMA, ','), (TOKEN_EQ, '=='), (TOKEN_NE, '!='), (TOKEN_LG, '<>'), (TOKEN_LE, '<='), (TOKEN_GE, '>='), (TOKEN_RE, '=~'), (TOKEN_LT, '<'), (TOKEN_GT, '>'), (TOKEN_NOT, self.logical_not_pattern), (TOKEN_BARE_PROPERTY, self.key_pattern), (TOKEN_LPAREN, '\\\\('), (TOKEN_RPAREN, '\\\\)'), (TOKEN_SKIP, '[ \\\\n\\\\t\\\\r\\\\.]+'), (TOKEN_ILLEGAL, '.')]\n        return re.compile('|'.join((f'(?P<{token}>{pattern})' for token, pattern in rules)), re.DOTALL)\n\n    def tokenize(self, path: str) -> Iterator[Token]:\n        \"\"\"Generate a sequence of tokens from a JSONPath string.\"\"\"\n        _token = partial(Token, path=path)\n        for match in self.rules.finditer(path):\n            kind = match.lastgroup\n            assert kind is not None\n            if kind == TOKEN_DOT_PROPERTY:\n                yield _token(kind=TOKEN_PROPERTY, value=match.group('G_PROP'), index=match.start('G_PROP'))\n            elif kind == TOKEN_BARE_PROPERTY:\n                yield _token(kind=TOKEN_BARE_PROPERTY, value=match.group(), index=match.start())\n            elif kind == TOKEN_LIST_SLICE:\n                yield _token(kind=TOKEN_SLICE_START, value=match.group('G_LSLICE_START'), index=match.start('G_LSLICE_START'))\n                yield _token(kind=TOKEN_SLICE_STOP, value=match.group('G_LSLICE_STOP'), index=match.start('G_LSLICE_STOP'))\n                yield _token(kind=TOKEN_SLICE_STEP, value=match.group('G_LSLICE_STEP') or '', index=match.start('G_LSLICE_STEP'))\n            elif kind == TOKEN_DOUBLE_QUOTE_STRING:\n                yield _token(kind=TOKEN_DOUBLE_QUOTE_STRING, value=match.group('G_DQUOTE'), index=match.start('G_DQUOTE'))\n            elif kind == TOKEN_SINGLE_QUOTE_STRING:\n                yield _token(kind=TOKEN_SINGLE_QUOTE_STRING, value=match.group('G_SQUOTE'), index=match.start('G_SQUOTE'))\n            elif kind == TOKEN_INT:\n                if match.group('G_EXP') and match.group('G_EXP')[1] == '-':\n                    yield _token(kind=TOKEN_FLOAT, value=match.group(), index=match.start())\n                else:\n                    yield _token(kind=TOKEN_INT, value=match.group(), index=match.start())\n            elif kind == TOKEN_RE_PATTERN:\n                yield _token(kind=TOKEN_RE_PATTERN, value=match.group('G_RE'), index=match.start('G_RE'))\n                yield _token(TOKEN_RE_FLAGS, value=match.group('G_RE_FLAGS'), index=match.start('G_RE_FLAGS'))\n            elif kind in (TOKEN_NONE, TOKEN_NULL):\n                yield _token(kind=TOKEN_NIL, value=match.group(), index=match.start())\n            elif kind == TOKEN_FUNCTION:\n                yield _token(kind=TOKEN_FUNCTION, value=match.group('G_FUNC'), index=match.start('G_FUNC'))\n            elif kind == TOKEN_SKIP:\n                continue\n            elif kind == TOKEN_ILLEGAL:\n                raise JSONPathSyntaxError(f'unexpected token {match.group()!r}', token=_token(TOKEN_ILLEGAL, value=match.group(), index=match.start()))\n            else:\n                yield _token(kind=kind, value=match.group(), index=match.start())",
    "jsonpath/parse.py": "\"\"\"The default JSONPath parser.\"\"\"\nfrom __future__ import annotations\nimport json\nimport re\nfrom typing import TYPE_CHECKING\nfrom typing import Callable\nfrom typing import Dict\nfrom typing import Iterable\nfrom typing import List\nfrom typing import Optional\nfrom typing import Union\nfrom jsonpath.function_extensions.filter_function import ExpressionType\nfrom jsonpath.function_extensions.filter_function import FilterFunction\nfrom .exceptions import JSONPathSyntaxError\nfrom .exceptions import JSONPathTypeError\nfrom .filter import CURRENT_KEY\nfrom .filter import FALSE\nfrom .filter import NIL\nfrom .filter import TRUE\nfrom .filter import UNDEFINED_LITERAL\nfrom .filter import BooleanExpression\nfrom .filter import FilterContextPath\nfrom .filter import FilterExpression\nfrom .filter import FloatLiteral\nfrom .filter import FunctionExtension\nfrom .filter import InfixExpression\nfrom .filter import IntegerLiteral\nfrom .filter import ListLiteral\nfrom .filter import Literal\nfrom .filter import Nil\nfrom .filter import Path\nfrom .filter import PrefixExpression\nfrom .filter import RegexLiteral\nfrom .filter import RootPath\nfrom .filter import SelfPath\nfrom .filter import StringLiteral\nfrom .path import JSONPath\nfrom .selectors import Filter\nfrom .selectors import IndexSelector\nfrom .selectors import JSONPathSelector\nfrom .selectors import KeysSelector\nfrom .selectors import ListSelector\nfrom .selectors import PropertySelector\nfrom .selectors import RecursiveDescentSelector\nfrom .selectors import SliceSelector\nfrom .selectors import WildSelector\nfrom .token import TOKEN_AND\nfrom .token import TOKEN_BARE_PROPERTY\nfrom .token import TOKEN_COMMA\nfrom .token import TOKEN_CONTAINS\nfrom .token import TOKEN_DDOT\nfrom .token import TOKEN_DOUBLE_QUOTE_STRING\nfrom .token import TOKEN_EOF\nfrom .token import TOKEN_EQ\nfrom .token import TOKEN_FAKE_ROOT\nfrom .token import TOKEN_FALSE\nfrom .token import TOKEN_FILTER\nfrom .token import TOKEN_FILTER_CONTEXT\nfrom .token import TOKEN_FLOAT\nfrom .token import TOKEN_FUNCTION\nfrom .token import TOKEN_GE\nfrom .token import TOKEN_GT\nfrom .token import TOKEN_IN\nfrom .token import TOKEN_INT\nfrom .token import TOKEN_INTERSECTION\nfrom .token import TOKEN_KEY\nfrom .token import TOKEN_KEYS\nfrom .token import TOKEN_LE\nfrom .token import TOKEN_LG\nfrom .token import TOKEN_LIST_START\nfrom .token import TOKEN_LPAREN\nfrom .token import TOKEN_LT\nfrom .token import TOKEN_MISSING\nfrom .token import TOKEN_NE\nfrom .token import TOKEN_NIL\nfrom .token import TOKEN_NONE\nfrom .token import TOKEN_NOT\nfrom .token import TOKEN_NULL\nfrom .token import TOKEN_OR\nfrom .token import TOKEN_PROPERTY\nfrom .token import TOKEN_RBRACKET\nfrom .token import TOKEN_RE\nfrom .token import TOKEN_RE_FLAGS\nfrom .token import TOKEN_RE_PATTERN\nfrom .token import TOKEN_ROOT\nfrom .token import TOKEN_RPAREN\nfrom .token import TOKEN_SELF\nfrom .token import TOKEN_SINGLE_QUOTE_STRING\nfrom .token import TOKEN_SLICE_START\nfrom .token import TOKEN_SLICE_STEP\nfrom .token import TOKEN_SLICE_STOP\nfrom .token import TOKEN_TRUE\nfrom .token import TOKEN_UNDEFINED\nfrom .token import TOKEN_UNION\nfrom .token import TOKEN_WILD\nfrom .token import Token\nif TYPE_CHECKING:\n    from .env import JSONPathEnvironment\n    from .stream import TokenStream\nINVALID_NAME_SELECTOR_CHARS = ['\\x00', '\\x01', '\\x02', '\\x03', '\\x04', '\\x05', '\\x06', '\\x07', '\\x08', '\\t', '\\n', '\\x0b', '\\x0c', '\\r', '\\x0e', '\\x0f', '\\x10', '\\x11', '\\x12', '\\x13', '\\x14', '\\x15', '\\x16', '\\x17', '\\x18', '\\x19', '\\x1a', '\\x1b', '\\x1c', '\\x1d', '\\x1e', '\\x1f']\n\nclass Parser:\n    \"\"\"A JSONPath parser bound to a JSONPathEnvironment.\"\"\"\n    PRECEDENCE_LOWEST = 1\n    PRECEDENCE_LOGICALRIGHT = 2\n    PRECEDENCE_LOGICAL_OR = 3\n    PRECEDENCE_LOGICAL_AND = 4\n    PRECEDENCE_RELATIONAL = 5\n    PRECEDENCE_MEMBERSHIP = 6\n    PRECEDENCE_PREFIX = 7\n    PRECEDENCES = {TOKEN_AND: PRECEDENCE_LOGICAL_AND, TOKEN_CONTAINS: PRECEDENCE_MEMBERSHIP, TOKEN_EQ: PRECEDENCE_RELATIONAL, TOKEN_GE: PRECEDENCE_RELATIONAL, TOKEN_GT: PRECEDENCE_RELATIONAL, TOKEN_IN: PRECEDENCE_MEMBERSHIP, TOKEN_LE: PRECEDENCE_RELATIONAL, TOKEN_LG: PRECEDENCE_RELATIONAL, TOKEN_LT: PRECEDENCE_RELATIONAL, TOKEN_NE: PRECEDENCE_RELATIONAL, TOKEN_NOT: PRECEDENCE_PREFIX, TOKEN_OR: PRECEDENCE_LOGICAL_OR, TOKEN_RE: PRECEDENCE_RELATIONAL, TOKEN_RPAREN: PRECEDENCE_LOWEST}\n    BINARY_OPERATORS = {TOKEN_AND: '&&', TOKEN_CONTAINS: 'contains', TOKEN_EQ: '==', TOKEN_GE: '>=', TOKEN_GT: '>', TOKEN_IN: 'in', TOKEN_LE: '<=', TOKEN_LG: '<>', TOKEN_LT: '<', TOKEN_NE: '!=', TOKEN_OR: '||', TOKEN_RE: '=~'}\n    COMPARISON_OPERATORS = frozenset(['==', '>=', '>', '<=', '<', '!=', '=~'])\n    INFIX_LITERAL_OPERATORS = frozenset(['==', '>=', '>', '<=', '<', '!=', '<>', '=~', 'in', 'contains'])\n    PREFIX_OPERATORS = frozenset([TOKEN_NOT])\n    RE_FLAG_MAP = {'a': re.A, 'i': re.I, 'm': re.M, 's': re.S}\n    _INVALID_NAME_SELECTOR_CHARS = f'[{''.join(INVALID_NAME_SELECTOR_CHARS)}]'\n    RE_INVALID_NAME_SELECTOR = re.compile(f'(?:(?!(?<!\\\\\\\\)\"){_INVALID_NAME_SELECTOR_CHARS})')\n\n    def parse(self, stream: TokenStream) -> Iterable[JSONPathSelector]:\n        \"\"\"Parse a JSONPath from a stream of tokens.\"\"\"\n        if stream.current.kind in {TOKEN_ROOT, TOKEN_FAKE_ROOT}:\n            stream.next_token()\n        yield from self.parse_path(stream, in_filter=False)\n        if stream.current.kind not in (TOKEN_EOF, TOKEN_INTERSECTION, TOKEN_UNION):\n            raise JSONPathSyntaxError(f'unexpected token {stream.current.value!r}', token=stream.current)\n\n    def parse_path(self, stream: TokenStream, *, in_filter: bool=False) -> Iterable[JSONPathSelector]:\n        \"\"\"Parse a top-level JSONPath, or one that is nested in a filter.\"\"\"\n        while True:\n            if stream.current.kind in (TOKEN_PROPERTY, TOKEN_BARE_PROPERTY):\n                yield PropertySelector(env=self.env, token=stream.current, name=stream.current.value, shorthand=True)\n            elif stream.current.kind == TOKEN_SLICE_START:\n                yield self.parse_slice(stream)\n            elif stream.current.kind == TOKEN_WILD:\n                yield WildSelector(env=self.env, token=stream.current, shorthand=True)\n            elif stream.current.kind == TOKEN_KEYS:\n                yield KeysSelector(env=self.env, token=stream.current, shorthand=True)\n            elif stream.current.kind == TOKEN_DDOT:\n                yield RecursiveDescentSelector(env=self.env, token=stream.current)\n            elif stream.current.kind == TOKEN_LIST_START:\n                yield self.parse_selector_list(stream)\n            else:\n                if in_filter:\n                    stream.push(stream.current)\n                break\n            stream.next_token()\n\n    def parse_slice(self, stream: TokenStream) -> SliceSelector:\n        \"\"\"Parse a slice JSONPath expression from a stream of tokens.\"\"\"\n        start_token = stream.next_token()\n        stream.expect(TOKEN_SLICE_STOP)\n        stop_token = stream.next_token()\n        stream.expect(TOKEN_SLICE_STEP)\n        step_token = stream.current\n        if not start_token.value:\n            start: Optional[int] = None\n        else:\n            start = int(start_token.value)\n        if not stop_token.value:\n            stop: Optional[int] = None\n        else:\n            stop = int(stop_token.value)\n        if not step_token.value:\n            step: Optional[int] = None\n        else:\n            step = int(step_token.value)\n        return SliceSelector(env=self.env, token=start_token, start=start, stop=stop, step=step)\n\n    def parse_selector_list(self, stream: TokenStream) -> ListSelector:\n        \"\"\"Parse a comma separated list JSONPath selectors from a stream of tokens.\"\"\"\n        tok = stream.next_token()\n        list_items: List[Union[IndexSelector, KeysSelector, PropertySelector, SliceSelector, WildSelector, Filter]] = []\n        while stream.current.kind != TOKEN_RBRACKET:\n            if stream.current.kind == TOKEN_INT:\n                if len(stream.current.value) > 1 and stream.current.value.startswith('0') or stream.current.value.startswith('-0'):\n                    raise JSONPathSyntaxError('leading zero in index selector', token=stream.current)\n                list_items.append(IndexSelector(env=self.env, token=stream.current, index=int(stream.current.value)))\n            elif stream.current.kind == TOKEN_BARE_PROPERTY:\n                list_items.append(PropertySelector(env=self.env, token=stream.current, name=stream.current.value, shorthand=False))\n            elif stream.current.kind == TOKEN_KEYS:\n                list_items.append(KeysSelector(env=self.env, token=stream.current, shorthand=False))\n            elif stream.current.kind in (TOKEN_DOUBLE_QUOTE_STRING, TOKEN_SINGLE_QUOTE_STRING):\n                if self.RE_INVALID_NAME_SELECTOR.search(stream.current.value):\n                    raise JSONPathSyntaxError(f'invalid name selector {stream.current.value!r}', token=stream.current)\n                list_items.append(PropertySelector(env=self.env, token=stream.current, name=self._decode_string_literal(stream.current), shorthand=False))\n            elif stream.current.kind == TOKEN_SLICE_START:\n                list_items.append(self.parse_slice(stream))\n            elif stream.current.kind == TOKEN_WILD:\n                list_items.append(WildSelector(env=self.env, token=stream.current, shorthand=False))\n            elif stream.current.kind == TOKEN_FILTER:\n                list_items.append(self.parse_filter(stream))\n            elif stream.current.kind == TOKEN_EOF:\n                raise JSONPathSyntaxError('unexpected end of query', token=stream.current)\n            else:\n                raise JSONPathSyntaxError(f'unexpected token in bracketed selection {stream.current.kind!r}', token=stream.current)\n            if stream.peek.kind == TOKEN_EOF:\n                raise JSONPathSyntaxError('unexpected end of selector list', token=stream.current)\n            if stream.peek.kind != TOKEN_RBRACKET:\n                stream.expect_peek(TOKEN_COMMA)\n                stream.next_token()\n                if stream.peek.kind == TOKEN_RBRACKET:\n                    raise JSONPathSyntaxError('unexpected trailing comma', token=stream.peek)\n            stream.next_token()\n        if not list_items:\n            raise JSONPathSyntaxError('empty bracketed segment', token=tok)\n        return ListSelector(env=self.env, token=tok, items=list_items)\n\n    def parse_filter(self, stream: TokenStream) -> Filter:\n        tok = stream.next_token()\n        expr = self.parse_filter_selector(stream)\n        if self.env.well_typed and isinstance(expr, FunctionExtension):\n            func = self.env.function_extensions.get(expr.name)\n            if func and isinstance(func, FilterFunction) and (func.return_type == ExpressionType.VALUE):\n                raise JSONPathTypeError(f'result of {expr.name}() must be compared', token=tok)\n        if isinstance(expr, (Literal, Nil)):\n            raise JSONPathSyntaxError('filter expression literals outside of function expressions must be compared', token=tok)\n        return Filter(env=self.env, token=tok, expression=BooleanExpression(expr))\n\n    def parse_boolean(self, stream: TokenStream) -> FilterExpression:\n        if stream.current.kind == TOKEN_TRUE:\n            return TRUE\n        return FALSE\n\n    def parse_nil(self, _: TokenStream) -> FilterExpression:\n        return NIL\n\n    def parse_undefined(self, _: TokenStream) -> FilterExpression:\n        return UNDEFINED_LITERAL\n\n    def parse_string_literal(self, stream: TokenStream) -> FilterExpression:\n        return StringLiteral(value=self._decode_string_literal(stream.current))\n\n    def parse_integer_literal(self, stream: TokenStream) -> FilterExpression:\n        return IntegerLiteral(value=int(float(stream.current.value)))\n\n    def parse_float_literal(self, stream: TokenStream) -> FilterExpression:\n        return FloatLiteral(value=float(stream.current.value))\n\n    def parse_prefix_expression(self, stream: TokenStream) -> FilterExpression:\n        tok = stream.next_token()\n        assert tok.kind == TOKEN_NOT\n        return PrefixExpression(operator='!', right=self.parse_filter_selector(stream, precedence=self.PRECEDENCE_PREFIX))\n\n    def parse_infix_expression(self, stream: TokenStream, left: FilterExpression) -> FilterExpression:\n        tok = stream.next_token()\n        precedence = self.PRECEDENCES.get(tok.kind, self.PRECEDENCE_LOWEST)\n        right = self.parse_filter_selector(stream, precedence)\n        operator = self.BINARY_OPERATORS[tok.kind]\n        if self.env.well_typed and operator in self.COMPARISON_OPERATORS:\n            self._raise_for_non_comparable_function(left, tok)\n            self._raise_for_non_comparable_function(right, tok)\n        if operator not in self.INFIX_LITERAL_OPERATORS:\n            if isinstance(left, (Literal, Nil)):\n                raise JSONPathSyntaxError('filter expression literals outside of function expressions must be compared', token=tok)\n            if isinstance(right, (Literal, Nil)):\n                raise JSONPathSyntaxError('filter expression literals outside of function expressions must be compared', token=tok)\n        return InfixExpression(left, operator, right)\n\n    def parse_grouped_expression(self, stream: TokenStream) -> FilterExpression:\n        stream.next_token()\n        expr = self.parse_filter_selector(stream)\n        stream.next_token()\n        while stream.current.kind != TOKEN_RPAREN:\n            if stream.current.kind == TOKEN_EOF:\n                raise JSONPathSyntaxError('unbalanced parentheses', token=stream.current)\n            if stream.current.kind not in self.BINARY_OPERATORS:\n                raise JSONPathSyntaxError(f\"expected an expression, found '{stream.current.value}'\", token=stream.current)\n            expr = self.parse_infix_expression(stream, expr)\n        stream.expect(TOKEN_RPAREN)\n        return expr\n\n    def parse_root_path(self, stream: TokenStream) -> FilterExpression:\n        root = stream.next_token()\n        return RootPath(JSONPath(env=self.env, selectors=self.parse_path(stream, in_filter=True), fake_root=root.kind == TOKEN_FAKE_ROOT))\n\n    def parse_self_path(self, stream: TokenStream) -> FilterExpression:\n        stream.next_token()\n        return SelfPath(JSONPath(env=self.env, selectors=self.parse_path(stream, in_filter=True)))\n\n    def parse_current_key(self, _: TokenStream) -> FilterExpression:\n        return CURRENT_KEY\n\n    def parse_filter_context_path(self, stream: TokenStream) -> FilterExpression:\n        stream.next_token()\n        return FilterContextPath(JSONPath(env=self.env, selectors=self.parse_path(stream, in_filter=True)))\n\n    def parse_regex(self, stream: TokenStream) -> FilterExpression:\n        pattern = stream.current.value\n        flags = 0\n        if stream.peek.kind == TOKEN_RE_FLAGS:\n            stream.next_token()\n            for flag in set(stream.current.value):\n                flags |= self.RE_FLAG_MAP[flag]\n        return RegexLiteral(value=re.compile(pattern, flags))\n\n    def parse_list_literal(self, stream: TokenStream) -> FilterExpression:\n        stream.next_token()\n        list_items: List[FilterExpression] = []\n        while stream.current.kind != TOKEN_RBRACKET:\n            try:\n                list_items.append(self.list_item_map[stream.current.kind](stream))\n            except KeyError as err:\n                raise JSONPathSyntaxError(f'unexpected {stream.current.value!r}', token=stream.current) from err\n            if stream.peek.kind != TOKEN_RBRACKET:\n                stream.expect_peek(TOKEN_COMMA)\n                stream.next_token()\n            stream.next_token()\n        return ListLiteral(list_items)\n\n    def parse_function_extension(self, stream: TokenStream) -> FilterExpression:\n        function_arguments: List[FilterExpression] = []\n        tok = stream.next_token()\n        while stream.current.kind != TOKEN_RPAREN:\n            try:\n                func = self.function_argument_map[stream.current.kind]\n            except KeyError as err:\n                raise JSONPathSyntaxError(f'unexpected {stream.current.value!r}', token=stream.current) from err\n            expr = func(stream)\n            peek_kind = stream.peek.kind\n            while peek_kind in self.BINARY_OPERATORS:\n                stream.next_token()\n                expr = self.parse_infix_expression(stream, expr)\n                peek_kind = stream.peek.kind\n            function_arguments.append(expr)\n            if stream.peek.kind != TOKEN_RPAREN:\n                stream.expect_peek(TOKEN_COMMA)\n                stream.next_token()\n            stream.next_token()\n        return FunctionExtension(tok.value, self.env.validate_function_extension_signature(tok, function_arguments))\n\n    def parse_filter_selector(self, stream: TokenStream, precedence: int=PRECEDENCE_LOWEST) -> FilterExpression:\n        try:\n            left = self.token_map[stream.current.kind](stream)\n        except KeyError as err:\n            if stream.current.kind in (TOKEN_EOF, TOKEN_RBRACKET):\n                msg = 'end of expression'\n            else:\n                msg = repr(stream.current.value)\n            raise JSONPathSyntaxError(f'unexpected {msg}', token=stream.current) from err\n        while True:\n            peek_kind = stream.peek.kind\n            if peek_kind in (TOKEN_EOF, TOKEN_RBRACKET) or self.PRECEDENCES.get(peek_kind, self.PRECEDENCE_LOWEST) < precedence:\n                break\n            if peek_kind not in self.BINARY_OPERATORS:\n                return left\n            stream.next_token()\n            left = self.parse_infix_expression(stream, left)\n        return left\n\n    def _decode_string_literal(self, token: Token) -> str:\n        if self.env.unicode_escape:\n            if token.kind == TOKEN_SINGLE_QUOTE_STRING:\n                value = token.value.replace('\"', '\\\\\"').replace(\"\\\\'\", \"'\")\n            else:\n                value = token.value\n            try:\n                rv = json.loads(f'\"{value}\"')\n                assert isinstance(rv, str)\n                return rv\n            except json.JSONDecodeError as err:\n                raise JSONPathSyntaxError(str(err).split(':')[1], token=token) from None\n        return token.value\n\n    def _raise_for_non_comparable_function(self, expr: FilterExpression, token: Token) -> None:\n        if isinstance(expr, Path) and (not expr.path.singular_query()):\n            raise JSONPathTypeError('non-singular query is not comparable', token=token)\n        if isinstance(expr, FunctionExtension):\n            func = self.env.function_extensions.get(expr.name)\n            if isinstance(func, FilterFunction) and func.return_type != ExpressionType.VALUE:\n                raise JSONPathTypeError(f'result of {expr.name}() is not comparable', token)"
  }
}