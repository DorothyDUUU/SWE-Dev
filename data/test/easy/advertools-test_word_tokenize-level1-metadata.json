{
  "dir_path": "/app/advertools",
  "package_name": "advertools",
  "sample_name": "advertools-test_word_tokenize",
  "src_dir": "advertools/",
  "test_dir": "tests/",
  "test_file": "tests/test_word_tokenize.py",
  "test_code": "from advertools.word_tokenize import word_tokenize\n\n\ndef test_word_tokenize_splits_by_correct_number():\n    s = ['this is a text to split by different lengths']\n    for i in range(1, 4):\n        result = word_tokenize(s, i)\n        assert {word.count(' ') for word in result[0]}.pop() == i - 1\n\n\ndef test_word_tokenize_converts_str_to_list():\n    s = 'this is a normal string'\n    result = word_tokenize(s)\n    assert isinstance(result, list)\n",
  "GT_file_code": {
    "advertools/word_tokenize.py": "\"\"\"\n.. _word_tokenize:\n\nTokenize Words (N-grams)\n========================\n\nAs word counting is an essential step in any text mining task, you first have\nto split the text into words.\n\nThe :func:`word_tokenize` function achieves that by splitting the text by\nwhitespace. Another important thing it does after splitting is to trim the\nwords of any non-word characters (commas, dots, exclamation marks, etc.).\n\nYou also have the option of specifying the length of the words that you want.\nThe default is 2, which can be set through the :attr:`phrase_len` parameter.\n\nThis function is mainly a helper function for\n:ref:`word_frequency <word_frequency>` to help with counting words and/or\nphrases.\n\"\"\"\nfrom .regex import WORD_DELIM\n\n\ndef word_tokenize(text_list, phrase_len=2):\n    \"\"\"Split ``text_list`` into phrases of length ``phrase_len`` words each.\n\n    A \"word\" is any string between white spaces (or beginning or\n    end of string) with delimiters stripped from both sides.\n    Delimiters include quotes, question marks, parentheses, etc.\n    Any delimiter contained within the string remains. See examples below.\n\n    :param text_list: List of strings.\n    :param phrase_len: Length of word tokens, defaults to 2.\n    :return tokenized: List of lists, split according to :attr:`phrase_len`.\n\n    >>> t = ['split me into length-n-words',\n    ... 'commas, (parentheses) get removed!',\n    ... 'commas within text remain $1,000, but not the trailing commas.']\n\n    >>> word_tokenize(t, 1)\n    [['split', 'me', 'into', 'length-n-words'],\n    ['commas', 'parentheses', 'get', 'removed'],\n    ['commas', 'within', 'text', 'remain', '$1,000',\n    'but', 'not', 'the', 'trailing', 'commas']]\n\n\n    The comma inside \"$1,000\" as well as the dollar sign remain, as they\n    are part of the \"word\", but the trailing comma is stripped.\n\n    >>> word_tokenize(t, 2)\n    [['split me', 'me into', 'into length-n-words'],\n    ['commas parentheses', 'parentheses get', 'get removed'],\n    ['commas within', 'within text', 'text remain', 'remain $1,000',\n    '$1,000 but', 'but not', 'not the', 'the trailing', 'trailing commas']]\n\n\n    >>> word_tokenize(t, 3)\n    [['split me into', 'me into length-n-words'],\n    ['commas parentheses get', 'parentheses get removed'],\n    ['commas within text', 'within text remain', 'text remain $1,000',\n    'remain $1,000 but', '$1,000 but not', 'but not the',\n    'not the trailing', 'the trailing commas']]\n    \"\"\"\n    if isinstance(text_list, str):\n        text_list = [text_list]\n    split = [text.lower().split() for text in text_list]\n    split = [[word.strip(WORD_DELIM) for word in text] for text in split]\n\n    return [[' '.join(s[i:i + phrase_len])\n             for i in range(len(s) - phrase_len + 1)] for s in split]\n"
  },
  "GT_src_dict": {
    "advertools/word_tokenize.py": {
      "word_tokenize": {
        "code": "def word_tokenize(text_list, phrase_len=2):\n    \"\"\"Split a list of strings into phrases of a specified word length.\n\nThe `word_tokenize` function takes a list of strings and splits each string \ninto phrases of length `phrase_len` words. It first normalizes the text by \nlowercasing and removing surrounding non-word characters defined by the \nconstant `WORD_DELIM`. The function ensures that delimiters within the words \nare preserved.\n\nParameters:\n- `text_list` (list of str): A list of strings to be tokenized into phrases.\n- `phrase_len` (int, optional): The desired length of word phrases, defaulting to 2.\n\nReturns:\n- list of lists: Each inner list contains the tokenized phrases for the \n  corresponding string in `text_list`.\n\nExample:\n>>> word_tokenize(['Hello, world!', 'Python is great.'], 1)\n[['hello', 'world'], ['python', 'is', 'great']]\n\nThe constant `WORD_DELIM`, imported from the module `.regex`, designates \nwhich characters to strip from the start and end of each word.\"\"\"\n    'Split ``text_list`` into phrases of length ``phrase_len`` words each.\\n\\n    A \"word\" is any string between white spaces (or beginning or\\n    end of string) with delimiters stripped from both sides.\\n    Delimiters include quotes, question marks, parentheses, etc.\\n    Any delimiter contained within the string remains. See examples below.\\n\\n    :param text_list: List of strings.\\n    :param phrase_len: Length of word tokens, defaults to 2.\\n    :return tokenized: List of lists, split according to :attr:`phrase_len`.\\n\\n    >>> t = [\\'split me into length-n-words\\',\\n    ... \\'commas, (parentheses) get removed!\\',\\n    ... \\'commas within text remain $1,000, but not the trailing commas.\\']\\n\\n    >>> word_tokenize(t, 1)\\n    [[\\'split\\', \\'me\\', \\'into\\', \\'length-n-words\\'],\\n    [\\'commas\\', \\'parentheses\\', \\'get\\', \\'removed\\'],\\n    [\\'commas\\', \\'within\\', \\'text\\', \\'remain\\', \\'$1,000\\',\\n    \\'but\\', \\'not\\', \\'the\\', \\'trailing\\', \\'commas\\']]\\n\\n\\n    The comma inside \"$1,000\" as well as the dollar sign remain, as they\\n    are part of the \"word\", but the trailing comma is stripped.\\n\\n    >>> word_tokenize(t, 2)\\n    [[\\'split me\\', \\'me into\\', \\'into length-n-words\\'],\\n    [\\'commas parentheses\\', \\'parentheses get\\', \\'get removed\\'],\\n    [\\'commas within\\', \\'within text\\', \\'text remain\\', \\'remain $1,000\\',\\n    \\'$1,000 but\\', \\'but not\\', \\'not the\\', \\'the trailing\\', \\'trailing commas\\']]\\n\\n\\n    >>> word_tokenize(t, 3)\\n    [[\\'split me into\\', \\'me into length-n-words\\'],\\n    [\\'commas parentheses get\\', \\'parentheses get removed\\'],\\n    [\\'commas within text\\', \\'within text remain\\', \\'text remain $1,000\\',\\n    \\'remain $1,000 but\\', \\'$1,000 but not\\', \\'but not the\\',\\n    \\'not the trailing\\', \\'the trailing commas\\']]\\n    '\n    if isinstance(text_list, str):\n        text_list = [text_list]\n    split = [text.lower().split() for text in text_list]\n    split = [[word.strip(WORD_DELIM) for word in text] for text in split]\n    return [[' '.join(s[i:i + phrase_len]) for i in range(len(s) - phrase_len + 1)] for s in split]",
        "docstring": "Split a list of strings into phrases of a specified word length.\n\nThe `word_tokenize` function takes a list of strings and splits each string \ninto phrases of length `phrase_len` words. It first normalizes the text by \nlowercasing and removing surrounding non-word characters defined by the \nconstant `WORD_DELIM`. The function ensures that delimiters within the words \nare preserved.\n\nParameters:\n- `text_list` (list of str): A list of strings to be tokenized into phrases.\n- `phrase_len` (int, optional): The desired length of word phrases, defaulting to 2.\n\nReturns:\n- list of lists: Each inner list contains the tokenized phrases for the \n  corresponding string in `text_list`.\n\nExample:\n>>> word_tokenize(['Hello, world!', 'Python is great.'], 1)\n[['hello', 'world'], ['python', 'is', 'great']]\n\nThe constant `WORD_DELIM`, imported from the module `.regex`, designates \nwhich characters to strip from the start and end of each word.",
        "signature": "def word_tokenize(text_list, phrase_len=2):",
        "type": "Function",
        "class_signature": null
      }
    }
  },
  "dependency_dict": {},
  "PRD": "# PROJECT NAME: advertools-test_word_tokenize\n\n# FOLDER STRUCTURE:\n```\n..\n\u2514\u2500\u2500 advertools/\n    \u2514\u2500\u2500 word_tokenize.py\n        \u2514\u2500\u2500 word_tokenize\n```\n\n# IMPLEMENTATION REQUIREMENTS:\n## MODULE DESCRIPTION:\nThe module facilitates text preprocessing by providing a utility for tokenizing text into segments of specified lengths, supporting both string and list inputs. Its core functionality lies in breaking down text into manageable word groups to aid in natural language processing tasks or other analytical workflows. By ensuring the flexibility to tokenize by different word counts and handling various input formats, the module simplifies the process of preparing text data for further processing, enabling developers to focus on higher-level application logic. This ensures accuracy and efficiency, eliminating the need for custom tokenization logic in user implementations.\n\n## FILE 1: advertools/word_tokenize.py\n\n- FUNCTION NAME: word_tokenize\n  - SIGNATURE: def word_tokenize(text_list, phrase_len=2):\n  - DOCSTRING: \n```python\n\"\"\"\nSplit a list of strings into phrases of a specified word length.\n\nThe `word_tokenize` function takes a list of strings and splits each string \ninto phrases of length `phrase_len` words. It first normalizes the text by \nlowercasing and removing surrounding non-word characters defined by the \nconstant `WORD_DELIM`. The function ensures that delimiters within the words \nare preserved.\n\nParameters:\n- `text_list` (list of str): A list of strings to be tokenized into phrases.\n- `phrase_len` (int, optional): The desired length of word phrases, defaulting to 2.\n\nReturns:\n- list of lists: Each inner list contains the tokenized phrases for the \n  corresponding string in `text_list`.\n\nExample:\n>>> word_tokenize(['Hello, world!', 'Python is great.'], 1)\n[['hello', 'world'], ['python', 'is', 'great']]\n\nThe constant `WORD_DELIM`, imported from the module `.regex`, designates \nwhich characters to strip from the start and end of each word.\n\"\"\"\n```\n\n# TASK DESCRIPTION:\nIn this project, you need to implement the functions and methods listed above. The functions have been removed from the code but their docstrings remain.\nYour task is to:\n1. Read and understand the docstrings of each function/method\n2. Understand the dependencies and how they interact with the target functions\n3. Implement the functions/methods according to their docstrings and signatures\n4. Ensure your implementations work correctly with the rest of the codebase\n",
  "file_code": {
    "advertools/word_tokenize.py": "\"\"\"\n.. _word_tokenize:\n\nTokenize Words (N-grams)\n========================\n\nAs word counting is an essential step in any text mining task, you first have\nto split the text into words.\n\nThe :func:`word_tokenize` function achieves that by splitting the text by\nwhitespace. Another important thing it does after splitting is to trim the\nwords of any non-word characters (commas, dots, exclamation marks, etc.).\n\nYou also have the option of specifying the length of the words that you want.\nThe default is 2, which can be set through the :attr:`phrase_len` parameter.\n\nThis function is mainly a helper function for\n:ref:`word_frequency <word_frequency>` to help with counting words and/or\nphrases.\n\"\"\"\nfrom .regex import WORD_DELIM"
  },
  "call_tree": {
    "tests/test_word_tokenize.py:test_word_tokenize_splits_by_correct_number": {
      "advertools/word_tokenize.py:word_tokenize": {}
    },
    "tests/test_word_tokenize.py:test_word_tokenize_converts_str_to_list": {
      "advertools/word_tokenize.py:word_tokenize": {}
    }
  }
}